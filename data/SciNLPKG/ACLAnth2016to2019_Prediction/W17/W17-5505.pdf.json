{"title": [{"text": "Generative Encoder-Decoder Models for Task-Oriented Spoken Dialog Systems with Chatting Capability", "labels": [], "entities": []}], "abstractContent": [{"text": "Generative encoder-decoder models offer great promise in developing domain-general dialog systems.", "labels": [], "entities": []}, {"text": "However, they have mainly been applied to open-domain conversations.", "labels": [], "entities": []}, {"text": "This paper presents a practical and novel framework for building task-oriented dialog systems based on encoder-decoder models.", "labels": [], "entities": []}, {"text": "This framework enables encoder-decoder models to accomplish slot-value independent decision-making and interact with external databases.", "labels": [], "entities": []}, {"text": "Moreover, this paper shows the flexibility of the proposed method by in-terleaving chatting capability with a slot-filling system for better out-of-domain recovery.", "labels": [], "entities": [{"text": "out-of-domain recovery", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.7629709243774414}]}, {"text": "The models were trained on both real-user data from a bus information system and human-human chat data.", "labels": [], "entities": []}, {"text": "Results show that the proposed framework achieves good performance in both offline evaluation metrics and in task success rate with human users.", "labels": [], "entities": []}], "introductionContent": [{"text": "Task-oriented spoken dialog systems have transformed human-computer interaction by enabling people interact with computers via spoken language (.", "labels": [], "entities": []}, {"text": "The task-oriented SDS is usually domain-specific.", "labels": [], "entities": []}, {"text": "The system creators first map the user utterances into semantic frames that contain domain-specific slots and intents using spoken language understanding (SLU).", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 124, "end_pos": 159, "type": "TASK", "confidence": 0.7384619613488516}]}, {"text": "Then a set of domain-specific dialog state variables is tracked to retain the context information over turns (.", "labels": [], "entities": []}, {"text": "Lastly, the dialog policy decides the next move from a list of dialog acts that covers the expected communicative functions from the system.", "labels": [], "entities": []}, {"text": "Although the above approach has been successfully applied to many practical systems, it has limited ability to generalize to out-of-domain (OOD) requests and to scale up to new domains.", "labels": [], "entities": []}, {"text": "For example, even within in a simple domain, real users often make requests that are not included in the semantic specifications.", "labels": [], "entities": []}, {"text": "Due to this, proper error handling strategies that guide users back to the in-domain conversation are crucial to dialog success ().", "labels": [], "entities": [{"text": "error handling", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.6731884032487869}]}, {"text": "Past error handling strategies were limited to a set of predefined dialog acts, e.g. request repeat, clarification etc., which constrained the system's capability in keeping users engaged.", "labels": [], "entities": [{"text": "error handling", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.7298293113708496}]}, {"text": "Moreover, there has been an increased interest in extending task-oriented systems to multiple topics () and multiple skills, e.g. grouping heterogeneous types of dialogs into a single system ( . Both cases require the system to be flexible enough to extend to new slots and actions.", "labels": [], "entities": []}, {"text": "Our goal is to move towards a domain-general task-oriented SDS framework that is flexible enough to expand to new domains and skills by removing domain-specific assumptions on the dialog state and dialog acts.", "labels": [], "entities": []}, {"text": "To achieve this goal, the neural encoderdecoder model() is a suitable choice, since it has achieved promising results in modeling open-domain conversations (.", "labels": [], "entities": []}, {"text": "It encodes the dialog history using deep neural networks and then generates the next system utterance word-by-word via recurrent neural networks (RNNs).", "labels": [], "entities": []}, {"text": "Therefore, unlike the traditional SDS pipeline, the encoder-decoder model is theoretically only limited by its input/output vocabulary.", "labels": [], "entities": []}, {"text": "A n\u00e0 'ive implementation of an encoderdecoder-based task-oriented system would use RNNs to encode the raw dialog history and generate the next system utterance using a separate RNN decoder.", "labels": [], "entities": []}, {"text": "However, while this implementation might achieve good performance in an offline evaluation of a closed dataset, it would certainly fail when used by humans.", "labels": [], "entities": []}, {"text": "There are several reasons for this: 1) real users can mention new entities that do not appear in the training data, such as anew restaurant name.", "labels": [], "entities": []}, {"text": "These entities are, however, essential in delivering the information that matches users' needs in a task-oriented system.", "labels": [], "entities": []}, {"text": "2) a task-oriented SDS obtains information from a knowledge base (KB) that is constantly updated (\"today's\" weather will be different every day), so simply memorizing KB results that occurred in the training data would produce false information.", "labels": [], "entities": []}, {"text": "Instead, an effective model should learn to query the KB constantly to get the most up-to-date information.", "labels": [], "entities": []}, {"text": "3) users may give OOD requests (e.g. say, \"how is your day\", to a slot-filling system), which must be handled gracefully in order to keep the conversation moving in the intended direction.", "labels": [], "entities": []}, {"text": "This paper proposes an effective encoderdecoder framework for building task-oriented SDSs.", "labels": [], "entities": []}, {"text": "We propose entity indexing to tackle the challenges of out-of-vocabulary (OOV) entities and to query the KB.", "labels": [], "entities": [{"text": "entity indexing", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7104923129081726}]}, {"text": "Moreover, we show the extensibility of the proposed model by adding chatting capability to a task-oriented encoder-decoder SDS for better OOD recovery.", "labels": [], "entities": [{"text": "OOD recovery", "start_pos": 138, "end_pos": 150, "type": "TASK", "confidence": 0.9191658198833466}]}, {"text": "This approach was assessed on the Let's Go Bus Information data from the 1st Dialog State Tracking Challenge (, and we report performance on both offline metrics and real human users.", "labels": [], "entities": [{"text": "Let's Go Bus Information data from the 1st Dialog State Tracking Challenge", "start_pos": 34, "end_pos": 108, "type": "DATASET", "confidence": 0.7822089539124415}]}, {"text": "Results show that this model attains good performance for both of these metrics.", "labels": [], "entities": []}], "datasetContent": [{"text": "The CMU Let's Go Bus Information System () is a task-oriented spoken dialog system that contains bus information.", "labels": [], "entities": [{"text": "CMU Let's Go Bus Information System", "start_pos": 4, "end_pos": 39, "type": "DATASET", "confidence": 0.7666722791535514}]}, {"text": "We combined the train1a and train1b datasets from DSTC 1 (, which contain 2608 total dialogs.", "labels": [], "entities": [{"text": "DSTC 1", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8556123673915863}]}, {"text": "The average dialog length is 9.07 turns.", "labels": [], "entities": []}, {"text": "The dialogs were randomly splitted into 85/5/10 proportions for train/dev/test data.", "labels": [], "entities": []}, {"text": "The data was noisy since the dialogs were collected from real users via telephone lines.", "labels": [], "entities": []}, {"text": "Furthermore, this version of Let's Go used an inhouse database containing the Port Authority bus schedule.", "labels": [], "entities": [{"text": "Port Authority bus schedule", "start_pos": 78, "end_pos": 105, "type": "DATASET", "confidence": 0.9011862277984619}]}, {"text": "In the current version, that database was replaced with the Google Directions API, which both reduces the human burden of maintaining a database and opens the possibility of extending Let's Go to cities other than Pittsburgh.", "labels": [], "entities": []}, {"text": "Connecting to Google Directions API involves a POST call to their URL, with our given access key as well as the parameters needed: departure place, arrival place and departure time, and the travel mode, which we always set as TRANSIT to obtain relevant bus routes.", "labels": [], "entities": [{"text": "TRANSIT", "start_pos": 226, "end_pos": 233, "type": "METRIC", "confidence": 0.9864397644996643}]}, {"text": "There are 14 distinct dialog acts available to the system, and each system utterance contains one or more dialog acts.", "labels": [], "entities": []}, {"text": "Lastly, the system vocabulary size is 1311 and the user vocabulary size is 1232.", "labels": [], "entities": []}, {"text": "After the EI process, the sizes become 214 and 936, respectively.", "labels": [], "entities": []}, {"text": "For chat data, we use a publicly available chat corpus used in ( 1 . In total, there are 3793 chatting adjacency pairs.", "labels": [], "entities": []}, {"text": "We control the number of data injections to 30% of the number of turns in the original DTSC dataset, which leads to a user vocabulary size of 3537 and system vocabulary size of 4047.", "labels": [], "entities": [{"text": "DTSC dataset", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.9561767280101776}]}, {"text": "This approach was assessed both offline and online evaluations.", "labels": [], "entities": []}, {"text": "The offline evaluation contains standard metrics to test open-domain encoderdecoder dialog models (.", "labels": [], "entities": []}, {"text": "System performance was assessed from three perspectives that are essential for taskoriented systems: dialog acts, slot-values, and KB query.", "labels": [], "entities": []}, {"text": "The online evaluation is composed of objective task success rate, the number of turns, and subjective satisfaction with human users.", "labels": [], "entities": []}, {"text": "Dialog Acts (DA): Each system utterance is made up of one or more dialog acts, e.g. \"leaving at, where do you want to go?\"", "labels": [], "entities": [{"text": "Dialog Acts (DA)", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5359032809734344}]}, {"text": "\u2192 [implicit-confirm, request(arrival place)].", "labels": [], "entities": []}, {"text": "To evaluate whether a generated utterance has the same dialog acts as the ground truth, we trained a multilabel dialog tagger using one-vs-rest Support Vector Machines (SVM) (), with bag-of-bigram features for each dialog act label.", "labels": [], "entities": []}, {"text": "Since the natural language generation module in Let's Go is handcrafted, the dialog act tagger achieved 99.4% average label accuracy on a held-out dataset.", "labels": [], "entities": [{"text": "dialog act tagger", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7125640114148458}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9600942134857178}]}, {"text": "We used this dialog act tagger to tag both the ground truth and the generated 1 github.com/echoyuzhou/ticktock text api responses.", "labels": [], "entities": []}, {"text": "Then we computed the micro-average precision, recall, and the F-score.", "labels": [], "entities": [{"text": "micro-average", "start_pos": 21, "end_pos": 34, "type": "METRIC", "confidence": 0.9527671337127686}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.8281012773513794}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9997754693031311}, {"text": "F-score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9979111552238464}]}, {"text": "Slots: This metric measures the model's performance in generating the correct slot-values.", "labels": [], "entities": []}, {"text": "The slot-values mostly occur in grounding utterances (e.g. explicit/implicit confirm) and KB queries.", "labels": [], "entities": []}, {"text": "We compute precision, recall, and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9994889497756958}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9995394945144653}, {"text": "F-score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9988369345664978}]}, {"text": "KB Queries: Although the slots metric already covers the KB queries, here the precision/recall/Fscore of system utterances that contain KB queries are also explicitly measured, due to their importance.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9993500113487244}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.7243737578392029}, {"text": "Fscore", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.7351702451705933}]}, {"text": "Specifically, this action measures whether the system is able to generate the special [kbquery] symbol to initiate a KB query, as well as how accurate the corresponding KB query arguments are.", "labels": [], "entities": []}, {"text": "BLEU (): compares the ngram precision with length penalty, and has been a popular score used to evaluate the performance of natural language generation (  and open-domain dialog models (: Performance of each model on automatic measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9859515428543091}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.5221177935600281}, {"text": "length penalty", "start_pos": 43, "end_pos": 57, "type": "METRIC", "confidence": 0.9601874053478241}]}, {"text": "Four systems were compared: the basic encoder-decoder models without EI (vanilla), the basic model with EI pre-processing (EI), the model with attentional decoder (EI+Attn) and the model trained on the dataset augmented with chatting data (EI+Attn+Chat).", "labels": [], "entities": [{"text": "EI pre-processing (EI)", "start_pos": 104, "end_pos": 126, "type": "METRIC", "confidence": 0.771013879776001}]}, {"text": "The comparison was carried out on exactly the same held-out test dataset that contains 261 dialogs.", "labels": [], "entities": []}, {"text": "It can be seen that all four models achieve similar performance on the dialog act metrics, even the vanilla model.", "labels": [], "entities": []}, {"text": "This confirms the capacity of encoder-decoders models to learn the \"shape\" of a conversation, since they have achieved impressive results in more challenging settings, e.g. modeling open-domain conversations.", "labels": [], "entities": []}, {"text": "Furthermore, since the DSTC1 data was collected over several months, there were minor updates made to the dialog manager.", "labels": [], "entities": [{"text": "DSTC1 data", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.9454079568386078}]}, {"text": "Therefore, there are inherent ambiguities in the data (the dialog manager may take different actions in the same situation).", "labels": [], "entities": []}, {"text": "We conjecture that \u223c80% is near the upper limit of our data in modeling the system's next dialog act given the dialog history.", "labels": [], "entities": []}, {"text": "On the other hand, these proposed methods significantly improved the metrics related to slots and KB queries.", "labels": [], "entities": []}, {"text": "The inclusion of EI alone was able to improve the F-score of slots by a relative 76%, which confirms that EI is crucial in developing slot-value independent encoder-decoder models for modeling task-oriented dialogs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.99759441614151}]}, {"text": "Likewise, the inclusion of attention further improved the prediction of slots in system utterances.", "labels": [], "entities": [{"text": "prediction of slots in system utterances", "start_pos": 58, "end_pos": 98, "type": "TASK", "confidence": 0.659670909245809}]}, {"text": "Adding attention also improved the performance of predicting KB queries, more so than the overall slot accuracy.", "labels": [], "entities": [{"text": "predicting KB queries", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8526290655136108}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.6985523104667664}]}, {"text": "This is expected, since KB queries are usually issued near the end of a conversation, which requires global reasoning over the entire dialog history.", "labels": [], "entities": []}, {"text": "The use of attention allows the decoder to look over the history and make better decisions rather than simply depending on the context summary in the last hidden layer of the encoder.", "labels": [], "entities": []}, {"text": "Because of the good performance achieved by the models with the attentional decoder, the attention weights in Equation 1 at every step of the decoding process in two example dialogs from test data are visualized.", "labels": [], "entities": [{"text": "Equation", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9448502063751221}]}, {"text": "For both figures, the vertical axes show the dialog history flowing from the top to the bottom.", "labels": [], "entities": []}, {"text": "Each row is a turn in the format of (system utterance # user utterance).", "labels": [], "entities": []}, {"text": "The top horizontal axis shows the predicted next system utterance.", "labels": [], "entities": []}, {"text": "The darkness of a bar indicates the value of the attention calculated in Equation 1.", "labels": [], "entities": [{"text": "Equation", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9864915013313293}]}, {"text": "The first example shows attention for grounding the new entity in the previous turn.", "labels": [], "entities": []}, {"text": "The attention weights become focus on the previous turn when predicting in the implicit confirm action.", "labels": [], "entities": []}, {"text": "The second dialog example shows a more challenging situation, in which the model is predicting a KB query.", "labels": [], "entities": []}, {"text": "We can see that the attention weights when generating each input argument of the KB query clearly focus on the specific mention in the dialog history.", "labels": [], "entities": []}, {"text": "The visualization confirms the effectiveness of the attention mechanism in dealing with long-term dependency at discourse level.", "labels": [], "entities": []}, {"text": "Surprisingly, the model trained on the data augmented with chat achieved slightly better slot accuracy performance, even though the augmented data is not directly related to task-oriented dialogs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.7404926419258118}]}, {"text": "Furthermore, the model trained on chataugmented data achieved better scores for the KB query metrics.", "labels": [], "entities": []}, {"text": "Several reasons may explain this improvement: 1) since chat data exposes the model to a significantly larger vocabulary, the resulting model is more robust to words that it had not seen in the original task-oriented-only training data, and 2) the augmented dialog turn can be seen as noise in the dialog history, which adds extra regularization to the model and enables the model to learn more robust long-term reasoning mechanisms.", "labels": [], "entities": []}, {"text": "Although the model achieves good performance in offline evaluation, this may not carray over to real user dialogs, where the system must simultaneously deal with several challenges, such as automatic speech recognition (ASR) errors, OOD requests, etc.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 190, "end_pos": 224, "type": "TASK", "confidence": 0.747158924738566}]}, {"text": "Therefore, areal user study was conducted to evaluate the performance of the proposed systems in the real world.", "labels": [], "entities": []}, {"text": "Due to the limited number of real users, only two best performing system were compared, EI+Attn and EI+Attn+Chat.", "labels": [], "entities": [{"text": "EI+Attn", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.8070858915646871}, {"text": "EI+Attn+Chat", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.8221564292907715}]}, {"text": "Users were able to talk to a web interface to the dialog systems via speech.", "labels": [], "entities": []}, {"text": "Google Chrome Speech API 2 served as the ASR and textto-speech (TTS) modules.", "labels": [], "entities": []}, {"text": "Turn-taking was done via the built-in Chrome voice activity detection (VAD) plus a finite state machine-based end-ofturn detector (.", "labels": [], "entities": [{"text": "Chrome voice activity detection (VAD)", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.6176952038492475}]}, {"text": "Lastly, a hybrid named entity recognizer (NER) was trained using Conditional Random Field (CRF)) and rules to extract 4 types of entities (location, hour, minute, pm/am) for the EI process.", "labels": [], "entities": [{"text": "entity recognizer (NER", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7437578439712524}]}, {"text": "The experiment setup is as follows: when a user logs into the website, the system prompts the user with a goal, which is a randomly chosen combination of departure place, arrival place and time (e.g. leave from CMU and go to the airport at 10:30 AM).", "labels": [], "entities": [{"text": "CMU", "start_pos": 211, "end_pos": 214, "type": "DATASET", "confidence": 0.9407148361206055}]}, {"text": "The system also instructs the user to say goodbye if the he/she thinks the goal is achieved or wants to give up.", "labels": [], "entities": []}, {"text": "The user begins a conversation with one of the two evaluated systems, with a 50/50 chance of choosing either system (not visible to the user).", "labels": [], "entities": []}, {"text": "After the user's session is finished, the system asks the him/her to give two scores between 1 and 5 for correctness and naturalness of the system respectively.", "labels": [], "entities": []}, {"text": "The subjects in this study consist of undergraduate and graduate students.", "labels": [], "entities": []}, {"text": "However, many subjects did not follow the prompted goal, but rather asked about bus routes of their own.", "labels": [], "entities": []}, {"text": "Therefore, the dialog was manually labeled for dialog success.", "labels": [], "entities": []}, {"text": "A dialog is successful if and only if the systems give at least one bus schedule that matches with all three slots expressed by the users.  was not statistically significant due to the limited number of subjects).", "labels": [], "entities": []}, {"text": "The precision of grounding the correct slots and predicting the correct KB query was also manually labelled.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991457462310791}]}, {"text": "EI+Attn model performs slightly better than the EI+Attn+Chat model in slot precision, while the latter model performs significantly better in KB query precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.8102612495422363}]}, {"text": "In addition, EI+Attn+Chat leads to slightly longer dialogs because sometimes it generates chatting utterances with users when it cannot understand users' utterances.", "labels": [], "entities": []}, {"text": "At last, we investigated the log files and identified the following major types of sources of dialog failure: RNN Decoder Invalid Output: Occasionally, the RNN decoder outputs system utterances as \"Okay going to.", "labels": [], "entities": []}, {"text": "Did I get that right?\", in which cannot be found in the indexed entity table.", "labels": [], "entities": []}, {"text": "Such invalid output confuses users.", "labels": [], "entities": []}, {"text": "This occurred in 149 of the dialogs, where 4.1% of system utterances contain invalid symbols.", "labels": [], "entities": []}, {"text": "Imitation of Suboptimal Dialog Policy: Since our models are only trained to imitate the suboptimal hand-crafted dialog policy, their limitations show when the original dialog manager cannot handle the situation, such as failing to understand slots that appeared in compound utterances.", "labels": [], "entities": [{"text": "Imitation of Suboptimal Dialog Policy", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7715377688407898}]}, {"text": "Future plans involves improving the models to perform better than the suboptimal teacher policy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of each model on automatic  measures.", "labels": [], "entities": []}, {"text": " Table 2: Performance of each model on automatic  measures. The standard deviations of subjective  scores are in parentheses.", "labels": [], "entities": []}]}