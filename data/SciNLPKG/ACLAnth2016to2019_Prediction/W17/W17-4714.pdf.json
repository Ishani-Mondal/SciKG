{"title": [{"text": "Adapting Neural Machine Translation with Parallel Synthetic Data", "labels": [], "entities": [{"text": "Adapting Neural Machine Translation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8877647370100021}]}], "abstractContent": [{"text": "Recent works have shown that the usage of a synthetic parallel corpus can be effectively exploited by a neural machine translation system.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew method for adapting a general neu-ral machine translation system to a specific task, by exploiting synthetic data.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.5884144107500712}]}, {"text": "The method consists in selecting, from a large monolingual pool of sentences in the source language, those instances that are more related to a given test set.", "labels": [], "entities": []}, {"text": "Next, this selection is automatically translated and the general neural machine translation system is fine-tuned with these data.", "labels": [], "entities": []}, {"text": "For evaluating the adaptation method, we first conducted experiments in two controlled domains, with common and well-studied corpora.", "labels": [], "entities": []}, {"text": "Then, we evaluated our proposal on areal e-commerce task, yielding consistent improvements in terms of translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT); has obtained state-of-the art performance in several domains and language pairs.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7869077026844025}]}, {"text": "Given the nature of NMT paradigms, the limitation for obtaining bilingual corpora-or their availability-has been one of the major obstacles faced when building competitive NMT systems.", "labels": [], "entities": []}, {"text": "Recently, the idea of using synthetic corpora in NMT has reported promising results with regard to the data scarcity in NMT.", "labels": [], "entities": []}, {"text": "Many different works demonstrated that the combination of real parallel corpora with synthetic bilingual corpus enhances the NMT translation quality.", "labels": [], "entities": [{"text": "NMT translation", "start_pos": 125, "end_pos": 140, "type": "TASK", "confidence": 0.9232646524906158}]}, {"text": "Following these good results, we aim to adapt general NMT models to real, specific tasks by using synthetic parallel data.", "labels": [], "entities": []}, {"text": "The core idea is to select the most valuable instances from a large pool of monolingual source sentences, with respect to a given test set.", "labels": [], "entities": []}, {"text": "Next, we automatically translate them.", "labels": [], "entities": []}, {"text": "Therefore, we obtain a synthetic parallel corpus, related to our test set domain.", "labels": [], "entities": []}, {"text": "Such synthetic corpus can be used to fine-tune a NMT system to the domain at hand.", "labels": [], "entities": []}, {"text": "The main contributions of this paper involve the necessary steps required to adapt a NMT system to a specific domain: \u2022 We propose a novel method to create the most adequate synthetic corpus leverages a vector-space representation of sentences, relying on the word embeddings by and.", "labels": [], "entities": []}, {"text": "\u2022 We describe the pipeline of our adaptation process, relating the selection, translation and fine-tuning processes.", "labels": [], "entities": []}, {"text": "\u2022 We study our adaptation technique on two classical domains.", "labels": [], "entities": []}, {"text": "Additionally, we validate our technique on areal e-commerce translation task.", "labels": [], "entities": [{"text": "areal e-commerce translation task", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.6381228864192963}]}, {"text": "\u2022 Results show important improvements over a baseline system.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "NMT technology is briefly described in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 summarizes the related work.", "labels": [], "entities": []}, {"text": "In Section 4, we present our selection method and we describe the adaptation pipeline.", "labels": [], "entities": []}, {"text": "Section 5 presents the experimental set-up and corpora.", "labels": [], "entities": []}, {"text": "Results are analyzed and discussed in Section 6.", "labels": [], "entities": []}, {"text": "Finally, conclusions and future work are traced in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the experimental framework employed to assess the performance of the NMT adaptation method described in Section 4.", "labels": [], "entities": [{"text": "NMT adaptation", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.9412491321563721}]}, {"text": "For this purpose, we studied its behavior in three corpora.", "labels": [], "entities": []}, {"text": "Two of them refer to controlled tasks; while the last one belongs to areal e-commerce task.", "labels": [], "entities": []}, {"text": "Translation quality was assessed according to the following well-known metrics: \u2022 BLEU (BiLingual Evaluation Understudy) (), measures n-gram precision with respect to a reference set, with a penalty for sentences that are too short.", "labels": [], "entities": [{"text": "BLEU (BiLingual Evaluation Understudy)", "start_pos": 82, "end_pos": 120, "type": "METRIC", "confidence": 0.815657377243042}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9318841695785522}]}, {"text": "\u2022 TER (Translation Error Rate) (), is an error metric that computes the minimum number of edits (including swaps) required to modify the system hypotheses so that they match the reference.", "labels": [], "entities": [{"text": "TER (Translation Error Rate)", "start_pos": 2, "end_pos": 30, "type": "METRIC", "confidence": 0.7728153765201569}]}, {"text": "For all results, we computed their confidence intervals (p = 0.05) by means of bootstrap resampling).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora main figures, in terms of number  of sentences (|S|), number of words (|W |), vocab- ulary size (|V |) and average sentence length (|W |).", "labels": [], "entities": []}, {"text": " Table 4: Translation results for the XRCE and IT tasks. BLEU and TER results given in percentage.  \u03a3 denotes an ensemble of 4 neural models. |W | is the average number of words per sentence.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9660302996635437}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9993222951889038}, {"text": "TER", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9977449178695679}]}, {"text": " Table 5: E-Com -Test set results. BLEU and TER  results given in percentage. \u03a3 denotes an ensemble  of 4 neural models. |W | is the average number of  words per sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9992209672927856}, {"text": "TER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.998339056968689}]}]}