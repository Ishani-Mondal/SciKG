{"title": [{"text": "Neural Question Answering at BioASQ 5B", "labels": [], "entities": [{"text": "Neural Question Answering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8111680547396342}, {"text": "BioASQ 5B", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.8339578211307526}]}], "abstractContent": [{"text": "This paper describes our submission to the 2017 BioASQ challenge.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.5765419602394104}]}, {"text": "We participated in Task B, Phase B which is concerned with biomedical question answering (QA).", "labels": [], "entities": [{"text": "biomedical question answering (QA)", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.7755672633647919}]}, {"text": "We focus on factoid and list question, using an extractive QA model, that is, we restrict our system to output substrings of the provided text snippets.", "labels": [], "entities": []}, {"text": "At the core of our system, we use FastQA, a state-of-the-art neural QA system.", "labels": [], "entities": [{"text": "FastQA", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8621206283569336}]}, {"text": "We extended it with biomedical word embeddings and changed its answer layer to be able to answer list questions in addition to fac-toid questions.", "labels": [], "entities": []}, {"text": "We pre-trained the model on a large-scale open-domain QA dataset, SQuAD, and then fine-tuned the parameters on the BioASQ training set.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.71098592877388}, {"text": "BioASQ training set", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.9253334204355875}]}, {"text": "With our approach, we achieve state-of-the-art results on factoid questions and competitive results on list questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "BioASQ is a semantic indexing, question answering (QA) and information extraction challenge ().", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7755052149295807}, {"text": "information extraction challenge", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.8211758534113566}]}, {"text": "We participated in Task B of the challenge which is concerned with biomedical QA.", "labels": [], "entities": [{"text": "biomedical QA", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.6963413953781128}]}, {"text": "More specifically, our system participated in Task B, Phase B: Given a question and gold-standard snippets (i.e., pieces of text that contain the answer(s) to the question), the system is asked to return a list of answer candidates.", "labels": [], "entities": []}, {"text": "The fifth BioASQ challenge is taking place at the time of writing.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.5232703983783722}]}, {"text": "Five batches of 100 questions each were released every two weeks.", "labels": [], "entities": []}, {"text": "Participating systems have 24 hours to submit their results.", "labels": [], "entities": []}, {"text": "At the time of writing, all batches had been released.", "labels": [], "entities": []}, {"text": "The questions are categorized into different question types: factoid, list, summary and yes/no.", "labels": [], "entities": []}, {"text": "Our work concentrates on answering factoid and list questions.", "labels": [], "entities": [{"text": "answering factoid and list questions", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.8986521482467651}]}, {"text": "For factoid questions, the system's responses are interpreted as a ranked list of answer candidates.", "labels": [], "entities": []}, {"text": "They are evaluated using meanreciprocal rank (MRR).", "labels": [], "entities": [{"text": "meanreciprocal rank (MRR)", "start_pos": 25, "end_pos": 50, "type": "METRIC", "confidence": 0.8628198981285096}]}, {"text": "For list questions, the system's responses are interpreted as a set of answers to the list question.", "labels": [], "entities": []}, {"text": "Precision and recall are computed by comparing the given answers to the goldstandard answers.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9907000660896301}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.999075174331665}]}, {"text": "F1 score, i.e., the harmonic mean of precision and recall, is used as the official evaluation measure 1 . Most existing biomedical QA systems employ a traditional QA pipeline, similar in structure to the baseline system by.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9863448739051819}, {"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9802737236022949}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9957551956176758}]}, {"text": "They consist of several discrete steps, e.g., namedentity recognition, question classification, and candidate answer scoring.", "labels": [], "entities": [{"text": "namedentity recognition", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.7100808173418045}, {"text": "question classification", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.8599511682987213}, {"text": "candidate answer scoring", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.6933825115362803}]}, {"text": "These systems require a large amount of resources and feature engineering that is specific to the biomedical domain.", "labels": [], "entities": []}, {"text": "For example, OAQA (, which has been very successful in last year's challenge, uses a biomedical parser, entity tagger and a thesaurus to retrieve synonyms.", "labels": [], "entities": []}, {"text": "Our system, on the other hand, is based on a neural network QA architecture that is trained endto-end on the target task.", "labels": [], "entities": []}, {"text": "We build upon FastQA (, an extractive factoid QA system which achieves state-of-the-art results on QA benchmarks that provide large amounts of training data.", "labels": [], "entities": []}, {"text": "For example,) provides a dataset of \u2248 100, 000 questions on Wikipedia articles.", "labels": [], "entities": []}, {"text": "Our approach is to train FastQA (with some extensions) on the SQuAD dataset and then fine-tune the model parameters on the BioASQ training set.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9049631357192993}, {"text": "BioASQ training set", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.9110053181648254}]}, {"text": "Note that by using an extractive QA network as our central component, we restrict our system's: Neural architecture of our system.", "labels": [], "entities": []}, {"text": "Question and context (i.e., the snippets) are mapped directly to start and end probabilities for each context token.", "labels": [], "entities": []}, {"text": "We use FastQA () with modified input vectors and an output layer that supports list answers in addition to factoid answers.", "labels": [], "entities": []}, {"text": "responses to substrings in the provided snippets.", "labels": [], "entities": []}, {"text": "This also implies that the network will not be able to answer yes/no questions.", "labels": [], "entities": []}, {"text": "We do, however, generalize the FastQA output layer in order to be able to answer list questions in addition to factoid questions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Preliminary results for factoid and list questions for all five batches and for our single and  ensemble systems. We report MRR and F1 scores for factoid and list questions, respectively. In paren- theses, we report the rank of the respective systems relative to all other systems in the challenge. The  last row averages the performance numbers of the respective system and question type across the five  batches.", "labels": [], "entities": [{"text": "MRR", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.996618390083313}, {"text": "F1 scores", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9566185474395752}]}]}