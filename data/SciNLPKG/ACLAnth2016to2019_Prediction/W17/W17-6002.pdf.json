{"title": [{"text": "Neural Regularized Domain Adaptation for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Neural Regularized Domain Adaptation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8521724939346313}, {"text": "Chinese Word Segmentation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.5991616447766622}]}], "abstractContent": [{"text": "For Chinese word segmentation, the large-scale annotated corpora mainly focus on newswire and only a handful of annotated data is available in other domains such as patents and literature.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.5971833566824595}]}, {"text": "Considering the limited amount of annotated target domain data, it is a challenge for seg-menters to learn domain-specific information while avoid getting over-fitted at the same time.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural regularized domain adaptation method for Chinese word segmentation.", "labels": [], "entities": [{"text": "neural regularized domain adaptation", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.6206672564148903}, {"text": "Chinese word segmentation", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6484817763169607}]}, {"text": "The teacher networks trained in source domain are employed to regularize the training process of the student network by preserving the general knowledge.", "labels": [], "entities": []}, {"text": "In the experiments , our neural regularized domain adaptation method achieves a better performance comparing to previous methods.", "labels": [], "entities": [{"text": "neural regularized domain adaptation", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.600034661591053}]}], "introductionContent": [{"text": "As the Chinese text comes without word delimiters, the Chinese word segmentation becomes a necessary step towards further syntactic analysis.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7134291927019755}, {"text": "syntactic analysis", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7167487740516663}]}, {"text": "With the evolving of statistical word segmentation techniques (, some of the state-of-the-art systems) reported high accuracy in largescale annotated dataset ().", "labels": [], "entities": [{"text": "statistical word segmentation", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.6111457943916321}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9991390705108643}]}, {"text": "However, as large-scale annotated corpora mainly focus on domains like newswire, it often brings a significant decrease in performance when we directly apply models trained on these corpora to other domains (.", "labels": [], "entities": []}, {"text": "Such a problem is mainly due to the differences in distributions between the training (source domain) and testing (target domain) data, and wellknown as domain adaptation.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the fully-supervised domain adaptation where large-scale annotated corpora of source domain and only a handful of annotated data of target domain are available.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7714948356151581}]}, {"text": "As the annotated data in target domain is often insufficient to train a effective model, the key problem is how to fully explore the information contained in the target domain data and avoid getting overfitted at the same time.", "labels": [], "entities": []}, {"text": "Regularization is often employed in previous domain adaptation methods to escape the trap of over-fitting.; introduced loss functions that prevent corresponding weights from deviating significantly from the source model parameters.", "labels": [], "entities": []}, {"text": "KullbackLeibler divergence was added to force the feature distribution from adapted model to be close to that from the unadapted model ().", "labels": [], "entities": []}, {"text": "adopted adversarial training to ensure that the feature distributions over the different domains are close to each other.", "labels": [], "entities": []}, {"text": "In this paper, we employ a neural regularized domain adaption method based on Knowledge Distillation () for Chinese word segmentation.", "labels": [], "entities": [{"text": "neural regularized domain adaption", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.6225367486476898}, {"text": "Chinese word segmentation", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.5852141280968984}]}, {"text": "Knowledge distillation is first designed and proposed to do model compression (, where a teacher model and a student model is involved.", "labels": [], "entities": [{"text": "Knowledge distillation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7020351886749268}, {"text": "model compression", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7520100474357605}]}, {"text": "The teacher model is a complex model and trained on largescale annotated data.", "labels": [], "entities": []}, {"text": "The student model is a small model and trained by mimicking the output of the teacher model.", "labels": [], "entities": []}, {"text": "Because knowledge distillation is able to transfer knowledge between models, this method is extended and applied to other tasks.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.7359946668148041}]}, {"text": "adopted this method to gradually add new capabilities to a multi-task system.", "labels": [], "entities": []}, {"text": "transferred the knowledge of first- order logic rules to enhance neural networks.", "labels": [], "entities": []}, {"text": "Domain adaptation is also explored by using knowledge distillation.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7770922482013702}, {"text": "knowledge distillation", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7306790351867676}]}, {"text": "utilized the unlabeled data to transfer the knowledge from the source models.", "labels": [], "entities": []}, {"text": "Support Vector Machine is used as base classifier to efficiently solve the imitation parameter.", "labels": [], "entities": []}, {"text": "employed a measure for obtaining the trustworthiness of a teacher model.", "labels": [], "entities": []}, {"text": "However, previous work mainly focus on semi-supervised domain adaptation of sentiment analysis, while we explore the fullysupervised domain adaptation of Chinese word segmentation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.890170693397522}, {"text": "Chinese word segmentation", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.6067236661911011}]}, {"text": "In the domain adaptation for Chinese word segmentation, two kinds of domain adaptation tasks have been explored.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7236130386590958}, {"text": "Chinese word segmentation", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.6090066532293955}]}, {"text": "One is annotation standard adaptation (, which explores the common underlying knowledge between the corpora with different annotation standards.", "labels": [], "entities": [{"text": "annotation standard adaptation", "start_pos": 7, "end_pos": 37, "type": "TASK", "confidence": 0.6706671913464864}]}, {"text": "The other is document type adaptation (, such as using newswire document to label novel ( . In this paper, we focus on the document type adaptation which is a challenging problem in many real-world applications.", "labels": [], "entities": [{"text": "document type adaptation", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.738290548324585}, {"text": "document type adaptation", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.698421041170756}]}, {"text": "As shown in, the model trained on publicly available newswire data outputs incorrect segmentation for patents.", "labels": [], "entities": []}, {"text": "In the previous work of this task, lexicons were proved effective for improving cross-domain performance ( ).", "labels": [], "entities": []}, {"text": "Cross-domain features were explored to capture the characteristics of distributions utilizing unlabelled data in both source and target domain (.", "labels": [], "entities": []}, {"text": "However, previous methods mainly focus on feature-based methods utilizing unlabelled data or external resources such as lexicons.", "labels": [], "entities": []}, {"text": "How to utilize a handful of annotated target domain data is still under exploration.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural regularized domain adaption method for Chinese word segmentation.", "labels": [], "entities": [{"text": "neural regularized domain adaption", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.6147494912147522}, {"text": "Chinese word segmentation", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.6481002767880758}]}, {"text": "A neural segmenter trained with source domain data is employed as the teacher model.", "labels": [], "entities": []}, {"text": "A student model is then trained with target domain data under the regularization from the teacher model.", "labels": [], "entities": []}, {"text": "The regularization retains the general information from source domain and prevents the student model from over-fitting during the target domain-specific training.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: (1) we propose a neural method for fullysupervised domain adaptation of Chinese word segmentation and show its effectiveness in the experiments.", "labels": [], "entities": [{"text": "domain adaptation of Chinese word segmentation", "start_pos": 85, "end_pos": 131, "type": "TASK", "confidence": 0.7193391819794973}]}, {"text": "(2) we perform our neural domain adaptation method with different hyper-parameters and show it works as an neural regularization.", "labels": [], "entities": [{"text": "neural domain adaptation", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.7185022234916687}]}, {"text": "(3) we analyse the results showing that our method explores the domain-specific information and preserves the general knowledge at the same time.", "labels": [], "entities": []}, {"text": "(4) we propose a split of CTB9 data and perform domain adaptation experiments on the CTB9.", "labels": [], "entities": [{"text": "CTB9 data", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.9191301763057709}, {"text": "CTB9", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9442653656005859}]}], "datasetContent": [{"text": "Following previous Chinese word segmentation domain adaptation methods, we employ the Chinese Treebank (CTB) () as the source domain data.", "labels": [], "entities": [{"text": "word segmentation domain adaptation", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.8360591158270836}, {"text": "Chinese Treebank (CTB)", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.9494319319725036}]}, {"text": "The Patent () and Zhuxian ( ) are used as the target domain data.", "labels": [], "entities": [{"text": "Patent", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.6215283870697021}, {"text": "Zhuxian", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.7027046084403992}]}, {"text": "The patent is often a description of a specifically designed system, which contains a high concentration of technical terms.", "labels": [], "entities": []}, {"text": "Zhuxian is a Internet novel and has a different writing style comparing to CTB.", "labels": [], "entities": [{"text": "Zhuxian", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9283880591392517}, {"text": "CTB", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.9621596336364746}]}, {"text": "Zhuxian also contains many novel specific named entity.", "labels": [], "entities": [{"text": "Zhuxian", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9580706357955933}]}, {"text": "The statistics of the data is shown in.", "labels": [], "entities": []}, {"text": "It is obvious that the amount of source domain data is much larger than target domain data.", "labels": [], "entities": []}, {"text": "We also perform our method between different genres of CTB9.", "labels": [], "entities": [{"text": "CTB9", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9428882598876953}]}, {"text": "The Newswire (nw) in CTB9 is chosen as the source domain data.", "labels": [], "entities": [{"text": "Newswire (nw) in CTB9", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9067827264467875}]}, {"text": "The Weblogs (wb), SMS/Chat messages (sc) and conversational speech (cs) are employed as the target domain data.", "labels": [], "entities": []}, {"text": "We split each genre into train, development, test set, and the filelist is shown in.", "labels": [], "entities": []}, {"text": "The statistics of the data is shown in: Statistics of genres used in our experiments.", "labels": [], "entities": []}, {"text": "wb, sc and cs refer to Weblogs, SMS/Chat messages and conversational speech.", "labels": [], "entities": []}, {"text": "We also perform our method between different genres of CTB9 as shown in: The experiment results of CTB9 between nw and wb, sc, cs genres.", "labels": [], "entities": []}, {"text": "data is not significantly larger than target domain data.", "labels": [], "entities": []}, {"text": "The nw, wb, sc, cs refer to Newswire, Weblogs, SMS/Chat messages, conversational speech respectively.", "labels": [], "entities": []}, {"text": "The nw is chosen as the source domain data and the others are employed as the target domain data.", "labels": [], "entities": []}, {"text": "The Baseline refers to the target domain performance of a baseline segmenter trained with the Newswire data.", "labels": [], "entities": [{"text": "Newswire data", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9845786392688751}]}, {"text": "The Target only refers to the target domain performance of a baseline segmenter trained with the target domain data only.", "labels": [], "entities": []}, {"text": "The Our method refers to the performance of our neural regularized domain adaptation method.", "labels": [], "entities": [{"text": "neural regularized domain adaptation", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.6699269562959671}]}, {"text": "Because few previous methods are adopted in CTB9, we only compare our method with a baseline model trained on source domain and a baseline model trained on target domain providing the performance of our method for further comparision of domain adaptation methods in the future.", "labels": [], "entities": [{"text": "CTB9", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8735345005989075}]}, {"text": "Our method achieves improvement over both Baseline and Target only.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The split filelist of each genre. We only list the filelist of development and test data. The rest of  data in each genre is used as training data.", "labels": [], "entities": []}, {"text": " Table 4: The results between CTB7 and Patent.  Patent 10, Patent 20, Patent 100 refers to 10%,  20%, 100% of Patent train set. Mix refers to the  method of training the model with mixed training  data from Source and Target.", "labels": [], "entities": [{"text": "CTB7", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9234218597412109}, {"text": "Patent", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.904529869556427}, {"text": "Patent train set", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.8837534586588541}]}, {"text": " Table 5: The results between CTB5 and Zhuxian", "labels": [], "entities": [{"text": "CTB5", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9452304244041443}, {"text": "Zhuxian", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9029831886291504}]}, {"text": " Table 6: The experiment results of CTB9 between  nw and wb, sc, cs genres.", "labels": [], "entities": []}]}