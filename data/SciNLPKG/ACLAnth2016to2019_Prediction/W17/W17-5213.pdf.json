{"title": [{"text": "Mining fine-grained opinions on closed captions of YouTube videos with an attention-RNN", "labels": [], "entities": [{"text": "attention-RNN", "start_pos": 74, "end_pos": 87, "type": "METRIC", "confidence": 0.9422759413719177}]}], "abstractContent": [{"text": "Video reviews are the natural evolution of written product reviews.", "labels": [], "entities": []}, {"text": "In this paper we target this phenomenon and introduce the first dataset created from closed captions of YouTube product review videos as well as anew attention-RNN model for aspect extraction and joint aspect extraction and sentiment classification.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.7862607836723328}, {"text": "joint aspect extraction", "start_pos": 196, "end_pos": 219, "type": "TASK", "confidence": 0.5827978452046713}, {"text": "sentiment classification", "start_pos": 224, "end_pos": 248, "type": "TASK", "confidence": 0.8392485678195953}]}, {"text": "Our model provides state-of-the-art performance on aspect extraction without requiring the usage of hand-crafted features on the SemEval ABSA corpus, while it outperforms the baseline on the joint task.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8443025350570679}, {"text": "SemEval ABSA corpus", "start_pos": 129, "end_pos": 148, "type": "DATASET", "confidence": 0.6645172635714213}]}, {"text": "In our dataset, the attention-RNN model outperforms the baseline for both tasks, but we observe important performance drops for all models in comparison to SemEval.", "labels": [], "entities": []}, {"text": "These results, as well as further experiments on domain adaptation for aspect extraction, suggest that differences between speech and written text, which have been discussed extensively in the literature, also extend to the domain of product reviews, where they are relevant for fine-grained opinion mining.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.8223167061805725}, {"text": "fine-grained opinion mining", "start_pos": 279, "end_pos": 306, "type": "TASK", "confidence": 0.5994554857412974}]}], "introductionContent": [{"text": "On-line videos have become indispensable to people's daily lives, as traffic statistics showed that by 2010 it accounted for 56.6% of the total global consumer traffic.", "labels": [], "entities": []}, {"text": "Studies support the notion that on-line reviews can have a strong influence in the decision-making of potential Internet buyers), thus becoming a major factor for both consumers and marketers (.", "labels": [], "entities": []}, {"text": "Video reviews are the natural evolution of written product reviews.", "labels": [], "entities": []}, {"text": "In fact, people are increasingly turning to platforms such as YouTube to help them shop, looking for product reviews.", "labels": [], "entities": []}, {"text": "YouTube unboxing videos have become a growing phenomenon.", "labels": [], "entities": []}, {"text": "In 2015 alone, people in the U.S. watched 60M hours of them on YouTube, totaling 1.1 B views.", "labels": [], "entities": []}, {"text": "The same year, views of product review videos increased by 40% compared to 2014, and more than 1 million channels related to product reviews were counted.", "labels": [], "entities": []}, {"text": "Despite all of this, the most widely used approaches in opinion mining focus only on tweets or written product reviews available on websites like Amazon.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.7902137041091919}]}, {"text": "Therefore, in this paper we present the first opinion mining study focusing on video product reviews.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7516013383865356}]}, {"text": "We take the fine-grained approach, which aims to detect the subjective expressions in text and to characterize their sentiment orientation, and analyze the closed captions of video product reviews extracted from YouTube.", "labels": [], "entities": [{"text": "characterize their sentiment orientation", "start_pos": 98, "end_pos": 138, "type": "TASK", "confidence": 0.7125879526138306}]}, {"text": "Fine-grained opinion mining is important fora variety of NLP problems, including opinion-oriented question answering and opinion summarization, having been studied extensively in recent years.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7252145260572433}, {"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.6862704008817673}, {"text": "opinion summarization", "start_pos": 121, "end_pos": 142, "type": "TASK", "confidence": 0.7448018789291382}]}, {"text": "In practical terms, this approach defines the tasks of aspect extraction (AE), sentiment classification (SC) and a joint setting (AESC).", "labels": [], "entities": [{"text": "aspect extraction (AE)", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7889806747436523}, {"text": "sentiment classification (SC)", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.8625020861625672}]}, {"text": "While AE and AESC have often been tackled as sequence labeling problem, where the sentence is a stream of tokens to be labeled using IOB and collapsed or sentiment-bearing IOB labels () respectively, SC can be regarded as a semantic compositional problem, where the obtained representation is used to predict the sentiment.", "labels": [], "entities": [{"text": "AE", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.761146605014801}, {"text": "AESC", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.501401960849762}, {"text": "sequence labeling", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.6608157604932785}]}, {"text": "Accounting for the patent differences between speech and written text, which have also led linguists to consider them as different domains exhibiting different syntactic and distributional properties, we created the first annotated dataset using closed captions of YouTube product review videos, which we named the Youtubean dataset.", "labels": [], "entities": [{"text": "Youtubean dataset", "start_pos": 315, "end_pos": 332, "type": "DATASET", "confidence": 0.9823955595493317}]}, {"text": "Motivated by the success of attention-based approaches in multiple NLP problems such as machine translation (), parsing (, slot-filling () and others (, we also introduce an attention-augmented RNN model for AE and AESC.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7990099787712097}, {"text": "parsing", "start_pos": 112, "end_pos": 119, "type": "TASK", "confidence": 0.9738125205039978}, {"text": "AESC", "start_pos": 215, "end_pos": 219, "type": "DATASET", "confidence": 0.7865413427352905}]}, {"text": "Compared to previous work, the attentional component makes our model specially suitable for AESC, since it directly addresses the compositional nature of the sentiment classification task as it allows the model to represent the input sentence as a convex combination of word representations.", "labels": [], "entities": [{"text": "AESC", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.5789899826049805}, {"text": "sentiment classification task", "start_pos": 158, "end_pos": 187, "type": "TASK", "confidence": 0.8930898507436117}]}, {"text": "This is confirmed by our results on the SemEval ABSA dataset (, given that our model offers state-of-the-art performance for AESC while also performing equivalently to the state-of-the-art for aspect extraction without the need for manually-crafted features.", "labels": [], "entities": [{"text": "SemEval ABSA dataset", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.7785555720329285}, {"text": "AESC", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.8123690485954285}, {"text": "aspect extraction", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.7830623984336853}]}, {"text": "We also show that our attention-RNN model outperforms the baseline for both AE and AESC on our dataset.", "labels": [], "entities": [{"text": "AE", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9891027212142944}, {"text": "AESC", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.824105978012085}]}, {"text": "However, we observed that compared to the SemEval corpora, all the tested models decreased their performance on it.", "labels": [], "entities": []}, {"text": "As indicated by a descriptive analysis of our corpus and by additional experiments using domain adaptation techniques for AE, which did not offer considerable gains, our results seem to support the existence of the aforementioned differences between speech and written text in the context of product reviews and their importance for fine-trained opinion mining.", "labels": [], "entities": [{"text": "AE", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.4927469491958618}, {"text": "opinion mining", "start_pos": 346, "end_pos": 360, "type": "TASK", "confidence": 0.7257731109857559}]}, {"text": "Our code and data are available for download on GitHub 1 .", "labels": [], "entities": [{"text": "GitHub 1", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9042657911777496}]}], "datasetContent": [{"text": "In YouTube, video authors con provide their own closed captions, or they can be generated automatically by the engine.", "labels": [], "entities": []}, {"text": "In both cases, these captions can be interpreted as a time-indexed transcript of the speech in the video.", "labels": [], "entities": []}, {"text": "Therefore, to minimize the amount of noise in the data, we utilized the user-provided closed captions of seven of the most popular reviews of the Samsung Galaxy S5 and creatd an annotated dataset for fine-grained opinion mining.", "labels": [], "entities": [{"text": "fine-grained opinion mining", "start_pos": 200, "end_pos": 227, "type": "TASK", "confidence": 0.6246979832649231}]}, {"text": "We obtained, cleaned and processed the data, and annotated the aspects following the guidelines by using brat 3 (Stenetorp et al., 2012).", "labels": [], "entities": []}, {"text": "We divided the annotation process into two steps.", "labels": [], "entities": []}, {"text": "First, two different annotators tagged aspects independently, obtaining an exact inter-annotation agreement of 0.705 F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9902043342590332}]}, {"text": "This value rose to 0.823 when allowing for partial matches, which we defined as any overlap between the annotated terms.", "labels": [], "entities": []}, {"text": "Discrepancies were discussed until a final setting was reached.", "labels": [], "entities": []}, {"text": "With these annotations fixed, we asked the same annotators to tag the sentiment of each extracted aspect.", "labels": [], "entities": []}, {"text": "On this task, the annotators obtained an average agreement of 0.942 F1-score.", "labels": [], "entities": [{"text": "agreement", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9864894151687622}, {"text": "F1-score", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9932518601417542}]}, {"text": "This time, discrepancies were discussed with a third person who acted as an arbiter, until an agreement was reached.", "labels": [], "entities": []}, {"text": "Both aspect extraction and sentiment classification inter-annotator agreements are comparable to the values obtained in similar tasks (: Descriptive corpora comparison.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7606879770755768}, {"text": "sentiment classification inter-annotator", "start_pos": 27, "end_pos": 67, "type": "TASK", "confidence": 0.9065587123235067}, {"text": "Descriptive corpora comparison", "start_pos": 137, "end_pos": 167, "type": "TASK", "confidence": 0.6566750208536783}]}, {"text": "provides some key information about the the source video reviews we have used to build our dataset, which we named the Youtubean dataset.", "labels": [], "entities": [{"text": "Youtubean dataset", "start_pos": 119, "end_pos": 136, "type": "DATASET", "confidence": 0.988408774137497}]}, {"text": "compares it to the SemEval Laptops and Restaurants corpora, regarded as the de facto datasets for written review mining.", "labels": [], "entities": [{"text": "SemEval Laptops and Restaurants corpora", "start_pos": 19, "end_pos": 58, "type": "DATASET", "confidence": 0.8258377432823181}, {"text": "written review mining", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7024503946304321}]}, {"text": "Several differences can be observed.", "labels": [], "entities": []}, {"text": "A big distinction lies in mean sentence and aspect lengths, both of which are considerably longer in Youtubean.", "labels": [], "entities": [{"text": "Youtubean", "start_pos": 101, "end_pos": 110, "type": "DATASET", "confidence": 0.9785569310188293}]}, {"text": "We also analyzed sentence syntax complexity in terms of the constituency tree depth, observing that our sentence trees are deeper on average.", "labels": [], "entities": []}, {"text": "Furthermore, Youtubean exhibits both longer and more frequent aspect mentions.", "labels": [], "entities": [{"text": "Youtubean", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.952362596988678}]}, {"text": "For our experiments, in addition to Youtubean, we also worked with the SemEval ABSA 2014 Laptops and Restaurants corpora (), which can be regarded as the de facto datasets for fine-grained review mining.", "labels": [], "entities": [{"text": "Youtubean", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.9701096415519714}, {"text": "SemEval ABSA 2014 Laptops and Restaurants corpora", "start_pos": 71, "end_pos": 120, "type": "DATASET", "confidence": 0.8298942233834948}, {"text": "fine-grained review mining", "start_pos": 176, "end_pos": 202, "type": "TASK", "confidence": 0.6233847339948019}]}, {"text": "For AE we use the train/test splits provided for Phase B. For AESC, since the test data does not have sentiment labels, we worked only with the training data.", "labels": [], "entities": [{"text": "AE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.4799346625804901}, {"text": "AESC", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.7595962285995483}]}, {"text": "On the other hand, since the size of Youtubean is smaller than the SemEval corpora, we used 5-fold cross validation to make results more robust.", "labels": [], "entities": [{"text": "Youtubean", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.975413978099823}]}, {"text": "For each fold, we used 10% of the development data as a validation set and compare our results using twosided t-tests.", "labels": [], "entities": []}, {"text": "For evaluation, we used the CoNLL conlleval script for evaluation based on F1-score.", "labels": [], "entities": [{"text": "CoNLL conlleval script", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.8482279380162557}, {"text": "F1-score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9952051639556885}]}, {"text": "To perform joint aspect extraction and sentiment classification, we only considered positive (+), negative (\u2212) and neutral (0) as sentiment classes, and the additional conflict class is mapped to neutral.", "labels": [], "entities": [{"text": "joint aspect extraction", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.6483368078867594}, {"text": "sentiment classification", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.9554200172424316}]}, {"text": "To gain insights on the output of the models for AESC, we decoupled the IOB collapsed tags using simple heuristics to recover the simple aspect extraction F1-score as well as classification performances for each sentiment class, but we used the joint tagging conlleval F1-score to evaluate the models.", "labels": [], "entities": [{"text": "AESC", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8440420627593994}, {"text": "F1-score", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.8120701909065247}]}, {"text": "As a baseline, we implemented the RNN architectures by, which are the stateof-the-art in fine-grained aspect extraction.", "labels": [], "entities": [{"text": "fine-grained aspect extraction", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.6554011106491089}]}, {"text": "We experimented with Jordan-style RNNs (JRNN), Elman-style RNNs (RNN), LSTMs and the bidirectional versions of these last two.", "labels": [], "entities": []}, {"text": "We followed to merge the forward and backward hidden states, setting y t = \u03c3( U ht + U ht ), where U , U are output matrices for the forward and backward hidden states ht , ht , respectively.", "labels": [], "entities": []}, {"text": "This gives the models more flexibility to capture complex relations in a sentence, making them able to learn how to weight future and past information.).", "labels": [], "entities": []}, {"text": "The usefulness of working with pre-trained embeddings for the baseline RNNs was already shown by ().", "labels": [], "entities": []}, {"text": "However, for comparison when experimenting with our model, we also used randomly initialized embeddings of sizes 50 and 300 to test this hypothesis.", "labels": [], "entities": []}, {"text": "To make our results more transparent, we explicitly experimented with two different preprocessing pipelines.", "labels": [], "entities": []}, {"text": "We used, which provides both a POS-tagger and a chunker, and CoreNLP ().", "labels": [], "entities": []}, {"text": "The latter lacks a chunker so we combined it with the CoNLL chunklink script 4 . As, we also experimented adding the same 14 linguistic binary features they used, which are based on POS-tags and chunk IOB-tags.", "labels": [], "entities": [{"text": "CoNLL chunklink script", "start_pos": 54, "end_pos": 76, "type": "DATASET", "confidence": 0.8987037142117819}]}, {"text": "These are concatenated to the hidden layer of the RNN before the final output non-linearity.", "labels": [], "entities": []}, {"text": "To train our baseline models we set a learning rate of 0.01 with decay and early stopping on the validation set.", "labels": [], "entities": []}, {"text": "We set a fixed window size of 1 for bi-directional and 3 for unidirectional models, and always train word embeddings.", "labels": [], "entities": []}, {"text": "Exploratory experiments showed that most models stop learning after a few epochs -3 or 4-so we only trained fora maximum of 5 epochs.", "labels": [], "entities": []}, {"text": "In the case of our attention-RNN model (ARNN), here we only report results using LSTMs, which outperformed all others cells we tried on preliminary experiments.", "labels": [], "entities": []}, {"text": "We explored different hyper-parameter configurations, including context window sizes of 1, 3 and 5 as well as hidden state sizes of 100, 200 and 300, and dropout keep probabilities of 0.5 and 0.8.", "labels": [], "entities": []}, {"text": "We also experimented concatenating the RNN hidden states after the first pass with the binary features used by ().", "labels": [], "entities": []}, {"text": "Finally, we also experimented with unidirectional versions of the RNNs.", "labels": [], "entities": []}, {"text": "For training, we used mini-batch stochastic gradient descent with a mini-batch size of 16 and padded sequences to a maximum size of 200 tokens.", "labels": [], "entities": []}, {"text": "We used exponential decay of ratio 0.9 and early stopping on the validation when there was no improvement in the F1-score after 1000 training steps.: Best results for our ARNN on AE for the Laptops dataset.", "labels": [], "entities": [{"text": "validation", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.9418075084686279}, {"text": "F1-score", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9988530874252319}, {"text": "AE", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.8140842914581299}, {"text": "Laptops dataset", "start_pos": 190, "end_pos": 205, "type": "DATASET", "confidence": 0.9569667875766754}]}, {"text": "Context windows proved beneficial as confirmed by the significantly different average F1-scores of 76.55, 77.59 and 77.28 for window sizes 1, 3 and 5 respectively.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9992406368255615}]}, {"text": "We also observed significant performance differences using SennaEmbeddings, which outperformed all others with an average F1-score of 77.94.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.999488353729248}]}, {"text": "GoogleNews and WikiDeps exhibited average F1-scores of 76.93 and 76.55, which are statistically different (p = 4.08 \u00d7 10 \u22126 ) and although they also outperformed random embeddings ford = 300, they performed statistically worse than random embeddings ford = 50.", "labels": [], "entities": [{"text": "GoogleNews", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9535424113273621}, {"text": "F1-scores", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9993727803230286}]}, {"text": "Linguistic binary features did not statistically contribute to the performance.: Results of our implemented baseline RNN models on AE for the Youtubean dataset.", "labels": [], "entities": [{"text": "Youtubean dataset", "start_pos": 142, "end_pos": 159, "type": "DATASET", "confidence": 0.986979603767395}]}], "tableCaptions": [{"text": " Table 1: Detail of the reviews used to create the Youtubean dataset.", "labels": [], "entities": [{"text": "Youtubean dataset", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9864248633384705}]}, {"text": " Table 2: Descriptive corpora comparison.", "labels": [], "entities": []}, {"text": " Table 3: Results of our implemented baseline  RNN models on the Laptops dataset.", "labels": [], "entities": [{"text": "Laptops dataset", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9649079144001007}]}, {"text": " Table 4: Best results for our ARNN on AE for the  Laptops dataset.", "labels": [], "entities": [{"text": "AE", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.8135405778884888}, {"text": "Laptops dataset", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9578649997711182}]}, {"text": " Table 5: Results of our implemented baseline  RNN models on the Restaurants dataset.", "labels": [], "entities": [{"text": "Restaurants dataset", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.7928701937198639}]}, {"text": " Table 6: Best results for our attention-RNNs on  AE on the Restaurants dataset.", "labels": [], "entities": [{"text": "AE", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.7305063009262085}, {"text": "Restaurants dataset", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.8022783696651459}]}, {"text": " Table 7: Results of our implemented baseline  RNN models on AE for the Youtubean dataset.", "labels": [], "entities": [{"text": "Youtubean dataset", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.9904567301273346}]}, {"text": " Table 8: Results for the WEIGHTED technique.", "labels": [], "entities": [{"text": "WEIGHTED", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.43106797337532043}]}, {"text": " Table 9: Results for the PRED technique.", "labels": [], "entities": [{"text": "PRED", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9223313331604004}]}, {"text": " Table 10: Best results for our ARNNs for AE on  Youtubean.", "labels": [], "entities": [{"text": "AE", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.6411833763122559}, {"text": "Youtubean", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.7072615623474121}]}, {"text": " Table 11: Results for AESC on Laptops", "labels": [], "entities": [{"text": "AESC", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.5674530267715454}]}, {"text": " Table 12: Results for AESC on Restaurants", "labels": [], "entities": [{"text": "AESC", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.5469130873680115}]}, {"text": " Table 13: Results for AESC on Youtubean.", "labels": [], "entities": [{"text": "AESC", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.6047783493995667}, {"text": "Youtubean", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.5192802548408508}]}]}