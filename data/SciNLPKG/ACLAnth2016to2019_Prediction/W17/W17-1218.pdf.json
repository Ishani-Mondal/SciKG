{"title": [{"text": "T \u00a8 ubingen system in VarDial 2017 shared task: experiments with language identification and cross-lingual parsing", "labels": [], "entities": [{"text": "T \u00a8 ubingen system in VarDial 2017 shared task", "start_pos": 0, "end_pos": 46, "type": "DATASET", "confidence": 0.7533546090126038}, {"text": "language identification", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7393047958612442}, {"text": "cross-lingual parsing", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7535315155982971}]}], "abstractContent": [{"text": "This paper describes our systems and results on VarDial 2017 shared tasks.", "labels": [], "entities": [{"text": "VarDial 2017 shared tasks", "start_pos": 48, "end_pos": 73, "type": "DATASET", "confidence": 0.7958594560623169}]}, {"text": "Besides three language/dialect discrimination tasks, we also participated in the cross-lingual dependency parsing (CLP) task using a simple methodology which we also briefly describe in this paper.", "labels": [], "entities": [{"text": "language/dialect discrimination tasks", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.6665718078613281}, {"text": "cross-lingual dependency parsing (CLP) task", "start_pos": 81, "end_pos": 124, "type": "TASK", "confidence": 0.804317797933306}]}, {"text": "For all the discrimination tasks, we used linear SVMs with character and word features.", "labels": [], "entities": []}, {"text": "The system achieves competitive results among other systems in the shared task.", "labels": [], "entities": []}, {"text": "We also report additional experiments with neural network models.", "labels": [], "entities": []}, {"text": "The performance of neural network models was close but always below the corresponding SVM classifiers in the discrimination tasks.", "labels": [], "entities": []}, {"text": "For the cross-lingual parsing task, we experimented with an approach based on automatically translating the source tree-bank to the target language, and training a parser on the translated treebank.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6787202060222626}]}, {"text": "We used off-the-shelf tools for both translation and parsing.", "labels": [], "entities": [{"text": "translation and parsing", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.6226438184579214}]}, {"text": "Despite achieving better-than-baseline results, our scores in CLP tasks were substantially lower than the scores of the other participants.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we describe our efforts in two rather different tasks during our participation in VarDial 2017 shared tasks (.", "labels": [], "entities": [{"text": "VarDial 2017 shared tasks", "start_pos": 97, "end_pos": 122, "type": "DATASET", "confidence": 0.8151199966669083}]}, {"text": "The first task, which we collectively call language identification task, aims to identify closely related languages or dialects.", "labels": [], "entities": [{"text": "language identification task", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.8078820506731669}]}, {"text": "VarDial 2017 hosted three related language identification tasks: Discriminating between similar languages (DSL) shared task which includes closely related languages in six groups, Arabic dialect identification (ADI), and German dialect identification (GDI).", "labels": [], "entities": [{"text": "VarDial 2017", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9181656837463379}, {"text": "Arabic dialect identification (ADI)", "start_pos": 180, "end_pos": 215, "type": "TASK", "confidence": 0.7869004706541697}, {"text": "German dialect identification (GDI)", "start_pos": 221, "end_pos": 256, "type": "TASK", "confidence": 0.6775314062833786}]}, {"text": "The second task, cross-lingual parsing (CLP), aims to exploit resources available fora related source language for parsing a target language for which no syntactically annotated corpora (treebank) is available.", "labels": [], "entities": [{"text": "cross-lingual parsing (CLP)", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.8516771793365479}]}, {"text": "This paper focuses on the language identification, while providing a brief summary of our methods and results for the CLP task as well.", "labels": [], "entities": [{"text": "language identification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.8031351566314697}, {"text": "CLP task", "start_pos": 118, "end_pos": 126, "type": "TASK", "confidence": 0.7902435064315796}]}, {"text": "Although language identification is a mostly solved problem, closely related languages and dialects still pose a challenge for the language identification systems (.", "labels": [], "entities": [{"text": "language identification", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7159624546766281}, {"text": "language identification", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.7499587535858154}]}, {"text": "For this task, we experimented with two different families of models: linear support vector machines (SVM), and (deep) neural network models.", "labels": [], "entities": []}, {"text": "For both models we used combination of character and word (n-gram) features.", "labels": [], "entities": []}, {"text": "Similar to our earlier experiments in VarDial 2016 shared task (C \u00b8 \u00a8 oltekin and Rama, 2016), the linear models performed better than the neural network models in all language identification tasks.", "labels": [], "entities": [{"text": "language identification tasks", "start_pos": 168, "end_pos": 197, "type": "TASK", "confidence": 0.8133256236712137}]}, {"text": "We describe both families of models, and compare the results obtained.", "labels": [], "entities": []}, {"text": "In the VarDial 2017 shared task campaign, the DSL and ADI shared tasks had both open and closed track submissions, while GDI had only closed tracks.", "labels": [], "entities": []}, {"text": "For all the tasks, we only participate in the closed track.", "labels": [], "entities": []}, {"text": "While discriminating closely related languages is a challenge for the language identification task, the similarities can be useful in other tasks.", "labels": [], "entities": [{"text": "language identification task", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.8074202934900919}]}, {"text": "By using information or resources available fora related (source) language one can build or improve natural language tools fora (target) language.", "labels": [], "entities": []}, {"text": "This is particularly useful for low-resource languages, and tasks that require difficult-to-build languagespecific tools or resources.", "labels": [], "entities": []}, {"text": "Parsing fits into this category well, since treebanks, the primary resources used for parsing, require considerable time and effort to create.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.935520589351654}, {"text": "parsing", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9715135097503662}]}, {"text": "Hence, transferring knowledge from one or more (not necessarily related) languages is studied extensively in some recent work and found to be useful (, just to name a few).", "labels": [], "entities": []}, {"text": "Particularly, it has been shown that these approaches tend to perform better than purely unsupervised methods, which can be another natural choice for parsing a language without a treebank.", "labels": [], "entities": []}, {"text": "There are two common approaches for transfer parsing.", "labels": [], "entities": [{"text": "transfer parsing", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.9451840817928314}]}, {"text": "The first one is often called model transfer, which typically involves training a delexicalized parser on the source language treebank, and using it on the target language, with further adaptation or lexicalization with the help of additional monolingual or parallel corpora).", "labels": [], "entities": [{"text": "model transfer", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7588770091533661}]}, {"text": "The second method is annotation transfer, which utilizes parallel resources to map the existing annotations for the source language to the target language (.", "labels": [], "entities": [{"text": "annotation transfer", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8436018824577332}]}, {"text": "In this work, we use a straightforward annotationtransfer method using freely available tools.", "labels": [], "entities": []}, {"text": "Similar to the language identification, we only participated in the closed track of the CLP task.", "labels": [], "entities": [{"text": "language identification", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.79050013422966}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section provides brief descriptions of the tasks and the data sets.", "labels": [], "entities": []}, {"text": "Section 3 describes the methods and the systems we used for both tasks, Section 4 presents our results and we conclude in Section 5 after a brief discussion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average characters and space-separated  tokens in the DSL data (training and development  set combined).", "labels": [], "entities": [{"text": "DSL data", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9655153453350067}]}, {"text": " Table 2: Average characters and space-separated  tokens in the ADI data (training and development  set combined).", "labels": [], "entities": [{"text": "ADI data", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9208548963069916}]}, {"text": " Table 3: Average characters and space-separated  tokens in the GDI data (only training set, no de- velopment set was porovided).", "labels": [], "entities": [{"text": "GDI data", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9357354640960693}]}, {"text": " Table 5: Main results of language identification  tasks on the test set as calculated by the organizers.", "labels": [], "entities": [{"text": "language identification  tasks", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8360582590103149}]}, {"text": " Table 6: Confusion matrix for the DSL task.", "labels": [], "entities": [{"text": "DSL task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.52527916431427}]}, {"text": " Table 7: Confusion matrix for the ADI task.", "labels": [], "entities": []}, {"text": " Table 8: Confusion matrix for the GDI task.", "labels": [], "entities": [{"text": "GDI task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.6703716218471527}]}, {"text": " Table 9: Examples of inter-group confusions from the DSL task.", "labels": [], "entities": []}, {"text": " Table 10: Best accuracy scores obtained on the  DSL data by combinations of character and word  n-grams of varying sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993535876274109}, {"text": "DSL data", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9706681966781616}]}, {"text": " Table 11: Labeled (LAS) and unlabeled (UAS) at- tachment scores obtained by the translation model  in comparison to the baseline provided by the or- ganizers.", "labels": [], "entities": [{"text": "Labeled (LAS) and unlabeled (UAS) at- tachment scores", "start_pos": 11, "end_pos": 64, "type": "METRIC", "confidence": 0.7072156965732574}]}]}