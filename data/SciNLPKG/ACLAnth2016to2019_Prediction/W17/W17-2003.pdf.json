{"title": [{"text": "Learning to Recognize Animals by Watching Documentaries: Using Subtitles as Weak Supervision", "labels": [], "entities": [{"text": "Learning to Recognize Animals by Watching Documentaries", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.7801799901894161}]}], "abstractContent": [{"text": "We investigate animal recognition models learned from wildlife video documentaries by using the weak supervision of the tex-tual subtitles.", "labels": [], "entities": [{"text": "animal recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7234778702259064}]}, {"text": "This is a challenging setting, since i) the animals occur in their natural habitat and are often largely occluded and ii) subtitles are to a great degree complementary to the visual content, providing a very weak supervisory signal.", "labels": [], "entities": []}, {"text": "This is in contrast to most work on integrated vision and language in the literature, where tex-tual descriptions are tightly linked to the image content, and often generated in a curated fashion for the task at hand.", "labels": [], "entities": []}, {"text": "We investigate different image representations and models, in particular a support vector machine on top of activations of a pre-trained convolutional neural network, as well as a Naive Bayes framework on a 'bag-of-activations' image representation, where each element of the bag is considered separately.", "labels": [], "entities": []}, {"text": "This representation allows key components in the image to be isolated, in spite of vastly varying backgrounds and image clutter, without an object detection or image segmentation step.", "labels": [], "entities": [{"text": "object detection", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.7085015326738358}, {"text": "image segmentation", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.6818588227033615}]}, {"text": "The methods are evaluated based on how well they transfer to unseen camera-trap images captured across diverse topograph-ical regions under different environmental conditions and illumination settings, involving a large domain shift.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is estimated 1 that video traffic will be 82 percent of all global Internet traffic by 2020.", "labels": [], "entities": []}, {"text": "The ubiquitousness of video on the web demands indexing tools that facilitate fast and easy access to relevant content.", "labels": [], "entities": []}, {"text": "Traditionally, video search has been based on user-tags.", "labels": [], "entities": [{"text": "video search", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.7168451398611069}]}, {"text": "However, in the recent past, research activities have been directed at automatic indexing of videos based on the content.", "labels": [], "entities": []}, {"text": "Contributing to this goal of automatic video indexing, we focus on the problem of wildlife recognition in nature documentaries with subtitles.", "labels": [], "entities": [{"text": "automatic video indexing", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.647833506266276}, {"text": "wildlife recognition in nature documentaries with subtitles", "start_pos": 82, "end_pos": 141, "type": "TASK", "confidence": 0.80880435023989}]}, {"text": "This setup is challenging from at least two perspectives: first, from the point of view of the content, and second, due to the nature of video documentaries.", "labels": [], "entities": []}, {"text": "As far as the content is concerned, we are dealing with animals shot in their natural habitat.", "labels": [], "entities": []}, {"text": "The problem of identifying animals in videos, especially those shot in the natural habitat presents several challenges.", "labels": [], "entities": []}, {"text": "Firstly, animals are among the most difficult objects to recognize in images and videos, mainly due to their deformable bodies that often self occlude and the large variation they pose in appearance and depiction).", "labels": [], "entities": []}, {"text": "Further, in the natural habitat, there are challenges due to camouflage and occlusion by flora.", "labels": [], "entities": []}, {"text": "Moreover, unlike faces or cuboidal objects such as furniture, we do not have accurate detectors that can localize the animal in a frame.", "labels": [], "entities": []}, {"text": "State-of-the-art object proposal methods such as ( yield an unacceptably low level of either recall or precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9982061386108398}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9665734767913818}]}, {"text": "The absence of detectors necessitates other mechanisms that allow segregation of the components of the image.", "labels": [], "entities": []}, {"text": "The nature of video documentaries presents yet another challenge.", "labels": [], "entities": []}, {"text": "Typically, in video documentaries such as ours, the subtitles are not parallel, but complementary to the visuals (See).", "labels": [], "entities": []}, {"text": "This is in contrast to most work on integrated vision and language in the literature, where textual descriptions are tightly linked to the image content.", "labels": [], "entities": []}, {"text": "This means we do not have examples that In the rivers and lakes of Africa, lives an animal which has a reputation for being the most unpredictable and dangerous of all.", "labels": [], "entities": []}, {"text": "can reliably tie together textual and visual entities.", "labels": [], "entities": []}, {"text": "In this work, we study image representations and models that cope with the above challenges.", "labels": [], "entities": []}, {"text": "These include a support vector machine on top of activations of a pretrained convolutional neural network, and a Naive Bayes framework on a 'bag-of-activations' image representation, where each element of the bag is considered separately.", "labels": [], "entities": []}, {"text": "While the former utilizes a global representation denoted by the feature vector comprising CNN activations, the latter works on per dimension basis, allowing key components in the image to be isolated, in spite of largely varying backgrounds and image clutter, without an object detection or image segmentation step.", "labels": [], "entities": [{"text": "object detection", "start_pos": 272, "end_pos": 288, "type": "TASK", "confidence": 0.7232293635606766}, {"text": "image segmentation", "start_pos": 292, "end_pos": 310, "type": "TASK", "confidence": 0.6866954416036606}]}, {"text": "We experiment with both continuous and discretized variants of the 'bag-of-activations' representation.", "labels": [], "entities": []}, {"text": "In particular, we investigate image representations and weakly supervised animal recognition models that can be learned without the need for bounding boxes, or curated data comprising manually annotated training examples.", "labels": [], "entities": [{"text": "animal recognition", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7448444366455078}]}, {"text": "The rest of this paper is organized as follows: Section 2 presents the background and related work.", "labels": [], "entities": []}, {"text": "Section 3 provides the problem definition.", "labels": [], "entities": []}, {"text": "Section 4 describes the image representations and animal recognition models based on CNN activations.", "labels": [], "entities": [{"text": "animal recognition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7114407271146774}]}, {"text": "Section 5 discusses the experiments and results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 provides the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset used in our experiments is that of.", "labels": [], "entities": []}, {"text": "This is a wildlife documentary named 'Great Wildlife Moments' 6 with subtitles from the BBC.", "labels": [], "entities": []}, {"text": "This is an interlaced video with a duration of 108 minutes at a frame rate of 25 frames per second, and the frame resolution is 720x576 pixels.", "labels": [], "entities": []}, {"text": "The remaining 300 contained 365 animals in total.", "labels": [], "entities": []}, {"text": "We run our algorithm on all the 602 key frames.", "labels": [], "entities": []}, {"text": "There were 19 species of animals.", "labels": [], "entities": []}, {"text": "The animal labeling is evaluated in terms of precision, recall and F 1 computed over the entire dataset as follows: precision = number of labels correctly assigned total number of labels assigned recall = number of labels correctly assigned actual number of animal present The evaluation covers two aspects: 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9993815422058105}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9994220733642578}, {"text": "F 1", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9902345836162567}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9991704225540161}, {"text": "recall", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9699171781539917}]}, {"text": "How well do the representation and model learned using the weak labels of our dataset perform on the same dataset?", "labels": [], "entities": []}, {"text": "(Section 5.1) 2.", "labels": [], "entities": []}, {"text": "How well do the representation and model learned using the weak labels of our dataset transfer to an external dataset shot over diverse topographical regions under different environmental conditions and illumination settings?", "labels": [], "entities": []}, {"text": "(Section 5.2) shows the performance of an SVM on the global representation and a naive Bayes classifier on the bag of activations using continuous features.", "labels": [], "entities": []}, {"text": "In either case, name n l is assigned to frame a i if p(y l |a i ) > p(y l |a i ), that is, the probability threshold for prediction was set at 0.5.", "labels": [], "entities": []}, {"text": "For the naive Bayes classifier, a Gaussian distribution was used to model the continuous features along each dimension.", "labels": [], "entities": []}, {"text": "While both models do not yield adequate performance, the naive Bayes certainly does far better compared to the SVM.", "labels": [], "entities": []}, {"text": "In this setup involving limited reliable example pairs, it is beneficial to treat each element of the CNN representation individually rather than using the full feature vector in a high-dimensional space.", "labels": [], "entities": []}, {"text": "shows the precision-recall curves of the SVM and the naive Bayes classifier.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9957016110420227}]}, {"text": "The naive Bayes is clearly better in this setup, except in the low recall / high precision region.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9980645775794983}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9012731313705444}]}, {"text": "Closer inspection reveals that the Gaussian distribution used in the Naive Bayes framework is not a good fit to the data (see for one example feature dimension).", "labels": [], "entities": []}, {"text": "shows the normal distribution plotted using the mean and the standard deviation along the first dimension for the entire dataset (red curve: N (0.0454, 0.0622)).", "labels": [], "entities": []}, {"text": "This is superimposed on the histogram of the real-valued (undiscretized) feature vector (in grey).", "labels": [], "entities": []}, {"text": "While there are certainly other distributions (such as Poisson or Binomial) that could be used to model the data, we show that the most commonly used Gaussian clearly does not fit the data.", "labels": [], "entities": []}, {"text": "Rather than forcing the data to fit into some distribution, we turn to a discretized setting as it allows use of a simple non-parametric model.: Results of using the discretized features (left: equal width discretization, right: equal frequency discretization) and applying the weak labels of our dataset", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of using the continuous features  and applying the weak labels of our dataset", "labels": [], "entities": []}, {"text": " Table 2: Results of using the discretized features (left: equal width discretization, right: equal frequency  discretization) and applying the weak labels of our dataset", "labels": [], "entities": []}, {"text": " Table 3: Performance of the animal recognition models learned using our data, applied on images from  Snapshot Serengeti (", "labels": [], "entities": [{"text": "animal recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7180928587913513}, {"text": "Snapshot Serengeti", "start_pos": 103, "end_pos": 121, "type": "DATASET", "confidence": 0.8711303472518921}]}]}