{"title": [{"text": "A Twitter Corpus and Benchmark Resources for German Sentiment Analysis", "labels": [], "entities": [{"text": "German Sentiment Analysis", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7331575254599253}]}], "abstractContent": [{"text": "In this paper we present SB10k, anew corpus for sentiment analysis with approx. 10,000 German tweets.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9560247659683228}]}, {"text": "We use this new corpus and two existing corpora to provide state-of-the-art benchmarks for sentiment analysis in German: we implemented a CNN (based on the winning system of SemEval-2016) and a feature-based SVM and compare their performance on all three corpora.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.9444831609725952}]}, {"text": "For the CNN, we also created German word embeddings trained on 300M tweets.", "labels": [], "entities": []}, {"text": "These word embeddings were then optimized for sentiment analysis using distant-supervised learning.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9804189205169678}]}, {"text": "The new corpus, the German word embeddings (plain and optimized), and source code to rerun the benchmarks are publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the advance of deep learning in text analytics, many benchmarks for text analytics tasks have been significantly improved in the last four years.", "labels": [], "entities": [{"text": "text analytics", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7398820221424103}, {"text": "text analytics tasks", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.8228157560030619}]}, {"text": "For this reason, Zurich University of Applied Sciences (ZHAW) and SpinningBytes AG are collaborating in a joint research project to develop stateof-the-art solutions for text analytics tasks in several European languages.", "labels": [], "entities": [{"text": "text analytics tasks", "start_pos": 170, "end_pos": 190, "type": "TASK", "confidence": 0.808003306388855}]}, {"text": "The goal is to adapt and optimize algorithms for tasks like sentiment analysis, named entity recognition (NER), topic extraction etc. into industry-ready software libraries.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.9633927643299103}, {"text": "named entity recognition (NER)", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.7593988428513209}, {"text": "topic extraction", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.883653312921524}]}, {"text": "One very challenging task is automatic sentiment analysis.", "labels": [], "entities": [{"text": "automatic sentiment analysis", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.7498573462168375}]}, {"text": "The goal of sentiment analysis is to classify a text into the classes positive, negative, mixed, or neutral.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9461084306240082}]}, {"text": "Interest in automatic sentiment analysis has recently increased in both academia and industry due to the huge number of documents which are publicly available on social media.", "labels": [], "entities": [{"text": "automatic sentiment analysis", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6865508159001669}]}, {"text": "In fact, there exist various initiatives in the scientific community (such as shared tasks at SemEval () or TREC (), competitions at Kaggle 1 , special tracks at major conferences like EMNLP or LREC, and several companies have built commercial sentiment analysis tools.", "labels": [], "entities": [{"text": "Kaggle 1", "start_pos": 133, "end_pos": 141, "type": "DATASET", "confidence": 0.8374689817428589}, {"text": "sentiment analysis", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.859862893819809}]}, {"text": "Deep learning for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.9765771329402924}]}, {"text": "Deep neural networks have become very successful for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9810994565486908}]}, {"text": "In fact, the winner and many top-ranked systems in SemEval-2016 were using deep neural networks (SemEval is an international competition that runs every year several tasks for semantic evaluation, including sentiment analysis)).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 207, "end_pos": 225, "type": "TASK", "confidence": 0.8499212265014648}]}, {"text": "The winning system uses a multi-layer convolutional neural network that is trained in three phases.", "labels": [], "entities": []}, {"text": "For English, this system achieves an F1-score of 62.7% on the test data of, and top scores on test data from previous years.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9997370839118958}]}, {"text": "For this reason, we decided to adapt the system for sentiment analysis in German.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.964973121881485}]}, {"text": "Details are described in Section 4.", "labels": [], "entities": []}, {"text": "A new corpus for German sentiment.", "labels": [], "entities": [{"text": "German sentiment", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.6704072654247284}]}, {"text": "In order to train the CNN, millions of unlabeled and weakly-labeled German tweets are used for creating the word embeddings.", "labels": [], "entities": [{"text": "CNN", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.9189432263374329}]}, {"text": "In addition, a sufficient amount of manually labeled tweets is required to train and optimize the system.", "labels": [], "entities": []}, {"text": "For languages such as English, Chinese or Arabic, there exist plenty of labeled training data for sentiment analysis, while for other European languages, the resources are often very limited (cf. \"Related Work\").", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.9737088978290558}]}, {"text": "For German, in particular, we are only aware of three sentiment corpora of significant size: the DAI tweet data set, which contains 1800 German tweets with tweet-level sentiments); the MGS corpus, which contains 109,130 German tweets; and the PotTS corpus, which contains 7992 German tweets that were annotated on phrase level.", "labels": [], "entities": [{"text": "DAI tweet data set", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.8263965249061584}, {"text": "MGS corpus", "start_pos": 185, "end_pos": 195, "type": "DATASET", "confidence": 0.9077481031417847}, {"text": "PotTS corpus", "start_pos": 243, "end_pos": 255, "type": "DATASET", "confidence": 0.9152670502662659}]}, {"text": "Unfortunately, the first corpus is too small for training a sentiment system, the the second corpus has a very low inter-annotator agreement (\u03b1 = 0.34), indicating low-quality annotations, and the third corpus is not on sentence level.", "labels": [], "entities": []}, {"text": "For this reason, we decided to construct a large sentiment corpus with German tweets, called SB10k.", "labels": [], "entities": [{"text": "SB10k", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.882605254650116}]}, {"text": "This corpus should allow to train highquality machine learning classifiers.", "labels": [], "entities": []}, {"text": "It contains 9783 German tweets, each labeled by three annotators.", "labels": [], "entities": []}, {"text": "Details of corpus construction and properties are described in Section 3.", "labels": [], "entities": [{"text": "corpus construction", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6998200714588165}]}, {"text": "We evaluate the performance of the CNN on the three German sentiment corpora CAI, MGS, and SB10k in Section 5.", "labels": [], "entities": [{"text": "German sentiment corpora CAI", "start_pos": 52, "end_pos": 80, "type": "DATASET", "confidence": 0.8762590438127518}, {"text": "MGS", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.5560470819473267}]}, {"text": "In addition, we compare the results to a baseline system, a feature-based Support Vector Machine (SVM).", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first large-scale benchmark for sentiment analysis on German tweets.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9522086977958679}]}, {"text": "Our main contributions are: \u2022 Benchmarks for sentiment analysis in German on three corpora.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.960347980260849}]}, {"text": "\u2022 A new corpus SB10k for German sentiment with approx. 10000 tweets, manually labeled by three annotators.", "labels": [], "entities": [{"text": "German sentiment", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.6019120216369629}]}, {"text": "\u2022 Publicly available word embeddings trained on 300M million German tweets (using word2vec), and modified word embeddings after distant-supervised learning with 40M million weakly-labeled sentiment tweets.", "labels": [], "entities": []}, {"text": "The new corpus, word embeddings for German (plain and fully-trained) and source code to re-run the benchmarks are available at www.spinningbytes.com/resources.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of tweets per sentiment in SB10k", "labels": [], "entities": [{"text": "SB10k", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.7734715938568115}]}, {"text": " Table 2: Benchmarks for sentiment in German. SVM and CNN were trained on fixed split of each  corpus (90%), and then tested on the remaining texts. For DAI, all texts were used for testing. F1 is  macroaveraged from F1 pos and F1 neg . Bold numbers identify higher F1 score of both classifiers for each  combination of test and training corpus (2 lines).", "labels": [], "entities": [{"text": "SVM", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9603058695793152}, {"text": "CNN", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.8316685557365417}, {"text": "F1", "start_pos": 191, "end_pos": 193, "type": "METRIC", "confidence": 0.9929125905036926}, {"text": "F1 score", "start_pos": 266, "end_pos": 274, "type": "METRIC", "confidence": 0.9796355366706848}]}]}