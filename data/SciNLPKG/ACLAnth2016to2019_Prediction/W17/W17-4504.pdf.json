{"title": [{"text": "Towards Improving Abstractive Summarization via Entailment Generation", "labels": [], "entities": [{"text": "Improving Abstractive Summarization", "start_pos": 8, "end_pos": 43, "type": "TASK", "confidence": 0.8471061587333679}]}], "abstractContent": [{"text": "ive summarization, the task of rewriting and compressing a document into a short summary, has achieved considerable success with neural sequence-to-sequence models.", "labels": [], "entities": [{"text": "summarization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.973939061164856}]}, {"text": "However, these models can still benefit from stronger natural language inference skills, since a correct summary is logically entailed by the input document, i.e., it should not contain any contradictory or unrelated information.", "labels": [], "entities": []}, {"text": "We incorporate such knowledge into an abstractive summarization model via multi-task learning, where we share its decoder parameters with those of an en-tailment generation model.", "labels": [], "entities": []}, {"text": "We achieve promising initial improvements based on multiple metrics and datasets (including a test-only setting).", "labels": [], "entities": []}, {"text": "The domain mis-match between the entailment (captions) and summarization (news) datasets suggests that the model is learning some domain-agnostic inference skills.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary.", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5368446111679077}, {"text": "extractive summarization", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.5758571624755859}]}, {"text": "Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.6012843251228333}]}, {"text": "Despite these promising recent improvements, Input Document: may is a pivotal month for moving and storage companies . Ground-truth Summary: moving companies hit bumps in economic road Baseline Summary: a month to move storage companies Multi-task Summary: pivotal month for storage firms: Motivating output example from our summarization+entailment multi-task model.", "labels": [], "entities": []}, {"text": "there is still scope in better teaching summarization models about the general natural language inference skill of logical entailment generation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.971820056438446}, {"text": "logical entailment generation", "start_pos": 115, "end_pos": 144, "type": "TASK", "confidence": 0.789191871881485}]}, {"text": "This is because the task of abstractive summarization involves two subtasks: salient (important) event detection as well as logical compression, i.e., the summary should not contain any information that is contradictory or unrelated to the original document.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.5914500653743744}, {"text": "event detection", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.7620727717876434}]}, {"text": "Current methods have to learn both these skills from the same dataset and a single model.", "labels": [], "entities": []}, {"text": "Therefore, there is benefit in learning the latter ability of logical compression via external knowledge from a separate entailment generation task, that will specifically teach the model how to rewrite and compress a sentence such that it logically follows from the original input.", "labels": [], "entities": [{"text": "logical compression", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7567154169082642}]}, {"text": "To achieve this, we employ the recent paradigm of sequence-to-sequence multi-task learning (.", "labels": [], "entities": []}, {"text": "We share the decoder parameters of the summarization model with those of the entailment-generation model, so as to generate summaries that are good at both extracting important facts from as well as being logically entailed by the input document.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9687700271606445}]}, {"text": "shows such an (actual) output example from our model, where it successfully learns both salient information extraction as well as entailment, unlike the strong baseline model.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7308982312679291}]}, {"text": "Empirically, we report promising initial improvements over some solid baselines based on several metrics, and on multiple datasets: Gigaword and also a test-only setting of DUC.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 132, "end_pos": 140, "type": "DATASET", "confidence": 0.5409863591194153}, {"text": "DUC", "start_pos": 173, "end_pos": 176, "type": "DATASET", "confidence": 0.5081337094306946}]}, {"text": "Impor-tantly, these improvements are achieved despite the fact that the domain of the entailment dataset (image captions) is substantially different from the domain of the summarization datasets (general news), which suggests that the model is learning certain domain-independent inference skills.", "labels": [], "entities": []}, {"text": "Our next steps to this workshop paper include incorporating stronger pointer-based models and employing the new multi-domain entailment corpus ().", "labels": [], "entities": []}], "datasetContent": [{"text": "Following previous work, we use the full-length", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summarization results on Gigaword. Rouge scores are full length F-1, following previous work.", "labels": [], "entities": [{"text": "F-1", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9727842807769775}]}]}