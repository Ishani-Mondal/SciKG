{"title": [{"text": "VOILA: An Optimised Dialogue System for Interactively Learning Visually-Grounded Word Meanings (Demonstration System)", "labels": [], "entities": [{"text": "Interactively Learning Visually-Grounded Word Meanings", "start_pos": 40, "end_pos": 94, "type": "TASK", "confidence": 0.5886658489704132}]}], "abstractContent": [{"text": "We present VOILA: an optimised, multi-modal dialogue agent for interactive learning of visually grounded word meanings from a human user.", "labels": [], "entities": []}, {"text": "VOILA is: (1) able to learn new visual categories interactively from users from scratch; (2) trained on real human-human dialogues in the same domain, and so is able to conduct natural spontaneous dialogue; (3) optimised to find the most effective trade-off between the accuracy of the visual categories it learns and the cost it incurs to users.", "labels": [], "entities": [{"text": "VOILA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8948115110397339}, {"text": "accuracy", "start_pos": 270, "end_pos": 278, "type": "METRIC", "confidence": 0.9977655410766602}]}, {"text": "VOILA is deployed on Furhat 1 , a human-like, multi-modal robot head with back-projection of the face, and a graphical virtual character.", "labels": [], "entities": [{"text": "VOILA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7944613695144653}, {"text": "Furhat 1", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9272717535495758}]}], "introductionContent": [{"text": "As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings.", "labels": [], "entities": []}, {"text": "Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and sentences in Natural Language (NL) and perceptual aspects of the external environment -this is widely known as the grounding problem.", "labels": [], "entities": []}, {"text": "Our work is similar in spirit to e.g.) but advances it in several aspects (.", "labels": [], "entities": []}, {"text": "In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent).", "labels": [], "entities": []}, {"text": "Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, overtime.", "labels": [], "entities": []}, {"text": "Unlike a lot of past work), here we assume that the agent is in the position of a child, who does not have any prior knowledge of perceptual categories.", "labels": [], "entities": []}, {"text": "Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans.", "labels": [], "entities": []}, {"text": "In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor.", "labels": [], "entities": []}, {"text": "What sets VOILA apart from other work in this area is: \u2022 VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see () for more detail).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9975873231887817}]}, {"text": "\u2022 VOILA is trained on a corpus of real HumanHuman conversations (, and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft Kinect for skeletal tracking and processing nonverbal signals.", "labels": [], "entities": [{"text": "Furhat", "start_pos": 254, "end_pos": 260, "type": "DATASET", "confidence": 0.9497302174568176}]}, {"text": "A graphical version of the character can also bee used (see 1).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}