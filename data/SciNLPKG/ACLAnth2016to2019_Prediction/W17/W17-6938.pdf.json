{"title": [{"text": "Vision and Language Integration: Moving beyond Objects", "labels": [], "entities": [{"text": "Language Integration", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.7468786537647247}]}], "abstractContent": [{"text": "The last years have seen an explosion of work on the integration of vision and language data.", "labels": [], "entities": []}, {"text": "New tasks like Image Captioning and Visual Questions Answering have been proposed and impressive results have been achieved.", "labels": [], "entities": [{"text": "Image Captioning", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.810661792755127}, {"text": "Visual Questions Answering", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.5816161334514618}]}, {"text": "There is now a shared desire to gain an in-depth understanding of the strengths and weaknesses of those models.", "labels": [], "entities": []}, {"text": "To this end, several datasets have been proposed to try and challenge the state-of-the-art.", "labels": [], "entities": []}, {"text": "Those datasets, however, mostly focus on the interpretation of objects (as denoted by nouns in the corresponding captions).", "labels": [], "entities": []}, {"text": "In this paper, we reuse a previously proposed methodology to evaluate the ability of current systems to move beyond objects and deal with attributes (as denoted by adjectives), actions (verbs), manner (adverbs) and spatial relations (prepo-sitions).", "labels": [], "entities": []}, {"text": "We show that the coarse representations given by current approaches are not informative enough to interpret attributes or actions, whilst spatial relations somewhat fare better, but only in attention models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nouns area crucial component of natural language sentences.", "labels": [], "entities": []}, {"text": "It is not a coincidence that children first learn to use nouns and only afterwords expand their vocabulary with verbs, adjectives and other parts of speech (.", "labels": [], "entities": []}, {"text": "Interestingly, the same development has taken place with Language and Vision models.", "labels": [], "entities": []}, {"text": "Object classification has long been the main concern of the computer vision field, only then followed by action classification shared tasks.", "labels": [], "entities": [{"text": "Object classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9347593188285828}, {"text": "action classification shared tasks", "start_pos": 105, "end_pos": 139, "type": "TASK", "confidence": 0.840610533952713}]}, {"text": "Recently, more ambitious competitions have been proposed, aiming to evaluate models' ability to connect whole sentences to images, through both Image Captioning (IC) or Visual Question Answering (VQA) tasks.", "labels": [], "entities": [{"text": "Image Captioning (IC) or Visual Question Answering (VQA)", "start_pos": 144, "end_pos": 200, "type": "TASK", "confidence": 0.76796276619037}]}, {"text": "Progress in this area has seemed swift and impressive, but the community is now scrutinising the results to understand whether enthusiasm is warranted.", "labels": [], "entities": [{"text": "enthusiasm", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9674152135848999}]}, {"text": "Several diagnostic datasets have been proposed with this goal in mind, highlighting various flaws in existing tasks.", "labels": [], "entities": []}, {"text": "Our paper is a contribution to these efforts, showing that the field may have moved too fast from noun to sentence interpretation, overlooking difficulties in understanding other parts-of-speech.", "labels": [], "entities": [{"text": "noun to sentence interpretation", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.6274679377675056}]}, {"text": "Our paper expands the existing FOIL dataset (.", "labels": [], "entities": [{"text": "FOIL dataset", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.824548065662384}]}, {"text": "FOIL consists of a set of images matched with captions containing one single mistake.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.3563242256641388}]}, {"text": "The mistakes are always nouns referring to objects not actually present in the image.", "labels": [], "entities": []}, {"text": "The work demonstrates that the language and vision modalities are not truly integrated in current computational models, as they fail to spot the mistake in the caption and to correct it appropriately (humans, on the other hand, obtain almost 100% accuracy on those tasks).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.996080219745636}]}, {"text": "In the present paper, we exploit the FOIL strategy to evaluate Language and Vision models on a larger set of possible mismatches between language and vision.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.6059702038764954}]}, {"text": "Beside considering nouns as possible 'foil' words, we also consider verbs, adjectives, adverbs and prepositions, as illustrated in.", "labels": [], "entities": []}, {"text": "The results obtained by state-of-the-art systems on this data demonstrate that current models are indeed little able to move beyond object understanding.", "labels": [], "entities": [{"text": "object understanding", "start_pos": 132, "end_pos": 152, "type": "TASK", "confidence": 0.7581460177898407}]}, {"text": "1 Original : A narrow room with various luggage and two men FOIL : A broad room with various luggage and two men Adjective Original : A child wearing a very large and loosely tied necktie FOIL : A child wearing a very large and narrowly tied necktie", "labels": [], "entities": [{"text": "FOIL", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9822298288345337}, {"text": "FOIL", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9704958200454712}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the dataset. Here Noun  *  is a subset of FOIL-COCO used in Shekhar et al. (2017).", "labels": [], "entities": [{"text": "FOIL-COCO", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9762629866600037}]}, {"text": " Table 2: Classification Task (T1). Overall (both original and foil captions) accuracy. Chance level 50%.  Noun Verb Adjective Adverb Preposition Total  Blind  57.39 77.90  83.10  54.62  70.88  75.48  LSTM + norm I 63.17 78.37  83.81  55.84  73.70  77.11  HieCoAtt  64.46 81.79  86.00  53.40  74.91  79.09  IC-Wang  47.59 34.93  28.67  44.92  32.68  31.58", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9987375140190125}, {"text": "Preposition Total  Blind", "start_pos": 134, "end_pos": 158, "type": "METRIC", "confidence": 0.5386561652024587}, {"text": "LSTM + norm I", "start_pos": 201, "end_pos": 214, "type": "METRIC", "confidence": 0.766247496008873}]}, {"text": " Table 3: Classification Task (T1). Accuracy results of the foil captions only. Chance level 50%.  Noun  Verb  Adjective  Adverb  Preposition  Blind  23.18  57.11  76.99  18.73  54.32", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9985585808753967}, {"text": "Chance", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9732272624969482}]}, {"text": " Table 4: Foil Detection Task (T2) and Foil Correction Task (T3).  Foil Detection Task (T2)  Foil Correction Task (T3)  Noun Verb  Adj.  Adv. Prep. Noun Verb Adj. Adv. Prep.  Chance  23.25 21.72 21.72 21.72 21.72  1.38 0.22 2.04 2.04 4.34  LSTM + norm I 26.32 7.96  4.06  9.68  6.46  4.7  1.14 1.33 0.36 1.54  HieCoAttn  38.79 3.57  2.34  9.26  6.09  4.21 0.98 2.48 0.24 1.47  IC-Wang  27.59 8.67  9.23 12.56 26.56 22.16 9.1 1.61 3.44 7.78", "labels": [], "entities": [{"text": "LSTM + norm I 26.32 7.96  4.06  9.68  6.46  4.7  1.14 1.33 0.36 1.54  HieCoAttn  38.79", "start_pos": 240, "end_pos": 326, "type": "DATASET", "confidence": 0.7537278942763805}]}]}