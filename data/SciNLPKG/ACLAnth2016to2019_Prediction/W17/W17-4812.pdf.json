{"title": [{"text": "Translating Implicit Discourse Connectives Based on Crosslingual Annotation and Alignment", "labels": [], "entities": [{"text": "Translating Implicit Discourse Connectives", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8321084380149841}, {"text": "Alignment", "start_pos": 80, "end_pos": 89, "type": "TASK", "confidence": 0.4109620451927185}]}], "abstractContent": [{"text": "Implicit discourse connectives and relations are distributed more widely in Chinese texts, when translating into English, such connectives are usually translated explicitly.", "labels": [], "entities": []}, {"text": "Towards Chinese-English MT, in this paper we describe cross-lingual annotation and alignment of discourse connectives in a parallel corpus, describing related surveys and findings.", "labels": [], "entities": [{"text": "Chinese-English MT", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.471597820520401}]}, {"text": "We then conduct some evaluation experiments to testify the translation of implicit connectives and whether representing implicit connec-tives explicitly in source language can improve the final translation performance significantly.", "labels": [], "entities": [{"text": "translation of implicit connectives", "start_pos": 59, "end_pos": 94, "type": "TASK", "confidence": 0.8067538887262344}]}, {"text": "Preliminary results show it has little improvement by just inserting explicit connectives for implicit relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse relations refer to various relations between elementary discourse units(EDUs) in discourse structures, these relations are usually expressed explicitly or implicitly by certain surface words known as discourse connectives.", "labels": [], "entities": []}, {"text": "Distribution of DCs varies between different languages.", "labels": [], "entities": []}, {"text": "Let's just take Chinese and English for example.", "labels": [], "entities": []}, {"text": "According to previous surveys, explicit and implicit DCs account for 22% and 76% respectively in the Chinese Discourse Treebank(CDTB) (), while they account for 45% and 40% in the Penn Discourse Treebank (PDTB) (, indicating that there are more implicit DCs in Chinese, correspondingly, discourse relations are usually implicit.", "labels": [], "entities": [{"text": "Chinese Discourse Treebank(CDTB)", "start_pos": 101, "end_pos": 133, "type": "DATASET", "confidence": 0.9282166957855225}, {"text": "Penn Discourse Treebank (PDTB)", "start_pos": 180, "end_pos": 210, "type": "DATASET", "confidence": 0.9591427048047384}]}, {"text": "DCs should have some impacts on the translation performance and quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9762231111526489}]}, {"text": "As Chinese tends to use more implicit DCs, such DCs will be expressed explicitly when necessary in ChineseEnglish translation.", "labels": [], "entities": []}, {"text": "Here is an example sentence show the implicit relation.", "labels": [], "entities": []}, {"text": "\u5929\u6c14\u9884\u62a5 \u8bf4 \u4eca\u5929 \u4f1a \u4e0b\u96e8 \uff0c weather report say today will rain \"Weather report says it will rain today,\" \u6211\u4eec \u51b3\u5b9a \u4e0d \u5728 \u516c\u56ed \u4e3e\u529e \u6f14\u5531\u4f1a\u3002", "labels": [], "entities": []}, {"text": "We decide not in park hold concert \"We decide not to hold the concert in the park.\"", "labels": [], "entities": []}, {"text": "There is no explicit DC between the two Chinese sub-sentences in the simple example, and the implicit discourse relation is CAUSAL.", "labels": [], "entities": [{"text": "CAUSAL", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.8832774758338928}]}, {"text": "While translating into English, it is better to add an explicit DC such as \"so/thus\" before the second subsentence to express the relation, which will also make the translation more fluent and more acceptable.", "labels": [], "entities": []}, {"text": "In this paper, based on bilingual corpus, we first present cross-lingual annotation of DCs on both cross-sentence and within-sentence levels, and describe some related findings, then make a further survey on how to translate implicit DCs in Chinese-English discourse-level MT, and whether translation of DCs will have some impacts on final MT outputs.", "labels": [], "entities": [{"text": "translate implicit DCs in Chinese-English discourse-level MT", "start_pos": 215, "end_pos": 275, "type": "TASK", "confidence": 0.6096032389572689}, {"text": "MT outputs", "start_pos": 340, "end_pos": 350, "type": "TASK", "confidence": 0.8971292972564697}]}, {"text": "The rest of the paper are organized as follows: section2 introduce some related works.", "labels": [], "entities": []}, {"text": "Section 3 present annotation and findings of DCs in the bilingual parallel corpus.", "labels": [], "entities": []}, {"text": "Section4 discuss some preliminary experiment results and analysis.", "labels": [], "entities": []}, {"text": "And last section follow the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct MT automatic evaluation experiments on the annotated Chinese sentences with inserted implicit DCs to testify the translation performance before and after representing implicit DCs with explicit ones.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9817542433738708}]}, {"text": "Evaluation metrics include BLEU () and METEOR (Lavie and Agarwal, 2007) scores, calculated by the Asiya toolkit 4 ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9992353916168213}, {"text": "METEOR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9964491128921509}, {"text": "Asiya toolkit 4", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.8056022723515829}]}, {"text": "With Moses decoder (, we train a phrase-based SMT model on another different version of News-Commentary corpus 5 provided respectively by OPUS (69,206 sentence pairs) and WMT2017 Shared Task 6 (235,724 pairs), and the model is tuned by MERT) with the development sets (2002 pairs) provided by WMT2017.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8965654373168945}, {"text": "News-Commentary corpus 5", "start_pos": 88, "end_pos": 112, "type": "DATASET", "confidence": 0.8769391576449076}, {"text": "OPUS", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.8850803971290588}, {"text": "WMT2017 Shared Task 6", "start_pos": 171, "end_pos": 192, "type": "DATASET", "confidence": 0.8487374484539032}, {"text": "WMT2017", "start_pos": 293, "end_pos": 300, "type": "DATASET", "confidence": 0.9877455830574036}]}, {"text": "GIZA++) is used for automatic word alignment and a 5-gram language model is trained on English Gigaword).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7072969973087311}]}, {"text": "1500 sentences randomly chosen from the annotated corpus in section3 are used as test sets.", "labels": [], "entities": []}, {"text": "The training data is not annotated with any discourse information, and thus the translation models are not trained with any discourse markups.", "labels": [], "entities": []}, {"text": "But as the training data include both explicit and implicit DCs, it is suitable for the experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Bilingual distribution of explicit and  implicit relations", "labels": [], "entities": []}, {"text": " Table 4: Cross-sentence DCs Alignment matrix", "labels": [], "entities": [{"text": "Cross-sentence DCs Alignment", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7920129497845968}]}, {"text": " Table 5: Within-sentence DCs Alignment matrix", "labels": [], "entities": [{"text": "Within-sentence DCs Alignment", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6900918781757355}]}]}