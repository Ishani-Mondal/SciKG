{"title": [{"text": "Correcting prepositional phrase attachments using multimodal corpora", "labels": [], "entities": [{"text": "Correcting prepositional phrase attachments", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6899750307202339}]}], "abstractContent": [{"text": "PP-attachments are an important source of errors in parsing natural language.", "labels": [], "entities": [{"text": "parsing natural language", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.8882086277008057}]}, {"text": "We propose in this article to use data coming from a multimodal corpus, combining tex-tual, visual and conceptual information, as well as a correction strategy, to propose alternative attachments in the output of a parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Prepositional phrase attachments (PPattachments) are known to bean important source of errors in parsing natural language.", "labels": [], "entities": [{"text": "Prepositional phrase attachments (PPattachments)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7821759333213171}, {"text": "parsing natural language", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.8763093948364258}]}, {"text": "The main reason being that, in many cases, correct attachments cannot be predicted accurately based on pure syntactic considerations: their prediction ask for precise lexical co-occurrences or non linguistic knowledge.", "labels": [], "entities": []}, {"text": "Such information is usually not found in treebanks that are limited in their size and therefore do not model many bi-lexical phenomena.", "labels": [], "entities": []}, {"text": "In this paper, we propose to combine textual, conceptual and visual information extracted from a multimodal corpus to train a PP-attachment correction model.", "labels": [], "entities": [{"text": "PP-attachment correction", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.8052271902561188}]}, {"text": "In order to do so, we have used a corpus made of pairs (S, P ) where S is a sentence and Pa picture.", "labels": [], "entities": []}, {"text": "Some words of S have been manually linked to bounding boxes in P and tagged with coarse-grained conceptual types.", "labels": [], "entities": []}, {"text": "The relative positions of the boxes in the pictures as well as conceptual types and the lexical nature of words involved in a PP-attachment are used as features fora classifier that classifies a PP-attachment as either corrector wrong.", "labels": [], "entities": []}, {"text": "Given the parse tree T of S, and a target preposition, its different possible attachment sites are identified and the classifier is used to select the most promising one.", "labels": [], "entities": []}, {"text": "Our contributions in this study are the selection and the manual annotation of a corpus of ambiguous PP-attachments from the multimodal corpus Flickr30k Entities (; the study of the relative importance of different kinds of features for the PP-attachment resolution problem, from very specific ones (lexical features) to very generic ones (spatial features); and the combination of them in a single model for improving the accuracy of a syntactic dependency parser.", "labels": [], "entities": [{"text": "Flickr30k Entities", "start_pos": 143, "end_pos": 161, "type": "DATASET", "confidence": 0.8463925123214722}, {"text": "PP-attachment resolution problem", "start_pos": 241, "end_pos": 273, "type": "TASK", "confidence": 0.8346783518791199}, {"text": "accuracy", "start_pos": 423, "end_pos": 431, "type": "METRIC", "confidence": 0.9981542229652405}]}, {"text": "The structure of the paper is the following: section 2 presents some related work in the fields of PP-attachment and multimodal language processing.", "labels": [], "entities": [{"text": "multimodal language processing", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.677708089351654}]}, {"text": "In section 3 the multimodal corpus is described as well as the manual annotation that has been performed on it.", "labels": [], "entities": []}, {"text": "In section 4 the error prediction classifier is described and its performance evaluated.", "labels": [], "entities": [{"text": "error prediction classifier", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.7542201479276022}]}, {"text": "In section 5, the correction strategy is described.", "labels": [], "entities": [{"text": "correction", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.8016502857208252}]}, {"text": "The experiments are described in section 6 and section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of our experiments on the test set are detailed in: PP-attachment accuracy on the test set. of the error detector: using only one type of features (Textual, Conceptual and Visual) and all features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9569642543792725}]}, {"text": "The last line gives the attachment accuracy on all preposition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9297449588775635}]}, {"text": "As one can see on, the global accuracy of the parser on all PP-attachment is equal to 75%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9848670363426208}]}, {"text": "This figure is lower than the 86% correct PP-attachment reported by on the Penn Treebank using the same kind of parser, which does not come as a surprise, given the different nature of these two corpora.", "labels": [], "entities": [{"text": "correct", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9831593036651611}, {"text": "Penn Treebank", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9887585341930389}]}, {"text": "Table 3 presents the accuracy of PP-attachment after correction with different feature set combinations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9996033310890198}]}, {"text": "Adding conceptual features to textual features improve accuracy, however spatial features have no impact when used in conjunction with other feature sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9980193376541138}]}, {"text": "Several conclusions can be drawn from these results: different prepositions have very different accuracy with the parser, ranging from 95% for preposition through, to 33% for preposition near.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9991143345832825}]}, {"text": "The correction strategy implemented has a positive impact on accuracy: changing some attachments proposed by the parser using an error corrector based on limited but specific data is useful.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.998768150806427}]}, {"text": "Similarly as the results obtained on the classification accuracy, textual features are the most useful ones.", "labels": [], "entities": [{"text": "classification", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.9680646061897278}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.8254464864730835}]}, {"text": "Used alone, they increase accuracy by 10 points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9995818734169006}]}, {"text": "Although improving accuracy by 2% when used alone, visual features have no impact when combined with other feature sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9990411400794983}]}, {"text": "The positive impact of visual features is concentrated on three prepositions in table 2: (near, behind and outside).", "labels": [], "entities": []}, {"text": "It is interesting to note that these prepositions are mostly locative.", "labels": [], "entities": []}, {"text": "It does therefore make sense that visual features only focusing on spatial information have some impact on these prepositions.", "labels": [], "entities": []}, {"text": "On the other extreme, preposition like during that are mostly temporal are logically not impacted by the correction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classifier accuracy by model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9434936046600342}]}, {"text": " Table 2: PP-attachment accuracy on the test set  per preposition (only those with at least 30 occur- rences).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9679548144340515}]}, {"text": " Table 3: PP-attachment accuracy on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9878765344619751}]}]}