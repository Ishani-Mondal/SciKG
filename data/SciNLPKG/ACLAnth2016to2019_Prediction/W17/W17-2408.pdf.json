{"title": [{"text": "Work Hard, Play Hard: Email Classification on the Avocado and Enron Corpora", "labels": [], "entities": [{"text": "Avocado and Enron Corpora", "start_pos": 50, "end_pos": 75, "type": "DATASET", "confidence": 0.9121494889259338}]}], "abstractContent": [{"text": "In this paper, we present an empirical study of email classification into two main categories \"Business\" and \"Personal\".", "labels": [], "entities": [{"text": "email classification", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7736880779266357}]}, {"text": "We train on the Enron email corpus, and test on the Enron and Avocado email corpora.", "labels": [], "entities": [{"text": "Enron email corpus", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.8977938095728556}, {"text": "Enron and Avocado email corpora", "start_pos": 52, "end_pos": 83, "type": "DATASET", "confidence": 0.8434253215789795}]}, {"text": "We show that information from the email exchange networks improves the performance of classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.9658656716346741}]}, {"text": "We represent the email exchange networks as social networks with graph structures.", "labels": [], "entities": []}, {"text": "For this classification task, we extract social networks features from the graphs in addition to lexical features from email content and we compare the performance of SVM and Extra-Trees classifiers using these features.", "labels": [], "entities": []}, {"text": "Combining graph features with lexical features improves the performance on both classifiers.", "labels": [], "entities": []}, {"text": "We also provide manually annotated sets of the Avocado and Enron email corpora as a supplementary contribution .", "labels": [], "entities": [{"text": "Avocado and Enron email corpora", "start_pos": 47, "end_pos": 78, "type": "DATASET", "confidence": 0.9008592963218689}]}], "introductionContent": [{"text": "Email has quickly become a crucial communication medium for both individuals and organizations.", "labels": [], "entities": []}, {"text": "show that atypical user daily receives 40-50 emails.", "labels": [], "entities": []}, {"text": "Because of its popularity, different research problems related to email classification tasks have arisen.", "labels": [], "entities": [{"text": "email classification tasks", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.8332730730374655}]}, {"text": "These tasks include spam-filtering, assigning priority to messages, and foldering messages according a user-specified strategy ().", "labels": [], "entities": []}, {"text": "In spite of the popularity of email, many classification tasks have been hampered due the lack of availability of task-related data, due to the privacy issues surrounding email.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.8966841697692871}]}, {"text": "However, two large data sets are available.", "labels": [], "entities": []}, {"text": "First, a large dataset of real emails, the Enron corpus, was made publicly available by the Federal Energy Regulatory Commission (FERC) during the legal investigation of the company's collapse.", "labels": [], "entities": [{"text": "Enron corpus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8881258368492126}, {"text": "Federal Energy Regulatory Commission (FERC)", "start_pos": 92, "end_pos": 135, "type": "TASK", "confidence": 0.6600721137864249}]}, {"text": "Second, in February 2015, the Linguistic Data Consortium distributed a data set of emails from an anonymous defunct information technology company referred as Avocado (.", "labels": [], "entities": [{"text": "Linguistic Data Consortium distributed a data set", "start_pos": 30, "end_pos": 79, "type": "DATASET", "confidence": 0.7380759205136981}]}, {"text": "In this paper, we present an empirical study on email classification into two categories: Business and Personal.", "labels": [], "entities": [{"text": "email classification", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.762939453125}]}, {"text": "We train only on the Enron corpus, but test on both the Enron and Avocado corpora for this classification task in order to investigate how dependent on the training corpus the learned models are.", "labels": [], "entities": [{"text": "Enron corpus", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.8962344527244568}]}, {"text": "In addition, we provide new annotated datasets based on the two corpora 1 . We manually annotated datasets based on the Enron and Avocado corpora for this classification task.", "labels": [], "entities": [{"text": "Enron and Avocado corpora", "start_pos": 120, "end_pos": 145, "type": "DATASET", "confidence": 0.7738073021173477}]}, {"text": "We use lexical features as well as social network features extracted from the email exchange network of both Enron and Avocado.", "labels": [], "entities": []}, {"text": "The experiments show that when the social network features combined with lexical features outperforms the lexical features alone.", "labels": [], "entities": []}, {"text": "We first present some related work on both the Enron and Avocado corpora (Section 2).", "labels": [], "entities": [{"text": "Enron and Avocado corpora", "start_pos": 47, "end_pos": 72, "type": "DATASET", "confidence": 0.7911677360534668}]}, {"text": "Then in Section 3, we describe the datasets and the annotation scheme used in this paper.", "labels": [], "entities": []}, {"text": "We discuss lexical features in Section 4, and show how to extract social network features from the email exchange in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we present some experiments with different settings (Section 6).", "labels": [], "entities": []}, {"text": "The experiments show that adding features extracted from graphs of the email exchange to the lexical features improves the classification performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "As apart of the work in this paper, we have used the Amazon Mechanical Turk (AMTurk) crowdsourcing platform to annotate a subset of the Enron corpus.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMTurk) crowdsourcing platform", "start_pos": 53, "end_pos": 107, "type": "DATASET", "confidence": 0.875459723174572}, {"text": "Enron corpus", "start_pos": 136, "end_pos": 148, "type": "DATASET", "confidence": 0.9340012967586517}]}, {"text": "In addition, due to license constraints, we have in-house annotated a subset of the Avocado corpus.", "labels": [], "entities": [{"text": "Avocado corpus", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.9532874226570129}]}, {"text": "We use these two sets as well as the dataset distributed by (which we refer to as the \"Sheffield set\") for the classification task in this paper.", "labels": [], "entities": [{"text": "classification task", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.8918441832065582}]}, {"text": "The annotated emails by turkers area subset of the Enron corpus released by, which has more than 36,000 threads and 270,000 emails.", "labels": [], "entities": [{"text": "Enron corpus released", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.945716381072998}]}, {"text": "We choose this version of Enron because it maintains the thread structure of emails.", "labels": [], "entities": []}, {"text": "From this collection, we have randomly sampled total of 3,941 threads with different numbers of emails per thread (2, 3, 4, and 5).", "labels": [], "entities": []}, {"text": "The total number of emails is 10,573.", "labels": [], "entities": []}, {"text": "We exclude 198 threads (5%) and 27 additional emails (0.26%) labeled as \"Cannot determine\".", "labels": [], "entities": []}, {"text": "The sample has 3,222 emails overlapping with the Sheffield set of (after excluding \"Cannot determine\" emails).", "labels": [], "entities": [{"text": "Sheffield set", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8561030626296997}]}, {"text": "We also exclude all emails in the Sheffield set that we could not match with an email in (.", "labels": [], "entities": [{"text": "Sheffield set", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9696109294891357}]}, {"text": "After obtaining the final labels as described in 3.2, we got 3,743 threads and 10,546 emails labeled as either \"Business\" or \"Personal\" from the Enron corpus.", "labels": [], "entities": [{"text": "Enron corpus", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.9007746875286102}]}, {"text": "shows the summary of the Enron datasets with the following notations: \u2022 Enron T : The threads and emails obtained from AMTurk as in 3.2.", "labels": [], "entities": [{"text": "Enron datasets", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9192932844161987}, {"text": "AMTurk", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.9272344708442688}]}, {"text": "\u2022 Sheffield all : All the Sheffield set except those that we could not match in ().", "labels": [], "entities": [{"text": "Sheffield set", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.7936789989471436}]}, {"text": "2 we treat classes as completely different categories when computing Cohen's kappa \u2022 Sheffield sub : A subsample of the the Sheffield set (\"Business Core\" and \"Personal Close\").", "labels": [], "entities": []}, {"text": "\u2022 Enron \u2229A : The intersection between Enron T and Sheffield all in which both agree in labels.", "labels": [], "entities": []}, {"text": "\u2022 Enron \u2229D : The intersection between Enron T and Sheffield all in which disagree in labels.", "labels": [], "entities": []}, {"text": "\u2022 Enron \u2229 : The intersection between Enron T and Sheffield all . \u2022 Enron \u222a : Sheffield all \u222a (Enron T \u2212 Enron \u2229 ).", "labels": [], "entities": []}, {"text": "In case of disagreement, we use Sheffield all labels.", "labels": [], "entities": []}, {"text": "The Avocado Email Collection has 62,278 threads and 937,958 emails.", "labels": [], "entities": [{"text": "Avocado Email Collection", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9423930644989014}]}, {"text": "We have randomly sampled total of 2,000 threads and 5,339 emails from the Avocado corpus with different number of emails per thread as in Enron.", "labels": [], "entities": [{"text": "Avocado corpus", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.9435911476612091}]}, {"text": "As described in Section 3.2, each annotator labeled 1,200 threads, with 400 threads in common.", "labels": [], "entities": []}, {"text": "The first annotator has 3,197 emails, while the second has 3,207, and 1,065 emails are in common.", "labels": [], "entities": []}, {"text": "After obtaining the final labels as described in Section 3.2, we got total of 1,976 threads and 5,280 emails labeled as either \"Business\" or \"Personal\" from the Avocado corpus.", "labels": [], "entities": [{"text": "Avocado corpus", "start_pos": 161, "end_pos": 175, "type": "DATASET", "confidence": 0.9503927230834961}]}, {"text": "shows the summary of the Avocado datasets with the following notations: \u2022 Avocado 1 : The threads and emails labeled by the first annotator as in 3.2.", "labels": [], "entities": [{"text": "Avocado datasets", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9066191613674164}]}, {"text": "\u2022 Avocado 2 : The threads and emails labeled by the second annotator as in 3.2.", "labels": [], "entities": []}, {"text": "\u2022 Avocado \u2229A : The intersection between Avocado 1 and Avocado 2 in which both agree in labels.", "labels": [], "entities": []}, {"text": "\u2022 Avocado \u2229D : The intersection between Avocado 1 and Avocado 2 in which they disagree in labels.", "labels": [], "entities": []}, {"text": "\u2022 Avocado \u2229 : The intersection between Avocado 1 and Avocado 2 . \u2022 Avocado \u222a : All the threads and emails labeled as in 3.", "labels": [], "entities": []}, {"text": "In this section, we present empirical results on the email classification task by conducting different: Grid-search parameter space.", "labels": [], "entities": [{"text": "email classification task", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.829230546951294}]}, {"text": "B: Business, P: Personal.", "labels": [], "entities": []}, {"text": "Balanced: class weights are adjusted inversely proportional to class frequencies in the training set experiments on lexical and social network feature sets.", "labels": [], "entities": []}, {"text": "We use three metrics to measure the performance, namely: accuracy score, Business F-1 score and Personal F-1 score.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.9913338720798492}, {"text": "Business F-1 score", "start_pos": 73, "end_pos": 91, "type": "METRIC", "confidence": 0.6590824524561564}, {"text": "Personal F-1 score", "start_pos": 96, "end_pos": 114, "type": "METRIC", "confidence": 0.8101502656936646}]}, {"text": "We are mainly interested in improving the Personal F-1 score since it is the minority class.", "labels": [], "entities": [{"text": "Personal F-1 score", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.7697738210360209}]}, {"text": "We compare the performance of SVM classifiers and extremely randomized trees (commonly known as Extra-Trees)) as implemented in the scikitlearn python library).", "labels": [], "entities": []}, {"text": "We tune the hyper-parameters using grid-search with 3-fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": "shows the grid-search space for the two classifiers.", "labels": [], "entities": []}, {"text": "As a preprocessing step, we apply logarithmic transformation on the network and metainformation feature values to be approximately normal in distribution.", "labels": [], "entities": []}, {"text": "Then, all feature values (i.e. lexical, network and meta-info) are standardized to have zero-mean and unit-variance.", "labels": [], "entities": []}, {"text": "Vector Set Accuracy (%) F-1 B (%) F-1 P (%  In this subsection, we perform experiments with different models tested on Enron \u222a dev and Avocado \u222a dev . We assume that the ultimate application of our work is a setting in which we train models on a company (i.e. Enron) and apply it to another company (i.e. Avocado).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.7916684150695801}]}, {"text": "First, we tune the hyper-parameters using gridsearch with 3-fold cross-validation on Enron \u222a tr and Enron \u2229A tr three times: first, using network and meta-information features only, second, using lexical (embedding) features only, third, using all features.", "labels": [], "entities": []}, {"text": "Then, we select the best SVM and Extra-trees models with the lexical features only and the models with all features.", "labels": [], "entities": []}, {"text": "We apply a paired t-test on the personal F-1 scores of of the models (i.e. SVM and Extra-trees models with lexical features only and with all features) using 10-fold crossvalidation.", "labels": [], "entities": []}, {"text": "The results of the paired t-test show that the improvement obtained from adding the network features is statistically significant on Enron \u222a tr (p < 0.05), but not on Enron \u2229A tr (p > 0.05) using both SVM and Extra-trees classifiers.", "labels": [], "entities": []}, {"text": "For evaluating how well the models will perform in an intra-corpus setting, we test on Enron \u222adev , using models trained on Enron \u222atr with different classifiers and feature sets.", "labels": [], "entities": []}, {"text": "show that adding network features helps in retrieving more personal emails (increasing the personal recall) when using both classifiers.", "labels": [], "entities": [{"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9182581901550293}]}, {"text": "In addition, it is clear from the results that the network features are more effective with Extra-Trees since adding them improves all the scores.", "labels": [], "entities": []}, {"text": "To evaluate the cross-corpora performance, we test on Avocado \u222a dev using different models trained on Enron \u222a tr and Enron \u2229A tr . summarizes the cross-corpora results.", "labels": [], "entities": []}, {"text": "We use Enron \u2229 Atr in this experiment to test how well a model performs on another corpus when training on a dataset with few but high-confidence labels, in comparison with training on a larger dataset with labels of lesser confidence.", "labels": [], "entities": []}, {"text": "The results show that a model trained on a large dataset with lesser confidence labels (i.e. Enron \u222a tr ) using lexical feature alone can retrieve many personal emails, but with a poor precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 185, "end_pos": 194, "type": "METRIC", "confidence": 0.9978259205818176}]}, {"text": "Unlike the intra-corpus setting, adding network features always increases the personal precision but decreases the personal recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9226506948471069}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9616243839263916}]}, {"text": "However, the best performance as measured by f-measure is achieved by combining the network and lexical features, and using SVMs, which is the same best configuration as in the intra-corpus evaluation setting.", "labels": [], "entities": []}, {"text": "For the inter-corpora evaluation, the best result is achieved using the smaller training corpus with higher quality labels.", "labels": [], "entities": []}, {"text": "In both settings (i.e. intra-corpus and crosscorpora), Extra-Trees classifiers suffer in retrieving personal emails causing a decrease in the F-1 personal score in comparison with SVM classifiers.", "labels": [], "entities": [{"text": "F-1 personal score", "start_pos": 142, "end_pos": 160, "type": "METRIC", "confidence": 0.9254598021507263}]}], "tableCaptions": [{"text": " Table 1: Summary of the Enron datasets", "labels": [], "entities": [{"text": "Enron datasets", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.8431878387928009}]}, {"text": " Table 5: Results from different GloVe word vector  sets and a BOW model as a baseline trained on  Enron \u2229A tr and tested on Enron \u2229A dev .", "labels": [], "entities": [{"text": "BOW", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9894606471061707}]}, {"text": " Table 6: Results of different classifiers tested on Enron \u222a dev . Net features include meta-information  features", "labels": [], "entities": [{"text": "Enron \u222a dev . Net", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.899370014667511}]}, {"text": " Table 7: Results of different classifiers tested on Avocado \u222a dev . Net features include meta-information  features", "labels": [], "entities": []}, {"text": " Table 8: Applying best models on test sets. Both models are SVM classifiers trained with all features.", "labels": [], "entities": [{"text": "Applying", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9806066155433655}]}]}