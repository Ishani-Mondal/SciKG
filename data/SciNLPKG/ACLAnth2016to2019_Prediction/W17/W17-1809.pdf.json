{"title": [{"text": "Neural Networks for Negation Cue Detection in Chinese", "labels": [], "entities": [{"text": "Negation Cue Detection", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6536081930001577}]}], "abstractContent": [{"text": "Negation cue detection involves identifying the span inherently expressing negation in a negative sentence.", "labels": [], "entities": [{"text": "Negation cue detection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9063406586647034}]}, {"text": "In Chinese, negative cue detection is complicated by morphological proprieties of the language.", "labels": [], "entities": [{"text": "negative cue detection", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.6973097920417786}]}, {"text": "Previous work has shown that negative cue detection in Chinese can benefit from specific lexical and morphemic features, as well as cross-lingual information.", "labels": [], "entities": [{"text": "negative cue detection", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.6446123023827871}]}, {"text": "We show here that they are not necessary: A bi-directional LSTM can perform equally well, with minimal feature engineering.", "labels": [], "entities": []}, {"text": "In particular, the use of a character-based model allows us to capture characteristics of negation cues in Chinese using word-embedding information only.", "labels": [], "entities": []}, {"text": "Not only does our model performs on par with previous work, further error analysis clarifies what problems remain to be addressed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Negation cue detection is the task of recognizing the tokens (words, multi-word units or morphemes) inherently expressing negation.", "labels": [], "entities": [{"text": "Negation cue detection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.899388829867045}]}, {"text": "For instance, the task in (1) is to detect the negation cue \"\u4e0d(not)\", indicating that the clause as a whole is negative.", "labels": [], "entities": []}, {"text": "(1) \u6240\u6709\u4f4f\u5ba2\u5747\u8868\u793a\u4e0d \u4e0d \u4e0d\u4f1a\u8ffd\u7a76\u9152\u5e97\u7684\u8fd9\u6b21\u7ba1 \u7406\u5931\u804c (All of guests said that they would not investigate the dereliction of hotel.)", "labels": [], "entities": []}, {"text": "Previous work has addressed this task in English as a prerequisite for detecting negation scope (.", "labels": [], "entities": [{"text": "detecting negation scope", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.864264706770579}]}, {"text": "But recently, the release of the CNeSp corpus ( allows allows the task to be addressed in Chinese as well.", "labels": [], "entities": [{"text": "CNeSp corpus", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.947160005569458}]}, {"text": "Detecting negation cues in Chinese texts is difficult because character cues can be homographs of or contained within words not expressing negation.", "labels": [], "entities": [{"text": "Detecting negation cues in Chinese texts", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8989298542340597}]}, {"text": "For instance, \"\u975e\u5e38(very)\" and \"\u672a\u6765(future)\" are not negation cues, while \"\u975e(not)\" and \"\u672a(not)\" are.", "labels": [], "entities": []}, {"text": "Moreover, even expressions that contain a negation cue may not correspond to clause-level negation, because the overall meaning of the expression is positive.", "labels": [], "entities": []}, {"text": "This can be observed in the expression \"\u975e\u8981\", roughly corresponding to the English expression \"couldn't help but/had to\" which contains the negation cue \"\u975e\", but which carries a positive meaning where the action indeed take place, as in: ...when we are arriving, they had to charge 200 yuan...", "labels": [], "entities": []}, {"text": "Finally, negation cues in Chinese are similar to English affixal cues (e.g. \"insufficient\"), where they become integral part with the word they modify (e.g. \u591f(\"sufficient\") \u2192 \u4e0d\u591f(\"insufficient\")).", "labels": [], "entities": [{"text": "negation cues", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9092213809490204}]}, {"text": "According to the CNeSp guidelines, both the negation affix and the root it attaches to are considered as part of the cue.", "labels": [], "entities": [{"text": "CNeSp", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.9115233421325684}]}, {"text": "The high combinatory power of negation affixes leads however to issues of data sparsity.", "labels": [], "entities": []}, {"text": "This is particularly relevant in the context of the CNeSp corpus, given that about 12% of negation in the test set is not present in the training set (.", "labels": [], "entities": [{"text": "CNeSp corpus", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9489906132221222}]}, {"text": "Specifically, using the CNeSp corpus, tried to automatically detect negation cues using a sequential classifier trained on a variety of features, including lexical (word n-grams), syntactic (PoS n-grams) and morphemic features (whether a character has appeared in training data as part of a cue).", "labels": [], "entities": [{"text": "CNeSp corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9602422714233398}]}, {"text": "In addition, to address the problem of affixal negation cues producing tokens in the test set that did not appear in the training set, Chinese-to-English word-alignment was also taken into account.", "labels": [], "entities": []}, {"text": "In contrast, the recent success of Neural Network models for negation scope detection) suggested investigating whether a character-based recurrent model can perform on par or better than this previous work.", "labels": [], "entities": [{"text": "negation scope detection", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.9364026188850403}]}, {"text": "After describing our model in Section 2, we show in Section 3.3 that a character-level representation with no feature engineering is able to achieve similar recall as models that use word-alignment information, as well as other features, to tackle the problem of data sparsity.", "labels": [], "entities": [{"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9924443364143372}]}, {"text": "Compared to other sequence classifiers however, we show that neural networks tend to overpredict negation cues (thereby damaging precision) and suffer from insufficient training data.", "labels": [], "entities": [{"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9984038472175598}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on development set for each of the CNeSp subcorpora.", "labels": [], "entities": [{"text": "CNeSp subcorpora", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.900158017873764}]}, {"text": " Table 3: Results on test set for each of the CNeSp subcorpora.", "labels": [], "entities": [{"text": "CNeSp subcorpora", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9095092713832855}]}, {"text": " Table 1: Details of the three CNeSp subcorpora.", "labels": [], "entities": []}, {"text": " Table 4: Difference between before and after post  process in financial sub corpora", "labels": [], "entities": []}]}