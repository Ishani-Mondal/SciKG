{"title": [], "abstractContent": [{"text": "Recent neural headline generation models have shown great results, but are generally trained on very large datasets.", "labels": [], "entities": [{"text": "neural headline generation", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.6853054960568746}]}, {"text": "We focus our efforts on improving headline quality on smaller datasets by the means of pre-training.", "labels": [], "entities": []}, {"text": "We propose new methods that enable pre-training all the parameters of the model and utilize all available text, resulting in improvements by up to 32.4% relative in perplexity and 2.84 points in ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 195, "end_pos": 200, "type": "METRIC", "confidence": 0.945899248123169}]}], "introductionContent": [{"text": "Neural headline generation (NHG) is the process of automatically generating a headline based on the text of the document using artificial neural networks.", "labels": [], "entities": [{"text": "Neural headline generation (NHG)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8428166707356771}]}, {"text": "Headline generation is a subtask of text summarization.", "labels": [], "entities": [{"text": "Headline generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8446284532546997}, {"text": "text summarization", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6856811940670013}]}, {"text": "While a summary may cover multiple documents, generally uses similar style to the summarized document, and consists of multiple sentences, headline, in contrast, covers a single document, is often written in a different style), and is much shorter (frequently limited to a single sentence).", "labels": [], "entities": []}, {"text": "Due to shortness and specific style, condensing the the document into a headline often requires the ability to paraphrase which makes this task a good fit for abstractive summarization approaches where neural networks based attentive encoderdecoder () type of models have recently shown impressive results (e.g.,; ).", "labels": [], "entities": []}, {"text": "While state-of-the art results have been obtained by training NHG models on large datasets like Gigaword, access to such resources is often not possible, especially when it comes to low-resource languages.", "labels": [], "entities": []}, {"text": "In this work we focus on maximizing performance on smaller datasets with different pre-training methods.", "labels": [], "entities": []}, {"text": "One of the reasons to expect pre-training to bean effective way to improve performance on small datasets, is that NHG models are generally trained to generate headlines based on just a few first sentences of the documents (.", "labels": [], "entities": [{"text": "NHG", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.7876176238059998}]}, {"text": "This leaves the rest of the text unutilized, which can be alleviated by pre-training subsets of the model on full documents.", "labels": [], "entities": []}, {"text": "Additionally, the decoder component of NHG models can be regarded as a language model (LM) whose predictions are biased by the external information from the encoder.", "labels": [], "entities": []}, {"text": "As a LM it sees only headlines during training, which is a small fraction of text compared to the documents.", "labels": [], "entities": []}, {"text": "Supplementing the training data of the decoder with documents via pre-training might enable it to learn more about words and language structure.", "labels": [], "entities": []}, {"text": "Although, some of the previous work has used pre-training before (), it is not fully explored how much pretraining helps and what is the optimal way to do it.", "labels": [], "entities": []}, {"text": "Another problem is, that in previous work only a subset of parameters (usually just embeddings) is pre-trained leaving the rest of the parameters randomly initialized.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: LM pre-training for fully initializing the encoder and decoder (sections 2.1 and 2.2); combining LM pre-training with distant supervision () pre-training using filtered sentences of the documents as noisy targets (i.e. predicting one sentence given the rest) to maximally utilize the entire available dataset and pre-train all the paramters of the NHG model (section 2.3); and analysis of the effect of pre-training different components of the NHG model (section 3.3).", "labels": [], "entities": [{"text": "NHG model", "start_pos": 390, "end_pos": 399, "type": "DATASET", "confidence": 0.9383996725082397}, {"text": "NHG model", "start_pos": 486, "end_pos": 495, "type": "DATASET", "confidence": 0.956313818693161}]}], "datasetContent": [{"text": "We evaluate the proposed pre-training methods in terms of ROUGE and perplexity on two relatively small datasets (English and Estonian).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9983958601951599}]}, {"text": "We use the CNN/Daily Mail dataset 1 for experiments on English (EN).", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset 1", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.948919395605723}]}, {"text": "The number of headline-document pairs is 287227, 13368 and 11490 in training, validation and test set correspondingly.", "labels": [], "entities": []}, {"text": "The preprocessing consists of tokenization, lowercasing, replacing numeric characters with #, and removing irrelevant parts (editor notes, timestamps etc.) from the beginning of the document with heuristic rules.", "labels": [], "entities": []}, {"text": "For Estonian (ET) experiments we use a similarly sized (341607, 18979 and 18977 training, validation and test split) dataset that also consist of news from two sources.", "labels": [], "entities": []}, {"text": "During preprocessing, compound words are split, words are truecased and numbers are written out as words.", "labels": [], "entities": []}, {"text": "We used Estnltk () stemmer for ROUGE evaluations.", "labels": [], "entities": [{"text": "Estnltk", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9599705934524536}]}], "tableCaptions": [{"text": " Table 1: Perplexities on the test set with a 95%  confidence interval (", "labels": [], "entities": []}, {"text": " Table 2: Recall and precision of ROUGE-1 and ROUGE-L on the test sets. Best scores in bold. Results  with statistically significant differences (95% confidence) compared to No pre-training underlined.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9924192428588867}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9996694326400757}, {"text": "ROUGE-1", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9851711392402649}, {"text": "ROUGE-L", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9706214070320129}]}]}