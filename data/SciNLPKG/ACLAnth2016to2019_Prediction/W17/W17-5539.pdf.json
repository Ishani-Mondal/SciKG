{"title": [{"text": "Using Reinforcement Learning to Model Incrementality in a Fast-Paced Dialogue Game", "labels": [], "entities": []}], "abstractContent": [{"text": "We apply Reinforcement Learning (RL) to the problem of incremental dialogue policy learning in the context of a fast-paced dialogue game.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.6868258655071259}, {"text": "dialogue policy learning", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6483777761459351}]}, {"text": "We compare the policy learned by RL with a high performance baseline policy which has been shown to perform very efficiently (nearly as well as humans) in this dialogue game.", "labels": [], "entities": [{"text": "RL", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.7608299851417542}]}, {"text": "The RL policy outperforms the baseline policy in offline simulations (based on real user data).", "labels": [], "entities": [{"text": "RL", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.49099215865135193}]}, {"text": "We provide a detailed comparison of the RL policy and the baseline policy, including information about how much effort and time it took to develop each one of them.", "labels": [], "entities": [{"text": "RL", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.8778849840164185}]}, {"text": "We also highlight the cases where the RL policy performs better, and show that understanding the RL policy can provide valuable insights which can inform the creation of an even better rule-based policy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building incremental spoken dialogue systems (SDSs) has recently attracted much attention.", "labels": [], "entities": [{"text": "incremental spoken dialogue systems (SDSs)", "start_pos": 9, "end_pos": 51, "type": "TASK", "confidence": 0.6860384855951581}]}, {"text": "One reason for this is that incremental dialogue processing allows for increased responsiveness, which in turn improves task efficiency and user satisfaction.", "labels": [], "entities": []}, {"text": "Incrementality in dialogue has been studied in the context of turn-taking, predicting the next user utterances/actions, and generating fast system responses (.", "labels": [], "entities": [{"text": "predicting the next user utterances/actions", "start_pos": 75, "end_pos": 118, "type": "TASK", "confidence": 0.7827384897640773}]}, {"text": "Over the years researchers have tried a variety of approaches to incremental dialogue processing.", "labels": [], "entities": [{"text": "incremental dialogue processing", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.6809685826301575}]}, {"text": "One such approach is using rules whose parameters maybe optimized using real user data (.", "labels": [], "entities": []}, {"text": "Reinforcement Learning (RL) is another method that has been used to learn policies regarding when the system should interrupt the user (barge-in), stay silent, or generate backchannels in order to improve the responsiveness of the SDS or increase task success (.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6977280020713806}]}, {"text": "We apply RL to the problem of incremental dialogue policy learning in the context of a fast-paced dialogue game.", "labels": [], "entities": [{"text": "RL", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.929054319858551}, {"text": "dialogue policy learning", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.6879291733105978}]}, {"text": "We use a corpus of real user data for both training and testing.", "labels": [], "entities": []}, {"text": "We compare the policies learned by RL with a high performance baseline policy which uses parameterized rules (whose parameters have been optimized using real user data) and has a carefully designed rule (CDR) structure.", "labels": [], "entities": []}, {"text": "From now on, we will refer to this baseline as the CDR baseline.", "labels": [], "entities": [{"text": "CDR baseline", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.967577189207077}]}, {"text": "Our contributions are as follows: We provide an RL method for incremental dialogue processing based on simplistic features which performs better in offline simulations (based on real user data) than the high performance CDR baseline.", "labels": [], "entities": [{"text": "RL", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9501221179962158}, {"text": "dialogue processing", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7645092010498047}]}, {"text": "Note that this is a very strong baseline which has been shown to perform very efficiently (nearly as well as humans) in this dialogue game ( . In many studies that use RL for dialogue policy learning, the focus is on the RL algorithms, the state-action space representation, and the reward function.", "labels": [], "entities": [{"text": "dialogue policy learning", "start_pos": 175, "end_pos": 199, "type": "TASK", "confidence": 0.7239317695299784}]}, {"text": "As a result, the rule-based baselines used for comparing the RL policies against are not as carefully engineered as they could be, i.e., they are not the result of iterative improvement and optimization using insights learned from data or user testing.", "labels": [], "entities": []}, {"text": "This is understandable since building a very strong baseline would be a big project by itself and would detract attention from the RL problem.", "labels": [], "entities": [{"text": "RL", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.9637892842292786}]}, {"text": "In our case, there was a pre-existing strong CDR baseline policy which inspired us to investigate whether it could be outperformed by an RL policy.", "labels": [], "entities": [{"text": "RL", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.9042611122131348}]}, {"text": "One of our main contributions is that we provide a detailed comparison of the RL policy and the CDR baseline policy, including information about how much effort and time it took to develop each one of them.", "labels": [], "entities": [{"text": "RL", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.8345950245857239}, {"text": "CDR baseline policy", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.9126074910163879}]}, {"text": "We also highlight the cases where the RL policy performs better, and show that understanding the RL policy can provide valuable insights which can inform the creation of an even better rule-based policy.", "labels": [], "entities": []}], "datasetContent": [{"text": "For testing, we use the real user held out conversation data from the HH and HA datasets.", "labels": [], "entities": [{"text": "HH and HA datasets", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.785881757736206}]}, {"text": "The IT and GT thresholds for the baseline Eve were also retrained ( ) using the same data and NLU as used to train the RL policy.", "labels": [], "entities": [{"text": "RL", "start_pos": 119, "end_pos": 121, "type": "TASK", "confidence": 0.8699741959571838}]}, {"text": "shows the setup for testing and comparing the actions of the RL policy and the baseline.", "labels": [], "entities": [{"text": "RL", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.8253734707832336}]}, {"text": "Every ASR partial corresponds to a state.", "labels": [], "entities": []}, {"text": "For every ASR partial we obtain the highest assigned confidence score from the NLU, use the time consumed feature from the game, and obtain the action from the policy.", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9579198956489563}, {"text": "NLU", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.9686309099197388}]}, {"text": "If the action chosen by the policy is \"WAIT\" then we sample the next state.", "labels": [], "entities": [{"text": "WAIT", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9650619626045227}]}, {"text": "For each pair of confidence and time consumed values we obtain the actions from the baseline and the RL policy separately and compare them with the ground truth to evaluate which policy performs better.", "labels": [], "entities": []}, {"text": "Once the policy decides to take either the As-I or As-S action then we advance the simulated game time by an additional interval of 750ms or 1500ms respectively.", "labels": [], "entities": []}, {"text": "This is to simulate the conditions in the real user game where we found that the users on average take 500ms to click the button to load the next set of TIs, and the agent takes 250ms to say the As-I utterance and 1000ms to say the As-S utterance.", "labels": [], "entities": []}, {"text": "The next TI is loaded at this point and then the process is repeated until the game time runs out for each user round.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of users and number of TI sub- dialogues used for our study.", "labels": [], "entities": [{"text": "TI sub- dialogues", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7335640341043472}]}, {"text": " Table 2: Comparison of points per second (PPS) and points (P) earned by the baseline and the RL agent  on the test set.", "labels": [], "entities": [{"text": "Comparison of points per second (PPS) and points (P) earned", "start_pos": 10, "end_pos": 69, "type": "METRIC", "confidence": 0.7813639406647}]}, {"text": " Table 3: The points scored (P) and the time con- sumed (t) in seconds for different image sets (Set).", "labels": [], "entities": [{"text": "time con- sumed (t)", "start_pos": 40, "end_pos": 59, "type": "METRIC", "confidence": 0.8470257265227181}]}]}