{"title": [{"text": "Entropy Reduction correlates with temporal lobe activity", "labels": [], "entities": [{"text": "Entropy Reduction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8930179476737976}]}], "abstractContent": [{"text": "Using the Entropy Reduction incremental complexity metric, we relate high gamma power signals from the brains of epileptic patients to incremental stages of syntactic analysis in English and French.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7065101265907288}]}, {"text": "We find that signals recorded intracranially from the anterior Inferior Temporal Sul-cus (aITS) and the posterior Inferior Temporal Gyrus (pITG) correlate with word-byword Entropy Reduction values derived from phrase structure grammars for those languages.", "labels": [], "entities": []}, {"text": "In the anterior region, this correlation persists even in combination with surprisal co-predictors from PCFG and ngram models.", "labels": [], "entities": []}, {"text": "The result confirms the idea that the brain's temporal lobe houses a parsing function, one whose incremental processing difficulty profile reflects changes in grammatical uncertainty .", "labels": [], "entities": []}], "introductionContent": [{"text": "Incremental complexity metrics connect word-byword processing data to computational proposals about how parsing might work in the minds of real people.", "labels": [], "entities": []}, {"text": "Entropy Reduction is such a metric.", "labels": [], "entities": [{"text": "Entropy Reduction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8192539811134338}]}, {"text": "It relates the comprehension difficulty that people experience at a word to decreases in uncertainty regarding the grammatical alternatives that are in play at any given point in a sentence (for a review, see.", "labels": [], "entities": []}, {"text": "Entropy Reduction plays a key role in accounts of many classic psycholinguistic phenomena) including the difficulty profile of prenominal relative clauses.", "labels": [], "entities": [{"text": "Entropy Reduction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.855202853679657}]}, {"text": "It has connected a wide range of behavioral measures to many * , \u2020 additional affiliation: Universit\u00e9 Paris 11 * additional affiliation:Col\u00ec ege de France different theoretical ideas about incremental processing, both with controlled stimuli and in naturalistic texts.", "labels": [], "entities": []}, {"text": "Entropy Reduction and related metrics of grammatical uncertainty have also proved useful in the analysis of EEG data by helping theorists to interpret well-known event-related potentials.", "labels": [], "entities": [{"text": "Entropy Reduction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6853724867105484}]}, {"text": "This paper applies Entropy Reduction (henceforth: ER) to another type of tightly time-locked brain data: high gamma power electrical signals recorded from the brains of patients awaiting resective surgery for intractable epilepsy.", "labels": [], "entities": [{"text": "Entropy Reduction (henceforth: ER)", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.6852688448769706}]}, {"text": "While experimental participants are reading sentences, entropy reductions from phrase structure grammars predict changes in this measured neural signal.", "labels": [], "entities": []}, {"text": "This occurred at sites within the temporal lobe that have been implicated, in various ways, in language processing).", "labels": [], "entities": [{"text": "language processing", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8006411194801331}]}, {"text": "The result generalizes across both French and English speakers.", "labels": [], "entities": []}, {"text": "The absence of similar correlations in a control condition with word lists suggests that the effect is indeed due to sentencestructural processing.", "labels": [], "entities": []}, {"text": "A companion paper explores algorithmic models of this processing (Nelson et al., Under review).", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized into five sections.", "labels": [], "entities": []}, {"text": "Section 2 first introduces intracranial recording techniques, as they were applied in our study.", "labels": [], "entities": []}, {"text": "Section 3 details the language models that we used, including both hierarchical phrase structure grammars and word-level Markov models.", "labels": [], "entities": []}, {"text": "Section 4 goes onto explain the statistical methods, including a complementary \"sham\" analysis of the word-list control condition where no sentence structure exists.", "labels": [], "entities": []}, {"text": "Section 5 reports the results of these analyses (e.g. 2 Methods: Intracranial recording", "labels": [], "entities": [{"text": "Intracranial recording", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.6587473005056381}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Hypothesis tests for fitted regression coefficients for two parameter models including a different  co-factor of interest with the Entropy Reduction regressor. Each pair of rows corresponds to the two  coefficients of a different two parameter model. Results are shown in the same format as Table 2.", "labels": [], "entities": []}]}