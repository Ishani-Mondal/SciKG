{"title": [{"text": "Extracting word lists for domain-specific implicit opinions from corpora", "labels": [], "entities": [{"text": "Extracting word lists", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8679034113883972}]}], "abstractContent": [{"text": "Sentiment analysis relies to a large extent on lexical resources.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9521555006504059}]}, {"text": "While lists of words bearing a context-independent evaluative polarity ('great', 'bad') are available for many languages now, the automatic extraction of domain-specific evaluative vocabulary still needs attention.", "labels": [], "entities": []}, {"text": "This holds especially for implicit opinions or so-called polar facts.", "labels": [], "entities": []}, {"text": "In our work, we focus on German and on a genre that has not received much attention yet: customer emails.", "labels": [], "entities": []}, {"text": "As the prime downstream application is identifying customers' complaints, we concentrate hereon finding negative words, but our method applies to positive ones as well.", "labels": [], "entities": []}, {"text": "Using a seed list approach, we provide a comparative analysis along three dimensions: effect of different seed lists, different linguistic analysis units, and different statistical correlation tests.", "labels": [], "entities": []}], "introductionContent": [{"text": "One interesting and difficult subtask of sentiment analysis is the automatic recognition of so-called implicit opinions or polar facts: Statements that express a valuation yet do not include context-independent polar words that belong in standard sentiment (polarity) dictionaries.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9144442081451416}]}, {"text": "With a polar fact, an author gives a description of some state of affairs, which prima facie appears to bean objective statement, but for the particular target at hand (or more precisely, all targets of its class) entails a polar opinion.", "labels": [], "entities": []}, {"text": "They have been studied for product reviews) and minutes of meetings, but are also relevant in many other genres such as political or legal discourse.", "labels": [], "entities": []}, {"text": "By their nature, polar fact expressions are domain-specific (see, e.g.,).", "labels": [], "entities": []}, {"text": "Therefore, it is important to be able to acquire the vocabulary fora domain when high-quality sentiment analysis is to be applied.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8813620805740356}]}, {"text": "In this paper, we provide a comparison of several variants of a seed-list approach to generating lists of such lexical items.", "labels": [], "entities": []}, {"text": "The starting point is a list of standard opinion words, which we use as seeds to extract collocating polar fact words and phrases from their contexts.", "labels": [], "entities": []}, {"text": "The genre we tackle is customer emails, and our data comes from four different content domains, which we illustrate with examples: \u2022 Fashion: \"The seam's coming undone.\"", "labels": [], "entities": [{"text": "Fashion", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.6928502321243286}]}, {"text": "\u2022 Food: \"Those cookies were really hard.\"", "labels": [], "entities": []}, {"text": "\u2022 Beauty: \"The perfume does not smell!\"", "labels": [], "entities": []}, {"text": "\u2022 Eyewear: \"With these lenses my sight is blurry.\"", "labels": [], "entities": []}, {"text": "Not surprisingly, most of such customer emails are negative rather than positive: People usually write to the vendor when there is something to complain about.", "labels": [], "entities": []}, {"text": "Being able to automatically identify such messages in a company's email stream is an important downstream application for the task we study here.", "labels": [], "entities": []}, {"text": "For that reason, in this paper we will focus on extracting negative polar lexical items; the method, however, would apply to positive items as well.", "labels": [], "entities": []}, {"text": "Our target language is German, but the method is language-neutral, and furthermore it should be applicable to other genres and domains, too.", "labels": [], "entities": []}, {"text": "So far there is only little related work on polar fact identification in general, and we are not aware of any that has addressed German.", "labels": [], "entities": [{"text": "polar fact identification", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6302303473154703}]}, {"text": "The central aims of our study are to investigate the effects of two different seed sets (varying in origin and size), and of different notions of \"minimal unit\" for defining the collocation context, and to compare the utility of different measures for lexical association.", "labels": [], "entities": []}, {"text": "The following section summarizes the relevant previous work, and Section 3 introduces our data set.", "labels": [], "entities": []}, {"text": "Then, Section 4 presents our experiments and results on word list generation, and Section 5 provides conclusions and an outlook on the next steps of this project.", "labels": [], "entities": [{"text": "word list generation", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7876602013905843}]}], "datasetContent": [{"text": "The system described below was implemented for both positive and negative words, but as explained earlier, we will focus hereon the negative set and provide evaluations only for that.", "labels": [], "entities": []}, {"text": "The basis for our approach is a seed list of negative words, and we experiment with two variants here: The first is a list of 170 domain-independent negative German words (lemmas) that we compiled manually from various sources, targeting specifically the customer-care email genre.", "labels": [], "entities": []}, {"text": "Some translated examples from this list are: unsatisfactory, unpleasant, dirty, pointless, weak, fault, unfortunately, unreliable, sad.", "labels": [], "entities": []}, {"text": "The second was obtained from automatically computing the intersection of negative entries in three existing German sentiment lexicons, see.", "labels": [], "entities": []}, {"text": "It consists of 9004 words.", "labels": [], "entities": []}, {"text": "Henceforth, we abbreviate these lists as NEG-170 and NEG-INTER, respectively.", "labels": [], "entities": [{"text": "NEG-170", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.8616468906402588}, {"text": "NEG-INTER", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.7453629970550537}]}, {"text": "For computing the polarity of segments, we also use a list of manually-compiled of positive words, POS-100.", "labels": [], "entities": [{"text": "POS-100", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.8898829221725464}]}, {"text": "The central goal of the experiments is to assess the influence of \u2022 two different ways of obtaining a seed list for negative words, \u2022 different notions of \"minimal unit\" for the polarity analysis (in the related work above, these were always complete Tweets; we need a more elaborate definition), and \u2022 different measures for computing lexical association.", "labels": [], "entities": []}, {"text": "For each ranked list of negative items as produced by one of the settings, we again consider the top 350 items and measure how many of them are indeed negative (i.e., they are among the 559 confirmed ones).", "labels": [], "entities": []}, {"text": "We determined these accuracy scores for the Fashion domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995549321174622}, {"text": "Fashion domain", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9124228954315186}]}, {"text": "For illustration, shows the top 12 entries in the lists of the extracted negative items for two association measures.", "labels": [], "entities": []}, {"text": "Evidently they differ in favouring single words versus tuples, which we comment on below.", "labels": [], "entities": []}, {"text": "The verb 'unterlaufen' happen in German collocates predominantly with negative event nouns ('Fehler', mistake) but is not by itself negative.", "labels": [], "entities": []}, {"text": "Thus, the items in the Bayesian list indeed all indicate negative sentiment; two thirds of them contain a generally-negative word such as problem, while one third represent domain-specific polar facts (e.g., (change,color)).", "labels": [], "entities": []}, {"text": "The Chi-squared list, on the other hand, has several items that cannot be identified as negative without knowing further context (e.g., small, large).", "labels": [], "entities": []}, {"text": "We surmise that they are often used with the intensifier 'zu' too, which then yiels a negative judgement.", "labels": [], "entities": []}, {"text": "The modal verb 'm\u00fcssen' must in a customer email has a negative ring, similar to 'unterlaufen' occur, which is present in this list as well.", "labels": [], "entities": []}, {"text": "contains twenty top negative words for the domains Fashion and Eyewear obtained with the Bayesian method, all of them with probability 1.0 of being negative.", "labels": [], "entities": [{"text": "Fashion", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9270510673522949}]}, {"text": "Tuples containing generallynegative words, such as problem, have been filtered out for presentation.", "labels": [], "entities": [{"text": "presentation", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.9612298011779785}]}, {"text": "As you can see, both lists contain genre-specific domain-independent words and tuples (such as \"(notice, return shipping)\"or \"(transfer, bill\"), but also domain-specific or at least domain-relevant words and tuples (such as \"worn out\" and \"loosened\", for the Fashion domain, and \"exact-FALSE\" and \"(have, assembly, glasses)-FALSE\", for the Eyewear domain).", "labels": [], "entities": [{"text": "Fashion domain", "start_pos": 259, "end_pos": 273, "type": "DATASET", "confidence": 0.943051278591156}, {"text": "FALSE", "start_pos": 324, "end_pos": 329, "type": "METRIC", "confidence": 0.9567286372184753}]}, {"text": "Many tuples and words are not negative per se, but they appear as negative because in the customer-care-genre and the specific domains they are only mentioned in relation to some trouble or issue; otherwise they are seldom the topic of some mail (e.g. \"loose money pocket\"or \"waterrepellent\") 8 . So, even if this kind of words are not polar, it maybe helpful to consider them for the purpose of identifying complaints.", "labels": [], "entities": []}, {"text": "We consistently obtained much better results for NEG-170 than for NEG-INTER.", "labels": [], "entities": [{"text": "NEG-170", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.8763333559036255}, {"text": "NEG-INTER", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.9108859896659851}]}, {"text": "For example, the overall best result (any unit, any association measure) for NEG-INTER is 0.35, while for NEG-170 we often get results over 0.5, as can be seen in.", "labels": [], "entities": [{"text": "NEG-INTER", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.8758252859115601}, {"text": "NEG-170", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9334073066711426}]}, {"text": "We assume that the long, automaticallygenerated NEG-INTER list has too much noise and produces many irrelevant results for our type of task.", "labels": [], "entities": []}, {"text": "Another possibility is that NEG-INTER does not contain some genre-specific sentiment words that occur particularly often in customer-care e-mails (e.g. \"Schrott\" (scrap), \"unerkl\u00e4rlich\" (unexplicable): Results (accuracy) for the NEG-170 seed set, Fashion domain suggests that having genre-specific seed words (in this case, customer-care sentiment words) maybe more useful to obtain domain-specific lists of words (e.g. for Fashion, Eyewear, Beauty, ...) than using general sentiment seeds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9982580542564392}, {"text": "NEG-170 seed set", "start_pos": 229, "end_pos": 245, "type": "DATASET", "confidence": 0.8128654460112253}, {"text": "Fashion domain", "start_pos": 247, "end_pos": 261, "type": "DATASET", "confidence": 0.9329158663749695}]}, {"text": "In, and in our other experiments, we find that clauses are generally the most successful unit of analysis for our task.", "labels": [], "entities": []}, {"text": "This can be related to the genre of the customer emails, which very often area mix of neutral reporting (\"I received the ordered box last week\", etc.) and a single specific complaint.", "labels": [], "entities": []}, {"text": "Sometimes, this complaint is actually part of a contrastive sentence (\"I like the color of the blouse, but it really doesn't fit\", etc.).", "labels": [], "entities": []}, {"text": "This is in contrast to text-level sentiment analysis, as needed for product or movie reviews, where the general tone of the overall text is to be determined.", "labels": [], "entities": [{"text": "text-level sentiment analysis", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7057753602663676}]}, {"text": "Overall, the relative likelihood ratio test, the frequency classes difference test, the Bayesian test and the PMI test achieve the best results.", "labels": [], "entities": [{"text": "relative likelihood ratio test", "start_pos": 13, "end_pos": 43, "type": "METRIC", "confidence": 0.7553382441401482}, {"text": "frequency classes difference test", "start_pos": 49, "end_pos": 82, "type": "METRIC", "confidence": 0.7356248199939728}, {"text": "Bayesian test", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.9538662731647491}, {"text": "PMI test", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.6548805832862854}]}, {"text": "The fact that the log likelihood and Chi-square tests turnout considerably worse might be a result of our setting where tuples play an important role and have generally a better chance to get a positive vote from the human annotators; in particular, a verbobject combination is often more easily judged as negative in the domain than just the single verb.", "labels": [], "entities": []}, {"text": "The two last-mentioned tests, however, yield more individual words than the others do, because the words occur more frequently than the tuples, and the two tests are more sensitive to this than the others are (which are based on probabilities independent of the absolute frequency in the corpus).", "labels": [], "entities": []}, {"text": "In general, the other tests rank higher items occurring only in one polarity, independently of their frequency.", "labels": [], "entities": []}, {"text": "Besides accuracy, we also measured precision, recall, and F1 for the various combinations of settings that we investigated, and with various thresholds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9995642304420471}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9997466206550598}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9996721744537354}, {"text": "F1", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9998446702957153}]}, {"text": "The maximum F1 score we obtained is 0.55 (for a threshold of 0.742, using the Bayesian measure), with precision at 0.65 and recall at 0.48.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.982403427362442}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9996601343154907}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9996287822723389}]}, {"text": "The optimal precision is (also using the Bayesian measure, with a threshold of 0.957) 0.73, accompanied by a recall of 0.4.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9995809197425842}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9995397329330444}]}], "tableCaptions": [{"text": " Table 1: Sample contingency table, as built for each candidate item (word, pred/arg tuple)", "labels": [], "entities": []}, {"text": " Table 4: Results (accuracy) for the NEG-170 seed set, Fashion domain", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9994484782218933}, {"text": "NEG-170 seed set", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.940549910068512}]}]}