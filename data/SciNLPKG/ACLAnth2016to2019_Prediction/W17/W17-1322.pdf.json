{"title": [{"text": "Arabic Textual Entailment with Word Embeddings", "labels": [], "entities": [{"text": "Arabic Textual Entailment", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5636768043041229}]}], "abstractContent": [{"text": "Determining the textual entailment between texts is important in many NLP tasks, such as summarization, question answering , and information extraction and retrieval.", "labels": [], "entities": [{"text": "Determining the textual entailment between texts", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7245774616797765}, {"text": "summarization", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.9923669695854187}, {"text": "question answering", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.8618076145648956}, {"text": "information extraction and retrieval", "start_pos": 129, "end_pos": 165, "type": "TASK", "confidence": 0.7817239463329315}]}, {"text": "Various methods have been suggested based on external knowledge sources; however, such resources are not always available in all languages and their acquisition is typically laborious and very costly.", "labels": [], "entities": []}, {"text": "Distributional word representations such as word embeddings learned overlarge corpora have been shown to capture syntactic and semantic word relationships.", "labels": [], "entities": []}, {"text": "Such models have contributed to improving the performance of several NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of textual entailment in Arabic.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7409851253032684}]}, {"text": "We employ both traditional features and distributional representations.", "labels": [], "entities": []}, {"text": "Crucially, we do not depend on any external resources in the process.", "labels": [], "entities": []}, {"text": "Our suggested approach yields state of the art performance on a standard data set, ArbTE, achieving an accuracy of 76.2 % compared to current state of the art of 69.3 %.", "labels": [], "entities": [{"text": "ArbTE", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.5123751163482666}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9994743466377258}]}], "introductionContent": [{"text": "Recently, there have been a number of studies addressing the problem of Recognizing Textual Entailment (RTE).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 72, "end_pos": 108, "type": "TASK", "confidence": 0.8935740093390147}]}, {"text": "The core problem is to recognize semantic variability in textual expression, which can potentially have the same meaning (.", "labels": [], "entities": []}, {"text": "Modeling this phenomenon has a significant impact on various NLP applications, such as question answering, machine translation, and summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.9234340190887451}, {"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8395113945007324}, {"text": "summarization", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.9902119040489197}]}, {"text": "Textual Entailment (TE) can be defined as a directional entailment relation between a pair of text fragments; if the meaning of the Hypothesis (H) can be inferred from the Text (T) ().", "labels": [], "entities": [{"text": "Textual Entailment (TE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8208375751972199}]}, {"text": "Since the first PASCAL RTE challenge () to date, different approaches have been proposed.", "labels": [], "entities": [{"text": "PASCAL RTE challenge", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.796902080376943}]}, {"text": "A popular trend is the use of supervised machine learning approaches that rely on extracting a set of features based on the underlying syntactic/semantic/lexical relation between the TH pair.", "labels": [], "entities": []}, {"text": "Most of the approaches have been applied and tested on English TE.", "labels": [], "entities": [{"text": "English TE", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.8490214049816132}]}, {"text": "Arabic, on the other hand, has relatively fewer studies for entailment detection.", "labels": [], "entities": [{"text": "entailment detection", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.9580840766429901}]}, {"text": "It is one of the most complex languages to process due to its morphological richness and relatively free word order as well as its diglossic nature (where the standard and the dialects mix inmost genres of data).", "labels": [], "entities": []}, {"text": "Moreover, Arabic still lacks the large scale handcrafted computational resources that have come in very handy for English such as a large WordNet or a resource such as VerbOcean ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.9418632388114929}]}, {"text": "Hence building a reliable RTE system for Arabic poses more challenges than those faced when dealing with English.", "labels": [], "entities": []}, {"text": "Accordingly, in this paper, we propose an approach that does not rely on such external resources but rather on modeling word relations derived from large scale corpora.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 provides an overview of textual entailment works in both English and Arabic, Section 3 describes the basic features and word distributional representation based features, Results and an evaluation of the system are presented in Section 4, and we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our system is a supervised model, therefore, we experiment with multiple supervised frameworks: an SVM classifier (LIBSVM), Logistic Regression (LR) using specifically the LIBLIN-EAR classifier, and Random Forest (RF).", "labels": [], "entities": []}, {"text": "All experiments are implemented using the WEKA software package).", "labels": [], "entities": [{"text": "WEKA software package", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.9418174425760905}]}, {"text": "All classifiers yield relatively similar performance with the LR classifier obtaining the best results, which is expected since both the feature space and the dataset are relatively small.", "labels": [], "entities": []}, {"text": "Therefore, we report results using LR only.", "labels": [], "entities": []}, {"text": "We report results on a development tuning set, DEV, and a TEST set.", "labels": [], "entities": [{"text": "DEV", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8221092820167542}, {"text": "TEST", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9698503613471985}]}, {"text": "We devised 3 training protocols: DEV1, DEV5, and DEV10.", "labels": [], "entities": [{"text": "DEV1", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.772516667842865}, {"text": "DEV5", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.6368017196655273}, {"text": "DEV10", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.931506872177124}]}, {"text": "Given the size of the labeled data, we run our experiments varying the training protocol and tuning steps while keeping the TEST as a held out data set constant for set ups DEV1 and DEV5.", "labels": [], "entities": [{"text": "TEST", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9982153177261353}, {"text": "DEV1", "start_pos": 173, "end_pos": 177, "type": "DATASET", "confidence": 0.9555917978286743}, {"text": "DEV5", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.9147207140922546}]}, {"text": "The tuning data, DEV1, comprises 10% of the data, our TRAIN1 data corresponds to 80%, and TEST (held out) is 10% of the data.", "labels": [], "entities": [{"text": "DEV1", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.7098323106765747}, {"text": "TRAIN1", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9954504370689392}, {"text": "TEST (held out)", "start_pos": 90, "end_pos": 105, "type": "METRIC", "confidence": 0.8946911931037903}]}, {"text": "In the second setup, for DEV5 we calculate the average performance as measured across 5-fold cross validation on 90% of the data.", "labels": [], "entities": [{"text": "DEV5", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9186965227127075}]}, {"text": "In DEV10 we carryout our experiments with 10-fold cross validation on the entire dataset so as to with typical features (length and similarity score, named entity), specifically, without word embeddings, as TYP; with word embeddings features alone, as WE; the last setting is with all features combined, as ALL, both TYP and WE combined.", "labels": [], "entities": [{"text": "DEV10", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9592959880828857}, {"text": "length and similarity score", "start_pos": 121, "end_pos": 148, "type": "METRIC", "confidence": 0.8495177030563354}]}, {"text": "We report results on two baselines based on the percentage of common words or lemmas between T and H.", "labels": [], "entities": []}, {"text": "BOW1 and BOW2 represent these baselines in.", "labels": [], "entities": [{"text": "BOW1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5254323482513428}, {"text": "BOW2", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.6014343500137329}]}, {"text": "In BOW1, we represent the overlap score as binary score (0 or 1) according to a predefined threshold of 75% overlap.", "labels": [], "entities": [{"text": "BOW1", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.5773301720619202}, {"text": "overlap score", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.9754348993301392}]}, {"text": "In the second baseline, BOW2, the word overlap score is used but, different from BOW1, we use the classifier to determine the optimal threshold based on the training data.", "labels": [], "entities": [{"text": "BOW2", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.6196907162666321}, {"text": "word overlap score", "start_pos": 34, "end_pos": 52, "type": "METRIC", "confidence": 0.8909833033879598}, {"text": "BOW1", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.5560991764068604}]}, {"text": "As can be seen, BOW2 is higher than BOW1 where a threshold is manually set; this is an artifact of the nature of this dataset where there is a high correlation between overlap percentage and the entailment relation, and the cutting point for the entailment learned by the classifier is optimal for this dataset.", "labels": [], "entities": [{"text": "BOW2", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9509173631668091}, {"text": "BOW1", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.7369067072868347}]}, {"text": "Therefore, we include both as baselines for the system.", "labels": [], "entities": []}, {"text": "Beside the baselines, illustrates the results obtained by previous studies on the same data set.", "labels": [], "entities": []}, {"text": "We only have the 10 fold (DEV10) results from other systems.", "labels": [], "entities": [{"text": "DEV10", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.6042437553405762}]}, {"text": "As illustrated, LR-ALL condition yields the best results for all test conditions consistently across all data sets improving over the baselines by a significant margin and outperforms LR-TYP and LR-WE.", "labels": [], "entities": [{"text": "LR-ALL", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9417423605918884}]}, {"text": "Other systems (ETED1, ETED2, ETED+ABC, ATE, and SANATE) have approached the Arabic TE in a dif-ferent way, wherein ETED systems the main focus was on the impact of Tree Edit Distance (TED) on the Arabic TE using different model extension, and in ATE and SANATE systems the focus was on the effect of negation and polarity on the Arabic TE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.7528318166732788}]}, {"text": "Our system outperforms these systems significantly.", "labels": [], "entities": []}, {"text": "Moreover, LR-TYP significantly outperforms LR-WE and achieves the best performance among all runs and all three setups.", "labels": [], "entities": [{"text": "LR-TYP", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8065243363380432}]}, {"text": "These results indicate that word embedding based features enhance the accuracy by about 2% increase from the TYP based system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9995119571685791}]}, {"text": "The 10 fold cross validation experimental setup is carried out to compare our performance against previous studies, namely, employing the same experimental setups in.", "labels": [], "entities": []}, {"text": "We can see that our result outperforms other works when using TYP and ALL which shows that not only the word embedding but also the basic similarity features that have been heavily implemented on the English system have improved the result over the Arabic entailment state of the art, along with explicit NE modeling as a bag of words and the calculation of similarity measures over it.", "labels": [], "entities": [{"text": "NE modeling", "start_pos": 305, "end_pos": 316, "type": "TASK", "confidence": 0.8867082893848419}]}, {"text": "On the other hand, the word embedding based features alone yield comparable results to the other systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance Accuracy % of our system on ArbTE datasets, along with results yielded by  comparative state of the art systems", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9774029850959778}, {"text": "ArbTE datasets", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.983932226896286}]}]}