{"title": [{"text": "Annotation schemes in North S\u00e1mi dependency parsing", "labels": [], "entities": [{"text": "S\u00e1mi dependency parsing", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.5446663200855255}]}], "abstractContent": [{"text": "In this paper we describe a comparison of two annotation schemes for dependency parsing of North S\u00e1mi, a Finno-Ugric language spoken in the north of Scandinavia and Finland.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8544229567050934}]}, {"text": "The two annotation schemes are the Giellatekno (GT) scheme which has been used in research and applications for the S\u00e1mi languages and Universal Dependencies (UD) which is a cross-lingual scheme aiming to unify annotation stations across languages.", "labels": [], "entities": []}, {"text": "We show that we are able to deterministi-cally convert from the Giellatekno scheme to the Universal Dependencies scheme without a loss of parsing performance.", "labels": [], "entities": []}, {"text": "While we do not claim that either scheme is a priori a more adequate model of North S\u00e1mi syntax, we do argue that the choice of annotation scheme is dependent on the intended application.", "labels": [], "entities": []}, {"text": "This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by-nd/4.0/ 1 66", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsing is an important step in many applications of natural language processing, such as information extraction, machine translation, interactive language learning and corpus search interfaces.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8570876717567444}, {"text": "natural language processing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6919451753298441}, {"text": "information extraction", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.8281628489494324}, {"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7825835049152374}]}, {"text": "There area number of approaches to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8852722644805908}]}, {"text": "These include rule-based approaches such as for German and for Portuguese, and statistical approaches, such as transition-based parsing, exemplified by MaltParser.", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.4786718487739563}, {"text": "MaltParser", "start_pos": 152, "end_pos": 162, "type": "DATASET", "confidence": 0.9721639156341553}]}, {"text": "In rule-based approaches the knowledge source is a set of rules (such as constraints), while in statistical approaches the knowledge source is a collection of parsed sentences called treebank.", "labels": [], "entities": []}, {"text": "Both rule-based and statistical approaches have in common that the parses they output conform to a given annotation scheme, that is a set of rules which define what linguistic structure should be applied to given constructions.", "labels": [], "entities": []}, {"text": "Annotation schemes can vary substantially according to how they encode an analysis of linguistic structure.", "labels": [], "entities": []}, {"text": "For example, one scheme may decide that the auxiliary verb is the head of an auxiliary-main verb construction because of subject agreement, while another may decide that the main verb is the head because of case government of arguments.", "labels": [], "entities": []}, {"text": "The choice of representation can depend heavily on application.", "labels": [], "entities": []}, {"text": "For certain applications, such as grammar checking, a more morphosyntactic scheme maybe appropriate, while for others, such as machine translation, a more syntacto-semantic scheme maybe more appropriate.", "labels": [], "entities": [{"text": "grammar checking", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.8650368750095367}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.8090464472770691}]}, {"text": "In this paper, we describe the conversion of the annotation schemes output by the Giellatekno parser to a corresponding UD-compliant scheme.", "labels": [], "entities": []}, {"text": "We also provide comparative statistics on the composition of the corpus used in the experiments before and after the conversion.", "labels": [], "entities": []}, {"text": "Afterwards, we report a comparison of parsing results using the two annotation schemes.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9721986055374146}]}, {"text": "The paper is structured as follows: Section 2 describes the corpus and source annotation scheme used in this paper; section 3 describes the conversion procedure and the target annotation scheme; section 4 describes an experiment in comparing annotation schemes and finally section 5 presents some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to test the utility of the two annotations schemes in a real-world setting, we trained and evaluated a number of models using the popular UDpipe toolkit.", "labels": [], "entities": [{"text": "UDpipe toolkit", "start_pos": 147, "end_pos": 161, "type": "DATASET", "confidence": 0.89979687333107}]}, {"text": "UDpipe is a toolkit for data-driven tokenisation, part-of-speech tagging and dependency parsing; it learns a statistical model for each of the tasks from treebank data and applies this model to process unseen sentences.", "labels": [], "entities": [{"text": "UDpipe", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8860406875610352}, {"text": "part-of-speech tagging", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7336148470640182}, {"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7500440776348114}]}, {"text": "We perform 10-fold cross-validation by randomising the order of sentences in the corpus and splitting them into 10 equally-sized parts.", "labels": [], "entities": []}, {"text": "In each iteration we held out one part for testing and used the rest for training.", "labels": [], "entities": []}, {"text": "We trained UDpipe for 10 epochs, the default setting.", "labels": [], "entities": [{"text": "UDpipe", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.8870335817337036}]}, {"text": "We calculated the labelled-attachment score (LAS) and unlabelledattachment score (UAS) for each of the models.", "labels": [], "entities": [{"text": "labelled-attachment score (LAS)", "start_pos": 18, "end_pos": 49, "type": "METRIC", "confidence": 0.9065471649169922}, {"text": "unlabelledattachment score (UAS)", "start_pos": 54, "end_pos": 86, "type": "METRIC", "confidence": 0.9426569700241089}]}, {"text": "In addition to the LAS and UAS we also calculated the raw label accuracy.", "labels": [], "entities": [{"text": "LAS", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9961547255516052}, {"text": "UAS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.8254809975624084}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9417747259140015}]}, {"text": "The same splits were used for both annotation schemes.", "labels": [], "entities": []}, {"text": "Despite the smaller label set, the UD annotation scheme performs worse than the GT scheme.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on rule-types. a \u2192 b denotes that the rules change GT label a to UD label  b. There was one additional rule which performed a tree transformation with no relabelling  operation to reattach punctuation.", "labels": [], "entities": []}, {"text": " Table 2: Comparative statistics for the Giellatekno and UD annotation schemes. Relations  (including direction) indicates the number of labels if head-direction indicators are taken into  account.", "labels": [], "entities": [{"text": "Giellatekno", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9058660864830017}]}]}