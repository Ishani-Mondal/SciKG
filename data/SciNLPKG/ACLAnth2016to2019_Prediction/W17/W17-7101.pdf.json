{"title": [{"text": "Exploring Multi-Modal Text+Image Models to Distinguish between Abstract and Concrete Nouns", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper explores variants of multi-modal computational models that aim to distinguish between abstract and concrete nouns.", "labels": [], "entities": []}, {"text": "We assumed that textual vs. visual modalities might have different strengths in providing information on abstract vs. concrete words.", "labels": [], "entities": []}, {"text": "While the overall predictions of our models were highly successful (reaching an accuracy of 96.45% in a binary classification and a Spearman correlation of 0.86 in a regression analysis), the differences between the textual, visual and combined modalities were however negligible, hence both text and images seem to provide reliable, non-complementary information to represent both abstract and concrete words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9987366795539856}]}], "introductionContent": [{"text": "Over the years, different disciplines have been interested in exploring the contributions of contextual and perceptual information inhuman language acquisition and processing.", "labels": [], "entities": [{"text": "contextual and perceptual information inhuman language acquisition and processing", "start_pos": 93, "end_pos": 174, "type": "TASK", "confidence": 0.7580616407924228}]}, {"text": "From a psycholinguistic perspective, the grounding theory indicates that the mental representation of a concept is built not only through linguistic exposure but also incorporating multi-modal information extracted from real world situations, including auditory, visual, etc.", "labels": [], "entities": []}, {"text": "From a computational perspective, multi-modality has been shown to enhance corpus-based cooccurrence models that predict lexical information on various tasks, such as simulating word association, predicting semantic and visual similarity, determining the compositionality of multi-word expressions, and distinguishing between abstract and concrete concepts (.", "labels": [], "entities": [{"text": "predicting semantic and visual similarity", "start_pos": 196, "end_pos": 237, "type": "TASK", "confidence": 0.8643864274024964}]}, {"text": "One focus of interest on the computational side has addressed the question of when and which perceptual information is helpful for semantic predictions in computational models, i.e., under which conditions perceptual information enhances or even outperforms textual information.", "labels": [], "entities": [{"text": "semantic predictions", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.76724573969841}]}, {"text": "For example, previous work described filters that added visual information to corpus-based information into a computational model of word meaning only in specific conditions: suggested the dispersion filter that integrates only images that resemble each other to a certain degree.", "labels": [], "entities": []}, {"text": "applied the same filter to a computational model of compositionality and added two more filters: the imageability filter integrating only images for highly imaginable words, as determined by existing imageability ratings; and the clustering filter only using images fora word that were similar to each other, as determined by a cluster analysis.", "labels": [], "entities": []}, {"text": "In this paper, we explore variants of multi-modal computational models that aim to distinguish between abstract and concrete nouns, to contribute to both psycholinguistic research by exploring differences in abstract vs. concrete concept representation and processing), and to computational linguistic research by exploring differences between textual and visual information in modelling semantic knowledge.", "labels": [], "entities": []}, {"text": "More specifically, we apply a binary classifier as well as a regression model to differentiate between abstract vs. concrete English nouns, as determined by concreteness ratings from.", "labels": [], "entities": []}, {"text": "As features we compare standard textual count co-occurrences from a web corpus), word2vec embeddings (, GoogLeNet image vectors (, and variants of text and image vector concatenation.", "labels": [], "entities": []}, {"text": "As qualitative analysis, we compare classification errors across modalities, to zoom into the strengths and limits of the modalities and model parameters.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Classification and regression results.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9763477444648743}]}]}