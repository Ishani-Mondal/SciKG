{"title": [{"text": "The Influence of Spelling Errors on Content Scoring Performance", "labels": [], "entities": [{"text": "Spelling Errors", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.8426534533500671}, {"text": "Content Scoring", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7122843265533447}]}], "abstractContent": [{"text": "Spelling errors occur frequently in educational settings, but their influence on automatic scoring is largely unknown.", "labels": [], "entities": []}, {"text": "We therefore investigate the influence of spelling errors on content scoring performance using the example of the short answer data set of the Automated Student Assessment Prize (ASAP).", "labels": [], "entities": [{"text": "short answer data set of the Automated Student Assessment Prize (ASAP)", "start_pos": 114, "end_pos": 184, "type": "DATASET", "confidence": 0.6417957567251645}]}, {"text": "We conduct an annotation study on the nature of spelling errors in the ASAP dataset and utilize these finding in machine learning experiments that measure the influence of spelling errors on automatic content scoring.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9242457747459412}]}, {"text": "Our main finding is that scoring methods using both token and character n-gram features are robust against spelling errors up to the error frequency seen in ASAP.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 157, "end_pos": 161, "type": "DATASET", "confidence": 0.6791199445724487}]}], "introductionContent": [{"text": "Spelling errors occur frequently in educational assessment situations, not only in language learning scenarios, but also with native speakers, especially when answers are written without the help of a spell-checker.", "labels": [], "entities": [{"text": "Spelling errors", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8120814561843872}]}, {"text": "In automatic content scoring for short answer questions, a model is learnt about which content needs to be present in a correct answer.", "labels": [], "entities": [{"text": "automatic content scoring", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.630340576171875}]}, {"text": "Spelling mistakes interfere with this process, as they should be mostly ignored for content scoring.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7696930170059204}]}, {"text": "It is still largely unknown how severe the problem is in a practical setting.", "labels": [], "entities": []}, {"text": "Consider the following answer to the first prompt of the short answer data set of the Automated Student Assessment Prize (ASAP): 2 Note that we do not distinguish between the terms error and mistake used by to denote competence and performance errors respectively.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP)", "start_pos": 86, "end_pos": 127, "type": "TASK", "confidence": 0.6408349999359676}, {"text": "error", "start_pos": 181, "end_pos": 186, "type": "METRIC", "confidence": 0.9542295336723328}]}, {"text": "We use the two terms interchangeably.", "labels": [], "entities": []}, {"text": "2 https://www.kaggle.com/c/asap-sas (1) Some additional information you will need are the material.", "labels": [], "entities": []}, {"text": "You also need to know the size of the contaneir to measure how the acid rain effected it.", "labels": [], "entities": []}, {"text": "You need to know how much vineager is used for each sample.", "labels": [], "entities": []}, {"text": "Another thing that would help is to know how big the sample stones are by measureing the best possible way.", "labels": [], "entities": []}, {"text": "In this answer, three non-word spelling errors (printed in bold) occur.", "labels": [], "entities": []}, {"text": "In addition, there is also one real-word spelling error, which leads to an existing word: effected, which should be affected.", "labels": [], "entities": []}, {"text": "While a teacher who is manually scoring learner answers can simply try to ignore spelling mistakes as far as possible, automatic scoring methods must include a spell-checking component to normalize an occurrence of vineager to vinegar.", "labels": [], "entities": []}, {"text": "Thus spell-checking components are also apart of some content scoring systems, such as the top two performing systems in the ASAP challenge.", "labels": [], "entities": [{"text": "ASAP challenge", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.6502252221107483}]}, {"text": "However, it is unclear what impact spelling errors really have on the performance of content scoring systems.", "labels": [], "entities": []}, {"text": "Many systems in the ASAP challenge, as well as some participating systems in the SemEval 2013 Student Response Analysis Task, used shallow features such as token n-grams.", "labels": [], "entities": [{"text": "ASAP challenge", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.7658458054065704}, {"text": "SemEval 2013 Student Response Analysis Task", "start_pos": 81, "end_pos": 124, "type": "TASK", "confidence": 0.8566965659459432}]}, {"text": "If a token in the test data is misspelled, then there is noway of knowing that it has the same meaning as the correct spelling of the word in the training data.", "labels": [], "entities": []}, {"text": "At the same time, individual spelling error instances are often not occurring uniquely in a dataset: Depending on factors such as the learner group (for example native speakers or language learners with a certain native language) or the data collection method (handwriting vs. typing) some spelling errors will occur frequently while others will be rare.", "labels": [], "entities": []}, {"text": "The mis-spelled form vineger, for example, might be frequent enough that an occurrence feature for the misspelled version provides valuable information which a classifier can learn.", "labels": [], "entities": []}, {"text": "Whether this observation mitigates the effect of spelling errors depends on the frequency of individual errors and therefore also on the shape of error distributions.", "labels": [], "entities": []}, {"text": "Contributions In this paper, we investigate how the presence or absence of spelling errors influences the task of content scoring, taking the afore-mentioned influence criteria of error frequency and error distribution into account.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.7608725130558014}]}, {"text": "We conduct our analyses and experiments on the frequently used ASAP content scoring dataset).", "labels": [], "entities": [{"text": "ASAP content scoring dataset", "start_pos": 63, "end_pos": 91, "type": "DATASET", "confidence": 0.671461433172226}]}, {"text": "The dataset contains 10 different prompts about different topics ranging from sciences over biology to literature questions.", "labels": [], "entities": []}, {"text": "Each prompt comes with 2,200 answers on average.", "labels": [], "entities": []}, {"text": "Although this dataset has been used in a lot of studies concerning content scoring, much about the spelling errors in the dataset is still unknown.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7456393539905548}]}, {"text": "Our manual annotations and corpus analyses will therefore also provide insight on the number, the nature and the distribution of spelling errors in this dataset.", "labels": [], "entities": []}, {"text": "First, we present an analysis of the frequency and distribution of non-word spelling errors in the ASAP corpus and compare several spelling dictionaries.", "labels": [], "entities": [{"text": "ASAP corpus", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.7585227489471436}]}, {"text": "We provide a gold-standard correction for the non-word errors found automatically by a spellchecker in the test section of the data.", "labels": [], "entities": []}, {"text": "We compare error correction methods based on phonetic and edit distance and extend them with a domainspecific method that prefers suggestions occurring in the material fora specific prompt.", "labels": [], "entities": []}, {"text": "Next, we investigate the effect of manipulating the number and distribution of spelling errors on the performance of an automatic content scoring system.", "labels": [], "entities": []}, {"text": "We experiment with two ways of regulating the number of misspellings.", "labels": [], "entities": []}, {"text": "We automatically and manually spellcheck the corpus to replace non-word spelling errors by their corrected version.", "labels": [], "entities": []}, {"text": "This only allows us to decrease the number of errors.", "labels": [], "entities": []}, {"text": "To increase the amount of spelling errors further, we also introduce errors artificially in two conditions: (i) adding random noise as a worst-case scenario, and (ii) adding mistakes according to the error distribution in the test data.", "labels": [], "entities": []}, {"text": "We find that token and character n-gram scoring features are largely robust against spelling errors with a frequency present in our data.", "labels": [], "entities": []}, {"text": "Character n-gram features are contributing towards this robustness.", "labels": [], "entities": []}, {"text": "When introducing more errors, we see a substantial drop in performance, such that the importance of spell-checking components in content scoring depends on the frequency of errors in the data.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.7582980692386627}]}], "datasetContent": [{"text": "To get a better understanding of the nature of spelling errors, we provide additional analyses on our annotations.", "labels": [], "entities": []}, {"text": "In our experimental studies, we examine the influence of spelling deviations and spell-checking on content scoring.", "labels": [], "entities": []}, {"text": "We train one classification model for each of the ten ASAP prompts, using the published data split into training data and \"public leaderbord\" data for testing.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.9033918976783752}]}, {"text": "We preprocess the data using the ClearNLP segmenter and POS tagger provided through DKPro Core (Eckart de).", "labels": [], "entities": [{"text": "DKPro Core (Eckart de)", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.8813370863596598}]}, {"text": "We use a standard feature set often used in content scoring () and extract token 1-3 grams and character 2-4 grams using the top 10,000 most frequent n-grams in each feature group.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7320906817913055}]}, {"text": "We then train a SVM classifier () with default parameter settings provided through DKPro TC ().", "labels": [], "entities": [{"text": "DKPro TC", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.8616863787174225}]}, {"text": "We evaluate using both accuracy and quadratically weighted kappa), as proposed in the Kaggle competition for this dataset and present results averaged across all 10 prompts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9992566704750061}]}, {"text": "One important property of this feature setup is that the character n-gram features could be able to cover useful information from misspelled words.", "labels": [], "entities": []}, {"text": "If the word experiment is, for example, misspelled as expirment, there are n-grams shared between these two versions, such as the character trigrams exp, men or ent.", "labels": [], "entities": []}, {"text": "Therefore, we also use a reduced feature set, where we only work with token n-gram features, in order to quantify the size of this effect.: Scoring performance on ASAP with and without spell checking  Ina first machine learning experiment, we investigate the influence of spell-checking on the performance of content scoring.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 309, "end_pos": 324, "type": "TASK", "confidence": 0.6887197941541672}]}, {"text": "We systematically vary three sets of influence factors: First, we use either our automatically corrected learner answers as a realistic spell checking scenario or the corrected gold-standard version of the learner answers.", "labels": [], "entities": [{"text": "spell checking", "start_pos": 136, "end_pos": 150, "type": "TASK", "confidence": 0.7363837659358978}]}, {"text": "We consider the latter an oracle condition to estimate an upper bound of improvement that filters out noise introduced by the spellchecker.", "labels": [], "entities": []}, {"text": "Second, we use two different feature sets: either the full feature set covering both token and character ngrams or the reduced feature set with only token n-grams.", "labels": [], "entities": []}, {"text": "Third, we vary which part of the data is spell-checked.", "labels": [], "entities": []}, {"text": "We either correct both the training and the test data or only training or only test data.", "labels": [], "entities": [{"text": "correct", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9620533585548401}]}, {"text": "In the oracle condition, we have only annotations for the test set, so that we use only the test condition here.", "labels": [], "entities": []}, {"text": "We see that it makes little difference whether we spell-check the data (be it automatically or manually).", "labels": [], "entities": []}, {"text": "One possible explanation for the very small difference is that there are many answers without any spelling mistakes at all.", "labels": [], "entities": []}, {"text": "Thus, we also comparing the performance for answers with different minimal number of errors.", "labels": [], "entities": []}, {"text": "shows the breakdown of the results per number of errors.", "labels": [], "entities": []}, {"text": "We observe a reduced performance in kappa for answers with more spelling errors, but do not see that repeated in the accuracy, i.e. misclassified answers with more errors have a higher tendency to be completely misclassified.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9990262985229492}]}, {"text": "As we have seen in the above experimental study, there is little difference between the scoring qual-: Scoring performance on ASAP when using only answers with a certain minimal number of errors for testing.", "labels": [], "entities": []}, {"text": "ity on original data vs. spell-checked data.", "labels": [], "entities": []}, {"text": "We already ruled out that this might be due to a noisy spell-checker by also evaluating on the annotated gold standard.", "labels": [], "entities": []}, {"text": "Another potential reason for our findings is that the amount of errors present in the data is just not large enough to make a difference.", "labels": [], "entities": []}, {"text": "To check that, we artificially introduce different amounts of new errors into the learner answers.", "labels": [], "entities": []}, {"text": "In this way, we can also simulate corpora with different properties, so that practitioners can check the average amount of spelling errors in their data and can get an estimation of what performance drop to expect.", "labels": [], "entities": []}, {"text": "Generating Spelling Errors In order to generate additional spelling errors, we use two different models: Random Error Generation introduces errors by either adding, deleting, or substituting a letter or by swapping two letters in randomly selected words.", "labels": [], "entities": [{"text": "Generating Spelling Errors", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7868567307790121}]}, {"text": "This error generation process is a worst case experiment in the sense that there is no predictable pattern in the produced errors.", "labels": [], "entities": [{"text": "error generation", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.6979441046714783}]}, {"text": "Informed Error Generation produces errors according to the distribution of errors in the data.", "labels": [], "entities": [{"text": "Informed Error Generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6069958706696829}]}, {"text": "This means we introduce errors only to words that were misspelled in our annotated gold-standard, and we introduce errors by using the misspelled version actually occurring in the data considering the error distribution.", "labels": [], "entities": []}, {"text": "In this way, there are chances that -like in real life -some errors will be more frequent than others such that a classifier might be able to learn from them.", "labels": [], "entities": []}, {"text": "We use both models with different configurations of the experimental setup.", "labels": [], "entities": []}, {"text": "We vary whether the errors are added to all words (all) or only to content words (cw).", "labels": [], "entities": []}, {"text": "This is because we observed that mainly longer and content words are misspelled.", "labels": [], "entities": []}, {"text": "We argue that these words can be more important in content scoring than small function words.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7679924070835114}]}, {"text": "Therefore, those realistic errors might do more harm than random errors.", "labels": [], "entities": []}, {"text": "In both conditions, we make sure that the overall error rate across all tokens matches the desired percentage.", "labels": [], "entities": [{"text": "error rate", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9528438746929169}]}, {"text": "Additionally, we use either only token features (tok), that will be more sensitive towards spelling errors, or also include the character features (tok+char), which we know to be more robust.", "labels": [], "entities": []}, {"text": "shows the performance of the two error generation models in their different variants for different amounts of artificial errors.", "labels": [], "entities": []}, {"text": "Note that there was a natural upper bound for the amount of errors which can be introduced using the informed error generator.", "labels": [], "entities": []}, {"text": "As expected, we see that content words are more important for scoring than function words as introducing errors to only content words yields a larger performance drop.", "labels": [], "entities": []}, {"text": "We see for both generation models that a scoring model using character n-grams is largely robust against spelling mistakes while a model using only information on the token level is not.", "labels": [], "entities": []}, {"text": "We also see that a more realistic error generation process is not as detrimental for the scoring performance as random errors.", "labels": [], "entities": [{"text": "error generation", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6657651215791702}]}, {"text": "Of course, our error model might be slightly over-optimistic and in real life with such a high number of errors we might see new orthographic variants for individual words that were not covered in our annotations.", "labels": [], "entities": []}, {"text": "We therefore believe the realistic curve to be somewhere between the informed and the random model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of different error detection  dictionaries", "labels": [], "entities": [{"text": "error detection  dictionaries", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.7705555558204651}]}, {"text": " Table 2: Performance of different error correction  methods", "labels": [], "entities": [{"text": "error correction", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7183988094329834}]}, {"text": " Table 3: Probability for tokens from a certain POS  class to be misspelled.", "labels": [], "entities": []}, {"text": " Table 4: Average number of spelling mistakes  per 100 tokens (punctuation excluded) and type- token-ratio for errors for the individual ASAP  prompts.", "labels": [], "entities": [{"text": "Average number of spelling mistakes", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.905045461654663}, {"text": "ASAP  prompts", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.7038325667381287}]}, {"text": " Table 5: Top-10 most frequent misspellings for  prompt 2", "labels": [], "entities": [{"text": "prompt", "start_pos": 49, "end_pos": 55, "type": "TASK", "confidence": 0.9669507145881653}]}, {"text": " Table 6: Scoring performance on ASAP with and  without spell checking", "labels": [], "entities": [{"text": "Scoring", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9678639769554138}, {"text": "ASAP", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.5640538930892944}, {"text": "spell checking", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.6583412140607834}]}, {"text": " Table 7: Scoring performance on ASAP when us- ing only answers with a certain minimal number  of errors for testing.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.541824221611023}]}]}