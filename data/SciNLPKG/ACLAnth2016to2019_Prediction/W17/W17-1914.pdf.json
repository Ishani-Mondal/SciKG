{"title": [{"text": "Word Sense Filtering Improves Embedding-Based Lexical Substitution", "labels": [], "entities": [{"text": "Word Sense Filtering Improves Embedding-Based Lexical Substitution", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8096379211970738}]}], "abstractContent": [{"text": "The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for sense.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.6325218876202902}, {"text": "lexical substitution", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7079013139009476}]}, {"text": "We show that a filtering mechanism based on a sense inventory optimized for substitutability can improve the results of these models.", "labels": [], "entities": []}, {"text": "Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution annotations in a development set.", "labels": [], "entities": []}, {"text": "The results show that lexical substitution can still benefit from senses which can improve the output of vector space paraphrase ranking models.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7717397809028625}]}], "introductionContent": [{"text": "Word sense has always been difficult to define and pin down.", "labels": [], "entities": [{"text": "Word sense", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.6156153976917267}]}, {"text": "Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further doubt on the usefulness of word sense.", "labels": [], "entities": []}, {"text": "Why bother to identify senses if even humans cannot agree upon their nature and number, and if simple word-embedding models yield good results without using any explicit sense representation?", "labels": [], "entities": []}, {"text": "Word-based models are successful in various semantic tasks even though they conflate multiple word meanings into a single representation.", "labels": [], "entities": []}, {"text": "Based on the hypothesis that capturing polysemy could further improve their performance, several works have focused on creating sense-specific word embeddings.", "labels": [], "entities": []}, {"text": "A common approach is to cluster the contexts in which the words appear in a corpus to induce senses, and relabel each word token with the clustered sense before learning embeddings (.", "labels": [], "entities": []}, {"text": "disambiguate the words in a corpus using a state-of-the-art WSD system and then produce continuous representations of word senses based on distributional information obtained from the annotated corpus.", "labels": [], "entities": []}, {"text": "Moving from word to sense embeddings generally improves their performance in word and relational similarity tasks but is not beneficial in all settings.", "labels": [], "entities": []}, {"text": "show that although multisense embeddings give improved performance in tasks such as semantic similarity, semantic relation identification and part-of-speech tagging, they fail to help in others, like sentiment analysis and named entity extraction (.", "labels": [], "entities": [{"text": "semantic relation identification", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.7205719550450643}, {"text": "part-of-speech tagging", "start_pos": 142, "end_pos": 164, "type": "TASK", "confidence": 0.7271995544433594}, {"text": "sentiment analysis", "start_pos": 200, "end_pos": 218, "type": "TASK", "confidence": 0.9457933604717255}, {"text": "named entity extraction", "start_pos": 223, "end_pos": 246, "type": "TASK", "confidence": 0.6137908895810446}]}, {"text": "We show how a sense inventory optimized for substitutability can improve the rankings provided by two sense-agnostic, vector-based lexical substitution models.", "labels": [], "entities": []}, {"text": "Lexical substitution requires systems to predict substitutes for target word instances that preserve their meaning in context.", "labels": [], "entities": [{"text": "Lexical substitution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9170435070991516}]}, {"text": "We consider a sense inventory with high substitutability to be one which groups synonyms or paraphrases that are mutually-interchangeable in the same contexts.", "labels": [], "entities": []}, {"text": "In contrast, sense inventories with low substitutability might group words linked by different types of relations.", "labels": [], "entities": []}, {"text": "We carryout experiments with a syntactic vector-space model) and a word-embedding model for lexical substitution.", "labels": [], "entities": []}, {"text": "Instead of using the senses to refine the vector representations as in, we use them to improve the lexical substitution rankings proposed by the models as a post-processing step.", "labels": [], "entities": []}, {"text": "Our results show that senses can improve the performance of vector-space models in lexical substitution tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run clustering experiments using targets and human-generated substitution data drawn from two lexical substitution datasets.", "labels": [], "entities": []}, {"text": "The first is the \"Concepts in Context\" (CoInCo) corpus), containing over 15K sentences corresponding to nearly 4K unique target words.", "labels": [], "entities": []}, {"text": "We divide the CoInCo dataset into development and test sets by first finding all target words that have at least 10 sentences.", "labels": [], "entities": [{"text": "CoInCo dataset", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.8962112367153168}]}, {"text": "For each of the 327 resulting targets, we randomly divide the corresponding sentences into 60% development instances and 40% test instances.", "labels": [], "entities": []}, {"text": "The resulting development and test sets have 4061 and 2091 sentences respectively.", "labels": [], "entities": []}, {"text": "We cluster the 327 target words in the resulting subset of CoInCo, performing all optimization using the development portion.", "labels": [], "entities": []}, {"text": "In order to evaluate how well our method generalizes to other data, we also create clusters for target words drawn from the SemEval 2007 English Lexical Substitution shared task dataset.", "labels": [], "entities": [{"text": "SemEval 2007 English Lexical Substitution shared task dataset", "start_pos": 124, "end_pos": 185, "type": "DATASET", "confidence": 0.6378808915615082}]}, {"text": "The entire test portion of the SemEval dataset contains 1700 annotated sentences for 170 target words.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.9046512842178345}]}, {"text": "We filter this data to keep only sentences with one or more human-annotated substitutes that overlap our PPDB XXL paraphrase vocabulary.", "labels": [], "entities": [{"text": "PPDB XXL paraphrase vocabulary", "start_pos": 105, "end_pos": 135, "type": "DATASET", "confidence": 0.7606711238622665}]}, {"text": "The resulting test set, which we use for evaluating SemEval targets, has 1178 sentences and 157 target words.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.9200162887573242}]}, {"text": "We cluster each of the 157 targets, using the CoInCo development data to optimize substitutability for the 32 SemEval targets that also appear in CoInCo.", "labels": [], "entities": [{"text": "CoInCo development data", "start_pos": 46, "end_pos": 69, "type": "DATASET", "confidence": 0.9020331501960754}]}, {"text": "For the rest of the SemEval targets we choose a number of senses equal to its WordNet synset count.", "labels": [], "entities": [{"text": "WordNet synset count", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.7804399728775024}]}], "tableCaptions": [{"text": " Table 2: Substitutablity (NMI) of resulting sense inventories, and GAP scores of the unfiltered and best sense-filtered rankings", "labels": [], "entities": [{"text": "Substitutablity (NMI)", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8093765527009964}, {"text": "GAP", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9733414649963379}]}, {"text": " Table 3: Substitutablity (NMI) of resulting sense inventories, and GAP scores of the unfiltered and best sense-filtered rankings", "labels": [], "entities": [{"text": "Substitutablity (NMI)", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8154405653476715}, {"text": "GAP", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9733127951622009}]}]}