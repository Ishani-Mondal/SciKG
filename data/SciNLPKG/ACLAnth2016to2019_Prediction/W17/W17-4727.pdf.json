{"title": [], "abstractContent": [{"text": "This article describes the Aalto University entry to the English-to-Finnish news translation shared task in WMT 2017.", "labels": [], "entities": [{"text": "English-to-Finnish news translation shared task in WMT 2017", "start_pos": 57, "end_pos": 116, "type": "TASK", "confidence": 0.7424299605190754}]}, {"text": "Our system is an open vocabulary neural machine translation (NMT) system, adapted to the needs of a morphologically complex target language.", "labels": [], "entities": [{"text": "open vocabulary neural machine translation (NMT)", "start_pos": 17, "end_pos": 65, "type": "TASK", "confidence": 0.7653350904583931}]}, {"text": "The main contributions of this paper are 1) implicitly incorporating morphological information to NMT through multi-task learning, 2) adding an attention mechanism to the character-level decoder, combined with character segmentation of names, and 3) anew overattending penalty to beam search.", "labels": [], "entities": [{"text": "character segmentation of names", "start_pos": 210, "end_pos": 241, "type": "TASK", "confidence": 0.8263568207621574}, {"text": "beam search", "start_pos": 280, "end_pos": 291, "type": "TASK", "confidence": 0.8477457165718079}]}], "introductionContent": [{"text": "The rich inflection, derivation and compounding in synthetic languages can result in very large vocabularies.", "labels": [], "entities": []}, {"text": "In statistical machine translation (SMT) large vocabularies cause sparsity issues.", "labels": [], "entities": [{"text": "statistical machine translation (SMT", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.8008189380168915}]}, {"text": "While continuous space representations make neural machine translation (NMT) more robust towards such sparsity, it suffers from a different set of problems related to large vocabularies.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.8381223678588867}]}, {"text": "A large vocabulary bloats memory and computation requirements, while still leaving the problem of out-ofvocabulary words unsolved.", "labels": [], "entities": []}, {"text": "Subword vocabularies have been proposed as a solution.", "labels": [], "entities": []}, {"text": "While the benefits of using subwords in SMT have been at best moderate (, subword decoding has become popular in NMT (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9810914993286133}]}, {"text": "A subword vocabulary of a moderate size ensures full coverage of an open vocabulary.", "labels": [], "entities": []}, {"text": "The downside is an increase in the length of the input and output sequences.", "labels": [], "entities": []}, {"text": "Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism.", "labels": [], "entities": []}, {"text": "An alternative approach is the hybrid wordcharacter decoder presented by.", "labels": [], "entities": []}, {"text": "In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a special <UNK> symbol.", "labels": [], "entities": []}, {"text": "A second character-level decoder then expands these <UNK> symbols into surface forms.", "labels": [], "entities": []}, {"text": "In addition to providing moderate length of input and output sequences together with an open vocabulary, the hybrid word-character decoder makes it simple to use labels based on the level of words, provided for example by morphological analyzers and parsers.", "labels": [], "entities": []}, {"text": "In SMT, such tools are typically used via factored translation models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9925550818443298}]}, {"text": "Factored translation has also been successfully applied in NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.7846047282218933}]}, {"text": "For example, augment the source words with four additional factors: PoS, lemma, dependency label and subwords.", "labels": [], "entities": [{"text": "PoS", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9353682398796082}]}, {"text": "use a decomposed generation process, in which they first output lemma, PoS, tense, person, gender, and number, from which the surface form is generated using a rule-based morphological analyzer.", "labels": [], "entities": []}, {"text": "Neural machine translation provides another way to utilize external annotations, multi-task learning (MTL).", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7460489670435587}, {"text": "multi-task learning (MTL)", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.6529415845870972}]}, {"text": "MTL is a well established machine learning approach that aims at improving the generalization performance of a task using other related tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5569761395454407}]}, {"text": "For example,  use autoencoding, parsing, and caption generation as auxiliary tasks to improve English-to-German translation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9756500720977783}, {"text": "caption generation", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8801310956478119}]}, {"text": "combine NMT with a Recurrent Neural Network Grammar.", "labels": [], "entities": []}, {"text": "The system learns to parse the target language as an auxiliary task when translating into English.", "labels": [], "entities": []}, {"text": "We propose an MTL approach inspired by fac-tored translation.", "labels": [], "entities": [{"text": "MTL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9479517340660095}, {"text": "fac-tored translation", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.5648955702781677}]}, {"text": "The output of a morphological analyzer for the target sentence is used as an auxiliary prediction target, while sharing network parameters to a larger extent than in the approach of . This approach has two advantages over factored models.", "labels": [], "entities": []}, {"text": "When training a system using factored output, embedded gold standard labels are given as input to the decoder.", "labels": [], "entities": []}, {"text": "During translation gold standard labels are not available, and predicted labels are instead fed back in.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9751401543617249}]}, {"text": "The confidence of the predictions is not accounted for when feeding back the labels.", "labels": [], "entities": [{"text": "confidence", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9658880233764648}]}, {"text": "This might worsen the problems caused by exposure bias, i.e., the mismatch between training and inference (.", "labels": [], "entities": []}, {"text": "If factored input is used, the external labeling tools need to be included also in the translation pipeline.", "labels": [], "entities": []}, {"text": "In MTL such tools are only necessary during training.", "labels": [], "entities": [{"text": "MTL", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9389392137527466}]}, {"text": "In terms of computational cost, a factored model needs to predict the auxiliary labels also during translation, slowing down inference and complicating the beam search.", "labels": [], "entities": []}, {"text": "A factored model might also need to use a larger beam to avoid hypotheses with the same surface form but different labels from crowding out more diverse hypotheses.", "labels": [], "entities": []}, {"text": "In MTL, the auxiliary tasks are only performed during training, and no changes need to be made to the inference.", "labels": [], "entities": [{"text": "MTL", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9402228593826294}]}, {"text": "The main contributions of this paper are combining word-level labels from morphological analysis with a hybrid word-character decoder, and adding an attention mechanism to the characterlevel decoder.", "labels": [], "entities": []}, {"text": "We also propose anew overattending penalty to the beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.8480914533138275}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of automatic evaluation. BLEU and chrF scores are percentages. TER from  http://matrix.statmt.org/matrix/systems_list/1871?metric_id=2 .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9991312623023987}, {"text": "TER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.998916506767273}]}, {"text": " Table 2: Results of ablation experiments. All runs  are ensembles of 4, to reduce variability.", "labels": [], "entities": []}]}