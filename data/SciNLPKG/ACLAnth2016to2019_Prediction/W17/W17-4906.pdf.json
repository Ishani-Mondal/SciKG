{"title": [{"text": "\"Deep\" Learning: Detecting Metaphoricity in Adjective-Noun Pairs *", "labels": [], "entities": []}], "abstractContent": [{"text": "Metaphor is one of the most studied and widespread figures of speech and an essential element of individual style.", "labels": [], "entities": []}, {"text": "In this paper we look at metaphor identification in Adjective-Noun pairs.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8866079151630402}]}, {"text": "We show that using a single neural network combined with pre-trained vector embeddings can outper-form the state of the art in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9973878264427185}]}, {"text": "In specific, the approach presented in this paper is based on two ideas: a) transfer learning via using pre-trained vectors representing adjective noun pairs, and b) a neural network as a model of composition that predicts a metaphoricity score as output.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.9117771089076996}]}, {"text": "We present several different architec-tures for our system and evaluate their performances.", "labels": [], "entities": []}, {"text": "Variations on dataset size and on the kinds of embeddings are also investigated.", "labels": [], "entities": []}, {"text": "We show considerable improvement over the previous approaches both in terms of accuracy and w.r.t the size of annotated training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9994825124740601}]}], "introductionContent": [{"text": "The importance of metaphor to characterize both individual and genre-related style has been underlined in several works (.", "labels": [], "entities": []}, {"text": "Studying the kinds of metaphors used in a text can contribute to differentiate between poetic and prosaic style, between different types of fiction, etc.", "labels": [], "entities": []}, {"text": "In literary studies, metaphor analysis is often undertaken on a stylistic perspective: \"after all, metaphor in literature is a stylistic device and its forms, meanings and use all fall within the remit of stylistics\" * This research is funded by the Centre of Linguistic Theory and Studies in Probability at the University of Gothenburg.).", "labels": [], "entities": [{"text": "metaphor analysis", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8504847586154938}]}, {"text": "Metaphor is thus often taken into consideration qualitative stylistic analyses).", "labels": [], "entities": []}, {"text": "Nonetheless, it is still very difficult to take metaphors into account in computational stylistics due to the complexity of automatic metaphor identification), which is the task of identifying metaphorical usages of text, sentences or subsentential fragments.", "labels": [], "entities": [{"text": "automatic metaphor identification", "start_pos": 124, "end_pos": 157, "type": "TASK", "confidence": 0.7140417098999023}, {"text": "identifying metaphorical usages of text, sentences or subsentential fragments", "start_pos": 181, "end_pos": 258, "type": "TASK", "confidence": 0.7187925934791565}]}, {"text": "This paper's focus of interest is the automatic detection of adjective-noun (AN) pairs like the following: (1) Clean floor / clean performance The above examples illustrate that adjectives \"normally\" used to describe physical characteristics, e.g. a feature that can be perceived through senses like size or weight, are reused to describe more abstract properties.", "labels": [], "entities": []}, {"text": "Thus, both a painting and an idea can be bright, both a table and a feeling can be heavy.", "labels": [], "entities": []}, {"text": "We will not provide a mean to retrieve AN metaphors in unconstrained texts (e.g. we won't focus on segmentation) but we will study ways to detect metaphoricity in given pairs.", "labels": [], "entities": []}, {"text": "Theoretical work on metaphor in the linguistics literature goes back along way and spans different theoretical paradigms.", "labels": [], "entities": []}, {"text": "One of the earliest and most influential works is Conceptual Metaphor Theory (CMT)) (originally published in 1981) and subsequently elaborated in a couple of papers.", "labels": [], "entities": [{"text": "Conceptual Metaphor Theory (CMT))", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.8082806368668874}]}, {"text": "According to CMT, metaphors in natural language can be seen as instances of conceptual metaphors.", "labels": [], "entities": []}, {"text": "A conceptual metaphor roughly corresponds to understanding a concept or an idea via association or relation with another idea or concept.", "labels": [], "entities": []}, {"text": "Other influential linguistic approaches to metaphor include pragmatic approaches cast within frameworks like relevance theory), and also approaches where some sort of formal semantics is used).", "labels": [], "entities": []}, {"text": "The common denominator in all these approaches is the recognition that there is systematicity in the way metaphorical meanings arise and also that the process of metaphor construction is extremely productive.", "labels": [], "entities": [{"text": "metaphor construction", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.7478409707546234}]}, {"text": "Thus, given these properties, one would expect metaphors to be quite common in Natural Language (NL).", "labels": [], "entities": []}, {"text": "Evidence from corpus linguistics seems to support this claim.", "labels": [], "entities": []}, {"text": "Metaphor detection in statistical NLP has been attempted through several different frames, such as topic modeling, semantic similarity graphs (, distributional clustering (, vector space based learning ( and, most of all, feature-based classifiers ().", "labels": [], "entities": [{"text": "Metaphor detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9138323068618774}, {"text": "topic modeling", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.7727900743484497}]}, {"text": "In the latter case, the challenge consists in selecting the right features to annotate the training data with, and to review their \"importance\" or weight based on machine learning results.", "labels": [], "entities": []}, {"text": "In this paper we show how using a singlelayered neural network combined with pre-trained distributional embeddings can outperform the state of the art in an AN metaphor detection task.", "labels": [], "entities": [{"text": "AN metaphor detection task", "start_pos": 157, "end_pos": 183, "type": "TASK", "confidence": 0.9136019945144653}]}, {"text": "More specifically, this paper's contributions are the following: \u2022 We introduce a system to predict AN metaphoricity and test it on the corpus introduced by, showing a significant improvement inaccuracy.", "labels": [], "entities": [{"text": "AN metaphoricity", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.6942083686590195}]}, {"text": "\u2022 We explore different variations of this model based on ideas found in the literature for composing distributional meaning and we evaluate them under different constraints.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: in Section 2 we present the background on AN metaphor detection and we detail the dataset we use to train our model.", "labels": [], "entities": [{"text": "AN metaphor detection", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.9437617063522339}]}, {"text": "In Section 3 we describe our approach, giving a general overview and further describing three alternative architectures on the same model.", "labels": [], "entities": []}, {"text": "In Section 4 we present several evaluations of our model.", "labels": [], "entities": []}, {"text": "synthesize some of our findings.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss our findings and possible future applications of the work described in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset we are using comes from ().", "labels": [], "entities": []}, {"text": "It contains 8592 annotated AN pairs, 3991 being literal and 4601 being metaphorical.", "labels": [], "entities": []}, {"text": "The dataset focuses on a set of 23 adjectives that: a) can potentially have both metaphorical and literal meanings, and b) are fairly productive.", "labels": [], "entities": []}, {"text": "The choice of adjectives was based on the test set of () and focuses on 23 adjectives.", "labels": [], "entities": []}, {"text": "In details, all adjectives belong to one of the following categories: 1.", "labels": [], "entities": []}, {"text": "temperature adjectives (e.g. cold) The dataset is publicly available here: http://bit.ly/1TQ5czN 2.", "labels": [], "entities": []}, {"text": "light adjectives (e.g. bright) 3.", "labels": [], "entities": []}, {"text": "texture adjectives (e.g. rough) 4.", "labels": [], "entities": []}, {"text": "substance adjectives (e.g. dense) 5.", "labels": [], "entities": []}, {"text": "clarity adjectives (e.g. clean) 6.", "labels": [], "entities": []}, {"text": "taste adjectives (e.g. bitter) 7.", "labels": [], "entities": []}, {"text": "strength adjectives (e.g. strong) 8.", "labels": [], "entities": []}, {"text": "depth adjectives (e.g. deep) The corpus was carefully builtin order to avoid non-ambiguous elements: all the AN phrases present in this dataset were extracted from large corpora and all phrases that seemed to require a larger context for their interpretation were filtered out in order to eliminate potentially ambiguous idiomatic expressions such as bright side.", "labels": [], "entities": []}, {"text": "In other terms, the corpus was designed to contain elements whose metaphoricity could be deduced by a human annotator without the need of a larger context.", "labels": [], "entities": []}, {"text": "More details about the construction of the dataset and annotation methodology can be found in ().", "labels": [], "entities": []}, {"text": "phrases with vectors, then based on this representation predicts a metaphoricity score as output.", "labels": [], "entities": []}, {"text": "Although we are going to present several variations of this framework, it's important to remember that the basic model is always a standard NN with a single fully connected hidden layer we will call p.", "labels": [], "entities": []}, {"text": "Our approach is thus based on the idea that welltrained distributional vectors contain more valuable information than their reciprocal similarity and, furthermore, that it is possible to treasure such information through machine learning in different tasks.", "labels": [], "entities": []}, {"text": "We use 300-dimensional word vectors trained on different corpora (see Evaluation for more details) . Our approach can be considered as away of transferring the learned representation from one task to another.", "labels": [], "entities": []}, {"text": "Although it is not possible to point out an explicit mapping between the word-vector learning task (e.g. Word2Vec model) and our metaphoricity task, as it is pointed out by Torrey and Shavlik 2009, we use neural networks which automatically learn how to adapt the feature representations between two tasks (.", "labels": [], "entities": []}, {"text": "In this way we stretch the original embeddings, trained in order to learn lexical similarity, to identify AN metaphors.", "labels": [], "entities": []}, {"text": "Our neural network, being a parameterized function, follows the generalized architecture of word-vector composition similar to: where u and v are two word vector representations to be composed, while p is the vector representation of their composition with the same dimensions.", "labels": [], "entities": []}, {"text": "The function fin our model is parameterized by \u03b8, a list of parameters to be learned as part of our neural network architecture.", "labels": [], "entities": []}, {"text": "Based on the argument by, parameters such as \u03b8 are encoded knowledge required by the compositional process.", "labels": [], "entities": []}, {"text": "In our case, the gradient based learning in neural networks will find these parameters as an optimization problem where p is just an intermediate representation in the pipeline of the neural network, which ends with a prediction of a metaphoricity score.", "labels": [], "entities": []}, {"text": "In other words, in order to predict the degree of metaphoricity, we end up learning a specific semantic space for phrase representations p and a vector q which actually does not represent a phrase itself, but rather the maximal possible level of metaphoricity given our training set.", "labels": [], "entities": []}, {"text": "The degree of metaphoricity of a phrase can thus be directly computed as cosine similarity between this vector and the phrase vector.", "labels": [], "entities": []}, {"text": "However, in the network we used a sigmoid function to produce the measure: where q and b 1 are parameters of the final layer and work as metaphoricity indicators, whil\u00eawhil\u00ea y is the predicted score (metaphoric or literal) for the composition p.", "labels": [], "entities": []}, {"text": "Given a dataset of D = {(x t , y t )} t\u2208{1,...,T } , the composition p can be formalized as a model for Bernoulli distribution: where each x t is an AN pair in the training dataset labeled with a binary value y t (0 or 1).", "labels": [], "entities": []}, {"text": "Given the labels in D, we interpret y t as a categorical probability score: the probability of a given phrase being metaphorical.", "labels": [], "entities": []}, {"text": "In this paper, we describe three alternative architectures to implement this framework.", "labels": [], "entities": []}, {"text": "All three, with small variations, show a robust ability to generalize on the dataset and perform correct predictions.", "labels": [], "entities": []}, {"text": "Our classifier achieved 91.5% accuracy trained on 500 labeled AN-phrases out of 8592 in the corpus and tested on the rest.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9916403293609619}]}, {"text": "Training on 8000 and testing on the rest gave us accuracy of 98.5%.", "labels": [], "entities": [{"text": "8000", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8330812454223633}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9998495578765869}]}, {"text": "We tested several combinations of the architectures we described in the paper.", "labels": [], "entities": []}, {"text": "For each of the three architectures, we also tested the Rectified linear unit (ReLU) as the non-linearity mentioned in Section 3.5.", "labels": [], "entities": [{"text": "Rectified linear unit (ReLU)", "start_pos": 56, "end_pos": 84, "type": "METRIC", "confidence": 0.9084547460079193}]}, {"text": "Our test also shows that a random constant matrix Wis enough to train the rest of the parameters (reported in).", "labels": [], "entities": []}, {"text": "In general, the best performing combinations involve the use of concatenation (the first architecture), while multiplication led to the lowest results.", "labels": [], "entities": []}, {"text": "In any case, all experiments returned accuracies above 75% . To test the robustness of our approach, we have evaluated our model's performance under several constraints: \u2022 Total separation of vocabulary in train and test sets () in case of out of vocabulary words.", "labels": [], "entities": []}, {"text": "\u2022 Use of different pretrained word embeddings).", "labels": [], "entities": []}, {"text": "\u2022 Qualitative selection of the training data based on the semantic categories of adjectives ().", "labels": [], "entities": []}, {"text": "Finally, we will provide some qualitative insights on how the model works.", "labels": [], "entities": []}, {"text": "Our model is based on the idea of transfer learning: using the learned representation fora new task, in this case the metaphor detection.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7291856706142426}]}, {"text": "Our model should generalize very fast with a small set of samples as training data.", "labels": [], "entities": []}, {"text": "In order to test this matter, we have to train and test on totally different samples so vocabulary doesn't overlap.", "labels": [], "entities": []}, {"text": "The splitting of the 8592 labeled phrases based on vocabulary gives us uneven sizes of training and test phrases . In using the pretrained These results are based on the first architecture, the performance of other architectures are not very different in this simple test.", "labels": [], "entities": []}, {"text": "The sample code is available on https://guclasp.github.io/anvec-metaphor/ The number of parameters in case of using concatenation (as in first architecture) is 180 601 and other compositions, including addition and multiplication, number of parameters is almost the half: 90 601.", "labels": [], "entities": []}, {"text": "We chose the vocabulary splitting points for every 10% from 10% to 90%, then we applied the splitting separately on nouns and adjective Word2Vec embeddings trained on Google News () we examined the accuracy, precision and recall of the our trained classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9991703033447266}, {"text": "precision", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.9980377554893494}, {"text": "recall", "start_pos": 222, "end_pos": 228, "type": "METRIC", "confidence": 0.9983358979225159}]}, {"text": "We have used three different word embeddings: Word2Vec embeddings trained on Google News (, GloVe embeddings) and Levy-Goldberg embeddings ().", "labels": [], "entities": []}, {"text": "These embeddings are not up-dated during the training process.", "labels": [], "entities": []}, {"text": "Thus, the classification task is always performed by learning weights for the preexisting vectors.", "labels": [], "entities": [{"text": "classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9618232846260071}]}, {"text": "The results of our experiment can be seen in.", "labels": [], "entities": []}, {"text": "All these embeddings have returned similar accuracies both when trained on scarce data (100 phrases) and when trained on half of the dataset (4000 phrases).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9792130589485168}]}, {"text": "Training on 100 phrases indicates the ability of our model to learn from scarce data.", "labels": [], "entities": []}, {"text": "One way of checking the consistency of our model under data scarcity is to perform flipped cross-validation: this is a cross-validation where, instead of training our model on 90% of the data and testing it on the remaining 10%, we flipped the sizes train it on 10% of the data and test it on the remaining 90%.", "labels": [], "entities": []}, {"text": "Results for both classic cross-validation and flipped cross-validation can be seen in.", "labels": [], "entities": []}, {"text": "Training on 10% of the data proved to consistently achieve accuracies not much lower than 90%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9990621209144592}]}, {"text": "In other terms, a model trained on 90% of the data does not do much better than a model trained on 10%.", "labels": [], "entities": []}, {"text": "Finally, we tried training our model on only one of the semantic categories we introduced at the beginning of the paper and testing it on the rest of the dataset.", "labels": [], "entities": []}, {"text": "Results can be seen in.", "labels": [], "entities": []}, {"text": "We can wonder \"why\" our system is working: with respect to more traditional machine learning approaches, there is no direct way to evaluate which features mostly contribute to the success of our system.", "labels": [], "entities": []}, {"text": "One way to have an idea of what is happening in the model is to use the \"metaphoricity vector\" we discussed in Section 3.", "labels": [], "entities": []}, {"text": "Such vector represents what is learned by our model and can help making it less opaque for us.", "labels": [], "entities": []}, {"text": "If we compute the cosine similarity between all the nouns in our dataset and this learned vector, we can see that nouns tend to polarize on an abstract/concrete axis: abstract nouns tend to be more similar to the learned vector than concrete nouns.", "labels": [], "entities": []}, {"text": "It is likely that our model is learning nouns'  level of abstractness as a mean to determine phrase metaphoricity.", "labels": [], "entities": [{"text": "phrase metaphoricity", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.7367844581604004}]}, {"text": "In we show the 10 most similar and the 10 least similar nouns obtained with this approach.", "labels": [], "entities": []}, {"text": "As can be seen, a concreteabstract polarity is apparently learned in training.", "labels": [], "entities": []}, {"text": "This factor was amply noted and even used in some feature-based metaphor classifiers, as we discussed in the beginning: the advantage of using continuous semantic spaces probably relies on the possibility of having a more nuanced and complex polarization of nouns along the concrete/abstract axes than using hand-annotated resources. and testing on all other categories still yields high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 388, "end_pos": 396, "type": "METRIC", "confidence": 0.9976884126663208}]}, {"text": "While the power of generalization of our model is still unclear, we can see that it can detect similar semantic mechanisms even without any vocabulary overlap.", "labels": [], "entities": []}, {"text": "The category taste is a partial exception: this category seems to be a relative \"outlier\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The reported accuracy from previous words on AN metaphor detection. The first two studies  used different datasets. We are using larger pre-trained vectors than", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9988872408866882}, {"text": "AN metaphor detection", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.9499111175537109}]}, {"text": " Table 2: The accuracy results after training the  model based on each architecture. In all setups,  we trained on 500 samples in 20 epochs. Using  a random W is equivalent to preventing our net- work from learning any form of compositionality  (we could consider it as a baseline for models with  trained W). As we discuss in the paper, the differ- ence in accuracies with the \"baseline\" (not training  W) shows that training W is helpful.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995679259300232}]}, {"text": " Table 3: This table shows consistent results in ac- curacy, precision and recall of the classifier trained  with different split points of vocabulary instead of  phrases. Splitting the vocabulary creates different  sizes of training phrases and test phrases.", "labels": [], "entities": [{"text": "ac- curacy", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9599458972613016}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.999006450176239}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9990967512130737}]}]}