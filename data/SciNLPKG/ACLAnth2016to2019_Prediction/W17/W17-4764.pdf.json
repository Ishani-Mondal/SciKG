{"title": [{"text": "Unbabel's Participation in the WMT17 Translation Quality Estimation Shared Task", "labels": [], "entities": [{"text": "WMT17 Translation Quality Estimation Shared", "start_pos": 31, "end_pos": 74, "type": "TASK", "confidence": 0.7293861985206604}]}], "abstractContent": [{"text": "This paper presents the contribution of the Unbabel team to the WMT 2017 Shared Task on Translation Quality Estimation.", "labels": [], "entities": [{"text": "WMT 2017 Shared Task on Translation Quality Estimation", "start_pos": 64, "end_pos": 118, "type": "TASK", "confidence": 0.5973662957549095}]}, {"text": "We participated on the word-level and sentence-level tracks.", "labels": [], "entities": []}, {"text": "We describe our two submitted systems: (i) STACKEDQE, a \"pure\" QE system, trained only on the provided training sets, which is a stacked combination of a feature-rich sequential linear model with a neural network , and (ii) FULLSTACKEDQE, which also stacks the predictions of an automatic post-editing system, trained on additional data.", "labels": [], "entities": [{"text": "FULLSTACKEDQE", "start_pos": 224, "end_pos": 237, "type": "METRIC", "confidence": 0.9955053329467773}]}, {"text": "When evaluated on the English-German and German-English datasets, FULLSTACKEDQE achieved word-level F MULT 1 scores of 56.6% and 52.9%, and sentence-level correlation Pearson scores of 64.1% and 62.6%, respectively.", "labels": [], "entities": [{"text": "German-English datasets", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.674200564622879}, {"text": "FULLSTACKEDQE", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.9524605870246887}, {"text": "F MULT 1 scores", "start_pos": 100, "end_pos": 115, "type": "METRIC", "confidence": 0.8474503457546234}, {"text": "sentence-level correlation Pearson scores", "start_pos": 140, "end_pos": 181, "type": "METRIC", "confidence": 0.7316506952047348}]}, {"text": "Our system ranked second in both tracks, being statistically indistinguishable from the best system in the word-level track.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation is the task of evaluating a translation system's quality without access to reference translations (.", "labels": [], "entities": [{"text": "Quality estimation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6563927531242371}]}, {"text": "This paper describes the contribution of the Unbabel team to the Shared Task on Sentence-Level and Word-Level Quality Estimation (QE Tasks 1 and 2) at the 2017 Conference on Statistical Machine Translation).", "labels": [], "entities": [{"text": "Word-Level Quality Estimation (QE Tasks 1 and 2) at the 2017 Conference on Statistical Machine Translation", "start_pos": 99, "end_pos": 205, "type": "TASK", "confidence": 0.6278302503956689}]}, {"text": "In the word-level task, the goal is to predict the word-level quality of machine translated text, by assigning a label of OK or BAD to each word in the translation.", "labels": [], "entities": [{"text": "OK", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9714466333389282}, {"text": "BAD", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.8024232387542725}]}, {"text": "The sentence-level task attempts to predict the HTER of each sentence, along with a ranking of the sentences.", "labels": [], "entities": [{"text": "HTER", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9566595554351807}]}, {"text": "Two language pairs and domains are considered: English-German (IT domain) and German-English (medical domain).", "labels": [], "entities": []}, {"text": "Our submission is largely based on the approach that we have recently proposed in, which ensembles a \"pure\" quality estimation system with predictions derived from an automatic post-editing system.", "labels": [], "entities": []}, {"text": "The focus was on developing a word-level system, and to use the word label predictions to predict the sentencelevel HTER.", "labels": [], "entities": []}, {"text": "Our system architecture is described in full detail in the following sections.", "labels": [], "entities": []}, {"text": "We first describe our \"pure\" QE system ( \u00a72), which consists of a neural model (NEURALQE) stacked into a linear feature-rich classifier (LINEARQE).", "labels": [], "entities": []}, {"text": "Then, we train an APE system (using a large amount of artificial \"roundtrip translations\") and adapt it to predict word-level quality labels (yielding APEQE, \u00a73).", "labels": [], "entities": [{"text": "APEQE", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.5012664198875427}]}, {"text": "We show that the pure and the APE-based QE system are highly complementary ( \u00a74): our best system is a stacked combination of LINEARQE, NEURALQE, and APEQE.", "labels": [], "entities": [{"text": "APE-based QE", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.4630899131298065}, {"text": "LINEARQE", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9698511362075806}, {"text": "NEURALQE", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.7094275951385498}, {"text": "APEQE", "start_pos": 150, "end_pos": 155, "type": "DATASET", "confidence": 0.5858598947525024}]}, {"text": "By employing a simple word-to-sentence conversion, we adapt our systems to sentence-level QE.", "labels": [], "entities": [{"text": "word-to-sentence conversion", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.7074577957391739}]}, {"text": "Overall, we achieve word-level F MULT 1 scores of 56.6% and 52.9% and sentence-level Pearson scores of 64.1% and 62.6% for English-German and German-English, respectively.", "labels": [], "entities": [{"text": "F MULT 1 scores", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.844389021396637}, {"text": "Pearson scores", "start_pos": 85, "end_pos": 99, "type": "METRIC", "confidence": 0.9304277300834656}]}, {"text": "The following external resources were used: part-of-speech tags and extra syntactic dependency information were obtained with TurboTagger and TurboParser), 1 trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 173, "end_pos": 186, "type": "DATASET", "confidence": 0.9953982830047607}, {"text": "German TIGER corpus", "start_pos": 227, "end_pos": 246, "type": "DATASET", "confidence": 0.6732456286748251}, {"text": "SPMRL shared task", "start_pos": 259, "end_pos": 276, "type": "TASK", "confidence": 0.7916681369145712}]}, {"text": "For the neural models, we used pre-trained word embeddings from Polyglot (Al-).", "labels": [], "entities": []}, {"text": "For our FULLSTACKEDQE submission, we also use additional data to train the APE-based QE systems: for English-German, the set of 500K artificial roundtrip translations provided by, and, for German-English, the UFAL Medical Corpus provided in the WMT17 Biomedical Translation task.", "labels": [], "entities": [{"text": "APE-based QE", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.5854880809783936}, {"text": "UFAL Medical Corpus", "start_pos": 209, "end_pos": 228, "type": "DATASET", "confidence": 0.9193280140558878}, {"text": "WMT17 Biomedical Translation task", "start_pos": 245, "end_pos": 278, "type": "TASK", "confidence": 0.8289398699998856}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of the several word-level  QE systems on the development and WMT16  English-German test datasets.", "labels": [], "entities": [{"text": "WMT16  English-German test datasets", "start_pos": 83, "end_pos": 118, "type": "DATASET", "confidence": 0.8888465911149979}]}, {"text": " Table 3: Performance of the several word-level  QE systems on the German-English development  dataset.", "labels": [], "entities": [{"text": "German-English development  dataset", "start_pos": 67, "end_pos": 102, "type": "DATASET", "confidence": 0.8512115478515625}]}, {"text": " Table 4: Performance of our sentence-level QE systems on the English-German and German-English  datasets, as measured by the official evaluation script. We show the performance of Martins et al. (2017)  for comparison.", "labels": [], "entities": [{"text": "German-English  datasets", "start_pos": 81, "end_pos": 105, "type": "DATASET", "confidence": 0.6994727551937103}]}, {"text": " Table 5: Performance of the submitted word-level  systems on the test set.", "labels": [], "entities": []}, {"text": " Table 6: Performance of the submitted sentence- level systems on the test set.", "labels": [], "entities": []}]}