{"title": [{"text": "Using Pseudowords for Algorithm Comparison: An Evaluation Framework for Graph-based Word Sense Induction", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.6165853639443716}]}], "abstractContent": [{"text": "In this paper we define two parallel data sets based on pseudowords, extracted from the same corpus.", "labels": [], "entities": []}, {"text": "They both consist of word-centered graphs for each of 1225 different pseudowords, and use respectively first-order co-occurrences and second-order semantic similarities.", "labels": [], "entities": []}, {"text": "We propose an evaluation framework on these data sets for graph-based Word Sense Induction (WSI) focused on the case of coarse-grained homonymy: We compare different WSI clustering algorithms by measuring how well their outputs agree with the a priori known ground-truth decomposition of a pseudoword.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7508718619743983}]}, {"text": "We perform this evaluation for four different clustering algorithms: the Markov cluster algorithm, Chi-nese Whispers, MaxMax and a gangplank-based clustering algorithm.", "labels": [], "entities": []}, {"text": "To further improve the comparison between these algorithms and the analysis of their behaviours, we also define anew specific evaluation measure.", "labels": [], "entities": []}, {"text": "As far as we know, this is the first large-scale systematic pseudoword evaluation dedicated to the induction of coarse-grained homonymous word senses.", "labels": [], "entities": [{"text": "induction of coarse-grained homonymous word senses", "start_pos": 99, "end_pos": 149, "type": "TASK", "confidence": 0.6902655164400736}]}], "introductionContent": [], "datasetContent": [{"text": "The method of pseudoword evaluation was first independently proposed in ( and.", "labels": [], "entities": [{"text": "pseudoword evaluation", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.8726879358291626}]}, {"text": "Given two words appearing in a corpus, e.g. cat and window, we replace all their occurrences therein with an artificial term formed by their combination (represented in our example as cat window), a so-called pseudoword that merges the contexts of its components (also called pseudosenses).", "labels": [], "entities": []}, {"text": "The original application of this evaluation assumes that all the components of a pseudoword are monosemous words, i.e. possess only one sense.", "labels": [], "entities": []}, {"text": "Ideally, an algorithm trying to induce the senses of a monosemous word from the corresponding word graph should return only one cluster, and we would expect it to find exactly two clusters in the case of a pseudoword with two components.", "labels": [], "entities": []}, {"text": "This makes evaluation more transparent, and we are restricting ourselves to monosemous words for this reason.", "labels": [], "entities": []}, {"text": "For the purpose of our evaluation, we extract monosemous nouns from the 105 million sentences of the corpus described in Section 2, over which we compute all SSIM-and cooccurrence-based distributional thesauri.", "labels": [], "entities": []}, {"text": "We divide all the nouns into 5 logarithmic frequency classes identified with respect to the frequency of the most common noun in the corpus.", "labels": [], "entities": []}, {"text": "For each class, we extract random candidates: We retain only those that possess one single meaning, i.e. for which Chinese Whispers (see Section 4.2) 6 yields one single cluster, additionally checking that they have only one synset in WordNet (which is commonly accepted to be fine-grained).", "labels": [], "entities": [{"text": "Chinese Whispers", "start_pos": 115, "end_pos": 131, "type": "DATASET", "confidence": 0.8898607492446899}, {"text": "WordNet", "start_pos": 235, "end_pos": 242, "type": "DATASET", "confidence": 0.9523075819015503}]}, {"text": "We repeat this process until we obtain 10 suitable candidates per frequency class.", "labels": [], "entities": []}, {"text": "In the end, we obtain a total of 50 words whose combinations give rise to 1225 different pseudowords.", "labels": [], "entities": []}, {"text": "We then proceed to create two kinds of pseudoword ego word graph data sets, as described in Section 2: one for co-occurrences and one for semantic similarities.", "labels": [], "entities": []}, {"text": "In both cases we limit the graphs to the topmost 500 terms, ranked by LMI.", "labels": [], "entities": [{"text": "LMI", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9389498233795166}]}, {"text": "The evaluation consists in running the clustering algorithms on the ego word graphs: since we know the underlying (pseudo)senses of each pseudoword AB, we also know for each node in its ego word graph if it belongs to the distributional thesaurus, and thus to the subgraph relative to A, B or both, and thus we already know our ground truth clustering T = (T A , TB ).", "labels": [], "entities": []}, {"text": "Clearly, the proportion between TA and TB might be very skewed, especially if A and B belong to very different frequency classes.", "labels": [], "entities": [{"text": "TA", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9778925776481628}, {"text": "TB", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.7604877948760986}]}, {"text": "Despite the criticism of the pseudoword evaluation for being too artificial and its senses not obeying the true sense distribution of a proper polysemic word, we note that this is a very realistic situation for homonymy, since sense distributions tend to be skewed and dominated by a most frequent sense (MFS).", "labels": [], "entities": []}, {"text": "In coarse-grained Word Sense Disambiguation evaluations, the MFS baseline is often in the range of 70% -80% (.", "labels": [], "entities": [{"text": "Word Sense Disambiguation evaluations", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.6997950971126556}]}, {"text": "Our starting assumption for very skewed cases is that a clustering algorithm will be biased towards the more frequent term of the two, that is, it will tendentially erroneously find only one cluster.", "labels": [], "entities": []}, {"text": "It could also be possible that all nodes relative to A at the same time also appear in the distributional thesaurus of B, so that the word A is overshadowed by B. We call this a collapsed pseudoword.", "labels": [], "entities": []}, {"text": "We decided not to take collapsed pseudowords into account for evaluation, since in this case the initial purpose of simulating a polysemous does not hold: we are left with an actually monosemous pseudoword.", "labels": [], "entities": []}, {"text": "We measure the quality of the clustering of a pseudoword ego graph in terms of the F-score of the BCubed metric (, alongside with normalized mutual information 7 (NMI)) and a measure developed by us, TOP2, loosely inspired by NMI.", "labels": [], "entities": [{"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9928576946258545}, {"text": "TOP2", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.5013564825057983}]}, {"text": "We define TOP2 as the average of the harmonic means of homogeneity and completeness of the two clusters that better represent the two components of the pseudoword.", "labels": [], "entities": [{"text": "TOP2", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9151090383529663}]}, {"text": "More formally, suppose that the pseudoword AB is the combination of the words A and B.", "labels": [], "entities": [{"text": "AB", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.8804997205734253}]}, {"text": "Consequently, we take TA = \u03b1, TB = \u03b2 as the ground truth clusters of V \\(\u03b3 \u222a \u03b4), which we will com- In this case one word is totally dominant over the other, and the pseudoword actually collapses onto one sense.", "labels": [], "entities": [{"text": "TA", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9438913464546204}]}, {"text": "As already mentioned, we decided to exclude collapsed pseudowords from evaluation.", "labels": [], "entities": []}, {"text": "To compute the BCubed F-score and NMI, we compare the ground truth clustering T = {\u03b1, \u03b2} to the clustering C \\(\u03b3 \u222a \u03b4) that we obtain from any algorithm under consideration.", "labels": [], "entities": [{"text": "BCubed F-score", "start_pos": 15, "end_pos": 29, "type": "METRIC", "confidence": 0.5663084983825684}, {"text": "NMI", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.662386417388916}]}, {"text": "However, for the TOP2 score we want to look only at the two clusters CA and CB that better represent component A and B respectively.", "labels": [], "entities": [{"text": "TOP2", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.7829923629760742}]}, {"text": "If the clustering consists of only one cluster, we define either CA = / 0 or CB = / 0 and put the harmonic mean of its purity and completeness equal to 0.", "labels": [], "entities": [{"text": "CA", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9813423752784729}, {"text": "CB", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.8899428248405457}, {"text": "purity", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.985723078250885}]}, {"text": "Therefore, in such case the TOP2 will never be greater than 1 2 . The motivation for the TOP2 score is that we know what we are looking for: namely, for two clusters that represent A and B.", "labels": [], "entities": [{"text": "TOP2", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9814436435699463}, {"text": "TOP2 score", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.8368013799190521}]}, {"text": "The TOP2 score then gives us a measure of how well the clustering algorithm succeeds in correctly concentrating all the information in exactly two clusters with the least dispersion; this can be generalized to the case of more than two pseudosenses.", "labels": [], "entities": [{"text": "TOP2 score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8942238390445709}]}], "tableCaptions": [{"text": " Table 1: Mean scores in percentages over all pseudowords for each clustering algorithm and the baseline,  for our three metrics and for both data sets. The 95% confidence interval is also reported for each mean  value. The best values on each data set and for each measure are boldfaced.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9859222173690796}]}, {"text": " Table 2: Mean scores per frequency class combination over both SSIM-based and the co-occurrence- based ego word graph data sets. The best values for each frequency class combination are highlighted.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9842538833618164}, {"text": "ego word graph data sets", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.741068959236145}]}]}