{"title": [{"text": "Improving Low-Resource Neural Machine Translation with Filtered Pseudo-parallel Corpus", "labels": [], "entities": [{"text": "Improving Low-Resource Neural Machine Translation", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7356895446777344}]}], "abstractContent": [{"text": "Large-scale parallel corpora are indispensable to train highly accurate machine translators.", "labels": [], "entities": [{"text": "machine translators", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.6932962536811829}]}, {"text": "However, manually constructed large-scale parallel corpora are not freely available in many language pairs.", "labels": [], "entities": []}, {"text": "In previous studies, training data have been expanded using a pseudo-parallel corpus obtained using machine translation of the monolingual corpus in the target language.", "labels": [], "entities": []}, {"text": "However, in low-resource language pairs in which only low-accuracy machine translation systems can be used, translation quality is reduces when a pseudo-parallel corpus is used naively.", "labels": [], "entities": [{"text": "translation", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.9561344981193542}]}, {"text": "To improve machine translation performance with low-resource language pairs, we propose a method to expand the training data effectively via filtering the pseudo-parallel corpus using a quality estimation based on back-translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8156915903091431}]}, {"text": "As a result of experiments with three language pairs using small, medium, and large parallel corpora, language pairs with fewer training data filtered out more sentence pairs and improved BLEU scores more significantly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9982274174690247}]}], "introductionContent": [{"text": "A large-scale parallel corpus is an essential resource for training statistical machine translation (SMT) and neural machine translation (NMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) and neural machine translation (NMT)", "start_pos": 68, "end_pos": 142, "type": "TASK", "confidence": 0.800347761465953}]}, {"text": "Creating a high-quality large-scale parallel corpus requires time, money and professionals to translate a large amount of texts.", "labels": [], "entities": []}, {"text": "As a result, many of the existing large-scale parallel corpora are limited to specific languages and domains.", "labels": [], "entities": []}, {"text": "In contrast, large monolingual corpora are easier to obtain.", "labels": [], "entities": []}, {"text": "Various approaches have been proposed to create a pseudo-parallel corpus from a monolingual corpus.", "labels": [], "entities": []}, {"text": "For example, proposed a method to generate a pseudo-parallel corpus based on a monolingual corpus of the source language and its automatic translation.", "labels": [], "entities": []}, {"text": "obtained substantial improvements by automatically translating a monolingual corpus of the target language into the source language, which they refer to as synthetic, and treating the obtained pseudo-parallel corpus as additional training data.", "labels": [], "entities": []}, {"text": "They used monolingual data of the target language to learn the language model more effectively.", "labels": [], "entities": []}, {"text": "However, they experimented on language pairs where relatively large-scale parallel corpora are available.", "labels": [], "entities": []}, {"text": "Thus, they did not need to fully exploit the training corpus nor care about the quality of the pseudo-parallel corpus.", "labels": [], "entities": []}, {"text": "Therefore, we propose a method to create a pseudo-parallel corpus by back-translating and filtering a monolingual corpus in the target language for low-resource language pairs.", "labels": [], "entities": []}, {"text": "If the target sentence and its back-translation are similar, we assume that the synthetic source sentence is appropriate regarding its monolingual target sentence and can be included into the filtered pseudoparallel corpus.", "labels": [], "entities": []}, {"text": "The quality of the pseudo-parallel corpus is especially important because low-quality parallel sentences will degrade NMT performance more than SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.9716149568557739}]}, {"text": "Our motivation is to filter out lowquality synthetic sentences that might be included in such a pseudo-parallel corpus to obtain a highquality pseudo-parallel corpus for low-resource language pairs.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first attempt to (1) filter a pseudo-parallel corpus using back-translation and (2) bootstrap NMT.", "labels": [], "entities": []}, {"text": "The main contributions of our research are as follows: \u2022 We filter a pseudo-parallel corpus using sentence-level similarity metric, in our case sentence-level BLEU (, and obtain a trainable high-quality pseudo-parallel corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9480010271072388}]}, {"text": "\u2022 We show that the proposed filtering method is useful for low-resource language pairs, although bootstrapping does not outperform the proposed filtering method significantly.", "labels": [], "entities": []}, {"text": "\u2022 We will release the obtained filtered pseudoparallel corpora . In this study, we used Japanese\u2194Russian as low-resource language pairs, French\u2192Malagasy as medium-resource language pairs and German\u2192English as high-resource language pairs.", "labels": [], "entities": []}, {"text": "We show that a previous state-of-the-art method) is effective for high-resource language pairs; however, in the case of low-resource language pairs, it is more effective to use a filtered pseudo-parallel corpus as additional training data.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 discusses previous studies related to improving low-resource machine translation systems; Section 3 outlines the proposed method for filtering a pseudo-parallel corpus and bootstrapping NMT; Sections 4 and 5 evaluate the proposed model; and Section 6 discusses the results.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.6734752655029297}]}, {"text": "Conclusions and suggestions for future work are presented in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Pseudo-parallel Corpus  The parallel corpora for low-resource Russian\u2194Japanese and for medium-resource French\u2192Malagasy 5 experiments were downloaded from OPUS.", "labels": [], "entities": [{"text": "OPUS", "start_pos": 154, "end_pos": 158, "type": "DATASET", "confidence": 0.950849711894989}]}, {"text": "For the medium-resource French-Malagasy language pair, we used the GlobalVoices corpus, which differs from the Tatoeba corpus used in the previous experiments.", "labels": [], "entities": [{"text": "GlobalVoices corpus", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.9671395123004913}, {"text": "Tatoeba corpus", "start_pos": 111, "end_pos": 125, "type": "DATASET", "confidence": 0.8554948568344116}]}, {"text": "Note that the GlobalVoices corpus has more available parallel data sentence pairs compared to 10,231).", "labels": [], "entities": [{"text": "GlobalVoices corpus", "start_pos": 14, "end_pos": 33, "type": "DATASET", "confidence": 0.9739761352539062}]}, {"text": "We split the Tatoeba parallel corpus for the: Russian\u2192Japanese translation BLEU scores.", "labels": [], "entities": [{"text": "Tatoeba parallel corpus", "start_pos": 13, "end_pos": 36, "type": "DATASET", "confidence": 0.8951480786005656}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9938439130783081}]}, {"text": "Sorting was performed using sent-BLEU score (Bootstrap 1).", "labels": [], "entities": [{"text": "Sorting", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.881131112575531}, {"text": "sent-BLEU score", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.8897386491298676}, {"text": "Bootstrap 1", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.8561429977416992}]}, {"text": "There is a significant difference: \u22c6: against \"Parallel\" baseline; \u2020: against \"Unfiltered\" baseline.", "labels": [], "entities": []}, {"text": "Russian\u2194Japanese experiments as follows: training set, 10,231 sentences; development set, 500 sentences; and test set, 500 sentences.", "labels": [], "entities": []}, {"text": "In addition, to perform Japanese\u2192Russian\u2192Japanese translation for the Russian to Japanese experiment, we sampled an additional 167,600 Japanese monolingual sentences from Tatoeba.", "labels": [], "entities": [{"text": "Japanese\u2192Russian\u2192Japanese translation", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.6565162390470505}]}, {"text": "We also sampled 75,401 Russian monolingual sentences from Tatoeba for Japanese\u2192Russian translation to facilitate Russian\u2192Japanese\u2192Russian translation.", "labels": [], "entities": [{"text": "Japanese\u2192Russian translation", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.5882757753133774}, {"text": "Russian\u2192Japanese\u2192Russian translation", "start_pos": 113, "end_pos": 149, "type": "TASK", "confidence": 0.6484652608633041}]}, {"text": "We performed experiments for the language pair French\u2192Malagasy language pairs using the data from the GlobalVoices corpus.", "labels": [], "entities": [{"text": "GlobalVoices corpus", "start_pos": 102, "end_pos": 121, "type": "DATASET", "confidence": 0.9750991463661194}]}, {"text": "Parallel data were split as follows: training set, 106,406 sentences; development set, 1,000 sentences; and test set, 1,000 sentences.", "labels": [], "entities": []}, {"text": "Note that 105,570 Malagasy monolingual sentences from GlobalVoices were used to create a French\u2192Malagasy pseudoparallel corpus.", "labels": [], "entities": [{"text": "GlobalVoices", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.8884325623512268}, {"text": "French\u2192Malagasy pseudoparallel corpus", "start_pos": 89, "end_pos": 126, "type": "DATASET", "confidence": 0.6335412442684174}]}, {"text": "For the German\u2192English experiments, we downloaded pre-trained German\u2194English models and 4,535,522 parallel sentences provided by OpenNMT and used the OpenNMT settings to preprocess all data.", "labels": [], "entities": []}, {"text": "We downloaded 4,208,439 German\u2192English sentences from automatically back-translated monolingual data and translated the synthetic German side back to English using  the pre-trained German\u2192English model to filter this pseudo-parallel corpus.", "labels": [], "entities": []}, {"text": "We used newtest2013 (3,000 sentence pairs) as a development set and newtest2014 (3,003 sentence pairs) as a test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Russian\u2192Japanese translation BLEU  scores. Sorting was performed using sent-LM  score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9939588308334351}, {"text": "Sorting", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9493190050125122}]}, {"text": " Table 3: Russian\u2192Japanese translation BLEU  scores. Sorting was performed using sent-BLEU  score (Bootstrap 1). There is a significant differ- ence: \u22c6: against \"Parallel\" baseline;  \u2020: against  \"Unfiltered\" baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9932801127433777}, {"text": "differ- ence", "start_pos": 136, "end_pos": 148, "type": "METRIC", "confidence": 0.961337149143219}]}, {"text": " Table 4: Russian\u2192Japanese translation BLEU  scores. Sorting was performed using sent-BLEU  score (Bootstrap 2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9905361533164978}, {"text": "Sorting", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9580594897270203}]}, {"text": " Table 5: Russian\u2192Japanese translation BLEU  scores. Sorting was performed using sent-BLEU  score (Bootstrap 3).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.99004727602005}, {"text": "Sorting", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9594392776489258}]}, {"text": " Table 6: Japanese\u2192Russian translation BLEU  scores. Sorting was performed using sent-BLEU  score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9890551567077637}, {"text": "Sorting", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.958246648311615}]}, {"text": " Table 7: French\u2192Malagasy translation BLEU  scores. Sorting was performed using sent-BLEU  score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.981732189655304}, {"text": "Sorting", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.9490928053855896}]}, {"text": " Table 8: German\u2192English translation BLEU  scores. Sorting was performed using sent-BLEU  score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9925699830055237}, {"text": "Sorting", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.961082398891449}]}]}