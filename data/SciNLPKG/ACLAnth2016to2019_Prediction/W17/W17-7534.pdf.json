{"title": [{"text": "Investigating how well contextual features are captured by bi-directional recurrent neural network models", "labels": [], "entities": []}], "abstractContent": [{"text": "Learning algorithms for natural language processing (NLP) tasks traditionally rely on manually defined relevant contextual features.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.8004685640335083}]}, {"text": "On the other hand, neural network models using an only distributional representation of words have been successfully applied for several NLP tasks.", "labels": [], "entities": []}, {"text": "Such models learn features automatically and avoid explicit feature engineering.", "labels": [], "entities": []}, {"text": "Across several domains, neural models become a natural choice specifically when limited characteristics of data are known.", "labels": [], "entities": []}, {"text": "However, this flexibility comes at the cost of interpretability.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.956811785697937}]}, {"text": "In this paper, we define three different methods to investigate ability of bi-directional recurrent neural networks (RNNs) in capturing contextual features.", "labels": [], "entities": []}, {"text": "In particular, we analyze RNNs for sequence tagging tasks.", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7718050678571066}]}, {"text": "We perform a comprehensive analysis on general as well as biomedical domain datasets.", "labels": [], "entities": []}, {"text": "Our experiments focus on important contextual words as features, which can easily be extended to analyze various other feature types.", "labels": [], "entities": []}, {"text": "We also investigate positional effects of context words and show how the developed methods can be used for error analysis.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.6435885727405548}]}], "introductionContent": [{"text": "Learning approaches for NLP tasks can be broadly put into two categories based on the way features are obtained or defined.", "labels": [], "entities": []}, {"text": "The traditional way is to design features according to a specific problem setting and then use appropriate learning ap-proach.", "labels": [], "entities": []}, {"text": "Examples of such methods include classification algorithms like SVM () and CRF () among others for several NLP tasks.", "labels": [], "entities": []}, {"text": "A significant proportion of overall effort is spent on feature engineering itself.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8596346974372864}]}, {"text": "The desire to obtain better performance on a particular problem makes the researchers come up with a domain and task-specific set of features.", "labels": [], "entities": []}, {"text": "The primary advantage of using these models is their interpretability.", "labels": [], "entities": []}, {"text": "However, dependence on handcrafted features limits their applicability in low resource domain where obtaining a rich set of features is difficult.", "labels": [], "entities": []}, {"text": "On the other hand, neural network models provide a more generalised way of approaching problems in NLP domain.", "labels": [], "entities": []}, {"text": "The models can learn relevant features with minimal efforts in explicit feature engineering.", "labels": [], "entities": []}, {"text": "This ability allows the use of such models for problems in low resource domain.", "labels": [], "entities": []}, {"text": "The primary drawback of neural network models is that they are too complicated to interpret as the features are not manually defined.", "labels": [], "entities": []}, {"text": "Neural networks have been applied significantly to various tasks without many insights on what the underlying structural properties are and how the models learn to classify the inputs correctly.", "labels": [], "entities": []}, {"text": "Mostly inspired by computer vision, several mathematical and visual techniques have been developed in this direction.", "labels": [], "entities": []}, {"text": "In contrast to the existing works, this study aims to investigate ability of recurrent neural models to capture important context words.", "labels": [], "entities": []}, {"text": "Towards this goal, we define multiple measures based on word erasure technique ().", "labels": [], "entities": [{"text": "word erasure", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.7235384285449982}]}, {"text": "We do a comprehensive analysis of performance of bi-directional recurrent neural network models for sequence tagging tasks using these measures.", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.783528337876002}]}, {"text": "273 Analysis is focused at understanding how well the relevant contextual words are being captured by different neural models in different settings.", "labels": [], "entities": []}, {"text": "The analysis provides a general tool to compare between different models, show that how neural networks follow our intuition by giving importance to more relevant words, study positional effects of context words and provide error analysis for improving the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider the task of sequence tagging problem for evaluation and analysis of the proposed methods to interpret neural network models.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7634238004684448}]}, {"text": "In particular, we choose the three variants of recurrent neural network models for Named Entity Recognition(NER) task.", "labels": [], "entities": [{"text": "Named Entity Recognition(NER) task", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.7823408969811031}]}, {"text": "In this work, we use two NER datasets from diverse domains.", "labels": [], "entities": [{"text": "NER datasets", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.8135193884372711}]}, {"text": "One is from generic domain whereas other is from biomedical domain.", "labels": [], "entities": []}, {"text": "Statistics of both datasets are given in.", "labels": [], "entities": []}, {"text": "CoNLL, 2003: This dataset was released as apart of CoNLL-2003 language independent named entity recognition task.", "labels": [], "entities": [{"text": "CoNLL, 2003", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9033454855283102}, {"text": "CoNLL-2003 language independent named entity recognition task", "start_pos": 51, "end_pos": 112, "type": "TASK", "confidence": 0.7502257142748151}]}, {"text": "Four named entity types have been used: location, person, organization and miscellaneous.", "labels": [], "entities": []}, {"text": "For this work, we have used the original split of the English dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9092674255371094}]}, {"text": "There were 8 tags used I-PER, B-LOC, I-LOC, B-ORG, I-ORG, B-MISC, I-MISC and O. We focus on three entity types, namely, location (LOC), person (P ER) and organization (ORG) in our analysis.", "labels": [], "entities": [{"text": "O.", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9974247217178345}]}, {"text": "For this dataset, we use pretrained GloVe 50 dimensional word vectors (.", "labels": [], "entities": []}, {"text": "JNLPBA, 2004: Released as apart of BioEntity recognition task () at JNLPBA in 2004, this dataset is from GENIA version 3.02 corpus (.", "labels": [], "entities": [{"text": "JNLPBA, 2004", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9313027262687683}, {"text": "BioEntity recognition task", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.7357422014077505}, {"text": "GENIA version 3.02 corpus", "start_pos": 105, "end_pos": 130, "type": "DATASET", "confidence": 0.9082436710596085}]}, {"text": "There are 5 classes in total -DNA, RNA, Cell line, Cell type and Protein.", "labels": [], "entities": []}, {"text": "We use all the classes in our analysis.", "labels": [], "entities": []}, {"text": "There are 11 tags, 2 (for begin and intermediate word) for each class and O for other context words.", "labels": [], "entities": []}, {"text": "We use 50 dimensional word vectors trained using skip-gram method on a biomedical corpus (.", "labels": [], "entities": []}, {"text": "For this work, we calculate the relevance scores for all the words which have their true tag as O for any test instance in the two datasets.", "labels": [], "entities": [{"text": "relevance", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9905427098274231}]}], "tableCaptions": [{"text": " Table 1: Statistics and performance of different models on two NER datasets used in this work.", "labels": [], "entities": [{"text": "NER datasets", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.8429384529590607}]}, {"text": " Table 2: Correlation values obtained corresponding to \"Ran- goon\" instance from CoNLL dataset.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9463214576244354}]}, {"text": " Table 3: Entity wise relevance scores for words in two in- dividual sentences using LSTM model: (a) Using M SLL  method for CoNLL instance and (b) Using M LRC method  with dot product for JNLPBA instance.", "labels": [], "entities": []}, {"text": " Table 4: Relevance scores for the word \"minister\" in three different test sentences from CoNLL dataset.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9944274425506592}, {"text": "CoNLL dataset", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.9835351705551147}]}, {"text": " Table 5: Relevance scores for an individual test sentence  from JNLPBA dataset, using LSTM and M LRC method  with dot product.", "labels": [], "entities": [{"text": "JNLPBA dataset", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.9837033152580261}]}]}