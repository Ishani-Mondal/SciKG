{"title": [{"text": "Is writing style predictive of scientific fraud?", "labels": [], "entities": []}], "abstractContent": [{"text": "The problem of detecting scientific fraud using machine learning was recently introduced , with initial, positive results from a model taking into account various general indicators.", "labels": [], "entities": [{"text": "detecting scientific fraud", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8338031768798828}]}, {"text": "The results seem to suggest that writing style is predictive of scientific fraud.", "labels": [], "entities": []}, {"text": "We revisit these initial experiments , and show that the leave-one-out testing procedure they used likely leads to a slight overestimate of the predictability, but also that simple models can outper-form their proposed model by some margin.", "labels": [], "entities": []}, {"text": "We goon to explore more abstract linguistic features, such as linguistic complexity and discourse structure, only to obtain negative results.", "labels": [], "entities": []}, {"text": "Upon analyzing our models, we do see some interesting patterns , though: Scientific fraud, for examples , contains less comparison, as well as different types of hedging and ways of presenting logical reasoning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cases of scientific misconduct are identified every year.", "labels": [], "entities": []}, {"text": "Scientific papers are retracted because of errors, or for suspected fraud, ranging from plagiarism and minor manipulations to faking the data and disguising the results.", "labels": [], "entities": []}, {"text": "It has been shown that, however, among the retracted articles indexed in PubMed, only 21.3% are retracted due to error, while 67.4% were removed due to misconduct, among which suspected fraud amounts to 43.4%, the others being due to duplicate publications or plagiarism).", "labels": [], "entities": [{"text": "PubMed", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9284286499023438}]}, {"text": "Ina recent paper, proposed the first analysis of writing style in fraudulent papers across authors and disciplines.", "labels": [], "entities": []}, {"text": "They approached the question of whether these authors have a specific writing style, from a psychological perspective.", "labels": [], "entities": []}, {"text": "They found that these papers exhibit a higher rate of jargon, make a higher use of references, and have a lower readability rate, suggesting that the authors try to obfuscate their writing, making them harder to read and analyze.", "labels": [], "entities": []}, {"text": "They report classification results using a leave-one-out strategy over the dataset, with a classification accuracy of 57.2%.", "labels": [], "entities": [{"text": "classification", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.9599869847297668}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9024742245674133}]}, {"text": "As suggested in the paper, we propose to improve this performance by evaluating different classification models.", "labels": [], "entities": []}, {"text": "In this paper, we first show that much better results can be obtained using a simple bag-of-words representation and Logistic Regression.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.6499354243278503}]}, {"text": "Our best model is a syntax-enhanced trigram-model.", "labels": [], "entities": []}, {"text": "We also show that the leave-one-out strategy used by the authors leads to an over-estimation of model precision, and we report new results based on a more robust strategy, taking into account the low number of instance available; namely a nested cross-validation ().", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9756987690925598}]}, {"text": "We also considered semantic and discourse features, but we did not observe improvements with such features.", "labels": [], "entities": []}, {"text": "Of course, that a bag-of-words model outperforms a model based on psychologically motivated features, may simply be the result of overfitting.", "labels": [], "entities": []}, {"text": "We present an extensive feature analysis to validate our models, as well as to test psychologically motivated hypotheses from the literature.", "labels": [], "entities": []}, {"text": "Contributions (i) We present a simple model with high accuracy, and show that it implicitly captures the previously-proposed psychologicallymotivated features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9990077614784241}]}, {"text": "(ii) We show that adding semantics and discourse features does not lead to improvements.", "labels": [], "entities": []}, {"text": "(iii) On the other hand, our feature analysis suggests that the models do learn to focus on concepts that are intuitively related to scientific misconduct, e.g., that scientific fraud contains less comparison. were the first to study writing style in fraudulent papers.", "labels": [], "entities": []}, {"text": "They gathered a corpus of 253 articles indexed in PubMed that have been retracted for fraudulent data, as well as 253 unretracted papers (see Section 3).", "labels": [], "entities": [{"text": "PubMed", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.9620959758758545}]}, {"text": "They define five indicators of obfuscation, and show that fraudulent papers tend to demonstrate a higher rate of linguistics obfuscation, corresponding to a lower readability, an higher use of jargon and a higher degree of abstraction.", "labels": [], "entities": []}, {"text": "Linked to studies on deception identification, they also report a lower rate of positive emotion terms and a higher rate of causal terms (e.g. \"depend\", \"induce\", \"manipulated\") in fraudulent papers.", "labels": [], "entities": [{"text": "deception identification", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8752827942371368}]}, {"text": "The readability score was computed using Coh-Metrix (, while the other scores were based on the Linguistic Inquiry and Word Count (LIWC;), a dictionary associating a word to various scores such as abstraction (a word is considered as jargon if it is not found in the dictionary).", "labels": [], "entities": [{"text": "Linguistic Inquiry and Word Count (LIWC", "start_pos": 96, "end_pos": 135, "type": "METRIC", "confidence": 0.6080818516867501}]}, {"text": "Finally, they report 57.2% inaccuracy using these five indicators as features, a score that we show is probably a little too optimistic, since it is based on a leaveone-out procedure (see Section 5).", "labels": [], "entities": []}, {"text": "We extend their work by first showing that a simple unigram model outperforms their model by a large margin, but also by considering more indicators, including discourse and syntax, and by showing, as mentioned, that their scores were probably over-estimated due to their validation strategy.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results (accuracy, in %).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9996886253356934}]}]}