{"title": [{"text": "Coreference Resolution for Swedish and German using Distant Supervision", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9643093645572662}]}], "abstractContent": [{"text": "Coreference resolution is the identification of phrases that refer to the same entity in a text.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8991279304027557}, {"text": "identification of phrases that refer to the same entity in a text", "start_pos": 30, "end_pos": 95, "type": "TASK", "confidence": 0.6299317429463068}]}, {"text": "Current techniques to solve coref-erences use machine-learning algorithms, which require large annotated data sets.", "labels": [], "entities": []}, {"text": "Such annotated resources are not available for most languages today.", "labels": [], "entities": []}, {"text": "In this paper, we describe a method for solving coreferences for Swedish and German using distant supervision that does not use manually annotated texts.", "labels": [], "entities": []}, {"text": "We generate a weakly labelled training set using parallel corpora, English-Swedish and English-German, where we solve the coreference for English using CoreNLP and transfer it to Swedish and German using word alignments.", "labels": [], "entities": []}, {"text": "To carry this out, we identify mentions from dependency graphs in both target languages using handwritten rules.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the end-to-end results using the evaluation script from the CoNLL 2012 shared task for which we obtain a score of 34.98 for Swedish and 13.16 for German and, respectively, 46.73 and 36.98 using gold mentions.", "labels": [], "entities": [{"text": "CoNLL 2012 shared task", "start_pos": 81, "end_pos": 103, "type": "DATASET", "confidence": 0.9035175293684006}]}], "introductionContent": [{"text": "Coreference resolution is the process of determining whether two expressions refer to the same entity and linking them in a body of text.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9243988394737244}]}, {"text": "The referring words and phrases are generally called mentions.", "labels": [], "entities": []}, {"text": "Coreference resolution is instrumental in many language processing applications such as information extraction, the construction of knowledge graphs, text summarizing, question answering, etc.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9350899457931519}, {"text": "information extraction", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.8421891033649445}, {"text": "text summarizing", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.7324970364570618}, {"text": "question answering", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8754928112030029}]}, {"text": "As most current high-performance coreference solvers use machine-learning techniques and supervised training, building solvers requires large amounts of texts, hand-annotated with coreference chains.", "labels": [], "entities": [{"text": "coreference solvers", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8176764249801636}]}, {"text": "Unfortunately, such corpora are expensive to produce and are far from being available for all the languages, including the Nordic languages.", "labels": [], "entities": []}, {"text": "In the case of Swedish, there seems to be only one available corpus annotated with coreferences: SUC-Core, which consists of 20,000 words and 2,758 coreferring mentions.", "labels": [], "entities": []}, {"text": "In comparison, the CoNLL 2012 shared task () uses a training set of more than a million word and 155,560 coreferring mentions for the English language alone.", "labels": [], "entities": [{"text": "CoNLL 2012 shared task", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.8342875838279724}]}, {"text": "Although models trained on large corpora do not automatically result in better solver accuracies, the two orders of magnitude difference between the English CoNLL 2012 corpus and SUCCore has certainly consequences on the model quality for English.", "labels": [], "entities": [{"text": "English CoNLL 2012 corpus", "start_pos": 149, "end_pos": 174, "type": "DATASET", "confidence": 0.8829057216644287}]}, {"text": "posited that larger and more consistent corpora as well as a standardized evaluation scenario would be away to improve the results in coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.965923398733139}]}, {"text": "The same should apply to Swedish.", "labels": [], "entities": []}, {"text": "Unfortunately, annotating 1,000,000 words by hand requires seems to be out of reach for this language for now.", "labels": [], "entities": []}, {"text": "In this paper, we describe a distant supervision technique to train a coreference solver for Swedish and other languages lacking large annotated corpora.", "labels": [], "entities": [{"text": "coreference solver", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8601756691932678}]}, {"text": "Instead of using SUC-Core to train a model, we used it for evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Similarly to the CoNLL 2011 and 2012 shared tasks, we evaluated our system using gold and predicted mention boundaries.", "labels": [], "entities": [{"text": "CoNLL 2011 and 2012 shared tasks", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.6992082794507345}]}, {"text": "When given the gold mentions, the solver knows the boundaries of all nonsingleton mentions in the test set, while with predicted mention boundaries, the solver has no prior knowledge about the test set.", "labels": [], "entities": [{"text": "solver", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.9430087804794312}]}, {"text": "We also followed the shared tasks in only using machineannotated parses as input.", "labels": [], "entities": []}, {"text": "The rationale for using gold mention boundaries is that they correspond to the use of an ideal method for mention identification, where the results are an upper bound for the solver as it does not consider singleton mentions).", "labels": [], "entities": [{"text": "mention identification", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7304701805114746}]}], "tableCaptions": [{"text": " Table 6: End-to-end results using predicted mentions", "labels": [], "entities": []}, {"text": " Table 7: End-to-end results using gold mention boundaries", "labels": [], "entities": []}]}