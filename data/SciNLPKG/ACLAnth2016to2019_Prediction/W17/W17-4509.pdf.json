{"title": [{"text": "Topic Model Stability for Hierarchical Summarization", "labels": [], "entities": [{"text": "Topic Model Stability", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7189159194628397}, {"text": "Hierarchical Summarization", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8130192458629608}]}], "abstractContent": [{"text": "We envisioned responsive generic hierarchical text summarization with summaries organized by topic and paragraph based on hierarchical structure topic models.", "labels": [], "entities": []}, {"text": "But we had to be sure that topic models were stable for the sampled corpora.", "labels": [], "entities": []}, {"text": "To that end we developed a methodology for aligning multiple hierarchical structure topic models run over the same corpus under similar conditions, calculating a representative centroid model, and reporting stability of the centroid model.", "labels": [], "entities": []}, {"text": "We ran stability experiments for standard corpora and a development corpus of Global Warming articles.", "labels": [], "entities": []}, {"text": "We found flat and hierarchical structures of two levels plus the root offer stable cen-troid models, but hierarchical structures of three levels plus the root didn't seem stable enough for use in hierarchical summariza-tion.", "labels": [], "entities": []}], "introductionContent": [{"text": "We envisioned a responsive generic hierarchical text summarization process for complex subjects and multiple page documents with resulting text summaries organized by topic and paragraph.", "labels": [], "entities": []}, {"text": "Information extraction and summary construction would be based on hierarchical structure topic models learned in the analysis phase.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8282821476459503}, {"text": "summary construction", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.8725878000259399}]}, {"text": "The hierarchical topic structure would provide the organization as well as the information quantity budget and extraction criteria for sections and paragraphs in hierarchical summarization.", "labels": [], "entities": []}, {"text": "Initial attempts along this path offered promise fora more coherent and organized summary fora small corpus of Global Warming articles from (Live Science, 2015) versus that obtained by flat topic structures.", "labels": [], "entities": [{"text": "Live Science, 2015)", "start_pos": 141, "end_pos": 160, "type": "DATASET", "confidence": 0.9239667654037476}]}, {"text": "However, multiple analyses of the same Global Warming corpus and various standard corpora under similar conditions rendered seemingly different hierarchical topic models.", "labels": [], "entities": []}, {"text": "Model differences remained even after transforming and reducing models based on required summary size and other extrinsic summary requirements.", "labels": [], "entities": []}, {"text": "So we decided to examine topic model stability with the goal of assuring that stable, representative, and credible topic models would be produced in our analysis phase.", "labels": [], "entities": []}, {"text": "This paper documents our effort at assuring hierarchical topic model stability for hierarchical summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.6699971556663513}]}, {"text": "It is inherent in Bayesian probabilistic topic modeling and similar methods that repeat analyses of the same corpus under the same conditions give different results.", "labels": [], "entities": [{"text": "Bayesian probabilistic topic modeling", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.6119985431432724}]}, {"text": "But we must have substantially similar results to do credible hierarchical summarization (or other application).", "labels": [], "entities": [{"text": "hierarchical summarization", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.5366226732730865}]}, {"text": "We require topic model stability, i.e., similar topic models for analyses performed under similar conditions.", "labels": [], "entities": []}, {"text": "Without stable results, we do not know which analyses to believe, if any, and we mistrust the methodology itself.", "labels": [], "entities": []}, {"text": "Furthermore, any application of the resulting topic model is not credible.", "labels": [], "entities": []}, {"text": "Organization of Paper Bayesian probabilisitic topic analysis ( \u00a72.1) expresses a corpus as the matrix product of topic compositions of words with document mixtures of topics.", "labels": [], "entities": [{"text": "Organization of Paper Bayesian probabilisitic topic analysis", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.69586700626782}]}, {"text": "In flat topic analysis, the matrix of topic-word compositions is organized as a flat vector of individual topics.", "labels": [], "entities": [{"text": "flat topic analysis", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.690373440583547}]}, {"text": "With hierarchical structure topic analysis, the topics take on a hierarchical tree structure.", "labels": [], "entities": [{"text": "hierarchical structure topic analysis", "start_pos": 5, "end_pos": 42, "type": "TASK", "confidence": 0.64352747797966}]}, {"text": "Topic model quality ( \u00a72.2) is typically assessed by predictive likelihood of words fora test corpus or by assessment of topic coherence.", "labels": [], "entities": []}, {"text": "Our stability assessment methodology seems largely com-plementary to quality assessment.", "labels": [], "entities": []}, {"text": "The Hungarian assignment algorithm has been used for aligning flat topic model pairs ( \u00a72.3), based on a cost matrix of pairwise topic alignments.", "labels": [], "entities": [{"text": "Hungarian assignment", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6024154275655746}]}, {"text": "We will use a pairwise topic similarity measure for populating the Hungarian algorithm's cost matrix.", "labels": [], "entities": []}, {"text": "Topic models, including hierarchical models, are being used to construct text summaries ( \u00a72.4), including hierarchical text summaries.", "labels": [], "entities": []}, {"text": "This provides sufficient reason to want to assure the stability of flat and hierarchical structure topic models.", "labels": [], "entities": []}, {"text": "We introduce the particular flat and hierarchical structure topic models ( \u00a73.1) used for this paper.", "labels": [], "entities": []}, {"text": "Ina simple yet significant innovation, we extend topic alignment ( \u00a73.2) to hierarchical structure topic model pairs via a recursive application of the Hungarian assignment algorithm starting with root topics of the model pair.", "labels": [], "entities": [{"text": "topic alignment", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7539183497428894}]}, {"text": "Surprisingly, we find time complexity of the hierarchical topic structure improves versus flat structure with increasing level of the hierarchy.", "labels": [], "entities": []}, {"text": "We measure stability ( \u00a73.3) as alignment (proportion of aligned topics), similarity (weighted cosine similarity over topic compositions), and divergence (Jensen-Shannon divergence over topic distributions).", "labels": [], "entities": [{"text": "alignment", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9464592933654785}, {"text": "similarity", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9896739721298218}]}, {"text": "Measures are defined for flat and then extended to hierarchical structure topic models.", "labels": [], "entities": []}, {"text": "The more topic models in the study, the more credible the stability analysis, since we are aligning more models and measuring stability based on more analyses.", "labels": [], "entities": []}, {"text": "For complex problems, however, more models also makes it more likely we would encounter alternative topic models, just as human topic modelers might.", "labels": [], "entities": []}, {"text": "We perform agglomerative clustering on topic model similarity ( \u00a73.4) to test whether models form a single or multiple stable topic model groups, or are unstable.", "labels": [], "entities": []}, {"text": "For each cluster, we align models and calculate topic frequency weighted centroids ( \u00a73.5) of topicword compositions for aligned topics.", "labels": [], "entities": []}, {"text": "Then we assess stability versus the centroid model ( \u00a73.6) similarly to that done previously for model pairs.", "labels": [], "entities": []}, {"text": "We demonstrate the methodology ( \u00a74) over flat and hierarchical structure models in an 18 run factorial experiment on three corpora, and in a separate ad hoc 16 run experiment on a larger corpus.", "labels": [], "entities": []}, {"text": "We return to our work on hierarchical summa-2 Software engineering already knows this -that hierarchical structure is less time complex than monolithic.", "labels": [], "entities": [{"text": "summa-2 Software engineering", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.7280335028966268}]}, {"text": "rization ( \u00a75) now armed with stable hierarchical topic models and examine our next steps as well as options for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "The purpose of the stability experiments is to demonstrate the methodology over corpora for flat and hierarchical structures.", "labels": [], "entities": []}, {"text": "When stable centroid models result from replicate topic analyses, they can credibly be transformed to take into account extrinsic summarization requirements, and carried forward to the information extraction phase of our hierarchical summarization process.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.7834571599960327}]}, {"text": "An 18 run factorial design (3 corpora x 3 levels x 2 growth rates) crosses JACM, GW, and PNAS corpora, with flat (L=0) and hierarchical (L=2,3) topic structures, and topic growth rates to achieve  two different topic count ranges.", "labels": [], "entities": []}, {"text": "Four replicate topic analyses were run at each factorial setting.", "labels": [], "entities": []}, {"text": "For training, our simplified Gibbs sampler used \u03b1=1.0 and \u03b7=0.01 with optimization.", "labels": [], "entities": []}, {"text": "The growth parameter \u03b3 was set to create topic counts at low (L), medium (M), and high (H) ranges.", "labels": [], "entities": []}, {"text": "Separately, an ad hoc experiment was performed on a set of 16 trials on the NIPS corpus with hierarchical (L=3) model using similar training control settings.", "labels": [], "entities": [{"text": "NIPS corpus", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.9813159704208374}]}, {"text": "This experiment demonstrates the occurrence of multiple clusters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Experimental results -stability.", "labels": [], "entities": []}, {"text": " Table 4: Ad hoc stability experiment on NIPS.", "labels": [], "entities": [{"text": "NIPS", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.8709754347801208}]}]}