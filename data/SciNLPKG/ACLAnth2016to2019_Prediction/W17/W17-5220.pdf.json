{"title": [{"text": "Lexicon Integrated CNN Models with Attention for Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.957298994064331}]}], "abstractContent": [{"text": "With the advent of word embeddings, lexicons are no longer fully utilized for sentiment analysis although they still provide important features in the traditional setting.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.9695218801498413}]}, {"text": "This paper introduces a novel approach to sentiment analysis that integrates lexicon embeddings and an attention mechanism into Convolutional Neural Networks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9642156958580017}]}, {"text": "Our approach performs separate convolutions for word and lexicon embeddings and provides a global view of the document using attention.", "labels": [], "entities": []}, {"text": "Our models are experimented on both the SemEval'16 Task 4 dataset and the Stanford Sentiment Treebank and show comparative or better results against the existing state-of-the-art systems.", "labels": [], "entities": [{"text": "SemEval'16 Task 4 dataset", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.650862954556942}, {"text": "Stanford Sentiment Treebank", "start_pos": 74, "end_pos": 101, "type": "DATASET", "confidence": 0.9176838397979736}]}, {"text": "Our analysis shows that lexicon embeddings allow building high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.9625226557254791}]}], "introductionContent": [{"text": "Sentiment analysis is a task of identifying sentiment polarities expressed in documents, typically positive, neutral, or negative.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9388429820537567}]}, {"text": "Although the task of sentiment analysis has been well-explored (, it is still very challenging due to the complexity of extracting human emotion from raw text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9604188501834869}]}, {"text": "The recent advance of deep learning has definitely elevated the performance of this task although there is much more room to improve.", "labels": [], "entities": []}, {"text": "In the traditional setting where statistical models are based on sparse features, lexicons consisting of words and their sentiment scores are shown to be highly effective for sentiment analysis because they provide features that may not be captured from training data ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.9583107829093933}]}, {"text": "However, since the appearance of word embeddings, the use of lexicons is getting faded away because word embeddings are believed to capture the sentiment aspects of those words.", "labels": [], "entities": []}, {"text": "This brought us two important questions: \u2022 Can lexicons be still useful for sentiment analysis when coupled with word embeddings?", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.9735572338104248}]}, {"text": "\u2022 If yes, what is the most effective way of incorporating lexicons with word embeddings?", "labels": [], "entities": []}, {"text": "To answer these questions, we first construct lexicon embeddings that are specifically designed for sentiment analysis and integrate them into the existing Convolutional Neural Network (CNN) model similar to.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.937509149312973}]}, {"text": "Three ways of lexicon integration to the CNN model are proposed, which show distinctive characteristics for different genres (Section 3.2).", "labels": [], "entities": []}, {"text": "We then incorporate an efficient attention mechanism to our CNN models, which provides a global view of the document by emphasizing (or de-emphasizing) important words and lexicons (Section 3.3).", "labels": [], "entities": []}, {"text": "Our models using lexicon embeddings are evaluated on two well-known datasets, the SemEval'16 dataset and the Stanford Sentiment Treebank, and show state-of-the-art results on both datasets (Section 4).", "labels": [], "entities": [{"text": "SemEval'16 dataset", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.8327243030071259}, {"text": "Stanford Sentiment Treebank", "start_pos": 109, "end_pos": 136, "type": "DATASET", "confidence": 0.884576698144277}]}, {"text": "To the best of our knowledge, this is the first time that lexicon embeddings are introduced for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.9499014317989349}]}], "datasetContent": [{"text": "Seven models are evaluated to show the effectiveness of lexicon embeddings to sentiment analysis: baseline (Section 3.1), naive concatenation (NC; Section 3.2.1), multichannel (MC; Section 3.2.2), separate convolution (SC; Section 3.2.3), and the three integration approaches with embedding attention ( * -EAV; Section 3.3).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8287263214588165}]}, {"text": "The comparisons of our proposed models to the previous state-of-the-art approaches are outlined in.", "labels": [], "entities": []}, {"text": "For all experiments, the fixed random seed of 1 is used to avoid performance boost from different randomness (see Section 4.4.1 for more discussions).", "labels": [], "entities": []}, {"text": "The following configuration are used for all models: \u2022 Filter size = (2, 3, 4, 5) for both word and lexicon embeddings.", "labels": [], "entities": [{"text": "Filter size", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.8858982622623444}]}, {"text": "\u2022 Number of filters = (64 and 9) for word and lexicon embeddings, respectively.", "labels": [], "entities": []}, {"text": "\u2022 Number of filters = (50 and 20) for constructing embedding attention vectors in word and lexicon embedding spaces, respectively.", "labels": [], "entities": []}, {"text": "It is worth mentioning that the performance of our baseline models improved quite a bit when the training corpora for word embeddings and sentiment analysis were tokenized coherently.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.871512234210968}]}, {"text": "Unlike several other work, we used the identical tokenization tool, NLP4J, to preprocess all corpora, which gave considerable boost in performance.", "labels": [], "entities": []}, {"text": "Comparing the baseline to SC, lexicon embeddings significantly improved accuracy for S16, about 2%, surpassing the previous state-of-the-art result achieved by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9991468191146851}]}, {"text": "However, SC did not show much improvement for SST where the baseline was already performing well.", "labels": [], "entities": [{"text": "SC", "start_pos": 9, "end_pos": 11, "type": "DATASET", "confidence": 0.5386221408843994}, {"text": "SST", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9817869663238525}]}, {"text": "Comparing these lexicon integrated models with the ones with embedding attention vectors ( * -EAV), EAV did not help much for S16 but significantly improved the performance for SST, achieving the state-of-the-art result of 48.8% fora single-layer CNN model.", "labels": [], "entities": [{"text": "SST", "start_pos": 177, "end_pos": 180, "type": "TASK", "confidence": 0.9690792560577393}]}, {"text": "The accuracy achieved by our best model is still 0.8% lower than the state-of-the-art result achieved by; however, considering their model uses five embedding channels and dual-layer convolutions whereas our model uses a single channel and a single-layer convolution, in other words, our model is much more compact, this is very promising.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996205568313599}]}, {"text": "These results suggest that lexicon embeddings coupled with the embedding attention vectors allow to build robust sentiment analysis models.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.9154731333255768}]}, {"text": "illustrates the robustness of our lexicon integrated models with respect to the size of word embeddings.", "labels": [], "entities": []}, {"text": "Our baseline produces inconsistent and unstable results as different sizes of word embeddings are used.", "labels": [], "entities": []}, {"text": "Furthermore, a larger size of word embeddings tends to significantly outperform a smaller size of word embeddings.", "labels": [], "entities": []}, {"text": "Such tendency is reduced with the incorporation of lexicon embeddings.", "labels": [], "entities": []}, {"text": "While the standard deviations for the accuracies achieved by the baseline using different sizes of word embeddings are 0.8491 and 1.1909 for S16 and SST, respectively, they are reduced to 0.4208 and 0.5764 respectively for lexicon integrated models.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9916913509368896}]}, {"text": "Furthermore, the accuracy achieved by the lexicon integrated model using the word embedding size 50 is higher or equal to the highest accuracy achieved by the baseline using the word embedding size 200, which implies that it is possible to build more compact models using lexicon embeddings without compromising accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9993762373924255}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9891180396080017}, {"text": "accuracy", "start_pos": 312, "end_pos": 320, "type": "METRIC", "confidence": 0.9938228130340576}]}], "tableCaptions": [{"text": " Table 1: Statistics of the SemEval'16 Task 4 dataset.  +/0/-: positive/neutral/negative, TRN/DEV/TST:  training, development, evaluation sets.", "labels": [], "entities": [{"text": "SemEval'16 Task 4 dataset", "start_pos": 28, "end_pos": 53, "type": "DATASET", "confidence": 0.8278284519910812}]}, {"text": " Table 2: Statistics of the Stanford Sentiment Tree- bank dataset. ++/+/0/-/-: very positive/positive/  neutral/negative/very negative.", "labels": [], "entities": [{"text": "Stanford Sentiment Tree- bank dataset", "start_pos": 28, "end_pos": 65, "type": "DATASET", "confidence": 0.936452329158783}]}, {"text": " Table 3: The percentage of word types covered by  our word and lexicon embeddings for each dataset.", "labels": [], "entities": []}]}