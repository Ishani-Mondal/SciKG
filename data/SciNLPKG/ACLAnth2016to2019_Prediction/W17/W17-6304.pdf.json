{"title": [{"text": "Improving neural tagging with lexical information", "labels": [], "entities": [{"text": "Improving neural tagging", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9031527638435364}]}], "abstractContent": [{"text": "Neural part-of-speech tagging has achieved competitive results with the incorporation of character-based and pre-trained word embeddings.", "labels": [], "entities": [{"text": "Neural part-of-speech tagging", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7892999450365702}]}, {"text": "In this paper, we show that a state-of-the-art bi-LSTM tagger can benefit from using information from morphosyntactic lexicons as additional input.", "labels": [], "entities": []}, {"text": "The tagger, trained on several dozen languages, shows a consistent, average improvement when using lexical information, even when also using character-based embeddings, thus showing the complementarity of the different sources of lexical information.", "labels": [], "entities": []}, {"text": "The improvements are particularly important for the smaller datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech tagging is now a classic task in natural language processing.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7735844850540161}, {"text": "natural language processing", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6351596812407175}]}, {"text": "Its aim is to associate each \"word\" with a morphosyntactic tag, whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.).", "labels": [], "entities": []}, {"text": "The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers.", "labels": [], "entities": [{"text": "PoS taggers", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.8935799896717072}]}, {"text": "A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models), decision trees, maximum entropy Markov models (MEMMs) and Conditional Random Fields (CRFs) (.", "labels": [], "entities": []}, {"text": "Recently, neural approaches have reached very competitive accuracy levels, improving over the state of the art in a number of settings (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9983128309249878}]}, {"text": "As a complement to annotated training corpora, external lexicons can be a valuable source of information.", "labels": [], "entities": []}, {"text": "First, morphosyntactic lexicons provide a large inventory of (word, PoS) pairs.", "labels": [], "entities": []}, {"text": "Such lexical information can be used in the form of constraints at tagging time ( or during the training process as additional features combined with standard features extracted from the training corpus (.", "labels": [], "entities": []}, {"text": "Second, lexical information encoded in vector representations, known as word embeddings, have emerged more recently (.", "labels": [], "entities": []}, {"text": "Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono-or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs).", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.9299537241458893}]}, {"text": "Character-level embeddings are of particular interest for PoS tagging as they generate vector representations that result from the internal characterlevel make-up of each word.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.9000704884529114}]}, {"text": "It can generalise over relevant sub-parts such as prefixes or suffixes, thus directly addressing the problem of unknown words.", "labels": [], "entities": []}, {"text": "However, unknown words do not always follow such generalisations.", "labels": [], "entities": []}, {"text": "In such cases, character-level models cannot bring any advantage.", "labels": [], "entities": []}, {"text": "This is a difference with external lexicons, which provides information about any word it contains, yet without any quantitative distinction between relevant and less relevant information.", "labels": [], "entities": []}, {"text": "Therefore, a comparative assessment of the ad-vantages of using character-level embeddings and external lexical information is an interesting idea to follow.", "labels": [], "entities": []}, {"text": "However, the inclusion of morphosyntactic information from lexicons into neural PoS tagging architecture, as a replacement or complement to character-based or pre-computed word embeddings, remains to be investigated.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.8233806788921356}]}, {"text": "In this paper, we describe how such an inclusion can be achieved and show, based on experiments using the Universal Dependencies corpora (version 1.3), that it leads to significant improvements over state-of-the-art results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use as a baseline the state-of-the-art bi-LSTM PoS tagger bilty, a freely available and \"significantly refactored version of the code originally used\" by.", "labels": [], "entities": []}, {"text": "We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning, 7 and 20 iterations for training.", "labels": [], "entities": []}, {"text": "We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section.", "labels": [], "entities": []}, {"text": "5 6 https://github.com/bplank/bilstm-aux 7 Plank et al.'s (2016) secondary task-predicting the frequency class of each word-results in better OOV scores but virtually identical overall scores when averaged overall tested languages/corpora.", "labels": [], "entities": [{"text": "OOV", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.9463706016540527}]}, {"text": "For each lexicon-related configuration, we trained three variants of the tagger: (i) a variant without using character-based embeddings and standard (zero) initialisation of word embeddings before training, (ii) a variant with character-based embeddings and standard initialisation of word embeddings, and (iii) when Polyglot embeddings are available for the language at hand, a variant with character-based embeddings and initialisation of the word embeddings with the Polyglot embeddings.", "labels": [], "entities": []}, {"text": "This is deliberately similar to: Accuracy of the best system using a lexicon for words out of the training corpus (OOTC), and for words out of the training corpus that are present in the lexicon (OOTC in Lex.), as well as difference between the best system and the baseline without lexicon for these two subsets of words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9738665819168091}]}], "tableCaptions": [{"text": " Table 1: Dataset information. Best per-language  lexicon along with its size and number of tags  over the UD1.3 corpora.", "labels": [], "entities": [{"text": "UD1.3 corpora", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.901159018278122}]}, {"text": " Table 3: Overall results. PoS accuracy scores are given for each language in the baseline configura- tion (the same as", "labels": [], "entities": [{"text": "PoS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9208992719650269}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8701236248016357}]}, {"text": " Table 4: Accuracy of the best system using a lexi- con for words out of the training corpus (OOTC),  and for words out of the training corpus that are  present in the lexicon (OOTC in Lex.), as well  as difference between the best system and the  baseline without lexicon for these two subsets of  words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9761444926261902}]}]}