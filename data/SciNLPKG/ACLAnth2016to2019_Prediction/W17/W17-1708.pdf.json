{"title": [{"text": "Factoring Ambiguity out of the Prediction of Compositionality for German Multi-Word Expressions", "labels": [], "entities": []}], "abstractContent": [{"text": "Ambiguity represents an obstacle for distributional semantic models (DSMs), which typically subsume the contexts of all word senses within one vector.", "labels": [], "entities": []}, {"text": "While individual vector space approaches have been concerned with sense discrimination (e.g., Sch\u00fctze (1998), Erk (2009), Erk and Pado (2010)), such discrimination has rarely been integrated into DSMs across semantic tasks.", "labels": [], "entities": [{"text": "sense discrimination", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7257843017578125}]}, {"text": "This paper presents a soft-clustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compo-sitionality for German noun-noun compounds and German particle verbs.", "labels": [], "entities": [{"text": "sense discrimination", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7358565777540207}]}], "introductionContent": [{"text": "Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means.", "labels": [], "entities": []}, {"text": "For example, studies such as,,, and have integrated the prediction of multi-word compositionality into statistical machine translation.", "labels": [], "entities": [{"text": "prediction of multi-word compositionality", "start_pos": 56, "end_pos": 97, "type": "TASK", "confidence": 0.7230141088366508}, {"text": "statistical machine translation", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.6505374411741892}]}, {"text": "We are interested in predicting the degrees of compositionality of two types of German multiword expressions: (i) German noun-noun compounds (NCs) represent nominal multi-word expressions (MWEs), e.g., Feuer|werk 'fire works' is composed of the constituents Feuer 'fire' and Werk 'opus'.", "labels": [], "entities": []}, {"text": "(ii) German particle verbs (PVs) are complex verbs such as an|strahlen ('beam/smile at') which are composed of a separable prefix particle (an) and abase verb (strahlen 'beam'/'smile').", "labels": [], "entities": []}, {"text": "Both types of German MWEs are highly frequent and highly productive in the lexicon.", "labels": [], "entities": [{"text": "German MWEs", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.8033633232116699}]}, {"text": "presents some example MWEs and their constituents with human ratings on compositionality.", "labels": [], "entities": []}, {"text": "Automatic approaches to predict compositionality degrees typically exploit distributional semantic models (DSMs), i.e. vector representations relying on the distributional hypothesis, that words with similar distributions have related meanings.", "labels": [], "entities": []}, {"text": "Regarding the compositionality prediction, DSMs represent the meanings of the MWEs and their constituents by distributional vectors, and the similarity of a compound-constituent vector pair is taken as the predicted degree of compoundconstituent compositionality.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.9348809123039246}]}, {"text": "Existing approaches addressed the compositionality of NCs () and complex verbs (, mainly dfor English and for German.", "labels": [], "entities": []}, {"text": "A major obstacle for DSMs is their conflation of contexts across individual word senses.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.950975239276886}]}, {"text": "DSMs typically subsume evidence of cooccurring items within one vector for the target word type, rather than discriminating contextual evidence for the specific target word senses.", "labels": [], "entities": []}, {"text": "Taking the German noun-noun compound Blatt|salat 'leaf salad' as an example, its modifier constituent Blatt has at least four senses: 'leaf', 'sheet of paper', 'newspaper' and 'hand of cards'.", "labels": [], "entities": [{"text": "Blatt", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9690305590629578}]}, {"text": "If we had individual sense vectors for each sense of Blatt, a DSM might successfully predict a strong compositionality for the compound Blatt|salat regarding this constituent, when comparing the compound vector with the 'leaf' sense vector, because the vectors agree on: Examples of German noun-noun compounds and German particle verbs, accompanied by translations and human mean ratings on the degrees of compound-constituent compositionality.", "labels": [], "entities": []}, {"text": "salient features such as green and fresh.", "labels": [], "entities": []}, {"text": "But traditionally, the constituent vector contains distributional information across all Blatt senses, and the similarity between the conflated word type vector and the compound vector is most probably determined by the predominant sense of the word type (which does not necessarily coincide with the relevant sense).", "labels": [], "entities": []}, {"text": "While individual vector space approaches have been concerned with sense discrimination (e.g.,,,), the approaches have rarely been integrated into DSMs across semantic tasks.", "labels": [], "entities": [{"text": "sense discrimination", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7171197235584259}]}, {"text": "Alternatively, sense disambiguation/discrimination approaches have been developed for SemEval tasks on Word Sense Disambiguation/Discrimination and (Crosslingual) Lexical Substitution (.", "labels": [], "entities": [{"text": "sense disambiguation/discrimination", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.8115978240966797}, {"text": "SemEval tasks", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.9235856533050537}, {"text": "Word Sense Disambiguation/Discrimination", "start_pos": 103, "end_pos": 143, "type": "TASK", "confidence": 0.7030470132827759}, {"text": "Crosslingual) Lexical Substitution", "start_pos": 149, "end_pos": 183, "type": "TASK", "confidence": 0.6124909073114395}]}, {"text": "As to our knowledge, few systems have attempted to distinguish between word senses and then address various semantic relatedness tasks, such as and.", "labels": [], "entities": []}, {"text": "Computational compositionality assessment has been studied for NCs) and PVs (.", "labels": [], "entities": [{"text": "Computational compositionality assessment", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6911152005195618}]}, {"text": "Most similar to our current work is, who addressed the problem of semantic ambiguity in MWEs by using a multi-sense skip gram model with two to five embeddings per word.", "labels": [], "entities": []}, {"text": "They expected multiple embeddings to capture different word senses.", "labels": [], "entities": []}, {"text": "They could, however, not find an improvement over the use of single-word embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we suggest soft clustering as an approximation to separate the different senses of a word type.", "labels": [], "entities": []}, {"text": "We expect that the assignments of compound and constituent words to clusters reflect the differences between word senses, and that the underlying features refer to the features of the respective word sense.", "labels": [], "entities": []}, {"text": "We assume further that if we find a pair <\u00b5, \u03ba> of an MWE \u00b5 and one of its constituents \u03ba with high distributional similarity in the same cluster, this indicates closeness in meaning and therefore strong compositionality.", "labels": [], "entities": []}, {"text": "We exploit the soft clusters by (a) identifying the relevant senses of the MWE and constituents based on overlap in cluster assignment, and by (b) comparing reduced vectors of MWEs and constituents when taking into account only a subset of clusterbased salient sense features, in order to optimize the prediction of compositionality.", "labels": [], "entities": []}], "datasetContent": [{"text": "Distributional Semantics Models Our DSM is a word space model that uses lemmatized words as dimensions in the high-dimensional vectors space.", "labels": [], "entities": []}, {"text": "The associative strength between target and context words is measured as Local Mutual Information (LMI)), based on context word frequency.", "labels": [], "entities": []}, {"text": "The context of the targets is defined as a window of n words to the left and the right of the target.", "labels": [], "entities": []}, {"text": "We use the cosine value of the angle between two vectors as a measure for semantic similarity and compositionality.", "labels": [], "entities": []}, {"text": "For technical reasons we ignore context words with a count of 5 or lessor an LMI value below 0.", "labels": [], "entities": [{"text": "LMI", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9530068039894104}]}, {"text": "We use the word vectors in three ways here: (a) we use them directly as window models in order to measure the distance between vector pairs for an MWE and each of its components (e.g. Blatt|salat vs. Blatt).", "labels": [], "entities": []}, {"text": "We also use them (b) as an input matrix for soft clustering and (c) we build word vector models for each cluster.", "labels": [], "entities": []}, {"text": "LSC for Soft Clustering We use Latent Semantic Classes (LSC) as a soft clustering algorithm).", "labels": [], "entities": []}, {"text": "LSC is a two-dimensional soft-clustering algorithm which learns three probability distributions: (a) across the clusters, (b) for the output probabilities of each element within a cluster and (c) for each feature type with regard to a cluster.", "labels": [], "entities": []}, {"text": "The access to all three probability distributions is crucial for our approach, since it allows to determine which features are salient for individual clusters.", "labels": [], "entities": []}, {"text": "The Pipeline We create two types of models: The window models are simple word space models which use LMI values based on counts of context words.", "labels": [], "entities": []}, {"text": "The clustering models apply soft clustering as a previous step to the determination of distributional similarity.", "labels": [], "entities": []}, {"text": "For their construction, we use the window-based models as an input to the LSC algorithm.", "labels": [], "entities": []}, {"text": "The clusters produced by LSC are used to create individual models for each cluster C in away that each of these cluster-specific models only contain vectors for the target words which are contained in C and represent only those features as dimensions which are predicted to be salient features for C.", "labels": [], "entities": []}, {"text": "The models vary with respect to the number of clusters created.", "labels": [], "entities": []}, {"text": "With this, we expect that in our example of Blatt|salat some clusters will capture the leafsense and others the sheet-or other senses.", "labels": [], "entities": []}, {"text": "The comparison between the vectors for Blatt and Blatt|salat is then done separately for each cluster, where the context dimensions of the vectors are reduced to only those context words which are also salient features of each cluster.", "labels": [], "entities": [{"text": "Blatt", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.8001991510391235}]}, {"text": "We expect that the pair of our example only occur in clusters which can be attributed to the leaf-sense.", "labels": [], "entities": []}, {"text": "Comparison across Clusters In cases like the NC Blatt|salat it appears that the word sense which should be considered for compositionality assessment is the one which is distributionally closest to the target MWE.", "labels": [], "entities": [{"text": "NC Blatt|salat", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.8696796596050262}]}, {"text": "But this is not necessarily the case for all MWEs.", "labels": [], "entities": []}, {"text": "The PV zu|schlagen is one example: it can mean both to hit hard and quickly or to take advantage of a good offer/bargain; in this case the MWE itself is ambiguous.", "labels": [], "entities": []}, {"text": "The base verb schlagen means to hit, so one sense of the PV is highly compositional and the other sense is less so; nevertheless none of the senses is predominant.", "labels": [], "entities": []}, {"text": "We use three methods to compare the distributional similarity across clusters: highest, lowest and average.", "labels": [], "entities": []}, {"text": "In the first two methods (highest/lowest) we select the cluster with the highest/lowest distributional similarity between \u00b5 and \u03ba and use its similarity value.", "labels": [], "entities": []}, {"text": "In the last method (average) the average similarity is computed among those clusters which contain both the MWE \u00b5 and the target component \u03ba, while clusters which do not contain the pair <\u00b5, \u03ba> are ignored.", "labels": [], "entities": []}, {"text": "Thresholds The fact that LSC outputs probabilities for both targets and features allows to set two different thresholds on these probabilities.", "labels": [], "entities": [{"text": "Thresholds", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9734650254249573}]}, {"text": "The threshold on the target output probability (tthreshold) controls the number of clusters to which a target element will be assigned.", "labels": [], "entities": []}, {"text": "The lower the threshold is set, the more elements each cluster will contain.", "labels": [], "entities": []}, {"text": "Lower threshold values also lead to higher average numbers of clusters to which each element is assigned.", "labels": [], "entities": []}, {"text": "The t-threshold influences the predictions of our models in that low values also increase the likelihood for each Cluster C and for each pair <\u03b1, \u03b2> of a MWE and a constituent word that both \u03b1 and \u03b2 are included in C.", "labels": [], "entities": []}, {"text": "The threshold on the feature output probability (f-threshold) allows to filter the vectors for both elements of <\u03b1, \u03b2> according to each cluster C so that only the dimensions representing the salient features for C are included in the vectors.", "labels": [], "entities": []}, {"text": "Corpus For the extraction of features we use the SdeWaC (v.3, 880 million words) corpus, in a tokenized), POS-tagged and lemmatized version.", "labels": [], "entities": [{"text": "SdeWaC (v.3, 880 million words) corpus", "start_pos": 49, "end_pos": 87, "type": "DATASET", "confidence": 0.6682174139552646}]}, {"text": "Gold Standards For NCs and PVs we use the following gold standards: \u2022 GS-NN: 868 German NCs (Schulte im) randomly selected from different frequency ranges, different ambiguity levels of the heads and different levels of modifier and head productivity.", "labels": [], "entities": [{"text": "GS-NN", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.9205538630485535}]}, {"text": "NCs were annotated by eight native speakers on a scale from 1 to 6 for compositionality with respect to both head and modifier constituents.", "labels": [], "entities": []}, {"text": "\u2022 GS-PV: 354 PVs, for 11 verb particles.", "labels": [], "entities": [{"text": "GS-PV", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.5772562623023987}]}, {"text": "PVs were randomly selected, balanced over 3 frequency bands.", "labels": [], "entities": []}, {"text": "The PVs were automatically Feature Sets We were interested in which parts of speech provide the best predictive features for compositionality.", "labels": [], "entities": []}, {"text": "We use only content-word categories: adjectives, nouns and verbs.", "labels": [], "entities": []}, {"text": "We use four different combinations: all content words and categories in isolation.", "labels": [], "entities": []}, {"text": "Measures Distributional similarity is measured with the cosine between vectors.", "labels": [], "entities": []}, {"text": "The cosine similarity values are used to rank the compared pairs from lowest to highest.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.6018615365028381}]}, {"text": "For the evaluation, system rankings and human judgment rankings of MWEs are compared to each other with Spearman's rank order correlation \u03c1 ().", "labels": [], "entities": [{"text": "rank order correlation \u03c1", "start_pos": 115, "end_pos": 139, "type": "METRIC", "confidence": 0.7737602144479752}]}, {"text": "Spearman's \u03c1 is a non-parametric measure which assesses monotonic relationships of ranks that range between -1 (inverse correlation) and 1 (perfect correlation); a \u03c1 value of 0 indicates alack of correlation.", "labels": [], "entities": [{"text": "inverse correlation) and 1 (perfect correlation)", "start_pos": 112, "end_pos": 160, "type": "METRIC", "confidence": 0.7615958087974124}]}, {"text": "Significance is determined with the use of the Fisher transformation.", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8968773484230042}]}, {"text": "Soft clustering does not guarantee that each of the pairs of NCs and a constituent word is placed together in at least one of the clusters.", "labels": [], "entities": []}, {"text": "This may potentially lead to problems of coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 41, "end_pos": 49, "type": "TASK", "confidence": 0.7867000102996826}]}, {"text": "In practice, however, we experience coverage problems only for very restrictive threshold settings.", "labels": [], "entities": [{"text": "coverage", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8440737724304199}]}, {"text": "The predictions of compositionality levels become more accurate with increasing window sizes.", "labels": [], "entities": []}, {"text": "For NC compositionality apparently more general information about the larger context plays an important role.", "labels": [], "entities": [{"text": "NC compositionality", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9426937401294708}]}, {"text": "Interestingly, no negative effect from larger contexts can be observed, even if smaller contexts tend to concentrate on closely related words such as complements, modifiers and the complementary parts of collocations in which the target word takes part.", "labels": [], "entities": []}, {"text": "All \u03c1 values above 0.108 are statistically highly significant (p<0.001 for n=868), which applies to nearly all of the observed values.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of German noun-noun compounds and German particle verbs, accompanied by trans- lations and human mean ratings on the degrees of compound-constituent compositionality.", "labels": [], "entities": []}]}