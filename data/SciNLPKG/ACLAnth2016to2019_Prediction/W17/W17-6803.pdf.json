{"title": [{"text": "Is Structure Necessary for Modeling Argument Expectations in Distributional Semantics?", "labels": [], "entities": [{"text": "Modeling Argument Expectations", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.8764086961746216}]}], "abstractContent": [{"text": "Despite the number of NLP studies dedicated to thematic fit estimation, little attention has been paid to the related task of composing and updating verb argument expectations.", "labels": [], "entities": [{"text": "thematic fit estimation", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.6260788242022196}]}, {"text": "The few exceptions have mostly modeled this phenomenon with structured distributional models, implicitly assuming a similarly structured representation of events.", "labels": [], "entities": []}, {"text": "Recent experimental evidence, however, suggests that human processing system could also exploit an unstructured \"bag-of-arguments\" type of event representation to predict upcoming input.", "labels": [], "entities": []}, {"text": "In this paper, we re-implement a traditional structured model and adapt it to compare the different hypotheses concerning the degree of structure in our event knowledge, evaluating their relative performance in the task of the argument expectations update.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important trend of current research in linguistics and cognitive science aims at investigating the mechanisms behind anticipatory processing in natural language (.", "labels": [], "entities": []}, {"text": "It is, indeed, uncontroversial that our cognitive system tries to predict incoming input on the basis of prior information, and this strategy is probably crucial for dealing with the rapidity of linguistic interactions.", "labels": [], "entities": []}, {"text": "By means of different experimental paradigms, several studies have focused on the role of event knowledge in the activation of expectations on verb arguments.", "labels": [], "entities": []}, {"text": "During sentence processing, verbs (arrest) activate expectations on their typical argument nouns (crook), and nouns do the same for other arguments frequently co-occurring in the same events (cop-crook).", "labels": [], "entities": []}, {"text": "The explanation proposed by these studies is that the human ability to anticipate the incoming input depends on general knowledge about events and their typical participants.", "labels": [], "entities": []}, {"text": "This knowledge, stored in the semantic memory, 'reacts' to the linguistic stimulus: the more the processed information is coherent with a prototypical event scenario, the easier for the comprehension system is to constrain the range of the events potentially described by the sentence and to predict the upcoming sentence arguments.", "labels": [], "entities": []}, {"text": "According to one of the most influential accounts of event-based prediction, the event representation includes both the thematic roles and the lexical meanings of the arguments, as well as the relations between different roles.", "labels": [], "entities": [{"text": "event-based prediction", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.6860063374042511}]}, {"text": "Therefore, there is the assumption of a structural distinction between the participants filling the roles () (i.e. some arguments are good agents, other are good patients etc.).", "labels": [], "entities": []}, {"text": "Such an account has been challenged by the experimental evidence fora 'bag-of-arguments' mechanism of verb predictions, discussed by.", "labels": [], "entities": [{"text": "verb predictions", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7255573570728302}]}, {"text": "Their experiments with Event-Related Potentials (ERPs) focused on the N400 component, 1 whose amplitude is generally interpreted as reflecting the predictability of a word in context (.", "labels": [], "entities": []}, {"text": "One of their findings was that there is no significant difference in the N400 amplitude at the target verb in sentences like (1a) and (1b) (normal vs. role-reversed argument configuration): (1) a.", "labels": [], "entities": [{"text": "N400 amplitude", "start_pos": 73, "end_pos": 87, "type": "METRIC", "confidence": 0.771633505821228}]}, {"text": "The restaurant owner forgot which customer the waitress had served during dinner yesterday. b. The restaurant owner forgot which waitress the customer had served during dinner yesterday.", "labels": [], "entities": []}, {"text": "That is to say, even if different roles are assigned to customer and waitress in (1a) and (1b), this difference seems to have no impact on the N400 amplitude.", "labels": [], "entities": []}, {"text": "Given the lack of influence of the structural roles, in order to circumscribe their hypothesis (which we will henceforth refer to as the bag-of-arguments hypothesis), the authors setup another experiment, in which they tested whether the predictions could be influenced also by other co-occurring words (i.e., not necessarily arguments; we will refer to this possibility as the bag-of-words hypothesis).", "labels": [], "entities": []}, {"text": "In order to carryout this test, they compared the amplitudes for sentences like (2a) and (2b) (argument substitution), finding a significantly smaller N400 component for the first sentence type.", "labels": [], "entities": []}, {"text": "The exterminator inquired which neighbor the landlord had evicted from the apartment complex. b. The neighbor inquired which exterminator the landlord had evicted from the apartment complex.", "labels": [], "entities": []}, {"text": "concluded that only the event arguments can influence predictions about a verb, and that arguments are represented in a sort of unstructured collection (i.e., bag-of-arguments).", "labels": [], "entities": []}, {"text": "Therefore, according to them, predictions would be sensitive to the meaning of the arguments, but not to their structural roles, which are computed later.", "labels": [], "entities": []}, {"text": "For example, the difference between typical agents and typical patients, according to this account, would not be included in the representation of an event.", "labels": [], "entities": []}, {"text": "In the last few years, a related issue has been debated in the field of distributional semantics, i.e. whether there is any added value in using structured representations of linguistic contexts over bag-ofwords ones (e.g., contexts represented as co-occurrence windows).", "labels": [], "entities": []}, {"text": "While structured models have been shown to outperform the latter in a number of semantic tasks, some bag-of-word models proved to be extremely competitive, at least under certain parameter settings (.", "labels": [], "entities": []}, {"text": "A recent paper by explicitly addressed the question of whether using structured distributional semantic models is worth the effort, by comparing the performance of syntax-based and window-based distributional models on four different tasks.", "labels": [], "entities": []}, {"text": "The authors showed that, even after extensive parameter tuning, the former have a significant advantage only in one task out of four (i.e., noun clustering).", "labels": [], "entities": [{"text": "noun clustering", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.7879887521266937}]}, {"text": "Interestingly, in the discussion they leave open the question of whether their results can generalize to linguistically challenging task such as the prediction of thematic fit ratings.", "labels": [], "entities": [{"text": "prediction of thematic fit ratings", "start_pos": 149, "end_pos": 183, "type": "TASK", "confidence": 0.7842979431152344}]}, {"text": "In this paper, we specifically investigate this point.", "labels": [], "entities": []}, {"text": "The main questions we want to address are: what are the implications of the bag-of-arguments hypothesis for current models of thematic fit?", "labels": [], "entities": []}, {"text": "More precisely, is it really necessary to have structured representations to carryout a thematic fit-related task, such as the argument expectation update?", "labels": [], "entities": []}, {"text": "In order to answer our questions, we implemented three models of argument expectations, adapting them to the above-mentioned hypotheses (i.e., structured and unstructured, the latter including both the bag-of-arguments and the bag-of-words hypothesis), and we compared their performance in a binary selection task.", "labels": [], "entities": []}, {"text": "One of the most influential distributional model of thematic fit was introduced by, who represented verb semantic roles with a prototype vector obtained by averaging the dependencybased vectors of the words typically filling those roles (i.e. the typical fillers).", "labels": [], "entities": []}, {"text": "Within the Distributional Memory (DM) framework, which was based on syntactic dependencies, Baroni and Lenci used grammatical functions such as subject and object to approximate the thematic roles of agent and patient, and they measured role typicality by means of a Local Mutual Information score) computed between verb, arguments and syntactic relations.", "labels": [], "entities": [{"text": "Distributional Memory (DM)", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.691178846359253}]}, {"text": "The basic assumption is that the higher the distributional similarity of a candidate argument with a role prototype, the higher its predictability as a filler for that role will be.", "labels": [], "entities": []}, {"text": "As a gold standard, the authors used the human-elicited thematic fit ratings collected by, and they evaluated the performance by measuring the correlation between these ratings and the scores generated by the model (as already proposed by).", "labels": [], "entities": []}, {"text": "Lenci (2011) later extended this 'structured-approach' to account for the dynamic update of the expectations on an argument, which depends on how other roles in the sentences are filled.", "labels": [], "entities": []}, {"text": "For instance, given the agent butcher the expected patient of the verb cut is likely to be meat, while given the agent coiffeur the expected patient of the same verb is likely to be hair.", "labels": [], "entities": []}, {"text": "By means of the same DM tensor, this study tested an additive and a multiplicative model to compose the distributional information coming from the agent and from the predicate of an agent-verb-patient triple (e.g., butcher-cut-meat), generating a prototype vector which represents the expectations on the patient filler, given the agent filler.", "labels": [], "entities": []}, {"text": "The triples of the Bicknell dataset (, which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task.", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.9389644265174866}]}, {"text": "Although the 'structured-approach' to thematic fit was influential fora number of other works, the task of modeling the update of the argument expectations has received relatively little attention.", "labels": [], "entities": []}, {"text": "An exception is the work by, who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation.", "labels": [], "entities": []}, {"text": "Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 188, "end_pos": 204, "type": "DATASET", "confidence": 0.9693891704082489}]}, {"text": "Notice that both the models of and necessarily rely on the hypothesis that the arguments are structurally distinct, since they are trained either on argument tuples containing fine-grained dependency information, or on sentences labeled with semantic roles.", "labels": [], "entities": []}, {"text": "Outside the specific area of study of thematic fit modeling, successfully used a type of unstructured representation for another sentence processing-related task, i.e. modeling N400 amplitudes with distributional spaces.", "labels": [], "entities": [{"text": "thematic fit modeling", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.6360041201114655}]}, {"text": "The authors proposed a method based on word2vec () to build vector representations of sentence context, and to quantify the relation of an upcoming target word to the context.", "labels": [], "entities": []}, {"text": "After training their word vectors on the Ukwac corpus () with the Skip-Gram architecture, they modeled the mental state of a comprehender at a certain point of a sentence as the average of the vectors of the words in the sentence up to that point.", "labels": [], "entities": [{"text": "Ukwac corpus", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.7937409579753876}]}, {"text": "The predictability of a target word in a sentence was measured as the cosine similarity between its vector, and the context-vector obtained by averaging the vectors of the preceding words.", "labels": [], "entities": []}, {"text": "Ettinger and colleagues tested their method on the sentences used in the ERP study by, in which three different conditions were defined, and they observed that the context-target similarity scores across conditions were following the same pattern of the N400 amplitudes of the original experiment.", "labels": [], "entities": []}, {"text": "Thus, this work shows how data on N400 variations can be modeled even by means of vectors with minimal or no syntactic information.", "labels": [], "entities": []}], "datasetContent": [{"text": "computed the thematic fit fora candidate filler (e.g., policeman) in an argument slot (e.g., agent) of an input lexical item (e.g., arrest) as the similarity score between the vector of the candidate filler and a prototype of the typical slot filler, built by summing the vectors for the top-k most typical fillers of input for slot (e.g., the typical agents for the arrest-event, such as cop, officer, policewoman, etc.).", "labels": [], "entities": []}, {"text": "In this model, syntactic relations were used to approximate verb-specific semantic roles and to identify the most typical fillers.", "labels": [], "entities": []}, {"text": "For example, the agent role is approximated by the subject relation, so that the typical f illers for the agent slot are the typical subjects of the input verb.", "labels": [], "entities": []}, {"text": "Similarly, the patient role is approximated by the object relation, so that the typical f illers for the patient slot are the typical objects of the input verb.", "labels": [], "entities": []}, {"text": "Once the expectations are calculated, the filler fit for the slot of input1, input2 can be computed by measuring the similarity (e.g., by vector cosine) between the filler and the expectations.", "labels": [], "entities": []}, {"text": "As an example, if we want to estimate how likely is burglar as a patient of the policeman arrested the..., we build a prototype out of the vectors of typical objects co-occurring with the subject policeman-n, then we do the same for the vectors of typical objects of the verb arrest-v, and finally we combine the prototype vectors through f (x), by either sum or multiplication.", "labels": [], "entities": []}, {"text": "At this point, we can estimate the filler fit by calculating the following similarity: Since distributional similarity is used as a measure of the predictability of a filler fora certain role, we expect that the thematic fit score of burglar for the patient slot of policeman, arrested will be much higher than for singer-n.", "labels": [], "entities": []}, {"text": "Indeed, burglars are more typical patients in this type of situation than singers are.", "labels": [], "entities": []}, {"text": "Notice that while in Lenci (2011) the update function modified the association scores between the predicate and the fillers, in the present case f (x) directly composes the prototype vectors associated with input1, input2.", "labels": [], "entities": []}, {"text": "In our experiments, we compared three different distributional semantic models (henceforth DSMs), all inspired by Lenci (2011): i) a structured model, which is similar to the one presented in Lenci (2011) (DEPS); ii) a variation of this system, modeling the bag-of-arguments hypothesis (BOA); iii) a baseline relying on the bag-of-words hypothesis (BOW).", "labels": [], "entities": [{"text": "BOW", "start_pos": 349, "end_pos": 352, "type": "METRIC", "confidence": 0.8477774858474731}]}, {"text": "The key difference between our models is to be found in the selection of the fillers (see): \u2022 DEPS: Similarly to Lenci's system, DEPS makes use of information on specific syntactic relations to select role fillers: the agent-role prototypes will be built out of the most typical subjects, the patient-role prototypes out of the most typical objects, and soon (see the last two rows of).", "labels": [], "entities": []}, {"text": "3 This means that not only the semantic information is taken in consideration (e.g. policeman), but also the thematic role of the filler (e.g. sbj:policeman, obj:policeman, etc.).", "labels": [], "entities": []}, {"text": "\u2022 BOA: Almost identical to the DEPS model, except for the fact that the most typical arguments are not bound to a specific syntactic slot.", "labels": [], "entities": [{"text": "BOA", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9983186721801758}, {"text": "DEPS", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.6732096672058105}]}, {"text": "Indeed, according to, the arguments of a verb like to serve (customer, waitress, tray, etc.) are represented like an unstructured collection.", "labels": [], "entities": []}, {"text": "In this type of model, thus, the top-k typical fillers will include all the strongly associated arguments, abstracting away from the specific syntactic relation.", "labels": [], "entities": []}, {"text": "\u2022 BOW: In this baseline, the typical fillers are not arguments, but words typically co-occurring with the targets in a window of fixed width (possibly having no syntactic relation to the targets).", "labels": [], "entities": [{"text": "BOW", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9974039196968079}]}, {"text": "Going back to the previous example, the core idea of the DEPS model is that processing a sentence fragment like the policeman arrested the...", "labels": [], "entities": []}, {"text": "leads to the activation only of the typical patients of such events, since the event knowledge is assumed to be structured.", "labels": [], "entities": []}, {"text": "Therefore, the predictability of an argument is measured in terms of its similarity with the prototype built out of the activated patients.", "labels": [], "entities": []}, {"text": "On the other side, the BOA model assumes no distinction between the arguments (i.e., whether they are agents, patients, locations or others), and consequently the sentence fragment above would activate all the typical arguments of the verb arrest.", "labels": [], "entities": [{"text": "BOA", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.96418696641922}]}, {"text": "This means that the predictability of an argument will be equivalent to its similarity with the prototype of a generic argument of the verb.", "labels": [], "entities": []}, {"text": "Finally, the BOW baseline has no notion of structure at all, not even the underspecified argument relation of the BOA model, and thus the prototypes of this model are just representations of the typical neighbors of the target words.", "labels": [], "entities": []}, {"text": "It should be recalled at this point that a bag-of-words account of prediction was ruled out by the experimental results by, since only arguments turned out to have an impact.", "labels": [], "entities": []}, {"text": "Nonetheless, since we have chosen a bag-of-words model with a very narrow window (i.e., two words on the left and right of the target), BOW could also capture indirectly syntactic information (i.e., words frequently co-occurring with the targets within a narrow window are very likely to be also syntactically related to them).", "labels": [], "entities": [{"text": "BOW", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.5219777822494507}]}, {"text": "Therefore, we expect it to be a reasonably strong baseline.", "labels": [], "entities": []}, {"text": "Distributional information is derived from the concatenation of the British National Corpus ( and of the Wacky () corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 68, "end_pos": 91, "type": "DATASET", "confidence": 0.9104965726534525}, {"text": "Wacky () corpus", "start_pos": 105, "end_pos": 120, "type": "DATASET", "confidence": 0.9200076460838318}]}, {"text": "Both were parsed with the Maltparser (Nivre and.", "labels": [], "entities": [{"text": "Maltparser", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.9315245747566223}]}, {"text": "From this concatenation, we built a dependency-based DSMs, where the tuples are weighted by means of Positive Local Mutual Information (PLMI, Evert).", "labels": [], "entities": []}, {"text": "Given the cooccurrence count O trf of the target t, the syntactic relation rand the filler f , we computed the expected count E trf (i.e., the simple joint probability of indipendent variables, corresponding to the product of the probabilities of the single events).", "labels": [], "entities": [{"text": "cooccurrence count O trf", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7064613103866577}, {"text": "expected count E trf", "start_pos": 111, "end_pos": 131, "type": "METRIC", "confidence": 0.8051645532250404}]}, {"text": "4 The DSM were built by means of the scripts of the DISSECT framework ( The PLMI for each target-relation-filler tuple is computed as follows: P LM I(t, r, f ) = max(LM I(t, r, f ), 0) Our DSM contains 28,817 targets (i.e., all nouns and verbs with frequency above 1000 in the training corpora), and all syntactic relations were included.", "labels": [], "entities": [{"text": "DISSECT framework", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9330820739269257}, {"text": "PLMI", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9278170466423035}]}, {"text": "We also built a window-based DSM to extract cooccurrence information for the BOW model, counting only the co-occurrences between the nouns and the verbs of the list above within a word window of width 2.", "labels": [], "entities": []}, {"text": "Prototypes The prototypes of all models were built out of the vectors of the k most typical fillers for each model type, and we tested 10, 20, 30, 40, and 50 as values of k.", "labels": [], "entities": []}, {"text": "As in previous studies, PLMI values were used as typicality scores: in the DEPS model, the typicality ranking of the fillers fora given role takes into account only the fillers occurring in the corresponding syntactic slot (e.g. the subject for the agent, the object for the patient etc.), whereas in the BOA model the typicality of a filler only depends on the PLMI score with the target, thus ignoring the type of syntactic relation.", "labels": [], "entities": [{"text": "DEPS", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8529248237609863}]}, {"text": "As for the BOW baseline, the words used for building the prototype are simply co-occurring with the targets within a word window of width 2, and such co-occurrences have been PLMI-weighted as well.", "labels": [], "entities": [{"text": "BOW baseline", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.7325307726860046}]}, {"text": "The compositional functions that we used to combine the prototypes are the vector sum and the pointwise vector multiplication).", "labels": [], "entities": []}, {"text": "An important difference between the compositional functions lies in the fact that, while the sum retains the dimensions that are not shared by both prototype vectors, the multiplication sets them to zero those dimensions.", "labels": [], "entities": []}, {"text": "This has an obvious impact on the computation of the cosine, as it could drastically reduce the number of dimensions on which the similarity score is computed.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 130, "end_pos": 146, "type": "METRIC", "confidence": 0.962352454662323}]}, {"text": "The models were tested on the datasets from the ERP experiments by and.", "labels": [], "entities": []}, {"text": "The Bicknell dataset was introduced to test the hypothesis that the typicality of a verb direct object depends on the subject argument.", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8933186829090118}]}, {"text": "With this purpose in mind, the authors selected 50 verbs, each paired with two agent nouns that significantly changed the scenario evoked by the subject-verb combination.", "labels": [], "entities": []}, {"text": "They obtained typical patients for each agent-verb pair by means of production norms, and they used such data to generate triples where the patient was congruent with the agent and with the verb.", "labels": [], "entities": []}, {"text": "For each congruent triple, an incongruent triple was generated as well, by combining each verb-congruent patient pair with the other agent noun, in order to have items describing atypical situations.", "labels": [], "entities": []}, {"text": "The final dataset is composed by 100 plausible-implausibile triples, which were used to build the sentences fora self-paced reading and for an ERP experiment.", "labels": [], "entities": []}, {"text": "The subjects were presented with sentence pairs such as: \u2022 The journalist checked the spelling of the last report.", "labels": [], "entities": [{"text": "spelling", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.8674269914627075}]}, {"text": "(plausible) \u2022 The mechanic checked the spelling of the last report.", "labels": [], "entities": [{"text": "spelling", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8349282145500183}]}, {"text": "reported shorter reading times and smaller N400 amplitudes for the plausible condition.", "labels": [], "entities": [{"text": "N400 amplitudes", "start_pos": 43, "end_pos": 58, "type": "METRIC", "confidence": 0.8882292211055756}]}, {"text": "The goal, fora thematic fit model of the argument expectations update, is to assign a higher cosine similarity score to the plausible triple, as in.", "labels": [], "entities": [{"text": "cosine similarity score", "start_pos": 93, "end_pos": 116, "type": "METRIC", "confidence": 0.7222698529561361}]}, {"text": "Moreover, Tilk et al.", "labels": [], "entities": []}, {"text": "(2016) evaluated their systems on two different versions of this task, since the triple pairs can be created by combining either triples differing only for the agent, or triples differing only for the patient.", "labels": [], "entities": []}, {"text": "Following the terminology from this latter study, we will refer to Accuracy 1 meaning the accuracy of the models in scoring differing-by-patient triples, and to Accuracy 2 meaning the accuracy in the classification of the differing-by-agent ones.", "labels": [], "entities": [{"text": "Accuracy 1", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9795625805854797}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9988284707069397}, {"text": "Accuracy 2", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9771482646465302}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9987886548042297}]}, {"text": "We also turned into similar triples the 50 verb-arguments combinations of the role reversal experiment by, by creating triple pairs corresponding to the normal and to the role-reversed condition.", "labels": [], "entities": [{"text": "role reversal", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.7801680862903595}]}, {"text": "For example, the sentences in Example (1) were turned into the form: customer-n waitress-n serve-v (normal) and waitress-n customer-n serve-v (role-reversed).", "labels": [], "entities": []}, {"text": "Notice that we preserved the order in which the experimental subjects saw the arguments and the verb, with the latter at the end.", "labels": [], "entities": []}, {"text": "Consequently, instead of composing the prototype vectors of the typical fillers of the patient role given an agent and a predicate, as we did for the Bicknell dataset, we derive the expectation vector for the verb from the composition of the prototypes of the typical predicates of the agent and of the patient.", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 150, "end_pos": 166, "type": "DATASET", "confidence": 0.9435452520847321}]}, {"text": "The binary selection task is the same used with the Bicknell dataset, the only difference being that the goal for our models is to assign higher scores to the triples in the standard argument configuration (i.e., the expectation vector should be closer to the verb vector in the normal condition).", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.9445770978927612}]}, {"text": "Only the DEPS results are reported for the Chow dataset, because unstructured models assign exactly the same score to normal and role-reversed triples (independently of the order in which the prototypes of the head verb for each argument are created, the combined prototype will be the same).", "labels": [], "entities": [{"text": "DEPS", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.5087081789970398}, {"text": "Chow dataset", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.7554219961166382}]}, {"text": "This is, of course, consistent with the report of the ERP experiment by Chow and colleagues, who found no differences in the N400 amplitudes elicited by the two sentence types.", "labels": [], "entities": []}, {"text": "The performance of the DEPS model on the Chow dataset is of particular interest, as the model has the structural information that is lacking in the other two.", "labels": [], "entities": [{"text": "DEPS", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.7580198049545288}, {"text": "Chow dataset", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.7823663353919983}]}, {"text": "If DEPS has to reproduce the N400 pattern found by Chow and colleagues, the scores for the normal and for the role-reversed conditions should not differ significantly.", "labels": [], "entities": [{"text": "DEPS", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.6847131848335266}]}], "tableCaptions": [{"text": " Table 4: Accuracy on Chow (100% coverage) for the DEPS model for different values of k", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991614818572998}, {"text": "Chow", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9565896391868591}, {"text": "coverage", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8350948691368103}, {"text": "DEPS", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.7475379705429077}]}]}