{"title": [{"text": "Data Augmentation for Visual Question Answering", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6374978721141815}, {"text": "Visual Question Answering", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6052248577276865}]}], "abstractContent": [{"text": "Data augmentation is widely used to train deep neural networks for image classification tasks.", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6769892871379852}, {"text": "image classification tasks", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.8197901248931885}]}, {"text": "Simply flipping images can help learning by increasing the number of training images by a factor of two.", "labels": [], "entities": []}, {"text": "However, data augmentation in natural language processing is much less studied.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7824034988880157}]}, {"text": "Here, we describe two methods for data augmentation for Visual Question Answering (VQA).", "labels": [], "entities": [{"text": "Question Answering (VQA)", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.8224874436855316}]}, {"text": "The first uses existing semantic annotations to generate new questions.", "labels": [], "entities": []}, {"text": "The second method is a genera-tive approach using recurrent neural networks.", "labels": [], "entities": []}, {"text": "Experiments show the proposed schemes improve performance of baseline and state-of-the-art VQA algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, both computer vision and natural language processing (NLP) have made enormous progress on many problems using deep learning.", "labels": [], "entities": []}, {"text": "Visual question answering (VQA) is a problem that fuses computer vision and NLP to build upon these successes.", "labels": [], "entities": [{"text": "Visual question answering (VQA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7684312264124552}]}, {"text": "In VQA, an algorithm is given an image and a question about the image, and it predicts the answer to the question (.", "labels": [], "entities": []}, {"text": "Although progress has been rapid, there is still a significant gap between the performance of the best VQA systems and humans.", "labels": [], "entities": []}, {"text": "For example, on the open-ended 'The VQA Dataset' that uses real images, the best systems in 2016 are at around 65% accuracy (e.g.,) compared to 83% for humans ().", "labels": [], "entities": [{"text": "VQA Dataset", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8613470792770386}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9986909031867981}]}, {"text": "Analysis of VQA algorithm performance as a function of the amount of training data show that existing algorithms would benefit greatly from more training data (Kafle and * Corresponding author..", "labels": [], "entities": []}, {"text": "One way to address this would be to annotate additional questions about images, but this is time-consuming and expensive.", "labels": [], "entities": []}, {"text": "Data augmentation is a much cheaper alternative.", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6983801424503326}]}, {"text": "Data augmentation is generating new training data from existing examples.", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6952120065689087}]}, {"text": "In this paper, we explore two data augmentation methods for generating new question-answer (QA) pairs for images.", "labels": [], "entities": []}, {"text": "The first method uses existing semantic annotations and templates to generate QA pairs, similar to the method in.", "labels": [], "entities": []}, {"text": "The second method is a generative approach using a recurrent neural network (RNN).", "labels": [], "entities": [{"text": "generative", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.9633148312568665}]}, {"text": "shows an example image from 'The VQA Dataset' along with the original questions and the questions generated using our methods.", "labels": [], "entities": [{"text": "VQA Dataset", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.8476805686950684}]}, {"text": "Our methods improve the variety and the number of questions for the image.", "labels": [], "entities": []}, {"text": "We evaluate how well each augmentation method performs on two VQA datasets.", "labels": [], "entities": [{"text": "VQA datasets", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.9474645555019379}]}, {"text": "Our results show that augmentation increases performance for both datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on two of the most popular VQA datasets: 'The VQA Dataset' ( and COCO-QA (.", "labels": [], "entities": [{"text": "VQA datasets", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9065858423709869}, {"text": "VQA Dataset", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9748946726322174}, {"text": "COCO-QA", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.8146435618400574}]}, {"text": "'The VQA Dataset' is currently the most popular VQA dataset and it contains both synthetic and real-world images.", "labels": [], "entities": [{"text": "VQA Dataset'", "start_pos": 5, "end_pos": 17, "type": "DATASET", "confidence": 0.9735218087832133}, {"text": "VQA dataset", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.9438954591751099}]}, {"text": "The real-world images are from the COCO dataset ().", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9659258723258972}]}, {"text": "All questions were generated by human annotators.", "labels": [], "entities": []}, {"text": "We refer to this portion as COCO-VQA, and use it for our experiments.", "labels": [], "entities": [{"text": "COCO-VQA", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.8661890029907227}]}, {"text": "COCO-QA () also uses images from COCO, with the questions generated by an NLP algorithm that uses COCO's captions.", "labels": [], "entities": [{"text": "COCO-QA", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.905220627784729}]}, {"text": "All questions belong to four categories: object, number, color, and location.", "labels": [], "entities": []}, {"text": "Many algorithms have been proposed for VQA.", "labels": [], "entities": [{"text": "VQA", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.6648959517478943}]}, {"text": "Some notable formulations include attention based methods (, Bayesian frameworks (, and compositional approaches.", "labels": [], "entities": []}, {"text": "Detailed reviews of existing methods can be found in and.", "labels": [], "entities": []}, {"text": "However, simpler models such as linear classifiers and multilayer perceptrons (MLPs) perform only slightly worse on many VQA datasets.", "labels": [], "entities": [{"text": "VQA datasets", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.8533139526844025}]}, {"text": "These baseline methods predict the answer using a vector of image features concatenated to a vector of question features.", "labels": [], "entities": []}, {"text": "We use the MLP model to conduct the bulk of the experiments, but we show that the proposed method is also effective on more sophisticated VQA systems like multimodal compact bilinear pooling (MCB) ().", "labels": [], "entities": [{"text": "multimodal compact bilinear pooling (MCB)", "start_pos": 155, "end_pos": 196, "type": "TASK", "confidence": 0.629185893705913}]}, {"text": "First, we use the simple MLP baseline model used in to assess the two data augmentation methods.", "labels": [], "entities": []}, {"text": "showed that MLP worked well across multiple datasets despite its simplicity.", "labels": [], "entities": [{"text": "MLP", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.8429238796234131}]}, {"text": "The MLP model treats VQA as a classification problem with concatenated image and question features given to the model as features and answers as categories.", "labels": [], "entities": []}, {"text": "CNN features from ResNet-152 ( ) and the skip-thought vectors () are used as image and question features respectively.", "labels": [], "entities": [{"text": "ResNet-152", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.8357037901878357}]}, {"text": "We evaluate the MLP model on COCO-VQA and COCO-QA datasets.", "labels": [], "entities": [{"text": "COCO-VQA", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9309019446372986}, {"text": "COCO-QA datasets", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9377172291278839}]}, {"text": "For COCO-QA, we excluded all the augmented QA pairs derived from COCO's validation images during training, as the test portion of COCO-QA contains questions for these images.", "labels": [], "entities": [{"text": "COCO-QA", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8132079243659973}]}, {"text": "shows the results for the MLP model when trained with and without augmentation.", "labels": [], "entities": [{"text": "MLP", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.4006597697734833}]}, {"text": "Some examples for the model trained with augmentation are are shown in.", "labels": [], "entities": []}, {"text": "Next, to demonstrate that the data augmentation scheme also helps improve more complex models, we train the state-of-the-art MCB model with attention (MCB+Att.+GloVe) ( with the template augmentation and compare the accuracy when the same model trained only on the COCO-VQA dataset.", "labels": [], "entities": [{"text": "Att.+GloVe)", "start_pos": 155, "end_pos": 166, "type": "METRIC", "confidence": 0.8227122724056244}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9993988275527954}, {"text": "COCO-VQA dataset", "start_pos": 265, "end_pos": 281, "type": "DATASET", "confidence": 0.9780188202857971}]}], "tableCaptions": [{"text": " Table 1: Number of questions in COCO-VQA compared to the number generated using the LSTM and  template methods.", "labels": [], "entities": [{"text": "COCO-VQA", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.784921407699585}]}, {"text": " Table 2: Results on COCO-VQA (test-dev) and  COCO-QA datasets for the MLP model trained  with and without augmentation.", "labels": [], "entities": [{"text": "COCO-VQA", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.7722784280776978}, {"text": "COCO-QA datasets", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9343492984771729}]}, {"text": " Table 3: Results on COCO-VQA (test-dev) for the  MCB+Att.+GloVe model trained with and without  template augmentation.", "labels": [], "entities": [{"text": "MCB+Att.+GloVe model", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.7533402542273203}]}]}