{"title": [{"text": "Supervised Methods for Ranking Relations in Web Search", "labels": [], "entities": [{"text": "Ranking Relations in Web Search", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.8499803185462952}]}], "abstractContent": [{"text": "In this paper we propose an efficient technique for ranking triples of knowledge base using information of full text.", "labels": [], "entities": []}, {"text": "We devise supervised machine learning algorithms to compute the relevance scores for item-property pairs where an item can have more than one value.Such a score measures the degree to which an entity belongs to a type, and this plays an important role in ranking the search results.", "labels": [], "entities": []}, {"text": "The problem is, in itself, new and not explored so much in the literature, possibly because of the heterogeneous behaviors of both semantic knowledge base and full-text articles.", "labels": [], "entities": []}, {"text": "The classifiers exploit statistical features computed from the Wikipedia articles and the semantic information obtained from the word embedding concepts.", "labels": [], "entities": []}, {"text": "We develop models based on traditional supervised models like Suport Vector Machine (SVM) and Random Forest (RF); and then using deep Convolution Neu-ral Network (CNN).", "labels": [], "entities": []}, {"text": "We perform experiments as provided by WSDM cup 2017, which provides about 1k human judgments of person-profession pairs.", "labels": [], "entities": [{"text": "WSDM cup 2017", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9563952684402466}]}, {"text": "Evaluation shows that machine learning based approaches produce encouraging performance with the highest accuracy of 71%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9993792772293091}]}, {"text": "The contributions of the current work are twofold , viz.", "labels": [], "entities": []}, {"text": "we focus on a problem that has not been explored much, and show the usage of powerful word-embedding features that produce promising results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most of the prior works in information retrieval (IR) focuses on retrieving information either using semantic knowledge base or text.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.8823286771774292}]}, {"text": "In the present day, Information Retrieval (IR) often involves both knowledge base as well as full text search.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.8792769312858582}]}, {"text": "One cannot succeed in retrieving semantic information with the other.Knowledge base is good at returning precise information, whereas full-text has the benefit of a wide information coverage, for example, Wikipedia articles.", "labels": [], "entities": []}, {"text": "Therefore, it is imperative that search uses information from both of the above and tries to find a best approximation.", "labels": [], "entities": []}, {"text": "In our current work we discuss the problem to rank, not entities from a full text search but triples from knowledge bases with the same subject and predicate properties.", "labels": [], "entities": []}, {"text": "Let us consider all the professions of a particular person, for example of Arnold Schwarzenegger: Actor, Athlete, Bodybuilder, Businessperson, Entrepreneur, Film Producer, Investor, Politician, Television Director, Writer.", "labels": [], "entities": []}, {"text": "All of them follow: \"Arnold Schwarzenegger-professionProfessionName\", but some of these are more relevant and prominent whereas others are less.", "labels": [], "entities": []}, {"text": "Hence it would be good to come up with a metric to segregate the most-relevant ones' from the less-relevant ones'.", "labels": [], "entities": []}, {"text": "The concept of relevance in itself is ambiguous.", "labels": [], "entities": []}, {"text": "So here we take the basis as the amount of information in the Wikipedia article of the entity.", "labels": [], "entities": []}, {"text": "This type of relevance plays an important role in improving search engines as well as knowledge bases upon which several question-answering systems are being built.", "labels": [], "entities": []}, {"text": "For example, all three tasks from the TREC 2011 Entity Track ask for the lists of entities of a particular type.", "labels": [], "entities": [{"text": "TREC 2011 Entity Track", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.8450144529342651}]}, {"text": "It is to be noted that ranking of triples using both semantic knowledge base and fulltext articles is not explored at the required level.", "labels": [], "entities": []}, {"text": "In order to solve this problem we at first propose models based on supervised machine learning algorithms, namely Support Vector Machine (SVM)and Random Forest (RF).", "labels": [], "entities": []}, {"text": "Therafter, we develop model based on deep Convolutional Neural Network (CNN).", "labels": [], "entities": []}], "datasetContent": [{"text": "The problem that we tackle is related to ranking the relevance of person-profession pairs based on the information present in Wikipedia.", "labels": [], "entities": []}, {"text": "This is an example of a non-functional relation between an entity and an abstract group.", "labels": [], "entities": []}, {"text": "We have a set of person names and their associated professions from.", "labels": [], "entities": []}, {"text": "The goal is to predict a score for each person-profession relation between 0-7, with 7 being the most relevant.", "labels": [], "entities": []}, {"text": "A typical set of training examples is: Wolfgang Amadeus Mozart Composer 7 Wolfgang Amadeus Mozart Pianist 5 Wolfgang Amadeus Mozart Violinist 2 We use the dataset of WSDM cup-2017 triple 1 scoring task, which provides a training and test set comprising of 1,225 person-profession pairs.", "labels": [], "entities": [{"text": "WSDM cup-2017 triple 1 scoring task", "start_pos": 166, "end_pos": 201, "type": "DATASET", "confidence": 0.8355522950490316}]}, {"text": "The labels had been obtained via crowd-sourcing wherein 7 independent judges rated each profession fora person as relevant or non-relevant.", "labels": [], "entities": []}, {"text": "The scores of these 7 judges were then added to form the composite score described above.", "labels": [], "entities": []}, {"text": "The training sets (the .train files provided above) contain only tuples from the respective .kb files.", "labels": [], "entities": []}, {"text": "The person names are exactly the names used by the English Wikipedia.", "labels": [], "entities": []}, {"text": "That is, http://en.wikipedia.org/wiki/PersonName takes you to the respective Wikipedia page.", "labels": [], "entities": []}, {"text": "For each of the names in persons, there are sentences in wiki-sentences ( 68,662 sentences for the most frequently mentioned person, 3 sentences for the least frequently mentioned person ).", "labels": [], "entities": []}, {"text": "We use three metrics to measure the efficiency of the baseline and the proposed models.", "labels": [], "entities": []}, {"text": "1. Accuracy -The percentage of personprofession triples that matched.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9982491731643677}]}, {"text": "2. Kendall's tau -\u03c4 p = 1/Z(n d +p.n t ) where n dis the number of discordant (inverted) pairs, n t is the number of pairs that are tied in the gold standard but not in the predicted ranking or vice versa, p is a penalization factor for these pairs which we set to 0.5, and the normalization factor Z (the number of ordered pairs plus p times the number of tied pairs in the gold standard).", "labels": [], "entities": []}, {"text": "This is to account for the tied rankings in the gold standardFagin et al.", "labels": [], "entities": []}, {"text": "3. Average Score Difference -Average of the difference of scores between gold mention and predictions.", "labels": [], "entities": [{"text": "Average Score Difference -Average", "start_pos": 3, "end_pos": 36, "type": "METRIC", "confidence": 0.87981675863266}]}, {"text": "The word vectors were trained on the wiki-sentences to get the context from the Wikipedia mentions.", "labels": [], "entities": []}, {"text": "The use of word embedding vectors improves the accuracy to some extent, and greatly helps in deriving more contextual information for 25% profession words for which we had very less person mentions from Freebase.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9991967082023621}]}, {"text": "This shows that they can be used in places where we have insufficient information in semantic space to derive context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset description  Filename  Description  professions  the 200 different professions  from professions.kb  professions.kb  all professions for a set of  343,329 persons  profession.train relevance scores for 515 tuples  (pertaining to 134 persons) from  profession.kb  persons  385,426 different person names  from the two .kb files and their  Freebase ids  wiki-sentences 2 33,159,353 sentences from  Wikipedia with annotations of  these 385,426 persons  profession.test  relevance scores for 513 tuples  (pertaining to 134 persons) from  profession.kb", "labels": [], "entities": []}, {"text": " Table 2: Scores for methods without word vectors(accuracy represents exact matches)  Method  Accuracy(\u03b4=0) Average Score Difference Kendall's Tau  Counting(Baseline)  0.68  1.92  0.42  SVM  0.69  1.86  0.37  Random Forest  0.71  1.80  0.34", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9955499172210693}, {"text": "Accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.6147444844245911}, {"text": "Average Score Difference Kendall's Tau  Counting", "start_pos": 108, "end_pos": 156, "type": "METRIC", "confidence": 0.909503528050014}, {"text": "SVM  0.69  1.86  0.37  Random Forest  0.71  1.80  0.34", "start_pos": 186, "end_pos": 240, "type": "DATASET", "confidence": 0.6750051710340712}]}, {"text": " Table 3: Scores for methods(accuracy represents exact matches)  Method  Accuracy(\u03b4=0) Average Score Difference Kendall's Tau  Counting(Baseline)  0.69  1.90  0.39  SVM  0.70  1.86  0.35  Random Forest  0.71  1.78  0.33", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.995452344417572}, {"text": "Accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.5690557360649109}, {"text": "Average Score Difference Kendall's Tau  Counting(Baseline)  0.69  1.90  0.39", "start_pos": 87, "end_pos": 163, "type": "METRIC", "confidence": 0.8373014262089362}, {"text": "SVM  0.70  1.86  0.35  Random Forest  0.71  1.78  0.33", "start_pos": 165, "end_pos": 219, "type": "DATASET", "confidence": 0.7155733638339572}]}]}