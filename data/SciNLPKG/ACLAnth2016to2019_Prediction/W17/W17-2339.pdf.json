{"title": [{"text": "Initializing neural networks for hierarchical multi-label text classification", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6145219703515371}]}], "abstractContent": [{"text": "Many tasks in the biomedical domain require the assignment of one or more pre-defined labels to input text, where the labels area part of a hierarchical structure (such as a taxonomy).", "labels": [], "entities": []}, {"text": "The conventional approach is to use a one-vs.-rest (OVR) classification setup, where a binary clas-sifier is trained for each label in the tax-onomy or ontology where all instances not belonging to the class are considered negative examples.", "labels": [], "entities": []}, {"text": "The main drawbacks to this approach are that dependencies between classes are not leveraged in the training and classification process, and the additional computational cost of training parallel classifiers.", "labels": [], "entities": []}, {"text": "In this paper, we apply anew method for hierarchical multi-label text classification that initializes a neural network model final hidden layer such that it leverages label co-occurrence relations such as hypernymy.", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.606006900469462}]}, {"text": "This approach elegantly lends itself to hierarchical classification.", "labels": [], "entities": [{"text": "hierarchical classification", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6365063637495041}]}, {"text": "We evaluated this approach using two hierarchical multi-label text classification tasks in the biomedical domain using both sentence-and document-level classification.", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.7035905917485555}, {"text": "sentence-and document-level classification", "start_pos": 124, "end_pos": 166, "type": "TASK", "confidence": 0.5960239271322886}]}, {"text": "Our evaluation shows promising results for this approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many tasks in biomedical natural language processing require the assignment of one or more labels to input text, where there exists some structure (such as a taxonomy or ontology) between the labels: for example, the assignment of Medical Subject Headings (MeSH) to PubMed abstracts.", "labels": [], "entities": [{"text": "biomedical natural language processing", "start_pos": 14, "end_pos": 52, "type": "TASK", "confidence": 0.629742220044136}, {"text": "assignment of Medical Subject Headings (MeSH) to PubMed abstracts", "start_pos": 217, "end_pos": 282, "type": "TASK", "confidence": 0.8187047134746205}]}, {"text": "A typical approach to classifying multi-label documents is to construct a binary classifier for each label in the taxonomy or ontology where all documents not belonging to the class are considered negative examples, i.e. one-vs.-rest (OVR) classification.", "labels": [], "entities": []}, {"text": "This approach has two major drawbacks: first, it makes the hard assumption that the classes are independent which often does not reflect reality; second, it is more computationally expensive (albeit by a constant factor): if there area very large number of classes, the approach becomes computationally unrealistic.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a simple and computationally fast approach for multi-label classification with a focus on labels that share a structure, such as a hierarchy (taxonomy).", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7811861932277679}]}, {"text": "This approach can work with established neural network architectures such as a convolutional neural network (CNN) by simply initializing the final output layer to leverage the co-occurrences between the labels in the training data.", "labels": [], "entities": []}, {"text": "Nodes represent possible labels that can be assigned to text: a dark grey node denotes an explicit label assignment and light grey denotes implicit assignment due to a hypernymy relationship with the explicitly assigned label.", "labels": [], "entities": []}, {"text": "First, we need to define hierarchical multi-label classification.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.674734890460968}]}, {"text": "In multi-label text classification, input text can be associated with multiple labels (label co-occurrence).", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6798789103825887}]}, {"text": "When the labels form a hierarchy, they share a hypernym-hyponym relation ().", "labels": [], "entities": []}, {"text": "When multiple labels are assigned to the text, if it is explicitly labeled by a subclass it must also implicitly include all of the its superclasses.", "labels": [], "entities": []}, {"text": "The co-occurrence between subclasses and superclasses as labels for the input text contains information we would like to leverage to improve multi-label classification using a neural network.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 141, "end_pos": 167, "type": "TASK", "confidence": 0.7945401966571808}]}, {"text": "In this paper we experiment with this approach using two hierarchical multi-label text classification tasks in the biomedical domain, using both document-and sentence-level classification.", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7016842365264893}, {"text": "document-and sentence-level classification", "start_pos": 145, "end_pos": 187, "type": "TASK", "confidence": 0.5774427056312561}]}, {"text": "We first briefly summarize related literature on the topic of multi-label classification using neural networks, we then describe our methodology and evaluation procedure, and then we present and discuss our results.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.8140720427036285}]}], "datasetContent": [{"text": "In this section, we describe our experimental setup and our baselines.", "labels": [], "entities": []}, {"text": "We ascertain the performance of our approach under a controlled experimental setup.", "labels": [], "entities": []}, {"text": "We compare two baseline models (described in the next section), and two variants of the initialization models corresponding to the two initialization schemes described in.", "labels": [], "entities": []}, {"text": "We will refer to the first scheme (allocating all units in the final hidden layer to representing label co-occurrences and zeroing all other units) as INIT-A, and the second scheme (allocating a random value drawn from a uniform distribution for non co-occurrence hidden units) as INIT-B.", "labels": [], "entities": [{"text": "INIT-B", "start_pos": 281, "end_pos": 287, "type": "DATASET", "confidence": 0.8440262675285339}]}, {"text": "We use the hyperparameters in and data splits in for all models.", "labels": [], "entities": []}, {"text": "We check the model's performance (F 1 -score) on development data at the end of every epoch.", "labels": [], "entities": [{"text": "F 1 -score)", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9841754317283631}]}, {"text": "We select the model from the best-performing epoch and train it until its performance does not improve for ten epochs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Our baseline model, based on Kim (2014)  model hyperparameters.", "labels": [], "entities": []}, {"text": " Table 2: Summary statistics for Tasks 1 and 2.", "labels": [], "entities": []}, {"text": " Table 3: Jaccard similarity scores (expressed as  percentages) between label pairs.", "labels": [], "entities": [{"text": "Jaccard similarity scores", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7787826657295227}]}, {"text": " Table 4: Performance results for Tasks 1 and 2.  All figures are micro-averages expressed as per- centages.", "labels": [], "entities": []}, {"text": " Table 5: The proportion (%) of exact matches.", "labels": [], "entities": []}, {"text": " Table 6: Post-processing label correction. O is  the predicted output, T is transitive correction, and  R is retractive correction. All figures are micro- averaged F 1 -scores expressed as percentages.", "labels": [], "entities": [{"text": "Post-processing label correction", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.5771245161692301}, {"text": "micro- averaged F 1 -scores", "start_pos": 149, "end_pos": 176, "type": "METRIC", "confidence": 0.8159247636795044}]}]}