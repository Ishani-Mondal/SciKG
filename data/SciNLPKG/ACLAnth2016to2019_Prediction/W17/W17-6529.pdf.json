{"title": [{"text": "Unity in Diversity: A unified parsing strategy for major Indian languages", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents our work to apply nonlinear neural network for parsing five r esource poor Indian L anguages belonging to two major language families-Indo-Aryan and Dravidian.", "labels": [], "entities": []}, {"text": "Bengali and Marathi are Indo-Aryan languages whereas Kannada, Telugu and Malayalam belong to the Dravidian family.", "labels": [], "entities": []}, {"text": "While little work has been done previously on Bengali and Telugu linear transition-based parsing, we present one of the first parsers for Marathi, Kannada and Malayalam.", "labels": [], "entities": [{"text": "linear transition-based parsing", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.5350117286046346}]}, {"text": "All the Indian languages are free word order and range from being moderate to very rich in morphology.", "labels": [], "entities": []}, {"text": "Therefore in this work we propose the usage of linguistically motivated morphological features (suffix and postposition) in the nonlinear framework, to capture the intricacies of both the language families.", "labels": [], "entities": []}, {"text": "We also capture chunk and gender, number, person information elegantly in this model.", "labels": [], "entities": []}, {"text": "We put forward ways to represent these features cost effectively using monolingual distributed em-beddings.", "labels": [], "entities": []}, {"text": "Instead of relying on expensive morphological analyzers to extract the information , these embeddings are used effectively to increase parsing accuracies for resource poor languages.", "labels": [], "entities": []}, {"text": "Our experiments provide a comparison between the two language families on the importance of varying morphological features.", "labels": [], "entities": []}, {"text": "Part of speech taggers and chunkers for all languages are also builtin the process.", "labels": [], "entities": [{"text": "speech taggers", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.7349236309528351}]}], "introductionContent": [{"text": "Over the years there have been several successful attempts in building data driven dependency parsers using rich feature templates) requiring a lot of feature engineering expertise.", "labels": [], "entities": []}, {"text": "Though these indicative features brought enormously high parsing accuracies, they were computationally expensive to extract and also posed the problem of data sparsity.", "labels": [], "entities": []}, {"text": "To address the problem of discrete representations of words, distributional representations became a critical component of NLP tasks such as POS tagging), constituency parsing () and machine translation).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 141, "end_pos": 152, "type": "TASK", "confidence": 0.8669097423553467}, {"text": "constituency parsing", "start_pos": 155, "end_pos": 175, "type": "TASK", "confidence": 0.8228167295455933}, {"text": "machine translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.8302282094955444}]}, {"text": "The distributed representations are shown to be more effective in non-linear architectures compared to the traditional linear classifier (.", "labels": [], "entities": []}, {"text": "Keeping inline with this trend,) introduced a compact neural network based classifier for use in a greedy, transition-based dependency parser that learns using dense vector representations not only of words, but also of part-of-speech (POS) tags, dependency labels, etc.", "labels": [], "entities": []}, {"text": "In our task of parsing Indian languages, a similar transition-based parser based on their model has been used.", "labels": [], "entities": [{"text": "parsing Indian languages", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.914698580900828}]}, {"text": "This model handles the problem of sparsity, incompleteness and expensive feature computation).", "labels": [], "entities": []}, {"text": "The last decade has seen quite a few attempts at parsing Indian languages Hindi, Telugu and Bengali ().", "labels": [], "entities": [{"text": "parsing Indian languages Hindi", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.8799081891775131}]}, {"text": "The research in this direction majorly focused on data driven transition-based parsing using MALT (, MST parser) or constraint based method ( Marathi, Telugu, Kannada, Malayalam.", "labels": [], "entities": [{"text": "MALT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9034960269927979}]}, {"text": "These languages belong to two major language families, Indo-Aryan and Dravidian.", "labels": [], "entities": []}, {"text": "The Dravidian languages -Telugu, Kannada and Malayalam are highly agglutinative.", "labels": [], "entities": []}, {"text": "The rich morphological nature of a language can prove challenging fora statistical parser as is noted by.", "labels": [], "entities": []}, {"text": "For morphologically rich, free word order languages high performance can be achieved using vibhakti and information related to tense, aspect, modality (TAM).", "labels": [], "entities": []}, {"text": "Syntactic features related to case and TAM marking have been found to be very useful in previous works on dependency parsing of Hindi ().", "labels": [], "entities": [{"text": "TAM marking", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.802614152431488}, {"text": "dependency parsing of Hindi", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.8651886731386185}]}, {"text": "We decided to experiment with these features for other Indian languages too as they follow more or less the same typology, all being free order and ranging from being moderate to very morphologically rich.", "labels": [], "entities": []}, {"text": "We propose an efficient way to incorporate this information in the aforementioned neural network based parser.", "labels": [], "entities": []}, {"text": "In our model, these features are included as suffix (last 4 characters) embeddings for all nodes.", "labels": [], "entities": []}, {"text": "Lexical embeddings of case and TAM markers occurring in all the chunk are also included.", "labels": [], "entities": [{"text": "TAM", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9414782524108887}]}, {"text": "We also include chunk tags and gender, number, person information as features in our model.", "labels": [], "entities": []}, {"text": "Taking cue from previous works where the addition of chunk tags 2 (Ambati et al., 2010a) and grammatical agreement ( has been proven to help Hindi and Urdu, our experiments test their effectiveness for other 5 languages in concern.", "labels": [], "entities": []}, {"text": "Computationally, obtaining chunk tags can be done with ease.", "labels": [], "entities": []}, {"text": "However, acquiring information related to gender, number, person for new sentences remains a challenge if we aim to parse resource poor languages for which sophisticated tools do not exist.", "labels": [], "entities": []}, {"text": "We show that adding both these features definitely increases accuracy but we are able to gain major advantage by just using the lexical features, suffix features and POS tags which can be readily made available for low resource languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9987857937812805}]}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we talk about the data and the dependency scheme followed.", "labels": [], "entities": []}, {"text": "Section 3 provides the 1 vibhakti is a generic term for postposition and suffix that represent case marking 2 a chunk is a set of adjacent words which are in dependency relation with each other, and are connected to the rest of the words by a single incoming arc to the chunk rationale behind using each feature taking into account language diversity.", "labels": [], "entities": []}, {"text": "Section 4 details about feature representations, models used and the experiments conducted.", "labels": [], "entities": []}, {"text": "In Section 5 we observe the effects of inclusion of rich morpho-syntactic features on different languages and back the results with linguistic reasoning.", "labels": [], "entities": []}, {"text": "In Section 6 we conclude and talk about future directions of research our work paves the way for.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we focus on establishing dependency relations between the chunk heads which we henceforth denote as inter-chunk parsing.", "labels": [], "entities": [{"text": "inter-chunk parsing", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.6897443532943726}]}, {"text": "The relations between the tokens of a chunk (intra-chunk dependencies) are not considered for experimentation as they can easily be predicted automatically using a finite set of rules ().", "labels": [], "entities": []}, {"text": "Moreover we also observed the high learnability of intra-chunk relations from an initial experiment.", "labels": [], "entities": []}, {"text": "We found the accuracies of intrachunk dependencies to be more than 99.00% for both Labeled Attachment and Unlabeled Attachment.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9894132614135742}]}, {"text": "The treebanks available to us are in the SSF format ().", "labels": [], "entities": []}, {"text": "We use in house built tool to convert from SSF to CoNLL format.", "labels": [], "entities": []}, {"text": "This tool uses head and vibhakti computation tools as its dependencies.", "labels": [], "entities": []}, {"text": "The head computation tool finds the head of a chunk based on certain rules written using POS tag information of nodes.", "labels": [], "entities": []}, {"text": "The vibhakti computation module is again a simple, rule based tool that uses POS tag information to decide whether a lexical unit qualifies as a postposition or not.", "labels": [], "entities": []}, {"text": "It then augments the head of the chunk with its postpositional features in the SSF format.", "labels": [], "entities": []}, {"text": "Our parser uses data in the converted CoNLL format.", "labels": [], "entities": [{"text": "CoNLL format", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.8947061598300934}]}, {"text": "We use the arc-eager parsing model for parsing sentences containing projective arcs only, discarding the non-projective sentences.", "labels": [], "entities": [{"text": "parsing sentences containing projective arcs", "start_pos": 39, "end_pos": 83, "type": "TASK", "confidence": 0.8808707356452942}]}, {"text": "The data set is split in the ratio of 80-10-10 for training, testing and tuning the parsing model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9678401350975037}]}, {"text": "Baseline for parsing is set using a delexicalised model having only POS tags as features . We explore with different feature sets by adding features like words, suffix, chunk tags and GNP information one by one.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9709481000900269}]}, {"text": "These features are represented as described below.", "labels": [], "entities": []}, {"text": "In order to parse in more realistic settings, we also show parsing results using predicted POS and chunk tags obtained from the models discussed below.", "labels": [], "entities": [{"text": "parse", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.9664073586463928}]}, {"text": "We report auto accuracy of the parsing model on the same training, development and testing sets that are used for parsing with gold tags.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.976068377494812}, {"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9810048937797546}, {"text": "parsing", "start_pos": 114, "end_pos": 121, "type": "TASK", "confidence": 0.9740394353866577}]}], "tableCaptions": [{"text": " Table 1: Treebank statistics for the 5 languages  used in the experiments", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of Chunker and POS Model for  Kannada (Kan), Malayalam (Mal), Bengali (Ben),  Marathi (Mar), Telugu (Tel) Bis and Anncorra  (Ann.) tagset. D=Development Set, T=Test Set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986945986747742}, {"text": "Anncorra  (Ann.) tagset", "start_pos": 133, "end_pos": 156, "type": "DATASET", "confidence": 0.8542827844619751}]}, {"text": " Table 4: Parsing accuracies of our neural network based parser for all 5 languages. Auto development  and test set contain predicted POS and chunk tags. Gloss of the features are f1 = POS only, f2 = f1+  suffix, f3 = POS + word, f4 = f3 + suffix , f5 = f4+ PSP, f6 = f5 + chunk, f7 = f6 + GNP", "labels": [], "entities": []}]}