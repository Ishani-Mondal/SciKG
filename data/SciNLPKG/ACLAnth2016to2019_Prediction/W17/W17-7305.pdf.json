{"title": [{"text": "Extracting tags from large raw texts using End-to-End memory networks", "labels": [], "entities": [{"text": "Extracting tags from large raw texts", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8804079790910085}]}], "abstractContent": [{"text": "Recently, new approaches based on Deep Learning have demonstrated good capacities to manage Natural Language Processing problems.", "labels": [], "entities": []}, {"text": "In this paper, after selecting End-to-End Memory Networks for its ability to efficiently capture context meanings, we study its behavior when facing large semantic problems (large texts, large vocabulary sets) and apply it to automatically extract tags from a website.", "labels": [], "entities": []}, {"text": "A new data set is proposed, results and parameters are discussed.", "labels": [], "entities": []}, {"text": "We show that the so formed system can capture the correct tags most of the time, and can bean efficient and advantageous way to complement other approaches because of its ability for generalization and semantic abstraction.", "labels": [], "entities": []}], "introductionContent": [{"text": "To automatically extract the meaning of a web page or a raw text is still a deep challenge.", "labels": [], "entities": [{"text": "automatically extract the meaning of a web page or a raw text", "start_pos": 3, "end_pos": 64, "type": "TASK", "confidence": 0.733417235314846}]}, {"text": "Rulebased approaches which have been applied for years are efficient but can't manage easily diversity, and suffer from hand-made drawbacks like the difficulty to model a rich word built of complex interacting semantical concepts.", "labels": [], "entities": []}, {"text": "Natural Language Processing (NLP) based on Deep Learning have recently been applied to capture the meaning of texts, and to build inferences so that it could be possible, as along term goal, to have a full and natural discussion with such a system ().", "labels": [], "entities": []}, {"text": "In this paper, after covering some of the deep learning methods to address NLP problem's, we select a promising End-to-End Memory Networks approach for its ability to capture the meaning of complex words associations.", "labels": [], "entities": []}, {"text": "As presented in section 2, this approach has been designed to preserve the memory of texts acquired.", "labels": [], "entities": []}, {"text": "For our study, we apply the approach to a large problem, so that we can see how the End-toEnd Memory Networks can be applied to the Web.", "labels": [], "entities": []}, {"text": "We choose to study long texts (biographies), with a large set of words leading to a high memory and computing consumption.", "labels": [], "entities": [{"text": "memory", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9630774855613708}]}, {"text": "The section 4 introduces and motivates the elaboration of anew dataset based on biographies proposed by the website http://biography.com.", "labels": [], "entities": []}, {"text": "Experimental results and parameters are then discussed in section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes and introduces some perspectives.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of this implementation is to test the limits of the end-to-end memory network and apply it on a complex and large data set extracted from a website, observing the results and evaluating the difficulties to reach the best solution.", "labels": [], "entities": []}, {"text": "This algorithm needs to tune many parameters (the memory size, the embedding dimensions and the number of epochs).", "labels": [], "entities": []}, {"text": "We tried two strategies to deal with the parameters: the first one was to vary values for the memory size, the number of epochs and the embedding dimensions.", "labels": [], "entities": []}, {"text": "The memory loads be the whole story, so each time the story is longer than the memory the model only keep the last sentences.", "labels": [], "entities": []}, {"text": "We used 4500 profiles for the training with one question for every profile and 1500 profiles for the testing).", "labels": [], "entities": []}, {"text": "Next we studied the influence of the embedding dimensions.", "labels": [], "entities": []}, {"text": "Increasing the embedding dimensions and memory size causes the program to run out of memory.", "labels": [], "entities": []}, {"text": "We can notice also that increasing  the embedding size will not enhance the results quality, when the best result in this testing was with embedding dimensions 100 and memory size 100 with a mean result of 0.685/1 information retrieving.", "labels": [], "entities": []}, {"text": "In order to check the validity of our hyper-parameters, we applied genetic algorithms).", "labels": [], "entities": []}, {"text": "The best result was 0.665 with the embedding dimensions set to 144, the memory size set to 86 and 20 epochs.", "labels": [], "entities": []}, {"text": "The parameters for the genetic algorithm was a population size of 35, 10 generations, the embedding dimensions allowed was set between 10 and 450 and the memory size allowed between 10 and 400.", "labels": [], "entities": []}, {"text": "We can compare our results with the children's books presented in section 2.4, even if it is not the same problem: our work is about extracting information from the a raw text from along story (the longest story has 410 sentences) with unrestricted sentences size (the longest sentence has 88 words), while the children's books focus on predicting the words with short story 20 sentences.", "labels": [], "entities": []}, {"text": "Nevertheless, both of them are using a large vocabulary size (around 50.000) and use End-to-End memory networks.", "labels": [], "entities": []}, {"text": "To have a semantic deep view of the results, let us see some of the predictions.", "labels": [], "entities": []}, {"text": "For example: Rielle Hunter has no occupation for her in the biography but the model predicted her as a queen, with deep looking we can see she is married with John Edwards whose occupation was an U.S. representative.", "labels": [], "entities": []}, {"text": "So the system probably exploited the relation between them, inferring on the way to defined her relation with her husband.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Result 1  Epochs Embedding Dimensions Memory Size Result  20  50  10  0.545  20  50  50  0.62  20  50  100  0.635  20  50  150  0.59  20  50  200  0.65  20  50  250  0.61  20  50  300  0.625  20  50  350  0.615  20  50  400  0.64", "labels": [], "entities": []}, {"text": " Table 4: Result 2  Epochs Embedding Dimensions Memory Size Result  40  50  10  0.56  40  50  50  0.62  40  50  100  0.65  40  50  150  0.655  40  50  200  0.63  40  50  250  0.605  40  50  300  0.63  40  50  350  0.62  40  50  400  0.60", "labels": [], "entities": []}, {"text": " Table 5: Result 3  Epochs Embedding Dimensions Memory Size Result  20  100  50  0.64  20  100  100  0.685  20  100  150  0.65  20  100  200  0.645  20  100  250  ME", "labels": [], "entities": []}, {"text": " Table 6: Result 4  Epochs Embedding Dimensions Memory Size Result  20  150  50  0.665  20  150  100  0.66  20  150  150  ME", "labels": [], "entities": []}, {"text": " Table 7: Children's books and Biography profiles", "labels": [], "entities": []}]}