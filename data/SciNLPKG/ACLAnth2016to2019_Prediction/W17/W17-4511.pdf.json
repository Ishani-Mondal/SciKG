{"title": [{"text": "Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization", "labels": [], "entities": [{"text": "Multi-Document Summarization", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6879834979772568}]}], "abstractContent": [{"text": "The centroid-based model for extractive document summarization is a simple and fast baseline that ranks sentences based on their similarity to a centroid vector.", "labels": [], "entities": [{"text": "extractive document summarization", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.6286802490552267}]}, {"text": "In this paper, we apply this ranking to possible summaries instead of sentences and use a simple greedy algorithm to find the best summary.", "labels": [], "entities": []}, {"text": "Furthermore, we show possibilities to scale up to larger input document collections by selecting a small number of sentences from each document prior to constructing the summary.", "labels": [], "entities": []}, {"text": "Experiments were done on the DUC2004 dataset for multi-document summarization.", "labels": [], "entities": [{"text": "DUC2004 dataset", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9775689542293549}, {"text": "multi-document summarization", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6269351840019226}]}, {"text": "We observe a higher performance over the original model, on par with more complex state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extractive multi-document summarization (MDS) aims to summarize a collection of documents by selecting a small number of sentences that represent the original content appropriately.", "labels": [], "entities": [{"text": "Extractive multi-document summarization (MDS)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8479211330413818}, {"text": "summarize a collection of documents", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.8267950177192688}]}, {"text": "Typical objectives for assembling a summary include information coverage and non-redundancy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.7155142426490784}]}, {"text": "A wide variety of methods have been introduced to approach MDS.", "labels": [], "entities": [{"text": "MDS", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9632277488708496}]}, {"text": "Many approaches are based on sentence ranking, i.e. assigning each sentence a score that indicates how well the sentence summarizes the input (.", "labels": [], "entities": []}, {"text": "A summary is created by selecting the top entries of the ranked list of sentences.", "labels": [], "entities": []}, {"text": "Since the sentences are often treated separately, these models might allow redundancy in the summary.", "labels": [], "entities": []}, {"text": "Therefore, they are often extended by an anti-redundancy filter while de-queuing ranked sentence lists.", "labels": [], "entities": []}, {"text": "Other approaches work at summary-level rather than sentence-level and aim to optimize functions of sets of sentences to find good summaries, such as KL-divergence between probability distributions ( or submodular functions that represent coverage, diversity, etc.", "labels": [], "entities": []}, {"text": "( The centroid-based model belongs to the former group: it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection ( ).", "labels": [], "entities": []}, {"text": "The sentences are ranked by their cosine similarity to the centroid vector.", "labels": [], "entities": []}, {"text": "This method is often found as a baseline in evaluations where it usually is outperformed (.", "labels": [], "entities": []}, {"text": "This baseline can easily be adapted to work at the summary-level instead the sentence level.", "labels": [], "entities": []}, {"text": "This is done by representing a summary as the centroid of its sentence vectors and maximizing the similarity between the summary centroid and the centroid of the document collection.", "labels": [], "entities": []}, {"text": "A simple greedy algorithm is used to find the best summary under a length constraint.", "labels": [], "entities": []}, {"text": "In order to keep the method efficient, we outline different methods to select a small number of candidate sentences from each document in the input collection before constructing the summary.", "labels": [], "entities": []}, {"text": "We test these modifications on the DUC2004 dataset for multi-document summarization.", "labels": [], "entities": [{"text": "DUC2004 dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9827439486980438}, {"text": "multi-document summarization", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.5762276649475098}]}, {"text": "The results show an improvement of Rouge scores over the original centroid method.", "labels": [], "entities": [{"text": "Rouge scores", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.9754830896854401}]}, {"text": "The performance is on par with state-of-the-art methods which shows that the similarity between a summary centroid and the input centroid is a well-suited function for global summary optimization.", "labels": [], "entities": [{"text": "global summary optimization", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.7459968527158102}]}, {"text": "The summarization approach presented in this paper is fast, unsupervised and simple to implement.", "labels": [], "entities": [{"text": "summarization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9575825333595276}]}, {"text": "Nevertheless, it performs as well as more complex state-of-the-art approaches in terms of Rouge scores on the DUC2004 dataset.", "labels": [], "entities": [{"text": "DUC2004 dataset", "start_pos": 110, "end_pos": 125, "type": "DATASET", "confidence": 0.986728698015213}]}, {"text": "It can be used as a strong baseline for future research or as a fast and easy-to-deploy summarization tool.", "labels": [], "entities": [{"text": "summarization", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.9614513516426086}]}], "datasetContent": [{"text": "For testing, we use the DUC2004 Task 2 dataset from the Document Understanding Conference (DUC).", "labels": [], "entities": [{"text": "DUC2004 Task 2 dataset from the Document Understanding Conference (DUC)", "start_pos": 24, "end_pos": 95, "type": "DATASET", "confidence": 0.9243552982807159}]}, {"text": "The dataset consists of 50 document clusters containing 10 documents each.", "labels": [], "entities": []}, {"text": "For tuning hyperparameters, we use the CNN/Daily Mail dataset () which provides summary bulletpoints for individual news articles.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.9373304843902588}]}, {"text": "In order to adapt the dataset for MDS, 50 CNN articles were randomly selected as documents to initialize 50 clusters.", "labels": [], "entities": [{"text": "MDS", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9296786785125732}]}, {"text": "For each of these seed articles, 9 articles with the highest word-overlap in the first 3 sentences were added to that cluster.", "labels": [], "entities": []}, {"text": "This resulted in 50 documents clusters, each containing 10 topically related articles.", "labels": [], "entities": []}, {"text": "The reference summaries for each cluster were created by interleaving the sentences of the article summaries until a length contraint (100 words) was reached.", "labels": [], "entities": []}, {"text": "(2014) published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods . We evaluate summaries generated by a selection of these methods on the same data that we use for testing.", "labels": [], "entities": [{"text": "SumRepo", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9136156439781189}, {"text": "DUC2004 dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9262621104717255}]}, {"text": "We calculate Rouge scores with the Rouge toolkit).", "labels": [], "entities": []}, {"text": "In order to compare our results to  we use the same Rouge settings as they do 2 and report results for Rouge-1, Rouge-2 and Rouge-4 recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.968469500541687}]}, {"text": "The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Rouge scores on DUC2004.", "labels": [], "entities": [{"text": "Rouge", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9710512757301331}, {"text": "DUC2004", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9530520439147949}]}]}