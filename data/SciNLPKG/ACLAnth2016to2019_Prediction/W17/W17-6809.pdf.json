{"title": [{"text": "If Sentences Could See: Investigating Visual Information for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.593318114678065}]}], "abstractContent": [{"text": "We investigate the effects of incorporating visual signal from images into unsupervised Semantic Textual Similarity (STS) measures.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.7294840713342031}]}, {"text": "STS measures exploiting visual signal alone are shown to outperform, in some settings, linguistic-only measures by a wide margin, whereas multi-modal measures yield further performance gains.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8970713019371033}]}, {"text": "We also show that selective inclusion of visual information may further boost performance in the multi-modal setup.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic textual similarity (, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences.", "labels": [], "entities": [{"text": "Semantic textual similarity (, inter alia)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7118895053863525}]}, {"text": "Despite the obvious applicability to sentence alignment for machine translation (MT)) or plagiarism detection, cross-lingual STS models were proposed only recently (.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7473282814025879}, {"text": "machine translation (MT))", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.8532366752624512}, {"text": "cross-lingual STS", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.6359950751066208}]}, {"text": "These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English.", "labels": [], "entities": []}, {"text": "Although research in cognitive science (e.g.,; Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity, predicting phrase compositionality, recognizing lexical entailment ( , and metaphor detection.", "labels": [], "entities": [{"text": "predicting phrase compositionality", "start_pos": 398, "end_pos": 432, "type": "TASK", "confidence": 0.8758911490440369}, {"text": "metaphor detection", "start_pos": 473, "end_pos": 491, "type": "TASK", "confidence": 0.862645149230957}]}, {"text": "While still predominantly applied in monolingual settings, representations originating from the visual modality are inherently language-invariable (.", "labels": [], "entities": []}, {"text": "As such, they could serve as a natural cross-language bridge in cross-lingual STS.", "labels": [], "entities": [{"text": "cross-lingual STS", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7388991415500641}]}, {"text": "In this work, we investigate unsupervised multi-modal and cross-lingual STS models that leverage visual information from images alongside linguistic information from textual corpora.", "labels": [], "entities": [{"text": "cross-lingual STS", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.688327968120575}]}, {"text": "We feed images retrieved for textual queries to the deep convolutional network (CNN) for image classification and use the CNN's abstract image features as visual semantic representations of words and sentences.", "labels": [], "entities": [{"text": "image classification", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.7553131282329559}]}, {"text": "We implement models that combine linguistic and visual information at different levels of granularity -early fusion (word level), middle fusion (sentence level), and late fusion (fusing similarity scores).", "labels": [], "entities": [{"text": "late", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9552133083343506}]}, {"text": "Results of an evaluation on two cross-lingual STS datasets (mutually very different in terms of text genre and average sentence length) show that (1) the proposed multi-modal STS models outperform uni-modal models relying only on visual or linguistic input and (2) in several experimental runs, purely visual STS models outperform purely linguistic STS models.", "labels": [], "entities": []}, {"text": "We obtain further performance gains by selectively exploiting visual information, conditioned on the dispersion of retrieved images.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we provide all details relevant to the evaluation of our unsupervised multi-modal STS models, from the description of datasets to the discussion of the experimental results.", "labels": [], "entities": []}, {"text": "We use two different STS datasets to evaluate all models: we opt for very different STS datasets to gain more insight about the effectiveness of visuallyinformed STS models in different settings.", "labels": [], "entities": []}, {"text": "The first dataset is the evaluation portion of the Microsoft Research video captions dataset (MSRVID) from the SemEval 2012 STS challenge.", "labels": [], "entities": [{"text": "Microsoft Research video captions dataset (MSRVID) from the SemEval 2012 STS challenge", "start_pos": 51, "end_pos": 137, "type": "DATASET", "confidence": 0.8703153899737767}]}, {"text": "MSRVID consists of 750 pairs of short English sentences containing rather concrete concepts (people and animals performing simple actions, e.g., \"A woman is slicing onions\").", "labels": [], "entities": [{"text": "MSRVID", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8450249433517456}]}, {"text": "We couple the MSRVID dataset with the cross-lingual English-Spanish STS dataset (NEWS-16) from the SemEval 2016 STS shared task.", "labels": [], "entities": [{"text": "MSRVID dataset", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.9102396368980408}, {"text": "English-Spanish STS dataset (NEWS-16)", "start_pos": 52, "end_pos": 89, "type": "DATASET", "confidence": 0.7241034706433614}, {"text": "SemEval 2016 STS shared task", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.5763958334922791}]}, {"text": "NEWS-16 comprises 301 pairs of long sentences taken from news stories.", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9461783170700073}]}, {"text": "Considering (1) that MSRVID is a monolingual English dataset and NEWS-16 considers only one language pair and (2) that we aim to evaluate cross-lingual STS models on several language pairs, we derived other cross-lingual versions of these datasets.", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9135750532150269}]}, {"text": "In addition to the standard monolingual English evaluation on the MSRVID dataset, we perform cross-lingual evaluations for three different language pairs: English-Spanish (EN-ES), English-Italian (EN-IT), and English-Croatian (EN-HR).", "labels": [], "entities": [{"text": "MSRVID dataset", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9785880148410797}]}, {"text": "We selected Spanish because of a readily available ES-EN NEWS-16 dataset and Italian because we had access to native speakers.", "labels": [], "entities": [{"text": "ES-EN NEWS-16 dataset", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.7915978034337362}]}, {"text": "To test the claim that the proposed approach is language-independent, we also include an under-resourced language in the evaluation.", "labels": [], "entities": []}, {"text": "As for Italian, we chose Croatian because we had access to a native speaker of that language.", "labels": [], "entities": []}, {"text": "We created the additional cross-lingual datasets (all three language pairs for MSRVID; EN-IT and EN-HR for NEWS-16) by: having native speakers fix the machine translation errors.", "labels": [], "entities": [{"text": "MSRVID", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.8699038624763489}, {"text": "NEWS-16", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.8733879923820496}]}, {"text": "We depict the differences between the datasets in terms of the average sentence length in number of words (ASL) and the average image dispersion of words (AID) in, for all four languages.", "labels": [], "entities": [{"text": "image dispersion of words (AID)", "start_pos": 128, "end_pos": 159, "type": "METRIC", "confidence": 0.567665559904916}]}, {"text": "The average image dispersion is much lower on the MSRVID dataset (especially for English), which implies that the NEWS-16 dataset has larger portion of concepts that simply do not have a standardized visual representation.", "labels": [], "entities": [{"text": "MSRVID dataset", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9548543691635132}, {"text": "NEWS-16 dataset", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.9678126871585846}]}, {"text": "A closer manual inspection of the two datasets revealed that NEWS-16, besides having more abstract concepts than MSRVID, contains also a much larger number of polysemous words.", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9129294157028198}, {"text": "MSRVID", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.8528469204902649}]}, {"text": "This is not surprising considering the news-story origin of the sentences in NEWS-16.", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9234300255775452}]}, {"text": "The differences in average image dispersion between the languages on the MSRVID dataset imply that Bing image search does not perform equally well for all languages.", "labels": [], "entities": [{"text": "MSRVID dataset", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9667720794677734}, {"text": "Bing image search", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.5427040755748749}]}, {"text": "We used the readily available word vectors for English (200-dimensional GloVe vectors trained on 6B tokens corpus), Spanish (300-dimensional Skip-Gram vectors trained on a 1.5B tokens corpus), and Italian (300-dimensional Skip-Gram vectors trained on a 2B tokens corpus).", "labels": [], "entities": []}, {"text": "For Croatian, we trained 200-dimensional Skip-Gram embedding vectors on the 1.2B token version of the hrWaC corpus.", "labels": [], "entities": [{"text": "hrWaC corpus", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.8683642148971558}]}, {"text": "To train the translation matrices, we selected the 4200 most frequent English words and translated them to the other three languages via Google translate, as done in prior work (.", "labels": [], "entities": []}, {"text": "Native speakers of target languages fixed incorrect machine translations.", "labels": [], "entities": []}, {"text": "We learned the optimal values of the translation matrices stochastically with the Adam algorithm () on the 4000 word translation pairs.", "labels": [], "entities": []}, {"text": "The obtained results of the evaluation of the translation quality on the remaining 200 test pairs -58.8% P@5 for EN-ES, 56.3% for EN-IT, and 56.2% for EN-HR -are comparable to those reported in the original study from . STS Models in Evaluation.", "labels": [], "entities": [{"text": "P", "start_pos": 105, "end_pos": 106, "type": "METRIC", "confidence": 0.9842672944068909}, {"text": ". STS Models", "start_pos": 218, "end_pos": 230, "type": "DATASET", "confidence": 0.9035056630770365}]}, {"text": "Our general approach includes several methods for measuring visual similarity   ii) Visual-only models use optimal alignment or aggregation similarity with the visual similarities from that yield the best performance (VIS-OA-AVG-MAX and VIS-AGG-SIM-AVG); iii) Multi-modal models exploit both the linguistic and visual signal by combining early or middle fusion with the averaged image embedding (EF-OA-AVG and MF-AVG).", "labels": [], "entities": []}, {"text": "Additionally, LF-WORD-OA performs the late fusion at the word level with optimal alignment similarity, whereas LF-SENT model computes the average of the similarity scores produced by the best-performing linguistic-only model and the best-performing visual-only model on the respective dataset.", "labels": [], "entities": []}, {"text": "For the last three models we also evaluate variants with image dispersion-based weighting (MF-AVG-ID, LF-WORD-OA-ID and LF-SENT-ID).", "labels": [], "entities": []}, {"text": "Results using Pearson correlation between human and automatic similarity scores are shown in. i) Multi-modal vs. uni-modal.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.9423876404762268}]}, {"text": "The visual-only models tend to outperform the linguistic-only models on MSRVID.", "labels": [], "entities": [{"text": "MSRVID", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.9036496877670288}]}, {"text": "The multi-modal models further improve the performance of the visual-only models.", "labels": [], "entities": []}, {"text": "We believe that this is the result of good visual representations we are able to obtain for concrete concepts, which are abundant in MSRVID.", "labels": [], "entities": [{"text": "MSRVID", "start_pos": 133, "end_pos": 139, "type": "DATASET", "confidence": 0.8499085307121277}]}, {"text": "In the multi-modal landscape, the late fusion at the level of similarity scores (i.e., the LF-SENT model) seems to be the best way to combine visual and linguistic information.", "labels": [], "entities": []}, {"text": "The performance of the visual-only models on NEWS-16 is, however, much lower than the performance of the linguistic-only models.", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9184362292289734}]}, {"text": "We believe that this is the direct consequence of obtaining rather noisy visual signal for the majority of concepts in this dataset.", "labels": [], "entities": []}, {"text": "We observe that only 17.9% of English words from NEWS-16 have the image dispersion score below 0.5 (the statistics is 38.7% on MSRVID).", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.8455126881599426}, {"text": "image dispersion score", "start_pos": 66, "end_pos": 88, "type": "METRIC", "confidence": 0.7817899982134501}, {"text": "MSRVID", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.8978635668754578}]}, {"text": "The number is even lower for the other three languages.", "labels": [], "entities": [{"text": "number", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9592312574386597}]}, {"text": "Therefore, the direct multi-modal models (i.e., without the selective inclusion of visual information) also perform worse than the linguistic-only models.", "labels": [], "entities": []}, {"text": "Aggregation-based models (TXT-OA, VIS-AGG-AVG, and MF-AVG) perform comparably to their respective optimal alignment counterparts (TXT-OA, VIS-OA-AVGMAX, and EF-OA-AVG) on MSRVID, but display drastically lower performances on NEWS-16.", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 225, "end_pos": 232, "type": "DATASET", "confidence": 0.9708448052406311}]}, {"text": "The explanation for this is rather intuitive -it is harder to aggregate the meaning of a sentence from the meaning of its words for long than for short sentences.", "labels": [], "entities": []}, {"text": "On the other hand, by aligning pairs of words and accounting for the number of alignments, the optimal alignment similarity is not affected by the sentence length.", "labels": [], "entities": []}, {"text": "ii) Monolingual vs. cross-lingual.", "labels": [], "entities": []}, {"text": "The performance gap on MSRVID in favor of visual-only and multi-modal models is significantly larger in the cross-lingual settings than in the monolingual English setting.", "labels": [], "entities": []}, {"text": "On one hand, the cross-lingual linguistic-only models suffer from the imperfect mappings between monolingual embedding spaces.", "labels": [], "entities": []}, {"text": "On the other hand, the visual signal seems not to deteriorate as much in quality for other languages.", "labels": [], "entities": []}, {"text": "The performance of visual-only and multimodal models is naturally lower for language pairs with languages for which more dispersed visual signals are used (IT and HR, seethe scores in).", "labels": [], "entities": []}, {"text": "iii) Selective inclusion of visual information.", "labels": [], "entities": [{"text": "Selective inclusion of visual information", "start_pos": 5, "end_pos": 46, "type": "TASK", "confidence": 0.7849604964256287}]}, {"text": "The models that selectively include visual information do not consistently improve the results of the direct multimodal models on MSRVID.", "labels": [], "entities": [{"text": "MSRVID", "start_pos": 130, "end_pos": 136, "type": "DATASET", "confidence": 0.827214777469635}]}, {"text": "Since the impact of the visual signal is scaled according to the the larger of the image dispersions, the selection model might discard useful visual information fora word/sentence on one side, because of the poor visual information on the other side.", "labels": [], "entities": []}, {"text": "On the other hand, we have less informative visual representations across the board on NEWS-16: here, a selective inclusion of visual information in the similarity-level late fusion model (LF-SENT-ID) has a slight edge on the linguistic-only model (TXTOA).", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9648519158363342}]}, {"text": "This improvement is small due to a shortage of concepts with sufficiently coherent visual representations in NEWS-16.", "labels": [], "entities": [{"text": "NEWS-16", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9390681982040405}]}, {"text": "This suggests that more sophisticated image extraction and content selection methods are required in future work.", "labels": [], "entities": [{"text": "image extraction", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8104412257671356}, {"text": "content selection", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7554869055747986}]}, {"text": "iv) Comparison with state-of-the-art.", "labels": [], "entities": []}, {"text": "For the monolingual English MSRVID dataset and the cross-lingual EN-ES NEWS-16 dataset, we also compare our results with the best-performing systems from the corresponding SemEval shared tasks.", "labels": [], "entities": [{"text": "English MSRVID dataset", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.7363554040590922}, {"text": "EN-ES NEWS-16 dataset", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.873627225557963}, {"text": "SemEval shared tasks", "start_pos": 172, "end_pos": 192, "type": "TASK", "confidence": 0.7457670370737711}]}, {"text": "\u02c7 Sari\u00b4c reach 88% correlation on MSRVID, which is 7% better than our LF-SENT-ID model.", "labels": [], "entities": [{"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9959002137184143}, {"text": "MSRVID", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.5928104519844055}]}, {"text": "The system of achieves the correlation score of 91% on the EN-ES NEWS-16, 8% above the performance of LF-SENT-ID.", "labels": [], "entities": [{"text": "correlation score", "start_pos": 27, "end_pos": 44, "type": "METRIC", "confidence": 0.9818671643733978}, {"text": "EN-ES NEWS-16", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.8488289713859558}]}, {"text": "We find these gaps in performance to be reasonably low, given that both state-of-the-art systems use a set of expensive language-specific tools (e.g., dependency parsers, NER).", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.7331600487232208}]}, {"text": "Moreover, the system of\u0160ari\u00b4cof\u02c7of\u0160ari\u00b4of\u0160ari\u00b4c et al. is supervised, whereas the Brychc\u00edn and Svoboda (2016) require a full-blown MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.9786738753318787}]}], "tableCaptions": [{"text": " Table 2: Statistics of the STS evaluation datasets.", "labels": [], "entities": [{"text": "STS evaluation datasets", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.765998125076294}]}, {"text": " Table 3: STS performance on the MSRVID and NEWS-16 datasets (Pearson \u03c1).", "labels": [], "entities": [{"text": "STS", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8648871779441833}, {"text": "MSRVID", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9310298562049866}, {"text": "NEWS-16 datasets", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.8050275444984436}, {"text": "Pearson \u03c1)", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9838091532389323}]}]}