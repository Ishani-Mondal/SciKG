{"title": [{"text": "Curriculum Design for Code-switching: Experiments with Language Identification and Language Modeling with Deep Neural Networks", "labels": [], "entities": [{"text": "Language Identification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7013518959283829}, {"text": "Language Modeling", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7107665091753006}]}], "abstractContent": [{"text": "Curriculum learning strategies are known to improve the accuracy, robustness and convergence rate for various language learning tasks using deep archi-tectures (Bengio et al., 2009).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9992936849594116}, {"text": "convergence rate", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.9722755253314972}]}, {"text": "In this work, we design and experiment with several training curricula for two tasks-word-level language detection and language modeling-for code-switched text data.", "labels": [], "entities": [{"text": "language detection", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7610261142253876}, {"text": "language modeling-for code-switched text data", "start_pos": 119, "end_pos": 164, "type": "TASK", "confidence": 0.8456396400928498}]}, {"text": "Our study shows that irrespective of the task or the underlying DNN architecture , the best curriculum for training the code-switched models is to first train a network with monolingual training instances, where each mini-batch has instances from both languages, and then train the resulting network on code-switched data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Code-switching (CS) refers to the linguistic phenomenon of fluid alternation between two or more languages during a single conversation or even an utterance.", "labels": [], "entities": [{"text": "Code-switching (CS)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6141929924488068}]}, {"text": "It is observed in all stable multilingual societies and recent studies have shown that social media posts from such societies almost always contain small to moderate amount of CS ().", "labels": [], "entities": []}, {"text": "For instance, shows that 2-12% (3.5% on average) of the tweets from the cities around the world are code-switched.", "labels": [], "entities": []}, {"text": "It is therefore imperative to build speech and text processing technologies that can handle CS.", "labels": [], "entities": []}, {"text": "Indeed, quite some amount of effort is being invested towards technology for CS (see,, and references therein).", "labels": [], "entities": []}, {"text": "It is of theoretical and practical interest to ponder on the question: whether fora particular NLP task (say ASR, MT or POS Tagging), it is possible to build CS models only from pretrained monolingual models or monolingual training data?", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.5987248718738556}]}, {"text": "Indeed, several studies in the past () have proposed techniques for combining monolingual models or training data coupled with a little amount of CS data to build models of CS text or speech.", "labels": [], "entities": []}, {"text": "These techniques have reported promising results.", "labels": [], "entities": []}, {"text": "However, all these studies, except, have tried to combine the outputs of pre-trained monolingual models in intelligent ways.", "labels": [], "entities": []}, {"text": "On the other hand, one might ask whether a single system trained on monolingual data from both the languages would be able to handle CS between these languages?", "labels": [], "entities": []}, {"text": "And, if we also had a little amount of CS data, how best to use it during the training process?", "labels": [], "entities": []}, {"text": "In this paper, we explore various training strategies, also known as Curriculum () for DNN-based architectures for codeswitching.", "labels": [], "entities": []}, {"text": "In particular, we design a set of strategies or curricula involving various ordering of the monolingual and CS data.", "labels": [], "entities": []}, {"text": "We experiment with these curricula for Language Identification (LID) and Language Modeling (LM) tasks.", "labels": [], "entities": [{"text": "Language Identification (LID)", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.806633734703064}, {"text": "Language Modeling (LM) tasks", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.8175151099761327}]}, {"text": "Our study shows that the best curriculum across the tasks as well as DNN architecture is the same one: first train a network with monolingual instances alternating between the languages, and then train the resultant network with CS data, if available.", "labels": [], "entities": []}, {"text": "The models trained solely with monolingual data also achieve reasonably high accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9853075742721558}]}, {"text": "As far as we know, this is the first study on curriculum design for CS.", "labels": [], "entities": []}, {"text": "Our study has two important implications: first, it shows that it is possible to train models for CS using primarily monolingual data; this obviates the need for creation of large amount of CS datasets.", "labels": [], "entities": []}, {"text": "Second, it also brings out the fact that training curriculum is extremely important while building CS models from monolingual data, and there seems to bean ideal way of ordering the training examples that works best across tasks and network structures.", "labels": [], "entities": []}], "datasetContent": [{"text": "Similar to our LID experiment datasets described in, we use the En and Es monolingual tweets from () (described as the weakly-labeled data in the paper), and the language-labeled CS data from ) for training.", "labels": [], "entities": [{"text": "LID experiment datasets", "start_pos": 15, "end_pos": 38, "type": "DATASET", "confidence": 0.6391763985157013}]}, {"text": "However, unlike the LID experiments, here a tweet, rather than a word, is considered as an instance; any tweet with CS is apart of the CS training instance.", "labels": [], "entities": []}, {"text": "Since we do not need the language labels for the LM training experiments, the tweets were stripped off those labels.", "labels": [], "entities": [{"text": "LM training", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8762578666210175}]}, {"text": "We used 212k En tweets, 92k Es tweets and 798 En-Es CS tweets as training data.", "labels": [], "entities": []}, {"text": "The amount of CS data used is a very small fraction (0.2%) of the size of the monolingual data and the amount of En data is more than double the Es data.", "labels": [], "entities": [{"text": "En", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9585404396057129}]}, {"text": "We also created a held-out test set for the LM experiments, which consists of 2200 En-Es tweets that were automatically tagged as code-switched by our best En-Es LID system described in the previous section.", "labels": [], "entities": [{"text": "LM", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.966131329536438}]}, {"text": "We use the standard evaluation metric \u2212 perplexity (PPL) on the held-out test set\u2212 to compare the LMs (lower the better).", "labels": [], "entities": [{"text": "evaluation metric \u2212 perplexity (PPL)", "start_pos": 20, "end_pos": 56, "type": "METRIC", "confidence": 0.7490971684455872}]}, {"text": "We experiment with all the training curricula described in Sec.", "labels": [], "entities": []}, {"text": "3, except C5 because from our experiments with C2 and C3, we find that adding CS data at the beginning produces worse results.", "labels": [], "entities": []}, {"text": "For experiments involving {T 1 , T 2 } (i.e., curricula C4 and C6), we provide the input by interleaving the tweets from T 1 and T 2 . Since En has almost twice as many tweets as Es, the extra En tweets that remain after interleaving all the Es tweets, are simply appended to the training set.", "labels": [], "entities": []}, {"text": "Finally, since we have very little CS data as compared to the monolingual data, for curriculum C7 we replicate the entire T 12 dataset after every 10k instances from {T 1 , T 2 }.", "labels": [], "entities": [{"text": "T 12 dataset", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.6340831617514292}]}, {"text": "Thus, we use 30 replicas of {T 12 } in this curriculum.", "labels": [], "entities": []}, {"text": "In addition to these curricula, we also build a baseline using only CS data (C0).", "labels": [], "entities": []}, {"text": "We build all the six models using the RNNLM single iteration setting first.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.7899823188781738}]}, {"text": "Then, we build models for the best performing curriculum using the multiple iteration recipe.", "labels": [], "entities": []}, {"text": "In addition, we build 5-gram models for all the 6 curricula and the C0 baseline.", "labels": [], "entities": []}, {"text": "shows the results of the LM experiments in terms of perplexity on the held-out test set.", "labels": [], "entities": []}, {"text": "As we see from the numbers in first column, the best performing RNNLMs are those that are trained initially with monolingual data and then trained with CS data (C3 and C6).", "labels": [], "entities": []}, {"text": "The model that is trained initially with CS data and then with monolingual data (C2) performs as badly as the model that was trained with only monolingual data in blocks (C1).", "labels": [], "entities": []}, {"text": "In addition, training models with alternate monolingual sentences gives better performance (C4, C5) than training it with large chunks of monolingual text (C1, C2).", "labels": [], "entities": []}, {"text": "Training with monolingual and CS data using curricula C3, C6 and C7 outperforms the CS-data only baseline (C0).", "labels": [], "entities": []}, {"text": "Also, C4, that uses alternate sentences of monolingual data outperforms this baseline, probably due to the large difference in data size between the monolingual and CS data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets for LID (in number of words).", "labels": [], "entities": []}, {"text": " Table 2: Curriculum training accuracies (in %) for  LID and Char-LID models. The maximum accu- racy for the models are in bold and are statistically  significantly higher (p < 0.001) than all other val- ues in the column for that model.", "labels": [], "entities": [{"text": "Curriculum training accuracies", "start_pos": 10, "end_pos": 40, "type": "METRIC", "confidence": 0.6919827858606974}, {"text": "accu- racy", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9726777672767639}]}, {"text": " Table 3: Overall accuracy of the Char-LID model  for C7 with varying amounts of training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9998513460159302}]}, {"text": " Table 4: Perplexity results for RNNLM and inter- polated RNNLM+ngram LM. Values in parenthe- sis are for the multiple iteration model.", "labels": [], "entities": []}]}