{"title": [{"text": "Encoding Word Confusion Networks with Recurrent Neural Networks for Dialog State Tracking", "labels": [], "entities": [{"text": "Encoding Word Confusion Networks", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7713897675275803}, {"text": "Dialog State Tracking", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7800213297208151}]}], "abstractContent": [{"text": "This paper presents our novel method to encode word confusion networks, which can represent a rich hypothesis space of automatic speech recognition systems, via recurrent neural networks.", "labels": [], "entities": [{"text": "word confusion networks", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7725761234760284}, {"text": "speech recognition", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7042557448148727}]}, {"text": "We demonstrate the utility of our approach for the task of dialog state tracking in spoken dialog systems that relies on automatic speech recognition output.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.8204275170962015}]}, {"text": "Encoding confusion networks outperforms encoding the best hypothesis of the automatic speech recognition in a neural system for dialog state tracking on the well-known second Dialog State Tracking Challenge dataset.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7272102534770966}, {"text": "dialog state tracking", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.7527921597162882}, {"text": "Dialog State Tracking Challenge dataset", "start_pos": 175, "end_pos": 214, "type": "DATASET", "confidence": 0.7527296066284179}]}], "introductionContent": [{"text": "Spoken dialog systems (SDSs) allow users to naturally interact with machines through speech and are nowadays an important research direction, especially with the great success of automatic speech recognition (ASR) systems).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 179, "end_pos": 213, "type": "TASK", "confidence": 0.8000731070836385}]}, {"text": "SDSs can be designed for generic purposes, e.g. smalltalk) or a specific task such as finding restaurants or booking flights.", "labels": [], "entities": []}, {"text": "Here, we focus on task-oriented dialog systems, which assist the users to reach a certain goal.", "labels": [], "entities": []}, {"text": "Task-oriented dialog systems are often implemented in a modular architecture to breakup the complex task of conducting dialogs into more manageable subtasks.", "labels": [], "entities": []}, {"text": "describe the following prototypical set-up of such a modular architecture: First, an ASR system converts the spoken user utterance into text.", "labels": [], "entities": [{"text": "ASR", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9714409112930298}]}, {"text": "Then, a spoken language understanding (SLU) module extracts the user's intent and coarse-grained semantic information.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 8, "end_pos": 43, "type": "TASK", "confidence": 0.7979320188363394}]}, {"text": "Next, a dialog state tracking (DST) component maintains a distribution over the state of the dialog, updating it in every turn.", "labels": [], "entities": [{"text": "dialog state tracking (DST)", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.7658364325761795}]}, {"text": "Given this information, the dialog policy manager decides on the next action of the system.", "labels": [], "entities": []}, {"text": "Finally, a natural language generation (NLG) module forms the system reply that is converted into an audio signal via a text-to-speech synthesizer.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.8346211314201355}]}, {"text": "Error propagation poses a major problem in modular architectures as later components depend on the output of the previous steps.", "labels": [], "entities": [{"text": "Error propagation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8787464499473572}]}, {"text": "We show in this paper that DST suffers from ASR errors, which was also noted by.", "labels": [], "entities": [{"text": "DST", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8899238109588623}, {"text": "ASR errors", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.873779296875}]}, {"text": "One solution is to avoid modularity and instead perform joint reasoning over several subtasks, e.g. many DST systems directly operate on ASR output and do not rely on a separate SLU module (.", "labels": [], "entities": []}, {"text": "End-to-end systems that can be directly trained on dialogs without intermediate annotations have been proposed for open-domain dialog systems.", "labels": [], "entities": []}, {"text": "This is more difficult to realize for task-oriented systems as they often require domain knowledge and external databases.", "labels": [], "entities": []}, {"text": "First steps into this direction were taken by and, yet these approaches do not integrate ASR into the joint reasoning process.", "labels": [], "entities": [{"text": "ASR", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9896861910820007}]}, {"text": "We take a first step towards integrating ASR in an end-to-end SDS by passing on a richer hypothesis space to subsequent components.", "labels": [], "entities": [{"text": "ASR", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9890762567520142}]}, {"text": "Specifically, we investigate how the richer ASR hypothesis space can improve DST.", "labels": [], "entities": [{"text": "ASR hypothesis", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.882300078868866}, {"text": "DST", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9741718173027039}]}, {"text": "We focus on these two components because they are at the beginning of the processing pipeline and provide vital information for the subsequent SDS components.", "labels": [], "entities": []}, {"text": "Typically, ASR systems output the best hypothesis or an n-best list, which the majority of DST approaches so far uses.", "labels": [], "entities": [{"text": "ASR", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9861146211624146}]}, {"text": "However, n-best lists can only represent a very limited amount of hypotheses.", "labels": [], "entities": []}, {"text": "Internally, the ASR system maintains a rich hypothesis space in the form of speech lattices or confusion networks (cnets) . We adapt recently proposed algorithms to encode lattices with recurrent neural networks (RNNs) () to encode cnets via an RNN based on Gated Recurrent Units (GRUs) to perform DST in a neural encoderclassifier system and show that this outperforms encoding only the best ASR hypothesis.", "labels": [], "entities": [{"text": "ASR", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9814374446868896}, {"text": "DST", "start_pos": 298, "end_pos": 301, "type": "TASK", "confidence": 0.9879189133644104}]}, {"text": "We are aware of two DST approaches that incorporate posterior word-probabilities from cnets in addition to features derived from the n-best lists), but to the best of our knowledge, we develop the first DST system directly operating on cnets.", "labels": [], "entities": [{"text": "DST", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.966292142868042}, {"text": "DST", "start_pos": 203, "end_pos": 206, "type": "TASK", "confidence": 0.9711503386497498}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Coverage of words from the manual tran- scripts in the DSTC2 development set of differ- ent batch ASR output types (%). In the pruned  cnet interjections and hypotheses with scores be- low 0.001 were removed.", "labels": [], "entities": [{"text": "DSTC2 development set", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.9523816108703613}]}, {"text": " Table 2: DSTC2 test set accuracy for 1-best ASR  outputs of ten runs with different random seeds in  the format average maximum  minimum .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9975100755691528}, {"text": "ASR", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9340807199478149}]}, {"text": " Table 3: DSTC2 test set accuracy of ten  runs with different random seeds in the format  average maximum  minimum . denotes a statistically signif- icant higher result than the baseline (p < 0.05,  Wilcoxon signed-rank test with Bonferroni correc- tion for ten repeated comparisons). The cnet en- semble corresponds to the best cnet model with  pruning threshold 0.001 and weighted pooling.", "labels": [], "entities": [{"text": "DSTC2 test", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8236713707447052}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9991759657859802}]}]}