{"title": [], "abstractContent": [{"text": "Recent advances in automatic text summa-rization have used deep neural networks to generate high-quality abstractive summaries , but the performance of these models strongly depends on large amounts of suitable training data.", "labels": [], "entities": [{"text": "text summa-rization", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.53867107629776}]}, {"text": "We propose anew method for mining social media for author-provided summaries, taking advantage of the common practice of appending a \"TL;DR\" to long posts.", "labels": [], "entities": []}, {"text": "A case study using a large Reddit crawl yields the Webis-TLDR-17 corpus, complementing existing corpora primarily from the news genre.", "labels": [], "entities": [{"text": "Webis-TLDR-17 corpus", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.9734097123146057}]}, {"text": "Our technique is likely applicable to other social media sites and general web crawls.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a document, automatic summarization is the task of generating a coherent shorter version of the document that conveys its main points.", "labels": [], "entities": [{"text": "summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.8134163022041321}]}, {"text": "Depending on the use case, the target length of a summary maybe chosen relative to that of the input document, or it maybe limited.", "labels": [], "entities": []}, {"text": "Either way, a summary must be considered \"accurate\" by a human judge in relation to its length: the shorter a summary has to be, the more it will have to abstract over the input text.", "labels": [], "entities": []}, {"text": "Automatic abstractive summarization can be considered one of the most challenging variants of automatic summarization.", "labels": [], "entities": [{"text": "Automatic abstractive summarization", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.4565161367257436}, {"text": "summarization", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.8075485229492188}]}, {"text": "But with recent advancements in the field of deep learning, new ground was broken using various kinds of neural network models (.", "labels": [], "entities": []}, {"text": "The performance of these kinds of summarization models strongly depends on large amounts of suitable training data.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9669408202171326}]}, {"text": "To the best of our knowledge, the top rows of list all English- Newswire 500 Webis-TLDR-17 Social Media 4 million language corpora that have been applied to training and evaluating single-document summarization networks in the past two to three years; only the two largest corpora are of sufficient size to serve as training sets by themselves.", "labels": [], "entities": [{"text": "English- Newswire 500 Webis-TLDR-17 Social Media", "start_pos": 55, "end_pos": 103, "type": "DATASET", "confidence": 0.8038561131272998}]}, {"text": "At the same time, all of these corpora cover more or less the same text genre, namely news.", "labels": [], "entities": []}, {"text": "This is probably due to the relative ease by which news articles can be obtained as well as the fact that the news tend to contain properly written texts, usually from professional journalists.", "labels": [], "entities": []}, {"text": "Notwithstanding the usefulness of existing corpora, we argue that the apparent lack of genre diversity currently poses an obstacle to deep learning-based summarization.", "labels": [], "entities": [{"text": "deep learning-based summarization", "start_pos": 134, "end_pos": 167, "type": "TASK", "confidence": 0.5768710772196451}]}, {"text": "In this regard, we identified a novel, large-scale source of suitable training data from the genre of social media.", "labels": [], "entities": []}, {"text": "We benefit from the common practice of social media users summarizing their own posts as a courtesy to their readers: the abbreviation TL;DR, originally used as a response meaning \"too long; didn't read\" to call out on unnecessarily long posts, has been adopted by many social media users writing long posts in anticipatory obedience and now typically indicates that a summary of the entire post follows.", "labels": [], "entities": [{"text": "abbreviation TL;DR", "start_pos": 122, "end_pos": 140, "type": "METRIC", "confidence": 0.7666023299098015}]}, {"text": "This provides us with a text and its summary-both written by the same person-which, when harvested at scale, is an excellent datum for developing and evaluating an automatic summarization system.", "labels": [], "entities": [{"text": "summarization", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.9165510535240173}]}, {"text": "In contrast to the state-of-the-art corpora, social me-dia texts are written informally and discuss everyday topics, albeit mostly unstructured and oftentimes poorly written, offering new challenges to the community.", "labels": [], "entities": []}, {"text": "Thus, we endeavored to extract a usable dataset specifically suited for abstractive summarization from Reddit, the largest discussion forum on the web, where TL;DR summaries are extensively used.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.5427826046943665}]}, {"text": "In what follows, we discuss in detail how the data was obtained and preprocessed to compile the Webis-TLDR-17 corpus.", "labels": [], "entities": [{"text": "Webis-TLDR-17 corpus", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.9775893092155457}]}], "datasetContent": [{"text": "Reddit is a community centered around social news aggregation, web content rating, and discussion, and, as of mid-2017, one of the ten mostvisited sites on the web according to Alexa.", "labels": [], "entities": [{"text": "web content rating", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.6840340296427408}, {"text": "Alexa", "start_pos": 177, "end_pos": 182, "type": "DATASET", "confidence": 0.9607888460159302}]}, {"text": "1 Community members submit and curate content consisting of text posts or web links, segregated into channels called subreddits, covering general topics such as Technology, Gaming, Finance, Wellbeing, as well as special-interest subjects that may only be relevant to a handful of users.", "labels": [], "entities": []}, {"text": "At the time of writing, there are about 1.1 million subreddits.", "labels": [], "entities": []}, {"text": "In each subreddit, users submit top-level postsreferred to as submissions-and others reply with comments, reflecting, contradicting, or supporting the submission.", "labels": [], "entities": []}, {"text": "Submissions consist of a title and either a web link, or a user-supplied body text; in the latter case, the submission is also called a self-post.", "labels": [], "entities": []}, {"text": "Comments always have a body textunless subsequently deleted by the author or a moderator-which may also include inline URLs.", "labels": [], "entities": []}, {"text": "Large crawls of Reddit comments and submissions have recently been made available to the NLP community.", "labels": [], "entities": []}, {"text": "For the purpose of constructing our summarization corpus, we employ the set of 286 million submissions and 1.6 billion comments posted to Reddit between 2006 and 2016.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Filtering steps to get the TL;DR corpus.", "labels": [], "entities": [{"text": "TL;DR corpus", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.5598321631550789}]}, {"text": " Table 4: Length statistics for the TL;DR corpus.", "labels": [], "entities": [{"text": "TL;DR corpus", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.7077028602361679}]}]}