{"title": [{"text": "Exploring Optimal Voting in Native Language Identification", "labels": [], "entities": [{"text": "Exploring Optimal Voting in Native Language Identification", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.6351804861000606}]}], "abstractContent": [{"text": "We describe the submissions entered by the National Research Council Canada in the Native Language Identification Shared Task 2017.", "labels": [], "entities": [{"text": "National Research Council Canada", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.9327589422464371}, {"text": "Native Language Identification Shared Task", "start_pos": 83, "end_pos": 125, "type": "TASK", "confidence": 0.7219539046287536}]}, {"text": "We mainly explored the use of voting, and various ways to optimize the choice and number of voting systems.", "labels": [], "entities": []}, {"text": "We also explored the use of features that rely on no linguistic preprocessing.", "labels": [], "entities": []}, {"text": "Long ngrams of characters obtained from raw text turned out to yield the best performance on all textual input (written essays and speech transcripts).", "labels": [], "entities": []}, {"text": "Voting ensembles turned out to produce small performance gains, with little difference between the various optimization strategies we tried.", "labels": [], "entities": []}, {"text": "Our top systems achieved accuracies of 87% on the ESSAY track, 84% on the SPEECH track, and close to 92% by combining essays, speech and i-vectors in the FUSION track.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9992068409919739}, {"text": "ESSAY", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.8648285865783691}, {"text": "SPEECH", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9792974591255188}, {"text": "FUSION", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.8536602854728699}]}], "introductionContent": [{"text": "This paper describes the system entered by the National Research Council Canada in the Native Language Identification (NLI) Shared Task 2017 ( . The task of Native Language Identification consists of predicting the native (L1) language of a foreign speaker, from textual and speech clues in a second (L2) language.", "labels": [], "entities": [{"text": "National Research Council Canada", "start_pos": 47, "end_pos": 79, "type": "DATASET", "confidence": 0.8990302979946136}, {"text": "Native Language Identification (NLI) Shared Task", "start_pos": 87, "end_pos": 135, "type": "TASK", "confidence": 0.7766014337539673}, {"text": "Native Language Identification", "start_pos": 157, "end_pos": 187, "type": "TASK", "confidence": 0.6253556410471598}, {"text": "predicting the native (L1) language of a foreign speaker", "start_pos": 200, "end_pos": 256, "type": "TASK", "confidence": 0.7846197269179604}]}, {"text": "Applications of this task are mostly in language learning and forensic/security, see, Section 1.1) fora good overview.", "labels": [], "entities": [{"text": "language learning", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8316512703895569}, {"text": "forensic/security", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.6247944235801697}]}, {"text": "This is an interesting example of a task that is difficult to perform for humans, especially when the number of target native languages is large.", "labels": [], "entities": []}, {"text": "In fact, in a comparison between automated and human evaluation, could only use 5 L1 languages, whereas the automated classifier covered 11 languages.", "labels": [], "entities": []}, {"text": "They also found that, even in these limited settings, humans generally under-performed the automated systems.", "labels": [], "entities": []}, {"text": "An international evaluation in showed that statistical methods could reach a high level of performance on this task (close to 84% accuracy) using a mixture of surface form features, linguistic features, and model combination.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9966670870780945}]}, {"text": "Ensemble methods, in particular, have proved crucial to reach top performance on this task and other related document categorization tasks like the discrimination of language variants ().", "labels": [], "entities": []}, {"text": "Recent work has confirmed this; we refer the reader to for an overview and evaluation of many combination approaches.", "labels": [], "entities": []}, {"text": "Our best attempts at the NLI-2013 evaluation used model combination by voting, a simple strategy in which each base model contributes a vote towards a category, and final prediction goes to the category with the most votes.", "labels": [], "entities": []}, {"text": "In this evaluation, we therefore explore this strategy further, looking into important aspects of the process: selecting the models to add to the combination, as well as their number.", "labels": [], "entities": []}, {"text": "An attractive perk of the voting/combination approach is that it provides a natural way to handle multimodal data such as the text and speech data available in the evaluation.", "labels": [], "entities": [{"text": "voting/combination", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.848701536655426}]}, {"text": "One can train models using either modality, and combine their predictions using voting.", "labels": [], "entities": []}, {"text": "This is known as the late fusion approach.", "labels": [], "entities": []}, {"text": "By contrast, the early fusion approach combines different sets of features and trains a single model on those.", "labels": [], "entities": []}, {"text": "We test and compare a simple early fusion model in the FUSION track below.", "labels": [], "entities": [{"text": "FUSION", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.815589964389801}]}, {"text": "Our second investigation is on the feature side.", "labels": [], "entities": []}, {"text": "In particular, we investigate the use of long character ngram, without any other linguistic processing.", "labels": [], "entities": []}, {"text": "Previous work reached state-of-the-art perfor-mance on the 2013 NLI Shared Task using string kernels (, considering subsequences of 5 to 9 characters.", "labels": [], "entities": [{"text": "2013 NLI Shared Task", "start_pos": 59, "end_pos": 79, "type": "DATASET", "confidence": 0.6766869351267815}]}, {"text": "On the task of discriminating similar languages ( , long character ngrams also reach top performance ( ) using subsequences of 5 and 6 characters.", "labels": [], "entities": []}, {"text": "We looked in more detail into how useful this type of feature could be in the context of NLI.", "labels": [], "entities": []}, {"text": "This contrasts with many systems used in the 2013 evaluation, including ours, which used a combination of lexical and syntactic features, including short character and word ngrams, part-of-speech and syntactic dependencies.", "labels": [], "entities": []}, {"text": "We test character ngrams up to 6grams, extracted from raw text without any linguistic preprocessing (no tokenization or casing normalization).", "labels": [], "entities": []}, {"text": "In the following section, we quickly review the data and introduce the approaches we tested for the NLI Shared Task 2017.", "labels": [], "entities": [{"text": "NLI Shared Task 2017", "start_pos": 100, "end_pos": 120, "type": "DATASET", "confidence": 0.6437022387981415}]}, {"text": "Section 3 presents our results, both during development and evaluated on the final test data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: NLI-2017 collection: #doc per L1.", "labels": [], "entities": [{"text": "NLI-2017 collection", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.9701278507709503}]}, {"text": " Table 2: Results for the closed ESSAY track: orga- nizer's baseline, our three submissions (our best  result emphasized, best results in bold) and the  best ranked system. 'Acc.' is accuracy and 'm- F1' is macro-averaged F1.", "labels": [], "entities": [{"text": "ESSAY track", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.7935447990894318}, {"text": "Acc.", "start_pos": 174, "end_pos": 178, "type": "METRIC", "confidence": 0.9986078143119812}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9995020627975464}, {"text": "m- F1'", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.7342966943979263}, {"text": "F1", "start_pos": 222, "end_pos": 224, "type": "METRIC", "confidence": 0.9351617097854614}]}, {"text": " Table 3: Results for the closed SPEECH track: or- ganizer's baseline, our two submissions (our best  result emphasized, best result in bold) and the best  ranked system. 'Acc.' is accuracy and 'm-F1' is  macro-averaged F1.", "labels": [], "entities": [{"text": "Acc.", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9979854822158813}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9993732571601868}, {"text": "F1", "start_pos": 220, "end_pos": 222, "type": "METRIC", "confidence": 0.9347989559173584}]}, {"text": " Table 4: Results for the closed FUSION track: or- ganizer's baseline, our two submissions (our best  result emphasized, best result in bold) and the best  ranked system.", "labels": [], "entities": [{"text": "FUSION", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.8500580787658691}]}]}