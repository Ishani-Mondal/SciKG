{"title": [{"text": "Story Cloze Task: UW NLP System", "labels": [], "entities": [{"text": "Story Cloze", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8818688988685608}, {"text": "UW NLP", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.9444052278995514}]}], "abstractContent": [{"text": "This paper describes University of Washington NLP's submission for the Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem 2017) shared task-the Story Cloze Task.", "labels": [], "entities": [{"text": "Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem 2017) shared task-the Story Cloze Task", "start_pos": 71, "end_pos": 185, "type": "TASK", "confidence": 0.6970170951551862}]}, {"text": "Our system is a linear classifier with a variety of features, including both the scores of a neural language model and style features.", "labels": [], "entities": []}, {"text": "We report 75.2% accuracy on the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9989266991615295}]}, {"text": "A further discussion of our results can be found in Schwartz et al.", "labels": [], "entities": []}], "introductionContent": [{"text": "As an effort to advance commonsense understanding, developed the story cloze task, which is the focus of the LSDSem 2017 shared task.", "labels": [], "entities": [{"text": "commonsense understanding", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.8493838906288147}, {"text": "LSDSem 2017 shared task", "start_pos": 109, "end_pos": 132, "type": "DATASET", "confidence": 0.8103362023830414}]}, {"text": "In this task, systems are given two short, self-contained stories, which differ only in their last sentence: one has aright (coherent) ending, and the other has a wrong (incoherent) ending.", "labels": [], "entities": []}, {"text": "The task is to tell which is the right story.", "labels": [], "entities": []}, {"text": "In addition to the task, the authors also introduced the ROC story corpus-a training corpus of five-sentence (coherent) stories.", "labels": [], "entities": [{"text": "ROC story corpus-a training corpus", "start_pos": 57, "end_pos": 91, "type": "DATASET", "confidence": 0.8251342356204987}]}, {"text": "shows an example of a coherent story and an incoherent story from the story cloze task.", "labels": [], "entities": []}, {"text": "In this paper, we describe University of Washington NLP's submission for the shared task.", "labels": [], "entities": []}, {"text": "Our system explores several types of features for the task.", "labels": [], "entities": []}, {"text": "First, we train a neural language model () on the ROC story corpus.", "labels": [], "entities": [{"text": "ROC story corpus", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.8998007376988729}]}, {"text": "We use the probabilities assigned by the model to each of the endings (right and wrong) as classification features.", "labels": [], "entities": []}, {"text": "Second, we attempt to distinguish between right and wrong endings using style features, such as sentence length, character n-grams and word n-", "labels": [], "entities": []}], "datasetContent": [{"text": "The story cloze task doesn't have a training corpus for the right and wrong endings.", "labels": [], "entities": []}, {"text": "Therefore, we use the development set as our training set, holding out 10% for development (3,366 training endings, 374 for development).", "labels": [], "entities": []}, {"text": "We keep the story cloze test set as is (3,742 endings).", "labels": [], "entities": []}, {"text": "We use Python's sklearn logistic regression implementation with L 2 regularization, performing grid search on the development set to tune a single hyperparameter-the regularization parameter.", "labels": [], "entities": []}, {"text": "For computing the RNN features, we start by tokenizing the text using the nltk tokenizer.", "labels": [], "entities": []}, {"text": "We then use TensorFlow 3 to train the RNNLM using a single-layer LSTM of hidden dimension 512.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9328423142433167}]}, {"text": "We use the ROC Stories for training, setting aside 10% for validation of the language model.", "labels": [], "entities": [{"text": "ROC Stories", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9240562319755554}]}, {"text": "We replace all words occurring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of 21,582.", "labels": [], "entities": []}, {"text": "Only during training, we apply a) and a learning rate of \u03b7 = .001, we train to minimize cross-entropy.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9740144610404968}]}, {"text": "The resulting RNN features (see Section 2.2) are taken in log space.", "labels": [], "entities": []}, {"text": "For the style features, we add a START symbol at the beginning of each sentence.", "labels": [], "entities": [{"text": "START", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9727915525436401}]}, {"text": "We keep n-gram (character or word) features that occur at least five times in the training set.", "labels": [], "entities": []}, {"text": "All stylistic feature values are normalized to the range.", "labels": [], "entities": []}, {"text": "For the part-ofspeech features, we tag all endings with the Spacy POS tagger.", "labels": [], "entities": [{"text": "Spacy POS tagger", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.8189945220947266}]}, {"text": "The total number of features used by our system is 7,651.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the test set of the story cloze  task. The first block are published results, the  second block are our results. LexVec results are  taken from (Speer et al., 2017). Human judgement  scores are taken from (Mostafazadeh et al., 2016).", "labels": [], "entities": [{"text": "LexVec", "start_pos": 134, "end_pos": 140, "type": "DATASET", "confidence": 0.8426746726036072}]}]}