{"title": [], "abstractContent": [{"text": "We describe anew version of MEANT, which participated in the metrics task of the Second Conference on Machine Translation (WMT 2017).", "labels": [], "entities": [{"text": "MEANT", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.81478351354599}, {"text": "Machine Translation (WMT 2017)", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.8220702310403188}]}, {"text": "MEANT 2.0 uses idf-weighted distributional ngram accuracy to determine the phrasal similarity of semantic role fillers and yields better correlations with human judgments of translation quality than earlier versions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.7388455271720886}, {"text": "semantic role fillers", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.6003706256548563}]}, {"text": "The improved phrasal similarity enables a subversion of MEANT to accurately evaluate translation adequacy for any output language, even languages without an automatic semantic parser.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.8935164213180542}]}, {"text": "Our results show that MEANT, which is a non-ensemble and untrained metric, consistently performs as well as the top participants in previous years-including ensemble and trained ones-across different output languages.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9603769183158875}]}, {"text": "We also present the timing statistics for MEANT for better estimation of the evaluation cost.", "labels": [], "entities": [{"text": "timing", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9962253570556641}, {"text": "MEANT", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.8952851891517639}]}, {"text": "MEANT 2.0 is open source and publicly available.", "labels": [], "entities": [{"text": "MEANT 2.0", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7620484232902527}]}], "introductionContent": [{"text": "We introduce anew version of MEANT, which participated in evaluating MT systems for all language pairs in the metrics task of the Second Conference on Machine Translation (WMT 2017).", "labels": [], "entities": [{"text": "MEANT", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9508755803108215}, {"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9869517087936401}, {"text": "Machine Translation (WMT 2017)", "start_pos": 151, "end_pos": 181, "type": "TASK", "confidence": 0.8335846215486526}]}, {"text": "MEANT 2.0 is a non-ensemble and untrained metric that only requires a monolingual corpus in the output language to build the word embeddings and an automatic shallow semantic parser to obtain the predicate-argument structure to evaluate MT systems fora language pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 237, "end_pos": 239, "type": "TASK", "confidence": 0.9688073992729187}]}, {"text": "We have also build a degraded subversion, MEANT 2.0 -nosrl, to evaluate MT systems for any output language by re-1 http://chikiu-jackie-lo.org/home/index.php/meant moving the dependency on semantic parsers for semantic role labeling (SRL) the reference and the machine translations.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.7792884111404419}, {"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9824110865592957}, {"text": "semantic role labeling (SRL)", "start_pos": 210, "end_pos": 238, "type": "TASK", "confidence": 0.7705937723318735}]}, {"text": "The correlation of MEANT with human judgments has been improved by using both inverse document frequency (idf) and distributional ngram accuracy within the phrasal similarity calculation: the former to weight the importance of each word for better adequacy, the latter to to account for word reordering for greater fluency.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.607159435749054}, {"text": "inverse document frequency (idf)", "start_pos": 78, "end_pos": 110, "type": "METRIC", "confidence": 0.8711646596590678}, {"text": "distributional ngram accuracy", "start_pos": 115, "end_pos": 144, "type": "METRIC", "confidence": 0.7218727270762125}]}, {"text": "Our results show that MEANT consistently performs as well as the top participants in previous years across different output languages, including ensemble and trained participants.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7340559363365173}]}, {"text": "We also present the timing statistics that show the relatively low cost of running MEANT.", "labels": [], "entities": [{"text": "timing", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9870705008506775}, {"text": "MEANT", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.42717015743255615}]}, {"text": "This highly portable and open source semantic MT evaluation metric is a more accurate alternative to BLEU in evaluating translation quality for low-resource languages.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.8659780323505402}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.99037766456604}]}], "datasetContent": [{"text": "We use the WMT 2014-2016 metrics task evaluation set) for our development experiments.", "labels": [], "entities": [{"text": "WMT 2014-2016 metrics task evaluation set", "start_pos": 11, "end_pos": 52, "type": "DATASET", "confidence": 0.8504270712534586}]}, {"text": "The official human judgments of translation quality were collected using relative ranking.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8494865894317627}]}, {"text": "The annotators were given the original input and the reference and were asked to order up to 5 different MT outputs according to the translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.933445930480957}]}, {"text": "Two other kinds of human judgments of translation quality were collected in the WMT 2016 metrics task.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8460722863674164}, {"text": "WMT 2016 metrics task", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.7603744715452194}]}, {"text": "The direct assessment evaluation protocol gave the annotators the reference and one MT output only and asked them to evaluate the translation adequacy of the MT output on an absolute scale.", "labels": [], "entities": []}, {"text": "The HUME metric) is very similar to HMEANT, which evaluates translation adequacy via semantic units in the input sentence annotated by humans following the UCCA ( guidelines.", "labels": [], "entities": [{"text": "HUME metric", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.8663821816444397}]}, {"text": "However, HUME also takes nominal and adjectival argument structures into account (instead of only predicate argument structure as in HMEANT).", "labels": [], "entities": [{"text": "HUME", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.8051831126213074}]}, {"text": "Due to space limitations, we only report the results of MEANT 2.0, MEANT 2.0 -nosrl, BLEU and the best correlation in each of the individual language pairs.", "labels": [], "entities": [{"text": "MEANT 2.0 -nosrl", "start_pos": 67, "end_pos": 83, "type": "METRIC", "confidence": 0.7156360596418381}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9969204664230347}]}, {"text": "Since we use exactly the same protocol for each of the test sets, our reported results are directly comparable with those reported in;.", "labels": [], "entities": []}, {"text": "We summarize the observations in the following sections.", "labels": [], "entities": []}, {"text": "shows the Pearson's correlation with the WMT 2014-2016 official human relative ranking scores at system-level.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9662534991900126}, {"text": "WMT 2014-2016 official human relative ranking scores", "start_pos": 41, "end_pos": 93, "type": "DATASET", "confidence": 0.7817084278379168}]}, {"text": "As expected, MEANT 2.0 performs significantly better than MEANT 2.0 -nosrl inmost of the language pairs.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.6221867203712463}]}, {"text": "Overall, both MEANT 2.0 and the nosrl variant are very competitive with other metrics for all test sets.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9584242105484009}]}, {"text": "The loading time of the word embedding model is proportional to the vocabulary size of the model reported in table 1; it takes less than a second to load 10k vocabularies into memory.", "labels": [], "entities": []}, {"text": "Automatic semantic role labeling (SRL) is the most time consuming step in running MEANT.", "labels": [], "entities": [{"text": "Automatic semantic role labeling (SRL)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6865493442331042}, {"text": "MEANT", "start_pos": 82, "end_pos": 87, "type": "TASK", "confidence": 0.6753450632095337}]}, {"text": "The time reported in table 7 includes parsing both the reference and the MT output.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9696791768074036}, {"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.7760367393493652}]}, {"text": "However, as pointed out above, common practice for MT system development is to frequently reuse the validation and evaluation sets.", "labels": [], "entities": [{"text": "MT system development", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.9138240814208984}]}, {"text": "Thus, semantic role labeling of the reference translation could be precomputed to reduce the time taken for the SRL step in the development cycle.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.6547428468863169}, {"text": "SRL", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9733220338821411}]}, {"text": "Finally, the time used in computing the MEANT score is proportional to the size of the evaluation set and the word embedding model.", "labels": [], "entities": [{"text": "MEANT score", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9677153527736664}]}, {"text": "The scoring step processes around 50 to 100 sentences each second.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pearson's correlation of the metric scores with the WMT 2014-2016 official human relative  ranking scores at system-level. For consistency with the task overview paper, en-de results are not  included into out-of-English system average in WMT 2014 results (Machacek and Bojar, 2014); system  average are not reported in WMT 2016 results (Bojar et al., 2016b).", "labels": [], "entities": [{"text": "WMT 2014-2016 official human relative  ranking", "start_pos": 62, "end_pos": 108, "type": "DATASET", "confidence": 0.8197246094544729}, {"text": "WMT 2014 results", "start_pos": 249, "end_pos": 265, "type": "DATASET", "confidence": 0.7809780240058899}, {"text": "WMT 2016 results", "start_pos": 330, "end_pos": 346, "type": "DATASET", "confidence": 0.8741053541501363}]}, {"text": " Table 3: Pearson's correlation of metric scores with the WMT 2016 direct assessment of translation  adequacy at system-level.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.7891589999198914}, {"text": "WMT", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.7523678541183472}]}, {"text": " Table 4: Kendall's correlation of metric scores with the WMT 2014-2016 official human relative ranking  judgments at segment-level. For consistency with the task overview paper, system averages are not  reported in WMT 2016 results (Bojar et al., 2016b).", "labels": [], "entities": [{"text": "WMT 2014-2016 official human relative ranking  judgments", "start_pos": 58, "end_pos": 114, "type": "DATASET", "confidence": 0.8118570872715541}, {"text": "WMT", "start_pos": 216, "end_pos": 219, "type": "DATASET", "confidence": 0.7291300296783447}]}, {"text": " Table 5: Pearson's correlation of metric scores with the WMT 2016 direct assessment of absolute trans- lation adequacy at segment-level.", "labels": [], "entities": [{"text": "WMT", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.7710180282592773}]}, {"text": " Table 6: Pearson's correlation of metric scores  with the WMT 2016 HUME human assessment at  segment-level.", "labels": [], "entities": [{"text": "WMT 2016 HUME human assessment", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.8682101488113403}]}, {"text": " Table 7: Average time in seconds for each step of  evaluating a typical WMT system using MEANT:  tokenizing both the reference and the MT output;  loading the distributional lexical semantic similar- ity model; semantic role labeling the reference and  the MT output; and scoring the MT output.", "labels": [], "entities": [{"text": "Average time", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.950906902551651}, {"text": "MEANT", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9004338979721069}]}]}