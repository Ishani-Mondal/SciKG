{"title": [{"text": "Learning Bilingual Projections of Embeddings for Vocabulary Expansion in Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.6954239308834076}]}], "abstractContent": [{"text": "We propose a simple log-bilinear softmax-based model to deal with vocabulary expansion in machine translation.", "labels": [], "entities": [{"text": "vocabulary expansion", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7404206395149231}, {"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7313981354236603}]}, {"text": "Our model uses word embeddings trained on significantly large unlabelled monolingual corpora and learns over a fairly small, word-to-word bilingual dictionary.", "labels": [], "entities": []}, {"text": "Given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language using the trained bilingual embeddings.", "labels": [], "entities": []}, {"text": "We integrate these translation options into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the English-Spanish language pair.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 55, "end_pos": 99, "type": "TASK", "confidence": 0.5952270999550819}]}, {"text": "When tested over an out-of-domain test-set, we get a significant improvement of 3.9 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9994230270385742}]}], "introductionContent": [{"text": "Data-driven machine translation systems are able to translate words that have been seen in the training parallel corpora, however translating unseen words is still a major challenge for even the best performing systems.", "labels": [], "entities": [{"text": "Data-driven machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5978567898273468}]}, {"text": "The amount of parallel data is finite (and sometimes scarce) and, therefore, word types like named entities, domain specific content words, or infrequent terms are rare.", "labels": [], "entities": []}, {"text": "This lack of information can potentially result in incomplete or erroneous translations.", "labels": [], "entities": []}, {"text": "This problem has been actively studied in the field of machine translation (MT); Dou and Knight, * This work was done while the authors were in TALP Research Center, Universitat Polit\u00e8cnica de Catalunya, 2012;).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8450720131397247}]}, {"text": "Lexiconbased resources have been used for resolving unseen content words by exploiting a combination of monolingual and bilingual resources.", "labels": [], "entities": []}, {"text": "In this context, distributed word representations, or word embeddings (WE), have been recently applied to resolve unseen word related problems (.", "labels": [], "entities": []}, {"text": "In general, word representations capture rich linguistic relationships and several works () try to use them to improve MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 119, "end_pos": 121, "type": "TASK", "confidence": 0.9956879019737244}]}, {"text": "However, very few approaches use them directly to resolve the out-of-vocabulary (OOV) problem in MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9797011613845825}]}, {"text": "Previous research in MT systems suggests that a significant number of named entities (NE) can be handled by using simple pre or post-processing methods, e.g., transliteration techniques).", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9915962815284729}]}, {"text": "However, a change in domain results in a significant increase in the number of unseen content words for which simple pre or post-processing methods are sub-optimal (.", "labels": [], "entities": []}, {"text": "Our work is inspired by the recent advances () in applications of word embeddings to the task of vocabulary expansion in the context of statistical machine translation (SMT).", "labels": [], "entities": [{"text": "vocabulary expansion", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.7268715649843216}, {"text": "statistical machine translation (SMT)", "start_pos": 136, "end_pos": 173, "type": "TASK", "confidence": 0.7763126542170843}]}, {"text": "Our focus in this paper is to resolve unseen content words by using continuous word embeddings on both the languages and learn a model over a small seed lexicon to map the embedding spaces.", "labels": [], "entities": []}, {"text": "To this extent, our work is similar to where the authors map distributional representations using a linear regression method similar to and insert anew feature based on cosine similarity metric into the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 203, "end_pos": 205, "type": "TASK", "confidence": 0.8136393427848816}]}, {"text": "On the other hand, there is a rich body of recent literature that focuses on obtaining bilingual word embeddings using either sentence aligned or document aligned corpora).", "labels": [], "entities": []}, {"text": "Our approach is significantly different as we obtain embeddings separately on monolingual corpora and then use supervision in the form of a small sparse bilingual dictionary, in some terms similar to.", "labels": [], "entities": []}, {"text": "We use a simple yet principled method to obtain a probabilistic conditional distribution of words directly and these probabilities allow us to expand the translation model for new words.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the log-bilinear softmax model, and its integration into an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9890077114105225}]}, {"text": "The experimental work is described in Section 3.", "labels": [], "entities": []}, {"text": "Finally, we conclude and sketch some avenues for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Top-10 accuracy (in percentage) for bilingual dictionary induction for English-German and  English-French.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9969749450683594}, {"text": "bilingual dictionary induction", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.5671242972215017}]}, {"text": " Table 2: OOVs on the dev and test sets.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.629788875579834}]}, {"text": " Table 3: Automatic evaluation of the translation  systems defined in Section 3. The best system is  bold-faced (see text for statistical significance).", "labels": [], "entities": []}]}