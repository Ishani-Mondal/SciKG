{"title": [{"text": "Normalizing Medieval German Texts: from rules to deep learning", "labels": [], "entities": [{"text": "Normalizing Medieval German Texts", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8677129000425339}]}], "abstractContent": [{"text": "The application of NLP tools to historical texts is complicated by a high level of spelling variation.", "labels": [], "entities": []}, {"text": "Different methods of historical text normalization have been proposed.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.6578255792458853}]}, {"text": "In this comparative evaluation I test the following three approaches to text canonicalization on historical German texts from 15 th-16 th centuries: rule-based, statistical machine translation, and neural machine translation.", "labels": [], "entities": [{"text": "text canonicalization", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.7011641263961792}, {"text": "statistical machine translation", "start_pos": 161, "end_pos": 192, "type": "TASK", "confidence": 0.6278742452462515}, {"text": "neural machine translation", "start_pos": 198, "end_pos": 224, "type": "TASK", "confidence": 0.7532890240351359}]}, {"text": "Character based neu-ral machine translation, not being previously tested for the task of normalization, showed the best results.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.5499596496423086}]}], "introductionContent": [{"text": "Due to an increased interest in Digital Humanities, more and more heritage texts are becoming available in digital format.", "labels": [], "entities": []}, {"text": "The ever growing amount of these text collections motivates researchers to use automatic methods for its processing.", "labels": [], "entities": []}, {"text": "In many cases, automatic processing of historical corpora is complicated by a high level of spelling variation.", "labels": [], "entities": [{"text": "automatic processing of historical corpora", "start_pos": 15, "end_pos": 57, "type": "TASK", "confidence": 0.8088330566883087}]}, {"text": "Non-standardized orthography, resulting in inconsistent data, is a substantial obstacle to the application of the existing NLP tools.", "labels": [], "entities": []}, {"text": "Normalization of historical texts, i.e., the mapping of historical word forms to their modern equivalents (see), has proven to bean effective method of improving the quality of the automatic processing of historical corpora.", "labels": [], "entities": []}, {"text": "Sentence in historical German (SOURCE) and its modernised spelling (NORM.).", "labels": [], "entities": [{"text": "Sentence in historical German (SOURCE)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.532627706016813}, {"text": "spelling (NORM.)", "start_pos": 58, "end_pos": 74, "type": "METRIC", "confidence": 0.8626641482114792}]}, {"text": "Various approaches to text normalization have been proposed.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8442190289497375}]}, {"text": "For instance, methods based on the Levenshtein edit distance algorithm and its variations are widely used for text canonicalization.", "labels": [], "entities": [{"text": "text canonicalization", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.794334888458252}]}, {"text": "described a technique performing automatic Levenshtein-based rule derivation from a word-aligned parallel corpus.", "labels": [], "entities": []}, {"text": "presented a different string similarity approach, using context-sensitive, weighted edit distance calculations combined with compound splitting.", "labels": [], "entities": []}, {"text": "Another approach, applying character-based statistical machine translation (SMT) is documented in (.", "labels": [], "entities": [{"text": "character-based statistical machine translation (SMT)", "start_pos": 27, "end_pos": 80, "type": "TASK", "confidence": 0.7120271665709359}]}, {"text": "conducted a comparative evaluation of the following three normalization approaches: filtering, Levenshteinbased and SMT-based, to show that the latter generally outperformed the former two methods.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 116, "end_pos": 125, "type": "TASK", "confidence": 0.9776041507720947}]}, {"text": "reported that a deep neural network architecture improves the normalization of historical texts, compared to both baseline using conditional random fields and Norma tool.", "labels": [], "entities": [{"text": "normalization of historical texts", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.8255001753568649}]}, {"text": "Deep learning methods are known to work best with large amounts of data, and yet the authors witnessed an improvement with only a few thousand tokens of training material.", "labels": [], "entities": []}, {"text": "Considering the above mentioned successful applications of both character-based SMT and neural networks for normalization of historical texts, I explore the suitability of character-based neural machine translation for this task., and presented character-based neural MT systems improving machine translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.7272742390632629}, {"text": "normalization of historical texts", "start_pos": 108, "end_pos": 141, "type": "TASK", "confidence": 0.8588280379772186}, {"text": "character-based neural machine translation", "start_pos": 172, "end_pos": 214, "type": "TASK", "confidence": 0.7111908942461014}, {"text": "machine translation", "start_pos": 289, "end_pos": 308, "type": "TASK", "confidence": 0.7350996732711792}]}, {"text": "Moreover, compared to the deep learning architecture described in (, a neural MT system does not require an explicit character alignment, which makes the normalization setup easier.", "labels": [], "entities": []}, {"text": "This paper reports the results of a comparative evaluation of normalization methods applied to Early New High German texts . For this assessment I tested the following normalization methods: edit-based, statistical machine translation, and neural machine translation.", "labels": [], "entities": [{"text": "Early New High German texts", "start_pos": 95, "end_pos": 122, "type": "DATASET", "confidence": 0.6821647644042969}, {"text": "statistical machine translation", "start_pos": 203, "end_pos": 234, "type": "TASK", "confidence": 0.6422402858734131}, {"text": "neural machine translation", "start_pos": 240, "end_pos": 266, "type": "TASK", "confidence": 0.7435412208239237}]}, {"text": "The first two approaches were previously tested on German texts from the same period, but the application of neural MT to text normalization has not yet been documented.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9162241220474243}, {"text": "text normalization", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7757620513439178}]}, {"text": "Section 2 introduces the data used for the experiments.", "labels": [], "entities": []}, {"text": "In Section 3, I will describe the normalization methods.", "labels": [], "entities": [{"text": "normalization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9774729013442993}]}, {"text": "Section 4 will present evaluation results.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 I will summarize the outcome of the comparative evaluation and give some possible direction for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given the small size of the manually normalized baseline (2500 historical-modern word pairs), I applied 10-fold cross-validation to evaluate the performance of the three normalization methods.", "labels": [], "entities": []}, {"text": "First, the experiments were conducted on the baseline, with 2000 pairs (2250 for Norma) of training data, 250 pairs in development set (for SMT and neural MT), and 250 pairs in the test set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9778532385826111}]}, {"text": "Then, the training set was augmented with LemmData and GerManC data, while using both development and test sets in their initial size.", "labels": [], "entities": [{"text": "GerManC data", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.7564482390880585}]}, {"text": "The neural MT system trained on the baseline combined with showed the best accuracy score, 0.81.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9148285984992981}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9996415376663208}]}, {"text": "It is followed by SMT results, 0.79, trained on 18,857 tokens of the baseline augmented with LemmData.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8858373761177063}]}, {"text": "To estimate the average variability in the output between the folds of test data, I calculated the standard deviation of the accuracy for each system (SD acc in).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9989765882492065}]}, {"text": "This measure demonstrates how close or faraway the data is from the mean (average accuracy, ACC in).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9030249714851379}, {"text": "ACC in)", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9683709144592285}]}, {"text": "It approximates the mean distance between each fold and the arithmetic mean.", "labels": [], "entities": []}, {"text": "The majority of the data (68.2% assuming that the distribution is normal) would be located between one standard deviation above and below the mean.", "labels": [], "entities": []}, {"text": "For instance, given the average accuracy 0.75 of the Norma baseline system, the standard deviation 0.03 means that the accuracy scores for the majority of the folds vary from 0.72 to 0.78.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9643390774726868}, {"text": "Norma baseline system", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.9709149400393168}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9993013143539429}]}, {"text": "The standard deviation between different systems changes slightly, from 0.02 to 0.04.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9290077984333038}]}, {"text": "It is interesting to observe, how the systems respond to the augmentation of the training set (see).", "labels": [], "entities": []}, {"text": "While the performance of the rulebased system, Norma, remains rather stable, it changes by the other two systems.", "labels": [], "entities": [{"text": "Norma", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9172850847244263}]}, {"text": "The SMT system first reacts positively to the increase of the training data with LemmData.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9888508915901184}, {"text": "LemmData", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9436679482460022}]}, {"text": "This data is similar to the baseline in its regional provenance, though is very varied with respect to the covered time periods (see).", "labels": [], "entities": []}, {"text": "When the training set was further augmented with GerManC, belonging to a later period of time, it resulted in a performance decrease.", "labels": [], "entities": [{"text": "GerManC", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9704407453536987}]}, {"text": "On the other hand, the performance of the neural MT system steadily increased with each addition of data.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.8435206413269043}]}, {"text": "This observation corresponds to the one made in ( where the normalization accuracy increased with a deep learning normalization method and remained stable or decreased with other methods, including Norma.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9264913201332092}]}, {"text": "The accuracy and character error rate scores of the three normalization systems compared in the best performing configurations does not differ much: from Norma's 0.75/0.14 to neural MT's 0.81/0.08.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994889497756958}, {"text": "character error rate", "start_pos": 17, "end_pos": 37, "type": "METRIC", "confidence": 0.6406681934992472}]}, {"text": "To estimate how different the output of the systems actually is, I conducted a quantitative analysis of the output (see).", "labels": [], "entities": []}, {"text": "First, I compared how similar is the output of the systems, i.e., how often the systems agree on a certain normalization.", "labels": [], "entities": []}, {"text": "The lowest, 70%, is the agreement between the three systems, and the highest, 80%, between the SMT and the neural MT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9337670207023621}]}, {"text": "In addition, based on the amount of the commonly incorrect cases, I calculated the percentage of the \"error agreement\", i.e., how often the systems produced the same erroneous normalization.", "labels": [], "entities": [{"text": "error agreement\"", "start_pos": 102, "end_pos": 118, "type": "METRIC", "confidence": 0.9576229055722555}]}, {"text": "The pair SMT/neural MT leads with 51% of error similarity.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.988439679145813}, {"text": "error similarity", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.892072319984436}]}, {"text": "Thus, the output produced by SMT and neural MT systems is the most similar.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.983180046081543}]}, {"text": "It can be explained by the statistical nature of both systems, in contrast to the rule-based Norma.", "labels": [], "entities": []}, {"text": "presents contrastive examples of the output, where one system produced the correct normalization, and the other two failed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora used in this case study.", "labels": [], "entities": []}, {"text": " Table 2: Averaged evaluation results, i.e., accuracy (ACC) and character error rate (CER) over 10 folds.", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 45, "end_pos": 59, "type": "METRIC", "confidence": 0.9601112753152847}, {"text": "character error rate (CER)", "start_pos": 64, "end_pos": 90, "type": "METRIC", "confidence": 0.91314830382665}]}, {"text": " Table 3: Analysis of the output: total amount of cases the systems agreed upon (Agreement) and amount  of cases where the systems produced the same incorrect normalization, calculated based on the number  of common incorrect cases.", "labels": [], "entities": []}]}