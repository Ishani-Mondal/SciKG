{"title": [{"text": "Class-based Prediction Errors to Detect Hate Speech with Out-of-vocabulary Words", "labels": [], "entities": []}], "abstractContent": [{"text": "Common approaches to text categoriza-tion essentially rely either on n-gram counts or on word embeddings.", "labels": [], "entities": [{"text": "text categoriza-tion", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.713935911655426}]}, {"text": "This presents important difficulties in highly dynamic or quickly-interacting environments , where the appearance of new words and/or varied misspellings is the norm.", "labels": [], "entities": []}, {"text": "A paradigmatic example of this situation is abusive online behavior, with social networks and media platforms struggling to effectively combat uncommon or non-blacklisted hate words.", "labels": [], "entities": []}, {"text": "To better deal with these issues in those fast-paced environments , we propose using the error signal of class-based language models as input to text classification algorithms.", "labels": [], "entities": [{"text": "text classification", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7301938533782959}]}, {"text": "In particular , we train a next-character prediction model for any given class, and then exploit the error of such class-based models to inform a neural network classifier.", "labels": [], "entities": []}, {"text": "This way, we shift from the ability to describe seen documents to the ability to predict unseen content.", "labels": [], "entities": []}, {"text": "Preliminary studies using out-of-vocabulary splits from abusive tweet data show promising results, out-performing competitive text categorization strategies by 4-11%.", "labels": [], "entities": []}], "introductionContent": [{"text": "The first steps in automatic text categorization rely on the description of the document content.", "labels": [], "entities": [{"text": "automatic text categorization", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6123580535252889}]}, {"text": "Typically, such content is characterized by some sort of word, stem, and/or n-gram counts (e.g. or, more recently, by unsupervised or semi-supervised word/sentence/paragraph embeddings (e.g..", "labels": [], "entities": []}, {"text": "While the common pipeline performs well fora myriad of tasks, there area number of situations where a large ratio of seen vs. unseen tokens threatens the performance of the classifier.", "labels": [], "entities": []}, {"text": "This is the case, for instance, with abusive language in online user content or microblogging sites (.", "labels": [], "entities": []}, {"text": "In such cases, the volume of annotated data is not massive, and the vocabulary changes rapidly and non-consistently across sites and user communities.", "labels": [], "entities": []}, {"text": "Sometimes these changes and inconsistencies are intentional, so as to hide the real meaning from traditional automatic detectors using hateword dictionaries or blacklists.", "labels": [], "entities": []}, {"text": "For example, the notorious online community 4chan launched \"Operation Google\", which aimed to replace racial slurs on social media with the names of prominent tech companies in a sort of adversarial attack on Google's Jigsaw project (.", "labels": [], "entities": []}, {"text": "To avoid relying on specific tokens, existing text classification approaches can incorporate socalled linguistic features (), such as number of tokens, length of tokens, number of punctuations, and soon, or syntactic features (, based on partof-speech tags, dependency relations, and sentence structure.", "labels": [], "entities": [{"text": "text classification", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7494296729564667}]}, {"text": "Distributional representations () and recurrent neural network (RNN) language models are also used, together with more classical approaches involving word or character n-grams (.", "labels": [], "entities": []}, {"text": "The task of automatic hate speech detection is usually framed as a supervised learning problem.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8046254316965739}]}, {"text": "Surface-level features like bag-of-words and embeddings systematically produce reasonable clas-sification results.", "labels": [], "entities": []}, {"text": "Character-level approaches perform better than token-level ones, since the rare spelling variations of slang and swear words will result in unknown tokens . More complex features like dependency-parse information or specific linguistic attributes like politeness-imperatives have been effective).", "labels": [], "entities": []}, {"text": "Also lexical resources like lists of slurs, are proven effective but only in combination with other features).", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the suitability of the proposed concepts, we choose the task of detecting abusive tweets with out-of-vocabulary (OOV) words, using a sample of 2 million tweets from the Twitter 1% feed.", "labels": [], "entities": []}, {"text": "Half of the sample is selected based on their use of 'hate words' from a crowdsourced dictionary 1 , while the other half is selected randomly.", "labels": [], "entities": []}, {"text": "We manually filter the dictionary to remove overly common and ambiguous words like \"india\", a localized slur for dark skinned people.", "labels": [], "entities": []}, {"text": "Due to lack of baselines and datasets that take into account OOV words, we create semi-synthetic splits with OOV words, in order to simulate data with new or heavily misspelled tokens.", "labels": [], "entities": []}, {"text": "We randomly perform 10 train/validation/test splits, compute the area under the receiver operating characteristic curve (AUC), and report the mean and standard deviation over the 10 test splits.", "labels": [], "entities": [{"text": "receiver operating characteristic curve (AUC)", "start_pos": 80, "end_pos": 125, "type": "METRIC", "confidence": 0.8985337955611092}]}, {"text": "Two setups are considered: 1 http://hatebase.org \u2022 Easy -For the positive class, we take the hate-word list and split it into 70% for training and 15% for validation and testing.", "labels": [], "entities": [{"text": "Easy", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9895339608192444}]}, {"text": "In performing such split, we force words with the same stem to end up in the same split.", "labels": [], "entities": []}, {"text": "We then select tweets from the entire corpus that contain at least one stemmed word from each split.", "labels": [], "entities": []}, {"text": "For the negative class, we just select randomly from the remaining tweets until we have balanced train/validation/test sets.", "labels": [], "entities": []}, {"text": "\u2022 Hard -Besides the list of hate words, we also consider a list of common words (top 1,000 to 3,000 most frequent English words 2 ).", "labels": [], "entities": []}, {"text": "We proceed as with the positive class of the Easy setup, and generate balanced train/validation/test splits for both abusive and non-abusive tweets.", "labels": [], "entities": []}, {"text": "In addition, to increase difficulty, we remove tweets with list words appearing in more than one split (that is, we ensure that the intersection of listed words is null between train/validation/test).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there are no established benchmark datasets for the problem of OOV word detection.", "labels": [], "entities": [{"text": "OOV word detection", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8974647720654806}]}, {"text": "Recent work on characterizing OOV words in twitter attempted to build a dataset of such words (), but mostly focused on content analysis and categorization (e.g., wassup and iknow, belong to word mergings).", "labels": [], "entities": []}, {"text": "We plan to develop big crowd-sourced datasets of OOV social media texts and provide them free to the research community.: AUC scores for the considered baselines (see text) and the proposed approach.", "labels": [], "entities": [{"text": "AUC", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9329729676246643}]}, {"text": "A null randomized model yields 0.514 (0.082) and 0.503 (0.090) for the Easy and Hard setups, respectively.", "labels": [], "entities": []}], "tableCaptions": []}