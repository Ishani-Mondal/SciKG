{"title": [{"text": "Structured Learning for Context-aware Spoken Language Understanding of Robotic Commands", "labels": [], "entities": [{"text": "Spoken Language Understanding of Robotic Commands", "start_pos": 38, "end_pos": 87, "type": "TASK", "confidence": 0.7782641351222992}]}], "abstractContent": [{"text": "Service robots are expected to operate in specific environments, where the presence of humans plays a key role.", "labels": [], "entities": []}, {"text": "A major feature of such robotics platforms is thus the ability to react to spoken commands.", "labels": [], "entities": []}, {"text": "This requires the understanding of the user utterance with an accuracy able to trigger the robot reaction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9986149072647095}]}, {"text": "Such correct interpretation of linguistic exchanges depends on physical , cognitive and language-dependent aspects related to the environment.", "labels": [], "entities": [{"text": "interpretation of linguistic exchanges", "start_pos": 13, "end_pos": 51, "type": "TASK", "confidence": 0.7778093963861465}]}, {"text": "In this work, we present the empirical evaluation of an adaptive Spoken Language Understanding chain for robotic commands, that explicitly depends on the operational environment during both the learning and recognition stages.", "labels": [], "entities": []}, {"text": "The effectiveness of such a context-sensitive command interpretation is tested against an extension of an already existing corpus of commands, that introduced explicit perceptual knowledge: this enabled deeper measures proving that more accurate disambiguation capabilities can be actually obtained.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, one of the most challenging issues that Service Robotics is facing is the automation of high level and collaborative interactions between humans and robots.", "labels": [], "entities": []}, {"text": "In such a robotic context, human language is the most natural way of communication as for its expressiveness and flexibility.", "labels": [], "entities": []}, {"text": "However, an effective communication in natural language between humans and robots is challenging mostly for the different cognitive abilities it involves.", "labels": [], "entities": []}, {"text": "For a robot to react to a simple command like \"take the mug in the kitchen\", a number of implicit assumptions should be met.", "labels": [], "entities": []}, {"text": "First, at least two entities, a mug and a kitchen, must exist in the environment and the speaker must be aware of such entities.", "labels": [], "entities": []}, {"text": "Accordingly, the robot must have access to an inner representation of its world, e.g., an explicit map of the environment.", "labels": [], "entities": []}, {"text": "Second, mappings from lexical references to real world entities must be developed or made available.", "labels": [], "entities": []}, {"text": "In this respect, the Grounding process) links symbols (e.g., words) to the corresponding perceptual information.", "labels": [], "entities": []}, {"text": "Hence, robot interactions need to be grounded, as meaning depends on the state of the physical world and the interpretation crucially interplays with perception, as pointed out by psycho-linguistic theories (.", "labels": [], "entities": []}, {"text": "The integration of perceptual information derived from the robot's sensors with an ontologically motivated description of the world has been adopted as an augmented representation of the environment, in the so-called semantic maps.", "labels": [], "entities": []}, {"text": "In these maps, the existence of real world objects can be associated to lexical information, in the form of entity names given by a knowledge engineer or spoken by a user fora pointed object, as in Human-Augmented Mapping (.", "labels": [], "entities": []}, {"text": "While Command Interpretation for Interactive Robotics has been mostly carried out over the only evidence specific to the linguistic level (see, for example, ()), we argue that a proper Spoken Language Understanding (SLU) for HumanRobot Interaction should be context-aware, in the sense that both the user and the robot live in and make references to a shared environment.", "labels": [], "entities": [{"text": "Command Interpretation", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.7312718331813812}]}, {"text": "For example, in the above command, \"taking\" is the intended action whenever a mug is actually in the kitchen, so that \"the mug in the kitchen\" refers to a single argument.", "labels": [], "entities": []}, {"text": "On the contrary, the command may refer to a \"bringing\" action, when no mug is in the kitchen and the mug and in the kitchen correspond to different semantic roles.", "labels": [], "entities": []}, {"text": "We are interested in an approach for the interpretation of robotic spoken commands that is consistent with (i) the world (with all the entities composing it), (ii) the Robotic Platform (with its inner representations and capabilities), and (iii) the linguistic information derived from the user's utterance.", "labels": [], "entities": [{"text": "interpretation of robotic spoken commands", "start_pos": 41, "end_pos": 82, "type": "TASK", "confidence": 0.7910037636756897}]}, {"text": "In this paper, we foster machine learning methodologies for Spoken Language Understanding that force the above research perspective: this is obtained by extending the linguistic evidence that can be extracted from the uttered commands with perceptual evidence directly derived by the semantic map of a robot.", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.9116833209991455}]}, {"text": "In particular, the interpretation process is modeled as a sequence labeling problem where the final labeler is trained by applying Structured Learning methods over realistic commands expressed in domestic environments, as in (.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.6625553965568542}]}, {"text": "The resulting interpretations adhere to Frame Semantics: this well-established theory provides a strong linguistic foundations to the overall process while enforcing its applicability, as it is made independent of the vast plethora of existing robotic platforms.", "labels": [], "entities": []}, {"text": "Such methodologies have been implemented in a free and ready-to-use framework, here presented, whose name is LU4R -an adaptive spoken Language Understanding framework for(4) Robots.", "labels": [], "entities": []}, {"text": "LU4R is entirely coded in Java and, thanks to its Client/Server architectural design, it is completely decoupled from the robot, enabling for an easy and fast deployment on every platform . As the aforementioned approaches rely on realistic data, in this work we also present an extended version of HuRIC -a Human Robot Interaction Corpus, originally introduced in () This resource is a collection of realistic spoken commands that users might express towards generic service robots.", "labels": [], "entities": []}, {"text": "In this resource, each sentence is labeled with morphosyntactic information (e.g., dependency relations, POS tags, . .", "labels": [], "entities": []}, {"text": "), along with its correct interpretation in terms of semantic frames ().", "labels": [], "entities": []}, {"text": "In our extension, each annotated sentence is paired with a semantic representation of the world, that justifies the command itself.", "labels": [], "entities": []}, {"text": "To the best of our knowledge this is the first corpus providing such a rich representation of a robotic spoken command 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to provide evidence about the benefits of perceptual knowledge, we report an evaluation of the interpretation process of robotic commands over the enhanced version of HuRIC, i.e., contemplating the semantic maps for each sentence.", "labels": [], "entities": [{"text": "HuRIC", "start_pos": 176, "end_pos": 181, "type": "DATASET", "confidence": 0.8782058954238892}]}, {"text": "The results, expressed in terms of Precision, Recall and F1 measure, focus on the semantic interpretation process, in particular Action Detection (AD), Argument Identification (AI) and Argument Classification (AC) steps, addressing two possible configurations: a basic setting where only linguistic information is exploited (i.e., noSM, as the semantic maps are ignored), and the configuration where semantic maps are included into the learning loop (i.e., SM).", "labels": [], "entities": [{"text": "Precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9845622181892395}, {"text": "Recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.8742325305938721}, {"text": "F1 measure", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9666191637516022}, {"text": "semantic interpretation", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.7157694399356842}]}, {"text": "F1 scores measure the quality of a specific module.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9900001883506775}]}, {"text": "While in the AD step the F1 refers provides also the standard deviations among the different folds.", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.995121419429779}]}, {"text": "We tested each sub-module in isolation, feeding each step with gold information provided by the previous step in the chain.", "labels": [], "entities": []}, {"text": "Moreover, the evaluation has been carried out considering the correct transcriptions, i.e., not contemplating the error introduced by the Automatic Speech Recognition system.", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 138, "end_pos": 166, "type": "TASK", "confidence": 0.5889718035856882}]}, {"text": "The overall results are encouraging for the application of the proposed approach in realistic scenarios.", "labels": [], "entities": []}, {"text": "In fact, the F1 is always higher than 94% in the recognition of semantic predicates used to express intended actions (AD).", "labels": [], "entities": [{"text": "F1", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9913663268089294}, {"text": "recognition of semantic predicates used to express intended actions (AD)", "start_pos": 49, "end_pos": 121, "type": "TASK", "confidence": 0.826018492380778}]}, {"text": "The system is able to recognize the involved entities (AC) with high accuracy as well, with a F1 higher than 93% in both noSM and SM settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9989932179450989}, {"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9992952346801758}]}, {"text": "This result is surprising when analyzing the complexity of the task.", "labels": [], "entities": []}, {"text": "In fact, the classifier is able to cope with a high level of uncertainty, as the amount of possible semantic roles is sizable, i.e., 34.", "labels": [], "entities": []}, {"text": "In general, the most challenging task seems to be the ability to recognize the spans composing a single frame element (AI).", "labels": [], "entities": []}, {"text": "Hence, the resulting interpretation will be wrong, as it does not reflect the semantics that is motivated by the environment.", "labels": [], "entities": []}, {"text": "In terms of F1 measure, this issue affects mainly the Argument Identification step (AI), rather than the Action Detection (AD) one, as for each (possibly) wrong frame, there could be more than two (possibly) wrong arguments.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9845724403858185}, {"text": "Argument Identification step (AI)", "start_pos": 54, "end_pos": 87, "type": "METRIC", "confidence": 0.8788407742977142}, {"text": "Action Detection (AD)", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.6424688637256623}]}, {"text": "For example, the sentence \"take the mug in the kitchen\" will be probably recognized to be a Taking action, even though it is labeled as Bringing, i.e., mug and kitchen are supposed to be far in the environment.", "labels": [], "entities": []}, {"text": "While the AD step will receive just one penalty for the wrong recognized action, the AI step is penalized twice, as two arguments were expected by the gold standard annotation, i.e., the the mug as THEME and the in the kitchen as GOAL, instead of one, i.e., the mug in the kitchen as a single THEME argument.", "labels": [], "entities": [{"text": "THEME", "start_pos": 198, "end_pos": 203, "type": "METRIC", "confidence": 0.8304146528244019}, {"text": "GOAL", "start_pos": 230, "end_pos": 234, "type": "METRIC", "confidence": 0.9876134395599365}]}, {"text": "When looking at the SM setting, it seems that the injection of perceptual knowledge into the semantic analysis process is able to mitigate the effect of the aforementioned phenomena and each SLU step gains in predictive performance.", "labels": [], "entities": [{"text": "SM", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9579044580459595}]}, {"text": "In the case of AD, the information about the entities shows a relative improvement of +2.03% in terms of F1 (94.37% vs 96.29%).", "labels": [], "entities": [{"text": "F1", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9997952580451965}]}, {"text": "This means that the semantic map allows to predict the intended action more accurately, whenever the underlying semantic ambiguity depends on the configuration of the environment.", "labels": [], "entities": []}, {"text": "The tight correlation between the predicted action and the frame elements suggests a similar behavior in Argument Identification.", "labels": [], "entities": [{"text": "Argument Identification", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.8130980134010315}]}, {"text": "In fact, as well as for the AD, in the AI step perceptual knowledge reveal its support in predicting the correct spans of semantic arguments, with a relative improvement of +3.34% w.r.t. the F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9776906669139862}]}, {"text": "Though a lower gain is observed (+1.04%), the introduction of Distributional Semantics improves the ability of recognizing the correct frame element fora given argument span, i.e., AC step.", "labels": [], "entities": []}, {"text": "This is probably due to the lexical generalization provided by the word embeddings, whenever alternative naming are used to refer to an entity of the semantic map.", "labels": [], "entities": []}, {"text": "Finally, small values of standard deviation suggest that the system seems to be rather stable across the different iterations of the experiment and that the results do not depend on specific splits of the entire dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Experimental evaluation of the semantic  interpretation process", "labels": [], "entities": [{"text": "semantic  interpretation", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.813942164182663}]}]}