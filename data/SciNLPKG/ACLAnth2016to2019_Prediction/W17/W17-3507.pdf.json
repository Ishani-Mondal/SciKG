{"title": [{"text": "Exploring the Behavior of Classic REG Algorithms in the Description of Characters in 3D Images *", "labels": [], "entities": []}], "abstractContent": [{"text": "Describing people and characters can be very useful in different contexts, such as computational narrative or image description for the visually impaired.", "labels": [], "entities": [{"text": "Describing people and characters", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8894776999950409}, {"text": "image description", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7233962267637253}]}, {"text": "However, a review of the existing literature shows that the automatic generation of people descriptions has not received much attention.", "labels": [], "entities": [{"text": "automatic generation of people descriptions", "start_pos": 60, "end_pos": 103, "type": "TASK", "confidence": 0.7095159769058228}]}, {"text": "Our work focuses on the description of people in snapshots from a 3D environment.", "labels": [], "entities": []}, {"text": "First, we have conducted a survey to identify the way in which people describe other people under different conditions.", "labels": [], "entities": []}, {"text": "We have used the information extracted from this survey to design several Referring Expression Generation algorithms which produce similar results.", "labels": [], "entities": [{"text": "Referring Expression Generation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.8817306558291117}]}, {"text": "We have evaluated these algorithms with users in order to identify which ones generate the best description for specific characters in different situations.", "labels": [], "entities": []}, {"text": "The evaluation has shown that, in order to generate good descriptions , a combination of different algorithms has to be used depending on the features and situation of the person to be described.", "labels": [], "entities": []}], "introductionContent": [{"text": "In every conversation, human beings refer to people, objects, places and situations, and we need to be able to describe them accurately so that the hearer knows who or what we are referring to.", "labels": [], "entities": []}, {"text": "In order to be able to automatically create descriptions that can be useful in real life situations -such as generating descriptions for the visually impaired -where the complexity of the information needed to generate them is noteworthy, we first need to tackle specific aspects of these problems that bring light to the more general problem we intend to solve.", "labels": [], "entities": []}, {"text": "In this work we focus on the description of people in snapshots from a 3D environment, considering that feature extraction can be perfectly performed.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7337293922901154}]}, {"text": "Whereas most approaches to image description work with real world images, we have opted for 3D images because they allow us to easily manipulate the entities and their features in order to test different hypothesis, and we can create more complex situations to test our algorithms.", "labels": [], "entities": [{"text": "image description", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7297464162111282}]}, {"text": "In addition, we only focus on the description of people.", "labels": [], "entities": []}, {"text": "Except for the TUNA corpus (, which contained a set of close-up photographs of people, and the algorithms that used it in the TUNA Challenges (, to the best of our knowledge there are no works only focusing on people when describing visual images taken in real environments.", "labels": [], "entities": [{"text": "TUNA corpus", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.947441041469574}]}, {"text": "The insights obtained in our work can improve the generation of descriptions for images where people are detected, as the examples presented by other studies show how references to people do notwork in the same way as references to other entities do.", "labels": [], "entities": []}, {"text": "As a first step towards the implementation of a REG algorithm for describing people in 3D environments, we have explored the performance of classic REG algorithms for the task.", "labels": [], "entities": []}, {"text": "We chose two wellknown algorithms that can be easily configured depending on the the type of entities to be described: the Greedy (  and the Incremental Algorithms (.", "labels": [], "entities": []}, {"text": "As these algorithms require a predefined list of attributes that define the referent's appearance, we carried out a small study in order to determine the attributes that people include when describing people in real-life images (section 3).", "labels": [], "entities": []}, {"text": "Then, we implemented the algorithms (and some variations) taking into account the obtained results (section 4), and asked people to judge the quality of their output when generating descriptions of characters in a 3D environment (section 5).", "labels": [], "entities": []}, {"text": "For both evaluations, we have taken an approach similar to the one in (), which consists of an internet-based evaluation that allows for lower costs (it becomes unnecessary to summon a group of subjects to tryout the system in a specific place).", "labels": [], "entities": []}, {"text": "Users could easily access each survey using a link we provided, and they could do this at anytime and from anyplace.", "labels": [], "entities": []}], "datasetContent": [{"text": "After implementing all the algorithms, we tested them in order to find out if there was one that worked better than the rest in all situations or which one worked best depending on the situation.", "labels": [], "entities": []}, {"text": "In this survey, we showed the participants snapshots of the university canteen taken in a 3D virtual environment built using the Unity 3D engine.", "labels": [], "entities": []}, {"text": "The characters' clothes and postures were modified to imitate the ones in the pictures (see.", "labels": [], "entities": []}, {"text": "The use of a 3D environment aids in the personalization of the scene, facilitating experiments in which any number of people and objects can be represented.", "labels": [], "entities": []}, {"text": "This way we were also able to appreciate the differences between the descriptions given fora photograph of areal scene, and a scene developed in a 3D virtual environment.", "labels": [], "entities": []}, {"text": "A total of fifty-two participants completed this survey: 54% were women and 46% were men; most of them (67%) were between eighteen and thirty years old, 17% were between thirty and forty years old, 4% were under eighteen, and 12% were over forty.", "labels": [], "entities": []}, {"text": "The structure of the survey and the order of the questions were carefully planned so they did not influence the users' opinions.", "labels": [], "entities": []}, {"text": "We wanted them to offer their own descriptions first, before reading and judging the descriptions generated by the algorithms.", "labels": [], "entities": []}, {"text": "We have also considered the effort and amount of time that they will have to spend on the survey, so they will not be tempted to leave it unfinished and we can get as many answers as possible.", "labels": [], "entities": []}, {"text": "Because both the way in which the test subjects describe someone in the 3D scene and their ability to recognize the target character given a referring expression were intended to be analyzed, this survey was divided in two parts.", "labels": [], "entities": []}, {"text": "In the first part, each test subject was asked to describe a certain person from three different scenes.", "labels": [], "entities": []}, {"text": "The goal was to examine whether their answers would be very different when faced with a 3D environment as opposed to photographs.", "labels": [], "entities": []}, {"text": "The results show that type, top color, top type and posture still were the most used attributes, and they were mentioned even more often than during the first survey.", "labels": [], "entities": []}, {"text": "The difference between inclusion of the color of the top garment and its type increased slightly, confirming that the color is a more salient attribute.", "labels": [], "entities": []}, {"text": "The inclusion of nearby people and nearby objects approximately doubled in both cases, possibly due to the simplified representation of the room and the characters.", "labels": [], "entities": []}, {"text": "In the case of the nearby people, we could see that it is not always the closest person that gets mentioned, but the person that stands out the most among the closest ones.", "labels": [], "entities": []}, {"text": "The use of hair color decreased slightly, and hair type/length was rarely used, possibly because there were not many variations of hairstyles in the scene.", "labels": [], "entities": []}, {"text": "Even though two of the referents had a beard, the test subjects only mentioned it in 5.77% of the descriptions, much less than in the first survey and contrary to our hypothesis.", "labels": [], "entities": []}, {"text": "This maybe either due to the quality of the characters used or simply because the beard is not a very salient attribute.", "labels": [], "entities": []}, {"text": "Overall, the results showed a similar order in the preferred attributes, with relations to large areas and other people gaining more importance, and small details being used less.", "labels": [], "entities": []}, {"text": "For the second part of the survey, the test subjects were shown four scenes (see, each of them with several referring expressions for one referent, created and linguistically realized by our algorithms.", "labels": [], "entities": []}, {"text": "Then, they had to rate each of the descriptions on a five point Likert scale, the lowest value being \"very bad\" and the highest \"very good\".", "labels": [], "entities": []}, {"text": "Not all the algorithms were rated in all the scenes, either because they did not provide any useful information (e.g. there were no nearby objects in scene 1, so NOGA and NOIA were discarded), or because they gener-: \u2022 Greedy (GA): \"The girl sitting down\" \u2022 Incremental (IA): \"The girl in the white tank top who is sitting down.", "labels": [], "entities": [{"text": "NOGA", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.7693299651145935}, {"text": "NOIA", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9387813210487366}, {"text": "Incremental (IA)", "start_pos": 258, "end_pos": 274, "type": "METRIC", "confidence": 0.9393931925296783}]}, {"text": "\u2022 Exhaustive (EA): \"The girl with medium length brown hair, with the white tank top and blue trousers.\"", "labels": [], "entities": [{"text": "Exhaustive (EA)", "start_pos": 2, "end_pos": 17, "type": "METRIC", "confidence": 0.878058671951294}]}, {"text": "\u2022 Nearby Objects with Greedy (NOGA): \"The girl sitting down near the window.\"", "labels": [], "entities": []}, {"text": "\u2022 Nearby Objects with Incremental (NOIA): \"The girl in the white tank top who is sitting down.", "labels": [], "entities": [{"text": "Incremental (NOIA)", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.827539712190628}]}, {"text": "She is near the window.\"", "labels": [], "entities": []}, {"text": "\u2022 Nearby People with Greedy (NPGA): \"The girl sitting down next to the boy in the dark blue sweater.\"", "labels": [], "entities": []}, {"text": "\u2022 Nearby People with Incremental (NPIA): \"The girl in the white tank top who is sitting down.", "labels": [], "entities": []}, {"text": "She is next to the boy in the dark blue sweater.\"", "labels": [], "entities": []}, {"text": "The obtained results are shown in, where the average score for the descriptions generated by the algorithms in each of the four scenes are shown.", "labels": [], "entities": []}, {"text": "Relational algorithms have proved to have very high ratings.", "labels": [], "entities": []}, {"text": "This suggests that, at least for the particular scenes and situations shown to the participants, relational algorithms which include nearby people or objects can be very useful if there are distractors or objects that standout and can be related to the intended referent.", "labels": [], "entities": []}, {"text": "The results from the second survey also showed that users do not benefit from the inclusion of the beard or information about the referent's bottom garment or shoes, so we eliminated these from the attributes list.", "labels": [], "entities": []}, {"text": "In scenarios in which people wear very unusual clothing this may not be a correct decision, but since we are working with characters with casual attire, the bottom half of their clothes are not different enough from each other to standout.", "labels": [], "entities": []}, {"text": "Additionally, many characters are sitting down or are partially covered and some parts of their clothes are often not visible to the observer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average scores obtained by the algorithms", "labels": [], "entities": [{"text": "Average scores", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9656611680984497}]}]}