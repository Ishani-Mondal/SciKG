{"title": [{"text": "CASICT-DCU Neural Machine Translation Systems for WMT17", "labels": [], "entities": [{"text": "CASICT-DCU Neural Machine Translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.5432346761226654}, {"text": "WMT17", "start_pos": 50, "end_pos": 55, "type": "TASK", "confidence": 0.609741747379303}]}], "abstractContent": [{"text": "We participated in the WMT 2016 shared news translation task on English \u2194 Chi-nese language pair.", "labels": [], "entities": [{"text": "WMT 2016 shared news translation task", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.7960406243801117}]}, {"text": "Our systems are based on the encoder-decoder neural machine translation model with the attention mechanism.", "labels": [], "entities": [{"text": "encoder-decoder neural machine translation", "start_pos": 29, "end_pos": 71, "type": "TASK", "confidence": 0.6743709743022919}]}, {"text": "We employ the Gated Recurrent Unit (GRU) with the linear associative connection to build deep encoder and address the unknown words with the dictionary replace approach.", "labels": [], "entities": []}, {"text": "The dictionaries are extracted from the parallel training data with unsupervised word alignment method.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7014358043670654}]}, {"text": "In the decoding procedure, the translation probabilities of the target word from different models are averagely", "labels": [], "entities": []}], "introductionContent": [{"text": "We build the Neural Machine Translation systems CASICT-DCU for WMT17 English \u2194 Chinese news translation task.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6364699900150299}, {"text": "WMT17 English \u2194 Chinese news translation", "start_pos": 63, "end_pos": 103, "type": "TASK", "confidence": 0.7239871223767599}]}, {"text": "Our systems are based on the encoder-decoder model with the attention mechanism, which is also known as the RNNSearch model (.", "labels": [], "entities": []}, {"text": "To construct the deep RNN network, we employ the Gated Recurrent Unit () with the linear associative connection ( to ensure the fluent gradient propagation.", "labels": [], "entities": []}, {"text": "Adadelta algorithm is used to optimize the parameters and stochastic gradient descent algorithm with small learning rate is used in the fine-tuning stage.", "labels": [], "entities": []}, {"text": "We extract dictionaries from parallel training data with the unsupervised method to address the unknown words in target translation according to the word alignment vector.", "labels": [], "entities": []}, {"text": "During the decoding, the ensemble strategy is used to combine the translation probabilities of the target word from different models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The model performances on the valida- tion set in English to Chinese direction.", "labels": [], "entities": [{"text": "valida- tion set", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.8285205215215683}]}, {"text": " Table 2: The model performances on the valida- tion set in Chinese to English direction.", "labels": [], "entities": [{"text": "valida- tion set", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.8334378898143768}]}, {"text": " Table 3: The performance of our systems on the  test set.", "labels": [], "entities": []}]}