{"title": [{"text": "YNU-HPCC at EmoInt-2017: Using a CNN-LSTM Model for Sentiment Intensity Prediction", "labels": [], "entities": [{"text": "EmoInt-2017", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.6355560421943665}, {"text": "Sentiment Intensity Prediction", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.924869159857432}]}], "abstractContent": [{"text": "The sentiment analysis in this task aims to indicate the sentiment intensity of the four emotions (e.g. anger, fear, joy, and sadness) expressed in tweets.", "labels": [], "entities": []}, {"text": "Compared to the polarity classification, such intensity prediction can provide more fine-grained sentiment analysis.", "labels": [], "entities": []}, {"text": "In this paper, we present a system that uses a convolu-tional neural network with long short-term memory (CNN-LSTM) model to complete the task.", "labels": [], "entities": []}, {"text": "The CNN-LSTM model has two combined parts: CNN extracts local n-gram features within tweets and LST-M composes the features to capture long-distance dependency across tweets.", "labels": [], "entities": []}, {"text": "Our submission ranked tenth among twenty two teams by average correlation scores on prediction intensity for all four types of emotions .", "labels": [], "entities": [{"text": "correlation", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9598663449287415}]}], "introductionContent": [{"text": "Advanced Social Network Services (SNSs) such as Twitter, Facebook, and Weibo provide an online platform, where people share their personal interests, activities, thoughts, and emotions.", "labels": [], "entities": []}, {"text": "Sentiment analysis technology is used to automatically draw affective information from text.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8999785780906677}]}, {"text": "In recent researches, the majority of existing approaches and works on sentiment analysis aim to complete classification tasks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9570702314376831}]}, {"text": "In contrast, it is often useful to know the degree of an emotion expressed in text for applications such as movies, products, public sentiments and politics.", "labels": [], "entities": []}, {"text": "Such attractive applications provide the motivation for the WASSA-2017 shared task on Emotion Intensity (EmoInt), which is a competition focused on automatically determining the intensity of emotions in tweets.", "labels": [], "entities": [{"text": "WASSA-2017 shared task on Emotion Intensity (EmoInt)", "start_pos": 60, "end_pos": 112, "type": "TASK", "confidence": 0.7413266367382474}]}, {"text": "The task involves one-dimensional sentiment analysis, which requires a system for determining the strength (with a real-value score between 0 and 1) of an emotion expressed in a tweet.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8021154701709747}]}, {"text": "All tweets are divided into four datasets, each of which expresses an emotion including anger, fear, joy, and sadness.", "labels": [], "entities": []}, {"text": "The tweets with higher scores correspond to a greater degree of emotion.", "labels": [], "entities": []}, {"text": "In the relevant research field of sentiment analysis, it has been shown that many models are available for both categorical approaches and dimensional approaches.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9669412970542908}]}, {"text": "A categorical approach focuses on sentiment classification, while a dimensional approach aims to predict the intensity of emotions.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.9285172820091248}]}, {"text": "Recently, many methods have been successfully introduced for categorical sentiment analysis, such as word embedding ( , convolutional neural networks (CNN), recurrent neural networks (RNN) (, long short-term memory (LSTM), and bi-directional L-STM (BiLSTM).", "labels": [], "entities": [{"text": "categorical sentiment analysis", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.8262720704078674}]}, {"text": "We have aimed to employ those methods for dimensional sentiment analysis, and the results show that our approach is feasible.", "labels": [], "entities": [{"text": "dimensional sentiment analysis", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.8647890488306681}]}, {"text": "In general, CNN can extract local n-gram features within texts but may fail to capture long-distance dependency.", "labels": [], "entities": []}, {"text": "LSTM can address this problem by sequentially modeling texts cross messages (.", "labels": [], "entities": []}, {"text": "In this paper (and for this competition), we primarily introduce a CNN-LSTM model combining CNN and LSTM.", "labels": [], "entities": []}, {"text": "First, we construct word vectors from pre-trained word vectors using word embedding.", "labels": [], "entities": []}, {"text": "The CNN applies convolutional and maxpooling layers, which are then used to extract ngram features.", "labels": [], "entities": []}, {"text": "Finally, LSTM composes those features and outputs the result.", "labels": [], "entities": []}, {"text": "By combining CN- N and LSTM, the model can extract both local information within tweets and long-distance dependency across tweets.", "labels": [], "entities": []}, {"text": "Our experiment reveals that the proposed model has the highest performance with data for anger and joy, while a simple CNN performs best for fear and sadness.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we described CNN, LSTM and their combination.", "labels": [], "entities": [{"text": "CNN", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.8153136372566223}]}, {"text": "The comparative experimental results are presented in section 3.", "labels": [], "entities": []}, {"text": "Finally, a conclusion is drawn in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The organizers of the competition provided four corpora, each of which corresponds to an emotion (anger, fear, joy and sadness).", "labels": [], "entities": []}, {"text": "The training datasets contain tweets along with a real-valued score (between 0 and 1) indicating the degree of the emotion felt by the speaker.", "labels": [], "entities": []}, {"text": "Dev sets were provided to help us tune the parameters of the model.", "labels": [], "entities": []}, {"text": "Here, we used the Stanford tokenizer to process tweets into an array of tokens.", "labels": [], "entities": []}, {"text": "Since the tweets in this task primarily contain English text, all punctuations are ignored and all non-English letters are treated as unknown words.", "labels": [], "entities": []}, {"text": "A small part of text contains emojis or emoticons, which perfectly match the conditions for emotional intensity.", "labels": [], "entities": []}, {"text": "Therefore, these emojis or emoticons are processed into related words with similar meanings.", "labels": [], "entities": []}, {"text": "Patterns are applied to every tweet presented in.", "labels": [], "entities": [{"text": "Patterns", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8664734959602356}]}, {"text": "We applied the four patterns and lowed all words to map the known pre-trained tokens.", "labels": [], "entities": []}, {"text": "Some words that do not exist in the known tokens are treated as unknown words.", "labels": [], "entities": []}, {"text": "In the word vectors, unknown word vectors randomly generated from a uniform distribution U (\u22120.25, 0.25).", "labels": [], "entities": []}, {"text": "In this experiment, we used pre-trained word vectors including GoogleNews 1 trained by the word2vec toolkit and another one trained by GloVe 2 ().", "labels": [], "entities": []}, {"text": "These programs   were used to initialize the weight of the embedding layer in order to build 300-dimension word vectors for all tweets.", "labels": [], "entities": []}, {"text": "GloVe is an unsupervised learning algorithm for obtaining vector representations of words.", "labels": [], "entities": []}, {"text": "This experiment used Keras with a TensorFlow backend.", "labels": [], "entities": []}, {"text": "We use two different pre-trained word vectors and four different datasets.", "labels": [], "entities": []}, {"text": "We introduce three other models (C-NN, LSTM and BiLSTM) as baseline algorithms.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9027466177940369}]}, {"text": "Details of those three models can respectively be found in, (Hochreiter and Schmidhuber, 1997; and).", "labels": [], "entities": []}, {"text": "The hyper-parameters were tuned to the performance of training and dev data using the sklearn grid search function (), which can search all possible parameter combinations to evaluate models and find the best one.", "labels": [], "entities": []}, {"text": "Different models for different data may have their own optimization parameters.", "labels": [], "entities": []}, {"text": "For anger emotion data, the CNN-LSTMs best-tuned parameters are as follows.", "labels": [], "entities": []}, {"text": "The number of filters (m) is 64; the length of the filter (l) is 3; the pool length (n) is 2; the dropout rate (p) is 0.1; the LSTM layer count (c) is 2, and the dimension of the LSTM hidden layer (d) is 300.", "labels": [], "entities": [{"text": "dropout rate (p)", "start_pos": 98, "end_pos": 114, "type": "METRIC", "confidence": 0.8662494421005249}]}, {"text": "The training runs with a batch size (b) of 100 and 30 epochs (e).", "labels": [], "entities": [{"text": "batch size (b)", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.7552417516708374}]}, {"text": "The other three emotions shown in.", "labels": [], "entities": []}, {"text": "The results also reveal that the models using pre-trained GloVe vectors and an Adam optimizer achieved the best performance.", "labels": [], "entities": []}, {"text": "The system is evaluated by calculating the Pearson correlation coefficient (r) and Spearman rank coefficient (s) with gold ratings.", "labels": [], "entities": [{"text": "Pearson correlation coefficient (r)", "start_pos": 43, "end_pos": 78, "type": "METRIC", "confidence": 0.9463579853375753}, {"text": "Spearman rank coefficient (s)", "start_pos": 83, "end_pos": 112, "type": "METRIC", "confidence": 0.8889640669027964}]}, {"text": "Higher rand s values indicate better performance on model prediction.", "labels": [], "entities": [{"text": "rand s", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9380335211753845}, {"text": "model prediction", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.8120467066764832}]}, {"text": "A total of twenty two teams took part in the task.", "labels": [], "entities": []}, {"text": "shows the detailed results of the proposed CNN-LSTM model against the three baseline models.", "labels": [], "entities": []}, {"text": "The averaged r from the four emotions is needed to determine the bottom-line competition metric by witch the submissions will be ranked.", "labels": [], "entities": []}, {"text": "Therefore, r is more worth considering for performance than s.", "labels": [], "entities": []}, {"text": "The proposed CNN-LSTM model outperformed the baseline models for anger and joy data.", "labels": [], "entities": []}, {"text": "Therefore, we chose the CNN-LSTM to create the final system to complete the subtasks of anger and joy, and ranked ninth for both rand son anger data, eleventh for r, and thirteenth for son joy data.", "labels": [], "entities": [{"text": "CNN-LSTM", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9671027660369873}]}, {"text": "In contrary, a simple CNN yielded better performance on fear and sadness data from the experimental results.", "labels": [], "entities": []}, {"text": "Therefore, for the fear and sadness subtasks, we used a simple CNN that ranked seventh for rand eighth for son fear data, and sixth for both rand son sadness data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The development data experimentation results on WASSA-2017 shard task on Emotion Intensity  (EmoInt).", "labels": [], "entities": [{"text": "WASSA-2017 shard task", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.5817729234695435}]}, {"text": " Table 2: The best-tuned parameters on each  dataset.", "labels": [], "entities": []}]}