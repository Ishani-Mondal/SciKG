{"title": [{"text": "Spectral Graph-Based Method of Multimodal Word Embedding *", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a novel method for multimodal word embedding, which exploit a generalized framework of multi-view spectral graph embedding to take into account visual appearances or scenes denoted by words in a corpus.", "labels": [], "entities": [{"text": "multimodal word embedding", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6110661427179972}]}, {"text": "We evaluated our method through word similarity tasks and a concept-to-image search task, having found that it provides word representations that reflect visual information, while somewhat trading-off the performance on the word similarity tasks.", "labels": [], "entities": []}, {"text": "Moreover , we demonstrate that our method captures multimodal linguistic regularities, which enable recovering relational similarities between words and images by vector arithmetic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embedding plays important roles in the field of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.7207365334033966}]}, {"text": "Many existing studies use word vectors for various downstream NLP tasks, such as text classification, Part-of-Speech tagging, and machine translation.", "labels": [], "entities": [{"text": "text classification", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.8041183054447174}, {"text": "Part-of-Speech tagging", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7863436937332153}, {"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8169942498207092}]}, {"text": "One of the most famous approaches is skip-gram model (, which is based on a neural network, and its extensions have also been widely studied as well.", "labels": [], "entities": []}, {"text": "There are alternative approaches depending on a spectral graph embedding framework () for word embedding.", "labels": [], "entities": []}, {"text": "For examples, proposed a method based on Canonical Correlation Analysis (CCA), while a PCA based word embedding method was proposed in.", "labels": [], "entities": []}, {"text": "* This work was partially supported by grants from Japan Society for the Promotion of Science KAKENHI (16H02789) to HS.", "labels": [], "entities": [{"text": "Japan Society for the Promotion of Science KAKENHI (16H02789)", "start_pos": 51, "end_pos": 112, "type": "DATASET", "confidence": 0.5289713957092979}, {"text": "HS", "start_pos": 116, "end_pos": 118, "type": "DATASET", "confidence": 0.7107362747192383}]}, {"text": "In recent years, many researchers have been actively studying the use of multiple modalities in the fields of both NLP and computer vision.", "labels": [], "entities": []}, {"text": "Those studies combine textual and visual information to propose methods for image-caption matching (, caption generation (), visual question answering (, quantifying abstractness ( ) of words, and soon.", "labels": [], "entities": [{"text": "image-caption matching", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7004159241914749}, {"text": "caption generation", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8111532926559448}, {"text": "question answering", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7013460397720337}]}, {"text": "As for word embedding, multimodal versions of word2vec () have been proposed in and.", "labels": [], "entities": []}, {"text": "The first one jointly optimize the objective of both skip-gram model and a cross-modal objective across texts and images, and the latter uses abstract scenes as surrogate labels for capturing visually grounded semantic relatedness.", "labels": [], "entities": []}, {"text": "More recently, proposed a multimodal word embedding methods based on a recurrent neural network to learn word vectors from their newly proposed large scale image caption dataset.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew spectral graphbased method of multimodal word embedding.", "labels": [], "entities": []}, {"text": "Specifically, we extend Eigenwords (, a CCA-based method for word embedding, by applying a generalized framework of spectral graph embedding (.", "labels": [], "entities": [{"text": "word embedding", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.7523811757564545}]}, {"text": "shows a schematic diagram of our method.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we call our method Multimodal Eigenwords (MM-Eigenwords).", "labels": [], "entities": []}, {"text": "The most similar existing method is Multimodal Skip-gram model (MMskip-gram) (, which slightly differ in that our model can easily deal with many-to-many relationships between words in a corpus and their relevant images, while MMskip-gram only considers one-to-one relationships between concrete words and images.", "labels": [], "entities": []}, {"text": "Using a corpus and datasets of image-word rela-: Our proposed method extends a CCAbased method of word embedding by means of multi-view spectral graph embedding frameworks of dimensionality reduction to deal with visual information associated with words in a corpus.", "labels": [], "entities": []}, {"text": "tionships, which are available in common benchmark datasets or on online photo sharing services, MM-Eigenwords jointly learns word vectors on a common multimodal space and a linear mapping from a visual feature space to the multimodal space.", "labels": [], "entities": []}, {"text": "Those word vectors also reflect similarities between words and images.", "labels": [], "entities": []}, {"text": "We evaluated the multimodal word representations obtained by our model through word similarity task and concept-to-image search, having found that our model has ability to capture both semantic and word-to-image similarities.", "labels": [], "entities": []}, {"text": "We also found that our model captures multimodal linguistic regularities (), whose examples are shown in.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiment, we used English Wikipedia corpus (2016 dump) , which consists of approximately 3.9 billion tokens.", "labels": [], "entities": [{"text": "English Wikipedia corpus (2016 dump)", "start_pos": 27, "end_pos": 63, "type": "DATASET", "confidence": 0.9342448966843742}]}, {"text": "We first used the script provided by Mahoney 2 to cleanup the original dump.", "labels": [], "entities": []}, {"text": "Afterward, we applied word2phrase () to the original corpus twice with a threshold value 500 to obtain multi-term phrases.", "labels": [], "entities": []}, {"text": "As for visual data, we downloaded images from the URLs in the NUS-WIDE image dataset (, which also provides Flickr tags of each image.", "labels": [], "entities": [{"text": "NUS-WIDE image dataset", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.9426445364952087}, {"text": "Flickr", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.8466171622276306}]}, {"text": "Although Flickr tags associated with each image could be very noisy and have varying abstractness, they provides a rich source of many-to-many relationships between images and words.", "labels": [], "entities": []}, {"text": "Since we were interested in investigating if the large, but noisy web data would play a role as a helpful source for multimodal word representations, we omitted preprocessing like manually removing noisy tags or highly abstract tags.", "labels": [], "entities": []}, {"text": "The images were converted to 4096-dim feature vectors using the Caffe toolkit (, together with a pre-trained 3 AlexNet model ().", "labels": [], "entities": [{"text": "Caffe toolkit", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9178138077259064}]}, {"text": "These feature vectors are the output of the fc7 layer on the AlexNet.", "labels": [], "entities": [{"text": "AlexNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9777008891105652}]}, {"text": "We randomly selected 100k images fora training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlations between word similarities based on the word vectors and that of the  human annotations, and the right part shows the accuracies of concept-to-image search evaluated by  precision@k.", "labels": [], "entities": [{"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9876357913017273}]}]}