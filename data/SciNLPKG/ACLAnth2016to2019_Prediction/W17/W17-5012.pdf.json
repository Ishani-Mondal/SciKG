{"title": [{"text": "An Investigation into the Pedagogical Features of Documents", "labels": [], "entities": []}], "abstractContent": [{"text": "Characterizing the content of a technical document in terms of its learning utility can be useful for applications related to education , such as generating reading lists from large collections of documents.", "labels": [], "entities": []}, {"text": "We refer to this learning utility as the \"pedagogical value\" of the document to the learner.", "labels": [], "entities": []}, {"text": "While pedagogical value is an important concept that has been studied extensively within the education domain, there has been little work exploring it from a computational, i.e., natural language processing (NLP), perspective.", "labels": [], "entities": []}, {"text": "To allow a computational exploration of this concept, we introduce the notion of \"peda-gogical roles\" of documents (e.g., Tutorial and Survey) as an intermediary component for the study of pedagogical value.", "labels": [], "entities": []}, {"text": "Given the lack of available corpora for our exploration , we create the first annotated corpus of pedagogical roles and use it to test base-line techniques for automatic prediction of such roles.", "labels": [], "entities": []}], "introductionContent": [{"text": "We define \"pedagogical value\" as the estimate of how useful a document is to an individual who seeks to learn about specific concepts described in the document.", "labels": [], "entities": []}, {"text": "A computational task that operationalizes the concept of pedagogical value is generating an ordered reading list of documents that a learner can traverse in order to maximize understanding of a subject.", "labels": [], "entities": []}, {"text": "When a professor manually constructs a reading list about a specific subject fora student, the professor incorporates substantial knowledge of the subject history and interdependencies with other related subjects.", "labels": [], "entities": []}, {"text": "The student's background and the relative qualities of documents on similar subjects are also considered.", "labels": [], "entities": []}, {"text": "Techniques for automatically generating reading lists should also consider the extent to which a learner will be able to learn from a particular document.", "labels": [], "entities": []}, {"text": "Previously, have studied the \"pedagogical value of papers\" in the context of paper recommendation.", "labels": [], "entities": [{"text": "paper recommendation", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.7073860168457031}]}, {"text": "In their work, they define the multiple \"pedagogical values\" of a paper as the paper's overall ratings, popularity, degree of peer recommendation, learner gain in new knowledge, learner interest, and learner background knowledge.", "labels": [], "entities": []}, {"text": "Other efforts on generating reading lists and document recommendation have focused on modeling concepts represented in documents, modeling concept dependencies (, and user modeling (), but there appears to be very limited work on characterizing the learning utility between a learner and a document.", "labels": [], "entities": [{"text": "document recommendation", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.6923802345991135}]}, {"text": "The abstract nature of pedagogical value motivates us to identify explicit document features that are salient to pedagogical value.", "labels": [], "entities": []}, {"text": "With graduate students as our target learners, we start with a simplified model of novice, intermediate, and advanced learners, and we focus on identifying pedagogical features of documents that could benefit different learners.", "labels": [], "entities": []}, {"text": "In our document annotation process, we collected annotations for the qualitative and largely objective judgments of categories that documents belong to: Tutorial, Survey, Software Manual, Resource, Reference Work, Empirical Results, and Other.", "labels": [], "entities": []}, {"text": "We identify the seven categories based on document objectives in presenting content, e.g., Tutorials teach the reader step by step how to do something, Resource papers point the reader to datasets and implementations.", "labels": [], "entities": []}, {"text": "Motivated by the need to conceptually organize information to be pedagogically useful, we refer to documents with different objectives as fulfilling different \"pedagogical roles.\"", "labels": [], "entities": []}, {"text": "In the rest of this paper, we will use the document category names to refer to the pedagogical roles.", "labels": [], "entities": []}, {"text": "Identifying important qualitative features of pedagogical value, such as the pedagogical role, gives a greater degree of interoperability and insight into how we can help students learn more effectively.", "labels": [], "entities": []}, {"text": "Education research explains the distinction between declarative and functioning knowledge: the former is knowledge of content and the latter is knowledge of how to interpret and put the content to work.", "labels": [], "entities": []}, {"text": "To apply content, learners must first understand the content; this explains why a novice and an advanced learner trying to learn the same subject would seek out documents with different pedagogical roles.", "labels": [], "entities": []}, {"text": "Tutorials, Reference Works, and Survey papers are better introductions fora novice with no knowledge of a subject.", "labels": [], "entities": []}, {"text": "In contrast, an expert would have enough background knowledge to dive right into advanced papers presenting stateof-the-art empirical results.", "labels": [], "entities": []}, {"text": "Although pedagogical roles are not the same as pedagogical value, these pedagogical features offer some insight as a starting point for estimating learning utility.", "labels": [], "entities": []}, {"text": "For our study, we collected annotations for over 1000 documents, which we document and make available for others to use.1 We also collected annotations for three ordinalscale questions of document complexity and quality as an exercise to gauge the feasibility of the task despite its subjective nature.", "labels": [], "entities": []}, {"text": "However, the resulting inter-annotator agreement results were too low to be meaningful.", "labels": [], "entities": []}, {"text": "These results stress the importance of identifying more objective user and document features relevant to pedagogical value; in this initial work, we focus on document features.", "labels": [], "entities": []}, {"text": "Our contribution is twofold: We provide the first annotated corpus of pedagogical roles for the study of pedagogical value, and we present baseline classification results using state-of-the-art techniques for others to work with.", "labels": [], "entities": []}, {"text": "Our goal is to establish a framework that can be extended to other domains, provide empirical results to validate our dataset and algorithms, and demonstrate the feasibility of the proposed role classification task.", "labels": [], "entities": [{"text": "role classification task", "start_pos": 190, "end_pos": 214, "type": "TASK", "confidence": 0.8596875866254171}]}, {"text": "In the rest of this paper, we will describe our methods for collecting, evaluating, and automatically generating annotations in Section 2, the results of our evaluations in Section 3, related work in Section 4, and concluding remarks in Section 5. 1https://doi.org/10.6084/m9.figshare.5202424", "labels": [], "entities": []}], "datasetContent": [{"text": "The kappa value, which measures the likelihood of annotator agreement occurring above chance, is 0.68 for the pedagogical role annotations.", "labels": [], "entities": [{"text": "kappa", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8960994482040405}]}, {"text": "This kappa value was calculated as an average over the kappa values for each subset of 100 documents.", "labels": [], "entities": []}, {"text": "Given the difficulty of annotating pedagogical roles, which was confirmed by annotators, we believe a kappa of 0.68 indicates substantial agreement between annotators (.", "labels": [], "entities": [{"text": "kappa", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9658185243606567}]}, {"text": "shows the details of inter-annotator agreement for annotated pedagogical roles from documents with only one majority role.", "labels": [], "entities": []}, {"text": "The rows are the majority roles, which we take to be the ground truth pedagogical roles of documents.", "labels": [], "entities": []}, {"text": "The columns show the third annotator's annotations; if the third   From, we can see that Survey documents are sometimes confused with Reference Works, Resource papers are sometimes confused with Other documents, and Software Manuals are rare.", "labels": [], "entities": []}, {"text": "We also see that Other documents have relatively higher rates of misclassification.", "labels": [], "entities": []}, {"text": "These results are consistent with feedback from annotators.", "labels": [], "entities": []}, {"text": "The reason why Survey documents are sometimes mistaken for Reference Works is because both examine abroad number of subjects in a domain; the distinction we make in our annotation guidelines is that Reference Works area collection of established authoritative facts such as those one might find in an encyclopedia, whereas Surveys focus on the discoveries of other publications.", "labels": [], "entities": []}, {"text": "When looking for Resource papers, annotators rely onlooking for few indicator sentences that maybe missed with a more superficial skim of the document.", "labels": [], "entities": []}, {"text": "Also, the Other documents belong to a range of additional pedagogical roles, though we do not make finer distinctions here.", "labels": [], "entities": []}, {"text": "For each annotated document, we kept the pedagogical roles that had majority annotation agreement across the three annotators who annotated the document.", "labels": [], "entities": []}, {"text": "If a document had no majority labels, the document was filtered out of the annotated document set.", "labels": [], "entities": []}, {"text": "This filtered document set of 1235 documents with 1264 annotated pedagogical roles is the one we use along with a supplementary set for all pedagogical role prediction techniques.", "labels": [], "entities": [{"text": "role prediction", "start_pos": 152, "end_pos": 167, "type": "TASK", "confidence": 0.7385416030883789}]}, {"text": "Figure 2: Distribution of all pedagogical role annotations in the full annotated corpus used for training classifiers We noticed alack of Surveys, Resources, and Software Manuals, so we internally annotated another supplementary set of 155 documents consisting mostly of documents of the underrepresented roles.", "labels": [], "entities": []}, {"text": "The full annotated corpus we use for classification has the distribution of roles shown in; this full corpus includes the filtered set of 1235 documents annotated by three annotators each and 155 internally annotated documents, fora total of 1489 pedagogical role annotations over 1390 documents.", "labels": [], "entities": []}, {"text": "Given the corpora we selected our set of documents to annotate from, it is not surprising that most of the documents are Empirical Results, Reference Works, Tutorials, or Other.", "labels": [], "entities": []}, {"text": "94% of the annotated documents have just one pedagogical role, and 99.1% have one or two pedagogical roles.8 The top three most common combinations of roles fora document are Resource and Empirical Results; Resource and Software Manual; Tutorial, Resource, and Software Manual.9 Many documents with multiple pedagogical roles are Resource documents because the authors make their work publicly available.", "labels": [], "entities": []}, {"text": "In, we see that for both random forest classification of TF-IDF scores (RF) and sentence embedding methods (CEN, KNN+BoSEC, KNN+BoSEC+), the more samples there are fora pedagogical role, the higher the scores are for the role.", "labels": [], "entities": []}, {"text": "The scores for Other documents are an anticipated exception to the trend, because we do not make more fine-grained distinctions between other pedagogical roles in this work.", "labels": [], "entities": []}, {"text": "Software Manuals are also an exception to this trend, as their scores are relatively high for the number of samples; this is because Software Manuals are typically written in a very distinct style.", "labels": [], "entities": []}, {"text": "CEN generally performs poorly across roles, doing worse than the baseline random forest classification with TF-IDF.", "labels": [], "entities": [{"text": "CEN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7891438603401184}]}, {"text": "This suggests that word frequency is more informative about the pedagogical roles of a document than a single representative vector per role.", "labels": [], "entities": []}, {"text": "With the exception of Software Manuals, RF is able to predict roles with more samples (Reference Work, Empirical Results, Other) with higher precision compared to roles with fewer samples.", "labels": [], "entities": [{"text": "RF", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9098305702209473}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.997963547706604}]}, {"text": "KNN+BoSEC and KNN+BoSEC+ have comparable precision for roles with more samples, but have significantly higher precision for roles with fewer samples.", "labels": [], "entities": [{"text": "KNN+BoSEC", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.754049022992452}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9990059733390808}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9971475005149841}]}, {"text": "Compared to RF, KNN+BoSEC and KNN+BoSEC+ also have higher recall across all roles.", "labels": [], "entities": [{"text": "BoSEC", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.6023358702659607}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9995220899581909}]}, {"text": "KNN+BoSEC+ has the highest F 1 scores for all pedagogical roles.", "labels": [], "entities": [{"text": "KNN+BoSEC+", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7647909075021744}, {"text": "F 1 scores", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9862077633539835}]}, {"text": "We attribute the fact that KNN+BoSEC+ is generally able to do better than KNN+BoSEC to using a custom sentence encoder trained on scientific documents.", "labels": [], "entities": []}, {"text": "Given that we use keyphrases to find documents that likely belong to specific pedagogical roles, we also want to see if we could achieve performance similar to that of our sentence embedding-based methods by simply classifying documents based on keyphrases.", "labels": [], "entities": []}, {"text": "We manually curate a list of keyphrases for two pedagogical roles: \"software manual,\" \"manual,\" and \"technical manual\" for Software Manuals, and \"tutorial\" for Tutorials.", "labels": [], "entities": []}, {"text": "We then classify a document as a certain role if any of the role's keyphrases are present in the first five sentences of the document, where the title counts as the first sentence.", "labels": [], "entities": []}, {"text": "Classifying Software Manuals with this method has a precision of 0.15, a recall of 0.09, and an F 1 score of 0.11.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9993983507156372}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9978547692298889}, {"text": "F 1 score", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9928832252820333}]}, {"text": "KNN+BoSEC+ dramatically outperforms this method with the specified keyphrases for Software manuals.", "labels": [], "entities": [{"text": "BoSEC", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.7066338658332825}]}, {"text": "Classifying Tutorials with this method has a precision of 0.60, a recall of 0.50, and an F 1 score of 0.55.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9992530941963196}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9991169571876526}, {"text": "F 1 score", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9921951293945312}]}, {"text": "While the keyphrase classification results for Tutorials are closer to the corresponding KNN+BoSEC+ results, we think that the KNN+BoSEC+ results would also improve if it had access to the list of keyphrases as features, though we leave that for future experimentation.", "labels": [], "entities": [{"text": "keyphrase classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7527824938297272}, {"text": "KNN+BoSEC+", "start_pos": 89, "end_pos": 99, "type": "DATASET", "confidence": 0.7339075803756714}, {"text": "KNN+BoSEC+", "start_pos": 127, "end_pos": 137, "type": "DATASET", "confidence": 0.6760035008192062}]}, {"text": "These initial keyphrase classification experiments suggest that sentence-embedding-based methods are generally more effective and robust than handcrafting keyphrases for each pedagogical role.", "labels": [], "entities": [{"text": "keyphrase classification", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.7837841212749481}]}, {"text": "The confusion matrix in allows us to make judgments about documents of different pedagogical roles, as predicted by KNN+BoSEC+.", "labels": [], "entities": [{"text": "KNN+BoSEC+", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.7633883953094482}]}, {"text": "The rows are the ground truth roles, and the columns are the predicted roles.", "labels": [], "entities": []}, {"text": "We can see that Surveys, Resources, and Other documents are often mistaken to be documents with Empirical Results.", "labels": [], "entities": []}, {"text": "Additionally, there are relatively more instances of Surveys, Resources, and Other documents where the classifier is unable to make a prediction.", "labels": [], "entities": []}, {"text": "Overall, these results suggest that the misclassifications are an effect of an unbalanced dataset with many more samples of Empirical Results, rather than an inherent lack of distinctness between documents of different roles.", "labels": [], "entities": []}, {"text": "Through a qualitative analysis of sentences from the clusters most frequently associated with each   pedagogical role, we observe that example sentences from different roles align with our intuitions of what exemplary sentences from different roles should be.", "labels": [], "entities": []}, {"text": "The Survey sentences describe progress in different areas of research; the Tutorial sentences explain details of specific concepts and methods; the Software Manual sentences give information about how to use a tool.10 Sentences from the most 10For more details, see in Supplemental Material.", "labels": [], "entities": [{"text": "Supplemental Material", "start_pos": 269, "end_pos": 290, "type": "DATASET", "confidence": 0.7760067284107208}]}, {"text": "frequent clusters of a role do not explicitly mention the roles of the paper, e.g., \"This paper presents a tutorial.", "labels": [], "entities": []}, {"text": "\" This phenomenon makes sense for two reasons.", "labels": [], "entities": []}, {"text": "One reason is that the majority of documents do not explicitly say what kind of document they are.", "labels": [], "entities": []}, {"text": "The second reason is that even when documents do explicitly state their role, the actual content of the document may disagree with the declared role.", "labels": [], "entities": []}, {"text": "For example, some papers are written to accompany tutorials presented at workshops.", "labels": [], "entities": []}, {"text": "The papers will explicitly declare themselves to be tutorials, but the paper will only include an abstract and not the tutorial itself.", "labels": [], "entities": []}, {"text": "Following our annotation guidelines, we do not label these documents as Tutorials.", "labels": [], "entities": []}, {"text": "This implicit characterization of a document's pedagogical roles through sentences means that a method that merely searches for explicit mentions of keywords or declaration of the document's roles would not bean effective approach to this problem.", "labels": [], "entities": []}, {"text": "Thus, these example sentences qualitatively validate our embedding and clustering approach to pedagogical role classification.", "labels": [], "entities": [{"text": "role classification", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.6841935962438583}]}], "tableCaptions": [{"text": " Table 1.  Although there are 1264 majority pedagogical role  annotations, we calculated the confusion matrix for  1206 roles from documents with only one majority  role each, for ease of interpretation. From the 1206  pedagogical roles, there are 1245 role pairs between  the majority role and the third annotator's annotated  role(s).", "labels": [], "entities": []}, {"text": " Table 1: Confusion matrix for annotated pedagogi- cal roles from documents with only one majority  role. Rows are the majority roles (chosen by two  or three annotators) that we treat as ground truth.  Columns are the third annotator's corresponding  annotations.", "labels": [], "entities": []}, {"text": " Table 2: Precision, recall, and F 1 scores by pedagogical roles for all methods. Support is the actual number  of documents with each role. avg / total computes weighted averages of scores across all roles. All values  are averaged over a five-fold cross validation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9901400208473206}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9978142976760864}, {"text": "F 1 scores", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9811785022417704}, {"text": "Support", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9705586433410645}]}, {"text": " Table 3: Ground truth pedagogical roles (rows) ver- sus predicted roles (columns) using KNN+BoSEC+.  We calculate the confusion matrix for documents  with only one ground truth role. All values are  averaged over a five-fold cross validation.", "labels": [], "entities": [{"text": "BoSEC", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.5368597507476807}]}]}