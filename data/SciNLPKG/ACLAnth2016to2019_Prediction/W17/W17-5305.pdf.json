{"title": [{"text": "Playing with Embeddings : Evaluating embeddings for Robot Language Learning through MUD Games", "labels": [], "entities": []}], "abstractContent": [{"text": "Acquiring language provides a ubiquitous mode of communication, across humans and robots.", "labels": [], "entities": []}, {"text": "To this effect, distributional representations of words based on co-occurrence statistics, have provided significant advancements ranging across machine translation to comprehension.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7462767064571381}]}, {"text": "In this paper, we study the suitability of using general purpose word-embeddings for language learning in robots.", "labels": [], "entities": []}, {"text": "We propose using text-based games as a proxy to evaluating word embedding on real robots.", "labels": [], "entities": []}, {"text": "Based in a risk-reward setting, we review the effectiveness of the embeddings in navigating tasks in fantasy games, as an approximation to their performance on more complex scenarios, like language assisted robot navigation.", "labels": [], "entities": [{"text": "language assisted robot navigation", "start_pos": 189, "end_pos": 223, "type": "TASK", "confidence": 0.5934704691171646}]}], "introductionContent": [{"text": "Language provides a natural interface for humans to communicate with robots.", "labels": [], "entities": []}, {"text": "With their increasing public presence, from self-driving cars and rescue operations to warehouses, it is imperative to reduce this barrier of communication, by improving language learning in mobile robots.", "labels": [], "entities": []}, {"text": "For instance, in search and rescue operations one might want to instruct the agents to \"Reach the third floor of the blue building\".", "labels": [], "entities": [{"text": "search and rescue", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6601724922657013}]}, {"text": "Given the nature of the tasks, it is essential for the agent to accurately infer its current state and parse natural language instructions to corresponding actions.", "labels": [], "entities": []}, {"text": "In recent years, learning continuous representations of words and symbols have become central to dealing with several key problems in natural * Equally contributed to the project.", "labels": [], "entities": []}, {"text": "Author names listed in alphabetical order.", "labels": [], "entities": []}, {"text": "Usually trained to maximize the likelihood of next utterance, these models effectively learn co-occurrence statistics based on corpora, motivated by the distributional hypothesis.", "labels": [], "entities": []}, {"text": "This objective function, often organizes objects and actions with similar semantic information, to close neighborhoods in the corresponding embedding space.", "labels": [], "entities": []}, {"text": "Here, the similarity between words is often defined by some metric for similarity of the word vectors.", "labels": [], "entities": []}, {"text": "Though evidently successful across several tasks, common general purpose word embeddings with fixed dimensions are often inadequate in their representation capacity.", "labels": [], "entities": []}, {"text": "In recent works, ( argue that language learning grounded in perception, motion or actions, are important to learn meaningful representations corresponding to consequences of actions on objects.", "labels": [], "entities": []}, {"text": "Specifically, in a scenario where robots are deployed in real world, it is important to distinguish even between", "labels": [], "entities": []}], "datasetContent": [{"text": "We propose using text-based MUD games as proxy for evaluating language learning in robotics.", "labels": [], "entities": []}, {"text": "As illustrated in) the nature of instructions generated by the game environment are quite similar when compared to real datasets, in this case (.", "labels": [], "entities": []}, {"text": "In this work, we use the Evennia 1 game environment, an open-source library to build online textual MUD games.", "labels": [], "entities": [{"text": "Evennia 1 game environment", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.8559531271457672}]}, {"text": "Evennia allows users to create complex environments with elaborate textural descriptions, by simply writing a batch file to describe the objects, actions and possible interactions in the environment.", "labels": [], "entities": []}, {"text": "Compared to other available datasets for navigation, the MUD environment provides a risk-reward scenario, where the environment can also be designed to have deterministic negative feedback to represent undesirable outcomes.", "labels": [], "entities": []}, {"text": "To provide feedback on the action taken, the agents are provided positive/negative rewards depending on the state of the game.", "labels": [], "entities": []}, {"text": "In case of quest completion, the agents are provided a large positive reward, while predefined bad intermediate checkpoints like colliding with walls, or falling off bridges are penalized with negative rewards.", "labels": [], "entities": [{"text": "quest completion", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.8483661115169525}]}, {"text": "Building on extensive literature in reward shaping in the Deep Reinforcement Learning framework, more complex environment can also be easily constructed to evaluate the generalizing capability of the embeddings.", "labels": [], "entities": [{"text": "reward shaping", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7853028774261475}]}, {"text": "We experiment on the Fantasy World following the environment in ().", "labels": [], "entities": [{"text": "Fantasy World", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9659506678581238}]}, {"text": "The vocabulary consists of 1340 unique tokens, with 100 different descriptions for the room.", "labels": [], "entities": []}, {"text": "Visiting the room provides random sequence of description as developed by the designers.", "labels": [], "entities": []}, {"text": "Average length of descriptions in the rooms was 65 words per descriptions.", "labels": [], "entities": [{"text": "length", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.7264516949653625}]}, {"text": "The possible actions per state average to 222 per state.", "labels": [], "entities": []}, {"text": "Please refer () for elaborate game statistics.", "labels": [], "entities": []}, {"text": "To evaluate the performance of agents when using different word-embeddings, different metrics could be defined depending on the requirement of the task.", "labels": [], "entities": []}, {"text": "In the particular case of evaluating the performance of the agent on navigation task, we follow common defined metrics as in ( \u2022 cumulative reward per episode averaged over the number of episodes.", "labels": [], "entities": [{"text": "navigation task", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.900250107049942}]}, {"text": "\u2022 fraction of quests successfully completed by the agent.", "labels": [], "entities": []}, {"text": "Considering the task as an downstream proxy for the actual environment, this does not evaluate the individual embeddings by similarity metrics, rather the generalization across environments.", "labels": [], "entities": []}, {"text": "In this work we compare word2vec (), GloVe () general use embeddings against a BOW-DQN baseline and LSTM-DQN trained with random initialization.", "labels": [], "entities": [{"text": "BOW-DQN baseline", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.8047824203968048}]}, {"text": "The baseline here (BOW-DQN) uses a simple bag-of-words model to represent the textual description.", "labels": [], "entities": [{"text": "BOW-DQN", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9360778331756592}]}, {"text": "We observe that using pre-trained embeddings generally improve the performance of the model.", "labels": [], "entities": []}, {"text": "The rewards in our primary models have high variance due to the stochastic nature of the policy.", "labels": [], "entities": []}, {"text": "In further work, we wish to build on recent advancements in Deep Reinforcement Learning literature, for training better models.", "labels": [], "entities": []}], "tableCaptions": []}