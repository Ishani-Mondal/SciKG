{"title": [{"text": "Neural-based Context Representation Learning for Dialog Act Classification", "labels": [], "entities": [{"text": "Neural-based Context Representation Learning", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7587743550539017}, {"text": "Dialog Act Classification", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.822794516881307}]}], "abstractContent": [{"text": "We explore context representation learning methods in neural-based models for dialog act classification.", "labels": [], "entities": [{"text": "context representation learning", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.7548220554987589}, {"text": "dialog act classification", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7652378877003988}]}, {"text": "We propose and compare extensively different methods which combine recurrent neural network architectures and attention mechanisms (AMs) at different context levels.", "labels": [], "entities": []}, {"text": "Our experimental results on two benchmark datasets show consistent improvements compared to the models without contextual information and reveal that the most suitable AM in the architecture depends on the nature of the dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "The study of spoken dialogs between two or more speakers can be approached by analyzing the dialog acts (DAs), which is the intention of the speaker at every utterance during a conversation.", "labels": [], "entities": []}, {"text": "shows a fragment of a conversation from the Switchboard (SwDA) dataset with DA annotation.", "labels": [], "entities": [{"text": "Switchboard (SwDA) dataset", "start_pos": 44, "end_pos": 70, "type": "DATASET", "confidence": 0.6180792212486267}]}, {"text": "Automatic DA classification is an important pre-processing step in natural language understanding tasks and spoken dialog systems.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8036309778690338}, {"text": "natural language understanding tasks", "start_pos": 67, "end_pos": 103, "type": "TASK", "confidence": 0.7121918052434921}]}, {"text": "This classification task has been approached using traditional statistical methods such as hidden Markov models (HMMs) (), conditional random fields (CRF) ( and support vector machines (SVMs) ().", "labels": [], "entities": []}, {"text": "However, recent works with deep learning (DL) techniques have brought state-ofthe-art models in DA classification, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs) ( and long short-term memory (LSTM) models.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.971196711063385}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Accuracy (%) of baselines and models  with different context processing methods.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9926235675811768}]}, {"text": " Table 5: Comparison of accuracy (%) on different  context lengths (n-context, where n is the number  of sentences as context).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.991452693939209}]}]}