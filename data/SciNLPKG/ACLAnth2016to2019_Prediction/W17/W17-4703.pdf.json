{"title": [{"text": "Word Representations in Factored Neural Machine Translation", "labels": [], "entities": [{"text": "Word Representations in Factored Neural Machine Translation", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.6387080933366504}]}], "abstractContent": [{"text": "Translation into a morphologically rich language requires a large output vocabulary to model various morphological phenomena , which is a challenge for neu-ral machine translation architectures.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.5731397271156311}]}, {"text": "To address this issue, the present paper investigates the impact of having two output factors with a system able to generate separately two distinct representations of the target words.", "labels": [], "entities": []}, {"text": "Within this framework, we investigate several word representations that correspond to different distributions of morpho-syntactic information across both factors.", "labels": [], "entities": []}, {"text": "We report experiments for translation from English into two morphologically rich languages, Czech and Latvian, and show the importance of explicitly modeling target morphology.", "labels": [], "entities": [{"text": "translation from English", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8600937525431315}]}], "introductionContent": [{"text": "Open vocabularies remain a challenge for Neural Machine Translation (NMT) (, both for linguistic and computational reasons.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.756658544143041}]}, {"text": "From a linguistic standpoint, morphological variation and lexical productivity cause word forms unseen in training to occur in source texts, which may also require to generate novel target word forms.", "labels": [], "entities": []}, {"text": "Using very large input/output vocabularies partially mitigates these issues, yet may cause serious instability (when computing embeddings of rare or unseen words) and complexity issues (when dealing with large softmax layers).", "labels": [], "entities": []}, {"text": "Several proposals have been put forward to address these problems, which are particularly harmful when one language is a morphologically rich * Both authors have contributed equally to this language (MRL), exhibiting larger token/type ratio than is observed for English.", "labels": [], "entities": []}, {"text": "One strategy is to improve NMT's internal procedures: for instance by using a structured output layer or by altering the training or decoding criteria ().", "labels": [], "entities": [{"text": "NMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8858399987220764}]}, {"text": "An alternative approach is to work with representations designed to remove some variations via source-side or target-side normalization procedures; or more radically to consider character-based representations (, which are however much more costly to train, and make long distance dependencies even longer.", "labels": [], "entities": []}, {"text": "None has however been as successful as the recent proposal of which seems to achieve aright balance between a limited vocabulary size and an ability to translate a fully open vocabulary.", "labels": [], "entities": []}, {"text": "In this work, we consider possible ways to extend this approach by also supplying target-side linguistic information in order to help the system generate correct target word forms.", "labels": [], "entities": []}, {"text": "Our proposal relies on two distinct components (a) linguistically or data-driven normalization procedures manipulating various source and target word segmentations, as well as eg. multiple factors on the target side (see \u00a7 4), and (b) a neural architecture equipped with a dual output layer to predict the target in two simpler tasks generating the lexi-cal unit and the morphological information ( \u00a7 3).", "labels": [], "entities": []}, {"text": "These components are assessed separately and in conjunction using translation from English into two MRLs: Czech and Latvian.", "labels": [], "entities": []}, {"text": "Our experiments show improvement over a strong) BPE-to-BPE baseline, incorporating ensemble of models and backtranslated data.", "labels": [], "entities": [{"text": "BPE-to-BPE baseline", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.6540162861347198}]}, {"text": "Overall, they suggest that BPE representations, which loosely simulates concatenative morphological processes, is complementary to feature-based morphological representations.", "labels": [], "entities": [{"text": "BPE representations", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7361341714859009}]}], "datasetContent": [{"text": "We introduce here the experimental setup for all the reported systems translating from English into Czech and Latvian.", "labels": [], "entities": []}, {"text": "Results are reported using the following automatic metrics: BLEU (), BEER) which tunes a large number of features to maximize the human ranking correlation at sentence level and Charac-TER (, a character-level version of TER which has shown a high correlation with human rankings (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9987843632698059}, {"text": "BEER", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9979142546653748}]}, {"text": "Each score on fully inflected word systems is averaged from two independent runs (for both single and ensembled models).", "labels": [], "entities": []}, {"text": "The results using the bitext provided at the WMT'17 the evaluation campaign are presented in for English-to-Czech 8 and in for English-to-Latvian.", "labels": [], "entities": [{"text": "bitext", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9470100998878479}, {"text": "WMT'17 the evaluation campaign", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.9061545580625534}]}, {"text": "We can observe that using the constrained decoding consistently improves the results, except when using split clusters.", "labels": [], "entities": []}, {"text": "In this last case, the system is forced to predict a PoS in the second factor whenever it has generated a cluster ID in the first factor.", "labels": [], "entities": [{"text": "PoS", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.6763562560081482}]}, {"text": "Since there is a reduced quantity of such cluster IDs, the model has no difficulty to learn the constraints by itself and therefore to map a cluster ID exclusively to a specific subset of PoS.", "labels": [], "entities": []}, {"text": "In the Latvian lemma setup, we observe that the improvement using constrained decoding is lower than for Czech (see), which is probably due to the quality of the noisy look-up table we have created for Latvian (see \u00a7 5.1).", "labels": [], "entities": []}, {"text": "Note that we have no such dependency on the lexical resources at decoding time for the normalized word setups, where improvements are comparable across both language pairs.", "labels": [], "entities": []}, {"text": "The systems using BPE tokens significantly outperform word-level systems, which confirms the analysis of.", "labels": [], "entities": []}, {"text": "The results show that BPE units are even more efficient when applied to normalized words, providing significant improvements over segmented inflected words of 1.79 and 1.85 BLEU points for Czech, and 0.78 and 1.06 for Latvian.", "labels": [], "entities": [{"text": "BPE", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9849931001663208}, {"text": "BLEU", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.998275637626648}]}, {"text": "The lemma representation was tested with the two FNMT models presented in \u00a7 3, one model using a single hidden-to-output layer (single h2o layer) and the other model using two separated hidden-to-output layers (separated h2o layers).: Scores for English-to-Czech systems trained on official bitext data We observe mixed results, here: the system with the single h2o layer has slightly better results for the word-to-word systems, but the BPE-to-BPE factored lemma system obtains better performance with the separated h2o layers architecture.", "labels": [], "entities": [{"text": "FNMT", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9055630564689636}]}, {"text": "For that reason, we decided to only use the separated h2o layers architecture for the next set of experiments involving synthetic data which is the aim of the next section. and 5 show the results of using selected parts of bitext and synthetic parallel data (see section 5.1) for both language pairs.", "labels": [], "entities": []}, {"text": "Each model trained with a selection of bitext and synthetic data was initialized with the parameters of its counterpart trained on bitext.", "labels": [], "entities": []}, {"text": "The BPE vocabulary used was the same as in the model used for initialization, which led the systems to generate unknown words.", "labels": [], "entities": [{"text": "initialization", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.9755373001098633}]}, {"text": "In our experiments, we forced the decoder to avoid unknown token generation.", "labels": [], "entities": [{"text": "unknown token generation", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6636798878510793}]}, {"text": "By using synthetic data, we are able to obtain a large improvement for all systems, which is inline with ().", "labels": [], "entities": []}, {"text": "We notice that the contrasts present in the previous section between the various word representations are less clear now.", "labels": [], "entities": []}, {"text": "The baseline system (first two rows) is the system which benefits the most from the additional data with +5.7 and +6.9 BLEU for Czech and Latvian.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9975296854972839}]}, {"text": "The performance of factored systems has also increased, but to a lesser extent, leading to slightly worse results compared to the baseline system.", "labels": [], "entities": []}, {"text": "This situation changes when the reinflected hypotheses are rescored.", "labels": [], "entities": []}, {"text": "We are then able to surpass the baseline system with normalized words.", "labels": [], "entities": []}, {"text": "The two language pairs react differently to kbest hypotheses rescoring (+k-best rescored in the tables).", "labels": [], "entities": []}, {"text": "For Czech, this has nearly no impact on translation quality according to the metrics, whereas it provides an important improvement in Latvian: +2.03 and +0.84 BLEU in the split cluster setup.", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9493817687034607}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9977840781211853}]}, {"text": "Note that this specific setup gives the best score we could achieve on newsdev-2017, without n-best rescoring or model ensembling.", "labels": [], "entities": []}, {"text": "We interpret this situation as a result of the difference in quality observed for the Czech and Latvian dictionaries used for reinflection.", "labels": [], "entities": []}, {"text": "Indeed, since Morphodita contains exclusively useful Czech reinflection candidates, a simple unigram model is sufficient to select the right word forms, making the generation of 10-best reinflection hypotheses useless.", "labels": [], "entities": []}, {"text": "On the other hand, the hypotheses returned by the look-up table we have used to generate Latvian word forms were noisy and required a rescoring from an MT system based on fully inflected words.", "labels": [], "entities": []}, {"text": "We obtained the best results for Newstest-2017 BLEU \u2191 BEER \u2191 CTER \u2193 BLEU \u2191 BEER \u2191 CTER \u2193 words-to-words fully inflected w.", "labels": [], "entities": [{"text": "Newstest-2017", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.885904848575592}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9853762984275818}, {"text": "BEER", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9915999174118042}, {"text": "CTER", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9078760147094727}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9843323230743408}, {"text": "BEER", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9530357122421265}, {"text": "CTER", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.8409956693649292}]}, {"text": "15: Scores for English-to-Czech systems (BPE-to-BPE) trained on selected bitext and synthetic parallel data.", "labels": [], "entities": []}, {"text": "this Latvian setup by generating the 100-best reinflection hypotheses, which provides less dependency on the quality of the dictionary and relies more on the knowledge learned by a word-formaware system.", "labels": [], "entities": []}, {"text": "Despite the fact that such a rescoring procedure is costly in terms of computational time, we observe that it can be a helpful solution when no resources of quality are available.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9666329622268677}]}, {"text": "Czech n-best reinflection, as opposed to kbest, turned out to be efficient, bringing the lemma-based systems to the level of the baselines and even above for the normalized word setups.", "labels": [], "entities": []}, {"text": "Whereas it does not improve with Latvian normalized words, we observe a positive impact on the lemma-based systems.", "labels": [], "entities": []}, {"text": "We assume that rescoring the n-best list is away to rely on an inflectedword-based system to make important decisions related to translation, as opposed to the much simpler monolingual process of reinflection mentioned above.", "labels": [], "entities": [{"text": "translation", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.9695062041282654}]}, {"text": "Latvian split-cluster models seem to have nothing to learn from such systems.", "labels": [], "entities": []}, {"text": "Factored norm performs best among all the presented models, showing consistent BLEU improvements over the baselines of 0.25 and 0.56 for Czech, and 0.57 and 0.89 for Latvian.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9995813965797424}]}, {"text": "We finally notice that ensembling two models slightly reduces those contrasts, and lemma-based systems are the ones that benefit the most from model ensembling.", "labels": [], "entities": []}, {"text": "Conclusions are not easy to draw, since across the different setups, the level of indepen-  It is important to note that automatic metrics may not do justice to the lexical and grammatical choices made by the factored systems.", "labels": [], "entities": [{"text": "indepen-", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9209640026092529}]}, {"text": "In an attempt to focus on the grammaticality of the FNMT systems, we conducted a qualitative analysis of the outputs.", "labels": [], "entities": [{"text": "FNMT", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8230217695236206}]}], "tableCaptions": [{"text": " Table 2: Scores for English-to-Czech systems trained on official bitext data", "labels": [], "entities": [{"text": "official bitext data", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.8682428598403931}]}, {"text": " Table 3: Scores for English-to-Latvian systems trained on official bitext.", "labels": [], "entities": [{"text": "official bitext", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.7247834205627441}]}, {"text": " Table 4: Scores for English-to-Czech systems (BPE-to-BPE) trained on selected bitext and synthetic  parallel data.", "labels": [], "entities": []}, {"text": " Table 5: Scores for English-to-Latvian systems (BPEs-to-BPEs) trained on selected bitext and synthetic  parallel data.", "labels": [], "entities": []}, {"text": " Table 6: Morphological prediction consistency (Entropy).", "labels": [], "entities": [{"text": "Morphological prediction consistency", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6450220346450806}]}]}