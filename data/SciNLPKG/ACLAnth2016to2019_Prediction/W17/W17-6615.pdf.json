{"title": [{"text": "Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings have been found to provide meaningful representations for words in an efficient way; therefore, they have become common in Natural Language Processing systems.", "labels": [], "entities": []}, {"text": "In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants.", "labels": [], "entities": []}, {"text": "We trained 31 word embedding models using FastText, GloVe, Wang2Vec and Word2Vec.", "labels": [], "entities": [{"text": "Wang2Vec", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.8919801712036133}, {"text": "Word2Vec", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9679881930351257}]}, {"text": "We evaluated them intrinsically on syntactic and semantic analogies and extrinsically on POS tagging and sentence semantic similarity tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.8149619996547699}, {"text": "sentence semantic similarity", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.6805393298467001}]}, {"text": "The obtained results suggest that word analogies are not appropriate for word embedding evaluation instead task-specific evaluations maybe a better option; Wang2Vec appears to be a robust model; the increase in performance in our evaluations with bigger models is not worth the increase in memory usage for models with more than 300 dimensions.", "labels": [], "entities": [{"text": "word embedding evaluation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7253246903419495}, {"text": "Wang2Vec", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.8638979196548462}]}], "introductionContent": [{"text": "Natural Language Processing (NLP) applications usually take words as basic input units; therefore, it is important that they be represented in a meaningful way.", "labels": [], "entities": []}, {"text": "In recent years, word embeddings have been found to efficiently provide such representations, and consequently, have become common in modern NLP systems.", "labels": [], "entities": []}, {"text": "They are vectors of real valued numbers, which represent words in an n-dimensional space, learned from large nonannotated corpora and able to capture syntactic, semantic and morphological knowledge.", "labels": [], "entities": []}, {"text": "Different algorithms have been developed to generate embeddings, inter alia].", "labels": [], "entities": []}, {"text": "They can be roughly divided into two families of methods: the first is composed of methods that work with a co-occurrence word matrix, such as Latent Semantic Analysis (LSA), Hyperspace Analogue to Language (HAL) and Global Vectors (GloVe).", "labels": [], "entities": []}, {"text": "The second is composed of predictive methods, which try to predict neighboring words given one or more context words, such as Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.9742141366004944}]}, {"text": "Given this variety of word embedding models, methods for evaluating them becomes a topic of interest.] developed a benchmark for embedding evaluation based on a series of analogies.", "labels": [], "entities": []}, {"text": "Each analogy is composed of two pairs of words that share some syntactic or semantic relationship, e.g., the names of two countries and their respective capitals, or two verbs in their present and past tense forms.", "labels": [], "entities": []}, {"text": "In order to evaluate an embedding model, applying some vectorial algebra operation to the vectors of three of the words should yield the vector of the fourth one.", "labels": [], "entities": []}, {"text": "A version of this dataset translated and adapted to Portuguese was created by.", "labels": [], "entities": []}, {"text": "However, in spite of being popular and computationally cheap, suggests that word analogies are not appropriate for evaluating embeddings.", "labels": [], "entities": []}, {"text": "Instead, they suggest using task-specific evaluations, i.e., to compare word embedding models on how well they perform on downstream NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants (Section 2).", "labels": [], "entities": []}, {"text": "We trained our models using four different algorithms with varying dimensions (Section 3).", "labels": [], "entities": []}, {"text": "We evaluated them on the aforementioned analogies as well as on POS tagging and sentence similarity, to assess both syntactic and semantic properties of the word embeddings (Section 4).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.7289834916591644}]}, {"text": "Section 5 revises recent studies evaluating Portuguese word embeddings.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: i) to make a set of 31 word embedding models publicly available 1 as well as the script used for corpus preprocessing and embedding evaluations 2 ; and ii) an intrinsic and extrinsic evaluation of word embedding models, indicating the lack of correlation between performance in syntactic and semantic analogies and syntactic and semantic NLP tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the robustness of the word embedding models we trained, we performed intrinsic and extrinsic evaluations.", "labels": [], "entities": []}, {"text": "For the intrinsic evaluation, we used the set of syntactic and semantic analogies from.", "labels": [], "entities": []}, {"text": "For extrinsic evaluation, we chose to apply the trained models on POS tagging and sentence similarity tasks.", "labels": [], "entities": [{"text": "extrinsic evaluation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7752386927604675}, {"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8662731051445007}, {"text": "sentence similarity tasks", "start_pos": 82, "end_pos": 107, "type": "TASK", "confidence": 0.7687509059906006}]}, {"text": "The tasks were chosen deliberately since they are linguistically aligned with the sets of analogies used in the first evaluation.", "labels": [], "entities": []}, {"text": "POS tagging is by nature a morphosyntactic task, and although some analogies are traditionally regarded as syntactic, they are actually morphological -for example, suffix operations.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.808284342288971}]}, {"text": "Sentence similarity is a semantic task since it evaluates if two sentences have similar meaning.", "labels": [], "entities": [{"text": "Sentence similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9551621973514557}]}, {"text": "It is expected that the models which achieve the best results in (morpho-)syntactic analogies also do so in POS tagging, and the same is true for semantic analogies and semantic similarity evaluation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.7597986459732056}, {"text": "semantic similarity evaluation", "start_pos": 169, "end_pos": 199, "type": "TASK", "confidence": 0.7241955995559692}]}, {"text": "We trained embeddings with the following dimensions: 50, 100, 300, 600 and 1,000.", "labels": [], "entities": []}, {"text": "Because Wang2Vec's implementation suffers from the vanishing gradient problem for high dimension matrices, it was not possible to train its CBOW models for 600 and 1,000 dimensions.", "labels": [], "entities": []}, {"text": "We evaluated our embeddings in the syntactic and semantic analogies provided by.", "labels": [], "entities": []}, {"text": "The benchmark contains five types of semantic analogy: (i) common capitals and countries, (ii) all capitals and countries, (iii) currency and countries, (iv) cities and states, and (v) family relations.", "labels": [], "entities": []}, {"text": "Moreover, nine types of syntactic analogy are also represented: adjectives and adverbs, opposite adjectives, base adjectives and comparatives, base adjectives and superlatives, verb infinitives and present participles, countries and nationalities (adjectives), verb infinitives and past tense forms, nouns in plural and singular, and verbs in plural and singular.", "labels": [], "entities": []}, {"text": "Since our corpus is composed of both Brazilian (PT-BR) and European (PT-EU) Portuguese, we also evaluated the models in the test sets for both variants, following.", "labels": [], "entities": []}, {"text": "shows the obtained results for the intrinsic evaluation.", "labels": [], "entities": []}, {"text": "On average, GloVe was the best model for both Portuguese variants.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.5427961349487305}]}, {"text": "The model which best performed on syntactic analogies was FastText, followed by Wang2Vec.", "labels": [], "entities": [{"text": "FastText", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9043956398963928}, {"text": "Wang2Vec", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9665310382843018}]}, {"text": "This makes sense since FastText is a morphological model, and Wang2Vec uses word order, which provides some minimal  syntactic knowledge.", "labels": [], "entities": [{"text": "Wang2Vec", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9307915568351746}]}, {"text": "In semantic analogies, the model which best performed was GloVe, followed by Wang2Vec.", "labels": [], "entities": [{"text": "Wang2Vec", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9680003523826599}]}, {"text": "GloVe is known for modeling semantic information well and Wang2Vec potentially captures semantics because it uses word order.", "labels": [], "entities": [{"text": "Wang2Vec", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.8711413145065308}]}, {"text": "The position of a negation word in a sentence can totally change its semantics.", "labels": [], "entities": []}, {"text": "If this negation is shuffled in a bag of words (Word2Vec CBOW), sentence semantic is diluted.", "labels": [], "entities": [{"text": "Word2Vec CBOW", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8203293085098267}]}, {"text": "All CBOW models, except for the Wang2Vec ones, achieved very low results in semantic analogies, similarly to the results from].", "labels": [], "entities": [{"text": "Wang2Vec", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.948638379573822}]}, {"text": "Wang2Vec CBOW differs from traditional CBOW in that it takes word order into account, and then we can speculate that an unordered bag-of-words is notable to capture a word's semantics so well.", "labels": [], "entities": [{"text": "Wang2Vec CBOW", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9112261831760406}]}, {"text": "We exemplify with our best (GloVe) and worst (Word2Vec CBOW) models using 600 dimensions.", "labels": [], "entities": []}, {"text": "Dealing with the well known analogy \"king to queen\", in Portuguese \"rei -homem + mulher \u2248 rainha\", our best model produced \"rainha\" as the most similar embedding (0,62 cosine similarity) and our worst model produced \"esposa\" (0,50 cosine similarity) with \"rainha\" in the 7th place (0,41 cosine similarity).", "labels": [], "entities": []}, {"text": "In this section we describe the experiments performed on POS tagging and Semantic Similarity tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.8975490629673004}, {"text": "Semantic Similarity tasks", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7687681516011556}]}], "tableCaptions": [{"text": " Table 1. Sources and statistics of corpora collected.", "labels": [], "entities": []}, {"text": " Table 2. Intrinsic evaluation on syntactic and semantic analogies.", "labels": [], "entities": []}, {"text": " Table 3. Extrinsic evaluation on POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.7904894649982452}]}, {"text": " Table 4. Extrinsic evaluation on Semantic Similarity task.", "labels": [], "entities": [{"text": "Semantic Similarity task", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.762401282787323}]}]}