{"title": [{"text": "Toward Never Ending Language Learning for Morphologically Rich Languages", "labels": [], "entities": []}], "abstractContent": [{"text": "This work deals with ontology learning from unstructured Russian text.", "labels": [], "entities": [{"text": "ontology learning from unstructured Russian text", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.8931529422601064}]}, {"text": "We implement one of the components of Never Ending Language Learner and introduce the algorithm extensions aimed to gather specificity of morphologically rich free-word-order language.", "labels": [], "entities": []}, {"text": "We perform several experiments comparing different settings of the training process.", "labels": [], "entities": []}, {"text": "We demonstrate that morphological features significantly improve the system precision while seed patterns help to improve the coverage .", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9974836707115173}]}], "introductionContent": [{"text": "Nowadays a big interest is paid to systems that can extract facts from the Internet (.", "labels": [], "entities": []}, {"text": "The main challenge is to design systems that do not require any human involvement and may efficiently store lots of information limited only by the amount of the knowledge uploaded to the Internet.", "labels": [], "entities": []}, {"text": "One of the ways of representing information for such systems is ontologies.", "labels": [], "entities": []}, {"text": "According to the famous definition by, ontology is \"an explicit specification of a conceptualization\", i.e. formalization of knowledge that underlines language utterance.", "labels": [], "entities": []}, {"text": "In the simplest case, ontology is a structure containing concepts and relations among them.", "labels": [], "entities": []}, {"text": "In addition, it may contain a set of axioms that define the relations and constraints on their interpretation).", "labels": [], "entities": []}, {"text": "One of the advantages of such structures is data formalization that simplifies the automatic processing.", "labels": [], "entities": []}, {"text": "Ontologies are widely used in information retrieval, texts analysis and semantic applications.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.8172314167022705}, {"text": "texts analysis", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.8147918879985809}]}, {"text": "In many practical applications, ontological concepts should be associated with lexicon, i.e. with language expressions and structures.", "labels": [], "entities": []}, {"text": "Even though ontologies themselves contain knowledge about the world, not a language, their primary goal is to ensure semantic interpretation of texts.", "labels": [], "entities": [{"text": "semantic interpretation of texts", "start_pos": 117, "end_pos": 149, "type": "TASK", "confidence": 0.8003530651330948}]}, {"text": "Thus, ontology learning from text is an emerging research direction.", "labels": [], "entities": [{"text": "ontology learning from text", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.9092031717300415}]}, {"text": "One of the approaches that are used to learn facts from unstructured text is called Never Ending Language Learning (NELL).", "labels": [], "entities": [{"text": "Never Ending Language Learning (NELL)", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.6051420569419861}]}, {"text": "One of the NELL advantages is its low demand for preprocessed data required for the learning process.", "labels": [], "entities": [{"text": "NELL", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.8418947458267212}]}, {"text": "Given an initial ontology that contains 10-20 seeds for each category as an input, NELL can achieve a high performance level on extracting facts and relations from a large corpus ().", "labels": [], "entities": [{"text": "NELL", "start_pos": 83, "end_pos": 87, "type": "TASK", "confidence": 0.8581615686416626}]}, {"text": "The first implementation of NELL () worked with English.", "labels": [], "entities": []}, {"text": "An attempt was made to extend the NELL approach for the Portuguese language.", "labels": [], "entities": [{"text": "NELL", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.8943282961845398}]}, {"text": "The main result of these experiments was that applying initial NELL parameters and ontology to non-English web-pages would not show high results; initial configuration did notwork well with Portuguese web-pages.", "labels": [], "entities": []}, {"text": "The authors made a conclusion that in order to extend the NELL approach to anew language, it is necessary to prepare anew seed ontology and contextual patterns that depend on the language rules.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a NELL extension to the Russian language.", "labels": [], "entities": []}, {"text": "Being a Slavic language, Russian has a rich morphology and free word order.", "labels": [], "entities": []}, {"text": "Thus, common expressions for semantic relations in text have a specific form: the word order is less reliable than for Germanic or Romance languages; the morphological properties of words are more crucial.", "labels": [], "entities": []}, {"text": "However, many pattern learning techniques are based on word order of pattern components and usually do not include morphology.", "labels": [], "entities": [{"text": "pattern learning", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.8204173147678375}]}, {"text": "Thus, the adaptation of the NELL approach to a Slavic language would require changes in the pattern structure.", "labels": [], "entities": []}, {"text": "We introduce an adaptation of NELL to Russian, test it on a small dataset of 2.5 million words for 9 ontology categories and demonstrate that utilizing of morphology is crucial for ontology learning for Russian.", "labels": [], "entities": []}, {"text": "This is the main contribution of this paper.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 overviews original NELL approach.", "labels": [], "entities": [{"text": "NELL", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.9773369431495667}]}, {"text": "Our improvements of the algorithm are presented in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes our data source, its preprocessing, and experiments we run.", "labels": [], "entities": []}, {"text": "Results of these experiments are presented and discussed in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6, we give a brief overview of the related papers.", "labels": [], "entities": []}, {"text": "We summarize the results and outline the future work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run experiments for all categories independently.", "labels": [], "entities": []}, {"text": "Then we collect all extracted instances and manually annotate them as corrector incorrect.", "labels": [], "entities": []}, {"text": "Then for each category c, we evaluated precision using the following formula:  where CorrInst(c) is the number of correct instances extracted for category c, and AllInst(c) is the whole number of instances, that were extracted by CPL for category c.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9994636178016663}, {"text": "CorrInst(c)", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9162802845239639}, {"text": "AllInst", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9802558422088623}]}, {"text": "When we use the THRESHOLD-SUPPORT strategy, we perform a final filtering using different minimal support values.", "labels": [], "entities": [{"text": "THRESHOLD-SUPPORT", "start_pos": 16, "end_pos": 33, "type": "METRIC", "confidence": 0.8834206461906433}]}, {"text": "For algorithm comparison, we use values 0 .1 , 0 .5 and 1 .0 The main experiment is devoted to CPL-RUS with THRESHOLD-SUPPORT strategy.", "labels": [], "entities": [{"text": "CPL-RUS", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.7340401411056519}, {"text": "THRESHOLD-SUPPORT", "start_pos": 108, "end_pos": 125, "type": "METRIC", "confidence": 0.9525958895683289}]}, {"text": "The algorithm converges after 6-10 iterations depending on category.", "labels": [], "entities": []}, {"text": "We run it on all the categories and investigate the dependency of precision on support value used to cutoff trusted instances after the algorithm converges.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.994723916053772}]}, {"text": "In addition, we perform a set of smaller experiments to study CPL properties and impact of different parameters.", "labels": [], "entities": []}, {"text": "We test: 1) usefulness of morphological features; 2) usefulness of pattern seeds; 3) differences between threshold selection strategies.", "labels": [], "entities": []}, {"text": "In the first experiment, we compare CPL-RUS and aversion of this algorithm which do not use morphology (thus, similar to the English CPL).", "labels": [], "entities": [{"text": "aversion", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.961578369140625}]}, {"text": "We will refer to the second one as CPL-NOMORPH.", "labels": [], "entities": [{"text": "CPL-NOMORPH", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8206287026405334}]}, {"text": "We run it on three ontology categories: VEGETABLE, FRUIT, and FOOD.", "labels": [], "entities": [{"text": "VEGETABLE", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.753585934638977}, {"text": "FRUIT", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9878917932510376}, {"text": "FOOD", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9871195554733276}]}, {"text": "The first run uses morphological constraints and the second allows words in all morphological forms.", "labels": [], "entities": []}, {"text": "In the second experiment, we investigate if the usage of seed patterns can improve the quality of the algorithm; the same experiment was conducted by).", "labels": [], "entities": []}, {"text": "As can be seen from the description in Section 2, CPL can learn without seed patterns, relying only on the set of initial categories and instances.", "labels": [], "entities": []}, {"text": "However, since the initial ontology is small, this might be not the optimal strategy.", "labels": [], "entities": []}, {"text": "We will refer to the second algorithm as CPL-NOPAT.", "labels": [], "entities": []}, {"text": "We run the algorithms on the same three categories: VEGETABLE, FRUIT, and FOOD.", "labels": [], "entities": [{"text": "VEGETABLE", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.8429826498031616}, {"text": "FRUIT", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9889600276947021}, {"text": "FOOD", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9853549003601074}]}, {"text": "In the third experiment, we compare two Threshold selection strategies described in Section 3.3: THRESHOLD-SUPPORT, based on minimal Support after the first iteration and THRESHOLD-50 that keeps the fixed number of patterns and instances and revise the trusted lists after each iteration..1 shows the main results of running CPL-RUS on the whole ontology using seeds.", "labels": [], "entities": [{"text": "THRESHOLD-SUPPORT", "start_pos": 97, "end_pos": 114, "type": "METRIC", "confidence": 0.9786087870597839}, {"text": "THRESHOLD-50", "start_pos": 171, "end_pos": 183, "type": "METRIC", "confidence": 0.9644172191619873}]}], "tableCaptions": [{"text": " Table 5.2. The precision  for all categories, in this case, is much lower,  which makes CPL-NOMORPH completely use- less. While CPL-RUS can achieve precision 1 .0  for VEGETABLE and FRUIT categories, the max- imum result for the same categories in uncon- strained mode is 0 .43 .", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9993139505386353}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9816438555717468}, {"text": "FRUIT", "start_pos": 183, "end_pos": 188, "type": "METRIC", "confidence": 0.8108419179916382}]}, {"text": " Table 5.2 presents results on comparison of the  learning progress for the three categories with and  without morphological constraints. As can be  seen, morphological constraints decrease the num- ber of extracted instances and patterns and slow  down the training process.", "labels": [], "entities": []}, {"text": " Table 4: Results of CPL-RUS.", "labels": [], "entities": [{"text": "CPL-RUS", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.7087265253067017}]}, {"text": " Table 5: Results of CPL-NOMORPH.", "labels": [], "entities": [{"text": "CPL-NOMORPH", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.5826418995857239}]}, {"text": " Table 6: Number of extracted instances and patterns in case of using/non-using morphological con- straints.", "labels": [], "entities": []}, {"text": " Table 7: Results for CPL-NOPAT.", "labels": [], "entities": [{"text": "CPL-NOPAT", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.622748851776123}]}, {"text": " Table 8: The number of extracted instances for  each category with/without seed patterns.", "labels": [], "entities": []}]}