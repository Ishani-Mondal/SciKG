{"title": [{"text": "Finnish resources for evaluating language model semantics", "labels": [], "entities": [{"text": "evaluating language model semantics", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.6651074066758156}]}], "abstractContent": [{"text": "Distributional language models have consistently been demonstrated to capture semantic properties of words.", "labels": [], "entities": []}, {"text": "However, research into the methods for evaluating the accuracy of the modeled semantics has been limited, particularly for less-resourced languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9968312382698059}]}, {"text": "This research presents three resources for evaluating the semantic quality of Finnish language dis-tributional models: (1) semantic similarity judgment resource, as well as (2) a word analogy and (3) a word intrusion test set.", "labels": [], "entities": [{"text": "semantic similarity judgment", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.6826700965563456}]}, {"text": "The use of evaluation resources is demonstrated in practice by presenting them with different language models built from varied corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the spirit of the distributional hypothesis stating that semantically similar words appear in similar contexts, distributional language models of recent years have successfully been able to capture semantics properties of words given large corpora (e.g.,).", "labels": [], "entities": []}, {"text": "However, there are only few resources for evaluating the accuracy or validity of language models, particularly for less-spoken languages, due to language dependence of such resources (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9956538677215576}]}, {"text": "Further, a single good resource may not be sufficient due to the complexity of semantics; performance in an intrinsic evaluation task may not predict performance in extrinsic downstream language technology applications (.", "labels": [], "entities": []}, {"text": "Therefore, the evaluation of semantics should be based on a variety of tasks, estimating different semantic phenomena (.", "labels": [], "entities": []}, {"text": "With respect to language models, two distinct measures of semantic quality can be identified: validity and completeness.", "labels": [], "entities": [{"text": "validity", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9928988218307495}]}, {"text": "The latter is dependent on the underlying corpus because a language model can only represent linguistic units which have been present in its training data.", "labels": [], "entities": []}, {"text": "While it is possible for some models to infer the meaning of novel input, the inference can be considered an additional training step of the model and thus an extension of the training corpus.", "labels": [], "entities": []}, {"text": "Completeness is also likely to affect the validity of a model; given the distributional hypothesis, more encompassing knowledge about the possible contexts of words results in more accurate knowledge of their semantics.", "labels": [], "entities": []}, {"text": "In this study, the lack of completeness is estimated only by presenting a rate of out-of-vocabulary (OOV) words for each separate evaluation resource, but not investigated further.", "labels": [], "entities": []}, {"text": "The aim of this research is to present scientist and practitioners working with Finnish tools to evaluate their language models with respect to semantics.", "labels": [], "entities": []}, {"text": "While most research compares the performance of models to that of humans, we also present effortlessly extensible tools requiring no human annotation.", "labels": [], "entities": []}, {"text": "Finally, baseline results for the evaluation methods are reported, utilizing varied corpora and language model architectures.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of different language models in the three presented evaluation tasks. The scores of  the best performing models for each corpus in each evaluation task are marked in bold.", "labels": [], "entities": []}]}