{"title": [{"text": "Predicting Pronouns with a Convolutional Network and an N-gram Model", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the UU-HARDMEIER system submitted to the DiscoMT 2017 shared task on cross-lingual pronoun prediction.", "labels": [], "entities": [{"text": "DiscoMT 2017 shared task", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.740422785282135}, {"text": "cross-lingual pronoun prediction", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.6420383850733439}]}, {"text": "The system is an ensemble of con-volutional neural networks combined with a source-aware n-gram language model.", "labels": [], "entities": []}, {"text": "1 Overview For the 2017 cross-lingual pronoun prediction shared task, we chose to create a system that could be implemented very quickly while still providing an interesting comparison to the other systems we expect to participate in the shared task.", "labels": [], "entities": [{"text": "cross-lingual pronoun prediction shared task", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.77159663438797}]}, {"text": "The core components of our system area convolutional neural network that evaluates the context of the source and target context of the examples.", "labels": [], "entities": []}, {"text": "As in our systems from the previous year (Hardmeier, 2016; Lo\u00e1iciga et al., 2016), we also use a source-aware n-gram language model as a complementary component.", "labels": [], "entities": []}, {"text": "In contrast to 2016, our neural network classifier does not attempt to model pronom-inal anaphora explicitly.", "labels": [], "entities": []}, {"text": "This change was made to simplify the model and avoid the heavyweight preprocessing that our earlier systems required.", "labels": [], "entities": []}, {"text": "Instead , we focused on implementing a more sophisticated system combination method that permits the construction of a larger ensemble of models.", "labels": [], "entities": []}, {"text": "2 Convolutional neural network The neural network architecture of our pronoun prediction model is loosely inspired by the winning system of the WMT 2016 shared task on cross-lingual pronoun prediction (Luotolahti et al., 2016).", "labels": [], "entities": [{"text": "pronoun prediction", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7451277673244476}, {"text": "WMT 2016 shared task on cross-lingual pronoun prediction", "start_pos": 144, "end_pos": 200, "type": "TASK", "confidence": 0.5536937192082405}]}, {"text": "However, since we expected a large proportion of the participating systems to use recurrent neural networks, we decided to use a simpler convolu-tional architecture instead.", "labels": [], "entities": []}, {"text": "The implementation of the network uses the Keras library (Chollet et al., 2015).", "labels": [], "entities": [{"text": "Keras library", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.8783235251903534}]}, {"text": "The network independently scans four different input areas for each example: left source, left target , right source and right target.", "labels": [], "entities": []}, {"text": "All four areas are defined with respect to the position of the element to be predicted, which is a placeholder to be filled on the target side aligned to a pronoun on the source side.", "labels": [], "entities": []}, {"text": "The left areas cover the context preceding the pronoun or placeholder, up to the beginning of the previous sentence or at most 50 tokens to the left of the anchoring position, whichever is shorter.", "labels": [], "entities": []}, {"text": "The right areas cover the context following the pronoun or placeholder, up to the end of the current sentence or at most 50 tokens if the sentence is longer.", "labels": [], "entities": []}, {"text": "The context size limit of 50 tokens is large enough to have no effect inmost cases, but it ensures that the training efficiency does not suffer from a few overlong sentences.", "labels": [], "entities": []}, {"text": "The source language pronoun aligned to the placeholder is included in both the left and right source context area, whereas the placeholder on the target side is excluded from the context areas.", "labels": [], "entities": []}, {"text": "The words of the source and target language are encoded as one-hot vectors using the vocabulary of the IWSLT part of the official training data.", "labels": [], "entities": [{"text": "IWSLT part of the official training data", "start_pos": 103, "end_pos": 143, "type": "DATASET", "confidence": 0.7696603664330074}]}, {"text": "Words occurring only once in the IWSLT training set are excluded from the vocabulary and treated as unknown words instead.", "labels": [], "entities": [{"text": "IWSLT training set", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.8289770682652792}]}, {"text": "The part-of-speech tags provided in the training data are ignored.", "labels": [], "entities": []}, {"text": "The one-hot vectors are mapped to dense embeddings through an embedding layer with tanh activation, whose weights are initialised randomly at training time.", "labels": [], "entities": []}, {"text": "The dense word embeddings form the input of one convolutional layer per input area.", "labels": [], "entities": []}, {"text": "The output of the convolutional layers undergo max pooling in a single step over the entire length of the input area.", "labels": [], "entities": []}, {"text": "Then the vectors resulting for the four input areas are concatenated together and used as the input of a densely connected layer with softmax activation 58", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Properties of the convolutional neural networks included in our submissions", "labels": [], "entities": []}, {"text": " Table 2: Official evaluation results for primary  (ensemble) and contrastive (n-gram) systems", "labels": [], "entities": []}, {"text": " Table 3: Weights summed over all epochs for  individual systems", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.982211172580719}]}]}