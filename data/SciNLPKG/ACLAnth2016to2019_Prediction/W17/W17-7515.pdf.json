{"title": [{"text": "Sentiment Analysis: An Empirical Comparative Study of Various Machine Learning Approaches", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9524809122085571}]}], "abstractContent": [{"text": "The aim of this paper is to experiment with different machine learning approaches to predict/classify the sentiment on various available sentiment corpuses named as Subjectivity v1.0 corpus, IMDB movie review corpus, Rotten Tomatoes (RT) Movie Reviews corpus, Twitter sentiment dataset.", "labels": [], "entities": [{"text": "IMDB movie review corpus", "start_pos": 191, "end_pos": 215, "type": "DATASET", "confidence": 0.7235478907823563}, {"text": "Rotten Tomatoes (RT) Movie Reviews corpus", "start_pos": 217, "end_pos": 258, "type": "DATASET", "confidence": 0.8407545983791351}, {"text": "Twitter sentiment dataset", "start_pos": 260, "end_pos": 285, "type": "DATASET", "confidence": 0.791780948638916}]}, {"text": "Variants of Naive Bayes (NB) and Support Vector Machines (SVM) have been often used for text categorization as base-line.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7403591573238373}]}, {"text": "In this paper, we have tried to show that how embodying bigram and trigram features with Logistic Regression (LR), Mutinomial Naive Bayes (MNB) and Support Vector Machine (SVM) show significant improvent in the sentiment anlay-sis.", "labels": [], "entities": []}, {"text": "Another observation we obtained is that LR outperforms the MNB and SVM in both large as well as short (snippets) sentiment text when sentiment classes are limited to two/three.", "labels": [], "entities": []}, {"text": "Furthermore, when the sentiment analysis task turn into a kind of multi-class classifiction instead of binary on large corpora, deep learning become dominant.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9701403975486755}]}, {"text": "We obtained testing accuracy of 96.6% and training accuracy of 98.8% on IMDB corpus by LR with unigram+bigram+trigram feature variant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9426034688949585}, {"text": "training", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9300450086593628}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.5917654037475586}, {"text": "IMDB corpus", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9709436595439911}]}, {"text": "Similarly, for Subjectivity v1.0 and twitter corpus, the same model returns better accuracy.", "labels": [], "entities": [{"text": "twitter corpus", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8486294746398926}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9981613755226135}]}, {"text": "But on the multi-class RT movie reviews corpus, Deep learning based proposed architecture-3 of type Extended-Convolution Neural Network (E-CNN) outperforms others.", "labels": [], "entities": [{"text": "RT movie reviews", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.8968320886294047}]}], "introductionContent": [{"text": "Recently, the field of Opinion Mining and Sentiment Analysis has enticed many researchers around the globe due to its capability of delivering valuable informative applications.", "labels": [], "entities": [{"text": "Opinion Mining", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.801730215549469}, {"text": "Sentiment Analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8098151981830597}]}, {"text": "People's opinion and reviews can play a crucial role in making decision's and choosing among multiple options when those choices are related on valuable resources for example expenditure of time and money to buy products as well as services.", "labels": [], "entities": []}, {"text": "These information mostly sourced from social web through several forums, blogs and social networking websites.", "labels": [], "entities": []}, {"text": "However, Due to its heterogeneous and unstructured nature, this information is not directly machine processable.", "labels": [], "entities": []}, {"text": "Thus, it set the reason for the emergence of Opinion Mining (OM) and Sentiment Analysis (SA) as a prominent area of research.", "labels": [], "entities": [{"text": "Opinion Mining (OM)", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7949894428253174}, {"text": "Sentiment Analysis (SA)", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.8318541884422302}]}, {"text": "Both the keywords are commonly used interchangeably to denote the same meaning.", "labels": [], "entities": []}, {"text": "However, some researchers believe, both aim to solve two slightly different problems.", "labels": [], "entities": []}, {"text": "According to, OM determine whether apiece of text contains opinion or not, a problem that considered as subjectivity analysis.", "labels": [], "entities": []}, {"text": "On the other hand, SA's task is to measure the polarity of text i.e. positive or negative.", "labels": [], "entities": []}, {"text": "Polarity classification is known to be very basic task of OM and SA.", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8034045696258545}, {"text": "OM", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.7903857231140137}]}, {"text": "Polarity classification as the name signifies, classify apiece of text related to opinion on a particular issue into two sentimental opposite class.", "labels": [], "entities": []}, {"text": "Moreover, it also helps in identifying pros and cons expressions of customer reviews which make the product evaluation and customer interest assessment more credible.", "labels": [], "entities": []}, {"text": "In the present scenario, sentiment analysis and opinion mining depends on the vector extraction of apiece of text in order to represent it's most salient and important features.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9854336082935333}, {"text": "opinion mining", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.8444820940494537}]}, {"text": "These features representing a specific patterns-set help in determining the proper sentiment/opinion class.", "labels": [], "entities": []}, {"text": "Term frequency, presence and tf-idf 1 are commonly used features.", "labels": [], "entities": [{"text": "Term frequency", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9081372022628784}, {"text": "presence", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.958387553691864}]}, {"text": "In this research, we study the empirical effects related to several variants of LR, MNB, SVM on various available sentiment datasets.", "labels": [], "entities": []}, {"text": "However, these approaches are already used enormously in text categorization, their performance varies due to inherent variability in features, datasets and model used.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7696009278297424}]}, {"text": "Through a set of experiments done on many datasets, we tried to show that the better selection of variants in many cases outperform the recent published state-of-the-art.", "labels": [], "entities": []}], "datasetContent": [{"text": "This is one of the renowned corpus for statistical sentiment analysis on the collection of movie reviews prepared by Pang and Lee ().", "labels": [], "entities": [{"text": "statistical sentiment analysis", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6855961283047994}]}, {"text": "The corpus 2 was prepared in order to classify movie reviews as positive or negative that are collected from the IMDB.com (Internet Movie DataBase).", "labels": [], "entities": [{"text": "IMDB.com (Internet Movie DataBase)", "start_pos": 113, "end_pos": 147, "type": "DATASET", "confidence": 0.8581940134366354}]}, {"text": "Initially, the corpus was consisted of 2000 full length reviews, 1000 each of positive as well as negative.", "labels": [], "entities": []}, {"text": "Later, the dataset transformed to carry reviews of sentiments scaled in range.", "labels": [], "entities": []}, {"text": "Recently, a contest was hosted on (KaggleCompetitions, 2017) with a huge corpus of movie reviews taken from rotten tomattoes on 5-star rating scale.", "labels": [], "entities": [{"text": "KaggleCompetitions, 2017)", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.9039993584156036}]}, {"text": "We used this updated large corpora in this paper to seethe the difference in results of various approaches.", "labels": [], "entities": []}, {"text": "As the collected reviews are classified according to the rating system in terms of 5-star, mulit-class machine classification approaches are applied to develop a robust sentiment classifi- The dataset is freely available at www.cs.cornell.", "labels": [], "entities": []}, {"text": "edu/people/pabo/movie-review-data/ review_polarity.tar.gz cation model.", "labels": [], "entities": []}, {"text": "Another movie review dataset has been collected by Andrew Maas at Stanford, sourced from IMDB ().", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.6644802987575531}, {"text": "IMDB", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9620606303215027}]}, {"text": "The dataset consists of 50,000 reviews in total, 25,000 of each positive as well as negative sentiments, conditioned on no more than 30 reviews from one movie.", "labels": [], "entities": []}, {"text": "The reviews are distributed evenly into positive and negative classes so that the random selection will result in 50% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9983934760093689}]}, {"text": "As movie reviews in IMDB are scored from 1 to 10 scale, the selected negative reviews are considered if its score is \u2264 4 out of 10 and for the positive reviews the threshold is set to \u2265 7 out of 10.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.8980571627616882}]}, {"text": "Other reviews (neutral reviews) are not considered in this dataset.", "labels": [], "entities": []}, {"text": "This dataset originally came from crowd flowers library 4).", "labels": [], "entities": []}, {"text": "The dataset was generated through undertaking the sort of complaints received by each airline entirely by major U.S. air carrier customer service.", "labels": [], "entities": []}, {"text": "The dataset includes tens of thousands of tweets as mentioned in the table 2, their respective carriers, the positive, negative, and neutral sentiment.", "labels": [], "entities": []}, {"text": "This is a manually labelled corpus.", "labels": [], "entities": []}, {"text": "In the process of corpus generation, users were asked to manually label the tweets as positive, negative or neutral with reasons of late flight, fast service etc.", "labels": [], "entities": [{"text": "corpus generation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7347157299518585}]}, {"text": "Data pre-processing is necessary task for sentiment analysis as it performs the process of cleaning and preparing text to be suitable as input to classification models ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9795229732990265}]}, {"text": "Most of the sentiment dataset are made of the content extracted from websites e.g. Movie Reviews websites, product opinion websites, tweets from twitter etc.", "labels": [], "entities": []}, {"text": "They all contain usually lots of noise and uninformative parts such as HTML tags, advertisements and scripts which needed to be removed before sending them for the classification.", "labels": [], "entities": []}, {"text": "In order to prepare datasets for applying various machine learning approaches, we have designed a set rules for removal of the noise and uninformative parts i.e. HTML tags, rating indicators etc.", "labels": [], "entities": []}, {"text": "For all datasets, similar steps of pre-processing methods are undertaken.", "labels": [], "entities": []}, {"text": "Following steps are followed for the same: \u2022 Removing URL and getting data inside HTML Tag.", "labels": [], "entities": []}, {"text": "Recently, word embeddings become top-notch in order to avail the use of dense or continuous vectors.", "labels": [], "entities": []}, {"text": "Its main benefit arguably is that it does not require expensive annotation, instead it can be derived from large unannotated corpora that are readily available.", "labels": [], "entities": []}, {"text": "Pre-trained embeddings can then be used in downstream tasks that use small amounts of labelled data.", "labels": [], "entities": []}, {"text": "Various Transformations are therefor use of word embeddings in a sentence i.e. mean transformation, image transformation.", "labels": [], "entities": [{"text": "image transformation", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7115532755851746}]}, {"text": "If each word in a sentence will haven embeddings, its mean transformation would be the mean of all then embeddings.", "labels": [], "entities": []}, {"text": "Thus, this will give rise to the feature vector of same length as the length of sentence.", "labels": [], "entities": []}, {"text": "On the other hand, if we consider the length of embeddings n and feature vector length m, [n * m] order can be considered as a gray scale image where every element represents pixel intensity and thus it can be feed into a convolution neural network or any other machine learning model as an image.", "labels": [], "entities": []}, {"text": "Support Vector Machines are used with different kernels for classification and also in Logistic Regression; we use regularization to penalize the weights to prevent over-fitting.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.879030168056488}]}, {"text": "With reference to published results on subjective v1.0, a sentiment corpus consists of snippets, short reviews, the results presented in this paper is more accurate.", "labels": [], "entities": []}, {"text": "Moreover, it also shows, the capability of SVM is better in classification of long reviews.", "labels": [], "entities": []}, {"text": "But for the short reviews or snippets as subjective corpus, Logistic regression and Naive Bayes are more accurate and robust.", "labels": [], "entities": []}, {"text": "Addition of bi-grams improves the performance significantly as shown in.", "labels": [], "entities": []}, {"text": "After the inclusion of trigram again improve the performance a bit more.", "labels": [], "entities": []}, {"text": "Both LR and MNB with unigram, bigram and trigram features provides 96.4% and 95.2% accuracy respectively as shown in.", "labels": [], "entities": [{"text": "MNB", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.8968688249588013}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9984097480773926}]}, {"text": "For the sentiment analysis experiment on twitter corpus, a number of encoding considered to draw feature set in order to apply some supervised learning methods.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.9546889960765839}]}, {"text": "Here also, feature vectors are constructed out of various possible combination of unigrams, bigrams with individual count encoding and tf-idf encoding.", "labels": [], "entities": []}, {"text": "Out of all combination, only those are shown here which draw significantly better result.", "labels": [], "entities": []}, {"text": "The accuracy measurement is done on the set environment of 5-fold and 7-fold avg. cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994133710861206}]}, {"text": "The overall average accuracy is obtained 82.1% as shown in the through logistic regression in combination with count encoding and unigram+bigram.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9994649291038513}]}, {"text": "The Rotten Tomato Dataset is a very large movie review corpus composed of 156,060 sentences rated under 5-star rating scheme in Negative, Somewhat Negative, Neutral, Somewhat Positive, Positive categories.", "labels": [], "entities": [{"text": "Rotten Tomato Dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.6492568055788676}]}, {"text": "We divided the overall corpus into a ratio of 7:2:1 for training, test and cross-validation set.", "labels": [], "entities": []}, {"text": "The problem of sentiment analysis now turned from binary classification to multi-category classification which make it difficult for the above implemented models to be incorporated here.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.9637921154499054}, {"text": "multi-category classification", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.7322768270969391}]}, {"text": "This is the reason, the accuracy of some linear approaches such as SVM and LR start declining.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9995368719100952}]}, {"text": "Therefore, deep learning is undertaken to seethe difference.", "labels": [], "entities": []}, {"text": "Four different architectures are devised empirically which show better accuracy compare to SVM and LR as shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9994959831237793}]}, {"text": "It is clear from the above discussion that logistic regression works better on the datasets like imdb, subjectivity v1.0 and twitter where the sentiment classes are limited to two/three and the corpus is build of short statements/reviews.", "labels": [], "entities": []}, {"text": "But for the large datasets like Rotten Tomato corpus which consists of millions of texts divided into many sentiment classes, a better model is required robust enough to capture and support entire feature set necessary 119  for the classification.", "labels": [], "entities": [{"text": "Rotten Tomato corpus", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.7964793642361959}]}, {"text": "For the twitter dataset, only unigrams and bigrams with count encoding give the better results.", "labels": [], "entities": [{"text": "twitter dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8513623178005219}]}, {"text": "As it can be seen, mean transformation of embeddings does not play major role in sentiment analysis whereas image transformation of embeddings achieve the best result among all other classifiers.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.945791482925415}]}, {"text": "It is not worth denying that mean transformation is not that good for representation of embeddings as feature vector.", "labels": [], "entities": []}, {"text": "Many other transformations for embeddings are there like median, mode, tf-idf but still the combination of convolution neural network with image transformation of embeddings beats them all.", "labels": [], "entities": []}, {"text": "So embeddings are quite useful if used wisely.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Recently published Results in the literature on various versions of (Pang et al., 2002) movie  review dataset.", "labels": [], "entities": [{"text": "Pang et al., 2002) movie  review dataset", "start_pos": 79, "end_pos": 119, "type": "DATASET", "confidence": 0.7503137687842051}]}, {"text": " Table 2: Statistics of the datasets used in this paper.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy chart of various approaches on  IMDB corpus.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9882606267929077}, {"text": "IMDB corpus", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8824659883975983}]}, {"text": " Table 4: Accuracy chart of various approaches on Subjectivity v1.0 corpus.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9769944548606873}]}, {"text": " Table 5: Avg. Cross Validation Accuracy chart of various approaches on Twitter dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9015643000602722}, {"text": "Twitter dataset", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.8825638294219971}]}, {"text": " Table 6: Accuracy chart of various approaces on  Rotten Tomatoes Dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984960556030273}, {"text": "Rotten Tomatoes Dataset", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.9834422866503397}]}]}