{"title": [], "abstractContent": [{"text": "In this brief report we present an overview of the MultiLing 2017 effort and workshop , as implemented within EACL 2017.", "labels": [], "entities": []}, {"text": "MultiLing is a community-driven initiative that pushes the state-of-the-art in Automatic Summarization by providing data sets and fostering further research and development of summarization systems.", "labels": [], "entities": [{"text": "Automatic Summarization", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.8176482617855072}, {"text": "summarization", "start_pos": 176, "end_pos": 189, "type": "TASK", "confidence": 0.9721961617469788}]}, {"text": "This year the scope of the workshop was widened, bringing together researchers that work on summarization across sources, languages and genres.", "labels": [], "entities": [{"text": "summarization", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.9811866283416748}]}, {"text": "We summarize the main tasks planned and implemented this year, also providing insights on next steps.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Each participating system of the task was to compute a summary for each document in at least one of the datasets 41 languages.", "labels": [], "entities": []}, {"text": "To remove any potential bias in the evaluation of generated summaries that are too small, the human summary length in characters was provided for each test document and generated summaries were expected to be close to it.", "labels": [], "entities": [{"text": "human summary length", "start_pos": 94, "end_pos": 114, "type": "METRIC", "confidence": 0.627833882967631}]}, {"text": "The testing dataset was created using the same steps as reported in Section 2 of (Giannakopoulos et al., 2015) and excluded the articles in the training dataset (which was the testing dataset for the task in 2015).", "labels": [], "entities": []}, {"text": "For each language contains the mean character size of the summary and body of the articles selected for the test dataset.", "labels": [], "entities": []}, {"text": "Within the dataset there is no correlation between the summary and body size of the articles, in fact, the variance in the summary size is small.", "labels": [], "entities": []}, {"text": "This is likely because Wikipedia style requirements dictate that a summary beat most four paragraphs, 2 regardless of article size, and paragraphs be reasonably sized.", "labels": [], "entities": []}, {"text": "For the evaluation the baseline summary for each article in the dataset was the prefix substring of the article's body text having the same length as the human summary of the article.", "labels": [], "entities": []}, {"text": "An oracle summary was also computed for each article using the combinatorial covering algorithm in () by selecting sentences from its body text to cover the tokens in the human summary using as few sentences as possible until its size exceeded the human summary, upon which it was truncated.", "labels": [], "entities": []}, {"text": "Preprocessing of all the submitted and human summaries was performed using the Natural Language Toolkit ().", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8989639282226562}]}, {"text": "Sentence splitting was done using punkt().", "labels": [], "entities": [{"text": "Sentence splitting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.865187406539917}, {"text": "punkt", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9142486453056335}]}, {"text": "Models based on the Wikipedia data were built for each language.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.9347705245018005}]}, {"text": "For each summary the pre-processing steps were: 1.", "labels": [], "entities": []}, {"text": "all multiple white-spaces and control characters are convert to a single space 2.", "labels": [], "entities": []}, {"text": "any leading space is removed 3.", "labels": [], "entities": []}, {"text": "the resulting text string is truncated to the human summary length 4.", "labels": [], "entities": []}, {"text": "the text is tokenized and, if possible, lemmatized 5.", "labels": [], "entities": []}, {"text": "all tokens without a letter or number are discarded 6.", "labels": [], "entities": []}, {"text": "all remaining tokens are lowercased.", "labels": [], "entities": []}, {"text": "As of the time of publication of the proceedings, three teams have participated and automatic methods of scoring the subumissions, using ROUGE) and MeMoG (Gia, ), are underway and will be presented at the EACL 2017 workshop.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.9960303902626038}, {"text": "MeMoG (Gia, )", "start_pos": 148, "end_pos": 161, "type": "METRIC", "confidence": 0.8682910323143005}, {"text": "EACL 2017 workshop", "start_pos": 205, "end_pos": 223, "type": "DATASET", "confidence": 0.8179040551185608}]}, {"text": "A human evaluation will proceed afterwards.", "labels": [], "entities": []}, {"text": "The summary evaluation task revisits the multilingually applicable evaluation challenge.", "labels": [], "entities": []}, {"text": "The aim is to introduce novel, automatic evaluation methods of summary evaluation.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7942539751529694}]}, {"text": "Even though, currently, systems are evaluated using the ROUGE) and MeMoG (Gia, ) metrics, there exists a big gap between automatic methods and manual annotations, especially in non-English settings (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9351589679718018}]}, {"text": "This year's task reuses the MultiLing 2013 and 2015 single-document and multi-document summarization corpora and evalautions.", "labels": [], "entities": []}, {"text": "Furthermore, we generate summary variations (often through inducing \"noise\"), which the evaluation systems will be asked to grade.", "labels": [], "entities": []}, {"text": "These variations include: \u2022 Sentence re-ordering; \u2022 Random sentence replacement; \u2022 Merging between different summaries.", "labels": [], "entities": [{"text": "Sentence re-ordering", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.9432452917098999}, {"text": "Random sentence replacement", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6622908115386963}]}, {"text": "All the above changes will be studied, to understand the strengths and weaknesses of different evaluation methods with respect to these synthetic deviations.", "labels": [], "entities": []}, {"text": "Then, a human evaluation will be conducted to see whether humans respond similarly to the automatic methods with respect to the different noise types.", "labels": [], "entities": []}, {"text": "The aim of this task and study is to understand how variations of text change its perceived quality of a summary.", "labels": [], "entities": []}, {"text": "It also aims to highlight the (in)sufficiency of existing methods in the multilingual setting and promote new, more robust approaches for summary evaluation.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.8627415597438812}]}], "tableCaptions": []}