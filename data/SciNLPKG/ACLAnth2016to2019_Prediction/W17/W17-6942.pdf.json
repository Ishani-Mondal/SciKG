{"title": [{"text": "Exploring Soft-Clustering for German (Particle) Verbs across Frequency Ranges", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we explore the role of verb frequencies and the number of clusters in soft-clustering approaches as a tool for automatic semantic classification.", "labels": [], "entities": [{"text": "automatic semantic classification", "start_pos": 125, "end_pos": 158, "type": "TASK", "confidence": 0.6540472904841105}]}, {"text": "Relying on a large-scale setup including 4,871 base verb types and 3,173 complex verb types, and focusing on synonymy as a task-independent goal in semantic classification, we demonstrate that low-frequency German verbs are clustered significantly worse than mid-or high-frequency German verbs, and that German complex verbs are in general more difficult to cluster than German base verbs.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.731257975101471}]}], "introductionContent": [{"text": "Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language.", "labels": [], "entities": [{"text": "Semantic classifications", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7878269255161285}]}, {"text": "Such classifications have been used in applications such as word sense disambiguation, machine translation (, and information extraction), among many others.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.7300190130869547}, {"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.843348890542984}, {"text": "information extraction", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.814714640378952}]}, {"text": "Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g.,; Schulte im;;;.", "labels": [], "entities": [{"text": "automatic semantic classification", "start_pos": 200, "end_pos": 233, "type": "TASK", "confidence": 0.625289648771286}]}, {"text": "Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g.,;;;.", "labels": [], "entities": [{"text": "sensedistinguishing classification", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.7999107241630554}]}, {"text": "In general, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup.", "labels": [], "entities": []}, {"text": "This paper nevertheless addresses clustering parameters that are presumably of general importance on the meta level: Focusing on synonymy as a task-independent goal in semantic classification, we provide an extensive clustering setup to explore the role of verb frequency ranges across various numbers of clusters.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 168, "end_pos": 191, "type": "TASK", "confidence": 0.732395276427269}]}, {"text": "The contributions of this paper are two-fold: We demonstrate that (1) low-frequency German verbs are clustered significantly worse than mid-or high-frequency German verbs, and that (2) German complex verbs are in general more difficult to cluster than German base verbs.", "labels": [], "entities": []}, {"text": "While (1) the effect of clustering low-frequency target verbs has been investigated by a restricted number of earlier approaches, e.g. Schulte im;;;, (2) might be considered as general knowledge but has -as far as we are aware of-not explicitly been proven before.", "labels": [], "entities": [{"text": "clustering low-frequency target verbs", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.8598073422908783}]}, {"text": "as one of the currently largest German web corpora, we extracted all base verbs and particle verbs from version DECOW14.", "labels": [], "entities": [{"text": "DECOW14", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9274386167526245}]}, {"text": "The corpus sentences were morphologically annotated and parsed using SMOR,) and the MATE dependency parser.", "labels": [], "entities": [{"text": "SMOR", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.7871360182762146}]}, {"text": "Relying on the morphological annotation, and after disregarding prefix verbs (i.e., non-separable complex verbs), we extracted a total of 4,871 base verb types and 3,173 particle verb types.", "labels": [], "entities": []}, {"text": "As vector spaces for the verbs, we relied on word2vec () using asymmetrical window of sizes 3 and 10.", "labels": [], "entities": []}, {"text": "The underlying corpus was again DECOW14.", "labels": [], "entities": [{"text": "DECOW14", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9684613943099976}]}, {"text": "We applied a min-frequency threshold of 50, the dimensionality was set to 400, and we used 10 corpus iterations and 15 negative samples.", "labels": [], "entities": []}, {"text": "Other parameters were set to default.", "labels": [], "entities": []}, {"text": "For soft clustering, we used Non-negative matrix factorization (NMF), a factorisation approach with an inherent (soft) clustering property ().", "labels": [], "entities": []}, {"text": "NMF has been applied successfully to other NLP tasks before, such as document clustering (, topic number estimation, and preposition classification).", "labels": [], "entities": [{"text": "NMF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8708890676498413}, {"text": "document clustering", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7834610641002655}, {"text": "topic number estimation", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.71615070104599}, {"text": "preposition classification", "start_pos": 121, "end_pos": 147, "type": "TASK", "confidence": 0.7456257343292236}]}, {"text": "We applied the NMF algorithm from the LAML (Linear Algebra and Machine Learning) Java library, version 1.6.2 (Qian, 2016).", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned in the introduction, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup.", "labels": [], "entities": [{"text": "clustering", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.9656378626823425}]}, {"text": "However, we consider synonymy in cluster analyses as a meta-level goal for clustering approaches, because synonymy represents the strongest type of semantic relatedness.", "labels": [], "entities": []}, {"text": "We therefore focus on the ability of the cluster analyses to detect synonymy as a task-independent goal in semantic classification, cf. Section 3.2.1.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.8298809826374054}]}, {"text": "As a more task-specific evaluation for semantic classification we also assess the ability of the cluster analyses to predict the degree of compositionality of the particle verbs, cf. Section 3.2.2.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7271663248538971}]}, {"text": "Considering a strong compositionality of a particle verb regarding its base verb as a case of near-synonymy, the second evaluation targets a semantic relatedness between the complex and the simplex verbs that is not too different to the synonymy evaluation, yet more task-oriented.", "labels": [], "entities": []}, {"text": "We assess the cluster analyses on their ability to contain pairs of synonymous verbs in the same clusters.", "labels": [], "entities": []}, {"text": "As basis for the evaluation, we use synonyms provided by the German online synonym dictionary Duden 1 . The dictionary contained 2,158 of our particle verbs (with an average of 19 synonyms), and 3,303 of our base verbs (with an average of 13 synonyms).", "labels": [], "entities": [{"text": "German online synonym dictionary Duden 1", "start_pos": 61, "end_pos": 101, "type": "DATASET", "confidence": 0.6922609955072403}]}, {"text": "As NMF clustering provides a membership score x \u2265 0 for each verb and each cluster, we assume that the higher the membership score of a verb fora certain cluster, the more likely the verb is to be part of it.", "labels": [], "entities": [{"text": "NMF clustering", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.6588765680789948}]}, {"text": "Before running the synonym evaluation, we thus apply an inclusion threshold in order to decide for each verb whether it is considered to be in a cluster or not.", "labels": [], "entities": []}, {"text": "Since there is no maximum membership score, and since the values lie on different scales depending on the clustering parameters, determining the ideal membership threshold for each of the clusterings is not straightforward.", "labels": [], "entities": []}, {"text": "We therefore employ a brute-force solution: after finding the largest membership score t max fora specific cluster analysis, the synonym evaluation is applied to all non-negative thresholds in the sett max \u2212 k \u00b7 0.001, k \u2208 N 0 . For example, if the largest membership value in a clustering is 0.8916, the synonym evaluation is applied to all thresholds in the set {0.8916, 0.8906, 0.8896, ..., 0.0036, 0.0026, 0.0016, 0.0006}.", "labels": [], "entities": []}, {"text": "For a given threshold value, the synonym evaluation counts all verb pairs given by the clustering.", "labels": [], "entities": []}, {"text": "Two verbs are considered a pair if they share one or more clusters.", "labels": [], "entities": []}, {"text": "Since verbs are included in more clusters as the threshold is lowered, we add an abort condition: as soon as 50% of all possible verb pairs are present in the clustering, the threshold is not lowered any further.", "labels": [], "entities": []}, {"text": "See fora small-scale example, listing all symmetric verb pairs for the gold standard and the clustering, marking the correct pairs among the clustering pairs, and calculating precision, recall and f-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9996520280838013}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9992008805274963}]}, {"text": "Since the clusterings in our experiments cover thousands of verbs, the actual number of verb pairs in our clusterings is large.", "labels": [], "entities": []}, {"text": "This results in f-scores on a very low magnitude, which is not important for our evaluation, however, as the scores are used to compare clustering parameter variations, rather than providing impressive evaluation scores.", "labels": [], "entities": []}, {"text": "As an alternative to the brute-force search for the best inclusion threshold, we also apply a method for assigning verbs to their top n clusters, with 1 \u2264 n \u2264 N 2 and N representing the total number of clusters.", "labels": [], "entities": []}, {"text": "In this variant, verbs are added to then clusters with the highest membership scores.", "labels": [], "entities": []}, {"text": "For example, suppose that in a clustering of verbs into 6 clusters, verb v 1 has the membership values 0.7, 0.4, 0.45, 0.2, 0.5, and 0.8 for clusters 1 to 6 respectively.", "labels": [], "entities": []}, {"text": "For n = 1, the verb will be included only in cluster 6, for n = 2, it will be considered part of clusters 6 and 1, and for n = 3, it belongs to clusters 6, 1, and 5.", "labels": [], "entities": []}, {"text": "This variant is referred to as top-n evaluation, whereas the previously described method is referred to as threshold evaluation.", "labels": [], "entities": []}, {"text": "In this evaluation, we predict the degree of compositionality of the complex particle verbs, i.e., the degree of relatedness between the particle verbs and their corresponding base verbs (such as abnehmen -nehmen 'take over -take', and anfangen -fangen 'begin -catch').", "labels": [], "entities": []}, {"text": "We assume that if a particle verb and its base verb tend to co-occur in the same cluster within a cluster analysis, then the particle verb is semantically transparent, rather than opaque.", "labels": [], "entities": []}, {"text": "The predictions are evaluated against an existing dataset of human ratings on German particle verb compositionality).", "labels": [], "entities": [{"text": "German particle verb compositionality", "start_pos": 78, "end_pos": 115, "type": "TASK", "confidence": 0.5261103361845016}]}, {"text": "The gold standard contains a total of 400 particle verbs across 11 particle types and 3 frequency bands.", "labels": [], "entities": []}, {"text": "Similarly to the evaluation metric described in the previous section, the compositionality evaluation is also applied to all thresholds in the sett max \u2212 k \u00b7 0.001, k \u2208 N 0 , with t max being the largest inclusion value found in the clustering, as well as to all top-n cluster assignments with 1 \u2264 n \u2264 N 2 . For each pair of particle verb and base verb, e.g., abnehmen -nehmen, we then compare the assignment of the two verbs to the same vs. different clusters in two different ways.", "labels": [], "entities": []}, {"text": "\u2022 Pointwise Mutual Information (PMI): We calculate log p(P V,BV ) p(P V )p(BV ) , with p(P V, BV ) the proportion of clusters containing both the particle verb P V and the base verb BV , and p(P V ) and p(BV ) the proportions of clusters containing the particle and base verbs individually.", "labels": [], "entities": [{"text": "Pointwise Mutual Information (PMI)", "start_pos": 2, "end_pos": 36, "type": "TASK", "confidence": 0.512612427274386}]}, {"text": "The proportions are relative to the total number of clusters, so p(P V, BV ) = 0.2 means that 20% of the clusters contain both P V and BV . A high PMI means that a pair tends to occur in the same clusters rather than in different ones.", "labels": [], "entities": [{"text": "PMI", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.9691073894500732}]}, {"text": "\u2022 Cosine similarity between average cluster centroid vectors: For each cluster, we calculate the centroid vector as the average overall verb vectors in that cluster.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 2, "end_pos": 19, "type": "METRIC", "confidence": 0.8470525145530701}]}, {"text": "In addition, we calculate average cluster centroid vectors for all verbs, as the average overall centroid vectors a verb has been assigned to.", "labels": [], "entities": []}, {"text": "Then, each two verbs are compared by calculating the cosine of the angle between the respective average cluster centroid vectors.", "labels": [], "entities": []}, {"text": "A high cosine similarity means that a pair tends to occur in the same clusters, or that the clusters in which the two verbs occur have similar centroids.", "labels": [], "entities": []}, {"text": "In the final evaluation step, we compute the correlation between the PV-BV similarity predictions relying on PMI/cosine in comparison to the gold standard ratings, using Spearman's Rank-Order Correlation Coefficient \u03c1 ().", "labels": [], "entities": [{"text": "Rank-Order Correlation Coefficient \u03c1", "start_pos": 181, "end_pos": 217, "type": "METRIC", "confidence": 0.7617889493703842}]}], "tableCaptions": [{"text": " Table 1: Results across evaluations and clustering parameters (* = p\u22640.05, ** = p\u22640.01, *** = p\u22640.001).", "labels": [], "entities": []}]}