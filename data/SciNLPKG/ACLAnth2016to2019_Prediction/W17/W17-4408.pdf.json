{"title": [{"text": "A Dataset and Classifier for Recognizing Social Media English", "labels": [], "entities": [{"text": "Recognizing Social Media English", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.8946598917245865}]}], "abstractContent": [{"text": "While language identification works well on standard texts, it performs much worse on social media language, in particular di-alectal language-even for English.", "labels": [], "entities": [{"text": "language identification", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.7494648694992065}]}, {"text": "First, to support work on English language identification , we contribute anew dataset of tweets annotated for English versus non-English, with attention to ambiguity, code-switching, and automatic generation issues.", "labels": [], "entities": [{"text": "English language identification", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6159180601437887}, {"text": "automatic generation", "start_pos": 188, "end_pos": 208, "type": "TASK", "confidence": 0.6528965681791306}]}, {"text": "It is randomly sampled from all public messages, avoiding biases towards pre-existing language classifiers.", "labels": [], "entities": []}, {"text": "Second, we find that a demographic language model-which identifies messages with language similar to that used by several U.S. ethnic populations on Twitter-can be used to improve English language identification performance when combined with a traditional supervised language identifier.", "labels": [], "entities": [{"text": "English language identification", "start_pos": 180, "end_pos": 211, "type": "TASK", "confidence": 0.616985817750295}]}, {"text": "It increases recall with almost no loss of precision , including, surprisingly, for English messages written by non-U.S. authors.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9991828799247742}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.99904865026474}]}, {"text": "Our dataset and identifier ensemble are available online.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We sampled 10,502 messages from January 1, 2013 to September 11, 2016 from an archive of publicly available geotagged tweets.", "labels": [], "entities": []}, {"text": "We annotated the tweets with three mutually exclusive binary labels: English, Not English, and Ambiguous.", "labels": [], "entities": [{"text": "Ambiguous", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9813226461410522}]}, {"text": "These tweets were further annotated with descriptive labels: \u2022 Code-switched: Tweets containing both text in English and text in another language.", "labels": [], "entities": []}, {"text": "\u2022 Ambiguous due to named entities: Tweets containing only named entities, such as Vegas!, and therefore whose language could not be unambiguously determined.", "labels": [], "entities": [{"text": "Ambiguous", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9527933597564697}]}, {"text": "\u2022 Automatically generated: Tweets whose content appeared to be automatically generated, such as I just finished running 15.21 km in 1h:17m:32s with #Endomondo #endorphins https://t.co/bugbJOvJ31.", "labels": [], "entities": []}, {"text": "We excluded any usernames and URLs in a tweet from the judgment of the tweet's language, but included hashtags.", "labels": [], "entities": []}, {"text": "contain the statistics for these labels in our annotated dataset.", "labels": [], "entities": []}, {"text": "For all our experiments, we evaluate only on the subset of messages in the dataset not labeled as ambiguous or automatically generated, which we call the evaluation dataset.", "labels": [], "entities": []}, {"text": "We investigate the effect of in-domain and extra out-of-domain training data with two datasets.", "labels": [], "entities": []}, {"text": "The first is a dataset released by Twitter of 120,575 tweets uniformly sampled from all Twitter data, which were first labeled by three different classifiers (Twitter's internal algorithm, Google's Compact Language Detector 2, and langid.py), then annotated by humans where classifiers disagreed.", "labels": [], "entities": []}, {"text": "We reserve our own dataset for evaluation, but use this dataset for in-domain training.", "labels": [], "entities": []}, {"text": "This dataset is only made available by tweet ID, and many of its messages are now missing; we were able to retrieve 74,259 tweets (61.6%).", "labels": [], "entities": []}, {"text": "For the rest of this work, we call this the Twitter70 dataset (since it originally covered about 70 languages).", "labels": [], "entities": [{"text": "Twitter70 dataset", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9667904376983643}]}, {"text": "In addition, following Jaech et al., we supplemented Twitter70 with out-of-domain Wikipedia data for 41 languages, 4 sampling 10,000 sentences from each language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics for each language label;  the evaluation count refers to the subset used for  evaluation.", "labels": [], "entities": []}, {"text": " Table 2: Dataset statistics for additional labels.", "labels": [], "entities": []}, {"text": " Table 3: English classification results on not  ambiguous, not automatically generated tweets.  \"+ Demo.\" indicates including in an ensemble with  the demographics-based English classifier.", "labels": [], "entities": [{"text": "English classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6867071092128754}]}, {"text": " Table 4: Language counts for countries with  at least 100 non-ambiguous, non-automatically  generated messages (out of 129 countries to- tal), with English recall for the best-performing  langid.py model and that model in an ensemble  classifier.", "labels": [], "entities": [{"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.8978177309036255}]}, {"text": " Table 6: Percent of the messages in each bin clas- sified correctly as English or non-English by each  classifier; t is the message length for the bin.", "labels": [], "entities": []}]}