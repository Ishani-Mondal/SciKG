{"title": [{"text": "The PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions", "labels": [], "entities": [{"text": "Automatic Identification of Verbal Multiword Expressions", "start_pos": 27, "end_pos": 83, "type": "TASK", "confidence": 0.7391847372055054}]}], "abstractContent": [{"text": "Multiword expressions (MWEs) are known as a \"pain in the neck\" for NLP due to their idiosyncratic behaviour.", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6932775497436523}]}, {"text": "While some categories of MWEs have been addressed by many studies, verbal MWEs (VMWEs), such as to take a decision, to break one's heart or to turnoff, have been rarely modelled.", "labels": [], "entities": []}, {"text": "This is notably due to their syntactic variability, which hinders treating them as \"words with spaces\".", "labels": [], "entities": []}, {"text": "We describe an initiative meant to bring about substantial progress in understanding, modelling and processing VMWEs.", "labels": [], "entities": []}, {"text": "It is a joint effort, carried out within a European research network, to elaborate universal terminologies and annotation guidelines for 18 languages.", "labels": [], "entities": []}, {"text": "Its main outcome is a multilingual 5-million-word annotated corpus which underlies a shared task on automatic identification of VMWEs.", "labels": [], "entities": [{"text": "automatic identification of VMWEs", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.6467873677611351}]}, {"text": "This paper presents the corpus annotation methodology and outcome, the shared task organisation and the results of the participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword expressions (MWEs) are known to be a \"pain in the neck\" for natural language processing (NLP) due to their idiosyncratic behaviour ().", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7480450391769409}, {"text": "natural language processing (NLP)", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.7800273299217224}]}, {"text": "While some categories of MWEs have been addressed by a large number of NLP studies, verbal MWEs (VMWEs), such as to take a decision, to break one's heart or to turnoff 1 , have been relatively rarely modelled.", "labels": [], "entities": []}, {"text": "Their particularly challenging nature lies notably in the following facts: 1.", "labels": [], "entities": []}, {"text": "Their components may not be adjacent (turn it off ) and their order may vary (the decision was hard to take); 2.", "labels": [], "entities": []}, {"text": "They may have both an idiomatic and a literal reading (to take the cake); 3.", "labels": [], "entities": []}, {"text": "Their surface forms maybe syntactically ambiguous (on is a particle in the verb-particle construction take on the task and a preposition in to sit on the chair); 4.", "labels": [], "entities": []}, {"text": "VMWEs of different categories may share the same syntactic structure and lexical choices (to make a mistake is a light-verb construction, to make a meal is an idiom), 5.", "labels": [], "entities": []}, {"text": "VMWEs behave differently in different languages and are modelled according to different linguistic traditions.", "labels": [], "entities": []}, {"text": "These properties are challenging for automatic identification of VMWEs, which is a prerequisite for MWE-aware downstream applications such as parsing and machine translation.", "labels": [], "entities": [{"text": "automatic identification of VMWEs", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.5885913297533989}, {"text": "MWE-aware downstream", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.8812344074249268}, {"text": "parsing", "start_pos": 142, "end_pos": 149, "type": "TASK", "confidence": 0.9408573508262634}, {"text": "machine translation", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.7232118844985962}]}, {"text": "Namely, challenge 1 hinders the use of traditional sequence labelling approaches and calls for syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7601690590381622}]}, {"text": "Challenges 2, 3 and 4 mean that VMWE identification and categorization cannot be based on solely syntactic patterns.", "labels": [], "entities": [{"text": "VMWE identification", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.9232112765312195}]}, {"text": "Challenge 5 defies crosslanguage VMWE identification.", "labels": [], "entities": [{"text": "VMWE identification", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.6877802312374115}]}, {"text": "We present an initiative aiming at boosting VMWE identification in a highly multilingual context.", "labels": [], "entities": [{"text": "VMWE identification", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8957365155220032}]}, {"text": "It is based on a joint effort, carried on within a European research network, to elaborate universal terminologies, guidelines and methodologies for 18 languages.", "labels": [], "entities": []}, {"text": "Its main outcome is a 5-million-word corpus annotated for VMWEs in all these languages, which underlies a shared task on automatic identification of VMWEs.", "labels": [], "entities": [{"text": "automatic identification of VMWEs", "start_pos": 121, "end_pos": 154, "type": "TASK", "confidence": 0.6496663391590118}]}, {"text": "Participants of the shared task were provided with training and test corpora, and could present systems within two tracks, depending on the use of external resources.", "labels": [], "entities": []}, {"text": "They were encouraged to submit results for possibly many covered languages.", "labels": [], "entities": []}, {"text": "In this paper, we describe the state of the art in VMWE annotation and identification ( \u00a7 2).", "labels": [], "entities": [{"text": "VMWE annotation and identification", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.6297536641359329}]}, {"text": "We then present the corpus annotation methodology ( \u00a7 3) and its outcome ( \u00a7 4).", "labels": [], "entities": []}, {"text": "The shared task organization ( \u00a7 5), the measures used for system evaluation ( \u00a7 6) and the results obtained by the participating systems ( \u00a7 7) follow.", "labels": [], "entities": []}, {"text": "Finally, we discuss conclusions and future work ( \u00a7 8).", "labels": [], "entities": []}], "datasetContent": [{"text": "The quality of system predictions is measured with the standard metrics of precision (P ), recall (R) and F 1 -score (F ).", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 75, "end_pos": 89, "type": "METRIC", "confidence": 0.952080100774765}, {"text": "recall (R)", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9634627997875214}, {"text": "F 1 -score (F )", "start_pos": 106, "end_pos": 121, "type": "METRIC", "confidence": 0.9738352043288094}]}, {"text": "VMWE categories are not taken into account in system ranking, and we do not require participant systems to predict them.", "labels": [], "entities": [{"text": "VMWE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7235524654388428}]}, {"text": "Token Gold System1 System2: Toy gold corpus with 3 tokens, 2 gold VMWEs, and 3 system predictions.", "labels": [], "entities": [{"text": "Token Gold System1 System2", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.9054173231124878}, {"text": "Toy gold corpus", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.7883131901423136}]}, {"text": "VMWE codes do not include VMWE categories.", "labels": [], "entities": [{"text": "VMWE codes", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9530616104602814}]}, {"text": "Each VMWE annotation or prediction can be represented as a set of token identifiers.", "labels": [], "entities": []}, {"text": "Consider, which presents a toy gold corpus containing 2 VMWEs over 3 tokens 17 and 3 system predictions.", "labels": [], "entities": []}, {"text": "If G denotes the set of gold VMWEs and Si the set of VMWEs predicted by system i, then the following holds 18 : \u2022 G = {{t1,t2}, {t3}}, |G| = 2, ||G|| = 3.", "labels": [], "entities": []}, {"text": "\u2022 S1 = {{t1}, {t2,t3}}, |S1| = 2, ||S1|| = 3.", "labels": [], "entities": []}, {"text": "\u2022 S2 = {{t1}, {t2}, {t3}}, |S2| = 3, ||S2|| = 3.", "labels": [], "entities": []}, {"text": "\u2022 S3 = {{t1}, {t2}, {t3}, {t1,t3}}, |S3| = 4, ||S3|| = 5.", "labels": [], "entities": []}, {"text": "A simple way to obtain P , Rand F is to consider every VMWE as an indivisible instance, and calculate the ratio of the VMWEs that were correctly predicted (precision) and correctly retrieved (recall).", "labels": [], "entities": [{"text": "Rand F", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.71140256524086}, {"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.994411051273346}, {"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.9941319823265076}]}, {"text": "We call this the per-VMWE scoring.", "labels": [], "entities": []}, {"text": "The per-VMWE scoring for the sample in is calculated as follows, with T P i being the number of true positive VMWEs predicted by system i: Per-VMWE scores maybe too penalising for large VMWEs or VMWEs containing elements whose lexicalisation is uncertain (e.g. definite or indefinite articles: a, the, etc.).", "labels": [], "entities": []}, {"text": "We define, thus, an alternative per-token evaluation measure, which allows a VMWE to be partially matched.", "labels": [], "entities": []}, {"text": "Such a measure must be applicable to all VMWEs, which is difficult, given the complexity of possible scenarios allowed in the representation of VMWEs, as discussed in Section 3.", "labels": [], "entities": []}, {"text": "This complexity hinders the use of evaluation measures found in the literature.", "labels": [], "entities": []}, {"text": "For example, use a measure based on pairs of MWE tokens, which is not always possible here given singletoken VMWEs.", "labels": [], "entities": []}, {"text": "The solution we adopted considers all possible bijections between the VMWEs in the gold and system sets, and takes a matching that maximizes the number of correct token predictions (true positives, denoted below as T P i max for each system i).", "labels": [], "entities": []}, {"text": "The application of this metric to the system outcome in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9587109684944153}]}, {"text": "2 is the following: \u2022 T P 1max = |{t1,t2} \u2229 {t1}| + |{t3} \u2229 {t2,t3}| = 2 R = T P 1max/||G|| = 2/3 P = T P 1max/||S1|| = 2/3.", "labels": [], "entities": []}, {"text": "\u2022 T P 2max = |{t1,t2} \u2229 {t1}| + |{t3} \u2229 {t3}| + |\u2205 \u2229 {t2}| = 2 R = T P 2max/||G|| = 2/3 P = T P 2max/||S2|| = 2/3.", "labels": [], "entities": []}, {"text": "the sum of sizes of the elements in A.", "labels": [], "entities": []}, {"text": "Formally, let G = {g 1 , g 2 , . .", "labels": [], "entities": []}, {"text": ", g |G| } and S = {s 1 , s 2 , . .", "labels": [], "entities": []}, {"text": ", s |S| } be the ordered sets of gold and system VMWEs in a given sentence, respectively . Let B be the set of all bijections b : {1, 2, .., N } \u2192 {1, 2, .., N }, where N = max(|G|, |S|).", "labels": [], "entities": []}, {"text": "We define g i = \u2205 for i > |G|, and s i = \u2205 for i > |S|.", "labels": [], "entities": []}, {"text": "We denote by T P max the maximum number of true positives for any possible bijection (we calculate over a set of pairs, taking the intersection of each pair and then adding up the number of matched tokens overall intersections): The values of T P max are added up for all sentences in the corpus, and precision/recall values are calculated accordingly.", "labels": [], "entities": [{"text": "precision", "start_pos": 301, "end_pos": 310, "type": "METRIC", "confidence": 0.9990720748901367}, {"text": "recall", "start_pos": 311, "end_pos": 317, "type": "METRIC", "confidence": 0.9099175333976746}]}, {"text": "Let T P j max , G j , S j and N j be the values of T P max , G, Sand N for the j-th sentence.", "labels": [], "entities": []}, {"text": "For a corpus of M sentences, we define: In any of the denominators above is equal to 0 (i.e. either the corpus contains no VMWEs or the system found no VMWE occurrence) the corresponding measure is defined as equal to 0.", "labels": [], "entities": []}, {"text": "Note that these measures operate both on a micro scale (the optimal bijections are looked for within a given sentence) and a macro scale (the results are summed up for all sentences in the corpus).", "labels": [], "entities": []}, {"text": "Alternatively, micro-only measures, i.e. the average values of precision and recall for individual sentences, could be considered.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9993853569030762}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9975294470787048}]}, {"text": "Given that the density of VMWEs per sentence can vary greatly, and in many languages the majority of sentences do not contain any VMWE, we believe that the macro measures are more appropriate.", "labels": [], "entities": []}, {"text": "Note also that the measures in (2) are comparable to the CEAF-M measures () used in the coreference resolution task.", "labels": [], "entities": [{"text": "CEAF-M", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.5815833210945129}, {"text": "coreference resolution task", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.9541745583216349}]}, {"text": "There, mentions are grouped into entities (clusters) and the best bijection between gold and system entities is searched for.", "labels": [], "entities": []}, {"text": "The main difference with our approach resides in the fact that, while coreference is an equivalence relation, i.e. each mention belongs to exactly one entity, VMWEs can exhibit overlapping and nesting.", "labels": [], "entities": []}, {"text": "This specificity (as in other related tasks, e.g. named entity recognition) necessarily leads to counter-intuitive results if recall or precision are considered alone.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.7043763995170593}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9911693334579468}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9546282887458801}]}, {"text": "A system which tags all possibles subsets of the tokens of a given sentence as VMWEs will always achieve recall equal to 1, while its precision will be above 0.", "labels": [], "entities": [{"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9992555975914001}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9990242719650269}]}, {"text": "Note, however, that precision cannot be artificially increased by repeating the same annotations, since the system results (i.e. Sand s i above) are defined as sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9992098808288574}]}, {"text": "Potential overlapping and nesting of VMWEs is also the reason of the theoretical exponential complexity of in function of the length of a sentence.", "labels": [], "entities": []}, {"text": "In our shared task, the maximum number of VMWEs in a sentence, whether in a gold corpus or in a system prediction (denoted by N max = max j=1,...,M N j ), never exceeds 20.", "labels": [], "entities": []}, {"text": "The theoretical time complexity of both measures in is Most systems used techniques originally developed for parsing: LATL employed Fips, a rulebased multilingual parser; the TRANSITION system is a simplified version of a transition-based dependency parsing system; LIF employed a probabilistic transition-based dependency parser and the SZEGED system made use of the POS and dependency modules of the Bohnet parser.", "labels": [], "entities": [{"text": "LATL", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.7834585905075073}, {"text": "TRANSITION", "start_pos": 175, "end_pos": 185, "type": "METRIC", "confidence": 0.8761833906173706}]}, {"text": "The ADAPT and RACAI systems employed sequence labelling with CRFs.", "labels": [], "entities": [{"text": "ADAPT", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7353282570838928}]}, {"text": "Finally, MUMULS exploited neural networks by using the open source library TensorFlow.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: IAA scores: #S, and #T show the the  number of sentences and tokens in the corpora  used for measuring the IAA, respectively. #A 1 and  #A 2 refer to the number of VMWE instances an- notated by each of the annotators.", "labels": [], "entities": [{"text": "IAA", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.7671301960945129}]}, {"text": " Table 2: Toy gold corpus with 3 tokens, 2 gold  VMWEs, and 3 system predictions. VMWE codes  do not include VMWE categories.", "labels": [], "entities": [{"text": "Toy gold corpus", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8562624851862589}]}, {"text": " Table 4: Overview of the training corpora: number of sentences, tokens, and annotated VMWEs, fol- lowed by broken down number of annotations per VMWE category.", "labels": [], "entities": []}, {"text": " Table 5: Overview of the test corpora: number of sentences, tokens, and annotated VMWEs, followed  by broken down number of annotations per VMWE category.", "labels": [], "entities": []}, {"text": " Table 6: Length in number of tokens of VMWEs and of discontinuities in the training corpora. Columns  1-3: average and mean absolute deviation (MAD) for length, number of VMWEs with length 1 (=1).  Columns 4-10: average and MAD for the length of discontinuities, absolute and relative number of  continuous VMWEs, number of VMWEs with discontinuities of length 1, 2 and 3. Last 2 columns:  absolute and relative number of VMWEs with discontinuities of length > 3.", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9951283931732178}, {"text": "mean absolute deviation (MAD)", "start_pos": 120, "end_pos": 149, "type": "METRIC", "confidence": 0.8860424856344858}, {"text": "MAD", "start_pos": 225, "end_pos": 228, "type": "METRIC", "confidence": 0.9982650876045227}]}, {"text": " Table 7: Results for Germanic languages.", "labels": [], "entities": []}, {"text": " Table 8: Results for Balto-Slavic languages.", "labels": [], "entities": []}, {"text": " Table 9: Results for Romance languages.", "labels": [], "entities": []}, {"text": " Table 10: Results for other languages.", "labels": [], "entities": []}]}