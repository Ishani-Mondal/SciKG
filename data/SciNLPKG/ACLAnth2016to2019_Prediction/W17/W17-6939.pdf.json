{"title": [{"text": "Can You See the (Linguistic) Difference? Exploring Mass/Count Distinction in Vision", "labels": [], "entities": [{"text": "Count Distinction", "start_pos": 56, "end_pos": 73, "type": "METRIC", "confidence": 0.9242863953113556}]}], "abstractContent": [{"text": "This work explores the linguistic distinction between count and mass nouns in the visual modality.", "labels": [], "entities": []}, {"text": "Since the former class typically refers to well-defined, countable objects, with the latter proto-typically including less countable substances, we explore to which extent the linguistic distinction is grounded in the visual representations of the entities denoted by count/mass nouns.", "labels": [], "entities": []}, {"text": "Using visual features extracted from a state-of-the-art Convolutional Neural Network (CNN), we show that the entities referred to as mass exhibit a lower variance both internally (i.e. intra-image) and externally (i.e. inter-image) compared to count.", "labels": [], "entities": []}, {"text": "That is, various instances of substances are internally more homogeneous and externally more consistent to each other than are count.", "labels": [], "entities": []}, {"text": "We compare variance across various CNN layers and show that it is indicative of the categorization when low-level features of the images are used, whereas any effect disappears when experimenting with higher-level, more abstract representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "The distinction between mass and count nouns is undoubtedly one of the most investigated topics informal linguistics, at least since (see fora brief review).", "labels": [], "entities": []}, {"text": "At the simplest, descriptive level of analysis, mass nouns are usually paired with substances (e.g. water, flour, sand, etc.), cannot be inflected in plural form, and are preceded by indefinite determiners like 'some', 'much', 'a little', etc.", "labels": [], "entities": []}, {"text": "In contrast, count nouns refer to isolable, well-defined objects (e.g. bicycle, house, tree, etc.), can take plural number, and are preceded by definite determiners like 'a/an', 'every', 'each', etc.", "labels": [], "entities": []}, {"text": "Such a division is of course an oversimplification and leaves an uncertain zone in which semantic features do not map directly into morphosyntactic properties.", "labels": [], "entities": []}, {"text": "In fact, nouns denoting the same referents maybe used as mass in one language and as count in another (e.g. 'capelli' in Italian is countable, whereas 'hair' is mass).", "labels": [], "entities": []}, {"text": "Moreover, nouns denoting aggregates like 'rice' or collections of semantically related objects such as 'furniture' or 'mail' may occur in mass contexts.", "labels": [], "entities": []}, {"text": "While the status of these latter nouns is largely debated in literature (see; for very opposite positions), substances in contrast are unanimously considered mass.", "labels": [], "entities": []}, {"text": "Many theories have been proposed based either on the syntactic or the denotational aspects of the two groups (see among others;;).", "labels": [], "entities": []}, {"text": "We do not enter into this debate and focus on a rather unexplored venue.", "labels": [], "entities": []}, {"text": "We investigate whether for the most prototypical cases, namely 'objects' for count and 'substances' for mass, the linguistic mass/count distinction is reflected in the perceptual properties of the referents.", "labels": [], "entities": []}, {"text": "In support of a perceptual and conceptual, pre-linguistic difference between objects (usually denoted by count nouns in language) and substances (usually mass) area number of studies reporting the ability of children to discriminate between them by relying solely on perceptual features of the entities, without using linguistic information (for a brief review seethe introduction in).", "labels": [], "entities": []}, {"text": "To evaluate our hypothesis, we employ a computational model trained to classify objects in images.", "labels": [], "entities": []}, {"text": "We test whether mass-substance images are internally (i.e. among the various regions of the same image) more homogeneous, and externally (i.e. among the various instances of the same entity) more consistent compared to entities denoted by count nouns (see.", "labels": [], "entities": []}, {"text": "In other words, 'substances' should be distinguished from 'objects' by means of the lower variance of their visual features (somewhat similar to in a lexical entailment detection task).", "labels": [], "entities": [{"text": "lexical entailment detection task", "start_pos": 150, "end_pos": 183, "type": "TASK", "confidence": 0.8213786780834198}]}, {"text": "Though similar with respect to shape, entities denoted by count nouns are likely to be very different with respect to many other low-level visual features (surface, texture, color, etc.).", "labels": [], "entities": []}, {"text": "As a consequence, they would require higher-level operations to be recognized and classified as belonging to a particular entity class.", "labels": [], "entities": []}, {"text": "Notable works have advanced the understanding of the perceptual cues linked to material recognition (Sharan (2009);).", "labels": [], "entities": [{"text": "material recognition", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.667827382683754}]}, {"text": "To our knowledge, the present study is the first attempt to investigate the mass/count distinction in vision as linked to a linguistic perspective, namely the relationship between visual features and a grammaticalized opposition attested in language.", "labels": [], "entities": []}, {"text": "The motivation is indeed different from the group of studies taking into account the distinction between things and stuff in the computer vision community;;)., for example, recently proposed an enriched version of the popular COCO dataset () containing pixel-level annotation for stuff in addition to the source annotation for things.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 226, "end_pos": 238, "type": "DATASET", "confidence": 0.935058057308197}]}, {"text": "In this resource, however, the stuff class does not align with the mass category defined linguistically.", "labels": [], "entities": []}, {"text": "To illustrate, it contains nouns like 'mirror', 'door', 'table', 'tree', 'mountain', and 'house', which are count nouns from a linguistic perspective.", "labels": [], "entities": []}, {"text": "As a consequence, using existing resources is not feasible for our purposes.", "labels": [], "entities": []}, {"text": "To test our hypothesis, we thus collect images representing objects and substances by relying on an existing lexical resource where countability annotation is available for English nouns.", "labels": [], "entities": []}, {"text": "We then extract visual features from various layers of a pretrained state-of-the-art Convolutional Neural Network (CNN) and compute intra-image (i.e. between the various regions of an image) and inter-image (i.e. between different images depicting the same entity) variance at each layer.", "labels": [], "entities": []}, {"text": "We show that visual features extracted from images depicting mass nouns exhibit a significantly lower intra-image variance compared to those representing count nouns, when computed at the early layers of the network (encoding low-level visual features).", "labels": [], "entities": []}, {"text": "That is, mass nouns refer to simpler, more homogeneous entities compared to the more varied representations of count nouns.", "labels": [], "entities": []}, {"text": "Moreover, at the early layers mass nouns are also more consistent between instances of the same object compared to count (i.e. lower inter-image variance).", "labels": [], "entities": []}, {"text": "Consistent with our expectations, any effect disappears when experimenting with higher-level visual features extracted from the last layers of the network.", "labels": [], "entities": []}], "datasetContent": [{"text": "To obtain mass/count categorization of nouns, and more specifically categorization of their respective senses, the Bochum English Countability Lexicon (BECL) () is used.", "labels": [], "entities": [{"text": "Bochum English Countability Lexicon (BECL)", "start_pos": 115, "end_pos": 157, "type": "DATASET", "confidence": 0.7688811847141811}]}, {"text": "This resource maps synsets within WordNet) to their respective countability classes, with noun senses annotated as either mass, count, both, or neither based on a series of syntactic patterns.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9582756757736206}, {"text": "count", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.9597448110580444}]}, {"text": "The annotation occurs at the sense level, and a given noun can therefore have various senses belonging to distinct countability classes.", "labels": [], "entities": []}, {"text": "Since our intention is to approach the matter from a vision perspective, we first check how many of the labeled synsets are available within ImageNet (), with an additional requirement that the images have available bounding box annotations.", "labels": [], "entities": []}, {"text": "Among the available synsets are 36 mass, 58 both, and a remarkable 686 count.", "labels": [], "entities": []}, {"text": "1 This could well be a byproduct of the fact that count nouns are seemingly easier to annotate with bounding boxes given that they are discrete instances and are more often present in the foreground of an image.", "labels": [], "entities": []}, {"text": "For this reason it could also be argued that more pictures are taken of count objects in general, which could explain the synset availability bias within ImageNet with regard to mass/count nouns.", "labels": [], "entities": []}, {"text": "Investigation into the synsets annotated as mass reveals entities such as various sports ('soccer', 'basketball', etc.) whose corresponding images depict countable entities such as players or balls.", "labels": [], "entities": []}, {"text": "We also encounter collective nouns such as 'equipment', 'furniture', 'luggage', 'housing', and 'artwork', which are merely collections of countable objects.", "labels": [], "entities": []}, {"text": "The both category is therefore more suitable for our purposes given that the senses fit the prototypical idea of amass noun as a substance.", "labels": [], "entities": []}, {"text": "This both categorization in BECL is intuitive given that mass nouns can also be used in count contexts, i.e. 'two wines' which would refer either to two glasses (containers) of wine or perhaps to two different types of wine.", "labels": [], "entities": [{"text": "BECL", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.544658362865448}]}, {"text": "In any case, nouns contained in this class ('flour', 'sugar', 'grain', etc.) are also viable from a vision perspective given their propensity for bounding box annotations.", "labels": [], "entities": []}, {"text": "Since the both category captures mass-substance nouns, we henceforth refer to it simply as mass.", "labels": [], "entities": []}, {"text": "Of these 58 mass noun senses none are animate, and so to avoid any possible confounding effects due to animacy we also constrain the countable objects to be inanimate, choosing the 58 most frequent where frequency is a BECL metric based on the Open American National Corpus (OANC).", "labels": [], "entities": [{"text": "BECL", "start_pos": 219, "end_pos": 223, "type": "METRIC", "confidence": 0.9739618897438049}, {"text": "Open American National Corpus (OANC)", "start_pos": 244, "end_pos": 280, "type": "DATASET", "confidence": 0.801147997379303}]}, {"text": "Images for the 58 + 58 synsets are downloaded and cropped according to bounding box annotations.", "labels": [], "entities": []}, {"text": "illustrates the various steps followed to build the dataset, with descriptive statistics of the dataset reported in  To investigate the progression from the low-level, more concrete image features to the more abstract representations, we use a deep Convolutional Neural Network (CNN).", "labels": [], "entities": []}, {"text": "This state-of-the-art CNN, namely the VGG-19 model (Simonyan and Zisserman), is pretrained on ImageNet ILSVRC data (Russakovsky et al.).", "labels": [], "entities": [{"text": "VGG-19", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.8978723883628845}, {"text": "ImageNet ILSVRC data", "start_pos": 94, "end_pos": 114, "type": "DATASET", "confidence": 0.9049135049184164}]}, {"text": "VGG-19 consists of 5 blocks of convolutional layers (hence, Conv), each followed by a max pooling layer which extracts the most relevant features and hence reduces the dimensions of the feature vector.", "labels": [], "entities": [{"text": "VGG-19", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9697051644325256}]}, {"text": "After the fifth convolutional block, 3 fully-connected layers (fc) are implemented.", "labels": [], "entities": []}, {"text": "We evaluate 4 out of the 5 convolutional blocks (Conv2-Conv5) by extracting the outputs of the first and last layers for each block 2 and the output of the 3 fully-connected layers (fc6, fc7, and fc8).", "labels": [], "entities": []}, {"text": "Convolutional layers are expected to capture low-level features (e.g. edges, texture, color, etc.) while the fully-connected layers compute abstract ones (see).", "labels": [], "entities": []}, {"text": "We check at which layer the mass and count synsets significantly differ with respect to their variance.", "labels": [], "entities": []}, {"text": "Two types of variance are computed for all cropped images of a given synset: intra-vector (intraimage) variance and inter-vector (inter-image).", "labels": [], "entities": []}, {"text": "See fora toy representation of both types of variance.", "labels": [], "entities": []}, {"text": "Intra-image After extracting and storing the feature vector for an image of a given synset at a given layer of the CNN, the variance of the feature vector is computed and subsequently averaged with the variances for all other images of the synset.", "labels": [], "entities": []}, {"text": "This provides us with the mean intra-image variance, or the average variability within a single image of a given synset.", "labels": [], "entities": []}, {"text": "This constitutes a measure of the relative homogeneity of the object, and picks upon the general complexity of the corresponding noun/sense.", "labels": [], "entities": []}, {"text": "Inter-image For the second type of variance, inter-vector variance, feature vectors for all images of a given synset are first extracted and stored from a given layer of the neural network.", "labels": [], "entities": []}, {"text": "In this case, we calculate 'vertical' or column-wise variance among each individual dimension for all images of the synset, after which the dimension variances are averaged.", "labels": [], "entities": []}, {"text": "This provides us with the inter-image variance, or the variability between distinct images of the same synset, which is a measure of the relative consistency: Difference between mass/count variance through the various layers of the network in both intra-(orange) and inter-(blue) settings.", "labels": [], "entities": []}, {"text": "*** refers to a significant difference at p<.001, ** at p<.01, * at p<.05. between instances of a given entity and its corresponding noun/sense.", "labels": [], "entities": []}, {"text": "Both types of variance are computed using the original, full-size vectors as extracted from the network.", "labels": [], "entities": []}, {"text": "That is, we do not employ any dimensionality reduction technique that could cause information loss affecting the variance values.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6760962307453156}]}, {"text": "To determine whether there is a significant difference between mass and count nouns, a two-tailed t-test is performed for each type of variance and for each layer of the CNN.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptive statistics of the dataset. From left to right, (1) number of synsets, (2) number of  unique nouns among synsets, (3) average number of images per synset, (4) min, max number of images  per synset, (5) average linguistic (OANC) frequency of the noun, (6) min, max frequency of the noun.", "labels": [], "entities": [{"text": "average linguistic (OANC) frequency", "start_pos": 223, "end_pos": 258, "type": "METRIC", "confidence": 0.6454681356747946}, {"text": "max frequency", "start_pos": 281, "end_pos": 294, "type": "METRIC", "confidence": 0.9680317640304565}]}, {"text": " Table 2: Synsets with highest (top-10) and lowest (bottom-10) variance in both intra-and inter-settings.  Underlined synsets are those belonging to the lesser-represented class in a given column. The bottom-10  columns are presented starting from lowest variance and in ascending order.", "labels": [], "entities": []}]}