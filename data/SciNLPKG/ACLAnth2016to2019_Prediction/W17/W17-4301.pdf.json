{"title": [{"text": "Dependency Parsing with Dilated Iterated Graph CNNs", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8147146999835968}]}], "abstractContent": [{"text": "Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale.", "labels": [], "entities": [{"text": "Dependency parses", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8093178570270538}]}, {"text": "Recent advances in GPU hardware have enabled neural networks to achieve significant gains over the previous best models, these models still fail to leverage GPUs' capability for massive parallelism due to their requirement of sequential processing of the sentence.", "labels": [], "entities": []}, {"text": "In response, we propose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for graph-based dependency parsing, a graph con-volutional architecture that allows for efficient end-to-end GPU parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7161918878555298}, {"text": "GPU parsing", "start_pos": 197, "end_pos": 208, "type": "TASK", "confidence": 0.6531078815460205}]}, {"text": "In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best neural network parsers.", "labels": [], "entities": [{"text": "English Penn TreeBank benchmark", "start_pos": 22, "end_pos": 53, "type": "DATASET", "confidence": 0.9096364974975586}]}], "introductionContent": [{"text": "By vastly accelerating and parallelizing the core numeric operations for performing inference and computing gradients in neural networks, recent developments in GPU hardware have facilitated the emergence of deep neural networks as state-ofthe-art models for many NLP tasks, such as syntactic dependency parsing.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 283, "end_pos": 311, "type": "TASK", "confidence": 0.7032110691070557}]}, {"text": "The best neural dependency parsers generally consist of two stages: First, they employ a recurrent neural network such as a bidirectional LSTM to encode each token in context; next, they compose these token representations into a parse tree.", "labels": [], "entities": []}, {"text": "Transition based dependency parsers) produce a well-formed tree by predicting and executing a series of shiftreduce actions, whereas graph-based parsers (Mc- Darker cell indicates more layers include that cell's representation.", "labels": [], "entities": []}, {"text": "Heads and labels corresponding to gold tree are indicated.", "labels": [], "entities": []}, {"text": "generally employ attention to produce marginals over each possible edge in the graph, followed by a dynamic programming algorithm to find the most likely tree given those marginals.", "labels": [], "entities": []}, {"text": "Because of their dependency on sequential processing of the sentence, none of these architectures fully exploit the massive parallel processing capability that GPUs possess.", "labels": [], "entities": []}, {"text": "If we wish to maximize GPU resources, graph-based dependency parsers are more desirable than their transitionbased counterparts since attention over the edgefactored graph can be parallelized across the entire sentence, unlike the transition-based parser which must sequentially predict and perform each transition.", "labels": [], "entities": []}, {"text": "By encoding token-level representations with 1 an Iterated Dilated CNN (ID-CNN) (, we can also remove the sequential dependencies of the RNN layers.", "labels": [], "entities": []}, {"text": "Unlike who use 1-dimensional convolutions over the sentence to produce token representations, our network employs 2-dimensional convolutions over the adjacency matrix of the sentence's parse tree, modeling attention from the bottom up.", "labels": [], "entities": []}, {"text": "By training with an objective that encourages our model to predict trees using only simple matrix operations, we additionally remove the additional computational cost of dynamic programming inference.", "labels": [], "entities": []}, {"text": "Combining all of these ideas, we present Dilated Iterated Graph CNNs (DIG-CNNs): a combined convolutional neural network architecture and training objective for efficient, end-to-end GPU graph-based dependency parsing.", "labels": [], "entities": [{"text": "GPU graph-based dependency parsing", "start_pos": 183, "end_pos": 217, "type": "TASK", "confidence": 0.5532007217407227}]}, {"text": "We demonstrate the efficacy of these models in experiments on English Penn TreeBank, in which our models perform similarly to the state-of-theart.", "labels": [], "entities": [{"text": "English Penn TreeBank", "start_pos": 62, "end_pos": 83, "type": "DATASET", "confidence": 0.9420503179232279}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Labeled and unlabeled attachment scores  of our model compared to state-of-the-art graph- based parsers", "labels": [], "entities": []}]}