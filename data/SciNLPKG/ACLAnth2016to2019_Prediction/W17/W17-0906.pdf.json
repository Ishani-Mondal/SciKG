{"title": [{"text": "LSDSem 2017 Shared Task: The Story Cloze Test", "labels": [], "entities": [{"text": "LSDSem 2017 Shared Task", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.755984827876091}, {"text": "Story Cloze Test", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.8963403105735779}]}], "abstractContent": [{"text": "The LSDSem'17 shared task is the Story Cloze Test, anew evaluation for story understanding and script learning.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.68573397397995}, {"text": "story understanding", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8187013864517212}, {"text": "script learning", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.854203462600708}]}, {"text": "This test provides a system with a four-sentence story and two possible endings, and the system must choose the correct ending to the story.", "labels": [], "entities": []}, {"text": "Successful narrative understanding (getting closer to human performance of 100%) requires systems to link various levels of semantics to commonsense knowledge.", "labels": [], "entities": [{"text": "narrative understanding", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7004120647907257}]}, {"text": "A total of eight systems participated in the shared task, with a variety of approaches including end-to-end neural networks, feature-based regression models , and rule-based methods.", "labels": [], "entities": []}, {"text": "The highest performing system achieves an accuracy of 75.2%, a substantial improvement over the previous state-of-the-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9997265934944153}]}], "introductionContent": [{"text": "Building systems that can understand stories or can compose meaningful stories has been a longstanding ambition in natural language understanding).", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.666620671749115}]}, {"text": "Perhaps the biggest challenge of story understanding is having commonsense knowledge for comprehending the underlying narrative structure.", "labels": [], "entities": [{"text": "story understanding", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8144639134407043}]}, {"text": "However, rich semantic modeling of the text's content involving words, sentences, and even discourse is crucially important.", "labels": [], "entities": []}, {"text": "The workshop on Linking Lexical, Sentential and Discourse-level Semantics (LSDSem) is committed to encouraging computational models and techniques which involve multiple levels of semantics.", "labels": [], "entities": [{"text": "Linking Lexical, Sentential and Discourse-level Semantics (LSDSem)", "start_pos": 16, "end_pos": 82, "type": "TASK", "confidence": 0.712690782546997}]}, {"text": "1 http://www.coli.uni-saarland.de/ \u02dc mroth/LSDSem/ The LSDSem'17 shared task is the Story Cloze Test (SCT;.", "labels": [], "entities": []}, {"text": "The SCT is one of the recent proposed frameworks on evaluating story comprehension and script learning.", "labels": [], "entities": [{"text": "script learning", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.864666223526001}]}, {"text": "In this test, the system reads a four-sentence story along with two alternative endings.", "labels": [], "entities": []}, {"text": "It is then tasked with choosing the correct ending.", "labels": [], "entities": []}, {"text": "summarize the outcome of experiments conducted using several models including the state-of-the-art script learning approaches.", "labels": [], "entities": []}, {"text": "They suggest that current methods are only slightly better than random performance and more powerful models will require richer modeling of the semantic space of stories.", "labels": [], "entities": []}, {"text": "Given the wide gap between human (100%) and state-of-the-art system (58.5%) performance, the time was ripe to hold the first shared task on SCT.", "labels": [], "entities": [{"text": "SCT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.8656469583511353}]}, {"text": "In this paper, we present a summary on the first organized shared task on SCT with eight participating systems.", "labels": [], "entities": [{"text": "SCT", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9392102360725403}]}, {"text": "The submitted approaches to this non-blind challenge ranged from simple rulebased methods, to linear classifiers and end-toend neural models, to hybrid models that leverage a variety of features on different levels of linguistic analysis.", "labels": [], "entities": []}, {"text": "The highest performing system achieves an accuracy of 75.2%, which substantially improves the previously established state-ofthe-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9997088313102722}]}, {"text": "We hope that our findings and discussions can help reshape upcoming evaluations and shared tasks involving story understanding.", "labels": [], "entities": [{"text": "story understanding", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7213095277547836}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: The size of the provided shared task  datasets.", "labels": [], "entities": []}, {"text": " Table 4: Overview of models and resources used by the participating teams. For each team only their  best performing system on the Spring 2016 Test Set is included, as submitted to CodaLab. Please refer  to the system description papers for a list of other models. Human is reported to perform at 100%.", "labels": [], "entities": [{"text": "Spring 2016 Test Set", "start_pos": 132, "end_pos": 152, "type": "DATASET", "confidence": 0.73389732837677}]}]}