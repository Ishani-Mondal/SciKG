{"title": [], "abstractContent": [{"text": "This paper describes the University of Edinburgh's submissions to the WMT17 shared news translation and biomedical translation tasks.", "labels": [], "entities": [{"text": "WMT17 shared news translation and biomedical translation tasks", "start_pos": 70, "end_pos": 132, "type": "TASK", "confidence": 0.6884393244981766}]}, {"text": "We participated in 12 translation directions for news, translating between English and Czech, German, Lat-vian, Russian, Turkish and Chinese.", "labels": [], "entities": []}, {"text": "For the biomedical task we submitted systems for English to Czech, German, Polish and Romanian.", "labels": [], "entities": []}, {"text": "Our systems are neural machine translation systems trained with Ne-matus, an attentional encoder-decoder.", "labels": [], "entities": []}, {"text": "We follow our setup from last year and build BPE-based models with parallel and back-translated monolingual training data.", "labels": [], "entities": []}, {"text": "Novelties this year include the use of deep ar-chitectures, layer normalization, and more compact models due to weight tying and improvements in BPE segmentations.", "labels": [], "entities": [{"text": "layer normalization", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.6676245480775833}, {"text": "BPE segmentations", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.7156454920768738}]}, {"text": "We perform extensive ablative experiments, reporting on the effectivenes of layer nor-malization, deep architectures, and different ensembling techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "We participated in the WMT17 shared news translation task for 12 translation directions, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese, and in the WMT17 shared biomedical translation task for English to Czech, German, Polish and Romanian.", "labels": [], "entities": [{"text": "WMT17 shared news translation task", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.8335278272628784}, {"text": "WMT17 shared biomedical translation task", "start_pos": 186, "end_pos": 226, "type": "TASK", "confidence": 0.7890489935874939}]}, {"text": "We submitted neural machine translation systems trained with Nematus (.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6775074601173401}]}, {"text": "Our setup is based on techniques described in last year's system description, including the use of subword models), back-translated monolingual data,, and re-ranking with right-toleft models.", "labels": [], "entities": []}, {"text": "This year, we experimented with deep network architectures, new ways to include monolingual data, and different ensembling variants.", "labels": [], "entities": []}, {"text": "Other novelties include obtaining more compact models via better BPE segmentation and by weight tying, and speeding up model training with layer normalization ( and adam (.", "labels": [], "entities": [{"text": "BPE segmentation", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.786931961774826}, {"text": "weight tying", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.6812135726213455}]}, {"text": "We perform extensive ablative experiments across language pairs to evaluate the effectiveness of each of these approaches.", "labels": [], "entities": []}, {"text": "When comparing this year's baseline models to our best results, we show consistent increases in scores of 2.2-5 BLEU for our 12 news task language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9969213008880615}]}, {"text": "Among the constrained submissions to the news task, our submissions are ranked tied first for 11 out of the 12 translation directions in which we participated.", "labels": [], "entities": []}, {"text": "For the biomedical task, we obtained the highest BLEU for all our submitted systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9992573857307434}]}, {"text": "For the 6 language pairs for which we participated both in WMT16 and WMT17, we also show the scores of last year's systems.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9526830911636353}, {"text": "WMT17", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9140498042106628}]}, {"text": "We observe solid improvements with increases of 1.5-3 BLEU for single models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9989317059516907}]}, {"text": "Some of these improvements are due to differences in training data, preprocessing and hyperparameters, but most of the increase is due to layer normalization and deeper models.", "labels": [], "entities": []}, {"text": "It is worth mentioning that our deeper models were trained on single GPUs, showing that the benefits of deeper models can be harnessed with limited hardware resources.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU scores for EN\u2194TR when adding  copied monolingual data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993849992752075}, {"text": "TR", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.4553263783454895}]}, {"text": " Table 2: BLEU scores for translating news into English (WMT 2016 and 2017 test sets -WMT 2017 dev  set is used where there was no 2016 test)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988346695899963}, {"text": "translating news", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9242909252643585}, {"text": "WMT 2016 and 2017 test sets", "start_pos": 57, "end_pos": 84, "type": "DATASET", "confidence": 0.9280500710010529}, {"text": "WMT 2017 dev  set", "start_pos": 86, "end_pos": 103, "type": "DATASET", "confidence": 0.8659796267747879}]}, {"text": " Table 3: BLEU scores for translating news out of English (WMT 2016 and 2017 test sets -WMT 2017  dev set is used where there was no 2016 test)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988433122634888}, {"text": "translating news out of English", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.9005541920661926}, {"text": "WMT 2016 and 2017 test sets", "start_pos": 59, "end_pos": 86, "type": "DATASET", "confidence": 0.9206523299217224}, {"text": "WMT 2017  dev set", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.8510585576295853}]}, {"text": " Table 4: Contrastive experiments for biomedical task. Submitted system marked in bold.", "labels": [], "entities": []}]}