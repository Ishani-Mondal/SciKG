{"title": [{"text": "Wordnet extension via word embeddings: Experiments on the Norwegian Wordnet", "labels": [], "entities": [{"text": "Norwegian", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9424798488616943}, {"text": "Wordnet", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.8375023603439331}]}], "abstractContent": [{"text": "This paper describes the process of automatically adding synsets and hypernymy relations to an existing wordnet based on word embeddings computed for POS-tagged lemmas in a large news corpus, achieving exact match attachment accuracy of over 80%.", "labels": [], "entities": [{"text": "exact match attachment accuracy", "start_pos": 202, "end_pos": 233, "type": "METRIC", "confidence": 0.8042965084314346}]}, {"text": "The reported experiments are based on the Norwegian Word-net, but the method is language independent and also applicable to other wordnets.", "labels": [], "entities": [{"text": "Norwegian Word-net", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.8140920400619507}]}, {"text": "Moreover, this study also represents the first documented experiments of the Nor-wegian Wordnet.", "labels": [], "entities": [{"text": "Nor-wegian Wordnet", "start_pos": 77, "end_pos": 95, "type": "DATASET", "confidence": 0.8201134502887726}]}], "introductionContent": [{"text": "This paper documents experiments with an unsupervised method for extending a wordnet with new words and automatically identifying the appropriate hypernymy relations.", "labels": [], "entities": []}, {"text": "Using word embeddings trained on a large corpus of news text (~330 million tokens), candidate hypernyms fora given target word are identified by computing nearest neighbors lists towards the wordnet and retrieving the ancestors of the neighbors.", "labels": [], "entities": []}, {"text": "The candidate hypernyms are then scored according to a combination of distributional similarity and distance in the wordnet graph.", "labels": [], "entities": []}, {"text": "While the particular experimental results reported here are for the Norwegian Wordnet (NWN), and using vectors estimated on POS tagged lemmas of the Norwegian news corpus, the methodology is generally applicable and not specific to neither the language nor the particular language resources used.", "labels": [], "entities": [{"text": "Norwegian Wordnet (NWN)", "start_pos": 68, "end_pos": 91, "type": "DATASET", "confidence": 0.9336756229400635}, {"text": "Norwegian news corpus", "start_pos": 149, "end_pos": 170, "type": "DATASET", "confidence": 0.7665499647458395}]}], "datasetContent": [{"text": "There are several ways one could choose to evaluate the quality of the words that are automatically inserted into the hierarchy.", "labels": [], "entities": []}, {"text": "For example, chose to manually evaluate a random sample of 200 unseen words, while treat the words already encoded in the hierarchy as gold data and then try to reattach these.", "labels": [], "entities": []}, {"text": "We here follow the latter approach.", "labels": [], "entities": []}, {"text": "However, while Jurgens and Pilehvar (2015) restrict their evaluation to monosemous words, we also include polysemous words in order to make the evaluation more realistic.", "labels": [], "entities": []}, {"text": "For evaluation and tuning we split the wordnet into a development set and a test set, with 1388 target words in each.", "labels": [], "entities": []}, {"text": "Potential targets only comprise words that have a hypernym encoded (which, in fact, are not that many, as NWN is relatively flat) and occur in the news corpus sufficiently often (\u2265 5) to be represented by a word embedding.", "labels": [], "entities": []}, {"text": "We evaluate hypernym selection according to both accuracy and attachment.", "labels": [], "entities": [{"text": "hypernym selection", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7929778695106506}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9995379447937012}, {"text": "attachment", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9888280630111694}]}, {"text": "While accuracy reflects the percentage of target words added that were correctly placed under the right hypernym, the attachment score is the percentage of target words that actually were inserted into NWN.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9995660185813904}, {"text": "attachment score", "start_pos": 118, "end_pos": 134, "type": "METRIC", "confidence": 0.9565511345863342}, {"text": "NWN", "start_pos": 202, "end_pos": 205, "type": "DATASET", "confidence": 0.9332680702209473}]}, {"text": "A candidate target word might end up not getting attached if it has no neighbors fulfilling the requirements described in Section 5.1.", "labels": [], "entities": []}, {"text": "Computing accuracy based only on exactly correct insertions is rather strict.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989110231399536}]}, {"text": "Intuitively, a hypernymy relation can be right or wrong with varying degrees.", "labels": [], "entities": []}, {"text": "We therefore also include a soft accuracy measure that aims to take account of this by counting how many hyponym or hypernym edges that separates a lemma from its correct position.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9254979491233826}]}, {"text": "Each edge will weight the count by a factor of 0.5, partly based on the accuracy measure of, who, instead of weighting the score, only measures accuracy as the number of edges away that a lemma is placed from its original position.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9993019104003906}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9989099502563477}]}, {"text": "We defined the formula for soft accuracy as:  The parameters that need to be empirically tuned are: the depth penalty d, the number of k nearest: Accuracy restricted to target words with a frequency higher than some given threshold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9736356139183044}, {"text": "depth penalty d", "start_pos": 104, "end_pos": 119, "type": "METRIC", "confidence": 0.9409534533818563}]}, {"text": "#Words shows the number of attached words.", "labels": [], "entities": []}, {"text": "neighbors to consider, and the minimum threshold m for the similarity of neighbors towards the target.", "labels": [], "entities": []}, {"text": "After an initial round of experiments that determined the approximate appropriate range of values for these parameters, we performed an exhaustive grid search for the best parameter combination among the following values: Optimizing for attachment accuracy, the best parameter configuration after tuning on the development set was found to be k=6, m=0.5, and d=0.05, yielding an accuracy of 55.80% and a degree of attachment of 96.33%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9736437797546387}, {"text": "accuracy", "start_pos": 379, "end_pos": 387, "type": "METRIC", "confidence": 0.9993484616279602}, {"text": "attachment", "start_pos": 414, "end_pos": 424, "type": "METRIC", "confidence": 0.5645847916603088}]}, {"text": "As one might expect that the embeddings are more reliable for high-frequent words than for low-frequent words, we also computed the dev-set accuracy relative to frequency of occurrence in the corpus used for estimating the embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9648077487945557}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We indeed see that the accuracy goes up when enforcing a higher frequency cutoff, reaching 64.47% when setting the cutoff to 1000, though per definition this means sacrificing coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9997014403343201}]}, {"text": "We also evaluated the effect of only inserting words that had hypernyms with a score higher than a given cutoff, which again naturally leads to a lower degree of attachment.", "labels": [], "entities": []}, {"text": "shows the accuracies over the development set when enforcing different cutoffs, showing an increased accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9988800883293152}]}, {"text": "We see that the best performance is when the cutoff on the hypernym score is set to 4.6, with a corresponding attachment accuracy of 83.26%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.6130163669586182}]}, {"text": "Held-out results Applying the model configuration (without cutoffs) to the held-out test words of NWN yields an attachment of 95.97% and an accuracy of 59.91% (soft acc. = 66.04%).", "labels": [], "entities": [{"text": "NWN", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8890623450279236}, {"text": "attachment", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9951156377792358}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.999584972858429}, {"text": "soft acc.", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.6697768568992615}]}, {"text": "We see that there is a slight increase in the accuracy: Accuracy restricted to hypernyms with a score higher than some given threshold, computed over the 1388 words in the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996751546859741}, {"text": "Accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9980897307395935}]}, {"text": "#Words shows the number of attached words, e.g. 1337 is 96.33% of 1388.", "labels": [], "entities": []}, {"text": "for the insertions performed with the word embeddings when moving from the development data to the held-out data.", "labels": [], "entities": []}, {"text": "As a baseline approach we also tried attaching each target word to the hypernym of its 1-nearest-neighbor.", "labels": [], "entities": []}, {"text": "(When there are several candidate hypernyms available, we simply pick the first candidate in the retrieved list.)", "labels": [], "entities": []}, {"text": "Yielding an accuracy of 47.61%, it is clear that we improve substantially over the baseline when instead performing insertion using the scoring function.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996769428253174}]}, {"text": "Applying the scoring function to the test set using the cutoff with the highest accuracy from, yields an accuracy of 84.96% (soft = 90.38%), though at the cost of a lower attachment rate (16.28%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9976189732551575}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9996517896652222}, {"text": "attachment rate", "start_pos": 171, "end_pos": 186, "type": "METRIC", "confidence": 0.977545440196991}]}], "tableCaptions": [{"text": " Table 1: Number of lemmas, synsets, senses and  monosemous/polysemous words for nouns, verbs  and adjectives in NWN.", "labels": [], "entities": [{"text": "NWN", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9083115458488464}]}, {"text": " Table 2: Accuracy restricted to target words with  a frequency higher than some given threshold.  #Words shows the number of attached words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987145662307739}]}, {"text": " Table 3: Accuracy restricted to hypernyms with  a score higher than some given threshold, com- puted over the 1388 words in the development set.  #Words shows the number of attached words, e.g.  1337 is 96.33% of 1388.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9979089498519897}]}]}