{"title": [{"text": "Sub-character Neural Language Modelling in Japanese", "labels": [], "entities": [{"text": "Neural Language Modelling", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6190217435359955}]}], "abstractContent": [{"text": "In East Asian languages such as Japanese and Chinese, the semantics of a character are (somewhat) reflected in its sub-character elements.", "labels": [], "entities": []}, {"text": "This paper examines the effect of using sub-characters for language modeling in Japanese.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7127223312854767}]}, {"text": "This is achieved by decomposing characters according to a range of character decomposition datasets, and training a neural language model over variously decomposed character representations.", "labels": [], "entities": []}, {"text": "Our results indicate that language modelling can be improved through the inclusion of sub-characters, though this result depends on a good choice of decomposition dataset and the appropriate granular-ity of decomposition.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.6959301084280014}]}], "introductionContent": [{"text": "The Japanese language makes use of Chinese-derived ideographs (\"kanji\") which contain sub-character elements (\"bushu\") that to varying degrees reflect the semantics of the character.", "labels": [], "entities": []}, {"text": "For example, the character \u9be8 (kujira \"whale\") consists of two sub-characters: \u2fc2 (sakana \"fish\") and \u4eac (kyou \"capital city\").", "labels": [], "entities": []}, {"text": "Similarly, the character \u9b83 (hirame \"flounder\") consists of the sub-characters \u2fc2 (sakana \"fish\") and \u5e73 (hira \"something broad and flat\").", "labels": [], "entities": []}, {"text": "Here, the sub-character \u2fc2 (sakana \"fish\") is a semantically significant element which appears in characters relating to marine life.", "labels": [], "entities": []}, {"text": "Current Japanese language models do not capture sub-character information, and hence lack the ability to capture such generalisations.", "labels": [], "entities": []}, {"text": "A key limitation of word-based language modelling is the tendency to produce poor estimations for rare or OOV (out-of-vocabulary) words, and character-based language models have been shown to solve some of the sparsity problem in English by modeling how words are constructed.", "labels": [], "entities": []}, {"text": "We take inspiration from this work, but observe for Japanese that since the kanji portion of the Japanese writing system contains thousands rather than dozens of characters, a characterbased language model will still be susceptible to sparsity.", "labels": [], "entities": []}, {"text": "Given that a large number of Japanese characters can be decomposed into sub-characters, we examine the question of whether sub-character language models can achieve similar gains in language model quality to character language models in English.", "labels": [], "entities": []}, {"text": "In this paper we train sub-character language models for Japanese based on decompositions available in several existing kanji datasets.", "labels": [], "entities": []}, {"text": "Our results suggest that decomposing characters is of value, but that the results are sensitive to the nature and granularity of the decomposition.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to investigate the usefulness of subcharacter decomposition for language models, we need someway of deriving these bushu from kanji.", "labels": [], "entities": []}, {"text": "Here, we consider four kanji datasets that provide decompositions for kanji characters: GlyphWiki, IDS, KanjiVG, and KRADFILE.", "labels": [], "entities": []}, {"text": "An example kanji decomposition under the four datasets is provided in Figure 1.", "labels": [], "entities": [{"text": "kanji decomposition", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.9057987630367279}]}, {"text": "Unique bushu describes the number of characters that have been used as bushu.", "labels": [], "entities": []}, {"text": "In general, if a kanji character is found in the decomposition of another kanji character, then it is counted as a bushu.", "labels": [], "entities": []}, {"text": "While most of the datasets use thousands of bushu, KRADFILE is notable in that it uses a much smaller bushu set.", "labels": [], "entities": [{"text": "KRADFILE", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.6957089304924011}]}, {"text": "With respect to Average bushu per kanji, there is strong similarity between GlyphWiki, IDS and KanjiVG, but KRADFILE produces almost double the number of bushu because the decompositions are exhaustive.", "labels": [], "entities": [{"text": "KRADFILE", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8466366529464722}]}, {"text": "Average branching factor describes the average number of bushu found through exhaustively decomposing over every kanji and its bushu (an example of exhaustive decomposition -which we call \"deep decomposition\" -can be seen in).", "labels": [], "entities": []}, {"text": "Because KRAD-FILE provides a single layer of decomposition that is complete and indivisible, we cannot decompose each bushu any further.", "labels": [], "entities": []}, {"text": "Therefore, KRADFILE has an average depth of 1.", "labels": [], "entities": [{"text": "KRADFILE", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.4660261571407318}]}, {"text": "For our experiments, we use version 1.5 of the NAIST text corpus (, a collection of Japanese newspaper articles which is widely used in Japanese NLP research ().", "labels": [], "entities": [{"text": "NAIST text corpus", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9772223234176636}]}, {"text": "The corpus consists of roughly 1.7 million character tokens, of which roughly 42% are kanji.", "labels": [], "entities": []}, {"text": "To build and test our models we use 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "Our language models are standard neural network models, implemented in Tensorflow; they consist of a embedding layer, with embeddings for each character (including kanji, bushu, and other elements of the Japanese writing systems) which are learned during training, a standard unidirectional LSTM (, and a layer which maps the output of the LSTM to a vector representing the probability of the next character; the hidden (embedding) size of the LSTM for our experiments is 128.", "labels": [], "entities": []}, {"text": "We train the language model by minimizing the cross-entropy between the output probabilities and the onehot vector corresponding to the correct answer, using the Adam optimizer with a batch size of 128 and a learning rate of 0.002.", "labels": [], "entities": []}, {"text": "In addition to the four kanji datasets, we consider two kinds of decomposition: shallow and deep.", "labels": [], "entities": []}, {"text": "Shallow decomposition refers to using only the first layer of decomposition of a kanji character, whereas deep expansion refers to an exhaustive decomposition of the kanji and all of its bushu.", "labels": [], "entities": []}, {"text": "We use these two methods to explore whether semantic information is reflected in deeper levels of decomposition.", "labels": [], "entities": []}, {"text": "In general, we aim to compare the performance of a language model based on the way kanji are decomposed and the depth of their decompositions.", "labels": [], "entities": []}, {"text": "includes examples of both shallow and deep decompositions, for kanji including \u8cc2 (mainai \"bribe\") from.", "labels": [], "entities": []}, {"text": "Decomposition is done in a left-to-right in-order traversal.", "labels": [], "entities": []}, {"text": "It is possible for multiple kanji to share the same bushu.", "labels": [], "entities": []}, {"text": "For example, according to KanjiVG, the characters \u7531 (yoshi \"cause/ reason\"), \u7532 (kou \"carapace/shell\"), and \u7533 (saru \"monkey\") are decomposed into \u2f65 (ta: Language model perplexity based on the different decompositions \"rice field\") and \u2f01 (tatebou \"vertical line\").", "labels": [], "entities": [{"text": "KanjiVG", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.8716439604759216}]}, {"text": "Because of this, using just the decomposition of a character can actually lead to a loss of information.", "labels": [], "entities": []}, {"text": "Thus, we postpend all decomposition sequences with the original kanji character to preserve the mapping of bushu to its original kanji.", "labels": [], "entities": []}, {"text": "We evaluate based on perplexity, normalizing the product of the probability of the (sub-)characters in our test set by the character length of the corpus (lower perplexity is better).", "labels": [], "entities": []}, {"text": "Because decomposing characters affects the superficial length of the corpus, however, we note that in the cases of decomposition we are normalizing using the original (undecomposed) corpus length in all cases, and not the decomposed token length.", "labels": [], "entities": []}, {"text": "This reflects the fact that by adding decompositions of a character we are not really adding new text to the corpus.", "labels": [], "entities": []}, {"text": "Both regular and decomposed language models in fact predict exactly the original contents of the corpus, but for the decomposed models the uncertainty associated with each kanji is distributed among the predictions of its bushu (and the postpended kanji), and can be retrieved simply by multiplying all the individual probabilities together.", "labels": [], "entities": []}, {"text": "shows the perplexity for each of the sub-character language models, for the two possible decomposition depths, as compared to a baseline where no decomposition occurs.", "labels": [], "entities": []}, {"text": "We report perplexity for each fold of our 5-fold cross-validation, as well as the average.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the four kanji datasets", "labels": [], "entities": []}, {"text": " Table 2: Language model perplexity based on the different decompositions", "labels": [], "entities": []}]}