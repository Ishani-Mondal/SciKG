{"title": [{"text": "Real-valued Syntactic Word Vectors (RSV) for Greedy Neural Dependency Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We show that a set of real-valued word vectors formed by right singular vectors of a transformed co-occurrence matrix are meaningful for determining different types of dependency relations between words.", "labels": [], "entities": []}, {"text": "Our experimental results on the task of dependency parsing confirm the superiority of the word vectors to the other sets of word vectors generated by popular methods of word embedding.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.8279733061790466}]}, {"text": "We also study the effect of using these vectors on the accuracy of dependency parsing in different languages versus using more complex parsing architectures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9989179372787476}, {"text": "dependency parsing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7499717772006989}]}], "introductionContent": [{"text": "Greedy transition-based dependency parsing is appealing thanks to its efficiency, deriving a parse tree fora sentence in linear time using a discriminative classifier.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 7, "end_pos": 42, "type": "TASK", "confidence": 0.6903263926506042}]}, {"text": "Among different methods of classification used in a greedy dependency parser, neural network models capable of using real-valued vector representations of words, called word vectors, have shown significant improvements in both accuracy and speed of parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9992073178291321}]}, {"text": "It was first proposed by to use word vectors in a 3-layered feed-forward neural network as the core classifier in a transition-based dependency parser.", "labels": [], "entities": []}, {"text": "The classifier is trained by the standard back-propagation algorithm.", "labels": [], "entities": []}, {"text": "Using a limited number of features defined over a certain number of elements in a parser configuration, they could build an efficient and accurate parser, called the Stanford neural dependency parser.", "labels": [], "entities": []}, {"text": "This architecture then was extended by and.) adds a search-based oracle and a set of morphological features to the original architecture in order to make it capable of parsing the corpus of universal dependencies.", "labels": [], "entities": []}, {"text": "UDPipe () adds abeam search decoding to Parsito in order to improve the parsing accuracy at the cost of decreasing the parsing speed.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9113619923591614}, {"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9725922346115112}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.936263382434845}]}, {"text": "We propose to improve the parsing accuracy in the architecture introduced by through using more informative word vectors.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.959977924823761}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9560906887054443}]}, {"text": "The idea is based on the greedy nature of the back-propagation algorithm which makes it highly sensitive to the initial state of the algorithm.", "labels": [], "entities": []}, {"text": "Thus, it is expected that more qualified word vectors positively affect the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9559854865074158}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9349261522293091}]}, {"text": "The word vectors in our approach are formed by right singular vectors of a matrix returned by a transformation function that takes a probability co-occurrence matrix as input and expand the data massed around zero.", "labels": [], "entities": []}, {"text": "We show how the proposed method is related to HPCA () and GloVe (.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9468909502029419}]}, {"text": "Using these word vectors with the Stanford parser we could obtain the parsing accuracy of 93.0% UAS and 91.7% LAS on Wall Street Journal (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9408474564552307}, {"text": "UAS", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9974141120910645}, {"text": "LAS", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9979088306427002}, {"text": "Wall Street Journal", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.9839470187822977}]}, {"text": "The word vectors consistently improve the parsing models trained with different types of dependencies in different languages.", "labels": [], "entities": []}, {"text": "Our experimental results show that parsing models trained with Stanford parser can be as accurate or in some cases more accurate than other parsers such as Parsito, and UDPipe.", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9801329374313354}]}], "datasetContent": [{"text": "We restrict our experiments to three languages, English, Swedish, and Persian.", "labels": [], "entities": []}, {"text": "Our experiments on English are organized as follows: Using different types of transformation functions, we first extract a set of word vectors that gives the best parsing accuracy on our development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9867115020751953}]}, {"text": "Then we study the effect of dimensionality on parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9711728096008301}]}, {"text": "Finally, we give a comparison between our best results and the results obtained from other sets of word vectors generated with popular methods of word embedding.", "labels": [], "entities": []}, {"text": "Using the best transformation function obtained for English, we extract word vectors for Swedish and Persian.", "labels": [], "entities": []}, {"text": "These word vectors are then used to train parsing models on the corpus of universal dependencies.", "labels": [], "entities": []}, {"text": "The English word vectors are extracted from a corpus consisting of raw sentences in Wall Street Journal (WSJ) The Persian text normalizer tool is used for sentence splitting and tokenization.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 84, "end_pos": 109, "type": "DATASET", "confidence": 0.948450118303299}, {"text": "Persian text normalizer", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.5560186505317688}, {"text": "sentence splitting", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7788825035095215}]}, {"text": "Word vectors for Swedish are extracted from Swedish Wikipedia available at Wikipedia Monolingual Corpora, Swedish web news corpora and Swedish Wikipedia corpus collected by Sprkbanken.", "labels": [], "entities": [{"text": "Swedish Wikipedia corpus collected", "start_pos": 135, "end_pos": 169, "type": "DATASET", "confidence": 0.7454340904951096}]}, {"text": "The OpenNLP sentence splitter and tokenizer are used for normalizing the corpora.", "labels": [], "entities": [{"text": "OpenNLP sentence splitter", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.8327716787656149}]}, {"text": "We replace all numbers with a special token NUMBER and convert uppercase letters to lowercase forms in English and Swedish.", "labels": [], "entities": [{"text": "NUMBER", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8804669976234436}]}, {"text": "Word vectors are extracted only for the unique words appearing at least 100 times.", "labels": [], "entities": []}, {"text": "We choose the cut-off word frequency of 100 because it is commonly used as a standard threshold in the other references.", "labels": [], "entities": []}, {"text": "The 10 000 most frequent words are used as context words in the co-occurrence matrix.", "labels": [], "entities": []}, {"text": "The word vectors are evaluated with respect to the accuracy of parsing models trained with them using the Stanford neural dependency parser", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9982960820198059}]}], "tableCaptions": [{"text": " Table 1 rep- resents some statistics of the corpora.", "labels": [], "entities": []}, {"text": " Table 2: The performance of parsing models trained with", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9731304049491882}]}, {"text": " Table 3: Performance of word embedding methods: Qual-", "labels": [], "entities": []}, {"text": " Table 4: Accuracy of dependency parsing. Par-St and Par-", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9906212091445923}, {"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7922337055206299}]}, {"text": " Table 5: Accuracy of dependency parsing on the corpus of", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963744282722473}, {"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8147358298301697}]}]}