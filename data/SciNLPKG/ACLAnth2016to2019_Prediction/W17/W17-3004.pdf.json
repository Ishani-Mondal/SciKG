{"title": [{"text": "Deep Learning for User Comment Moderation", "labels": [], "entities": [{"text": "User Comment Moderation", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.5672247211138407}]}], "abstractContent": [{"text": "Experimenting with anew dataset of 1.6M user comments from a Greek news portal and existing datasets of English Wikipedia comments, we show that an RNN outper-forms the previous state of the art in moderation.", "labels": [], "entities": []}, {"text": "A deep, classification-specific attention mechanism improves further the overall performance of the RNN.", "labels": [], "entities": [{"text": "RNN", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.7430615425109863}]}, {"text": "We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.", "labels": [], "entities": []}], "introductionContent": [{"text": "User comments play a central role in social media and online discussion fora.", "labels": [], "entities": []}, {"text": "News portals and blogs often also allow their readers to comment in order to get feedback, engage their readers, and build customer loyalty.", "labels": [], "entities": []}, {"text": "User comments, however, and more generally user content can also be abusive (e.g., bullying, profanity, hate speech).", "labels": [], "entities": []}, {"text": "Social media are increasingly under pressure to combat abusive content.", "labels": [], "entities": []}, {"text": "News portals also suffer from abusive user comments, which damage their reputation and make them liable to fines, e.g., when hosting comments encouraging illegal actions.", "labels": [], "entities": []}, {"text": "They often employ moderators, who are frequently overwhelmed by the volume of comments.", "labels": [], "entities": []}, {"text": "Readers are disappointed when non-abusive comments do not appear quickly online because of moderation delays.", "labels": [], "entities": []}, {"text": "Smaller news portals maybe unable to employ moderators, and some are forced to shutdown their comments sections entirely.", "labels": [], "entities": []}, {"text": "We examine how deep learning ( can be used to moderate user comments.", "labels": [], "entities": []}, {"text": "We experiment with anew dataset of approx. 1.6M manually moderated user See, for example, http://niemanreports.org/ articles/the-future-of-comments/.", "labels": [], "entities": []}, {"text": "comments from a Greek sports portal (Gazzetta), which we make publicly available.", "labels": [], "entities": [{"text": "Greek sports portal (Gazzetta)", "start_pos": 16, "end_pos": 46, "type": "DATASET", "confidence": 0.782741000254949}]}, {"text": "Furthermore, we provide word embeddings pre-trained on 5.2M comments from the same portal.", "labels": [], "entities": []}, {"text": "We also experiment on the datasets of, which contain English Wikipedia comments labeled for personal attacks, aggression, toxicity.", "labels": [], "entities": []}, {"text": "Ina fully automatic scenario, a system directly accepts or rejects comments.", "labels": [], "entities": []}, {"text": "Although this scenario maybe the only available one, e.g., when portals cannot afford moderators, it is unrealistic to expect that fully automatic moderation will be perfect, because abusive comments may involve irony, sarcasm, harassment without profanity etc., which are particularly difficult for machines to handle.", "labels": [], "entities": []}, {"text": "When moderators are available, it is more realistic to develop semi-automatic systems to assist rather than replace them, a scenario that has not been considered in previous work.", "labels": [], "entities": []}, {"text": "Comments for which the system is uncertain) are shown to a moderator to decide; all other comments are accepted or rejected by the system.", "labels": [], "entities": []}, {"text": "We discuss how moderation systems can be tuned, depending on the availability and workload of moderators.", "labels": [], "entities": []}, {"text": "We also introduce additional evaluation measures for the semi-automatic scenario.", "labels": [], "entities": []}, {"text": "On both Gazzetta and Wikipedia comments and for both scenarios (automatic, semi-automatic), we show that a recursive neural network (RNN) outperforms the system of, the previous state of the art for comment moderation, which employed logistic regression (LR) or a multi-layered Perceptron (MLP).", "labels": [], "entities": []}, {"text": "We also propose an attention mechanism that improves the overall performance of the RNN.", "labels": [], "entities": []}, {"text": "Our attention differs from most previous ones () in that it is used in text classification, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models).", "labels": [], "entities": [{"text": "text classification", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.79216068983078}]}, {"text": "In effect, our attention mechanism detects the words of a comment that affect mostly the classification decision (accept, reject), by examining them in the context of the particular comment.", "labels": [], "entities": []}, {"text": "Our main contributions are: (i) We release anew dataset of 1.6M moderated user comments.", "labels": [], "entities": []}, {"text": "(ii) We are among the first to apply deep learning to user comment moderation, and we show that an RNN with a novel classification-specific attention mechanism outperforms the previous state of the art.", "labels": [], "entities": [{"text": "user comment moderation", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.707516094048818}]}, {"text": "(iii) Unlike previous work, we also consider a semi-automatic scenario, along with threshold tuning and evaluation measures for it.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first discuss the datasets we used, to help acquaint the reader with the problem.", "labels": [], "entities": []}, {"text": "There are approx. 1.45M training comments) in the Gazzetta dataset; we call them G-TRAIN-L.", "labels": [], "entities": [{"text": "Gazzetta dataset", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9862174987792969}, {"text": "G-TRAIN-L", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.6098847389221191}]}, {"text": "Some experiments use only the first 100K comments of G-TRAIN-L, called G-TRAIN-S.", "labels": [], "entities": []}, {"text": "An additional set of 60,900 comments (Oct. 7 to) was split to development (G-DEV, 29,700 comments), large test (G-TEST-L, 29,700), and small test set (G-TEST-S, 1,500).", "labels": [], "entities": [{"text": "G-DEV", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.7598932981491089}]}, {"text": "Gazzetta's moderators (2 full-time, plus journalists occasionally helping) are occasionally instructed to be stricter (e.g., during violent events).", "labels": [], "entities": [{"text": "Gazzetta", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9229131937026978}]}, {"text": "To get a more accurate view of performance in normal situations, we manually re-moderated (labeled as 'accept' or 'reject') the comments of G-TEST-S, producing G-TEST-S-R.", "labels": [], "entities": []}, {"text": "The reject ratio is approximately 30% in all subsets, except for G-TEST-S-R where it drops to 22%, because there are no occasions where the moderators were instructed to be stricter in G-TEST-S-R.", "labels": [], "entities": [{"text": "reject ratio", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9712145924568176}]}, {"text": "Each G-TEST-S-R comment was re-moderated by 5 annotators.", "labels": [], "entities": []}, {"text": "alpha was 0.4762, close to the value (0.45) reported by for Wikipedia comments.", "labels": [], "entities": [{"text": "alpha", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8972029685974121}]}, {"text": "Using Cohen's Kappa, the mean pairwise agreement was 0.4749.", "labels": [], "entities": []}, {"text": "The mean pairwise percentage of agreement (% of comments each pair of annotators agreed on) was 81.33%.", "labels": [], "entities": [{"text": "pairwise percentage of agreement", "start_pos": 9, "end_pos": 41, "type": "METRIC", "confidence": 0.679342046380043}]}, {"text": "Cohen's Kappa and Krippendorff's alpha lead to moderate scores, because they account for agreement by chance, which is high when there is class imbalance (22% reject, 78% accept in G-TEST-S-R).", "labels": [], "entities": []}, {"text": "We also provide 300-dimensional word embeddings, pre-trained on approx. 5.2M comments (268M tokens) from Gazzetta using WORD2VEC.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.9020770192146301}]}, {"text": "This larger dataset cannot be used to train classifiers, because most of its comments are from a period (before 2015) when Gazzetta did not employ moderators.", "labels": [], "entities": [{"text": "Gazzetta", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.9274570345878601}]}, {"text": "Attacks dataset: This dataset contains approx. 115K comments, which were labeled as personal attacks (reject) or not (accept) using crowdsourcing.", "labels": [], "entities": []}, {"text": "Each comment was labeled by at least 10 annotators.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement, measured on a random sample of 1K comments using Krippendorff's (2004) alpha, was 0.45.", "labels": [], "entities": [{"text": "agreement", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.815005898475647}]}, {"text": "The gold label of each comment is determined by the majority of annotators, leading to binary labels (accept, reject).", "labels": [], "entities": []}, {"text": "Alternatively, the gold label is the percentage of annotators that labeled the comment as 'accept' (or 'reject'), leading to probabilistic labels.", "labels": [], "entities": []}, {"text": "The dataset is split in three parts: training (W-ATT-TRAIN, 69,526 comments), development (W-ATT-DEV, 23,160), and test.", "labels": [], "entities": [{"text": "W-ATT-TRAIN", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.883913516998291}]}, {"text": "In all three parts, the rejected comments are 12%, but this ratio is artificial (in effect, Wulczyn et al. oversampled comments posted by banned users), unlike Gazzetta subsets where the truly observed accept/reject ratios are used.", "labels": [], "entities": []}, {"text": "Toxicity dataset: This dataset was created like the previous one, but contains more comments, now labeled as toxic (reject) or not (accept).", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was not reported.", "labels": [], "entities": []}, {"text": "Again, binary or probabilistic gold labels can be used.", "labels": [], "entities": []}, {"text": "The dataset is split in three parts: training (W-TOX-TRAIN, 95,692 comments),, and test (W-TOX-TEST, 31,866).", "labels": [], "entities": []}, {"text": "In all three parts, the rejected (toxic) comments are 10%, again an artificial ratio.", "labels": [], "entities": []}, {"text": "Wikipedia comments are longer (median 38 and 39 tokens for attacks, toxicity) compared to Gazzetta's (median 25).", "labels": [], "entities": []}, {"text": "also created an 'aggression' dataset containing the same comments as the personal attacks one, but now labeled as aggressive or not.", "labels": [], "entities": []}, {"text": "The (probabilistic) labels of the two datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we do not consider the aggression dataset further.", "labels": [], "entities": [{"text": "Pearson)", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9307853877544403}]}, {"text": "Following, we report in, along with Spearman correlations between system-generated probabilities P (accept|c) and human probabilistic gold labels (Section 2.2) when probabilistic gold labels are available.", "labels": [], "entities": []}, {"text": "A first observation is that increasing the size of the Gazzetta training set (G-TRAIN-S to G-TRAIN-L, 14 More precisely, when computing F \u03b2 , we reorder the development comments by time posted, and split them into batches of 100.", "labels": [], "entities": [{"text": "Gazzetta training set", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.9279725154240926}]}, {"text": "For each ta (and tr) value, we compute F \u03b2 per batch and macro-average across batches.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9693798124790192}]}, {"text": "The resulting thresholds lead to F \u03b2 scores that are more stable overtime.", "labels": [], "entities": [{"text": "F \u03b2 scores", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9829954703648885}]}, {"text": "When computing AUC, the gold label is the majority label of the annotators.", "labels": [], "entities": [{"text": "computing AUC", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.4776645004749298}]}, {"text": "When computing Spearman, the gold label is probabilistic (% of annotators that accepted the comment).", "labels": [], "entities": []}, {"text": "The decisions of the systems are always probabilistic.), but not always on Wikipedia comments).", "labels": [], "entities": []}, {"text": "Another observation is that da-RNN is always worse than a-RNN), confirming that the hidden states of the RNN area better input to the attention mechanism than word embeddings.", "labels": [], "entities": []}, {"text": "The performance of da-RNN deteriorates further when equal attention is assigned to the hidden states (eq-RNN), when the weighted sum of hidden states (h sum ) is replaced by the weighted sum of word embeddings (da-CENT), or both (eq-CENT).", "labels": [], "entities": []}, {"text": "Also, da-CENT outperforms eq-CENT, indicating that the attention mechanism improves the performance of simply averaging word embeddings.", "labels": [], "entities": []}, {"text": "The Wikipedia subsets are easier (all methods perform better on Wikipedia subsets, compared to Gazzetta).", "labels": [], "entities": []}, {"text": "shows F 2 (P reject , P accept ) on G-TEST-L, G-TEST-S, W-ATT-TEST, W-TOX-TEST, when ta , tr are tuned on the corresponding development tests for varying coverage.", "labels": [], "entities": [{"text": "F 2", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9876353144645691}]}, {"text": "For the Gazzetta datasets, we show results training on G-TRAIN-S (solid lines) and G-TRAIN-L (dashed).", "labels": [], "entities": [{"text": "Gazzetta datasets", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9706933498382568}]}, {"text": "The differences between RNN and a-RNN are again small, but it is now easier to see that a-RNN is overall better.", "labels": [], "entities": []}, {"text": "Again, a-RNN and RNN are better than and DETOX, and the results improve with a larger training set (dashed).", "labels": [], "entities": [{"text": "RNN", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9317013025283813}, {"text": "DETOX", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9872028231620789}]}, {"text": "On W-ATT-TEST and W-.", "labels": [], "entities": [{"text": "W-ATT-TEST", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.9324121475219727}, {"text": "W-.", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8693926135698954}]}, {"text": "On the more difficult Gazzetta datasets, a-RNN still obtains P accept , P reject \u2265 0.85 when tuned for 50% coverage.", "labels": [], "entities": [{"text": "Gazzetta datasets", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.9792400598526001}, {"text": "P accept", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9431813657283783}, {"text": "P reject", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.8700337707996368}]}, {"text": "When tuned for 100% coverage, comments for which the system is uncertain (gray zone) cannot be avoided and there are inevitably more misclassifications; the use of F 2 during threshold tuning places more emphasis on avoiding wrongly accepted comments, leading to high P accept (\u2265 0.82), at the expense of wrongly rejected comments, i.e., sacrificing P reject (\u2265 0.56).", "labels": [], "entities": [{"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9503912925720215}, {"text": "F 2", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9706124663352966}, {"text": "P accept", "start_pos": 268, "end_pos": 276, "type": "METRIC", "confidence": 0.9305103123188019}, {"text": "P reject", "start_pos": 350, "end_pos": 358, "type": "METRIC", "confidence": 0.9453137814998627}]}, {"text": "On the re-moderated G-TEST-S-R (similar diagrams, not shown), P accept , P reject become 0.96, 0.88 for coverage 50%, and 0.92, 0.48 for coverage 100%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9715782999992371}, {"text": "coverage", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9630429744720459}]}], "tableCaptions": [{"text": " Table 2: Results on Gazzetta comments.", "labels": [], "entities": [{"text": "Gazzetta comments", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9640474915504456}]}, {"text": " Table 3: Results on Wikipedia comments.", "labels": [], "entities": []}]}