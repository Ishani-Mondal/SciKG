{"title": [{"text": "CIC-FBK Approach to Native Language Identification", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.6242259244124094}]}], "abstractContent": [{"text": "We present the CIC-FBK system, which took part in the Native Language Identification (NLI) Shared Task 2017.", "labels": [], "entities": [{"text": "Native Language Identification (NLI) Shared Task", "start_pos": 54, "end_pos": 102, "type": "TASK", "confidence": 0.8043951690196991}]}, {"text": "Our approach combines features commonly used in previous NLI research, i.e., word n-grams, lemma n-grams, part-of-speech n-grams, and function words, with recently introduced character n-grams from misspelled words, and features that are novel in this task, such as typed character n-grams, and syntactic n-grams of words and of syntactic relation tags.", "labels": [], "entities": []}, {"text": "We use log-entropy weighting scheme and perform classification using the Support Vector Machines (SVM) algorithm.", "labels": [], "entities": []}, {"text": "Our system achieved 0.8808 macro-averaged F1-score and shared the 1 st rank in the NLI Shared Task 2017 scoring.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9608924984931946}, {"text": "NLI Shared Task 2017 scoring", "start_pos": 83, "end_pos": 111, "type": "DATASET", "confidence": 0.8083848714828491}]}], "introductionContent": [{"text": "Native language identification (NLI) is a natural language processing (NLP) task that aims at automatically identifying the native language (L1) of a language learner based on his/her writing in the second language (L2).", "labels": [], "entities": [{"text": "Native language identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7924119730790457}, {"text": "natural language processing (NLP) task", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.7960982578141349}]}, {"text": "Identifying the native language is based on the hypothesis that the L1 of a learner impacts his/her L2 writing due to the language transfer effect.", "labels": [], "entities": [{"text": "Identifying the native language", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8476788848638535}]}, {"text": "NLI can be used fora variety of purposes, including marketing, security, and educational applications.", "labels": [], "entities": []}, {"text": "From the machinelearning perspective, the NLI task is viewed as a multi-class, single-label classification problem, in which automatic methods have to assign class labels (L1s) to objects (texts).", "labels": [], "entities": []}, {"text": "Recent trends in NLI include cross-genre and cross-corpus NLI scenarios, as well as identifying the L1 based on writings in other non-English L2s and crosslingual NLI research).", "labels": [], "entities": []}, {"text": "However, following the practice of the first NLI shared task, this year's task focuses on L2 English data ( . This can be related to the use of English as lingua franca on the Internet and academia, when NLI methods are particularly useful for languages with a large number of foreign speakers.", "labels": [], "entities": []}, {"text": "Moreover, following the 2016 Computational Paralinguistics Challenge () and the VarDial workshop (, this year's competition covers an NLI task based on the spoken response.", "labels": [], "entities": []}, {"text": "Overall, this year's task consists of three tracks: NLI on the essay only, NLI on the spoken response only, and NLI on both essay and spoken response.", "labels": [], "entities": [{"text": "NLI", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9724162220954895}, {"text": "NLI", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9423609375953674}]}, {"text": "In this paper, we describe the CIC-FBK approach to the essay-only track.", "labels": [], "entities": []}, {"text": "Previous works on identifying the native language from texts explored a large variety of features, including lexical and part-of-speech (POS) features (), character n-grams (), spelling errors (, and syntactic features (.", "labels": [], "entities": []}, {"text": "Following previous research on the NLI task, we incorporate commonly used word n-grams, lemma n-grams, POS n-grams, and function words.", "labels": [], "entities": []}, {"text": "In order to capture the L1 influences at the character level, we use recently introduced character n-grams from misspelled words, as well as 10 categories of character n-gram features proposed by.", "labels": [], "entities": []}, {"text": "We also include syntactic features by extracting syntactic dependencybased n-grams of words and of syntactic relation tags ( ) using the algorithm designed by.", "labels": [], "entities": []}, {"text": "We describe the features used by the CIC-FBK system in more detail in subsection 3.1.", "labels": [], "entities": []}, {"text": "Our system achieved 0.8808 macro-averaged F1-score and 0.8809 accuracy in the essay-only track and shared the 1 st rank in the NLI Shared Task 2017 scoring, obtaining the 2 nd absolute score with the difference of 0.0010 F1-score and 0.0009 accuracy with the 1 st place.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9710030555725098}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9986795783042908}, {"text": "NLI Shared Task 2017 scoring", "start_pos": 127, "end_pos": 155, "type": "DATASET", "confidence": 0.7920823812484741}, {"text": "F1-score", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9898138642311096}, {"text": "accuracy", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9957875609397888}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: 10-fold cross-validation accuracy of each  feature type individually on the merged training  and development sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9800097346305847}]}, {"text": " Table 3: Results for the essay-only track for the 1 st  ranked teams. The results for our team are high- lighted in bold typeface.", "labels": [], "entities": []}]}