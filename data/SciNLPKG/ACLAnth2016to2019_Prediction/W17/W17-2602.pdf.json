{"title": [{"text": "Context encoders as a simple but powerful extension of word2vec", "labels": [], "entities": [{"text": "Context encoders", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9048156142234802}]}], "abstractContent": [{"text": "With a strikingly simple architecture and the ability to learn meaningful word em-beddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today.", "labels": [], "entities": []}, {"text": "However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings and, additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot.", "labels": [], "entities": []}, {"text": "Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc).", "labels": [], "entities": []}, {"text": "By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for words with multiple meanings can be created based on the words' local contexts.", "labels": [], "entities": []}, {"text": "The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.", "labels": [], "entities": [{"text": "CoNLL 2003 named entity recognition (NER) task", "start_pos": 96, "end_pos": 142, "type": "TASK", "confidence": 0.8548249999682108}]}], "introductionContent": [{"text": "Representation learning is very prominent in the field of natural language processing (NLP).", "labels": [], "entities": [{"text": "Representation learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9689853489398956}, {"text": "natural language processing (NLP)", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.7925174434979757}]}, {"text": "For example, word embeddings learned by neural language models (NLM) were shown to improve the performance when used as features for supervised learning tasks such as named entity recognition (NER).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 167, "end_pos": 197, "type": "TASK", "confidence": 0.8002685606479645}]}, {"text": "The popular word2vec model (,b) learns meaningful word embeddings by considering only the words' local contexts and thanks to its shallow architecture it can be trained very efficiently on large corpora.", "labels": [], "entities": []}, {"text": "The model, however, only learns a single representation for words from a fixed vocabulary.", "labels": [], "entities": []}, {"text": "This means, if in a task we encounter anew word that was not present in the texts used for training, we cannot create an embedding for this word without repeating the time consuming training procedure of the model.", "labels": [], "entities": []}, {"text": "Additionally, a single embedding does not optimally represent words with multiple meanings.", "labels": [], "entities": []}, {"text": "For example, \"Washington\" is both the name of a US state as well as a former president and only by taking into account the word's local context one can identify the proper sense.", "labels": [], "entities": []}, {"text": "Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective, we propose an extension of the model we call context encoders (ConEc).", "labels": [], "entities": []}, {"text": "This allows for an easy creation of OOV embeddings as well as a better representation of words with multiple meanings simply by multiplying the trained word2vec embeddings with the words' average context vectors.", "labels": [], "entities": []}, {"text": "As demonstrated on the CoNLL 2003 NER challenge, using the word embeddings created with ConEc instead of word2vec as features improves the classification performance significantly.", "labels": [], "entities": [{"text": "CoNLL 2003 NER challenge", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.8884298801422119}]}, {"text": "Related work In the past, NLM have addressed the issue of polysemy in various ways.", "labels": [], "entities": []}, {"text": "For example, sense2vec is an extension of word2vec, wherein a preprocessing step all words in the training corpus are annotated with their part-of-speech (POS) tag and then the embeddings are learned for tokens consisting of the words themselves and their POS tags, thereby generating different representations e.g. for words that are used both as a noun and verb).", "labels": [], "entities": []}, {"text": "Other methods first cluster the contexts the words appear ( or use additional resources such as wordnet to identify multiple meanings of words (.", "labels": [], "entities": []}, {"text": "One possibility to create OOV embeddings is to learn representations for all character n-grams in the texts and then compute the embedding of a word by combining the embeddings of the n-grams occurring in it (.", "labels": [], "entities": []}, {"text": "However, none of these NLM are designed to solve both the OOV and polysemy problem at the same time and compared to word2vec they require more parameters, resources, or additional steps in the training procedure.", "labels": [], "entities": []}, {"text": "ConEc on the other hand can generate OOV embeddings as well as better representations for words with multiple meanings simply by multiplying the matrix of trained word2vec embeddings with the words' average context vectors.", "labels": [], "entities": [{"text": "ConEc", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8708869814872742}]}], "datasetContent": [{"text": "The word embeddings learned by word2vec and context encoders are evaluated on the CoNLL 2003 NER benchmark task ().", "labels": [], "entities": [{"text": "CoNLL 2003 NER benchmark task", "start_pos": 82, "end_pos": 111, "type": "DATASET", "confidence": 0.921632730960846}]}, {"text": "We use a CBOW word2vec model trained with negative sampling as described above where k = 13, the embedding dimensionality dis 200 and we use a context window of 5 words.", "labels": [], "entities": []}, {"text": "The word embeddings Overall results, where the mean performance using word2vec embeddings (dashed lines) is considered as our baseline, all other embeddings are computed with ConEcs using various combinations of the words' global and local CVs.", "labels": [], "entities": []}, {"text": "Right panel: Increased performance (mean and standard deviation) on the test fold when using ConEc: Multiplying the word2vec embeddings with global CVs yields a performance gain of 2.5 percentage points (A).", "labels": [], "entities": [{"text": "ConEc", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.759981095790863}, {"text": "A", "start_pos": 204, "end_pos": 205, "type": "METRIC", "confidence": 0.9742156863212585}]}, {"text": "By additionally using local CVs to create OOV word embeddings we gain another 1.7 points (B).", "labels": [], "entities": []}, {"text": "When using a combination of global and local CVs (with a = 0.6) to distinguish between the different meanings of words, the F1-score increases by another 5.1 points (C), yielding a F1-score of 39.92%, which marks a significant improvement compared to the 30.59% reached with word2vec features.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9995269775390625}, {"text": "F1-score", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9993433356285095}]}, {"text": "created by ConEc are built directly on top of the word2vec model by multiplying the original embeddings (W 0 ) with the respective context vectors.", "labels": [], "entities": []}, {"text": "Code to replicate the experiments is available online.", "labels": [], "entities": []}, {"text": "Named Entity Recognition The main advantage of context encoders is that they can use local context to create OOV embeddings and distinguish between the different senses of words.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6957658727963766}]}, {"text": "The effects of this are most prominent in a task such as NER, where the local context of a word can make all the difference, e.g. to distinguish between the \"Chicago Bears\" (an organization) and the city of Chicago (a location).", "labels": [], "entities": [{"text": "NER", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8753167986869812}]}, {"text": "We tested this on the CoNLL 2003 NER task by using the word embeddings as features together with a logistic regression classifier.", "labels": [], "entities": [{"text": "CoNLL 2003 NER task", "start_pos": 22, "end_pos": 41, "type": "DATASET", "confidence": 0.9065994024276733}]}, {"text": "The reported F1-scores were computed using the official evaluation script.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9980906844139099}]}, {"text": "The results achieved with various word embeddings on the training, development and test part of the CoNLL task are reported in.", "labels": [], "entities": []}, {"text": "Please note that we are using this task as an extrinsic evaluation to illustrate the advantages of ConEc embeddings over the regular word2vec embeddings.", "labels": [], "entities": []}, {"text": "To isolate the effects on the performance, we are only using these word embeddings https://github.com/cod3licious/conec as features, while of course the performance on this NER challenge is typically much higher when other features such as a word's case or POS tag are included as well.", "labels": [], "entities": []}, {"text": "The word2vec embeddings were trained on the documents used in the training part of the task and OOV words in the development and test parts are represented as zero vectors.", "labels": [], "entities": []}, {"text": "With three parameter settings we illustrate the advantages of ConEc: A) Multiplying the word2vec embeddings by the words' average context vectors generally improves the embeddings.", "labels": [], "entities": []}, {"text": "To show this, ConEc word embeddings were computed using only global CVs (Eq.", "labels": [], "entities": []}, {"text": "1 with a = 1), which means OOV words again have a zero representation.", "labels": [], "entities": []}, {"text": "With these embeddings (labeled 'global' in) the performance improves on the dev and test folds of the task.", "labels": [], "entities": []}, {"text": "B) Useful OOV embeddings can be created from the local context of anew word.", "labels": [], "entities": []}, {"text": "To show this, the ConEc embeddings for words from the training vocabulary (w \u2208 N ) were computed as in A), but now the embeddings for OOV words (w / \u2208 N ) were computed using local CVs (Eq.", "labels": [], "entities": []}, {"text": "1 with a = 1 \u2200 w \u2208 N and a = 0 \u2200 w / \u2208 N ; referred to as 'OOV' in the.", "labels": [], "entities": [{"text": "OOV", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.8772918581962585}]}, {"text": "The training performance stays the same, of course, as here all words have an embedding based on their global contexts, but there is a jump in the ConEc performance on the dev and test folds, where OOV words now have a representation based on their local contexts.", "labels": [], "entities": []}, {"text": "C) Better embeddings fora word with multiple meanings can be created by using a combination of the word's average global and local CVs as input to the ConEc.", "labels": [], "entities": [{"text": "ConEc", "start_pos": 151, "end_pos": 156, "type": "DATASET", "confidence": 0.925062358379364}]}, {"text": "To show this, the OOV embeddings were computed as in B), but now for the words occurring in the training vocabulary, the local context was taken into account as well by setting a < 1 (Eq. 1 with a \u2208 [0, 1) \u2200 w \u2208 N and a = 0 \u2200 w / \u2208 N ).", "labels": [], "entities": []}, {"text": "The best performances on all folds are achieved when averaging the global and local CVs with around a = 0.6 before multiplying them with the word2vec embeddings, which clearly shows that ConEc embeddings created by incorporating local context can help distinguish between multiple meanings of words.", "labels": [], "entities": []}], "tableCaptions": []}