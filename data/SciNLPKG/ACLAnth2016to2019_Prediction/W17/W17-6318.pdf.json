{"title": [{"text": "Evaluating LSTM models for grammatical function labelling", "labels": [], "entities": [{"text": "grammatical function labelling", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.6431588133176168}]}], "abstractContent": [{"text": "To improve grammatical function labelling for German, we augment the labelling component of a neural dependency parser with a decision history.", "labels": [], "entities": []}, {"text": "We present different ways to encode the history, using different LSTM architectures, and show that our models yield significant improvements , resulting in a LAS for German that is close to the best result from the SPMRL 2014 shared task (without the reranker).", "labels": [], "entities": [{"text": "LAS", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.9588438272476196}, {"text": "SPMRL 2014 shared task", "start_pos": 215, "end_pos": 237, "type": "TASK", "confidence": 0.5874417051672935}]}], "introductionContent": [{"text": "For languages with a non-configurational word order and rich(er) morphology, such as German, grammatical function (GF) labels are essential for interpreting the meaning of a sentence.", "labels": [], "entities": [{"text": "interpreting the meaning of a sentence", "start_pos": 144, "end_pos": 182, "type": "TASK", "confidence": 0.845745712518692}]}, {"text": "Case syncretism in the German case paradigm makes GF labelling a challenging task.", "labels": [], "entities": [{"text": "GF labelling", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.8604543507099152}]}, {"text": "See (1) for an example where the nouns in the sentence are ambiguous between different cases, which makes it hard fora statistical parser to recover the correct reading.", "labels": [], "entities": []}, {"text": "We approach the problem of GF labelling as a subtask of dependency parsing, where we first generate unlabelled trees and, in the second step, try to find the correct labels.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7656655311584473}]}, {"text": "This pipeline architechture gives us more flexibility, allowing us to use the labeller in combination with our parser, but also to apply it to the unlabelled output of other parsing systems without the need to change or retraining the parsers.", "labels": [], "entities": []}, {"text": "The approach also makes it straightforward to test different architectures for GF labelling.", "labels": [], "entities": [{"text": "GF labelling", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.6878127455711365}]}, {"text": "We are especially interested in the influence of different input structures representing different (surface versus structural) orders of the input.", "labels": [], "entities": []}, {"text": "In particular, we compare models where we present the unlabelled tree in linear order with a model where we encode the parser output as a tree.", "labels": [], "entities": []}, {"text": "We show that all models are able to learn GFs with a similar overall LAS, but the model where the tree is encoded in a breadth-first order outperforms all other models on labelling core argument GFs.", "labels": [], "entities": [{"text": "LAS", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9926289319992065}]}], "datasetContent": [{"text": "Our interest is focussed on German, but to put our work in context, we follow and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German.", "labels": [], "entities": []}, {"text": "For English, we use the Penn Treebank (PTB) () with standard training/dev/test splits.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.9664038062095642}]}, {"text": "The POS tags are assigned using the Stanford POS tagger () with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies).", "labels": [], "entities": [{"text": "Stanford POS tagger", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.8713005582491556}]}, {"text": "The German and Czech data come from the CoNLL-X shared task () and our data split follows.", "labels": [], "entities": []}, {"text": "As the CoNLL-X testsets are rather small (\u223c 360 sentences), we also in the TREELSTM model.", "labels": [], "entities": [{"text": "CoNLL-X testsets", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.8839053213596344}, {"text": "TREELSTM", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9907453060150146}]}, {"text": "train and test on the much larger German SPMRL 2014 shared task data () (5,000 test sentences).", "labels": [], "entities": [{"text": "German SPMRL 2014 shared task data", "start_pos": 34, "end_pos": 68, "type": "DATASET", "confidence": 0.7677998195091883}]}, {"text": "For the SPMRL data we use the predicted POS tags provided by the shared task organisers.", "labels": [], "entities": [{"text": "SPMRL", "start_pos": 8, "end_pos": 13, "type": "TASK", "confidence": 0.8761477470397949}]}], "tableCaptions": [{"text": " Table 1: Results for different labellers applied to  the unlabelled parser output. The first row re- ports UAS for the input to the labellers. The last  row (DENSE) shows the results from", "labels": [], "entities": [{"text": "UAS", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9875158071517944}, {"text": "DENSE)", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.8529092371463776}]}, {"text": " Table 2: LAS for core argument functions (Ger- man SPMRL data), and frequency (#) of GF in the  testset (SB: subj, OA: acc.obj, DA: dat.obj, PD:  pred, AG: gen.attribute, PG: phrasal genitive, OC:  clausal obj, OG: gen.obj).", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.989538848400116}, {"text": "Ger- man SPMRL data", "start_pos": 43, "end_pos": 62, "type": "DATASET", "confidence": 0.5464685022830963}]}, {"text": " Table 3: Avg. dependency length and ratio of left  arcs vs. all (left + right) arc dependencies for args.  (* in the Czech data, Obj subsumes all types of  objects, not only direct objects)", "labels": [], "entities": [{"text": "Avg. dependency length", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.6559991538524628}, {"text": "Czech data", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.8339061141014099}]}]}