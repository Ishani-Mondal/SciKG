{"title": [{"text": "Distributional Lesk: Effective Knowledge-Based Word Sense Disambiguation", "labels": [], "entities": [{"text": "Distributional Lesk", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.900188684463501}, {"text": "Effective Knowledge-Based Word Sense Disambiguation", "start_pos": 21, "end_pos": 72, "type": "TASK", "confidence": 0.6489280343055726}]}], "abstractContent": [{"text": "We propose a simple, yet effective, Word Sense Disambiguation method that uses a combination of a lexical knowledge-base and embeddings.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.627349317073822}]}, {"text": "Similar to the classic Lesk algorithm, it exploits the idea that overlap between the context of a word and the definition of its senses provides information on its meaning.", "labels": [], "entities": []}, {"text": "Instead of counting the number of words that overlap, we use embeddings to compute the similarity between the gloss of a sense and the context.", "labels": [], "entities": []}, {"text": "Evaluation on both Dutch and English datasets shows that our method outperforms other Lesk methods and improves upon a state-of-the-art knowledge-based system.", "labels": [], "entities": [{"text": "Dutch and English datasets", "start_pos": 19, "end_pos": 45, "type": "DATASET", "confidence": 0.5879750102758408}]}, {"text": "Additional experiments confirm the effect of the use of glosses and indicate that our approach works well in different domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in natural language processing.", "labels": [], "entities": [{"text": "automatically finding the correct meaning of a word in context", "start_pos": 13, "end_pos": 75, "type": "TASK", "confidence": 0.7641416668891907}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.7439388086398443}, {"text": "natural language processing", "start_pos": 149, "end_pos": 176, "type": "TASK", "confidence": 0.6337161858876547}]}, {"text": "Although the best performing WSD systems are those based on supervised learning methods (, a large amount of manually annotated data is required for training.", "labels": [], "entities": [{"text": "WSD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.965486466884613}]}, {"text": "Furthermore, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (.", "labels": [], "entities": []}, {"text": "As an alternative to supervised systems, knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains ( ).", "labels": [], "entities": []}, {"text": "They only require two types of information: a set of dictionary entries with definitions for each possible word meaning, and the context in which the word occurs.", "labels": [], "entities": []}, {"text": "An example of such a system is the Lesk algorithm that exploits the idea that the overlap between the definition of a word and the definitions of the words in its context can provide information about its meaning.", "labels": [], "entities": []}, {"text": "In this paper, we propose a knowledge-based WSD method that is loosely based on the Lesk algorithm exploiting both the context of the words and the definitions (hereafter referred to as glosses) of the senses.", "labels": [], "entities": [{"text": "WSD", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9304772615432739}]}, {"text": "Instead of counting the number of words that overlap, we use word-and sense embeddings to compute the similarity between the gloss of a sense and the context of the word.", "labels": [], "entities": []}, {"text": "The strong point of our method is that it only requires large unlabeled corpora and a sense inventory such as WordNet, and therefore does not rely on annotated data.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9754793047904968}]}, {"text": "Also, it is readily applicable to other languages if a sense inventory is available.", "labels": [], "entities": []}], "datasetContent": [{"text": "The performance of our algorithm was tested on both Dutch and English sentences in an all-words setup.", "labels": [], "entities": []}, {"text": "Our sense inventory for Dutch is Cornetto while, for English, we use WordNet 1.7.1.", "labels": [], "entities": [{"text": "WordNet 1.7.1", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9309067726135254}]}, {"text": "In Cornetto, 51.0% of the senses have glosses associated with them and in the Princeton WordNet, almost all of them do.", "labels": [], "entities": [{"text": "Princeton WordNet", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.823054313659668}]}, {"text": "The DutchSemCor corpus ) is used for Dutch evaluation and, for English, we use SemCor).", "labels": [], "entities": [{"text": "DutchSemCor corpus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9761073887348175}]}, {"text": "For both languages, a random subset of 5000 manually annotated sentences from each corpus was created.", "labels": [], "entities": []}, {"text": "Additionally, we test on the Senseval-2 (SE-2) and Senseval-3 (SE-3) all-words datasets () . We build 300-dimensional word embeddings on the Dutch Sonar corpus (Oostdijk et al., 2013) using word2vec CBOW ( , and create sense-and lexeme embeddings with AutoExtend.", "labels": [], "entities": [{"text": "Dutch Sonar corpus", "start_pos": 141, "end_pos": 159, "type": "DATASET", "confidence": 0.8990780909856161}]}, {"text": "For English, we use the embeddings from Rothe and Sch\u00fctze (2015) 2 . They lie within the same vector space as the pre-trained word embeddings by , trained on apart of the Google News dataset, which contains about 100 billion words.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 171, "end_pos": 190, "type": "DATASET", "confidence": 0.9332597255706787}]}, {"text": "This model (similar to the Dutch model) contains 300-dimensional vectors for 3 million words and phrases.", "labels": [], "entities": []}, {"text": "We evaluate our method by comparing it with a random baseline and Simplified Lesk with expanded glosses (SE-Lesk)).", "labels": [], "entities": []}, {"text": "Additionally, we compare our system to a state-of-the-art knowledge-based WSD system, UKB , that, similar to our method, does not require any manually tagged data.", "labels": [], "entities": [{"text": "UKB", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9485955834388733}]}, {"text": "UKB can be used for graph-based WSD using a pre-existing knowledge base.", "labels": [], "entities": [{"text": "UKB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9773482084274292}, {"text": "WSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9238842725753784}]}, {"text": "It applies random walks, e.g. Personalized PageRank, on the Knowledge Base graph to rank the vertices according to the context.", "labels": [], "entities": []}, {"text": "We use UKBs Personalized PageRank method word-by-word with WordNet 1.7 and eXtended WordNet for English, as this setup yielded the best results in Agirre and Soroa.", "labels": [], "entities": [{"text": "UKBs", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.9763407111167908}]}, {"text": "For Dutch, we use the Cornetto database as input graph.", "labels": [], "entities": [{"text": "Cornetto database", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.9285286664962769}]}, {"text": "We do not compare our system to the initial results of AutoExtend (Rothe and Sch\u00fctze, 2015) as they tested it in a supervised setup using sense embeddings as features.", "labels": [], "entities": []}, {"text": "However, as is customary in WSD evaluation, we do compare our system to the most frequent WordNet sense baseline, which is notoriously difficult to beat due to the highly skewed distribution of word senses.", "labels": [], "entities": [{"text": "WSD evaluation", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.922903448343277}, {"text": "WordNet sense baseline", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.8725032210350037}]}, {"text": "As this baseline relies on manually annotated data, which our system aims to avoid, we consider this baseline to be semi-supervised and therefore an upper bound.", "labels": [], "entities": []}, {"text": "For Dutch, the manually annotated part of DutchSemCor is balanced per sense which means that an equal number of examples for each sense is annotated.", "labels": [], "entities": [{"text": "DutchSemCor", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9413812756538391}]}, {"text": "It is therefore not a reliable source for computing the most frequent sense.", "labels": [], "entities": []}, {"text": "Alternatively, similar to , we derive sense frequencies by using the automatically annotated counts in DutchSemCor 4 , assuming that the automatic annotation sufficiently reflects the true distribution for this purpose.", "labels": [], "entities": [{"text": "DutchSemCor 4", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.9563077688217163}]}, {"text": "The most frequent sense baseline for Dutch is, therefore, lower as compared to the English one, where the most frequent sense of a word is fully based on manual annotation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on DutchSemCor (DSC), SemCor (SC) Senseval-2 (SE-2) and Senseval3 (SE-3)", "labels": [], "entities": [{"text": "DutchSemCor (DSC)", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9134112745523453}]}, {"text": " Table 2: Effects of lexemes, glosses and sorting. The second and the fourth column show results of a  system that only uses the lexeme (Lex) or gloss vectors (Gloss) respectively. In the third and last column  sorting (+S) is added.", "labels": [], "entities": [{"text": "Gloss", "start_pos": 160, "end_pos": 165, "type": "METRIC", "confidence": 0.9106835722923279}]}, {"text": " Table 3. On every subsec-", "labels": [], "entities": []}, {"text": " Table 3: Results for the Dutch all-words task on a random subset of each of the four largest datasets from  DutchSemCor: discussion lists (dl), subtitles (st), Wikipedia (wp) and newspapers (ns).", "labels": [], "entities": [{"text": "DutchSemCor", "start_pos": 109, "end_pos": 120, "type": "DATASET", "confidence": 0.9421181678771973}]}]}