{"title": [{"text": "Character-based Bidirectional LSTM-CRF with words and characters for Japanese Named Entity Recognition", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.684443344672521}]}], "abstractContent": [{"text": "Recently, neural models have shown superior performance over conventional models in NER tasks.", "labels": [], "entities": [{"text": "NER tasks", "start_pos": 84, "end_pos": 93, "type": "TASK", "confidence": 0.9397552609443665}]}, {"text": "These models use CNN to extract sub-word information along with RNN to predict a tag for each word.", "labels": [], "entities": []}, {"text": "However, these models have been tested almost entirely on English texts.", "labels": [], "entities": []}, {"text": "It remains unclear whether they perform similarly in other languages.", "labels": [], "entities": []}, {"text": "We worked on Japanese NER using neural models and discovered two obstacles of the state-of-the-art model.", "labels": [], "entities": [{"text": "NER", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.6341099143028259}]}, {"text": "First, CNN is unsuitable for extracting Japanese sub-word information.", "labels": [], "entities": [{"text": "CNN", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.9160215258598328}, {"text": "extracting Japanese sub-word information", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.8464861661195755}]}, {"text": "Secondly, a model predicting a tag for each word cannot extract an entity when apart of a word composes an entity.", "labels": [], "entities": []}, {"text": "The contributions of this work are (i) verifying the effectiveness of the state-of-the-art NER model for Japanese, (ii) proposing a neural model for predicting a tag for each character using word and character information.", "labels": [], "entities": []}, {"text": "Experimentally obtained results demonstrate that our model outper-forms the state-of-the-art neural English NER model in Japanese.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entity Recognition (NER) is designed to extract entities such as location and product from texts.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7707403500874838}]}, {"text": "The results are used in sophisticated tasks including summarizations and recommendations.", "labels": [], "entities": [{"text": "summarizations", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.9862042665481567}]}, {"text": "In the past several years, sequential neural models such as long-short term memory (LSTM) have been applied to NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9701045155525208}]}, {"text": "They have outperformed the conventional models (.", "labels": [], "entities": []}, {"text": "Recently, Convolutional Neural Network (CNN) was introduced into many models for extracting sub-word information from a word (.", "labels": [], "entities": []}, {"text": "The models achieved higher performance because CNN can capture capitalization, suffixes, and prefixes (.", "labels": [], "entities": []}, {"text": "These models predict a tag for each word assuming that words can be separated clearly by explicit word separators (e.g. blank spaces).", "labels": [], "entities": []}, {"text": "We refer to such model as a \"wordbased model\", even if inputs include characters.", "labels": [], "entities": []}, {"text": "When Japanese NER employs a recent neural model, two obstacles arise.", "labels": [], "entities": [{"text": "NER", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.7766793370246887}]}, {"text": "First, extracting sub-word information by CNN is unsuitable for Japanese language.", "labels": [], "entities": [{"text": "CNN", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.7325599193572998}]}, {"text": "The reasons are that Japanese words tend to be shorter than English and Japanese characters have no capitalization.", "labels": [], "entities": []}, {"text": "Secondly, the word-based model cannot extract entities when apart of a word composes an entity.", "labels": [], "entities": []}, {"text": "Japanese language has no explicit word separators.", "labels": [], "entities": []}, {"text": "Word boundaries occasionally become ambiguous.", "labels": [], "entities": []}, {"text": "Therefore, the possibility exists that entity boundary does not match word boundaries.", "labels": [], "entities": []}, {"text": "We define such phenomena as \"boundary conflict\".", "labels": [], "entities": []}, {"text": "To avoid this obstacle, NER using finer-grained compose units than words are preferred in Japanese NERs ().", "labels": [], "entities": [{"text": "NER", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9304693341255188}]}, {"text": "We follow these approaches and expand the state-of-the-art neural NER model to predict a tag for each character: a \"characterbased model\".", "labels": [], "entities": []}, {"text": "The contributions of our study are: (i) application of a state-of-the-art NER model to Japanese NER and verification of its effectiveness, and (ii) proposition of a \"character-based\" neural model with concatenating words and characters.", "labels": [], "entities": []}, {"text": "Experimental results show that our model outperforms the state-of-the-art neural NER model in Japanese.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models using the Mainichi newspaper corpus.", "labels": [], "entities": [{"text": "Mainichi newspaper corpus", "start_pos": 33, "end_pos": 58, "type": "DATASET", "confidence": 0.9927555521329244}]}, {"text": "We specifically examine the four categories of the highest frequency: Product, Location, Organization, Time. of Mainichi newspaper articles which include almost 500 million words are used for pre-training.", "labels": [], "entities": [{"text": "Time.", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9843816161155701}, {"text": "Mainichi newspaper articles", "start_pos": 112, "end_pos": 139, "type": "DATASET", "confidence": 0.9659486015637716}]}, {"text": "We conduct parameter tuning using the development dataset.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7131747901439667}]}, {"text": "We choose the unit number of LSTM as 300, the size of word embedding as 500, that of character embedding as 50, the maximum epoch as 20, and the batch size as 60.", "labels": [], "entities": []}, {"text": "We use Adam (, with the learning rate of 0.001 for optimization.", "labels": [], "entities": []}, {"text": "We use MeCab () for word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7472224235534668}]}, {"text": "Other conditions are the same as those reported for an earlier study (Ma and Hovy, 2016).: F1 score of each models.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.974054217338562}]}, {"text": "Average is a weighted average.", "labels": [], "entities": [{"text": "Average", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9378960728645325}]}, {"text": "\u2020 expresses the best result in the word-based models for each entity category.", "labels": [], "entities": []}, {"text": "Bold means the best result in all models for each entity category.", "labels": [], "entities": []}, {"text": "\"output\" means the unit of prediction; \"input\" shows information used as inputs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the corpus.", "labels": [], "entities": []}, {"text": " Table 2: F1 score of each models. Average is a weighted average.  \u2020 expresses the best result in the  word-based models for each entity category. Bold means the best result in all models for each entity  category. \"output\" means the unit of prediction; \"input\" shows information used as inputs.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9796993434429169}]}, {"text": " Table 4: Number of entities with boundary con- flicts and that of entities extracted by Char- BLSTM-CRF.", "labels": [], "entities": [{"text": "BLSTM-CRF", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.6024345755577087}]}]}