{"title": [{"text": "Joint UD Parsing of Norwegian Bokm\u00e5l and Nynorsk", "labels": [], "entities": [{"text": "UD Parsing", "start_pos": 6, "end_pos": 16, "type": "TASK", "confidence": 0.6686480343341827}, {"text": "Norwegian Bokm\u00e5l", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.7078326642513275}, {"text": "Nynorsk", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.662760317325592}]}], "abstractContent": [{"text": "This paper investigates interactions in parser performance for the two official standards for written Norwegian: Bokm\u00e5l and Nynorsk.", "labels": [], "entities": [{"text": "Bokm\u00e5l", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.9771484136581421}, {"text": "Nynorsk", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.8865628242492676}]}, {"text": "We demonstrate that while applying models across standards yields poor performance, combining the training data for both standards yields better results than previously achieved for each of them in isolation.", "labels": [], "entities": []}, {"text": "This has immediate practical value for processing Norwegian, as it means that a single parsing pipeline is sufficient to cover both varieties, with no loss inaccuracy.", "labels": [], "entities": []}, {"text": "Based on the Norwegian Universal Dependencies treebank we present results for multiple taggers and parsers, experimenting with different ways of varying the training data given to the learners, including the use of machine translation.", "labels": [], "entities": [{"text": "Norwegian Universal Dependencies treebank", "start_pos": 13, "end_pos": 54, "type": "DATASET", "confidence": 0.9214727580547333}, {"text": "machine translation", "start_pos": 215, "end_pos": 234, "type": "TASK", "confidence": 0.7131841033697128}]}], "introductionContent": [{"text": "There are two official written standards of the Norwegian language; Bokm\u00e5l (literally 'book tongue') and Nynorsk (literally 'new Norwegian').", "labels": [], "entities": [{"text": "Bokm\u00e5l", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.8078681230545044}]}, {"text": "While Bokm\u00e5l is the main variety, roughly 15% of the Norwegian population uses Nynorsk.", "labels": [], "entities": [{"text": "Bokm\u00e5l", "start_pos": 6, "end_pos": 12, "type": "DATASET", "confidence": 0.9450345635414124}]}, {"text": "However, language legislation specifies that minimally 25% of the written public service information should be in Nynorsk.", "labels": [], "entities": [{"text": "Nynorsk", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9353464841842651}]}, {"text": "The same minimum ratio applies to the programming of the Norwegian Public Broadcasting Corporation (NRK).", "labels": [], "entities": [{"text": "Norwegian Public Broadcasting Corporation (NRK)", "start_pos": 57, "end_pos": 104, "type": "DATASET", "confidence": 0.8790027584348407}]}, {"text": "The two varieties are so closely related that they may in practice be regarded as 'written dialects'.", "labels": [], "entities": []}, {"text": "However, lexically there can be relatively large differences.", "labels": [], "entities": []}, {"text": "shows an example sentence in both Bokm\u00e5l and Nynorsk.", "labels": [], "entities": [{"text": "Bokm\u00e5l", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.9786020517349243}, {"text": "Nynorsk", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8573494553565979}]}, {"text": "While the word order is identical and many of the words are clearly related, we see that only 2 out of 9 word forms are identical.", "labels": [], "entities": []}, {"text": "When quantifying the degree of lexical overlap with respect to the treebank data we will be using, (Section 3) we find that out of the 6741 non-punctuation word forms in the Nynorsk development set, 4152, or 61.6%, of these are unknown when measured against the Bokm\u00e5l training set.", "labels": [], "entities": [{"text": "Nynorsk development set", "start_pos": 174, "end_pos": 197, "type": "DATASET", "confidence": 0.9517402251561483}, {"text": "Bokm\u00e5l training set", "start_pos": 262, "end_pos": 281, "type": "DATASET", "confidence": 0.9823537667592367}]}, {"text": "For comparison, the corresponding proportion of unknown word forms in the Bokm\u00e5l development set is 36.3%.", "labels": [], "entities": [{"text": "Bokm\u00e5l development set", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9770787358283997}]}, {"text": "These lexical differences are largely caused by differences in productive inflectional forms, as well as highly frequent functional words like pronouns and determiners.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that Bokm\u00e5l and Nynorsk are different enough that parsers trained on data fora given standard alone cannot be applied to the other standard without avast drop inaccuracy.", "labels": [], "entities": [{"text": "Bokm\u00e5l", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.9639779925346375}]}, {"text": "At the same time, we demonstrate that they are similar enough that mixing the training data for both standards yields better performance.", "labels": [], "entities": []}, {"text": "This also reduces the complexity required for parsing Norwegian, in that a single pipeline is enough.", "labels": [], "entities": [{"text": "parsing Norwegian", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8537039756774902}]}, {"text": "When processing mixed texts (as is typically the casein any real-world setting), the alternatives are to either (a) maintain two distinct pipelines and select the right one by applying an initial step of language identification (for each document, say), or (b) use a single-standard pipeline only and accept a substantial loss inaccuracy (on the order of 20-25 percentage points in LAS and 15 points in tagger accuracy) whenever text of the non-matched standard is encountered.", "labels": [], "entities": [{"text": "language identification", "start_pos": 204, "end_pos": 227, "type": "TASK", "confidence": 0.6920981407165527}, {"text": "accuracy", "start_pos": 410, "end_pos": 418, "type": "METRIC", "confidence": 0.9407421350479126}]}, {"text": "In addition to simply combining the labeled training data as is, we also assess the feasibility of applying machine translation to increase the amount of available data for each variety.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7565263211727142}]}, {"text": "All final models and data sets used in this paper are made available online.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section briefly outlines some key components of our experimental setup.", "labels": [], "entities": []}, {"text": "We will be reporting results of two pipelines for tagging and parsing -one based on TnT and Mate and one based on UDPipe -described in the following.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.6060310304164886}]}, {"text": "TnT & Mate The widely used TnT tagger), implementing a 2nd order Markov model, achieves high accuracy as well as very high speed.", "labels": [], "entities": [{"text": "TnT tagger", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.6362271308898926}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9990869760513306}, {"text": "speed", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9794937968254089}]}, {"text": "TnT was used by when evaluating the proposed universal tag set.", "labels": [], "entities": [{"text": "TnT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.519105076789856}]}, {"text": "found the Mate dependency parser to have the best performance for parsing of NDT, and recent dependency parser comparisons () have also found Mate to perform very well for English.", "labels": [], "entities": [{"text": "Mate dependency parser", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6940766374270121}, {"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9721316695213318}, {"text": "NDT", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.7557451725006104}]}, {"text": "The fast training time of Mate also facilitates rapid experimentation.", "labels": [], "entities": [{"text": "Mate", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9058119654655457}]}, {"text": "Mate implements the secondorder maximum spanning tree dependency parsing algorithm of Carreras with the passiveaggressive perceptron algorithm of implemented with a hash kernel for faster processing times.", "labels": [], "entities": [{"text": "maximum spanning tree dependency parsing", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.7359040737152099}]}, {"text": "UDPipe UDPipe () provides an open-source C++ implementation of an entire end-to-end pipeline for dependency parsing.", "labels": [], "entities": [{"text": "UDPipe UDPipe", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8486148118972778}, {"text": "dependency parsing", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8561466336250305}]}, {"text": "All components are trainable and default settings are provided based on tuning towards the UD treebanks.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9370781183242798}]}, {"text": "The two components of UDPipe used in our experiments comprise the MorphoDiTa tagger () and the Parsito parser (.", "labels": [], "entities": []}, {"text": "MorphoDiTa implements an averaged perceptron algorithm) while Parsito is a greedy transition-based parser based on the neural network classifier described by.", "labels": [], "entities": []}, {"text": "When training the components, we use the same parametrization as reported in after tuning the parser for version 1.2 of the Bokm\u00e5l UD data.", "labels": [], "entities": [{"text": "Bokm\u00e5l UD data", "start_pos": 124, "end_pos": 138, "type": "DATASET", "confidence": 0.9765970309575399}]}, {"text": "For the parser, this includes form embeddings of dimension 50, PoS tag, FEATS and arc label embeddings of dimension 20, and a 200-node hidden layer.", "labels": [], "entities": [{"text": "PoS tag", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.7884556949138641}, {"text": "FEATS", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9853844046592712}]}, {"text": "For each experiment, we pre-train the form embeddings on the training data (i.e., the raw text of whatever portion of the labeled training data is used fora given experiment) using word2vec (, again with the same parameters as reported by fora skipgram model with a window often context words.", "labels": [], "entities": []}, {"text": "Parser training on predicted tags All parsers evaluated in this paper are both tested and trained using PoS tags predicted by a tagger rather than gold tags.", "labels": [], "entities": []}, {"text": "Training on predicted tags makes the training set-up correspond more closely to a realistic test setting and makes it possible for the parser to adapt to errors made by the tagger.", "labels": [], "entities": []}, {"text": "While this is often achieved using jackknifing (n-fold training and tagging of the labeled training data), we here simply apply the taggers to the very same data they have been trained on, reflecting the 'training error' of the taggers.", "labels": [], "entities": []}, {"text": "We have found that training on such 'silver-standard' tags improves parsing scores substantially compared to training on gold tags.", "labels": [], "entities": [{"text": "parsing", "start_pos": 68, "end_pos": 75, "type": "TASK", "confidence": 0.9649193286895752}]}, {"text": "In fact, also found that this set-up actually yields higher parsing scores compared to 10-fold tagging of the training data.", "labels": [], "entities": []}, {"text": "Of course, the test sets for which we evaluate the performance is still unseen data for the taggers.", "labels": [], "entities": []}, {"text": "Data split For Bokm\u00e5l we use the same split for training, development and testing as defined for NDT by.", "labels": [], "entities": [{"text": "Bokm\u00e5l", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9687336087226868}, {"text": "NDT", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.8792794346809387}]}, {"text": "As no pre-defined split was established for Nynorsk we defined this ourselves, following the same 80-10-10 proportions and also taking care to preserve contiguous texts in the various sections while also keeping them balanced in terms of genre.", "labels": [], "entities": [{"text": "Nynorsk", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9054398536682129}]}, {"text": "Evaluation The taggers are evaluated in terms of tagging accuracy (Acc in the following tables) while the parsers are evaluated by labeled and unlabeled attachment score (LAS and UAS).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.97502601146698}, {"text": "Acc", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9924935102462769}, {"text": "attachment score (LAS and UAS)", "start_pos": 153, "end_pos": 183, "type": "METRIC", "confidence": 0.6468392014503479}]}, {"text": "For the TnT tagger, accuracy is computed with the tnt-diff script of the TnT-distribution, and scores are computed over the base PoS tags, disregarding morphological features.", "labels": [], "entities": [{"text": "TnT tagger", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.5630924850702286}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9993446469306946}]}, {"text": "Mate is evaluated using the MaltEval tool.", "labels": [], "entities": []}, {"text": "For the second pipeline, we rely on UDPipe's built-in evaluation support, which also implements MaltEval.", "labels": [], "entities": []}, {"text": "The 'cross-standard' experiments in Section 5.1 showed that models trained on labeled data for one of the two varieties of written Norwegian perform poorly when applied to the other.", "labels": [], "entities": [{"text": "Section 5.1", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.864197701215744}]}, {"text": "For all tested configurations, we observe a loss of between 20 and 25 percentage points in labeled attachment score compared to training and testing on one and the same variety.", "labels": [], "entities": [{"text": "labeled attachment score", "start_pos": 91, "end_pos": 115, "type": "METRIC", "confidence": 0.6478992998600006}]}, {"text": "At the same time, it is important to realize that the results for 'within-standard' processing of either the Bokm\u00e5l or Nynorsk treebank data in isolation, correspond to an idealized setting that is not representative of how written Norwegian is encountered 'in the wild'.", "labels": [], "entities": [{"text": "Bokm\u00e5l or Nynorsk treebank data", "start_pos": 109, "end_pos": 140, "type": "DATASET", "confidence": 0.8430039763450623}]}, {"text": "In the news sources, blogs, government reports and parliament transcripts that form the basis for the treebank, both varieties of Norwegian will occur, intermixed.", "labels": [], "entities": []}, {"text": "In practice, this means that the actual parsing results can be expected to lie somewhere in between the extremes reported in.", "labels": [], "entities": [{"text": "parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.9747826457023621}]}, {"text": "Of course, a language identification module could be trained and applied as a pre-processing step for selecting the appropriate model, but in practice it would be much more convenient if we were able to have a single model that could process both varieties equally well.", "labels": [], "entities": [{"text": "language identification", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.723753422498703}]}, {"text": "In the next section, we look into various ways of mixing training data for the two written standards of Norwegian in order to create improved models for cross-standard joint processing.", "labels": [], "entities": []}, {"text": "Moreover, given the empirical indications in Section 5.2 that more labeled training data could benefit the taggers and parsers, this strategy is also motivated by wanting to improve the absolute results for each standard in isolation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the UD development data for  tagging and parsing the two written standards for  Norwegian, Bokm\u00e5l (BM) and Nynorsk (NN), in- cluding 'cross-standard' training and testing.", "labels": [], "entities": [{"text": "UD development data", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.7703087329864502}, {"text": "tagging and parsing", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.68618377049764}, {"text": "Bokm\u00e5l (BM) and Nynorsk (NN)", "start_pos": 112, "end_pos": 140, "type": "DATASET", "confidence": 0.8367405931154887}]}, {"text": " Table 3: Held-out test results for Norwegian Bokm\u00e5l and Nynorsk tagged and parsed with TnT+Mate  and UDPipe, using either the Bokm\u00e5l or Nynorsk training data alone (rows BM or NN), Bokm\u00e5l and  Nynorsk mixed (BM+NN), or Bokm\u00e5l and Nynorsk combined with machine-translated data, i.e., the  original versions of both varieties as well as the translations of each into the other (BM+NN+MT).", "labels": [], "entities": [{"text": "Bokm\u00e5l or Nynorsk training data", "start_pos": 127, "end_pos": 158, "type": "DATASET", "confidence": 0.6413509905338287}]}]}