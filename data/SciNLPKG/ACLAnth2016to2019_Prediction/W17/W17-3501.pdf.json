{"title": [{"text": "Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation", "labels": [], "entities": [{"text": "Linguistic realisation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.715492844581604}, {"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.736298069357872}, {"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9550014734268188}, {"text": "AMR-to-text generation", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.9698043465614319}]}], "abstractContent": [{"text": "In this paper, we study AMR-to-text generation , framing it as a translation task and comparing two different MT approaches (Phrase-based and Neural MT).", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.9948241412639618}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.971028208732605}]}, {"text": "We systematically study the effects of 3 AMR preprocess-ing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase.", "labels": [], "entities": [{"text": "Compression", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9709407687187195}, {"text": "MT phase", "start_pos": 136, "end_pos": 144, "type": "TASK", "confidence": 0.8906023800373077}]}, {"text": "Our results show that preprocessing indeed helps, although the benefits differ for the two MT models.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9141449332237244}]}, {"text": "The implementation of the models are publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) is the process of generating coherent natural language text from non-linguistic data).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7795594533284506}]}, {"text": "While there is broad consensus among NLG scholars on the output of NLG systems (i.e., text), there is far less agreement on what the input should be; see fora recent review.", "labels": [], "entities": []}, {"text": "Over the years, NLG systems have taken a wide range of inputs, including for example images (, numeric data () and semantic representations ().", "labels": [], "entities": []}, {"text": "This study focuses on generating natural language based on Abstract Meaning Representations (AMRs) (.", "labels": [], "entities": [{"text": "Abstract Meaning Representations (AMRs)", "start_pos": 59, "end_pos": 98, "type": "TASK", "confidence": 0.6869016190369924}]}, {"text": "AMRs encode the meaning of a sentence as a rooted, directed and acyclic graph, where nodes represent concepts, and labeled directed edges represent relations among these concepts.", "labels": [], "entities": []}, {"text": "The formalism strongly relies on the PropBank notation.", "labels": [], "entities": [{"text": "PropBank notation", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.9235005676746368}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: MT scores for the evaluated models by the  size of the training data. Best baseline, PBMT and  NMT results were underlined.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9749757051467896}]}]}