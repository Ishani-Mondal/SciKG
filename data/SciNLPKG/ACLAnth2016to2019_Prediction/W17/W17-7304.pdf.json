{"title": [{"text": "Full-Network Embedding in a Multimodal Embedding Pipeline", "labels": [], "entities": []}], "abstractContent": [{"text": "The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space.", "labels": [], "entities": [{"text": "image annotation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7109746336936951}, {"text": "image retrieval tasks", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7701552311579386}]}, {"text": "In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme.", "labels": [], "entities": []}, {"text": "Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations.", "labels": [], "entities": []}, {"text": "To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art.", "labels": [], "entities": []}, {"text": "Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding.", "labels": [], "entities": [{"text": "image retrieval tasks", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8335498968760172}]}, {"text": "These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Image annotation (also known as caption retrieval) is the task of automatically associating an input image with a describing text.", "labels": [], "entities": [{"text": "caption retrieval)", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8771731853485107}]}, {"text": "Image annotation methods are an emerging technology, enabling semantic image indexing and search applications.", "labels": [], "entities": [{"text": "Image annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7595947980880737}, {"text": "semantic image indexing", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.6143226623535156}]}, {"text": "The complementary task of associating an input text with a fitting image (known as image retrieval or image search) is also of relevance for the same sort of applications.", "labels": [], "entities": [{"text": "image retrieval or image search", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.7349617063999176}]}, {"text": "State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space.", "labels": [], "entities": []}, {"text": "While several techniques for merging both spaces have been proposed, little effort has been made in finding the most appropriate image embeddings to be used in that process.", "labels": [], "entities": []}, {"text": "In fact, most approaches simply use a one-layer CNN embedding.", "labels": [], "entities": []}, {"text": "In this paper we explore the impact of using a Full-Network embedding (FNE) to generate the required image embedding, replacing the one-layer embedding.", "labels": [], "entities": []}, {"text": "We do so by integrating the FNE into the multimodal embedding pipeline defined by, which is based in the use of a Gated Recurrent Units neural network (GRU) for text encoding and CNN for image encoding.", "labels": [], "entities": [{"text": "FNE", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.7116600871086121}, {"text": "text encoding", "start_pos": 161, "end_pos": 174, "type": "TASK", "confidence": 0.750434935092926}, {"text": "image encoding", "start_pos": 187, "end_pos": 201, "type": "TASK", "confidence": 0.7145076394081116}]}, {"text": "Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality.", "labels": [], "entities": []}, {"text": "These particularities result in a richer visual embedding space, which maybe more reliably mapped to a common visual-textual embedding space.", "labels": [], "entities": []}, {"text": "The generic pipeline defined by Kiros et al. has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks.", "labels": [], "entities": [{"text": "image annotation and image search tasks", "start_pos": 79, "end_pos": 118, "type": "TASK", "confidence": 0.7199574361244837}]}, {"text": "We choose to test our contribution on this pipeline for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation).", "labels": [], "entities": [{"text": "caption generation", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.9210148751735687}]}, {"text": "This assumption would be dimmer if a more problem-specific methodology was chosen instead.", "labels": [], "entities": []}, {"text": "Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks.", "labels": [], "entities": [{"text": "FNE", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.905232846736908}, {"text": "caption related tasks", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.9183182319005331}]}, {"text": "We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k, Flickr30k and MSCOCO.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7583412826061249}, {"text": "Flickr30k", "start_pos": 167, "end_pos": 176, "type": "DATASET", "confidence": 0.7218850255012512}, {"text": "MSCOCO", "start_pos": 181, "end_pos": 187, "type": "DATASET", "confidence": 0.9027251601219177}]}, {"text": "Results obtained by the pipeline including the FNE are compared with the original pipeline of [1] using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets.", "labels": [], "entities": [{"text": "FNE", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.6594755053520203}]}], "datasetContent": [{"text": "In this section we evaluate the impact of using the FNE in a multimodal pipeline (FN-MME) for both image annotation and image retrieval tasks.", "labels": [], "entities": [{"text": "FNE", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8316119909286499}, {"text": "image retrieval", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7358412444591522}]}, {"text": "To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the original multimodal pipeline reported by Kiros et al.", "labels": [], "entities": [{"text": "FNE", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.5765616297721863}, {"text": "FN-MME", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.7837727069854736}]}, {"text": "Additionally, we define a second baseline by using the original multimodal pipeline with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.).", "labels": [], "entities": [{"text": "FNE experiments", "start_pos": 145, "end_pos": 160, "type": "DATASET", "confidence": 0.9032137095928192}]}, {"text": "We refer to this second baseline as CNN-MME*.", "labels": [], "entities": [{"text": "CNN-MME", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8860117793083191}]}, {"text": "In our experiments we use three different publicly available datasets: The Flickr8K dataset contains 8,000 hand-selected images from Flickr, depicting actions and events.", "labels": [], "entities": [{"text": "Flickr8K dataset", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.9703018367290497}, {"text": "Flickr", "start_pos": 133, "end_pos": 139, "type": "DATASET", "confidence": 0.884487509727478}]}, {"text": "Five correct captions are provided for each image.", "labels": [], "entities": []}, {"text": "Following the provided splits, 6,000 images are used for train, 1,000 are used in validation and 1,000 more are kept for testing.", "labels": [], "entities": []}, {"text": "The Flickr30K dataset is an extension of Flickr8K.", "labels": [], "entities": [{"text": "Flickr30K dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.979633629322052}, {"text": "Flickr8K", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9787786602973938}]}, {"text": "It contains 31,783 photographs of everyday activities, events and scenes.", "labels": [], "entities": []}, {"text": "Five correct captions are provided for each image.", "labels": [], "entities": []}, {"text": "In our experiments 29,000 images are used for training, 1,014 conform the validation set and 1,000 are kept for test.", "labels": [], "entities": []}, {"text": "These splits are the same ones used by Kiros et al. and by Karpathy and Fei-Fei.", "labels": [], "entities": []}, {"text": "The MSCOCO dataset includes images of everyday scenes containing common objects in their natural context.", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9158566892147064}]}, {"text": "For captioning, 82,783 images and 413,915 captions are available for training, while 40,504 images and 202,520 captions are available for validation.", "labels": [], "entities": [{"text": "captioning", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.964889645576477}]}, {"text": "Captions from the test set are not publicly available.", "labels": [], "entities": []}, {"text": "Previous contributions consider using a subset of the validation set for validation and a different subset for test.", "labels": [], "entities": []}, {"text": "In most cases, such subsets are composed by either 1,000 or 5,000 images for each set, with their corresponding 5 captions per image.", "labels": [], "entities": []}, {"text": "In our experiments we consider both settings.", "labels": [], "entities": []}, {"text": "The caption sentences are word-tokenized using the Natural Language Toolkit (NLTK) for Python.", "labels": [], "entities": []}, {"text": "The choice of the word embedding size and the number of GRUs has been analyzed to obtain a range of suitable parameters to test in the validation set.", "labels": [], "entities": []}, {"text": "The total number of different words is 8,919 for Flickr8k, 22,962 for Flickr30k and 32,775 for MSCOCO.", "labels": [], "entities": [{"text": "Flickr8k", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9546104669570923}, {"text": "Flickr30k", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.9527734518051147}, {"text": "MSCOCO", "start_pos": 95, "end_pos": 101, "type": "DATASET", "confidence": 0.9653475880622864}]}, {"text": "Using all the words present in the dataset is likely to produce overfitting problems when training on examples containing words that only occur a few times.", "labels": [], "entities": []}, {"text": "This overfitting problem may not have a huge impact on performance, but it may add undesired noise in the multimodal representation.", "labels": [], "entities": []}, {"text": "The original setup limited the word embedding to the 300 most frequent words, while using 300 GRUs.", "labels": [], "entities": []}, {"text": "The Bi-LSTM model in contrast defines the vocabulary size to include words appearing more than 5 time in the dataset, leading to dictionaries of size 2,018 for Flickr8k, 7,400 for Flickr30k and 8,801 for MSCOCO.", "labels": [], "entities": [{"text": "Flickr8k", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9043663144111633}, {"text": "Flickr30k", "start_pos": 180, "end_pos": 189, "type": "DATASET", "confidence": 0.9426603317260742}, {"text": "MSCOCO", "start_pos": 204, "end_pos": 210, "type": "DATASET", "confidence": 0.9366702437400818}]}, {"text": "Our own preliminary experiments on the validation set showed that increasing multimodal space dimensionality and dictionary length slightly improved the performance of image retrieval, in detriment of image annotation.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 168, "end_pos": 183, "type": "TASK", "confidence": 0.7516940236091614}]}, {"text": "However, the combined performance difference remains rather small when using non-extreme parameter values (e.g., a model with 10,000 words vocabulary on MSCOCO dataset show a 0.4% average recall reduction when compared with a 2,000 words model).", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 153, "end_pos": 167, "type": "DATASET", "confidence": 0.9747959077358246}, {"text": "recall reduction", "start_pos": 188, "end_pos": 204, "type": "METRIC", "confidence": 0.9864790141582489}]}, {"text": "Since we are building a model for solving both tasks, we kept the parameters obtaining the highest combined score in the validation set.", "labels": [], "entities": []}, {"text": "For the Flickr datasets, the word embedding is limited to the 1,000 most frequent words.", "labels": [], "entities": [{"text": "Flickr datasets", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9808588027954102}]}, {"text": "For the MSCOCO dataset, we use a larger dictionary, considering the 2,000 most frequent words.", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.9451811611652374}]}, {"text": "In both cases we use 2,048 GRUs, which is also the dimensionality of the resultant multimodal embedding space.", "labels": [], "entities": []}, {"text": "For generating the image embedding we use the classical VGG16 CNN architecture as source model pretrained for ImageNet.", "labels": [], "entities": [{"text": "VGG16 CNN architecture", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9477385679880778}]}, {"text": "This architecture is composed of 16 convolutional layers combined with pooling layers followed by two fully connected layers and the final softmax output layer.", "labels": [], "entities": []}, {"text": "When using the FNE, this results in a image embedding space of 12,416 dimensions.", "labels": [], "entities": [{"text": "FNE", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9436827301979065}]}, {"text": "On all our experiments (both the CNN-MME* and the FN-MME) the margin parameter \u03b1 is set to 0.2, and the batch size to 128 image-caption pairs.", "labels": [], "entities": [{"text": "FN-MME", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8421093821525574}, {"text": "margin parameter \u03b1", "start_pos": 62, "end_pos": 80, "type": "METRIC", "confidence": 0.9576492110888163}]}, {"text": "Within the same batch, every possible alternative image-caption pair is used as contrasting example.", "labels": [], "entities": []}, {"text": "The models are trained up to 25 epochs, and the best performing model on the validation set is chosen (i.e., early stopping).", "labels": [], "entities": []}, {"text": "We use gradient clipping for the GRUs with a threshold of 2.", "labels": [], "entities": [{"text": "GRUs", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.530931830406189}]}, {"text": "We use ADAM as optimization algorithm, with a learning rate of 0.0002 for the Flickr datasets, and 0.00025 for MSCOCO.", "labels": [], "entities": [{"text": "Flickr datasets", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9764192700386047}, {"text": "MSCOCO", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.9424448013305664}]}, {"text": "To evaluate both image annotation and image retrieval we use the following metrics: \u2022 Recall@K (R@K) is the fraction of images for which a correct caption is ranked within the top-K retrieved results (and vice-versa for sentences).", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7548886835575104}, {"text": "Recall@K (R@K)", "start_pos": 86, "end_pos": 100, "type": "METRIC", "confidence": 0.9221241325139999}]}, {"text": "Results are provided for R@1, R@5 and R@10.", "labels": [], "entities": [{"text": "R", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.7738117575645447}]}, {"text": "\u2022 Median rank (Med r) of the highest ranked ground truth result.", "labels": [], "entities": [{"text": "Median rank (Med r)", "start_pos": 2, "end_pos": 21, "type": "METRIC", "confidence": 0.8926327427228292}]}], "tableCaptions": [{"text": " Table 1: Results obtained for the Flickr8 dataset. R@K is Recall@K (high is good). Med r is Median  rank (low is good). Best results are shown in bold.", "labels": [], "entities": [{"text": "Flickr8 dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9824846088886261}, {"text": "Recall@K", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9527059197425842}]}, {"text": " Table 2: Results obtained for the Flickr30 dataset. R@K is Recall@K (high is good). Med r is Median  rank (low is good). Best results are shown in bold.", "labels": [], "entities": [{"text": "Flickr30 dataset", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9843250513076782}, {"text": "Recall@K", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9543830951054891}]}, {"text": " Table 3: Results obtained for the MSCOCO dataset. Top part shows results when using 1,000 images  as test set, while the bottom shows results when using 5,000 images for test. R@K is Recall@K (high  is good). Med r is Median rank (low is good). Best results are shown in bold.  \u2020 results not in original  paper.", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9156801104545593}, {"text": "Recall@K", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9415811697642008}]}]}