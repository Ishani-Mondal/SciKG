{"title": [{"text": "Using hyperlinks to improve multilingual partial parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "Syntactic annotation is costly and not available for the vast majority of the world's languages.", "labels": [], "entities": [{"text": "Syntactic annotation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8808566331863403}]}, {"text": "We show that sometimes we can do away with less labeled data by exploiting more readily available forms of markup.", "labels": [], "entities": []}, {"text": "Specifically, we re-visit an idea from Valentin Spitkovsky's work (2010), namely that hyperlinks typically bracket syntactic constituents or chunks.", "labels": [], "entities": []}, {"text": "We strengthen his results by showing that not only can hyperlinks help in low resource scenarios, exemplified hereby Quechua, but learning from hy-perlinks can also improve state-of-the-art NLP models for English newswire.", "labels": [], "entities": []}, {"text": "We also present out-of-domain evaluation on English Ontonotes 4.0.", "labels": [], "entities": [{"text": "English Ontonotes 4.0", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.8803769946098328}]}], "introductionContent": [{"text": "Syntactic analysis can be used to improve knowledge extraction, speech synthesis, machine translation, and error correction, for example, but the quality of syntactic parsers relies heavily on the quality and amount of available annotated data.", "labels": [], "entities": [{"text": "Syntactic analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.780973494052887}, {"text": "knowledge extraction", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7334691286087036}, {"text": "speech synthesis", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7688633799552917}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8017111420631409}, {"text": "error correction", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.6926634311676025}]}, {"text": "This holds in particular for full syntactic parsing, but even for more robust partial parsers, good models require large and representative, annotated corpora.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.6273253709077835}]}, {"text": "Such annotated corpora are costly to produce and generally not available for the vast majority of the world's languages.", "labels": [], "entities": []}, {"text": "Even for English, resources are limited, and state-of-the-art parsers for English newswire are trained on 30 years old newswire from a single newspaper.", "labels": [], "entities": []}, {"text": "When evaluated on more recent newswire or other newspapers, we observe significant performance drops.", "labels": [], "entities": []}, {"text": "This is a combination of overfitting and data scarcity.", "labels": [], "entities": []}, {"text": "While more annotated resources can improve this situation, annotation does not seem to scale with our needs for automated syntactic analysis, or with the rapid development of modern languages like English.", "labels": [], "entities": [{"text": "automated syntactic analysis", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.6839779019355774}]}, {"text": "Hence, we have to consider other types of data to adapt our models to other varieties of newswire, or of language, more generally.", "labels": [], "entities": []}, {"text": "Using (more representative) raw text in combination with (less representative) annotated data to do semi-supervised learning is challenging, but occasionally successful.", "labels": [], "entities": []}, {"text": "In this paper, we consider an equally readily available, potential source of weak supervision, namely hypertext.", "labels": [], "entities": []}, {"text": "Consider the following hypertext: The violence, which has already been called some evocative names --<href>intifada<\\ href>, <href>jihad<\\href>, <href>jihad<guerilla war<\\href>, <href>insurrection<\\href>, <href>rebellion<\\href>, and <href>civil war<\\href> --prompts several reflections.", "labels": [], "entities": []}, {"text": "This sentence is a random sentence taken from the Internet.", "labels": [], "entities": []}, {"text": "The mark-up is hyperlinks, referring the reader to related websites.", "labels": [], "entities": []}, {"text": "The hyperlinks mark passsages of the text highlighting the topics of the linked websites.", "labels": [], "entities": [{"text": "passsages", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9829209446907043}]}, {"text": "The marked passages are intifada, jihad, guerilla war, insurrection, rebellion and civil war.", "labels": [], "entities": []}, {"text": "Note that these are not just words, but also phrases.", "labels": [], "entities": []}, {"text": "In this example, they are all noun phrases.", "labels": [], "entities": []}, {"text": "also looked at hyperlinks and observed that the vast majority of marked passages were syntactic constituents such as noun and verb phrases.", "labels": [], "entities": []}, {"text": "He then went onto show that this data is potentially useful for unsupervised induction of dependency parsers.", "labels": [], "entities": [{"text": "induction of dependency parsers", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.6406155228614807}]}, {"text": "We build directly on this work, but goon to show that hyperlinks are not just useful for unsupervised induction of NLP models.", "labels": [], "entities": []}, {"text": "It is also possible to improve state-of-the-art supervised NLP models, by jointly learning to predict hyperlinks from raw HTML files.", "labels": [], "entities": []}, {"text": "Specifically, we show that hard parameter sharing of hidden layers with a deep bi-LSTM model for predicting hyperlinks is an efficient regularizer for several state-of-the-art NLP models.", "labels": [], "entities": []}, {"text": "Contributions Our contributions are as follows: (a) We revisit the idea of using raw HTML data for weak supervision of NLP models.", "labels": [], "entities": []}, {"text": "(b) We show that multi-task learning with hyperlink prediction as an auxiliary task improves performance across three tasks: syntactic chunking, semantic supersense tagging, and CCG supertagging.", "labels": [], "entities": [{"text": "semantic supersense tagging", "start_pos": 145, "end_pos": 172, "type": "TASK", "confidence": 0.6394596099853516}]}, {"text": "We also see improvements on out-of-domain English data, as well as in experiments with syntactic chunking with hyperlinks for Quechua.", "labels": [], "entities": []}, {"text": "Related work Hard parameter sharing of hidden layers has become a popular approach to multi-task learning.", "labels": [], "entities": []}, {"text": "It was originally introduced in, but first applied in NLP in, and it was shown, empirically, to bean effective regularizer across two different NLP tasks in . Using more readily available data resources that are not annotated by linguists, but still carry linguistic signals, was previously explored by and.", "labels": [], "entities": []}, {"text": "Baxter shows, in the context of linear models, that if two problems, P and R, share optimal hypothesis classes, then the induction of a model from a sample of P can efficiently regularize the induction of a model from a sample of R.", "labels": [], "entities": []}, {"text": "This is too strong an assumption for our purposes, obviously, since even our label sets are different, but we also have more wiggle-room than heavily mean-constrained linear models, for example.", "labels": [], "entities": []}, {"text": "In fact, hidden layer sharing relaxes the above assumption quite a bit.", "labels": [], "entities": []}, {"text": "We do not need the optimal hypothesis classes to overlap.", "labels": [], "entities": []}, {"text": "Hidden layer sharing can work even with the optimal hypothesis classes of P and R distinct, if there is a joint representation such that P and R both become linearly separable.", "labels": [], "entities": []}, {"text": "Whether this is the case, is an empirical question.", "labels": [], "entities": []}], "datasetContent": [{"text": "Model Our model merges two deep recurrent neural networks through hard parameter sharing.", "labels": [], "entities": []}, {"text": "We use three-layered, bi-directional long short-term memory networks (LSTMs), in away similar to . We optimize hyper-parameters on development data for chunking in a single-task architecture, training for 10 epochs, and using a hidden layer size that is equal to the embedding layer size.", "labels": [], "entities": []}, {"text": "For English, we use the SENNA word embedding for English and 50-dimensional hidden layers.", "labels": [], "entities": []}, {"text": "For Quechua, we use Polyglot embeddings and 64-dimensional hidden layers.", "labels": [], "entities": []}, {"text": "Ina multi-task learning (MTL) setting, we have several prediction tasks over the same input space.", "labels": [], "entities": [{"text": "multi-task learning (MTL)", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6524949312210083}]}, {"text": "In our case, the input is the words in a sentence, and the different tasks are syntactic chunking, semantic supersense tagging and CCG supertagging.", "labels": [], "entities": [{"text": "semantic supersense tagging", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6626448333263397}]}, {"text": "Each task has its own output vocabulary (a task specific tagset), but all of them map any length n input sequence into a length n output sequence.", "labels": [], "entities": []}, {"text": "The most common approach to multi-task learning in NLP these days is to share parameters across most of the hidden layers of two or more single task networks.", "labels": [], "entities": []}, {"text": "In the k-layers deep bi-LSTM tagger described above this is naturally achieved by sharing the bi-LSTM part of the network across tasks, but training a specialized classification tagger ft (v k i ) for each task t.", "labels": [], "entities": []}, {"text": "Note that this particular kind of multi-task learning can also be cast as a kind of meanconstrained matrix regularization.", "labels": [], "entities": []}, {"text": "While in some sense, hard parameter sharing is more heavily regularized than more traditional approaches to multitask learning, such as mean-constrained L2 regularization, we obtain more wiggle room by only sharing the embedding and LSTM parameters.", "labels": [], "entities": []}, {"text": "Our model is implemented in pyCNN and made available at: English data In our English in-domain experiments, we use three datasets for our target tasks, namely the Penn Treebank for syntactic chunking (, the SemCor corpus for semantic supersense tagging), and the CCGBank 2 for CCG super-tagging.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.9933443665504456}, {"text": "syntactic chunking", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.6755809336900711}, {"text": "SemCor corpus", "start_pos": 207, "end_pos": 220, "type": "DATASET", "confidence": 0.7315566390752792}, {"text": "semantic supersense tagging", "start_pos": 225, "end_pos": 252, "type": "TASK", "confidence": 0.5731503168741862}]}, {"text": "See for an example of all three layers of annotation.", "labels": [], "entities": []}, {"text": "Balance between tasks Our auxiliary datasets are relatively small, in the light of hyperlinks being readily available.", "labels": [], "entities": []}, {"text": "In hard parameter sharing, it is important not to swamp the main task, and as our learning curve experiments indicate, it would not be beneficial to sample more auxiliary task data.", "labels": [], "entities": [{"text": "hard parameter sharing", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.6203131973743439}]}, {"text": "Soft parameter sharing approaches may better leverage large volumes of hyperlink data.", "labels": [], "entities": []}, {"text": "See discussion of learning curves in \u00a73.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Improvements in F 1 using hyperlinks as auxiliary data across three NLP tasks.  S\u00f8gaard and Goldberg (2016) for comparison (S&G16). S&G16 (best) is similar to our hyperlinks  model, but uses POS-tag annotated data for co-supervising the initial LSTM layer instead of hyper- links data for co-supervising all the hidden layers. Previous work on SemCor assumes gold-standard  POS tags and achieves up to 80% F 1 -score. We are not aware of previous work on Quechua.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 416, "end_pos": 426, "type": "METRIC", "confidence": 0.9896619021892548}]}, {"text": " Table 2: Small, but consistent improvement for domain adaptation for English chunking", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.814938098192215}, {"text": "English chunking", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.6864718645811081}]}]}