{"title": [{"text": "The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al.", "labels": [], "entities": [{"text": "RepEval 2017 Shared Task", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.5145846381783485}, {"text": "sentence representation learning", "start_pos": 96, "end_pos": 128, "type": "TASK", "confidence": 0.7483981847763062}]}, {"text": "All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al..", "labels": [], "entities": []}, {"text": "The best single model used stacked BiLSTMs with residual connections to extract sentence features and reached 74.5% accuracy on the genre-matched test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9992241859436035}]}, {"text": "Surprisingly, the results of the competition were fairly consistent across the genre-matched and genre-mismatched test sets, and across subsets of the test data representing a variety of linguistic phenomena , suggesting that all of the submitted systems learned reasonably domain-independent representations for sentence meaning.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Second Workshop on Evaluating Vector Space Representations for NLP (RepEval 2017) features a shared task competition meant to evaluate natural language understanding models based on sentence encoders-that is, models that transform sentences into fixed-length vector representations and reason using those representations.", "labels": [], "entities": [{"text": "Evaluating Vector Space Representations for NLP (RepEval 2017)", "start_pos": 23, "end_pos": 85, "type": "TASK", "confidence": 0.5399167865514756}]}, {"text": "Submitted systems are evaluated on the task of natural language inference (NLI, also known as recognizing textual entailment, or RTE) on the Multi-Genre NLI corpus (MultiNLI;.", "labels": [], "entities": []}, {"text": "Each example in the corpus consists of a pair of sentences, and systems must predict whether the relationship between the two sentences is entailment, neutral or contradiction in a balanced three-way classification setting.", "labels": [], "entities": []}, {"text": "We selected the task of NLI with the intent to evaluate as directly as possible the degree to which each model can extract and manipulate distributed representations of sentence meaning.", "labels": [], "entities": []}, {"text": "In order fora system to perform well at natural language inference, it needs to handle nearly the full complexity of natural language understanding, 1 but its framing as a sentence-pair classification problem makes it suitable as an evaluation task fora broad range of models, and avoids issues of sequence generation, structured prediction, or memory access that can complicate evaluation in other settings.", "labels": [], "entities": [{"text": "sequence generation", "start_pos": 298, "end_pos": 317, "type": "TASK", "confidence": 0.7109769880771637}, {"text": "structured prediction", "start_pos": 319, "end_pos": 340, "type": "TASK", "confidence": 0.6992902457714081}]}, {"text": "The shared task includes two evaluations, a standard in-domain (matched) evaluation in which the training and test data are drawn from the same sources, and a cross-domain (mismatched) evaluation in which the training and test data differ substantially.", "labels": [], "entities": []}, {"text": "This cross-domain evaluation tests the ability of submitted systems to learn representations of sentence meaning that capture broadly useful features.", "labels": [], "entities": []}, {"text": "This paper briefly introduces the task and dataset, presents the rules and results of the competition, and analyzes and compares the submitted systems.", "labels": [], "entities": []}, {"text": "All the submitted systems are broadly Met my first girlfriend that way.", "labels": [], "entities": []}], "datasetContent": [{"text": "MultiNLI ( consists of 393k pairs of sentences from abroad range of genres of written and spoken English, balanced across three labels.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8962715864181519}]}, {"text": "Each premise sentence (the first sentence in each pair) is derived from one often sources of text, which constitute the ten genre sections of the corpus.", "labels": [], "entities": []}, {"text": "Each hypothesis sentence and pair label was composed by a crowd worker in response to a premise.", "labels": [], "entities": []}, {"text": "MultiNLI was designed and collected in the style of the Stanford NLI Corpus (SNLI;), but covers a broader range of styles of text, rather than the relatively homogeneous captions used in SNLI.", "labels": [], "entities": [{"text": "Stanford NLI Corpus (SNLI", "start_pos": 56, "end_pos": 81, "type": "DATASET", "confidence": 0.9198244214057922}]}, {"text": "Testing and development sets are available for all genres, with 2000 examples per set per genre.", "labels": [], "entities": []}, {"text": "Only five genres have accompanying training sets.", "labels": [], "entities": []}, {"text": "So, for the matched development and test sets, models are tested on examples derived from the same sources as those in the training set, while for the mismatched sets, the text source is not repre-sented in the training data.", "labels": [], "entities": []}, {"text": "presents example sentences from the corpus and presents some key statistics.", "labels": [], "entities": []}, {"text": "For a detailed discussion of the corpus, refer to.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: RepEval 2017 shared task competition results. The Model Details column lists some of the key  strategies used in each system, using keywords: STACK: use of multilayer bidirectional RNNs, CHAR:  character-level embeddings, ENHEMB: embeddings enhanced with auxiliary features, POOL: max or  mean pooling over RNN states, ATTN: intra-sentence attention, PRODDIFF: elementwise sentence prod- uct and difference features in the final entailment classifier, SNLI: use of the SNLI training set.", "labels": [], "entities": [{"text": "RepEval 2017 shared task competition", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6846639156341553}, {"text": "STACK", "start_pos": 152, "end_pos": 157, "type": "METRIC", "confidence": 0.9762548208236694}, {"text": "ATTN", "start_pos": 329, "end_pos": 333, "type": "METRIC", "confidence": 0.8024088740348816}, {"text": "SNLI training set", "start_pos": 479, "end_pos": 496, "type": "DATASET", "confidence": 0.7427189548810323}]}, {"text": " Table 4: This table shows the accuracy of different models for each tagged subset of our 1,000-example  development set sample. The '(S)' indicates that results for the single model are shown. Some results  that stand out to us are shown in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9993065595626831}]}, {"text": " Table 5: A thousand sentences are randomly sam- pled from the matched test set and their pairwise  distances to all sentences in the test set (premises  and hypotheses) are calculated. This table shows  the percentage of times the first nearest neighbor  belongs to the same genre as the sample sentence.", "labels": [], "entities": []}]}