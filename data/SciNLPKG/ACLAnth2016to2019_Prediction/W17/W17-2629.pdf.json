{"title": [], "abstractContent": [{"text": "Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation.", "labels": [], "entities": [{"text": "image generation", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.7929709553718567}]}, {"text": "Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods.", "labels": [], "entities": []}, {"text": "In this paper, we take a step towards generating natural language with a GAN objective alone.", "labels": [], "entities": [{"text": "GAN", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9046609401702881}]}, {"text": "We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estima-tors and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset.", "labels": [], "entities": [{"text": "Chinese poem generation", "start_pos": 186, "end_pos": 209, "type": "TASK", "confidence": 0.5568039814631144}]}, {"text": "We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language mod-eling results.", "labels": [], "entities": []}, {"text": "A conditional version is also described that can generate sequences conditioned on sentence characteristics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep neural networks have recently enjoyed some success at modeling natural language (.", "labels": [], "entities": []}, {"text": "Typically, recurrent and convolutional language models are trained to maximize the likelihood of observing a word or character given the previous observations in the sequence P (w 1 . .", "labels": [], "entities": []}, {"text": "w n ) = p(w 1 ) n i=2 P (w i |w 1 . .", "labels": [], "entities": []}, {"text": "w i\u22121 ).", "labels": [], "entities": []}, {"text": "These models are commonly trained using a technique called teacher forcing where the inputs to the network are fixed and the model is trained to predict only the next * Indicates first authors.", "labels": [], "entities": []}, {"text": "Ordering determined by coin flip.", "labels": [], "entities": []}, {"text": "item in the sequence given all previous observations.", "labels": [], "entities": []}, {"text": "This corresponds to maximum-likelihood training of these models.", "labels": [], "entities": []}, {"text": "However this one-step ahead prediction during training makes the model prone to exposure bias (.", "labels": [], "entities": []}, {"text": "Exposure bias occurs when a model is only trained conditioned on groundtruth contexts and is not exposed to its own errors).", "labels": [], "entities": []}, {"text": "An important consequence to exposure bias is that generated sequences can degenerate as small errors accumulate.", "labels": [], "entities": []}, {"text": "Many important problems in NLP such as machine translation and abstractive summarization are trained via a maximum-likelihood training objective (, but require the generation of extended sequences and are evaluated based on sequence-level metrics such as BLEU () and ROUGE.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8470607101917267}, {"text": "abstractive summarization", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.6545977592468262}, {"text": "BLEU", "start_pos": 255, "end_pos": 259, "type": "METRIC", "confidence": 0.9979128241539001}, {"text": "ROUGE", "start_pos": 267, "end_pos": 272, "type": "METRIC", "confidence": 0.9684712290763855}]}, {"text": "One possible direction towards incorporating a sequence-level training objective is to use Generative Adversarial Networks (GANs) ().", "labels": [], "entities": []}, {"text": "While GANs have yielded impressive results for modeling images), advances in their use for natural language generation has lagged behind.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.7254102826118469}]}, {"text": "Some progress has been made recently in incorporating a GAN objective in sequence modeling problems including natural language generation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.6451779504617056}]}, {"text": "use an adversarial criterion to match the hidden state dynamics of a teacher forced recurrent neural network (RNN) and one that samples from its own output distribution across multiple time steps.", "labels": [], "entities": []}, {"text": "Unlike the approach in , sequence GANs ( and maximum-likelihood augmented GANs () use an adversarial loss at outputs of an RNN.", "labels": [], "entities": []}, {"text": "Using a GAN at the outputs of an RNN however isn't trivial since sampling from these outputs to feed to the discrimi-nator is a non-differentiable operation.", "labels": [], "entities": []}, {"text": "As a result gradients cannot propagate to the generator from the discriminator.", "labels": [], "entities": []}, {"text": "use policy gradient to estimate the generator's gradient and () present an importance sampling based technique.", "labels": [], "entities": []}, {"text": "Other alternatives include RE-INFORCE, the use of a Gumbel softmax () and the straighthrough estimator () among others.", "labels": [], "entities": [{"text": "RE-INFORCE", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9936518669128418}]}, {"text": "In this work, we address the discrete output space problem by simply forcing the discriminator to operate on continuous valued output distributions.", "labels": [], "entities": []}, {"text": "The discriminator sees a sequence of probabilities over every token in the vocabulary from the generator and a sequence of 1-hot vectors from the true data distribution as in.", "labels": [], "entities": []}, {"text": "This technique is identical to that proposed by, which is parallel work to this.", "labels": [], "entities": []}, {"text": "In this paper we provide a more complete empirical investigation of this approach to applying GANs to discrete output spaces.", "labels": [], "entities": [{"text": "GANs", "start_pos": 94, "end_pos": 98, "type": "TASK", "confidence": 0.9551252126693726}]}, {"text": "We present results using recurrent as well as convolutional architectures on three language modeling datasets of different sizes at the word and character-level.", "labels": [], "entities": []}, {"text": "We also present quantitative results on generating sentences that adhere to a simple context-free grammar (CFG), and a richer probabilistic context-free grammar (PCFG).", "labels": [], "entities": []}, {"text": "We compare our method to previous works that use a GAN objective to generate natural language, on a Chinese poetry generation dataset.", "labels": [], "entities": [{"text": "Chinese poetry generation dataset", "start_pos": 100, "end_pos": 133, "type": "DATASET", "confidence": 0.6427593380212784}]}, {"text": "In addition, we present a conditional) that generates sentences conditioned on sentiment and questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "GAN based methods have often been critiqued for lacking a concrete evaluation strategy (, however recent work () uses an annealed importance based technique to overcome this problem.", "labels": [], "entities": [{"text": "GAN based", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.860681027173996}]}, {"text": "In the context of generating natural language, it is possible to come up with a simpler approach to evaluate compute the likelihoods of generated samples.", "labels": [], "entities": []}, {"text": "We synthesize a data generating distribution under which we can compute likelihoods in a tractable manner.", "labels": [], "entities": []}, {"text": "We propose a simple evaluation strategy for evaluating adversarial methods of generating natural language by constructing a data generating distribution from a CFG or P\u2212CFG.", "labels": [], "entities": []}, {"text": "It is possible to determine if a sample belongs to the CFG or the probability of a sample under a P\u2212CFG by using a constituency parser that is provided with all of the productions in a grammar.", "labels": [], "entities": [{"text": "CFG", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.959688663482666}]}, {"text": "also present a simple idea to estimate the likelihood of generated samples by using a randomly initialized LSTM as their data gener-ating distribution.", "labels": [], "entities": []}, {"text": "While this is a viable strategy to evaluate generative models of language, a randomly initialized LSTM provides little visibility into the complexity of the data distribution itself and presents no obvious way to increase its complexity.", "labels": [], "entities": []}, {"text": "CFGs and PCFGs however, provide explicit control of the complexity via their productions.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9099477529525757}, {"text": "PCFGs", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.8455595970153809}]}, {"text": "They can also be learned via grammar induction on large treebanks of natural language and so the data generating distribution is not synthetic as in (.", "labels": [], "entities": []}, {"text": "Typical language models are evaluated by measuring the likelihood of samples from the true data distribution under the model.", "labels": [], "entities": []}, {"text": "However, with GANs it is impossible to measure likelihoods under the model itself and so we measure the likelihood of the model's samples under the true data distribution instead.", "labels": [], "entities": []}, {"text": "We divide our experiments into four categories: \u2022 Generating language that belongs to a toy CFG and an induced PCFG from the Penn Treebank ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.9662274122238159}]}, {"text": "\u2022 Chinese poetry generation with comparisons to ( and).", "labels": [], "entities": [{"text": "Chinese poetry generation", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.6040964921315511}]}, {"text": "\u2022 Generated samples from a dataset consisting of simple English sentences, the 1-billionword and Penn Treebank datasets.", "labels": [], "entities": [{"text": "Penn Treebank datasets", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.9955275654792786}]}, {"text": "\u2022 Conditional GANs that generate sentences conditioned on certain sentence attributes such as sentiment and questions.", "labels": [], "entities": [{"text": "GANs", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.8944993615150452}]}], "tableCaptions": [{"text": " Table 1: Accuracy and uniqueness measure of samples generated by different models. LSTM, LSTM-P  refers to the LSTM model with the output peephole and the WGAN-GP and GAN-GP refer to models  that use a gradient penalty in the discriminator's training objective", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9806982278823853}, {"text": "WGAN-GP", "start_pos": 156, "end_pos": 163, "type": "DATASET", "confidence": 0.821817934513092}]}, {"text": " Table 2: BLEU scores on the poem-5 and poem-7 datasets", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990622401237488}, {"text": "poem-7 datasets", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8611184060573578}]}]}