{"title": [{"text": "The Use of Object Labels and Spatial Prepositions as Keywords in a Web-Retrieval-Based Image Caption Generation System", "labels": [], "entities": [{"text": "Web-Retrieval-Based Image Caption Generation", "start_pos": 67, "end_pos": 111, "type": "TASK", "confidence": 0.7271221205592155}]}], "abstractContent": [{"text": "In this paper, a retrieval-based caption generation system that searches the web for suitable image descriptions is studied.", "labels": [], "entities": [{"text": "retrieval-based caption generation", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.6756638288497925}]}, {"text": "Google's search-by-image is used to find potentially relevant web multimedia content for query images.", "labels": [], "entities": []}, {"text": "Sentences are extracted from web pages and the likelihood of the descriptions is computed to select one sentence from the retrieved text documents.", "labels": [], "entities": []}, {"text": "The search mechanism is modified to replace the caption generated by Google with a caption composed of labels and spatial prepositions as part of the query's text alongside the image.", "labels": [], "entities": []}, {"text": "The object labels are obtained using an off-the-shelf R-CNN and a machine learning model is developed to predict the prepositions.", "labels": [], "entities": []}, {"text": "The effect on the caption generation system performance when using the generated text is investigated.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.9462752044200897}]}, {"text": "Both human evaluations and automatic metrics are used to evaluate the retrieved descriptions.", "labels": [], "entities": []}, {"text": "Results show that the web-retrieval-based approach performed better when describing single-object images with sentences extracted from stock photography web-sites.", "labels": [], "entities": []}, {"text": "On the other hand, images with two image objects were better described with template-generated sentences composed of object labels and prepositions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automatic generation of concise natural language descriptions for images is currently gaining immense popularity in both Computer Vision and Natural Language Processing communities (.", "labels": [], "entities": [{"text": "automatic generation of concise natural language descriptions", "start_pos": 4, "end_pos": 65, "type": "TASK", "confidence": 0.7715510981423515}]}, {"text": "The general process of automatically describing an image fundamentally involves the visual analysis of the image content such that a succinct natural language statement, verbalising the most salient image features, can be generated.", "labels": [], "entities": []}, {"text": "In addition, natural language generation methods are needed to construct linguistically and grammatically correct sentences.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.7576295137405396}]}, {"text": "Describing image content is very useful in applications for image retrieval based on detailed and specific image descriptions, caption generation to enhance the accessibility of current and existing image collections and most importantly as an assistive technology for visually impaired people ( ).", "labels": [], "entities": [{"text": "Describing image content", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8430532415707906}, {"text": "image retrieval", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7159058898687363}, {"text": "caption generation", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8906926214694977}]}, {"text": "Research work on automatic image description generation can be organised in three categories (.", "labels": [], "entities": [{"text": "automatic image description generation", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.6924231052398682}]}, {"text": "The first group generates textual descriptions from scratch by analysing the composition of an image in terms of image objects, attributes, scene types and event actions, extracted from image visual features.", "labels": [], "entities": []}, {"text": "The other groups describe images by retrieving sentences either from visual space composed of image-description pairs or from a multimodal space that combines image and sentences in one single space.", "labels": [], "entities": []}, {"text": "As opposed to direct-generationbased methods, the latter two approaches generate less verbose and more human-like descriptions.", "labels": [], "entities": []}, {"text": "In this paper, a web-retrieval-based system that exploits the ever-growing vision-text content is studied while exploring how object labels and prepositions affect the retrieval of image descriptions.", "labels": [], "entities": []}, {"text": "This paper is organised as follows: section 2 gives an overview of existing image caption algorithms.", "labels": [], "entities": [{"text": "image caption", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.7049078345298767}]}, {"text": "Section 3 outlines the problem definition and section 4 presents a web-retrieval-based framework followed by its implementation details in section 5.", "labels": [], "entities": []}, {"text": "The dataset and evaluation are discussed in sections 6 and 7 respectively.", "labels": [], "entities": []}, {"text": "The results are presented in section 8 followed by a discussion in section 9.", "labels": [], "entities": []}, {"text": "Finally, section 10 concludes with the main observations and the future direction.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the proposed image description framework, a specific subset of human-annotated images featured in MSCOCO 7 testing set was used.", "labels": [], "entities": [{"text": "image description", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7238309979438782}, {"text": "MSCOCO 7 testing set", "start_pos": 110, "end_pos": 130, "type": "DATASET", "confidence": 0.8855365514755249}]}, {"text": "Since the preposition prediction task is targeted to generate prepositions between two image objects, describing images having exactly two image objects was of particular interest to this study.", "labels": [], "entities": [{"text": "preposition prediction task", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8988094131151835}]}, {"text": "Therefore, the following steps were carried out to select images consisting of two image objects.", "labels": [], "entities": []}, {"text": "From the ViSen's MSCOCO testing set, 1975 instances having strictly one single preposition between two image objects were found and extracted.", "labels": [], "entities": [{"text": "ViSen's MSCOCO testing set", "start_pos": 9, "end_pos": 35, "type": "DATASET", "confidence": 0.9178506374359131}]}, {"text": "Finally, 1000 images were randomly selected from the latter subset.", "labels": [], "entities": []}, {"text": "Since images may contain background image objects, the same object detector employed in the proposed framework was used for detecting  Both human and computational evaluation were used to evaluate the web-retrieval-based framework.", "labels": [], "entities": []}, {"text": "The automatic evaluation was performed by using existing metrics, intended to measure the similarity between generated descriptions and corresponding human ground truth descriptions.", "labels": [], "entities": []}, {"text": "The measures include BLEU), ROUGE L (, METEOR (Denkowski and Lavie, 2014) and CIDEr (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9992938041687012}, {"text": "ROUGE L", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9521456062793732}, {"text": "METEOR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9919779300689697}, {"text": "CIDEr", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.8842964172363281}]}, {"text": "To complement the automatic evaluation, human judgments for image descriptions were obtained from a qualified English teacher.", "labels": [], "entities": []}, {"text": "Since the human evaluation process is considerably time-consuming, human judgments were collected fora sample of 200 images split equally for single and double-object images.", "labels": [], "entities": []}, {"text": "The same human evaluation criteria proposed by was used to evaluate the generated descriptions.", "labels": [], "entities": []}, {"text": "Human evaluation was conducted by rating the grammar, main aspects, correctness, order and the human-likeness of descriptions using a five-point Likert scale.", "labels": [], "entities": [{"text": "correctness", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9556465148925781}]}], "tableCaptions": [{"text": " Table 1: The accuracies obtained from the Visen's MSCOCO original object labels. The accuracies  for different configuration setups are presented, based on different geometric feature sets, in relation  to different textual label encoding. LE stands for the Label Encoder which encodes object labels with  corresponding integers, IV for Indicator Vectors and W2V for Word2Vec.", "labels": [], "entities": [{"text": "Visen's MSCOCO original object labels", "start_pos": 43, "end_pos": 80, "type": "DATASET", "confidence": 0.842878133058548}, {"text": "Word2Vec", "start_pos": 368, "end_pos": 376, "type": "DATASET", "confidence": 0.9493187665939331}]}, {"text": " Table 3: Automatic evaluation of single-object images.", "labels": [], "entities": []}, {"text": " Table 5. Particu- larly, generation-based (G) descriptions obtained  a grammatical median score of 1, confirming that  one-word descriptions do not produce grammati- cally correct sentences. The results also confirm  that the used object detector accurately describes  the dominant objects in an image. By considering", "labels": [], "entities": [{"text": "grammatical median score", "start_pos": 72, "end_pos": 96, "type": "METRIC", "confidence": 0.7637410561243693}]}]}