{"title": [{"text": "Predicting User Competence from Linguistic Data", "labels": [], "entities": [{"text": "Predicting User Competence from Linguistic Data", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8729225099086761}]}], "abstractContent": [{"text": "We investigate the problem of predicting the competence of users of the crowd-sourcing platform Zooniverse by analyzing their chat texts.", "labels": [], "entities": []}, {"text": "Zooniverse is an online platform where objects of different types are displayed to volunteer users to classify.", "labels": [], "entities": [{"text": "Zooniverse", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8803440928459167}]}, {"text": "Our research focuses on the Zoonivers Galaxy Zoo project, where users classify the images of galaxies and discuss their classifications in text.", "labels": [], "entities": [{"text": "Zoonivers Galaxy Zoo project", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.9152688980102539}]}, {"text": "We apply natural language processing methods to extract linguistic features including syntactic categories, bag-of-words, and punctuation marks.", "labels": [], "entities": []}, {"text": "We trained three supervised machine-learning classifiers on the resulting dataset: k-nearest neighbors, decision trees (with gradient boosting) and naive Bayes.", "labels": [], "entities": []}, {"text": "They are evaluated (regarding accuracy and F-measure) with two different but related domain datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9993844032287598}, {"text": "F-measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9979308843612671}]}, {"text": "The performance of the classifiers varies across the feature set configurations designed during the training phase.", "labels": [], "entities": []}, {"text": "A challenging part of this research is to compute the competence of the users without ground truth data available.", "labels": [], "entities": []}, {"text": "We implemented a tool that estimates the proficiency of users and annotates their text with computed competence.", "labels": [], "entities": []}, {"text": "Our evaluation results show that the trained classifier models give results that are significantly better than chance and can be deployed for other crowd-sourcing projects as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "The science crowd sourcing platform Zooniverse hosts a large number of different projects where volunteers/users (in this paper, the term \"volunteer\" is used interchangeably with \"user\") help scientists by classifying various kinds of data.", "labels": [], "entities": [{"text": "science crowd sourcing", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6924551129341125}]}, {"text": "In order to make the experience as positive as possible for the volunteers, so that they are more likely to stay on and contribute to the projects, the Zooniverse team is very interested in anything that can help them understand their volunteers better.", "labels": [], "entities": [{"text": "Zooniverse team", "start_pos": 152, "end_pos": 167, "type": "DATASET", "confidence": 0.9009264409542084}]}, {"text": "In this article, we explore how much the text comments left by volunteers in the chat rooms accompanying the project Galaxy Zoo can help us in determining their level of proficiency or competence in classifying images.", "labels": [], "entities": []}, {"text": "Proficiency is only one among many interesting qualities, and the text data is only one tool for measuring it.", "labels": [], "entities": []}, {"text": "The output from the machine learning algorithms we use can be combined with other measures to learn more about user proficiency.", "labels": [], "entities": []}, {"text": "Here, though, we focus on the following main question: Does the linguistic data from the chats contain useful information about the volunteers, in particular about the quality of their classifications?", "labels": [], "entities": []}, {"text": "The reason for focusing on Galaxy Zoo, rather than one of the many other projects run by Zooniverse, is that it is one of the oldest and largest projects, which means that there is quite a lot of data available -many users, many classifications, many text comments.", "labels": [], "entities": [{"text": "Galaxy Zoo", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.9232946634292603}]}, {"text": "There are several challenges that have to be addressed when trying to answer our question.", "labels": [], "entities": []}, {"text": "The hardest one is how to measure the quality of users' classifications.", "labels": [], "entities": []}, {"text": "The problem is that there is no ground truth data available.", "labels": [], "entities": []}, {"text": "For most of the galaxy photos that volunteers have classified, we do not know the correct answer.", "labels": [], "entities": []}, {"text": "No expert in the field has studied and classified them, since the whole point of using volunteers is that the experts do not have the time to do so.", "labels": [], "entities": []}, {"text": "Our approach to this challenge is to use majority votes, i.e., we consider the answer to a question given by the majority of the users to be the correct one.", "labels": [], "entities": []}, {"text": "This is by no means an unobjectionable assumption.", "labels": [], "entities": []}, {"text": "We describe our approach in more 476 detail and provide some justification for it in Section 3.", "labels": [], "entities": []}, {"text": "Once a quality measure for each user that has also provided sufficiently many textual comments has been computed, we employ three different machine learning algorithms to the data in order to see whether the values can be predicted from text.", "labels": [], "entities": []}, {"text": "Each algorithm is tested on six different sets of features of the textual data.", "labels": [], "entities": []}, {"text": "The algorithms we use are k-Nearest Neighbors, Naive Bayesian Classification, and Decision Trees (with gradient boosting).", "labels": [], "entities": [{"text": "Naive Bayesian Classification", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.589191218217214}]}, {"text": "The results achieved are not spectacular, but they show that analysis of the textual data gives a significantly better than chance prediction of the quality of a users classifications.", "labels": [], "entities": []}, {"text": "As mention above, this can be combined with other measures to get better predictions.", "labels": [], "entities": []}, {"text": "To investigate how well our methods generalize to other settings we also test them on data from the Zooniverse Snapshot Serengeti project.", "labels": [], "entities": [{"text": "Zooniverse Snapshot Serengeti project", "start_pos": 100, "end_pos": 137, "type": "DATASET", "confidence": 0.9392419457435608}]}, {"text": "The results are encouraging in that they are comparable to the results for Galaxy Zoo.", "labels": [], "entities": [{"text": "Galaxy Zoo", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9567051827907562}]}, {"text": "We discuss related work in Section 2, the calculation of majority votes in Section 3, the experimental setup in Section 4, the experimental results in Section 5 and, finally, the discussion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted text comments written by 7,839 volunteer.", "labels": [], "entities": []}, {"text": "We only targeted those users who classified at least 10 subjects and discussed at least one of their classifications in chat text.", "labels": [], "entities": []}, {"text": "The users were divided into three categories of equal size based on their computed competence levels on a scale ranging from 0 to 1: low ([0, 0.52]), medium ((0.52, 0.59]) and high).", "labels": [], "entities": []}, {"text": "Having an equal number of users in each category helps to achieve balanced data and in eliminating bias during the machine learning phase.", "labels": [], "entities": []}, {"text": "The raw data was obtained from Zooniverse Galaxy Zoo as a database dump.", "labels": [], "entities": [{"text": "Zooniverse Galaxy Zoo", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.9783109426498413}]}, {"text": "The entire text data contains around 26,617 sentences with average sentence length of 5.02.", "labels": [], "entities": []}, {"text": "We extracted three types of linguistic features out of the text data: bag-of-words, syntactic and punctuation marks.", "labels": [], "entities": []}, {"text": "The number of classifications is also included in each feature set as special feature or metadata.", "labels": [], "entities": []}, {"text": "479  We trained and evaluated three machine learning classifiers: Decision Trees (DT) with gradient boosting, Naive Bayes (NB) and k-Nearest Neighbor (KNN).", "labels": [], "entities": []}, {"text": "These three methods were also used in As the implementation of these classifiers is available in Rapidminer Studio, we trained them on the Galaxy Zoo data set after configuring the model parameters associated with each classifier.", "labels": [], "entities": [{"text": "Galaxy Zoo data set", "start_pos": 139, "end_pos": 158, "type": "DATASET", "confidence": 0.974063441157341}]}, {"text": "We adopted the best practices of the machine learning life cycle that includes randomly sampling and dividing the data into a training set, a validation (development) set and a test (evaluation) set, deciding the size of each set and balancing the proportion of examples in each class of users.", "labels": [], "entities": []}, {"text": "According to this, the classifiers are trained on 80% of the entire text corpus with the selected feature sets.", "labels": [], "entities": []}, {"text": "The remaining 20% is used to evaluate the trained models.", "labels": [], "entities": []}, {"text": "We set aside 10% of the training set as a development data set to optimize model parameters.", "labels": [], "entities": []}, {"text": "The models were evaluated with two equal size test sets by using accuracy and F-measure metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9994077682495117}, {"text": "F-measure", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9889517426490784}]}, {"text": "The first set is from the same domain as the training set, and the second one is from the Zooniverse Snapshot Serengeti forum discussion posts.", "labels": [], "entities": [{"text": "Zooniverse Snapshot Serengeti forum discussion posts", "start_pos": 90, "end_pos": 142, "type": "DATASET", "confidence": 0.957563062508901}]}, {"text": "To be able to use the Snapshot Serengeti data, we had to overcome the mismatch of the intervals of the competence scales of the two domains.", "labels": [], "entities": [{"text": "Snapshot Serengeti data", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.9093779722849528}]}, {"text": "We had to use a strategy that allows adapting the way that the competence scale for the Galaxy Zoo is divided to label its users to the Snapshot Serengeti users.", "labels": [], "entities": [{"text": "Galaxy Zoo", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.982546478509903}, {"text": "Snapshot Serengeti users", "start_pos": 136, "end_pos": 160, "type": "DATASET", "confidence": 0.902778685092926}]}, {"text": "In Woldemariam (2017), there are two scales used to divide the Snapshot Serengeti users, the first scale divides the user into three groups (Low, Medium and High) and the calibrated scale divides the users into five groups (very Low, Low, Medium, High, very High).", "labels": [], "entities": [{"text": "Snapshot Serengeti users", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.8367796142896017}]}, {"text": "Thus, we decided to use the first scale, as it is closer to the Galaxy Zoo scale in terms of the number of divisions, though the intervals between the groups are not exactly the same.", "labels": [], "entities": [{"text": "Galaxy Zoo scale", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.8363603750864664}]}], "tableCaptions": [{"text": " Table 1: Models Evaluation and Comparison Results, the All(3) column is equivalent with  BoW+PunMM+Syn", "labels": [], "entities": []}]}