{"title": [{"text": "Emergent Predication Structure in Hidden State Vectors of Neural Readers", "labels": [], "entities": [{"text": "Emergent Predication Structure in Hidden State Vectors of Neural Readers", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.6157313764095307}]}], "abstractContent": [{"text": "A significant number of neural architec-tures for reading comprehension have recently been developed and evaluated on large cloze-style datasets.", "labels": [], "entities": []}, {"text": "We present experiments supporting the emergence of \"predication structure\" in the hidden state vectors of these readers.", "labels": [], "entities": []}, {"text": "More specifically , we provide evidence that the hidden state vectors represent atomic formulas \u03a6[c] where \u03a6 is a semantic property (predicate) and c is a constant symbol entity identifier.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading comprehension is a type of question answering task where the answer is to be found in a passage about particular entities and events.", "labels": [], "entities": [{"text": "Reading comprehension", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8286179900169373}, {"text": "question answering task", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7935018837451935}]}, {"text": "In particular, the entities and events should not be mentioned in structured databases of general knowledge.", "labels": [], "entities": []}, {"text": "Reading comprehension problems are intended to measure a system's ability to extract semantic information about entities and relations directly from unstructured text.", "labels": [], "entities": []}, {"text": "Several large scale reading comprehension datasets have been introduced recently, including the CNN & Daily Mail datasets (, the Children's Book Test (CBT) (, and the Who-did-What dataset.", "labels": [], "entities": [{"text": "CNN & Daily Mail datasets", "start_pos": 96, "end_pos": 121, "type": "DATASET", "confidence": 0.9018855094909668}, {"text": "Children's Book Test (CBT)", "start_pos": 129, "end_pos": 155, "type": "DATASET", "confidence": 0.7736856767109462}, {"text": "Who-did-What dataset", "start_pos": 167, "end_pos": 187, "type": "DATASET", "confidence": 0.7428819239139557}]}, {"text": "The large sizes of these datasets enable the application of deep learning.", "labels": [], "entities": []}, {"text": "These are all cloze-style datasets where a question is constructed by deleting a word or phrase from an article summary (in CNN/Daily Mail), from a sentence in a children's story (in CBT), or by deleting a person from the first sentence of a different news article on the same entities and events (in Whodid-What).", "labels": [], "entities": [{"text": "CNN/Daily Mail)", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.9072508215904236}]}, {"text": "In this paper we present empirical evidence for the emergence of predication structure in a certain class of neural readers.", "labels": [], "entities": [{"text": "predication structure", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.9090484082698822}]}, {"text": "To understand predication structure, it is helpful to review the anonymization performed in the CNN/Daily Mail dataset.", "labels": [], "entities": [{"text": "predication structure", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.968292236328125}, {"text": "CNN/Daily Mail dataset", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.9428374528884887}]}, {"text": "In this dataset named entities are replaced by anonymous entity identifiers such as \"entity37\".", "labels": [], "entities": []}, {"text": "The passage might contain \"entity52 gave entity24 a rousing applause\" and the question might be \"X received a rounding applause from entity52\".", "labels": [], "entities": []}, {"text": "The task is to fill in X from a given multiple choice list of candidate entity identifiers.", "labels": [], "entities": []}, {"text": "A fixed relatively small set of the same entity identifiers are used overall the problems and the same problem is presented many times with different entity identifiers shuffled.", "labels": [], "entities": []}, {"text": "This prevents a given entity identifier from having any semantically meaningful vector embedding.", "labels": [], "entities": []}, {"text": "The embeddings of the entity identifiers are presumably just pointers to semantics-free tokens.", "labels": [], "entities": []}, {"text": "We will write entity identifiers as logical constant symbols such as c rather than strings such as \"entity37\".", "labels": [], "entities": []}, {"text": "\"Aggregation\" readers, including Memory Networks (, the Attentive Reader (, and the Stanford Reader (), use bidirectional LSTMs or GRUs to construct a contextual embedding ht of each position tin the passage and also an embedding h q of the question q.", "labels": [], "entities": [{"text": "Stanford Reader", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.8723603785037994}]}, {"text": "They then select an answer c using a criterion similar to argmax ct < ht , h q > < ht , e(c) > where e(c) is the vector embedding of the constant symbol (entity identifier) c.", "labels": [], "entities": [{"text": "argmax", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.967659592628479}]}, {"text": "In practice the innerproduct < ht , h q > is normalized overt using a softmax to yield attention weights \u03b1 t overt and (1) becomes argmax c < e(c), Here t \u03b1 t ht can be viewed as a vector representation of the passage.", "labels": [], "entities": []}, {"text": "We argue that for aggregation readers, roughly defined by (2), the hidden state ht of the passage at position (or word) t can be viewed as a vector concatenation ht = [s(\u03a6 t ), s(c t )] where \u03a6 t is a property (or statement or predicate) being stated of a particular constant symbol ct . Here s(\u03a6 t ) and s(c t ) are unknown emergent embeddings of \u03a6 t and ct respectively.", "labels": [], "entities": []}, {"text": "A logician might write this ash t = \u03a6 t [c t ].", "labels": [], "entities": []}, {"text": "Furthermore, the question can be interpreted as having the form \u03a8 where the problem is to find a constant symbol c such that the passage implies \u03a8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before presenting various models for machine comprehension we give a general formulation of the machine comprehension task.", "labels": [], "entities": []}, {"text": "We take an instance of the task to be a four tuple (q, p, a, A), where q is a question given as a sequence of words containing a special token fora \"blank\" to be filled in, p is a document consisting of a sequence of words, A is a set of possible answers and a \u2208 A is the ground truth answer.", "labels": [], "entities": []}, {"text": "All words are drawn from a vocabulary V.", "labels": [], "entities": []}, {"text": "We assume that all possible answers are words from the vocabulary, that is A \u2286 V, and that the ground truth answer appears in the document, that is a \u2208 p.", "labels": [], "entities": []}, {"text": "The problem can be described as that of selecting the answer a \u2208 A that answers question q based on information from p.", "labels": [], "entities": []}, {"text": "We now briefly summarize important features of the related datasets in reading comprehension.", "labels": [], "entities": []}, {"text": "CNN & Daily Mail: constructed these datasets from a large number of news articles from the CNN and Daily Mail news websites.", "labels": [], "entities": [{"text": "CNN & Daily Mail", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9162064492702484}, {"text": "Daily Mail news websites", "start_pos": 99, "end_pos": 123, "type": "DATASET", "confidence": 0.8857238441705704}]}, {"text": "The main article is used as the context, while the cloze style question is formed from one short article summary sentence appearing in conjunction with the published article.", "labels": [], "entities": []}, {"text": "To avoid the model using external world knowledge when answering the question, the named entities in the entire dataset were replaced by anonymous entity IDs which were then further shuffled for each example.", "labels": [], "entities": []}, {"text": "This forces models to rely on the context document to answer each question.", "labels": [], "entities": []}, {"text": "In this anonymized corpus the entity identifiers are taken to be apart of the vocabulary and the answer set A consists of the entity identifiers occurring in the passage.", "labels": [], "entities": []}, {"text": "Who-did-What (WDW): The Who-did-What dataset () contains 127,000 multiple choice cloze questions constructed from the LDC English Gigaword newswire corpus.", "labels": [], "entities": [{"text": "Who-did-What (WDW)", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.5262480601668358}, {"text": "Who-did-What dataset", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.7903368175029755}, {"text": "LDC English Gigaword newswire corpus", "start_pos": 118, "end_pos": 154, "type": "DATASET", "confidence": 0.8922004461288452}]}, {"text": "In contrast with CNN and Daily Mail, WDW avoids using article summaries for question formation.", "labels": [], "entities": [{"text": "question formation", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.8453488349914551}]}, {"text": "Instead, each problem is formed from two independent articles: one is given as the passage to be read and a different article on the same entities and events is used to form the question.", "labels": [], "entities": []}, {"text": "Further, WDW avoids anonymization -each choice is a person named entity.", "labels": [], "entities": [{"text": "WDW", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.7056288123130798}]}, {"text": "In this dataset the answer set A consists of the person named entities occurring in the passage.", "labels": [], "entities": []}, {"text": "Finally, the problems have been filtered to remove a fraction that are easily solved by simple baselines.", "labels": [], "entities": []}, {"text": "It has two training sets.", "labels": [], "entities": []}, {"text": "The larger training set (\"relaxed\") is created using less baseline filtering, while the smaller training set (\"strict\") uses the same filtering as the validation and test sets.", "labels": [], "entities": []}, {"text": "It is also worth mentioning several related datasets.", "labels": [], "entities": []}, {"text": "The MCTest dataset ( consists of children's stories and questions written by crowdsourced workers.", "labels": [], "entities": [{"text": "MCTest dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9620058536529541}]}, {"text": "The dataset only contains 660 documents and is too small to train deep models.", "labels": [], "entities": []}, {"text": "The bAbI dataset) is constructed automatically using synthetic text generation and can be perfectly answered by hand-written algorithms ().", "labels": [], "entities": [{"text": "bAbI dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.875404953956604}]}, {"text": "The SQuAD dataset consists of passage-question pairs where the passage is a Wikipedia article and the questions are written via crowdsourcing.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7874956727027893}]}, {"text": "The dataset contains over 100,000 problems, but the answer is often a word sequence which is difficult to handle with the reader models considered here.", "labels": [], "entities": []}, {"text": "The Children's Book Test (CBT) () takes any sequence of 21 consecutive sentences from a children's book: the first 20 sentences are used as the passage, and the goal is to infer a missing word in the 21st sentence.", "labels": [], "entities": [{"text": "Children's Book Test (CBT)", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.7669206602232796}]}, {"text": "The task complexity varies with the type of the omitted word (verb, preposition, named entity, or common noun).", "labels": [], "entities": []}, {"text": "The LAMBADA dataset) is a word prediction dataset which requires abroad discourse context, though the correct answer might not actually be contained in the context.", "labels": [], "entities": [{"text": "LAMBADA dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7658258974552155}, {"text": "word prediction", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7167701870203018}]}, {"text": "Nevertheless, when the correct answer is in the context, neural readers can be applied effectively ().", "labels": [], "entities": []}, {"text": "We implemented the neural readers using Theano ( and and train them on a single NVIDIA Tesla K40 GPU.", "labels": [], "entities": [{"text": "Theano", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.9411447644233704}]}, {"text": "Negative log-likelihood is employed as training criterion.", "labels": [], "entities": []}, {"text": "We used stochastic gradient descent (SGD) with the Adam update rule ( and set the learning rate to 0.0005.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.6624953210353851}]}, {"text": "For the Stanford Reader and One-Hot Pointer Reader, we use the Stanford Reader's default settings.", "labels": [], "entities": [{"text": "Stanford Reader", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9596622586250305}, {"text": "Stanford Reader", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9357832670211792}]}, {"text": "For the Gated-Attention reader, the lookup table was initialized using pre-trained GloVe (Jeffrey et al., 2014) vectors.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.8726190328598022}]}, {"text": "1 Input to hidden state weights were initialized by random orthogonal matrices () and biases were initialized to zero.", "labels": [], "entities": []}, {"text": "Hidden to hidden state weights were initialized by identity matrices to force the model to remember longer information.", "labels": [], "entities": []}, {"text": "To compute the attention weight, we use \u03b1 t = ht W \u03b1 h q and initialize W \u03b1 with random uniform distribution.", "labels": [], "entities": []}, {"text": "We also use gradient clipping) with a threshold of 10 and mini-batches of size 32.", "labels": [], "entities": []}, {"text": "During training we randomly shuffle all examples within each epoch.", "labels": [], "entities": []}, {"text": "To speedup training, we always pre-fetch 10 batches worth of examples and sort them according to document length as done by.", "labels": [], "entities": []}, {"text": "When using anonymization, we randomly reshuffle the entity identifier to match the procedure proposed by.", "labels": [], "entities": []}, {"text": "During training we evaluate the accuracy after each epoch and stop training when the accuracy on the validation set starts decreasing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9993451237678528}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9992895126342773}]}, {"text": "We tried limiting the vocabulary to the most frequent tokens but did not observe any performance improvement compared with using all distinct tokens as the vocabulary.", "labels": [], "entities": []}, {"text": "Since part of our experiments need to check word embedding assignment issues, we finally use all the distinct tokens as vocabulary.", "labels": [], "entities": [{"text": "word embedding assignment", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6065113445123037}]}, {"text": "To find the optimal embedding and hidden state dimension, we tried several groups of different combinations, and the optimal values were 200 and 384, respectively.", "labels": [], "entities": []}, {"text": "When anonymizing the Who-did-What dataset, we can either use simple string matching to replace answers in the question and passage with en-tity identifiers, or we can use the Stanford named entity recognizer (NER) 2 to detect named entities and replace the answer named entities in the question and passage with entity identifiers.", "labels": [], "entities": [{"text": "Who-did-What dataset", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.7085548937320709}]}, {"text": "We found the latter to bring 2% improvement compared with simple string matching.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics to support (15) and (16). These statistics are computed for the Stanford Reader.", "labels": [], "entities": [{"text": "Stanford Reader", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9334858059883118}]}, {"text": " Table 2: Accuracy on Who-did-What dataset. Each result is based on a single model. Results for neural  readers other than NSE are based on replications of those systems. All models were trained on the  relaxed training set which uniformly yields better performance than the restricted training set. The first  group of models are explicit reference models and the second group are aggregation models. + indicates  anonymization with better reference identifier.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9174913763999939}, {"text": "Who-did-What dataset", "start_pos": 22, "end_pos": 42, "type": "DATASET", "confidence": 0.6689077764749527}]}]}