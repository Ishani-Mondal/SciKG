{"title": [], "abstractContent": [{"text": "We introduce and describe the results of a novel shared task on bandit learning for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7632577121257782}]}, {"text": "The task was organized jointly by Amazon and Heidelberg University for the first time at the Second Conference on Machine Translation (WMT 2017).", "labels": [], "entities": [{"text": "Machine Translation (WMT 2017)", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.8228731552759806}]}, {"text": "The goal of the task is to encourage research on learning machine translation from weak user feedback instead of human references or post-edits.", "labels": [], "entities": [{"text": "learning machine translation", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6228479246298472}]}, {"text": "On each of a sequence of rounds, a machine translation system is required to propose a translation for an input, and receives a real-valued estimate of the quality of the proposed translation for learning.", "labels": [], "entities": []}, {"text": "This paper describes the shared task's learning and evaluation setup, using services hosted on Amazon Web Services (AWS), the data and evaluation metrics, and the results of various machine translation architectures and learning protocols.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bandit Learning for machine translation (MT) is a framework to train and improve MT systems by learning from weak or partial feedback: Instead of a gold-standard human-generated translation, the learner only receives feedback to a single proposed translation (hence the term 'partial'), inform of a translation quality judgement (a real number which can be as weak as a binary acceptance/rejection decision).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.8638886868953705}, {"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.979197084903717}]}, {"text": "In the shared task, user feedback was simulated by a service hosted on Amazon Web Services (AWS).", "labels": [], "entities": []}, {"text": "Participants can submit translations and receive feedback on translation quality.", "labels": [], "entities": []}, {"text": "This is used to adapt an out-of-domain MT model, pre-trained on mostly news texts, to anew domain (e-commerce), for the translation direction of German (DE) to English (EN).", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9657360315322876}, {"text": "translation direction of German (DE) to English (EN)", "start_pos": 120, "end_pos": 172, "type": "TASK", "confidence": 0.8547691156466802}]}, {"text": "While in our setup feedback was simulated by evaluating a reward function on the predicted translation against a gold standard reference, the reference translation itself was never revealed to the learner, neither at training nor attest time.", "labels": [], "entities": []}, {"text": "This learning scenario has been investigated under the names of learning from bandit feedback 1 or reinforcement learning (RL) 2 , and has important real world applications such as online advertising).", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6057266891002655}]}, {"text": "In the advertising application, the problem is to select the best advertisement fora user visiting a publisher page.", "labels": [], "entities": []}, {"text": "A key element is to estimate the click-through rate (CTR), i.e., the probability that an ad will be clicked by a user so that the advertiser has to pay.", "labels": [], "entities": [{"text": "click-through rate (CTR)", "start_pos": 33, "end_pos": 57, "type": "METRIC", "confidence": 0.9056236863136291}]}, {"text": "This probability is modeled by features representing user, page, and ad, and is estimated by trading off exploration (a new ad needs to be displayed in order to learn its click-through rate) and exploitation (displaying the ad with the current best estimate is better in the short term) in displaying ads to users.", "labels": [], "entities": []}, {"text": "In analogy to the online advertising scenario, one could imagine a scenario of personalization in machine translation where translations have to be adapted to the user's specific purpose and domain.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7207385301589966}]}, {"text": "Similar to online advertising, where it is unrealistic to expect more detailed feedback than a user click on a displayed ad, the feedback in adaptive machine translation should be weaker than a reference translation or a post-edit created by The name is inherited from a model wherein each round a gambler pulls an arm of a different slot machine ('onearmed bandit'), with the goal of maximizing his reward relative to the maximal possible reward, without apriori knowledge of the optimal slot machine.", "labels": [], "entities": [{"text": "adaptive machine translation", "start_pos": 141, "end_pos": 169, "type": "TASK", "confidence": 0.7002124985059103}]}, {"text": "See Bubeck and CesaBianchi (2012) for an overview.a professional translator.", "labels": [], "entities": []}, {"text": "Instead, the goal is to elicit binary or real-valued judgments of translation quality from laymen users (for example, show that consistent assessments of real-valued translation quality can be provided by crowdsourcing), or to infer feedback signals from user interactions with the translated content on a web page (for example, by interpreting a copy-paste action of the MT output as positive quality signal, and a correction as a negative quality signal).", "labels": [], "entities": []}, {"text": "The goal of this shared task is to evaluate existing algorithms for learning MT systems from weak feedback () on real-world data and compare them to new algorithms, with a focus on performing online learning efficiently and effectively from bandit feedback, i.e. the best algorithms are those that perform fast online learning and, simultaneously, achieve high translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9159156680107117}]}, {"text": "In the following, we present a description of the protocol and infrastructure of our online learning task, and of the data for pretraining, online training, and evaluation (Section 2).", "labels": [], "entities": []}, {"text": "We introduce the online and batch evaluation metrics used in the shared task (Section 3), and describe static baseline systems (Section 4) and submitted online learning systems (Section 5).", "labels": [], "entities": []}, {"text": "We present and discuss the results of the task (Section 6-7), showing that NMT systems with batch domain adaptation provide very good baselines, however, online learning based on SMT or NMT can catch up overtime by adapting to the provided feedback.", "labels": [], "entities": [{"text": "SMT", "start_pos": 179, "end_pos": 182, "type": "TASK", "confidence": 0.9139782190322876}]}], "datasetContent": [{"text": "In our shared task, participants were allowed to use their favorite MT systems as starting points to integrate online bandit learning methods.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9436988830566406}]}, {"text": "This leads to the difficulty of separating the contributions of the underlying MT architecture and the online bandit learning method.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9551768898963928}]}, {"text": "We attempted to tackle this problem by using different evaluation metrics that focus on these respective aspects: 1.", "labels": [], "entities": []}, {"text": "Online cumulative reward: This metric measures the cumulative sum C = K k=1 \u2206(t k ) of the per-sentence BLEU score \u2206 against the number of iterations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.97285595536232}]}, {"text": "This metric has been used in reinforcement learning competitions ().", "labels": [], "entities": []}, {"text": "For systems with the same design, this metric favors those that do a good job at balancing exploration and exploitation to achieve high scores over the full data sequence.", "labels": [], "entities": []}, {"text": "Unlike in these competitions, where environments (i.e., action spaces and context features) were fixed, in our task the environment is heterogeneous due to the use of different underlying MT architectures.", "labels": [], "entities": [{"text": "MT", "start_pos": 188, "end_pos": 190, "type": "TASK", "confidence": 0.9551505446434021}]}, {"text": "Thus, systems that start outwith a well-performing pretrained out-of-domain model have an advantage over systems that might improve moreover worse starting points.", "labels": [], "entities": []}, {"text": "Furthermore, even systems that do not perform online learning at all can achieve high cumulative rewards.", "labels": [], "entities": []}, {"text": "2. Online regret: In order to overcome the problems of the cumulative reward metric, we can use a metric from bandit learning that measures the regret R = 1 that is suffered by the system when predicting translation t k instead of the optimal translation t * k produced by an oracle system.", "labels": [], "entities": []}, {"text": "Plotting a running average of regret against the number of iterations allows separating the gains due to the MT architecture from the gains due to the learning algorithm: Systems that do learn will decrease regret, systems that do not learn will not.", "labels": [], "entities": []}, {"text": "In our task, we use as oracle system a model that is trained on in-domain data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data statistics for source side of in- domain and out-of-domain parallel data.", "labels": [], "entities": []}]}