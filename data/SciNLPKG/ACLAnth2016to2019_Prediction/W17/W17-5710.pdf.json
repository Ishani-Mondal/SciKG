{"title": [], "abstractContent": [{"text": "System architecture, experimental settings and experimental results of the EHR team for the WAT2017 tasks are described.", "labels": [], "entities": [{"text": "EHR team", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9436575770378113}, {"text": "WAT2017 tasks", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.7476049065589905}]}, {"text": "We participate in three tasks: JPCen-ja, JPCzh-ja and JPCko-ja.", "labels": [], "entities": [{"text": "JPCen-ja", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.8989483118057251}, {"text": "JPCzh-ja", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.8467403054237366}, {"text": "JPCko-ja", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9055554270744324}]}, {"text": "Although the basic architecture of our system is NMT, reranking technique is conducted using SMT results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9711123108863831}]}, {"text": "One of the major drawback of NMT is under translation and over-translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9590276479721069}]}, {"text": "On the other hand, SMT infrequently makes such translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9910176992416382}]}, {"text": "So, using reranking of n-best NMT outputs by the SMT output, discarding such translations can be expected.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.8357048630714417}]}, {"text": "We can improve BLEU score from 46.03 to 47.08 by this technique in JPCzh-ja task.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.975167840719223}, {"text": "JPCzh-ja", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.8111417889595032}]}], "introductionContent": [{"text": "Rapidly progressing of NMT techniques make paradigm change in machine translation not only for the research purpose but for the practical field.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8172600567340851}]}, {"text": "Although the NMT provides high quality and fluent translations, it has several drawbacks.", "labels": [], "entities": [{"text": "NMT", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.9075693488121033}]}, {"text": "One of them is under-and over-translation which is infrequent in a SMT output.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.9119856953620911}]}, {"text": "We propose a reranking method for n-best NMT outputs using a SMT output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9530360698699951}]}, {"text": "We compare n-best NMT outputs with a SMT output by the measure of IMPACT which is one of the automatic evaluation measure of machine translation results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9874474406242371}, {"text": "IMPACT", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9962844848632812}, {"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7716737389564514}]}, {"text": "The NMT output which has the highest IMPACT score referring to SMT output is selected as the system output.", "labels": [], "entities": [{"text": "IMPACT score", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.9059726297855377}, {"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9762722849845886}]}, {"text": "In the following sections, we describe system architecture and experimental settings in section 2, experimental results and discussions in section 3 and conclusion in section 4. 2 System architecture and experimental settings", "labels": [], "entities": []}], "datasetContent": [{"text": "The official evaluation results of our submissions are shown in ().", "labels": [], "entities": []}, {"text": "In the Table 1, \"Original system\" means the NMT without reranking and \"SMT\" means SMT part of our system.", "labels": [], "entities": []}, {"text": "For JPCen-ja task, reranking decreases BLEU, RIBES and AMFM scores and also HUMAN score.", "labels": [], "entities": [{"text": "JPCen-ja", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7277666330337524}, {"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9995608925819397}, {"text": "RIBES", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9945642948150635}, {"text": "AMFM scores", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9699969291687012}, {"text": "HUMAN score", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9847554564476013}]}, {"text": "Although the overall evaluation result doesn't show the effectiveness of the reranking, several improvements are observed.", "labels": [], "entities": []}, {"text": "Original translation of the example 1 has under-translation.", "labels": [], "entities": []}, {"text": "Only the first two words (The oldest) and the punctuation mark (.) are translated in the original translation.", "labels": [], "entities": []}, {"text": "Original translations of example 2 has also under-translation.", "labels": [], "entities": []}, {"text": "None of words \"( ACT , READ , PRE ) , GBSTB , GBSTT , FXb 2 , PUMP , FXB , FXT , SWL , and RFX\" is translated.", "labels": [], "entities": [{"text": "READ", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.7472038269042969}]}, {"text": "On the other hand, reranking system does not make such under-translations.", "labels": [], "entities": []}, {"text": "Original translation of example 3 has over-translation.", "labels": [], "entities": []}, {"text": "\" \u7570 \u306a \u308b (differ)\" occurs two times.", "labels": [], "entities": [{"text": "\u7570 \u306a \u308b (differ)\"", "start_pos": 2, "end_pos": 17, "type": "METRIC", "confidence": 0.7351105064153671}]}, {"text": "But the reranked translation has no overtranslation.", "labels": [], "entities": []}, {"text": "JPCzh-ja JPCko-ja: JPCen-ja task's examples having effectiveness of reranking (recover of under-translation: example 1 and 2; recover of over-translation: example 3) For JPCen-ja task, comparing our submission of data ID 1407 (EHR) and another submission (OTHER), BLEU score of the EHR is 44.63 and it is less than the OTHER's score (50.27).", "labels": [], "entities": [{"text": "JPCzh-ja JPCko-ja", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.895526111125946}, {"text": "BLEU", "start_pos": 264, "end_pos": 268, "type": "METRIC", "confidence": 0.9998321533203125}, {"text": "EHR", "start_pos": 282, "end_pos": 285, "type": "METRIC", "confidence": 0.5520076751708984}]}, {"text": "On the other hand, HUMAN score of the EHR is 60.00 and it is greater than the OTHER's score (56.25).", "labels": [], "entities": [{"text": "HUMAN score", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9744833111763}, {"text": "EHR", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.6111542582511902}, {"text": "OTHER's score", "start_pos": 78, "end_pos": 91, "type": "METRIC", "confidence": 0.7245617508888245}]}, {"text": "There are 20 data that the BLEU 1 score of EHR is less than the OTHER's score but the HUMAN score of EHR is greater than the OTHER's score 2 . We examine these data and find that several data have the differences between the source expression and the reference expression.", "labels": [], "entities": [{"text": "BLEU 1 score", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.9825110832850138}, {"text": "EHR", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.927997350692749}, {"text": "HUMAN score", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.9766961932182312}, {"text": "EHR", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.8236904740333557}]}, {"text": "1 Sentence level BLEU is calculated by mteval-v13a.pl in the Moses package.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.8760994076728821}]}, {"text": "For the BLEU score, \"less\" means \"less or equal -10\" and \"greater\" means \"greater or equal 10\".", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.9622952938079834}]}, {"text": "If the difference of BLEU is between -10 to 10, it is considered \"tie\".", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9990166425704956}, {"text": "tie", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9837548136711121}]}, {"text": "We distinguish between \"additive translation\" and \"overtranslation\".", "labels": [], "entities": [{"text": "additive translation", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7430320084095001}]}, {"text": "The former means the translation including shows examples of source, reference, EHR output and OTHER output.", "labels": [], "entities": [{"text": "EHR", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.8911384344100952}, {"text": "OTHER", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.8828825354576111}]}, {"text": "Example 1 has the voice change (the source is passive and the reference is active).", "labels": [], "entities": []}, {"text": "Example 2 has the topic change (the topic of the source is \"valve\" and the topic of the reference is \"\u7d4c\u8def (passage)\".", "labels": [], "entities": []}, {"text": "Example 3 has the additive translation 3 (the source \"GELD\" corresponds the reference \"\u6709\u6a5f EL \u30c7\u30a3\u30b9\u30d7\u30ec\u30fc\uff08 \uff2f\uff25\uff2c \uff24 \uff0c \uff4f\uff52\uff47\uff41\uff4e\uff49\uff43\uff45\uff4c\uff45\uff43\uff54\uff52\uff4f \u2212 \uff4c\uff55\uff4d\uff49 \uff4e\uff45\uff53\uff43\uff45\uff4e\uff54\uff44\uff49\uff53\uff50\uff4c\uff41\uff59 \uff09\").", "labels": [], "entities": [{"text": "GELD", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.6859877109527588}, {"text": "EL \u30c7\u30a3\u30b9\u30d7\u30ec\u30fc", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.883318563302358}]}, {"text": "Example 4 has the subtractive translation (the source has complemental information and the latter means the translation including needless information.", "labels": [], "entities": []}, {"text": "We distinguish between \"subtractive translation\" and \"under-translation\".", "labels": [], "entities": []}, {"text": "The former means the translation omitting complemental information and the latter means the translation omitting needful information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official evaluation results (Japanese segmenter is Juman)", "labels": [], "entities": []}, {"text": " Table 2: JPCen-ja task's examples having effectiveness of reranking (recover of under-translation:  example 1 and 2; recover of over-translation: example 3)", "labels": [], "entities": []}, {"text": " Table 5: JPCko-ja task's examples having effectiveness of reranking (recover of under-translation:  example 1 and 2)", "labels": [], "entities": [{"text": "recover", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.940818190574646}]}, {"text": " Table 6. Example 1 has an under-transla- tion in the word based translation (\"\u6ef4\u5ea6\"). Ex- ample 2 also has an under-translation in the word", "labels": [], "entities": []}, {"text": " Table 6: JPCzh-ja task's examples having BLEU and HUMAN scores contradiction  (under-translation in word based: example 1 and 2; different translation: example 3 and 4)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9989110231399536}, {"text": "HUMAN", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9462819695472717}]}, {"text": " Table 7: JPCko-ja task's examples having BLEU and HUMAN scores contradiction  (different translation: example 1 and 3; un-translation in word based: example 2)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9991403818130493}, {"text": "HUMAN", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9623468518257141}]}]}