{"title": [{"text": "Authorship Attribution with Convolutional Neural Networks and POS-Eliding", "labels": [], "entities": []}], "abstractContent": [{"text": "We use a convolutional neural network to perform authorship identification on a very homogeneous dataset of scientific publications.", "labels": [], "entities": [{"text": "authorship identification", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.887494295835495}]}, {"text": "In order to investigate the effect of domain biases, we obscure words below a certain frequency threshold, retaining only their POS-tags.", "labels": [], "entities": []}, {"text": "This procedure improves test performance due to better generalization on unseen data.", "labels": [], "entities": []}, {"text": "Using our method, we are able to predict the authors of scientific publications in the same discipline at levels well above chance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational authorship identification is a task of great interest for many historical and forensic applications.", "labels": [], "entities": [{"text": "Computational authorship identification", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8191165725390116}]}, {"text": "In order to judge the applicability of current and future authorship identification techniques, they need to have been tested in a variety of realistic settings.", "labels": [], "entities": [{"text": "authorship identification", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.9328323900699615}]}, {"text": "As it stands, the accuracy of procedures for automatic authorship attribution varies widely with the setting of the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9992188215255737}, {"text": "automatic authorship attribution", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6520534157752991}]}, {"text": "Among the variables affecting the accuracy of authorship attribution systems identified by are the number of target authors a text is to be attributed to, the presence of an other-class in the test set (containing texts not written by any of the authors in the training set), the length of the text segments to be classified, and the amount of training data available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9960488677024841}]}, {"text": "Another important variable which is frequently unaddressed in the computational authorship attribution literature but which deserves closer attention is the monotonicity or diversity of genres and domains in the data, as well as the domain-and genre-specificity of the writings of individual authors.", "labels": [], "entities": []}, {"text": "This work introduces a task setting for authorship attribution that is highly invariant with respect to genre and domain, as well as design ideas for systems adapted to this challenging setting.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8553404808044434}]}, {"text": "We conducted a controlled study on the effects of domain and genre bias on authorship attribution by means of an ablation analysis where words in a text, but not their automatically predicted POStag, are obscured at various frequency cutoffs.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7466637790203094}, {"text": "POStag", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.8673970103263855}]}, {"text": "The aim is the design of a system which can perform authorship attribution of texts which are extremely similar in terms of genre and domain among a large class of target authors, based solely on features extracted from POS-tags and a small core vocabulary.", "labels": [], "entities": []}, {"text": "The central research question is how well computational authorship attribution works when based on purely stylometric (as opposed to content) features.", "labels": [], "entities": [{"text": "computational authorship attribution", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.6163040598233541}]}, {"text": "In doing so, we shed light on the effect that thematic biases have on results in the area of computational authorship attribution.", "labels": [], "entities": [{"text": "computational authorship attribution", "start_pos": 93, "end_pos": 129, "type": "TASK", "confidence": 0.684059202671051}]}], "datasetContent": [{"text": "In our experiments, we used single-author papers from the ACL Anthology Reference Corpus ().", "labels": [], "entities": [{"text": "ACL Anthology Reference Corpus", "start_pos": 58, "end_pos": 88, "type": "DATASET", "confidence": 0.935115784406662}]}, {"text": "The corpus contains scientific papers published in the proceedings of various conferences and workshops in the areas of computational linguistics and natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 150, "end_pos": 177, "type": "TASK", "confidence": 0.6448077360788981}]}, {"text": "The earliest data is from 1965, the latest data is from 2007.", "labels": [], "entities": []}, {"text": "We designated all papers published in the year 2006 as development data and all papers published in 2007 as test data, with the remaining data used for training.", "labels": [], "entities": []}, {"text": "New authors without publications before this date were not treated any differently from those which were represented in the training data.", "labels": [], "entities": []}, {"text": "We only retained publications from authors with at least two single-author papers, although we do not require both or even one of them to be part of the training data.", "labels": [], "entities": []}, {"text": "Our dataset contained 808 distinct authors.", "labels": [], "entities": []}, {"text": "We discarded the first 10 lines of each document in order to strip publications of author names, email addresses and workplace information.", "labels": [], "entities": []}, {"text": "We also removed any lines containing the author's last name (for example, as part of a self-citation or email ad- dress).", "labels": [], "entities": []}, {"text": "We partitioned training, development and test data into segments of 1,500 words each, discarding any segments shorter than 1,500 words at the end of a publication.", "labels": [], "entities": []}, {"text": "Authorship prediction is performed on the level of these segments.", "labels": [], "entities": [{"text": "Authorship prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7187419086694717}]}, {"text": "gives an overview of corpus statistics.", "labels": [], "entities": []}, {"text": "For POS-tagging, we used the Stanford POStagger (.", "labels": [], "entities": [{"text": "Stanford POStagger", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.7579849660396576}]}, {"text": "In addition to POS-tags, we use the pre-trained word embeddings available from Google 5 trained using the skip-gram objective ( as input features for our convolutional neural network.", "labels": [], "entities": []}, {"text": "Word frequencies were computed on the News Commentary and News Discussions English datasets provided by the WMT15 workshop.", "labels": [], "entities": [{"text": "News Commentary and News Discussions English datasets", "start_pos": 38, "end_pos": 91, "type": "DATASET", "confidence": 0.6167984477111271}, {"text": "WMT15 workshop", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.892377108335495}]}, {"text": "For authorship prediction, we used a convolutional neural network (CNN) similar to that of.", "labels": [], "entities": [{"text": "authorship prediction", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.9774812459945679}]}, {"text": "Each sentence is represented as a padded concatenation of word embedding vectors and POS-tag one-hot encodings.", "labels": [], "entities": []}, {"text": "The network then applies a single layer of convolving filters with varying window sizes, and a max-overtime pooling layer which retains only the maximum value.", "labels": [], "entities": []}, {"text": "The resulting features are passed to a fully-connected softmax layer to obtain a probability distribution over labels.", "labels": [], "entities": []}, {"text": "gives an overview of the model architecture.", "labels": [], "entities": []}, {"text": "We used the implementation of Kim (2014), 7 which we modified in a number of ways.", "labels": [], "entities": [{"text": "Kim (2014), 7", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9221408069133759}]}, {"text": "We used static channels only and did not modify the pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Our input feature map contained not only the 300-dimensional word embeddings, but also a one-hot representation of POS-tags.", "labels": [], "entities": []}, {"text": "We used 100 convolution filters of length 1, 2 and 3 words each and a batch size of 20 sentences.", "labels": [], "entities": []}, {"text": "Like that of, our fully connected layer was trained with dropout.", "labels": [], "entities": []}, {"text": "The dropout rate was set to 0.5 during training.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9692817628383636}]}, {"text": "The network scans the entire input text of a segment using a sliding-window approach before applying max-pooling overtime and making a prediction of authorship based on the prediction of the softmax layer.", "labels": [], "entities": []}, {"text": "We tested the following frequencycutoff settings: 1.", "labels": [], "entities": []}, {"text": "Retain only the 1,000 most frequent words in our large, out-of-domain corpus of English, use their word embeddings as input features alongside a one-hot encoding of their POStags as predicted by the Stanford POS-tagger.", "labels": [], "entities": []}, {"text": "Replace all other words with an unknown token.", "labels": [], "entities": []}, {"text": "Generate a separate random embedding for each combination of the unknown token with a particular POS-tag and, in addition, retain the one-hot encoding of the POS-tags of all unknown tokens.", "labels": [], "entities": []}, {"text": "Same as (1), but retain the 5,000, 10,000 and 50,000 most frequent words, respectively.", "labels": [], "entities": []}, {"text": "5. Retain all words and use their embeddings as input features, including a 1-hot encoding of their POS-tag.", "labels": [], "entities": [{"text": "POS-tag", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.85829097032547}]}, {"text": "Generate a random word embedding for unknown words, as in Kim (2014).", "labels": [], "entities": []}, {"text": "Training was run fora maximum of 50 epochs.", "labels": [], "entities": []}, {"text": "After each epoch, we measured the prediction accuracy on the development data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9743145704269409}]}, {"text": "After training was complete, we tested the model parameters with the best development accuracy on the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.7908228039741516}]}, {"text": "In addition to evaluating the authorship predictions of the model, we evaluate rank accuracies as well in order to investigate whether the models are able to reduce the list of possible authors fora segment to a short candidate list which contains the correct author.", "labels": [], "entities": []}, {"text": "This can be achieved in a straightforward manner by simply sorting the activations of the softmax layer of the convolutional network fora test segment in order to obtain a ranked candidate list.", "labels": [], "entities": []}, {"text": "Our initial research hypothesis was that (1 -4) would perform significantly worse than (5), while strongly outperforming a random baseline.", "labels": [], "entities": []}, {"text": "This would demonstrate that authorship attribution (in a probabilistic sense) is possible based on stylometric features alone, but not to the same level of accuracy as when content clues are used as well.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8414067029953003}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9978950023651123}]}, {"text": "gives an overview of the results for outright prediction of authorship.", "labels": [], "entities": [{"text": "prediction of authorship", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7118574778238932}]}, {"text": "We find that at a frequency cutoff of 50,000 words, our system outperforms a setting in which the full vocabulary is used, while at lower frequency cutoffs performance is slightly reduced.", "labels": [], "entities": []}, {"text": "It should be noted that all of our systems far outperform a random assignment of authors, which would be correct in approximately 808 (0.12%) of cases.", "labels": [], "entities": []}, {"text": "Performance in terms of accuracy for our best system is thus two orders of magnitude above random assignment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9992794394493103}]}, {"text": "In order to enable meaningful comparison of our models to other work, we additionally tested our approach on a commonly used benchmark dataset.", "labels": [], "entities": []}, {"text": "We chose Task I of the PAN 2012 authorship attribution shared task, which involves authorship attribution among a closed class of 14 novelists.", "labels": [], "entities": [{"text": "PAN 2012 authorship attribution shared task", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.7196758091449738}]}, {"text": "The training data was again partitioned into segments of 1,500 words.", "labels": [], "entities": []}, {"text": "The training procedure was identical to the one employed on the ACL Anthology dataset.", "labels": [], "entities": [{"text": "ACL Anthology dataset", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.9589336117108663}]}, {"text": "We set aside 200 segments as development data, which left 1,694 segments for training.", "labels": [], "entities": []}, {"text": "The test data comprised 14 novel-length texts.", "labels": [], "entities": []}, {"text": "Prediction on the test data was performed on segments of a maximum length of 1,500 words, although we allowed for shorter segments at the end of texts.", "labels": [], "entities": []}, {"text": "For prediction on the text level, we simply aggregated segment-level predictions by majority vote.", "labels": [], "entities": [{"text": "prediction", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9800817370414734}]}, {"text": "Results are summarized in table 4.", "labels": [], "entities": []}, {"text": "Overall, we observed a similar effect as on the ACL Anthology dataset: The full vocabulary model performed much worse than models with a frequency cutoff.", "labels": [], "entities": [{"text": "ACL Anthology dataset", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9601525664329529}]}, {"text": "In contrast to the ACL Anthology dataset, the best results were achieved at a frequency cutoff of 1,000.", "labels": [], "entities": [{"text": "ACL Anthology dataset", "start_pos": 19, "end_pos": 40, "type": "DATASET", "confidence": 0.9680044452349345}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics for the ACL Anthology  dataset.", "labels": [], "entities": [{"text": "ACL Anthology  dataset", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.9512660304705302}]}, {"text": " Table 2: Prediction accuracies for the five fre- quency cutoffs on development as test data (ACL).  The best result is marked in boldface.", "labels": [], "entities": [{"text": "Prediction accuracies", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8667287230491638}]}, {"text": " Table 3: Rank accuracies for different ranks r  on holdout test data (ACL). For example, a re- sult of 24.46% at r = 5 means that for 24.46%  of segments in the test data, the correct author was  among the top-5 predicted authors of the model.  Best results are marked in boldface.", "labels": [], "entities": [{"text": "re- sult", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9622408946355184}]}, {"text": " Table 4: Prediction accuracies on PAN 2012, task  I on segment and text levels for different frequency  cutoffs. Best results are marked in boldface.", "labels": [], "entities": [{"text": "Prediction accuracies", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8381919264793396}, {"text": "PAN 2012", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9160785377025604}]}]}