{"title": [{"text": "A Large Scale Quantitative Exploration of Modeling Strategies for Content Scoring", "labels": [], "entities": [{"text": "Content Scoring", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7518772184848785}]}], "abstractContent": [{"text": "We explore various supervised learning strategies for automated scoring of content knowledge fora large corpus of 130 different content-based questions spanning four subject areas (Science, Math, English Language Arts, and Social Studies) and containing over 230,000 responses scored by human raters.", "labels": [], "entities": []}, {"text": "Based on our analyses , we provide specific recommendations for content scoring.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7716915905475616}]}, {"text": "These are based on patterns observed across multiple questions and assessments and are, therefore, likely to generalize to other scenarios and prove useful to the community as automated content scoring becomes more popular in schools and classrooms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic scoring of free-text content-based questions is a challenging task.", "labels": [], "entities": [{"text": "scoring of free-text content-based questions", "start_pos": 10, "end_pos": 54, "type": "TASK", "confidence": 0.7739881277084351}]}, {"text": "Although it may appear similar to the task of automatically scoring student responses for writing quality), it has important differences.", "labels": [], "entities": []}, {"text": "Scoring for content deals with responses to open-ended questions designed to test primarily what the student knows, has learned, or can do in a specific subject area such as Computer Science, Math, Biology, or Music with fluency being secondary.", "labels": [], "entities": [{"text": "Scoring for content deals with responses to open-ended questions designed to test primarily what the student knows, has learned, or can do in a specific subject area such as Computer Science, Math, Biology, or Music", "start_pos": 0, "end_pos": 215, "type": "Description", "confidence": 0.8246456652879715}]}, {"text": "It is not important if the student makes some spelling mistakes or grammatical errors as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response.", "labels": [], "entities": []}, {"text": "Assessing the content of student responses requires a different set of features that pay attention to whether students are using the right concepts, the right relationships between those concepts, and whether they are providing the right amount of detail.", "labels": [], "entities": []}, {"text": "In addition, scoring for content generally requires building separate scoring models for each question since each question usually focuses on a specific set of concepts within a specific subject area.", "labels": [], "entities": []}, {"text": "However, automated scoring for writing quality can utilize \"generic\" scoring models that can be used to assess student responses independent of the question to which they were written since the aspects of writing being measured are topic-independent (.", "labels": [], "entities": []}, {"text": "In this paper, we focus on a content scoring approach that learns a scoring model by extracting a large number of sparse, binary features from human-scored responses to a given question.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.6933067589998245}]}, {"text": "The model can then predict scores for previously unseen responses to the question.", "labels": [], "entities": []}, {"text": "There are many decisions that one needs to make for such an approach: what machine learning algorithm is likely to give the best performance?", "labels": [], "entities": []}, {"text": "Is it better to use regression or classification?", "labels": [], "entities": []}, {"text": "Is it worth allocating additional data and time for tuning the hyper-parameters of the learning algorithm?", "labels": [], "entities": []}, {"text": "For many practical applications, the amount of humanscored data available may not even be sufficient for model training and evaluation let alone for these types of meta-analyses.", "labels": [], "entities": []}, {"text": "We conduct analyses on a large corpus of real student responses to identify patterns that are consistent across multiple questions and contexts and are, therefore, likely to generalize to other scenarios.", "labels": [], "entities": []}, {"text": "Our corpus contains 130 different questions spanning four different subject areas and more than 230,000 human-scored responses.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the largest collection of responses ever used in a study on automated content scoring.", "labels": [], "entities": [{"text": "automated content scoring", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.5941565136114756}]}, {"text": "The large number of questions allows us to test many of our hypotheses in a rigorous manner and convert the answers into specific recommendations for the community that we hope will be useful in guiding further development of supervised content scoring models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although we have a fairly large number of responses for each question, we choose to use crossvalidation instead of a single train-test split in or-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A detailed breakdown of our corpus by subject. N: number of questions; Number of Responses:  minimum and maximum number of human-scored responses available for questions on this subject;  Score Range: ranges of possible scores that can be awarded to responses for questions on this subject  according to the human-authored scoring rubrics; Grade Level: the grades of students whose responses  were used for analysis; Response Lengths: minimum and maximum number of characters in responses  to questions on a given subject.", "labels": [], "entities": [{"text": "Score Range", "start_pos": 198, "end_pos": 209, "type": "METRIC", "confidence": 0.9224485754966736}, {"text": "Grade Level", "start_pos": 350, "end_pos": 361, "type": "METRIC", "confidence": 0.9441854357719421}]}, {"text": " Table 3: Average MSE across 130 questions for  each of the eight learners using hyper-parameter  values that are either not tuned (default) values or  tuned via grid search. Lower values are better.", "labels": [], "entities": [{"text": "MSE", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.46204498410224915}]}]}