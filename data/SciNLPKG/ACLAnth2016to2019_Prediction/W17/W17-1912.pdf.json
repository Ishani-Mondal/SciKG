{"title": [{"text": "TTCS E : a Vectorial Resource for Computing Conceptual Similarity", "labels": [], "entities": [{"text": "Computing Conceptual Similarity", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.7577606042226156}]}], "abstractContent": [{"text": "In this paper we introduce the TTCS E , a linguistic resource that relies on BabelNet, NASARI and ConceptNet, that has now been used to compute the conceptual similarity between concept pairs.", "labels": [], "entities": [{"text": "TTCS E", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.6480230987071991}, {"text": "NASARI", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.8983432650566101}]}, {"text": "The conceptual representation herein provides uniform access to concepts based on Babel-Net synset IDs, and consists of a vector-based semantic representation which is compliant with the Conceptual Spaces, a geometric framework for common-sense knowledge representation and reasoning.", "labels": [], "entities": [{"text": "common-sense knowledge representation", "start_pos": 232, "end_pos": 269, "type": "TASK", "confidence": 0.6411350270112356}]}, {"text": "The TTCS E has been evaluated in a preliminary experimentation on a conceptual similarity task.", "labels": [], "entities": [{"text": "TTCS E", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.5025745183229446}]}], "introductionContent": [{"text": "The development of robust and wide-coverage resources to use in different sorts of application (such as text mining, categorization, etc.) has known in the last few years a tremendous growth.", "labels": [], "entities": [{"text": "text mining", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.8338861763477325}]}, {"text": "In this paper we focus on computing conceptual similarity between pairs of concepts, based on a resource that extends and generalizes an attempt carried out in ().", "labels": [], "entities": [{"text": "computing conceptual similarity between pairs of concepts", "start_pos": 26, "end_pos": 83, "type": "TASK", "confidence": 0.7737138782228742}]}, {"text": "In particular, the TTCS E -so named after Text to Conceptual Spaces-Extended-has been acquired by integrating two different sorts of linguistic resources, such as the encyclopedic knowledge available in BabelNet ( and NASARI, and the common-sense grasped by).", "labels": [], "entities": [{"text": "TTCS E", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.5360800474882126}, {"text": "NASARI", "start_pos": 218, "end_pos": 224, "type": "DATASET", "confidence": 0.9055814146995544}]}, {"text": "The resulting representation enjoys the interesting property of being anchored to both resources, thereby providing a uniform conceptual access grounded on the sense identifiers provided by BabelNet.", "labels": [], "entities": []}, {"text": "Conceptual Spaces (CSs) can bethought of as a particular class of vector representations where knowledge is represented as a set of limited though cognitively relevant quality dimensions; in this representation a geometrical structure is associated to each quality dimension, mostly based on cognitive accounts.", "labels": [], "entities": []}, {"text": "In this setting, concepts correspond to convex regions, and regions with different geometrical properties correspond to different sorts of concepts).", "labels": [], "entities": []}, {"text": "The geometrical features of CSs have a direct appeal for common-sense representation and commonsense reasoning, since prototypes (the most relevant representatives of a category from a cognitive point of view, see) correspond to the geometrical centre of a convex region, the centroid.", "labels": [], "entities": [{"text": "common-sense representation", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.8243132531642914}, {"text": "commonsense reasoning", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7999281585216522}]}, {"text": "Also exemplars-based representations can be mapped onto points in a multidimensional space, and their similarity can be computed as the distance intervening between each two points, based on some suitable metrics such as Euclidean or Manhattan distance.", "labels": [], "entities": []}, {"text": "The CS framework has been recently used to extend and complement the representational and inferential power allowed by formal ontologies -and in general symbolic representation-, that are not suited for representing defeasible, prototypical knowledge, and for dealing with the corresponding typicality-based conceptual reasoning (.", "labels": [], "entities": []}, {"text": "Also, wide-coverage semantic resources such as DBPedia and ConceptNet, in fact, in different cases fail to represent the sort of common-sense information based on prototypical and default information which is usually required to perform forms of plausible reasoning.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.93316650390625}]}, {"text": "In this paper we explore whether and to what extent a linguistic resource describing concepts by means of qualitative and synthetic vectorial representation is suited to assess the conceptual similarity between pairs of concepts.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experimentation we addressed the conceptual similarity task at the sense level, that is the TTCS E system has been fed with sense pairs.", "labels": [], "entities": [{"text": "TTCS E system", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.8361834685007731}]}, {"text": "We considered three datasets, 6 namely the RG, MC and WS-Sim datasets, first designed in and), respectively.", "labels": [], "entities": [{"text": "WS-Sim datasets", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.7851123511791229}]}, {"text": "Historically, while the first two (RG and MC) datasets were originally conceived for similarity measures, the WS-Sim dataset was developed as a subset of a former dataset built by) created to test similarity by including pairs of words related through specific relationships, such as synonymy, hyponymy, and unrelated.", "labels": [], "entities": [{"text": "WS-Sim dataset", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.9081653356552124}]}, {"text": "All senses were mapped onto WordNet 3.0.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9598568081855774}]}, {"text": "The similarity scores computed by the TTCS E system have been assessed through Pearsons r \u03c1 r RG 0.78 0.85 MC 0.77 0.80 WS-Sim 0.64 0.54: Spearman (\u03c1) and Pearson (r) correlations obtained over the three datasets. and Spearmans \u03c1 correlations, that are largely adopted for the conceptual similarity task (for a recent compendium of the approaches please refer to ).", "labels": [], "entities": [{"text": "TTCS E system", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.7580810387929281}, {"text": "Pearsons r \u03c1 r RG 0.78 0.85 MC 0.77 0.80 WS-Sim 0.64", "start_pos": 79, "end_pos": 131, "type": "METRIC", "confidence": 0.9023015598456064}, {"text": "Spearman (\u03c1)", "start_pos": 138, "end_pos": 150, "type": "METRIC", "confidence": 0.8790586441755295}, {"text": "Pearson (r) correlations", "start_pos": 155, "end_pos": 179, "type": "METRIC", "confidence": 0.9042382955551147}]}, {"text": "The former measure captures the linear correlation of two variables as their covariance divided by the product of their standard deviations, thus basically allowing to grasp differences in their values, whilst the Spearman correlation is computed as the Pearson correlation between the rank values of the considered variables, so it is reputed to be best suited to assess results in a similarity ranking setting where relative scores are relevant (.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 254, "end_pos": 273, "type": "METRIC", "confidence": 0.9312016665935516}]}, {"text": "shows the results obtained by the system in a preliminary experimentation.", "labels": [], "entities": []}, {"text": "Provided that the present task of sense-level similarity is slightly different from word-level similarity (about this distinction, please refer to ( ), and our results can be thus hardly compared to those in literature, the reported figures are still far from the state of the art, where the Spearman correlation \u03c1 reaches 0.92 for the RG dataset), 0.92 for the MC dataset (, and 0.81 for the WS-Sim dataset.", "labels": [], "entities": [{"text": "Spearman correlation \u03c1", "start_pos": 292, "end_pos": 314, "type": "METRIC", "confidence": 0.6593819657961527}, {"text": "RG dataset", "start_pos": 336, "end_pos": 346, "type": "DATASET", "confidence": 0.8671219944953918}, {"text": "MC dataset", "start_pos": 362, "end_pos": 372, "type": "DATASET", "confidence": 0.9687545895576477}, {"text": "WS-Sim dataset", "start_pos": 393, "end_pos": 407, "type": "DATASET", "confidence": 0.9836798906326294}]}, {"text": "However, we remark that the TTCS E employs vectors of a very limited size w.r.t. the standard vector-based resources used in the current models of distributional semantics (as mentioned, each vector is defined, on average, through 14.90 concepts).", "labels": [], "entities": [{"text": "TTCS E", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.6833721101284027}]}, {"text": "Moreover, due to the explicit grounding provided by connecting the NASARI feature values to the corresponding properties in ConceptNet, the TTCS E can be used to provide the scores returned as output with an explanation, based on the shared concepts along some given dimension.", "labels": [], "entities": [{"text": "NASARI feature values", "start_pos": 67, "end_pos": 88, "type": "DATASET", "confidence": 0.8875916997591654}, {"text": "TTCS E", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.7571777105331421}]}, {"text": "At the best of our knowledge, this is a unique feature, that cannot be easily reached by methods based on Latent Semantic Analysis (such as those pioneered by) and can be only partly approached by techniques exploiting taxonomic structures.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.5898158252239227}]}, {"text": "Conversely, few and relevant traits are present in the final linguistic resource, which is thus synthetic and more cognitively plausible.", "labels": [], "entities": []}, {"text": "In some cases -27 concept pairs out of the overall 190 pairs-the system was notable to retrieve an ID for one of the concepts in the pair: such pairs were excluded from the computation of the final accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9592171311378479}]}, {"text": "Missing concepts maybe lacking in (at least one of) the resources upon which the TTCS E is built: including further resources may thus be helpful to overcome this limitation.", "labels": [], "entities": [{"text": "TTCS E", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.764852374792099}]}, {"text": "Also, difficulties stemmed from insufficient information for the concepts at stake: this phenomenon was observed, e.g., when both concepts have been found, but no common dimension has been filled.", "labels": [], "entities": []}, {"text": "This sort of difficulty shows that the coverage of the resource still needs to be enhanced by improving the extraction phase, so to add further concepts per dimension, and to fill more dimensions.", "labels": [], "entities": []}], "tableCaptions": []}