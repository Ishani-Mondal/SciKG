{"title": [{"text": "Towards Neural Machine Translation with Latent Tree Attention", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.7839028636614481}]}], "abstractContent": [{"text": "Building models that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in natural language processing.", "labels": [], "entities": []}, {"text": "We introduce such a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel atten-tional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.804349809885025}]}, {"text": "When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many efforts to exploit linguistic hierarchy in NLP tasks make use of the output of a self-contained parser system trained from a human-annotated treebank ().", "labels": [], "entities": []}, {"text": "An alternative approach aims to jointly learn the task at hand and relevant aspects of linguistic hierarchy, inducing from an unannotated training dataset parse trees that mayor may not correspond to treebank annotation practices).", "labels": [], "entities": []}, {"text": "Most deep learning models for NLP that aim to make use of linguistic hierarchy integrate an external parser, either to prescribe the recursive structure of the neural network) or to provide a supervision signal or training data fora network that predicts its own structure).", "labels": [], "entities": []}, {"text": "But some recently described neural network models take the second approach and treat hierarchical structure as a latent variable, applying inference over graph-based conditional random fields ( , the straight-through estimator (, or policy gradient reinforcement learning () to workaround the inapplicability of gradient-based learning to problems with discrete latent states.", "labels": [], "entities": []}, {"text": "For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones ( and neural MT models augmented with morphosyntactic input features, a tree-structured encoder (, and a jointly trained parser () each outperforming purely-sequential baselines.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7975531220436096}]}, {"text": "Drawing on many of these precedents, we introduce an attentional neural machine translation model whose encoder and decoder components are both tree-structured neural networks that predict their own constituency structure as they consume or emit text.", "labels": [], "entities": [{"text": "attentional neural machine translation", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.6072270423173904}]}, {"text": "The encoder and decoder networks are variants of the RNNG model introduced by, allowing tree structures of unconstrained arity, while text is ingested at the character level, allowing the model to discover and make use of structure within words.", "labels": [], "entities": []}, {"text": "The parsing decisions of the encoder and decoder RNNGs are parameterized by a stochastic policy trained using a weighted sum of two objectives: a language model loss term that rewards predicting the next character with high likelihood, and a tree attention term that rewards one-to-one attentional correspondence between constituents in the encoder and decoder.", "labels": [], "entities": []}, {"text": "We evaluate this model on the German-English language pair of the flickr30k dataset, where it obtains similar performance to a strong character-level baseline.", "labels": [], "entities": [{"text": "flickr30k dataset", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9730492830276489}]}, {"text": "Analysis of the latent trees produced by the encoder and decoder shows that the model learns a reasonable segmentation and shallow parse, and most phrase-level constituents constructed while ingesting the German input sentence correspond meaningfully to constituents built while generating the English output.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}