{"title": [], "abstractContent": [{"text": "The field of Quality Estimation (QE) has the goal to provide automatic methods for the evaluation of Machine Translation (MT), that do not require reference translations in their computation.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.7146220564842224}, {"text": "evaluation of Machine Translation (MT)", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.7797270800386157}]}, {"text": "We present our submission to the sentence level WMT17 Quality Estimation Shared Task.", "labels": [], "entities": [{"text": "WMT17 Quality Estimation Shared Task", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.5689028382301331}]}, {"text": "It combines tree and sequence kernels for predicting the post-editing effort of the target sentence.", "labels": [], "entities": []}, {"text": "The kernels exploit both the source and target sentences, but also a back-translation of the candidate translation.", "labels": [], "entities": []}, {"text": "The evaluation results show that the kernel approach combined with the base-line features brings substantial improvement over the baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "The evaluation of Machine Translation (MT) output is a sub-field of MT research that has experienced a great amount of interest in the past years.", "labels": [], "entities": [{"text": "evaluation of Machine Translation (MT) output", "start_pos": 4, "end_pos": 49, "type": "TASK", "confidence": 0.8586583957076073}, {"text": "MT research", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9261727333068848}]}, {"text": "The process of MT evaluation involves three factors: an input segment in a source language, the candidate translation (also known as target sentence) which represents the output of a MT system when translating from the source language to the target language and a reference translation in the target language.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9657206237316132}, {"text": "MT", "start_pos": 183, "end_pos": 185, "type": "TASK", "confidence": 0.966500997543335}]}, {"text": "The assessment of MT quality can be divided into two categories depending on whether it requires the presence of a reference translation or not.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9954584836959839}]}, {"text": "The reference-based evaluation scores the candidate translation by comparing it to the reference translation.", "labels": [], "entities": []}, {"text": "On the other hand, the reference-free evaluation, also known as quality estimation (QE), predicts the quality of a candidate translation based solely on the information contained in the source and target sentences.", "labels": [], "entities": []}, {"text": "QE can be performed at different levels of granularity: word, sentence or phrase and it involves classifying, ranking or predicting scores for the candidate translations.", "labels": [], "entities": [{"text": "QE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7897015810012817}]}, {"text": "A sentence-level QE system is conventionally constructed based on a set of features encoding the information contained in the source and target sentences, which are used for learning a prediction model.", "labels": [], "entities": []}, {"text": "The features employed for this task can be of different types, like surface features, language model features or linguistic features.", "labels": [], "entities": []}, {"text": "The positive influence of syntactic features on the performance of QE systems has been extensively studied, including in, or more recently in.", "labels": [], "entities": []}, {"text": "However, the process of identifying the best performing set of features, is a task that is both expensive and requires a considerable amount of engineering effort).", "labels": [], "entities": []}, {"text": "On the other hand, kernel methods do not require the explicit definition of the features, and rely on the scalar product between vectors for capturing the similarity shared by the sentence pairs.", "labels": [], "entities": []}, {"text": "In this paper we present our submission to the WMT17 Shared Task on sentence level Quality Estimation, that makes use of sequence and tree kernels in predicting a continuous score representing the post-editing effort for the target sentence.", "labels": [], "entities": [{"text": "WMT17 Shared Task", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.730237086613973}, {"text": "sentence level Quality Estimation", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.49068867415189743}]}, {"text": "The novel contribution of our system is the combination of different types of kernels.", "labels": [], "entities": []}, {"text": "Moreover, we use a back-translation of the target sentence into the source language as an additional data representation to be exploited by the kernels, together with the usual source and target sentences representations.", "labels": [], "entities": []}, {"text": "Furthermore, we construct additional explicit features by applying the kernel functions directly on the pair of source and back-translation sentences, a method that to our knowledge has not been used before.", "labels": [], "entities": []}, {"text": "The evaluation performed demonstrates that the combination of the kernel approach and the baseline together with the newly introduced feature vectors brings consistent improvement over the baseline system.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "The related work is presented in Section 2, while the methods employed and the implementation are described in Section 3.", "labels": [], "entities": []}, {"text": "The experimental setup and the evaluation results are introduced in Section 4, while the last section summarizes our findings and presents future work ideas.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation was performed using the datasets released for the QE sentence-level shared task by the Second Conference On Machine Translation (WMT17) . The data consists of tuples, containing the source segment, the target sentence and a manually post-edited version of the target sentence, together with their associated post-editing score.", "labels": [], "entities": [{"text": "QE sentence-level shared task by the Second Conference On Machine Translation (WMT17)", "start_pos": 65, "end_pos": 150, "type": "TASK", "confidence": 0.5874964445829391}]}, {"text": "The WMT17 dataset is composed of both English-German and German-English tuples.", "labels": [], "entities": [{"text": "WMT17 dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9744489192962646}]}, {"text": "The English-German dataset, pertaining to the IT domain, consists of 23000 tuples for training, with additional 1000 instances for development.", "labels": [], "entities": [{"text": "English-German dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.6965405344963074}]}, {"text": "Two sets, comprised of 2000 units each, were made available for testing.", "labels": [], "entities": []}, {"text": "On the other hand, the German-English dataset provides 25000 tuples for training, 1000 units for development and a test set consisting of 2000 instances, with the general domain categorized as Pharmaceutical.", "labels": [], "entities": [{"text": "German-English dataset", "start_pos": 23, "end_pos": 45, "type": "DATASET", "confidence": 0.8458956480026245}, {"text": "Pharmaceutical", "start_pos": 193, "end_pos": 207, "type": "DATASET", "confidence": 0.9046624302864075}]}, {"text": "The QE baseline systems used for evaluation are based on the sets of 17 baseline features made available by the QE sentence-level shared task.", "labels": [], "entities": [{"text": "QE sentence-level shared task", "start_pos": 112, "end_pos": 141, "type": "TASK", "confidence": 0.5069669038057327}]}, {"text": "They consist of surface features (e.g the number of tokens/punctuation marks in the source sentence), language model features (e.g LM probability of the source/target sentences), but also n-gram based features (e.g percentage of unigrams in quartile 4 of frequency (higher frequency words) in a corpus of the source language).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results for the DE-EN dev set.", "labels": [], "entities": [{"text": "DE-EN dev set", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8129315574963888}]}, {"text": " Table 2: Evaluation results for the DE-EN test set.", "labels": [], "entities": [{"text": "DE-EN test set", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8393423159917196}]}, {"text": " Table 3: Evaluation results for the EN-DE dev set. The highlighted numbers correspond to the systems  submitted to the shared task.", "labels": [], "entities": [{"text": "EN-DE dev set", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8012964725494385}]}, {"text": " Table 4: Evaluation results for the EN-DE 2016 test set", "labels": [], "entities": [{"text": "EN-DE 2016 test set", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.9241294711828232}]}, {"text": " Table 5: Evaluation results for the EN-DE 2017 test set.", "labels": [], "entities": [{"text": "EN-DE 2017 test set", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.9451328366994858}]}]}