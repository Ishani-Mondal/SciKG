{"title": [{"text": "Detection of Verbal Multi-Word Expressions via Conditional Random Fields with Syntactic Dependency Features and Semantic Re-Ranking", "labels": [], "entities": []}], "abstractContent": [{"text": "A description of a system for identifying Verbal Multi-Word Expressions (VMWEs) in running text is presented.", "labels": [], "entities": [{"text": "identifying Verbal Multi-Word Expressions (VMWEs) in running text", "start_pos": 30, "end_pos": 95, "type": "TASK", "confidence": 0.6966400802135467}]}, {"text": "The system mainly exploits universal syntactic dependency features through a Conditional Random Fields (CRF) sequence model.", "labels": [], "entities": []}, {"text": "The system competed in the Closed Track at the PARSEME VMWE Shared Task 2017, ranking 2nd place inmost languages on full VMWE-based evaluation and 1st in three languages on token-based evaluation.", "labels": [], "entities": [{"text": "PARSEME VMWE Shared Task 2017", "start_pos": 47, "end_pos": 76, "type": "DATASET", "confidence": 0.7215389251708985}]}, {"text": "In addition, this paper presents an option to re-rank the 10 best CRF-predicted sequences via semantic vectors, boosting its scores above other systems in the competition.", "labels": [], "entities": []}, {"text": "We also show that all systems in the competition would struggle to beat a simple lookup base-line system and argue fora more purpose-specific evaluation scheme.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automatic identification of Multi-Word Expressions (MWEs) or collocations has long been recognised as an important but challenging task in Natural Language Processing (NLP)).", "labels": [], "entities": [{"text": "automatic identification of Multi-Word Expressions (MWEs) or collocations", "start_pos": 4, "end_pos": 77, "type": "TASK", "confidence": 0.8227259755134583}]}, {"text": "An effort in response to this challenge is the Shared Task on detecting multi-word, verbal constructions () organised by the PARSing and Multiword Expressions (PARSEME) European COST Action . The Shared Task consisted of two tracks: a closed one, restricted to the data provided by the organisers, and an open track that permitted participants to employ additional external data.", "labels": [], "entities": [{"text": "detecting multi-word, verbal constructions", "start_pos": 62, "end_pos": 104, "type": "TASK", "confidence": 0.6676929950714111}, {"text": "PARSing and Multiword Expressions (PARSEME) European COST Action", "start_pos": 125, "end_pos": 189, "type": "TASK", "confidence": 0.674260014295578}]}, {"text": "The ADAPT team participated in the Closed 1 http://www.parseme.eu Track with a system 2 that exploits syntactic dependency features in a Conditional Random Fields (CRF) sequence model (), ranking 2nd place in the detection of full MWEs inmost languages . To the best of our knowledge, this is the first time that a CRF model is applied to the identification of verbal MWEs (VMWEs) in a large collection of distant languages.", "labels": [], "entities": [{"text": "identification of verbal MWEs (VMWEs)", "start_pos": 343, "end_pos": 380, "type": "TASK", "confidence": 0.8269686528614589}]}, {"text": "In addition to our CRF-based solution officially submitted to the closed track, our team also explored an option to re-rank the top 10 sequences predicted by the CRF decoder using a regression model trained on word co-occurrence semantic vectors computed from Europarl.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 260, "end_pos": 268, "type": "DATASET", "confidence": 0.9925117492675781}]}, {"text": "This semantic re-ranking step would qualify for the open track, however its results were not submitted to the official competition as we were unable to obtain its results in time for it.", "labels": [], "entities": []}, {"text": "This paper describes our official CRF-based solution (Sec. 3), as well as our unofficial Semantic Re-Ranker (.", "labels": [], "entities": []}, {"text": "Since the Shared Task's main goal is to enable a discussion of the challenges of identifying VMWEs across languages, this paper also offers some observations (Sec. 5).", "labels": [], "entities": []}, {"text": "In particular, we found that test files contain VMWEs that also occur in the training files, helping all systems in the competition, but also implying that a simple lookup system that only predicts MWEs it encountered in the training set will fare very well in the competition, and will in fact beat most systems.", "labels": [], "entities": []}, {"text": "We also argue fora more purpose-based evaluation scheme.", "labels": [], "entities": []}, {"text": "And we offer our conclusions and ideas for future work (Sec. 6).", "labels": [], "entities": []}, {"text": "have been developed, such as combining statistical and symbolic methods (), single and multi-prototype word embeddings (, integrating MWE identification within larger NLP tasks such as parsing) and machine translation).", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.962586909532547}, {"text": "machine translation", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.7818695902824402}]}, {"text": "More directly related to our closed-track approach are works such as that of, who showed that information about the degree of compositionality of MWEs helps the word alignment of verbs, and of who used sentence surface features based on the canonical form of VMWEs.", "labels": [], "entities": [{"text": "word alignment of verbs", "start_pos": 161, "end_pos": 184, "type": "TASK", "confidence": 0.7978852242231369}]}, {"text": "In addition, applied a Hidden Semi-CRF model to capture latent semantics from Chinese microblogging posts; used double-chained CRF for minimal semantic units detection in SemEval task.", "labels": [], "entities": [{"text": "minimal semantic units detection", "start_pos": 135, "end_pos": 167, "type": "TASK", "confidence": 0.7877219021320343}]}, {"text": "And discussed that syntactic construction classes are helpful for verb-noun and verb-particle MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8674151003360748}]}, {"text": "also used a sequence tagger to annotate MWEs, including VMWEs, while and have used CRF taggers for identifying contiguous MWEs.", "labels": [], "entities": []}, {"text": "In relation to our open-track approach, exploited large corpora to identify Arabic MWEs, and applied fixed-size continuous vector representations for various length of phrases and chunks in the MWE identification task.", "labels": [], "entities": [{"text": "identify Arabic MWEs", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.6256255110104879}, {"text": "MWE identification task", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.9500479102134705}]}, {"text": "used a re-ranker for MWEs in an n-best parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "F1 scores on the test set for the Semantic ReRanking of CRF outputs can be seen in under the \"sem\" heading.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9948370456695557}]}, {"text": "As can be seen, in nearly every language the Semantic Re-Ranking improves the CRF best prediction considerably.", "labels": [], "entities": [{"text": "CRF best prediction", "start_pos": 78, "end_pos": 97, "type": "METRIC", "confidence": 0.7687788009643555}]}, {"text": "These promising results are obtained with the first \"proof of concept\" version of the Semantic ReRanking component, that we plan to develop further in future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 scores (per category and overall) on the test set for our official CRF-based (\"crf\") and our  unofficial Semantic Re-Ranking (\"sem\") systems, with per category and overall MWE counts (\"n\") in  the test set. PS refers to the MWEs in the test set that were Previously Seen in the training set: the % of  Previously Seen MWEs and the F1 Score obtained by interpreting % as a Recall score and assuming a  100% Precision score.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9991899132728577}, {"text": "F1 Score", "start_pos": 344, "end_pos": 352, "type": "METRIC", "confidence": 0.9868226945400238}, {"text": "Precision score", "start_pos": 419, "end_pos": 434, "type": "METRIC", "confidence": 0.9717162847518921}]}, {"text": " Table 2: Number of languages each system ranked  at. Systems in grey italics are open systems, the  rest are closed. PS and sem are unofficial systems.", "labels": [], "entities": []}]}