{"title": [{"text": "A Report on the 2017 Native Language Identification Shared Task", "labels": [], "entities": [{"text": "2017 Native Language Identification Shared", "start_pos": 16, "end_pos": 58, "type": "TASK", "confidence": 0.640230405330658}]}], "abstractContent": [{"text": "Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7676166296005249}]}, {"text": "It is typically framed as a classification task where the set of L1s is known a priori.", "labels": [], "entities": [{"text": "classification task", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8939961493015289}]}, {"text": "Two previous shared tasks on NLI have been organized where the aim was to identify the L1 of learners of English based on essays (2013) and spoken responses (2016) they provided during a standardized assessment of academic English proficiency.", "labels": [], "entities": []}, {"text": "The 2017 shared task combines the inputs from the two prior tasks for the first time.", "labels": [], "entities": []}, {"text": "There are three tracks: NLI on the essay only, NLI on the spoken response only (based on a transcription of the response and i-vector acoustic features), and NLI using both responses.", "labels": [], "entities": []}, {"text": "We believe this makes fora more interesting shared task while building on the methods and results from the previous two shared tasks.", "labels": [], "entities": []}, {"text": "In this paper, we report the results of the shared task.", "labels": [], "entities": []}, {"text": "A total of 19 teams competed across the three different sub-tasks.", "labels": [], "entities": []}, {"text": "The fusion track showed that combining the written and spoken responses provides a large boost in prediction accuracy.", "labels": [], "entities": [{"text": "prediction", "start_pos": 98, "end_pos": 108, "type": "TASK", "confidence": 0.9586604833602905}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9394207000732422}]}, {"text": "Multiple classifier systems (e.g. ensembles and meta-classifiers) were the most effective in all tasks, with most based on traditional classifiers (e.g. SVMs) with lexical/syntactic features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their writing or speech in another language (L2).", "labels": [], "entities": [{"text": "Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their writing or speech in another language (L2)", "start_pos": 0, "end_pos": 177, "type": "Description", "confidence": 0.6929120493657661}]}, {"text": "NLI works by identifying language use patterns that are common to certain groups of speakers that share the same native language.", "labels": [], "entities": []}, {"text": "This process is underpinned by the presupposition that an author's linguistic background will dispose them towards particular language production patterns in their learned languages, as influenced by their mother tongue.", "labels": [], "entities": []}, {"text": "Predicting the native language of a writer has applications in different fields.", "labels": [], "entities": []}, {"text": "It can be used for authorship identification (, forensic analysis, tracing linguistic influence in potentially multi-author texts ( , and naturally to support Second Language Acquisition research.", "labels": [], "entities": [{"text": "authorship identification", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.8984204530715942}, {"text": "forensic analysis", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8068228960037231}, {"text": "Second Language Acquisition", "start_pos": 159, "end_pos": 186, "type": "TASK", "confidence": 0.6580401659011841}]}, {"text": "It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user ().", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.6388454834620158}]}, {"text": "Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages ( . Work by,, and set the stage for much of the recent research efforts.", "labels": [], "entities": [{"text": "predicting the native language of an ESL (English as a Second Language) writer", "start_pos": 28, "end_pos": 106, "type": "TASK", "confidence": 0.8424768924713135}]}, {"text": "However, it was the 2013 Native Language Identification Shared Task () that led to an explosion of interest in this area by making public a large dataset developed specifically for this task called the TOEFL11 ().", "labels": [], "entities": [{"text": "2013 Native Language Identification Shared Task", "start_pos": 20, "end_pos": 67, "type": "TASK", "confidence": 0.7346514960130056}, {"text": "TOEFL11", "start_pos": 202, "end_pos": 209, "type": "DATASET", "confidence": 0.8338808417320251}]}, {"text": "In that shared task, 29 teams participated, making it one of the largest NLP competitions that year alone.", "labels": [], "entities": []}, {"text": "In addition to analyzing the written responses, a recent trend in NLP research has been the use of speech transcripts (generated manually or via Automatic Speech Recognition) and audio features for dialect identification ), a task that involves identifying specific dialects of pluricentric languages, such as Spanish or Arabic.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 198, "end_pos": 220, "type": "TASK", "confidence": 0.7490567266941071}]}, {"text": "The combination of transcripts and acoustic features has also provided good results for dialect identification (), demonstrating that it is possible to improve performance by combining this information.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7856484055519104}]}, {"text": "While there has been growing interest in using such features, the use of speech transcripts for NLI is not entirely new.", "labels": [], "entities": []}, {"text": "In fact, the very first NLI study by was based on applying a Naive Bayes classifier to transcriptions of speech from native and non-native speakers, albeit using limited data.", "labels": [], "entities": []}, {"text": "However, this strand of NLI research has not received much attention, most likely due to the costly and laborious nature of collecting and transcribing non-native speech.", "labels": [], "entities": []}, {"text": "Following this trend, the 2016 Computational Paralinguistics Challenge () also included an NLI task based on the spoken response using the raw audio.", "labels": [], "entities": []}, {"text": "The NLI Shared Task 2017 attempts to combine these approaches by including a written response (essay) and a spoken response (speech transcript and i-vector acoustic features) for each candidate.", "labels": [], "entities": []}, {"text": "The competition also allows for the fusion of all features, a novel task that has not been previously tried.", "labels": [], "entities": []}, {"text": "Another motivation for this task was the rapid growth of deep learning methods for natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 83, "end_pos": 116, "type": "TASK", "confidence": 0.7090146169066429}]}, {"text": "In prior shared tasks, there were several barriers to using deep learning for NLP.", "labels": [], "entities": []}, {"text": "However, deep learning has now had a positive impact on many tasks across NLP and it is an area of investigation on whether the same successes can be found in NLI.", "labels": [], "entities": []}, {"text": "In the following section, we provide a summary of the prior work in Native Language Identification, for both text and speech based tracks.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.6565929353237152}]}, {"text": "Next, in \u00a73, we describe the data used for training, de-velopment, and testing in this shared task.", "labels": [], "entities": []}, {"text": "In \u00a74 we describe the results of each sub-task, with a short description of each team's submission.", "labels": [], "entities": []}, {"text": "Then in \u00a75, we discuss the commonalities and trends in and across the three sub-tasks, and present an ensemble analysis of all submissions.", "labels": [], "entities": []}, {"text": "Finally, in \u00a76, we offer conclusions and ideas for avenues of research in this growing field.", "labels": [], "entities": []}], "datasetContent": [{"text": "The majority of NLI research to date has reported results using accuracy as the main metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9993656277656555}]}, {"text": "For this task, however, we decided to use the macroaveraged F1-score as the official evaluation metric.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.969258189201355}]}, {"text": "The macro-averaged F1-score is calculated by first computing the F1-score for each class, and then taking the average across all classes).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9689570665359497}, {"text": "F1-score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9971900582313538}]}, {"text": "This metric favors more consistent performance across classes rather than simply measuring global performance across all samples.", "labels": [], "entities": []}, {"text": "Accuracy was still reported for completeness.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9916467070579529}, {"text": "completeness", "start_pos": 32, "end_pos": 44, "type": "METRIC", "confidence": 0.8570704460144043}]}, {"text": "We also used statistical significance testing for ranking purposes.", "labels": [], "entities": []}, {"text": "McNemar's test (with an alpha value of 0.05) was applied to the ordered results to identify groups of teams where the highest and lowest results were not significantly different, and they were therefore assigned the same rank.", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7635305523872375}]}, {"text": "For comparison, we compare to two types of baselines: a random baseline and one that use a linear SVM classifier.", "labels": [], "entities": []}, {"text": "There were three random baselines, one for each task, and five simple SVM baselines in total across the three tasks.", "labels": [], "entities": []}, {"text": "For the essay-only task there was one baseline based on raw unigram frequencies from the essay texts.", "labels": [], "entities": []}, {"text": "For the speech-only task there were two baselines: one an SVM based on raw unigram frequencies from the orthographic transcriptions alone, and a second SVM that combined the unigram features with the i-vectors using horizontal concatenation.", "labels": [], "entities": []}, {"text": "For the fusion task there were two baselines: one, an SVM combining the unigrams from the essays and the transcriptions, and a second SVM combining the unigrams from the essays and the transcriptions with the i-vectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official results in the essay-only track. The official metric is the macro-averaged F1-score.  Accuracy (Acc.) is also reported. Rankings are determined by statistical significance testing (see  \u00a73.1).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9111655354499817}, {"text": "Accuracy (Acc.)", "start_pos": 105, "end_pos": 120, "type": "METRIC", "confidence": 0.8814281821250916}]}, {"text": " Table 2: Official results in the speech-only track. The official metric is the macro-averaged F1-score.  Accuracy (Acc.) is also reported. Rankings are determined by statistical significance testing (see  \u00a73.1).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9029366970062256}, {"text": "Accuracy (Acc.)", "start_pos": 106, "end_pos": 121, "type": "METRIC", "confidence": 0.8798272609710693}]}, {"text": " Table 3: Official results in the fusion track. The official metric is the macro-averaged F1-score. Accuracy  (Acc.) is also reported. Team rankings are determined by statistical significance testing (see  \u00a73.1).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9305859804153442}, {"text": "Accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9992700219154358}, {"text": "Acc.)", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.915111780166626}]}, {"text": " Table 4: Oracle results on the NLI 2013 and 2017 shared task systems. The ensemble includes each  team's best system in each track. Results are reported as the macro-averaged F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9699838757514954}]}]}