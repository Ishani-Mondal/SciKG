{"title": [{"text": "LIUM-CVC Submissions for WMT17 Multimodal Translation Task", "labels": [], "entities": [{"text": "WMT17 Multimodal Translation", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6682018538316091}]}], "abstractContent": [{"text": "This paper describes the monomodal and multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT17 Shared Task on Multimodal Translation.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6147955556710561}, {"text": "WMT17 Shared Task on Multimodal Translation", "start_pos": 115, "end_pos": 158, "type": "TASK", "confidence": 0.5855436970790228}]}, {"text": "We mainly explored two mul-timodal architectures where either global visual features or convolutional feature maps are integrated in order to benefit from visual context.", "labels": [], "entities": []}, {"text": "Our final systems ranked first for both En\u2192De and En\u2192Fr language pairs according to the automatic evaluation metrics METEOR and BLEU.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9455316066741943}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9962576627731323}]}], "introductionContent": [{"text": "With the recent advances in deep learning, purely neural approaches to machine translation, such as Neural Machine Translation (NMT),) have received a lot of attention because of their competitive performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7340067923069}, {"text": "Neural Machine Translation (NMT),)", "start_pos": 100, "end_pos": 134, "type": "TASK", "confidence": 0.8235289951165518}]}, {"text": "Another reason for the popularity of NMT is its flexible nature allowing researchers to fuse auxiliary information sources in order to design sophisticated networks like multi-task, multi-way and multi-lingual systems to name a few (.", "labels": [], "entities": []}, {"text": "Multimodal Machine Translation (MMT) aims to achieve better translation performance by visually grounding the textual representations.", "labels": [], "entities": [{"text": "Multimodal Machine Translation (MMT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.817324678103129}]}, {"text": "Recently, anew shared task on Multimodal Machine Translation and Crosslingual Image Captioning (CIC) was proposed along with WMT16 ( . In this paper, we present MMT systems jointly designed by LIUM and CVC for the second edition of this task within WMT17.", "labels": [], "entities": [{"text": "Multimodal Machine Translation and Crosslingual Image Captioning (CIC)", "start_pos": 30, "end_pos": 100, "type": "TASK", "confidence": 0.6842742294073105}, {"text": "WMT16", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.9446632862091064}, {"text": "WMT17", "start_pos": 249, "end_pos": 254, "type": "DATASET", "confidence": 0.9663628339767456}]}, {"text": "Last year we proposed a multimodal attention mechanism where two different attention distributions were estimated over textual and image representations using shared transformations ().", "labels": [], "entities": []}, {"text": "More specifically, convolutional feature maps extracted from a ResNet-50 CNN ( pre-trained on the ImageNet classification task ( were used to represent visual information.", "labels": [], "entities": [{"text": "ResNet-50 CNN", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.8105083107948303}, {"text": "ImageNet classification task", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.7822227279345194}]}, {"text": "Although our submission ranked first among multimodal systems for CIC task, it was notable to improve over purely textual NMT baselines in neither tasks ( ).", "labels": [], "entities": []}, {"text": "The winning submission for MMT) was a phrase-based MT system rescored using a language model enriched with FC 7 global visual features extracted from a pre-trained VGG-19 CNN).", "labels": [], "entities": [{"text": "MMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9345856308937073}, {"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.8500117659568787}, {"text": "VGG-19 CNN", "start_pos": 164, "end_pos": 174, "type": "DATASET", "confidence": 0.8934973478317261}]}, {"text": "State-of-the-art results were obtained after WMT16 by using a separate attention mechanism for different modalities in the context of CIC () and MMT ().", "labels": [], "entities": [{"text": "WMT16", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.9165118336677551}]}, {"text": "Besides experimenting with multimodal attention, and Libovick\u00b4yLibovick\u00b4y and Helcl (2017) also proposed a gating extension inspired from which is believed to allow the decoder to learn when to attend to a particular modality although Libovick\u00b4yLibovick\u00b4y and Helcl (2017) report no improvement over baseline NMT.", "labels": [], "entities": []}, {"text": "There have also been attempts to benefit from different types of visual information instead of relying on features extracted from a CNN pretrained on ImageNet.", "labels": [], "entities": []}, {"text": "One such study from extended the sequence of source embeddings consumed by the RNN with several regional features extracted from a region-proposal network).", "labels": [], "entities": [{"text": "RNN", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8553600907325745}]}, {"text": "The architecture thus predicts a single attention distribution over a sequence of mixed-modality representations leading to significant improvement over their NMT baseline.", "labels": [], "entities": []}, {"text": "More recently, a radically different multi-task architecture called) is proposed to learn visually grounded representations by sharing an encoder between two tasks: a classical encoder-decoder NMT and a visual feature reconstruction using as input the source sentence representation.", "labels": [], "entities": []}, {"text": "This year, we experiment 1 with both convolutional and global visual vectors provided by the organizers to better exploit multimodality (Section 3).", "labels": [], "entities": []}, {"text": "Data preprocessing for both English\u2192{German,French} and training hyperparameters are detailed respectively in Section 2 and Section 4.", "labels": [], "entities": []}, {"text": "The results based on automatic evaluation metrics are reported in Section 5.", "labels": [], "entities": []}, {"text": "The paper ends with a discussion in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Flickr En\u2192De results: underlined METEOR scores are from systems significantly different  (p-value \u2264 0.05) than the baseline using the approximate randomization test of multeval for 5 runs. (D6)", "labels": [], "entities": [{"text": "METEOR", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9857116341590881}]}, {"text": " Table 2: MSCOCO En\u2192De results: the best  Flickr system trg-mul", "labels": [], "entities": [{"text": "MSCOCO En", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.574887290596962}, {"text": "Flickr", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9080218076705933}]}, {"text": " Table 4: MSCOCO En\u2192Fr results: ens-mmt-6,  the best performing ensemble on Test2016 corpus  (see", "labels": [], "entities": [{"text": "MSCOCO En\u2192Fr results", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.5646765947341919}, {"text": "Test2016 corpus", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.9881353080272675}]}, {"text": " Table 5.1) has been used for this submission  as well.", "labels": [], "entities": []}]}