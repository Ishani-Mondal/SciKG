{"title": [{"text": "Decoupling Encoder and Decoder Networks for Abstractive Document Summarization", "labels": [], "entities": [{"text": "Abstractive Document Summarization", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.7463161945343018}]}], "abstractContent": [{"text": "ive document summarization seeks to automatically generate a summary fora document, based on some abstract \"understanding\" of the original document.", "labels": [], "entities": []}, {"text": "State-of-the-art techniques traditionally use attentive encoder-decoder architectures.", "labels": [], "entities": []}, {"text": "However, due to the large number of parameters in these models, they require large training datasets and long training times.", "labels": [], "entities": []}, {"text": "In this paper, we propose decoupling the encoder and decoder networks, and training them separately.", "labels": [], "entities": []}, {"text": "We encode documents using an unsupervised document encoder, and then feed the document vector to a recurrent neural network decoder.", "labels": [], "entities": []}, {"text": "With this decoupled architecture, we decrease the number of parameters in the decoder substantially, and shorten its training time.", "labels": [], "entities": []}, {"text": "Experiments show that the decoupled model achieves comparable performance with state-of-the-art models for in-domain documents, but less well for out-of-domain documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstractive document summarization is a challenging natural language understanding task.", "labels": [], "entities": [{"text": "Abstractive document summarization", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8187907536824545}, {"text": "natural language understanding", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.6521394650141398}]}, {"text": "Abstractive methods first encode the original document into a high-level representation, and then decode it into the target summary.", "labels": [], "entities": []}, {"text": "proposed the task of headline generation as the first step towards abstractive summarization.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8867707252502441}, {"text": "abstractive summarization", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.5548205971717834}]}, {"text": "Instead of using the full document, the authors experimented with using the first sentence as input, with the aim of generating a coherent headline given the sentence.", "labels": [], "entities": []}, {"text": "The current state-of-art system for the task is based on an attentive encoder and a recurrent decoder (, which is an extension of the methodology of.", "labels": [], "entities": []}, {"text": "The encoder and decoder are trained jointly, and the decoder attends to different parts of the document during generation.", "labels": [], "entities": []}, {"text": "It has a large number of parameters, and thus requires large-scale training data and long training times.", "labels": [], "entities": []}, {"text": "In this paper, we propose decoupling the encoder and decoder.", "labels": [], "entities": []}, {"text": "We encode documents using doc2vec (, as it has been demonstrated to be a competitive unsupervised document encoder (.", "labels": [], "entities": []}, {"text": "We incorporate doc2vec vectors of input documents to the decoder as an additional signal, to generate sentences that are not only coherent but are also related to the original documents.", "labels": [], "entities": []}, {"text": "Compared to the standard joint encoder-decoder design, the decoupled architecture has less parameters for the decoder, and thus requires less training data and trains faster.", "labels": [], "entities": []}, {"text": "The downside of the decoupled architecture is that the doc2vec signal is not updated in the decoder, and its document representation could be sub-optimal for the decoder to generate good summaries.", "labels": [], "entities": []}, {"text": "Our experiments reveal that the decoupled architecture works well in-domain, but less well out-of-domain, as a consequence of the fixed capacity of the document encoding as well as having no explicit copy mechanism.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our decoupled architecture for the headline generation task.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8805546760559082}]}, {"text": "Following, we run experiments using GIGAWORD, DUC03 and DUC04.", "labels": [], "entities": [{"text": "GIGAWORD", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.6827870607376099}, {"text": "DUC03", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9044842720031738}, {"text": "DUC04", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9292065501213074}]}, {"text": "GIGAWORD is preprocessed according to, yielding 4.3 million examples.", "labels": [], "entities": [{"text": "GIGAWORD", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8680325746536255}]}, {"text": "For in-domain experiments, we randomly sample 2,000 examples for each validation and test set, and use the remaining examples for training.", "labels": [], "entities": []}, {"text": "We tune hyper-parameters based on validation perplexity and evaluate performance on the test set", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of ROUGE scores (full  length F-score) for in-domain experiments.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.9632368981838226}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.8600924611091614}]}, {"text": " Table 3: Comparison of ROUGE scores (recall at  75 bytes) for out-of-domain experiments.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.9649643301963806}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.991028368473053}]}]}