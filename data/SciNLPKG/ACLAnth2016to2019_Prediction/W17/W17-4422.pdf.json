{"title": [{"text": "Transfer Learning and Sentence Level Features for Named Entity Recognition on Tweets", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6867565313975016}]}], "abstractContent": [{"text": "We present our system for the WNUT 2017 Named Entity Recognition challenge on Twitter data.", "labels": [], "entities": [{"text": "WNUT 2017 Named Entity Recognition challenge", "start_pos": 30, "end_pos": 74, "type": "TASK", "confidence": 0.7863999009132385}, {"text": "Twitter data", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.6731753796339035}]}, {"text": "We describe two modifications of a basic neural network architecture for sequence tagging.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.6948471814393997}]}, {"text": "First, we show how we exploit additional labeled data, where the Named Entity tags differ from the target task.", "labels": [], "entities": []}, {"text": "Then, we propose away to incorporate sentence level features.", "labels": [], "entities": []}, {"text": "Our system uses both methods and ranked second for entity level annotations, achieving an F1-score of 40.78, and second for surface form annotations, achieving an F1-score of 39.33.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9996534585952759}, {"text": "F1-score", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9988110065460205}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) is an important Natural Language Processing task.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7582525908946991}]}, {"text": "Its goal is to tag entities such as names of people and locations in text.", "labels": [], "entities": []}, {"text": "State-of-the-art systems can achieve F1-scores of up to 92 points on English news texts (.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9991488456726074}]}, {"text": "Achieving good performance on more complex domains such as user generated texts on social media is still a hard problem.", "labels": [], "entities": []}, {"text": "The best system submitted for the WNUT 2016 shared task achieved an F1-score of 52.41 on English Twitter data (.", "labels": [], "entities": [{"text": "WNUT 2016 shared task", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.7419601082801819}, {"text": "F1-score", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9995961785316467}, {"text": "English Twitter data", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.9109010497728983}]}, {"text": "In this work, we present our submission for the WNUT 2017 shared task on \"Novel and Emerging Entity Recognition\").", "labels": [], "entities": [{"text": "WNUT 2017 shared task", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.8213134706020355}, {"text": "Novel and Emerging Entity Recognition", "start_pos": 74, "end_pos": 111, "type": "TASK", "confidence": 0.48440841436386106}]}, {"text": "We extend a basic neural network architecture for sequence tagging () by incorporating sentence level feature vectors and exploiting additional labeled data for transfer learning.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.715256005525589}, {"text": "transfer learning", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.9263741672039032}]}, {"text": "We build on and take inspiration from recent work from on NER for French Twitter data (.", "labels": [], "entities": [{"text": "French Twitter data", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.8602189819018046}]}, {"text": "Our submitted solution reached an F1-score of 41.76 for entity level annotations and 57.98 on surface form annotations.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9997362494468689}]}, {"text": "This places us second on entity level annotations, where the best system achieved an F1-score of 41.90, and fourth on surface form annotations, where the best system achieved an F1-score of 66.59.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9993599057197571}, {"text": "F1-score", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9981928467750549}]}], "datasetContent": [{"text": "We implemented the system described in Section 2.4 using the Tensorflow framework . We monitored the systems performance during training and aborted experiments that had an F1-score of less than 40 after two epochs (evaluated on the development set).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9992051720619202}]}, {"text": "We let successful experiments run for the full 6 epochs (cf. Section 3.2).", "labels": [], "entities": []}, {"text": "For the submission to WNUT 2017, we ran 6 successful experiments and submitted the one which  had the highest entity level F1-score on the development set.", "labels": [], "entities": [{"text": "WNUT 2017", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.8697700202465057}, {"text": "F1-score", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9451903700828552}]}, {"text": "Following the submission, we conducted additional experiments to investigate the influence of the transfer learning approach and sent2vec features on the system performance.", "labels": [], "entities": []}, {"text": "For each of the 4 systems described in Section 2, we ran 6 experiments.", "labels": [], "entities": []}, {"text": "We use the same parameters as shown in Section 3.2.", "labels": [], "entities": []}, {"text": "shows precision, recall and F1-score of our system.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9997420907020569}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9997453093528748}, {"text": "F1-score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9997594952583313}]}, {"text": "We compute the mean and standard deviations over the 6 successful experiments we considered for submission (cf. Section 3).", "labels": [], "entities": [{"text": "standard deviations", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.8739280104637146}]}, {"text": "shows the breakdown of the performance of the annotations we submitted for the WNUT 2017 shared task.", "labels": [], "entities": [{"text": "WNUT 2017 shared task", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.8286126255989075}]}, {"text": "shows the performance of the different subsystems proposed in Section 2.", "labels": [], "entities": []}, {"text": "We report the mean and standard deviation over the 6 experiments we performed after submission, for every system.", "labels": [], "entities": [{"text": "mean", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9726138710975647}, {"text": "standard deviation", "start_pos": 23, "end_pos": 41, "type": "METRIC", "confidence": 0.8939298093318939}]}], "tableCaptions": [{"text": " Table 2: Aggregated performance of all experi- ments, run before the submission, evaluated on the  test set", "labels": [], "entities": [{"text": "Aggregated", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9809445142745972}]}, {"text": " Table 3: Performance of the submitted annotations  evaluated on the test set", "labels": [], "entities": []}, {"text": " Table 4: Performance of the different subsystems evaluated on the test set, after the submission", "labels": [], "entities": []}]}