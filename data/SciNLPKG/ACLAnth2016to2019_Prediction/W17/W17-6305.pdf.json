{"title": [{"text": "Prepositional Phrase Attachment over Word Embedding Products", "labels": [], "entities": [{"text": "Prepositional Phrase Attachment", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7956493298212687}]}], "abstractContent": [{"text": "We present a low-rank multi-linear model for the task of solving prepositional phrase attachment ambiguity (PP task).", "labels": [], "entities": [{"text": "solving prepositional phrase attachment ambiguity (PP task)", "start_pos": 57, "end_pos": 116, "type": "TASK", "confidence": 0.8290454745292664}]}, {"text": "Our model exploits tensor products of word embeddings, capturing all possible conjunctions of latent embeddings.", "labels": [], "entities": []}, {"text": "Our results on a wide range of datasets and task settings show that tensor products are the best compositional operation and that a relatively simple multi-linear model that uses only word embeddings of lexical features can outperform more complex non-linear architectures that exploit the same information.", "labels": [], "entities": []}, {"text": "Our proposed model gives the current best reported performance on an out-of-domain evaluation and performs competively on out-of-domain dependency parsing datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Prepositional Phrase (PP) attachment problem () is a classic ambiguity problem and is one of the main sources of errors for syntactic parsers (.", "labels": [], "entities": [{"text": "Prepositional Phrase (PP) attachment problem", "start_pos": 4, "end_pos": 48, "type": "TASK", "confidence": 0.6833739876747131}]}, {"text": "For the first case, the correct attachment is the prepositional phrase attaching to the restaurant, the noun.", "labels": [], "entities": []}, {"text": "Whereas, in the second case the attachment site is the verb went.", "labels": [], "entities": []}, {"text": "While the attachments are ambiguous, the ambiguity is more severe when unseen or infrequent words like Hudson are encountered.", "labels": [], "entities": []}, {"text": "Classical approaches for the task exploit a wide range of lexical, syntactic, and semantic features and make use of knowledge resources like WordNet and VerbNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.9462611675262451}]}, {"text": "* This work was carried out when the author was a PhD student at the Universitat Polit\u00e8cnica de Catalunya In recent years, word embeddings have become a very popular representation for lexical items).", "labels": [], "entities": []}, {"text": "The idea is that the dimensions of a word embedding capture lexical, syntactic, and semantic features of words -in essence, the type of information that is exploited in PP attachment systems.", "labels": [], "entities": []}, {"text": "Recent work in dependency parsing suggests that these embeddings can also be useful to resolve PP attachment ambiguities.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8142358362674713}, {"text": "PP attachment ambiguities", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.8451562921206156}]}, {"text": "We follow this last line of research and investigate the use of word embeddings for PP attachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.9283046424388885}]}, {"text": "Different from previous works, we consider several types of compositions for the vector embeddings corresponding to the words involved in a PP attachment decision.", "labels": [], "entities": [{"text": "PP attachment decision", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.8460811376571655}]}, {"text": "In particular, our model will define parameters over the tensor product of these embeddings.", "labels": [], "entities": []}, {"text": "We control the capacity of the model by imposing low-rank constraints on the corresponding tensor which we formulate as a convex loss minimization.", "labels": [], "entities": []}, {"text": "We conduct experiments on several datasets and settings and show that this relatively simple multi-linear model can give performances comparable (and in some cases, even superior) than more complex neural network models that use the same information.", "labels": [], "entities": []}, {"text": "Our results suggest that for the PP attachment problem, exploring product spaces of dense word representations produces improvements in performance comparable to those obtained by incorporating non-linearities via a neural network.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9563981294631958}]}, {"text": "With these imrpovements, our tensor products outperform state-of-the art dependency parsers on PP attachment decisions.", "labels": [], "entities": [{"text": "PP attachment decisions", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.8500673174858093}]}, {"text": "first proposed a formulation of PP attachment as a binary prediction problem.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9511719644069672}]}, {"text": "The task is as follows: we are given a fourway tuple v, o, p, m where v is a verb, o is a noun object, p is a preposition, and m is a modifier noun; the goal is to decide whether the prepositional phrase p, m attaches to the verb v or to the noun object o.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents experiments using tensor models for PP attachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9239000082015991}]}, {"text": "Our interest is to eval-  We use standard datasets for PP attachment for two settings: binary and multiple attachments.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.8990724384784698}]}, {"text": "In both cases, the evaluation metric is the attachment accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.6293643712997437}]}, {"text": "The details are as follows.", "labels": [], "entities": []}, {"text": "For Arabic, we used pre-trained 100-dimensional word embeddings from the arTenTen corpus that are distributed with the data.", "labels": [], "entities": [{"text": "arTenTen corpus", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.7863084375858307}]}, {"text": "We created a special unknown vector for unseen words by averaging the word vectors of least frequent words (i.e., with frequency less than 5).", "labels": [], "entities": []}, {"text": "Further, we appended a fixed dimension set to 1 to all word vectors.", "labels": [], "entities": []}, {"text": "As explained in Section 3, when doing tensor compositions, this special dimension has the effect of keeping all lower-order conjunctions, including each elementary coefficient of the word embeddings and a bias term.", "labels": [], "entities": []}, {"text": "This section presents a series of experiments using the classic binary setting by.", "labels": [], "entities": []}, {"text": "We start comparing word embeddings of different types (Skipgram and Skip-dep) trained on different source data, for different dimensions.", "labels": [], "entities": []}, {"text": "For this comparison we use the tensor product model of Eq.", "labels": [], "entities": []}, {"text": "2, that resolves the attachment using only a product of word embeddings, and used * regularization.", "labels": [], "entities": []}, {"text": "presents the results on the RRR development set.", "labels": [], "entities": [{"text": "RRR development set", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.795906662940979}]}, {"text": "Looking at results using Skip-gram, we observe two clear trends that are expected: results improve whenever (1) we increase the dimensionality of the embeddings (n); and (2) we increase the size of the corpus used to induce the embeddings (BLLIP is the smallest, NYT is the largest).", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 240, "end_pos": 245, "type": "METRIC", "confidence": 0.991584300994873}, {"text": "NYT", "start_pos": 263, "end_pos": 266, "type": "DATASET", "confidence": 0.7540674209594727}]}, {"text": "When looking at the performance of models using Skip-dep vectors, which are induced using parse trees, then the results are better than when using Skip-gram.", "labels": [], "entities": []}, {"text": "This is a signal that syntactic-based word embeddings favor PP attachment, which after all is a syntactic disambiguation task.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.8496241867542267}]}, {"text": "We note that this was also found by.", "labels": [], "entities": []}, {"text": "The peak performance is for Skip-dep using 100 dimensional vectors trained on BLLIP.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.7521077990531921}]}, {"text": "For this test, we do not see a benefit from training on larger data.", "labels": [], "entities": []}, {"text": "Our model composes word embeddings using tensor products.: Attachment accuracy on the RRR development set for tensor product models using different word embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.884499728679657}, {"text": "RRR development set", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.7169947922229767}]}, {"text": "We vary the type of word embedding (Skip-gram, Skip-dep), the source data used to induce vectors (BLLIP, Wikipedia, NYT) and the dimensionality of the vectors (50, 100, 300).", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.6321579813957214}, {"text": "NYT", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.8754397630691528}]}, {"text": "The last row \"Skip-gram & Skip-dep\" corresponds to the concatenation of two 50-dimensional word embeddings, fora total of 100 dimensions.", "labels": [], "entities": []}, {"text": "We now examine the performance of our models on the setting and data by, which deals with multiple head candidates.", "labels": [], "entities": []}, {"text": "We, we found necessary to use positional information of the head candidate, as described by Eq.", "labels": [], "entities": []}, {"text": "5. Without it the performance was much worse (possibly because in this data, a large number of samples attach to the first or second candidate in the list -about 93% of cases on the English data).", "labels": [], "entities": [{"text": "English data", "start_pos": 182, "end_pos": 194, "type": "DATASET", "confidence": 0.8865215182304382}]}, {"text": "For English, we present results for models trained with nuclearnorm ( * ) and 2 regularization, using 50-dimensional embeddings.", "labels": [], "entities": []}, {"text": "Imposing low-rank on the product tensor yields some gains with respect to 2 , however the improvements are not drastic.", "labels": [], "entities": []}, {"text": "This is probably because embeddings are already compressed representations, and even products of them do not result in overfitting to training.", "labels": [], "entities": []}, {"text": "We obtain a slight gain by using 100-dimensional embeddings, which results in an accuracy of 88.4 for English and 81.1 for Arabic.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9994540810585022}]}, {"text": "In any case, one characteristic of low-rank regularization is the inherent compression of the tensor.", "labels": [], "entities": []}, {"text": "plots accuracy versus rank for the tensor working with 50-dimensional embeddings composed with positional information : with rank 50 the model obtains 88% of accuracy while reducing the number of parameters by a factor of 6.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9995688796043396}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9991984963417053}]}, {"text": "For PP attachment, this has a computational advantage: the prepositional phrase needs to be projected only once (from 2,601 dimensions to k, where k is the rank) for all the head candidates in the sentence.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9697670638561249}]}, {"text": "We compare our method to a series of results by.", "labels": [], "entities": []}, {"text": "Their \"basic\" model We consider 7 head positions, and word vectors are 51 dimensions in practicei (with the dummy dimention).", "labels": [], "entities": []}, {"text": "Thus, the unfolded matrix W has 357 rows and 2,601 columns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Attachment accuracy on the RRR development set for tensor product models using different  word embeddings. We vary the type of word embedding (Skip-gram, Skip-dep), the source data used to  induce vectors (BLLIP, Wikipedia, NYT) and the dimensionality of the vectors (50, 100, 300). The last  row \"Skip-gram & Skip-dep\" corresponds to the concatenation of two 50-dimensional word embeddings,  for a total of 100 dimensions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9846044182777405}, {"text": "RRR development set", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.7513457735379537}, {"text": "BLLIP", "start_pos": 216, "end_pos": 221, "type": "METRIC", "confidence": 0.5951640009880066}, {"text": "NYT", "start_pos": 234, "end_pos": 237, "type": "DATASET", "confidence": 0.7892550826072693}]}, {"text": " Table 3: Accuracy results over the RRR, NYT and WIKI test sets. (*) indicates that the system uses  additional semantic features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992399215698242}, {"text": "RRR", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8143519759178162}, {"text": "NYT", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.8639688491821289}, {"text": "WIKI test sets", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.8846664627393087}]}, {"text": " Table 4: Test accuracy for PP attachment with  multiple head candidates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.99276202917099}, {"text": "PP attachment", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9450914561748505}]}]}