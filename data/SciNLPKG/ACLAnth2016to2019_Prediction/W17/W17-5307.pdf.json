{"title": [{"text": "Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference", "labels": [], "entities": [{"text": "Recurrent Neural Network-Based Sentence Encoder", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7781809449195862}, {"text": "Natural Language Inference", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.6963899930318197}]}], "abstractContent": [{"text": "The RepEval 2017 Shared Task aims to evaluate natural language understanding models for sentence representation, in which a sentence is represented as a fixed-length vector with neural networks and the quality of the representation is tested with a natural language inference task.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7329927533864975}]}, {"text": "This paper describes our system (alpha) that is ranked among the top in the Shared Task, on both the in-domain test set (obtain-ing a 74.9% accuracy) and on the cross-domain test set (also attaining a 74.9% accuracy), demonstrating that the model generalizes well to the cross-domain data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9797409772872925}, {"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9750085473060608}]}, {"text": "Our model is equipped with intra-sentence gated-attention composition which helps achieve a better performance.", "labels": [], "entities": []}, {"text": "In addition to submitting our model to the Shared Task, we have also tested it on the Stan-ford Natural Language Inference (SNLI) dataset.", "labels": [], "entities": [{"text": "Stan-ford Natural Language Inference (SNLI) dataset", "start_pos": 86, "end_pos": 137, "type": "DATASET", "confidence": 0.5183244533836842}]}, {"text": "We obtain an accuracy of 85.5%, which is the best reported result on SNLI when cross-sentence attention is not allowed , the same condition enforced in RepEval 2017.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9993901252746582}, {"text": "SNLI", "start_pos": 69, "end_pos": 73, "type": "TASK", "confidence": 0.5527043342590332}, {"text": "RepEval 2017", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.8716336786746979}]}], "introductionContent": [{"text": "The RepEval 2017 Shared Task aims to evaluate language understanding models for sentence representation with natural language inference (NLI) tasks, where a sentence is represented as a fixedlength vector.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7474136352539062}]}, {"text": "Modeling inference inhuman language is very challenging but is a basic problem in natural language understanding.", "labels": [], "entities": [{"text": "Modeling inference inhuman language", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8964059799909592}, {"text": "natural language understanding", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.662459005912145}]}, {"text": "Specifically, NLI is concerned with determining whether a hypothesis sentence h can be inferred from a premise sentence p.", "labels": [], "entities": []}, {"text": "Most previous top-performing neural network models on NLI use attention models between a premise and its hypothesis, while how much information can be encoded in a fixed-length vector without such cross-sentence attention deserves some further understanding.", "labels": [], "entities": []}, {"text": "In this paper, we describe the model we submitted to the, which achieves the top performance on both the indomain and cross-domain test set.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data RepEval 2017 use Multi-Genre NLI corpus (MultiNLI) (, which focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral).", "labels": [], "entities": []}, {"text": "The corpus hasten genres, such as fiction, letters, telephone speech and soon.", "labels": [], "entities": []}, {"text": "Training set only has five genres of them, therefore there are in-domain and cross-domain development/test sets.", "labels": [], "entities": []}, {"text": "SNLI) corpus can be used as an additional training/development set, which includes content from the single genre of image captions.", "labels": [], "entities": [{"text": "SNLI) corpus", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7666307091712952}]}, {"text": "However, we don't use SNLI as an additional training/development data in our experiments.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.8163040280342102}]}, {"text": "Training We use the in-domain development set to select models for testing.", "labels": [], "entities": []}, {"text": "To help replicate our results, we publish our code at https: //github.com/lukecq1231/enc_nli (the core code is also used or adapted fora summarization) and a question-answering task ().", "labels": [], "entities": []}, {"text": "We use the Adam () for optimization.", "labels": [], "entities": []}, {"text": "Stacked BiLSTM has 3 layers, and all hidden states of BiLSTMs and MLP have 600 dimensions.", "labels": [], "entities": []}, {"text": "The character embedding has 15 dimensions, and CNN filters length is, each of those is 100 dimensions.", "labels": [], "entities": [{"text": "CNN filters length", "start_pos": 47, "end_pos": 65, "type": "METRIC", "confidence": 0.6413620710372925}]}, {"text": "We use pretrained GloVe-840B-300D vectors () as our word-level embeddings and fix these embeddings during the training process.", "labels": [], "entities": []}, {"text": "Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples.", "labels": [], "entities": []}, {"text": "shows the results of different models.", "labels": [], "entities": []}, {"text": "The first group of models are copied from.", "labels": [], "entities": []}, {"text": "The first sentence encoder is based on continuous bag of words (CBOW), the second is based on BiLSTM, and the third model is Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016b) reimplemented by, which represents the state of the art on SNLI dataset.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 248, "end_pos": 260, "type": "DATASET", "confidence": 0.7813607156276703}]}, {"text": "However, ESIM uses attention between sentence pairs, which is not a sentenceencoder based model.", "labels": [], "entities": []}, {"text": "The second group of models are the results of other teams which participate the RepEval 2017 Share Task competition (.", "labels": [], "entities": [{"text": "RepEval 2017 Share Task competition", "start_pos": 80, "end_pos": 115, "type": "TASK", "confidence": 0.6549968957901001}]}], "tableCaptions": [{"text": " Table 1: Accuracies of the models on MultiNLI.  Note that  *  indicates that the model participate in  the competition on June 15th, 2017.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.998360812664032}, {"text": "MultiNLI", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9413766264915466}]}, {"text": " Table 2: Accuracies of the models on SNLI.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9980472326278687}, {"text": "SNLI", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.7456808686256409}]}]}