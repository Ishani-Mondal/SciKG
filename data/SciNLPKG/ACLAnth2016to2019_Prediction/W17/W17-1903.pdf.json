{"title": [{"text": "Improving Verb Metaphor Detection by Propagating Abstractness to Words, Phrases and Individual Senses", "labels": [], "entities": [{"text": "Improving Verb Metaphor Detection", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8956853747367859}]}], "abstractContent": [{"text": "words refer to things that cannot be seen, heard, felt, smelled, or tasted as opposed to concrete words.", "labels": [], "entities": []}, {"text": "Among other applications, the degree of abstract-ness has been shown to be a useful information for metaphor detection.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.9758898019790649}]}, {"text": "Our contribution to this topic are as follows: i) we compare supervised techniques to learn and extend abstractness ratings for huge vocabularies ii) we learn and investigate norms for multi-word units by propagating abstractness to verb-noun pairs which lead to better metaphor detection, iii) we overcome the limitation of learning a single rating per word and show that multi-sense abstractness ratings are potentially useful for metaphor detection.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 270, "end_pos": 288, "type": "TASK", "confidence": 0.8811835944652557}, {"text": "metaphor detection", "start_pos": 433, "end_pos": 451, "type": "TASK", "confidence": 0.8536828458309174}]}, {"text": "Finally, with this paper we publish automatically created abstractness norms for 3 million English words and multi-words as well as automatically created sense-specific ab-stractness ratings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The standard approach to studying abstractness is to place words on a scale ranging between abstractness and concreteness.", "labels": [], "entities": []}, {"text": "Alternately, abstractness can also be given a taxonomic definition in which the abstractness of a word is determined by the number of subordinate words (.", "labels": [], "entities": []}, {"text": "In psycholinguistics abstractness is commonly used for concept classification ().", "labels": [], "entities": [{"text": "concept classification", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.724559485912323}]}, {"text": "In computational work, abstractness has become an established information for the task of automatic detection of metaphorical language.", "labels": [], "entities": [{"text": "automatic detection of metaphorical language", "start_pos": 90, "end_pos": 134, "type": "TASK", "confidence": 0.8227928638458252}]}, {"text": "So far metaphor detection has been carried out using a variety of features including selectional preferences, word-level semantic similarity (, topic models (, word embeddings and visual information ( ).", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.9547446370124817}]}, {"text": "The underlying motivation of using abstractness in metaphor detection goes back to, who argue that metaphor is a method for transferring knowledge from a concrete domain to an abstract domain.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8720841705799103}]}, {"text": "Abstractness was already applied successfully for the detection of metaphors across a variety of languages).", "labels": [], "entities": [{"text": "Abstractness", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8626596927642822}, {"text": "detection of metaphors", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.8037822445233663}]}, {"text": "The abstractness information itself is typically taken from a dictionary, created either by manual annotation or by extending manually collected ratings with the help of supervised learning techniques that rely on word representations.", "labels": [], "entities": []}, {"text": "While potentially less reliable, automatically created norm-based abstractness ratings can easily cover huge dictionaries.", "labels": [], "entities": []}, {"text": "Although some methods have been used to learn abstractness, literature lacks a comparison of these learning techniques.", "labels": [], "entities": []}, {"text": "We compare and evaluate different learning techniques.", "labels": [], "entities": []}, {"text": "In addition we show and investigate the usefulness of extending abstractness ratings to phrases as well as individual word senses.", "labels": [], "entities": []}, {"text": "We extrinsically evaluate these techniques on two verb metaphor detection tasks: (i) a type-based setting that makes use of phrase ratings, (ii) a token-based classification for multi-sense abstractness norms.", "labels": [], "entities": [{"text": "verb metaphor detection", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.781939427057902}]}, {"text": "Both settings benefit from our approach.", "labels": [], "entities": []}, {"text": "used the same algorithm fora large collection of German lemmas, and in the same way additional created ratings for multiple norms including valency, arousal and imageability.", "labels": [], "entities": []}, {"text": "A different method that has been used to extend abstractness norms based on low-dimensional word embeddings and a Linear Regression classifier ().", "labels": [], "entities": []}, {"text": "We compare approaches across different publicly available vector representations 1 , to study potential differences across vector dimensionality we compare vectors between 50 and 300 dimensions.", "labels": [], "entities": []}, {"text": "The Glove vectors () have been trained on 6billion tokens of Wikipedia plus Gigaword (V=400K), while the word2vec cbow model () was trained on a Google internal news corpus with 100billion tokens (V=3million).", "labels": [], "entities": []}, {"text": "For training and testing we relied on the ratings from, Dividing the ratings into 20% test (7 990) and 80% training (31 964) for tuning hyper parameters we took 1 000 ratings from the training data.", "labels": [], "entities": []}, {"text": "We kept the ratio between word classes.", "labels": [], "entities": []}, {"text": "Evaluation is done by comparing the new created ratings against the test (gold) ratings using Spearman's rank-order correlation.", "labels": [], "entities": []}, {"text": "We first reimplemented the algorithm from Turney and Littman shows clearly that we can learn abstractness ratings with a very high correlation on the test data using the word representations from Google (W2V300) together with a neural network for regression (\u03c1=.90).", "labels": [], "entities": []}, {"text": "The NN method significantly outperforms all other methods, using Steiger (1980)'s test (p < 0.001).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Spearman's \u03c1 for the test ratings. Com- paring representations and regression methods.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.6176624695460001}]}, {"text": " Table 2: AUC Score single features and com- binations. Classifying literal and metaphorical  phrases based on the Saif M. Mohammad and Tur- ney (2016) dataset.", "labels": [], "entities": [{"text": "Classifying literal and metaphorical  phrases", "start_pos": 56, "end_pos": 101, "type": "TASK", "confidence": 0.8051313757896423}, {"text": "Saif M. Mohammad and Tur- ney (2016) dataset", "start_pos": 115, "end_pos": 159, "type": "DATASET", "confidence": 0.569889718836004}]}, {"text": " Table 3: F-score (Metaphor). Classifying literal  and metaphorical verbs based on the VUA and  TroFi dataset. MS = multi-sense, 1S= single sense.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9867401719093323}, {"text": "VUA", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.9328914284706116}, {"text": "TroFi dataset", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.8298507630825043}]}]}