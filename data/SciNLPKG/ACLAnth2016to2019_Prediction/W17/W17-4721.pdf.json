{"title": [], "abstractContent": [{"text": "This paper describes LIMSI's submissions to the news shared task at WMT'17 for English into Czech and Latvian, as well as related experiments.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.6922765374183655}]}, {"text": "This year's novelties consist in the use of a neural machine translation system with a factored output predicting simultaneously a lemma decorated with morphological information and a fine-grained part-of-speech.", "labels": [], "entities": []}, {"text": "Such a type of system drew our attention to the specific step of reinflection, where lemmas and parts-of-speech are transformed into fully inflected words.", "labels": [], "entities": []}, {"text": "Finally, we ran experiments showing an efficient strategy for parameter initialization, as well as data filtering procedures.", "labels": [], "entities": [{"text": "parameter initialization", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6388887614011765}, {"text": "data filtering", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.7787226438522339}]}], "introductionContent": [{"text": "The contribution of LIMSI laboratory to the WMT 2017 News shared task consisted in the submission of different systems for English-to-Czech, as well as with this year's \"guest\" language pair: English-to-Latvian.", "labels": [], "entities": [{"text": "WMT 2017 News shared task", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.8390397429466248}]}, {"text": "Our main focus was on translation into morphologically rich languages (MRL), a challenging question in current state-of-the-art neural machine translation (NMT) architectures.", "labels": [], "entities": [{"text": "translation into morphologically rich languages (MRL)", "start_pos": 22, "end_pos": 75, "type": "TASK", "confidence": 0.8862197995185852}]}, {"text": "Indeed, the variety of target word forms in these languages requires the use of an open vocabulary.", "labels": [], "entities": []}, {"text": "To tackle this issue, we have experimented with a factored neural machine translation system predicting simultaneously at each timestep a normalized word and a fine-grained part-of-speech (section 3).", "labels": [], "entities": [{"text": "factored neural machine translation", "start_pos": 50, "end_pos": 85, "type": "TASK", "confidence": 0.6578298211097717}]}, {"text": "A normalized word (section 5.2) is a specific representation where we removed part of the morphological content of the word, keeping only the features that are relevant to the source language.", "labels": [], "entities": []}, {"text": "Such a factored architecture required a nontrivial step consisting in reinflecting the MT predictions, i.e. transforming normalized words and parts-of-speech into fully inflected words.", "labels": [], "entities": [{"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.9350773692131042}]}, {"text": "To this end, we have experimented with a character-based language model that is used to select ambiguous word forms returned by a look-up table (section 5.5).", "labels": [], "entities": []}, {"text": "Further experiments show the use of an autoencoder to initialize the NMT system's encoder (section 4.1), which enables a faster convergence of the parameter and therefore a lower training time.", "labels": [], "entities": []}, {"text": "Finally, we report experiments performed with different data filtering procedures (section 4.2) and their impact on translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.9636777639389038}]}], "datasetContent": [{"text": "The systems we have submitted at WMT'17 are more specifically the following: \u2022 English-to-Czech baseline: Ensemble of 5 best models.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.7893404364585876}]}, {"text": "\u2022 English-to-Czech factored: Ensemble of 2 best models with nk-best rescoring using the single best baseline.", "labels": [], "entities": []}, {"text": "\u2022 English-to-Latvian baseline: Ensemble of 3 best models with n-best rescoring using the single best Nematus system.", "labels": [], "entities": []}, {"text": "\u2022 English-to-Latvian factored: Ensemble of 3 best models with nk-best rescoring using the single best Nematus system.", "labels": [], "entities": []}, {"text": "The results are reported for these systems in tables 3 and 4, using BLEU, as well as BEER) and CharacTER (, which have shown a high correlation with human rankings for MRL (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9987668991088867}, {"text": "BEER", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9989210367202759}, {"text": "CharacTER", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.8401035666465759}, {"text": "MRL", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.8928099274635315}]}, {"text": "As mentioned in Section 5.3, k-best hypothesis from factored systems are rescored using a fully inflected word-based system.", "labels": [], "entities": []}, {"text": "For Czech, we set: Scores for different English-to-Latvian reinflection methods.", "labels": [], "entities": []}, {"text": "For Latvian, the k = 100 best hypothesis were taken from the dictionary, in order to mitigate the poor quality of this dictionary by relying more on the rescoring system.", "labels": [], "entities": []}, {"text": "Additionally to the k-best hypothesis from the dictionary, we also took the n-best hypothesis from the factored NMT system (n = 30), which lead to the rescoring of nk-best hypothesis by an inflected word based system.", "labels": [], "entities": []}, {"text": "The improvement given by the nk-best setups show the advantage of using a word based model to select the right word forms instead of relying on simple unigram frequencies.", "labels": [], "entities": []}, {"text": "To address the disadvantages of the reinflection methods presented in section 5.3, we investigated a neural reinflection model.", "labels": [], "entities": []}, {"text": "The general architecture is presented in.", "labels": [], "entities": []}, {"text": "The model first takes as input a n-gram centered on the position to reinflect.", "labels": [], "entities": []}, {"text": "To each position corresponds a lexical unit and T PoS-tags, which are represented by embeddings l i and (t n i ) n=1..T . These are concatenated into a context representation xi and transformed into a hidden representation The second input is a candidate inflected form w inf lected i . We represent it as the sequence of its characters, and use a convolutional layer) to build its vectorial representation e w inf lected i . The product of these two representations goes through a sigmoid activation function.", "labels": [], "entities": []}, {"text": "We train the model in a supervised way, by feeding positive and negative examples of inflected forms, with labels 1 and 0.", "labels": [], "entities": []}, {"text": "At test time, the model is given all possible inflected forms obtained in the dictionary, and we choose the one obtaining the best score.", "labels": [], "entities": []}, {"text": "However, our first results show accuracies under the performances of the unigram model presented in section 5.3, for both Czech and Latvian (see Tables 5 and 6).", "labels": [], "entities": []}, {"text": "In future work, we plan to use such a model with abeam search.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of BLEU scores of different  filtering processes for English-to-Czech with Ne- matus systems. All the systems are evaluated with  the beam search of size 2. The term \"basic\" is  referred to the data without any filtering or align- ment. The term discard EU is adopted to refer to  the training without Czeng EU corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9963253140449524}, {"text": "Czeng EU corpus", "start_pos": 323, "end_pos": 338, "type": "DATASET", "confidence": 0.7794599334398905}]}, {"text": " Table 3: Scores for English-to-Czech systems", "labels": [], "entities": []}, {"text": " Table 4: Scores for English-to-Latvian systems", "labels": [], "entities": []}, {"text": " Table 5: Scores for different English-to-Czech reinflection methods.", "labels": [], "entities": []}, {"text": " Table 6: Scores for different English-to-Latvian reinflection methods.", "labels": [], "entities": []}, {"text": " Table 7: Sentence pair evaluation for English-to-Czech (A-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9015459219614664}]}, {"text": " Table 8: Sentence pair evaluation for English-to-Czech (B-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8976996938387553}]}, {"text": " Table 9: Sentence group evaluation for English-to-Czech with Entropy (C-set).", "labels": [], "entities": [{"text": "Sentence group evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8829156359036764}]}, {"text": " Table 10: Sentence pair evaluation for English-to-Latvian (A-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.9129999279975891}]}, {"text": " Table 11: Sentence pair evaluation for English-to-Latvian (B-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.9103485345840454}]}, {"text": " Table 12: Sentence group evaluation for English-to-Latvian with Entropy (C-set).", "labels": [], "entities": [{"text": "Sentence group evaluation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.8853275577227274}]}]}