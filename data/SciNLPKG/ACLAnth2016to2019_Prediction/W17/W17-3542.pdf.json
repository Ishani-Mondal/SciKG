{"title": [{"text": "Neural Paraphrase Generation using Transfer Learning", "labels": [], "entities": [{"text": "Neural Paraphrase Generation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8288677533467611}]}], "abstractContent": [{"text": "Progress in statistical paraphrase generation has been hindered fora longtime by the lack of large monolingual parallel corpora.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7842772305011749}]}, {"text": "In this paper, we adapt the neural machine translation approach to paraphrase generation and perform transfer learning from the closely related task of entailment generation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.723030149936676}, {"text": "paraphrase generation", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.9362159967422485}, {"text": "transfer learning", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.8382181525230408}, {"text": "entailment generation", "start_pos": 152, "end_pos": 173, "type": "TASK", "confidence": 0.7655676305294037}]}, {"text": "We evaluate the model on the Microsoft Research Paraphrase (MSRP) corpus and show that the model is able to generate sentences that capture part of the original meaning, but fails to pickup on important words or to show large lexical variation .", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase (MSRP) corpus", "start_pos": 29, "end_pos": 72, "type": "DATASET", "confidence": 0.6664588877132961}]}], "introductionContent": [{"text": "Paraphrase generation is the problem of restating a given sentence such that its overall meaning is preserved.", "labels": [], "entities": [{"text": "Paraphrase generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9432868659496307}]}, {"text": "This can be seen as a task useful in and of itself or it can serve in proxy applications such as sentence summarization, sentence simplification, question expansion in question answering or rephrasing utterances generated by a conversational agent.", "labels": [], "entities": [{"text": "sentence summarization", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.6989676356315613}, {"text": "sentence simplification", "start_pos": 121, "end_pos": 144, "type": "TASK", "confidence": 0.7142021656036377}, {"text": "question expansion in question answering", "start_pos": 146, "end_pos": 186, "type": "TASK", "confidence": 0.7521286368370056}]}, {"text": "Paraphrase generation has been previously treated as a monolingual machine translation (MT) problem).", "labels": [], "entities": [{"text": "Paraphrase generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9610911011695862}, {"text": "monolingual machine translation (MT)", "start_pos": 55, "end_pos": 91, "type": "TASK", "confidence": 0.8350147406260172}]}, {"text": "Lately, Neural Machine Translation (NMT) has revived interest in statistical machine translation through the use of sequence-to-sequence (SEQ2SEQ) models that learn to maximize the probability of a sentence in a target language, given a sentence in a source language ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.8047406574090322}, {"text": "statistical machine translation", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.636037806669871}]}, {"text": "The SEQ2SEQ model is composed of an encoder that recurrently consumes the words in the source sentence and a decoder that sequentially predicts words in the target sentence, conditioned on the encoder's last hidden state and the previously translated words.", "labels": [], "entities": []}, {"text": "This model was later improved by using an attention mechanism () that allowed the decoder to focus on the relevant words from the source sentence.", "labels": [], "entities": []}, {"text": "NMT can then be used for paraphrase generation by maximizing the probability P (Y |Y ), where (Y, Y ) is a pair of paraphrases.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.9823994338512421}]}, {"text": "While parallel corpora are abundantly available for machine translation, paraphrase corpora featuring pairs of complex sentences are prohibitively small for training large models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7247257381677628}]}, {"text": "We propose to overcome this aspect by performing transfer learning from a similar task -entailment generation, which is facilitated by the large number of entailment pairs featured in the Stanford Natural Language Inference ( corpus.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference ( corpus", "start_pos": 188, "end_pos": 232, "type": "DATASET", "confidence": 0.7571280747652054}]}], "datasetContent": [{"text": "Paraphrases can be seen as mostly bidirectional textual entailments.", "labels": [], "entities": []}, {"text": "Sentential paraphrase corpora are prohibitively small for training large neural networks, but textual entailment corpora are quite large thanks to the SNLI dataset.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 151, "end_pos": 163, "type": "DATASET", "confidence": 0.8696884512901306}]}, {"text": "Our aim is to exploit this situation by performing transfer learning from the entailment generation (EG) task (given sentence S, generate sentence T that can be inferred from S) to the paraphrase generation (PG) task.", "labels": [], "entities": [{"text": "paraphrase generation (PG) task", "start_pos": 185, "end_pos": 216, "type": "TASK", "confidence": 0.8300846119721731}]}, {"text": "We also fine-tune the weights on the larger PPDB corpus before transferring to the Microsoft Research Paraphrase Corpus (, used for the paraphrase detection task.", "labels": [], "entities": [{"text": "PPDB corpus", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9003019034862518}, {"text": "Microsoft Research Paraphrase Corpus", "start_pos": 83, "end_pos": 119, "type": "DATASET", "confidence": 0.9156670719385147}, {"text": "paraphrase detection task", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.900848905245463}]}, {"text": "In addition, we also test the multiple transfer in reverse order.", "labels": [], "entities": []}, {"text": "We use the MSRP and PPDB datasets featuring paraphrase pairs and the SNLI dataset featuring tex-  tual entailments.", "labels": [], "entities": [{"text": "MSRP", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.88436359167099}, {"text": "PPDB datasets", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.8046442568302155}, {"text": "SNLI dataset", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9600995182991028}]}, {"text": "We discard the negative examples from the MSRP dataset.", "labels": [], "entities": [{"text": "MSRP dataset", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9061552882194519}]}, {"text": "We discard the neutral and contradiction examples and only keep entailment pairs from the SNLI corpus.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.7789843678474426}]}, {"text": "We also use the small (XS) phrasal subset of the PPDB dataset, due to its higher-scoring pairs as compared to the other variants of PPDB.", "labels": [], "entities": [{"text": "PPDB dataset", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.8955245018005371}]}, {"text": "We also augmented all datasets with the inverse pair (Y, X) for each pair of sentences (X, Y ) -this approach is completely justified for paraphrases, but it also makes sense for SNLI if we treat an entailment pair just as a paraphrase pair.", "labels": [], "entities": []}, {"text": "The MSRP dataset is small, but it features long sentences with lots of numbers and proper nouns, which is rather problematic when predicting words from fixed-size vocabularies.", "labels": [], "entities": [{"text": "MSRP dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8940359354019165}]}, {"text": "The PPDB dataset contains a large number of short, but high-quality paraphrase pairs.", "labels": [], "entities": [{"text": "PPDB dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9294540882110596}]}, {"text": "We hypothesize that the SNLI entailments could prove useful in paraphrase generation, due to the large lexical overlap between the premise and the hypothesis.", "labels": [], "entities": [{"text": "SNLI entailments", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8141332268714905}, {"text": "paraphrase generation", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.9655326306819916}]}, {"text": "An overview of the datasets and their train/validation/test sizes is shown in.", "labels": [], "entities": []}, {"text": "To generate paraphrases, we use beam-search with abeam size of 5.", "labels": [], "entities": []}, {"text": "We report the BLEU score) and the perplexity of the reconstructed sentences for the MSRP test corpus.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9785021543502808}, {"text": "MSRP test corpus", "start_pos": 84, "end_pos": 100, "type": "DATASET", "confidence": 0.9159431656201681}]}, {"text": "Although no standard metric has proved conclusive for evaluating paraphrase generation, BLEU score has been shown to correlate fairly well with human judgements (Chen and Dolan, 2011), especially when more references are being used.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.9414530992507935}, {"text": "BLEU score", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9840487837791443}]}, {"text": "We also plot the perplexity on the training and validations sets of different transfer scenarios.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets statistics (number of pairs)", "labels": [], "entities": []}, {"text": " Table 2: BLEU score and perplexity on the MSRP test set over", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9703284800052643}, {"text": "MSRP test set", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.8839394450187683}]}]}