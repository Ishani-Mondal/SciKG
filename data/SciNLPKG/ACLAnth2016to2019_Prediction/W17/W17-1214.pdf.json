{"title": [{"text": "Improving the Character Ngram Model for the DSL Task with BM25 Weighting and Less Frequently Used Feature Sets", "labels": [], "entities": [{"text": "BM25 Weighting", "start_pos": 58, "end_pos": 72, "type": "METRIC", "confidence": 0.7853592336177826}]}], "abstractContent": [{"text": "This paper describes the system developed by the Centre for English Corpus Linguistics (CECL) to discriminating similar languages , language varieties and dialects.", "labels": [], "entities": []}, {"text": "Based on a SVM with character and POStag n-grams as features and the BM25 weighting scheme, it achieved 92.7% accuracy in the Discriminating between Similar Languages (DSL) task, ranking first among eleven systems but with a lead over the next three teams of only 0.2%.", "labels": [], "entities": [{"text": "BM25", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.733076810836792}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9937140345573425}, {"text": "Discriminating between Similar Languages (DSL) task", "start_pos": 126, "end_pos": 177, "type": "TASK", "confidence": 0.7050159759819508}]}, {"text": "A simpler version of the system ranked second in the German Dialect Identification (GDI) task thanks to several ad hoc postprocess-ing steps.", "labels": [], "entities": [{"text": "German Dialect Identification (GDI) task", "start_pos": 53, "end_pos": 93, "type": "TASK", "confidence": 0.7772379389830998}]}, {"text": "Complementary analyses carried out by a cross-validation procedure suggest that the BM25 weighting scheme could be competitive in this type of tasks, at least in comparison with the sublinear TF-IDF.", "labels": [], "entities": [{"text": "BM25", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.7260919213294983}]}, {"text": "POStag n-grams also improved the system performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the participation of the Centre for English Corpus Linguistics (CECL) in the fourth edition of the VarDial Evaluation Campaign, which deals with the automatic identification of similar languages (such as excerpts of journalistic texts in Malay and Indonesian), language varieties (such as excerpts of Canadian and Hexagonal French) and dialects (such as Swiss German dialects) ().", "labels": [], "entities": []}, {"text": "The VarDial tasks share many similarities with the Native Language Identification (NLI)) so that several teams) relied on their participation in the NLI task to develop a system for VarDial.", "labels": [], "entities": []}, {"text": "As we achieved an excellent level of performance in the NLI task (, we decided to reuse the approach developed on that occasion, which was based on n-grams of characters, words and part of speech (POS) tags, and on global statistical indices such as the number of tokens per documents or the word mean length.", "labels": [], "entities": []}, {"text": "In the NLI task, n-grams of characters had proved to be as effective as the combination of ngrams of words and POStags.", "labels": [], "entities": []}, {"text": "The character ngrams also obtained the best results in the 2016 Discriminating between Similar Languages (DSL) shared task () as well as in previous editions.", "labels": [], "entities": [{"text": "Discriminating between Similar Languages (DSL) shared task", "start_pos": 64, "end_pos": 122, "type": "TASK", "confidence": 0.46997453769048053}]}, {"text": "These performances led us to privilege this approach especially since we did not have an off-the-shelf POS-tagger for some of the languages to be discriminated.", "labels": [], "entities": []}, {"text": "We nevertheless used POStag n-grams in addition to character ngrams for the three languages for which aversion of TreeTagger is available.", "labels": [], "entities": []}, {"text": "The CECL system was specifically developed for the DSL task in which it obtained the best performance (0.927) according to the weighted F1 measure, but it should be noted that its lead on the system ranked second is only 0.002.", "labels": [], "entities": [{"text": "F1", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9873771071434021}]}, {"text": "A simplified version, due to the different nature of the material to be processed, was applied to a second task, the German Dialect Identification (GDI) task which was organized for the first time.", "labels": [], "entities": [{"text": "German Dialect Identification (GDI) task", "start_pos": 117, "end_pos": 157, "type": "TASK", "confidence": 0.7530587358134133}]}, {"text": "The task aim was to distinguish manually annotated speech transcripts from four Swiss German dialect areas: Basel (BS), Bern (BE), Lucerne (LU) and Zurich (ZH).", "labels": [], "entities": []}, {"text": "This task is particularly difficult because many transcripts are very short and because it is not unusual to find in the learning material identical transcripts (e.g., aber) belonging to the four categories.", "labels": [], "entities": []}, {"text": "In this task, the CECL system came second, obtaining a weighted F1 of 0.661, 0.001 less than the system ranked first.", "labels": [], "entities": [{"text": "CECL", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.9358769655227661}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9884337782859802}]}, {"text": "The next section presents the main characteris-tics of the system within the context of previous research.", "labels": [], "entities": []}, {"text": "The third section describes the material of each task in which we participated and the technical characteristics of the system.", "labels": [], "entities": []}, {"text": "The fourth section reports the results obtained on the test set, but also an evaluation of the benefits/losses brought by the various components of the system by means of a cross-validation procedure.", "labels": [], "entities": []}, {"text": "In the conclusion, we discuss the main limits of this work and consider a few avenues for improvement.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Set of features and C value for the six language groups.", "labels": [], "entities": []}, {"text": " Table 2: Confusion matrix for the DSL task.", "labels": [], "entities": [{"text": "DSL task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.49061721563339233}]}, {"text": " Table 3: Accuracy for the two weighting schemes  (DSL).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989892840385437}]}, {"text": " Table 4: Benefits in accuracy for the three complementary sets of features (DSL).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.999534010887146}]}, {"text": " Table 5: Percentage breakdown of the documents  into the four categories (GDI).", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9326815605163574}]}, {"text": " Table 6: Categorisation of the documents accord- ing to the probability estimate ranking (GDI).", "labels": [], "entities": [{"text": "probability estimate ranking (GDI)", "start_pos": 61, "end_pos": 95, "type": "METRIC", "confidence": 0.8548287749290466}]}, {"text": " Table 7: Accuracy for the two weighting schemes  (GDI).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989621639251709}]}]}