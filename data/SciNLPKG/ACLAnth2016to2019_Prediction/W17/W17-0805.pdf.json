{"title": [], "abstractContent": [{"text": "In this piece of industrial application, we focus on the identification of omission in statement pairs for an online news platform.", "labels": [], "entities": [{"text": "identification of omission in statement pairs", "start_pos": 57, "end_pos": 102, "type": "TASK", "confidence": 0.8591960271199545}]}, {"text": "We compare three annotation schemes, namely two crowdsourcing schemes and an expert annotation.", "labels": [], "entities": []}, {"text": "The simplest of the two crowdsourcing approaches yields a better annotation quality than the more complex one.", "labels": [], "entities": []}, {"text": "We use a dedicated classifier to assess whether the annotators' behaviour can be explained by straightforward linguistic features.", "labels": [], "entities": []}, {"text": "However , for our task, we argue that expert and not crowdsourcing-based annotation is the best compromise between cost and quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina user survey, the news aggregator Storyzy 1 found out that the two main obstacles for user satisfaction when accessing their site's content were redundancy of news items, and missing information respectively.", "labels": [], "entities": []}, {"text": "Indeed, in the journalistic genre that is characteristic of online news, editors make frequent use of citations as prominent information; yet these citations are not always in full.", "labels": [], "entities": []}, {"text": "The reasons for leaving information out are often motivated by the political leaning of the news platform.", "labels": [], "entities": []}, {"text": "Existing approaches to the detection of political bias rely on bag-of-words models) that examine the words present in the writings.", "labels": [], "entities": [{"text": "detection of political bias", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.8348216563463211}]}, {"text": "Our goal is to go beyond such approaches, which focus on what is said, by instead focusing on what is omitted.", "labels": [], "entities": []}, {"text": "Thus, this method requires a pair of statements; an original one, and a shortened version with some deleted words or spans.", "labels": [], "entities": []}, {"text": "The task is then to determine whether the 1 http://storyzy.com information left out in the second statement conveys substantial additional information.", "labels": [], "entities": []}, {"text": "If so, the pair presents an omission; cf..", "labels": [], "entities": []}, {"text": "Omission detection in sentence pairs constitutes anew task, which is different from the recognition of textual entailment-cf. ()-because in our case we are certain that the longer text entails the short one.", "labels": [], "entities": [{"text": "Omission detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9274449944496155}, {"text": "recognition of textual entailment-cf.", "start_pos": 88, "end_pos": 125, "type": "TASK", "confidence": 0.7471616268157959}]}, {"text": "What we want to estimate is whether the information not present in the shorter statement is relevant.", "labels": [], "entities": []}, {"text": "To tackle this question, we used a supervised classification framework, for which we require a dataset of manually annotated sentence pairs.", "labels": [], "entities": []}, {"text": "We conducted an annotation task on a sample of the corpus used by the news platform (Section 3).", "labels": [], "entities": []}, {"text": "In this corpus, reference statements extracted from news articles are used as long 'reference' statements, whereas their short 'target' counterparts were selected by string and date matching.", "labels": [], "entities": []}, {"text": "We followed by examining which features help identify cases of omission (Section 4).", "labels": [], "entities": []}, {"text": "In addition to straightforward measures of word overlap (the Dice coefficient), we also determined that there is a good deal of lexical information that determines whether there is an omission.", "labels": [], "entities": [{"text": "Dice coefficient)", "start_pos": 61, "end_pos": 78, "type": "METRIC", "confidence": 0.7789705197016398}]}, {"text": "This work is, to the best of our knowledge, the first empirical study on omission identification in statement pairs.", "labels": [], "entities": [{"text": "omission identification in statement pairs", "start_pos": 73, "end_pos": 115, "type": "TASK", "confidence": 0.825913381576538}]}], "datasetContent": [{"text": "Once the manually annotated corpus is built, we can assess the learnability of the Omission-Same decision problem, which constitutes a binary classification task.", "labels": [], "entities": []}, {"text": "We aimed at measuring whether the annotators' behavior can be explained by simple proxy linguistic properties like word overlap or length of the statements and/or lexical properties.", "labels": [], "entities": []}, {"text": "Features: For a reference statement r, a target statement t and a set M of the words that only appear in r, we generate the following feature sets: 1. Dice (F a ): Dice coefficient between rand t. shows the classification results.", "labels": [], "entities": []}, {"text": "We use all exhaustive combinations of these feature sets to train a discriminative classifier, namely a logistic regression classifier, to obtain a best feature combination.", "labels": [], "entities": []}, {"text": "We consider a feature combination to be the best when it outperforms the others in both accuracy and F1 for the Omission label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9997770190238953}, {"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9997367262840271}, {"text": "Omission label", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.8076982200145721}]}, {"text": "We compare all systems against the most frequent label (MFL) baseline.", "labels": [], "entities": [{"text": "frequent label (MFL) baseline", "start_pos": 40, "end_pos": 69, "type": "METRIC", "confidence": 0.812233696381251}]}, {"text": "We evaluate each feature twice, namely using five-cold cross validation (CV-5 OM p ), and in a split scenario where we test on the 100 examples of OE after training with the remaining 400 examples from OM p (Test OE).", "labels": [], "entities": []}, {"text": "The three best systems (i.e. non-significantly different from each other when tested on OM p ) are shown in the lower section of the table.", "labels": [], "entities": []}, {"text": "We test for significance using Student's two-tailed test and p <0.05.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.9883401393890381}]}, {"text": "As expected, the overlap (F a ) and length metrics (F b ) make the most competitive standalone features.", "labels": [], "entities": [{"text": "overlap (F a )", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9039684057235717}, {"text": "length metrics (F b )", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.9598324398199717}]}, {"text": "However, we want to measure how much of the labeling of omission is determined by which words are left out, and not just by how many.", "labels": [], "entities": [{"text": "labeling of omission", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8625322977701823}]}, {"text": "The system trained on BoW outperforms the system on DWR.", "labels": [], "entities": [{"text": "BoW", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.969814121723175}, {"text": "DWR", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.9759823083877563}]}, {"text": "However, BoW features contain a proxy for statement length, i.e. if n words are different between ref and target, then n features will fire, and thus approximate the size of M . A distributional semantic model such as GloVe is however made up of non-sparse, real-valued vec-  tors, and does not contain such a proxy for word density.", "labels": [], "entities": []}, {"text": "If we examine the contribution of using F d as a feature model, we see that, while it falls short of its BoW counterpart, it beats the baseline by a margin of 5-10 points.", "labels": [], "entities": [{"text": "BoW", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.9505261182785034}]}, {"text": "In other words, regardless of the size of M , there is lexical information that explains the choices of considering an omission.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of annotated instances. The 'Instance' column contains the full reference statement,  with the elements not present in the target statement marked in italics. The last three columns display  the proportion of Omission labels provided by the three annotation setups.", "labels": [], "entities": [{"text": "Instance", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9565879702568054}]}, {"text": " Table 2: Dataset, Krippendorff's \u03b1, median anno- tation time, raw proportion of Omision, and label  distribution using voting and MACE.", "labels": [], "entities": [{"text": "median anno- tation time", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.9256462693214417}, {"text": "raw proportion of Omision", "start_pos": 63, "end_pos": 88, "type": "METRIC", "confidence": 0.698906809091568}]}, {"text": " Table 3: Accuracy and F1 for the Omission la- bel for all feature groups, plus for the best feature  combination in both evaluation methods. Systems  significantly under baseline are marked in grey.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996776580810547}, {"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9997692704200745}, {"text": "Omission", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9937254786491394}]}]}