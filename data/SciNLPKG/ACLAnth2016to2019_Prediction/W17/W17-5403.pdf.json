{"title": [], "abstractContent": [{"text": "Grapheme-to-phoneme conversion (g2p) is necessary for text-to-speech and automatic speech recognition systems.", "labels": [], "entities": [{"text": "Grapheme-to-phoneme conversion", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7363057732582092}, {"text": "automatic speech recognition", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6733213067054749}]}, {"text": "Most g2p systems are monolingual: they require language-specific data or handcrafting of rules.", "labels": [], "entities": []}, {"text": "Such systems are difficult to extend to low resource languages, for which data and handcrafted rules are not available.", "labels": [], "entities": []}, {"text": "As an alternative, we present a neu-ral sequence-to-sequence approach to g2p which is trained on spelling-pronunciation pairs in hundreds of languages.", "labels": [], "entities": []}, {"text": "The system shares a single encoder and decoder across all languages, allowing it to utilize the intrinsic similarities between different writing systems.", "labels": [], "entities": []}, {"text": "We show an 11% improvement in phoneme error rate over an approach based on adapting high-resource monolingual g2p models to low-resource languages.", "labels": [], "entities": [{"text": "error rate", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.8964440822601318}]}, {"text": "Our model is also much more compact relative to previous approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Accurate grapheme-to-phoneme conversion (g2p) is important for any application that depends on the sometimes inconsistent relationship between spoken and written language.", "labels": [], "entities": [{"text": "Accurate grapheme-to-phoneme conversion", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8650778929392496}]}, {"text": "Most prominently, this includes text-to-speech and automatic speech recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.6373292903105418}]}, {"text": "Most work on g2p has focused on a few languages for which extensive pronunciation data is available, inter alia).", "labels": [], "entities": []}, {"text": "Most languages lack these resources.", "labels": [], "entities": []}, {"text": "However, a low resource language's writing system is likely to be similar to the writing systems of languages that do have sufficient pronunciation data.", "labels": [], "entities": []}, {"text": "Therefore g2p maybe possible for low resource languages if this high resource data can be properly utilized.", "labels": [], "entities": []}, {"text": "We attempt to leverage high resource data by treating g2p as a multisource neural machine translation (NMT) problem.", "labels": [], "entities": [{"text": "multisource neural machine translation (NMT)", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.7484672878469739}]}, {"text": "The source sequences for our system are words in the standard orthography in any language.", "labels": [], "entities": []}, {"text": "The target sequences are the corresponding representation in the International Phonetic Alphabet (IPA).", "labels": [], "entities": [{"text": "International Phonetic Alphabet (IPA)", "start_pos": 65, "end_pos": 102, "type": "DATASET", "confidence": 0.9105166792869568}]}, {"text": "Our results show that the parameters learned by the shared encoder-decoder are able to exploit the orthographic and phonemic similarities between the various languages in our data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present experiments with two versions of our sequence-to-sequence model.", "labels": [], "entities": []}, {"text": "Although this is an unusually wide beam and had negligible performance effects, it was necessary to compute our error metrics.", "labels": [], "entities": []}, {"text": "We use the following three evaluation metrics: \u2022 Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences.", "labels": [], "entities": [{"text": "Phoneme Error Rate (PER)", "start_pos": 49, "end_pos": 73, "type": "METRIC", "confidence": 0.8146312683820724}, {"text": "Levenshtein distance", "start_pos": 81, "end_pos": 101, "type": "METRIC", "confidence": 0.9382413327693939}]}, {"text": "\u2022 Word Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 2, "end_pos": 23, "type": "METRIC", "confidence": 0.8424870570500692}]}, {"text": "\u2022 Word Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system.", "labels": [], "entities": [{"text": "Word Error Rate 100 (WER 100)", "start_pos": 2, "end_pos": 31, "type": "METRIC", "confidence": 0.869574673473835}]}, {"text": "In system evaluations, WER, WER 100, and PER numbers presented for multiple languages are averaged, weighting each language equally (following.", "labels": [], "entities": [{"text": "PER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9875498414039612}]}, {"text": "It would be interesting to compute error metrics that incorporate phoneme similarity, such as those proposed by.", "labels": [], "entities": []}, {"text": "PER weights all phoneme errors the same, even though some errors are more harmful than others: /d/ and /k/ are usually contrastive, whereas /d/ and /d \"/ almost never are.", "labels": [], "entities": []}, {"text": "Such statistics would be especially interesting for evaluating a multilingual system, because different languages often map the same grapheme to phonemes that are only subtly different from each other.", "labels": [], "entities": []}, {"text": "However, these statistics have not been widely reported for other g2p systems, so we omit them here.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameters for multilingual g2p  models", "labels": [], "entities": []}, {"text": " Table 5: High Resource Results", "labels": [], "entities": []}, {"text": " Table 6: Results on languages not in the training  corpus", "labels": [], "entities": []}]}