{"title": [{"text": "Deep Learning for Biomedical Information Retrieval: Learning Textual Relevance from Click Logs", "labels": [], "entities": [{"text": "Biomedical Information Retrieval", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.6706650654474894}, {"text": "Learning Textual Relevance from Click Logs", "start_pos": 52, "end_pos": 94, "type": "TASK", "confidence": 0.7054211994012197}]}], "abstractContent": [{"text": "We describe a Deep Learning approach to modeling the relevance of a document's text to a query, applied to biomedical literature.", "labels": [], "entities": []}, {"text": "Instead of mapping each document and query to a common semantic space, we compute a variable-length difference vector between the query and document which is then passed through a deep convolution stage followed by a deep regression network to produce the estimated probability of the document's relevance to the query.", "labels": [], "entities": []}, {"text": "Despite the small amount of training data, this approach produces a more robust predictor than computing similarities between semantic vector representations of the query and document, and also results in significant improvements over traditional IR text factors.", "labels": [], "entities": []}, {"text": "In the future , we plan to explore its application in improving PubMed search.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of this research was to explore Deep Learning models for learning textual relevance of documents to simple keyword-style queries, as applied to biomedical literature.", "labels": [], "entities": []}, {"text": "We wanted to address two main research questions: (1) Without using a curated thesaurus of synonyms and related terms, or an industry ontology like Medical Subject Headings (MeSH R ) ( , can a neural network relevance model go beyond measuring the presence of query words in a document, and capture some of the semantics in the rest of the document text?", "labels": [], "entities": [{"text": "Medical Subject Headings (MeSH R", "start_pos": 148, "end_pos": 180, "type": "TASK", "confidence": 0.7057050963242849}]}, {"text": "(2) Can a deep learning model demonstrate robust performance despite training on a relatively small amount of labelled data?", "labels": [], "entities": []}, {"text": "We had access to a month of click logs from PubMed R 1 , a biomedical literature search engine serving about 3 million queries a day, 20 results per page ().", "labels": [], "entities": [{"text": "PubMed R 1", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9102449615796407}]}, {"text": "Most current users of the system are domain experts looking for the most recent papers by an author or search with complex topical boolean query expressions on document aspects.", "labels": [], "entities": []}, {"text": "For a small proportion (\u223c 5%) of the searches in PubMed, the retrieved articles are sorte by relevance, instead of the default sort order by date.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9627895355224609}]}, {"text": "Usage analysis has shown (ibid.) that topic-based queries area significant part of the search traffic.", "labels": [], "entities": []}, {"text": "Such queries often combine two or more entities (e.g. gene and disease), and while users still use short queries, the users are persistent and will frequently reformulate their queries to narrow the search results.", "labels": [], "entities": []}, {"text": "So improving the ranking is important to satisfy the needs of PubMed's expanding user base.", "labels": [], "entities": []}, {"text": "Traditional lexical Information Retrieval (IR) factors measure the prominence of query terms in documents treated as bags of words.", "labels": [], "entities": [{"text": "lexical Information Retrieval (IR)", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.7808758517106374}]}, {"text": "While such factors like Okapi BM25 ( and Query Likelihood () are quite effective, there are several cases where they fail.", "labels": [], "entities": [{"text": "Okapi", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.5730720162391663}, {"text": "BM25", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.736031711101532}]}, {"text": "Two that we wanted to target were: (i) under-specified query problem, where even irrelevant documents have prominent presence of the query terms, and relevance requires analysis of the topics and semantics not directly specified in the query, and (ii) the term mismatch problem (, which requires detection of related alternative terms or phrases in the document when the actual query terms are not in the document.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the performance of the relevance models on the following ranking metrics: NDCG at rank 20, Precision at ranks 5, 10 and 20, and Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "NDCG", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.6968451142311096}, {"text": "Precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9683390855789185}, {"text": "Mean Average Precision (MAP)", "start_pos": 139, "end_pos": 167, "type": "METRIC", "confidence": 0.9717804392178854}]}, {"text": "Scoring ties were resolved by sorting on decreasing document-id.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ranking metrics on the Full Test Data", "labels": [], "entities": [{"text": "Full Test Data", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9600251118342081}]}, {"text": " Table 3: Ranking metrics on selected subsets of the Test Data", "labels": [], "entities": [{"text": "Test Data", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.7682804465293884}]}, {"text": " Table 4: Ranking metrics for Relevance-Weighted models", "labels": [], "entities": []}]}