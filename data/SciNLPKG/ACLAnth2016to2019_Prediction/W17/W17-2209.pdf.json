{"title": [{"text": "Modeling intra-textual variation with entropy and surprisal: topical vs. stylistic patterns", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a data-driven approach to investigate intra-textual variation by combining entropy and surprisal.", "labels": [], "entities": []}, {"text": "With this approach we detect linguistic variation based on phrasal lexico-grammatical patterns across sections of research articles.", "labels": [], "entities": []}, {"text": "Entropy is used to detect patterns typical of specific sections.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9256042838096619}]}, {"text": "Surprisal is used to differentiate between more and less informationally-loaded patterns as well as types of information (topical vs. stylistic).", "labels": [], "entities": [{"text": "Surprisal", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7969861030578613}]}, {"text": "While we here focus on research articles in biology/genetics, the methodology is especially interesting for digital humanities scholars, as it can be applied to any text type or domain and combined with additional variables (e.g. time, author or social group) to obtain insights on intra-textual variation.", "labels": [], "entities": []}], "introductionContent": [{"text": "While there is an abundance of studies on linguistic variation according to domain, register and genre, text-internal variation, i.e. variation based on changing micro-purposes within a text, has received much less attention.", "labels": [], "entities": []}, {"text": "As such internal shifts occur in all kinds of discourse -be it in spoken (such as spontaneous conversation or speeches) or written mode (such as literary texts, written editorials, research articles) -there has been recently a growing interest in this type of variation.", "labels": [], "entities": []}, {"text": "In general, knowledge on intra-textual variation leads to a more comprehensive understanding of the data underlying computational modeling, analysis, interpretation, etc.", "labels": [], "entities": [{"text": "computational modeling, analysis, interpretation", "start_pos": 116, "end_pos": 164, "type": "TASK", "confidence": 0.6497578024864197}]}, {"text": "In the field of NLP, there is a growing need in the development of applications that consider variation also at the textual level to improve performance.", "labels": [], "entities": []}, {"text": "Considering research articles, approaches within BioNLP, for instance, have moved from focusing on abstracts as sources of text mining to using also full-text articles, not least because this data is made available through repositories such as PubMedCentral (PMC).", "labels": [], "entities": [{"text": "text mining", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.720436692237854}, {"text": "PubMedCentral (PMC)", "start_pos": 244, "end_pos": 263, "type": "DATASET", "confidence": 0.8581210225820541}]}, {"text": "To obtain good performance, corpora created from such resources are highly annotated with linguistic as well as semantic categories characterizing e.g. gene names.", "labels": [], "entities": []}, {"text": "From these, specific features are selected with a trade-off between ease of extraction and desired type of information.", "labels": [], "entities": []}, {"text": "In the field of DH, intra-textual variation is considered especially in literary studies, computational stylistics, and authorship attribution.", "labels": [], "entities": [{"text": "DH", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9507207870483398}, {"text": "authorship attribution", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.7127366811037064}]}, {"text": "shows, for example, how knowledge about differences between text parts helps to improve computational stylistic approaches.", "labels": [], "entities": []}, {"text": "In corpus linguistics, the common approach to intra-textual variation is to start with a set of pre-defined linguistic features (.", "labels": [], "entities": []}, {"text": "While the choice of features is clearly linguistically informed, this initial step in analysis is manual and needs to be carried out anew for every new text type or register considered.", "labels": [], "entities": []}, {"text": "Also, analysis is restricted to frequency (i.e. unconditioned probabilities).", "labels": [], "entities": []}, {"text": "We present a methodology for investigating intra-textual variation that is data-driven and based on conditional probabilities which are calculated using two information-theoretic measures, entropy and surprisal.", "labels": [], "entities": []}, {"text": "Being data-driven, our approach can be applied to any text type or domain, avoiding extensive annotations and manual selection of features possibly involved in variation.", "labels": [], "entities": []}, {"text": "Based on probabilities conditioned on ambient and extralinguistic context, it allows to capture variation in a more fine-grained manner than by considering mere frequencies.", "labels": [], "entities": []}, {"text": "As a testbed for our approach, we use scientific research articles in genetics, as they clearly exhibit the typical IMRaD (Introduction, Methods, Results and Discussion) structure of scientific articles, with internal shifts in purpose (see e.g.).", "labels": [], "entities": [{"text": "IMRaD", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.8658065795898438}]}, {"text": "We use relative entropy to detect features typical of specific sections.", "labels": [], "entities": []}, {"text": "By considering surprisal (i.e. probabilities of features in their ambient context), we are able to detect the amount and type of information these typical features convey, e.g. more informationally-loaded expressions (e.g. terminology) vs. less informationally-loaded expressions (e.g. linguistic formula, such as These results show that).", "labels": [], "entities": []}, {"text": "Thus, besides possible topical variation within articles across sections, we are able to detect also variation of stylistic lexico-grammatical patterns.", "labels": [], "entities": []}, {"text": "While our focus is on research articles, the methodology can be applied to any text type or domain to detect (intra-textual) variation in a datadriven way.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Journals with corpus size and number of  texts", "labels": [], "entities": []}, {"text": " Table 4: Classification results with typical POS tri- grams and AvS ranges.", "labels": [], "entities": [{"text": "POS tri- grams", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.7778497338294983}, {"text": "AvS", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9873486757278442}]}, {"text": " Table 5: Classification results by F-Measure for  each section (RandomForest)", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9949157238006592}, {"text": "RandomForest", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.7124388813972473}]}]}