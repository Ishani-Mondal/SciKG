{"title": [{"text": "Deep Architectures for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.7108712196350098}]}], "abstractContent": [{"text": "It has been shown that increasing model depth improves the quality of neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.6634867389996847}]}, {"text": "However, different architectural variants to increase model depth have been proposed, and so far, there has been no thorough comparative study.", "labels": [], "entities": []}, {"text": "In this work, we describe and evaluate several existing approaches to introduce depth in neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.6615249117215475}]}, {"text": "Additionally , we explore novel architectural variants, including deep transition RNNs, and we vary how attention is used in the deep decoder.", "labels": [], "entities": []}, {"text": "We introduce a novel \"BiDeep\" RNN architecture that combines deep transition RNNs and stacked RNNs.", "labels": [], "entities": []}, {"text": "Our evaluation is carried out on the En-glish to German WMT news translation dataset, using a single-GPU machine for both training and inference.", "labels": [], "entities": [{"text": "German WMT news translation dataset", "start_pos": 49, "end_pos": 84, "type": "DATASET", "confidence": 0.8239379167556763}]}, {"text": "We find that several of our proposed architectures improve upon existing approaches in terms of speed and translation quality.", "labels": [], "entities": [{"text": "speed", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9848883152008057}]}, {"text": "We obtain best improvements with a BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU over a strong shallow baseline.", "labels": [], "entities": [{"text": "BiDeep", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9103719592094421}, {"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9979081153869629}]}, {"text": "We release our code for ease of adoption.", "labels": [], "entities": [{"text": "adoption", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.9600788354873657}]}], "introductionContent": [{"text": "Neural machine translation (NMT) is a wellestablished approach that yields the best results on most language pairs ().", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8086796998977661}]}, {"text": "Most systems are based on the sequence-to-sequence model with attention) which employs single-layer recurrent neural networks both in the encoder and in the decoder.", "labels": [], "entities": []}, {"text": "Unlike feed-forward networks where depth is straightforwardly defined as the number of noninput layers, recurrent neural network architectures with multiple layers allow different connection schemes () that give rise to different, orthogonal, definitions of depth (  which can affect the model performance depending on a given task.", "labels": [], "entities": []}, {"text": "This is further complicated in sequence-to-sequence models as they contain multiple sub-networks, recurrent or feed-forward, each of which can be deep in different ways, giving rise to a large number of possible configurations.", "labels": [], "entities": []}, {"text": "In this work we focus on stacked and deep transition recurrent architectures as defined by.", "labels": [], "entities": []}, {"text": "Different types of stacked architectures have been successfully used for NMT (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.8793860673904419}]}, {"text": "However, there is alack of empirical comparisons of different deep architectures.", "labels": [], "entities": []}, {"text": "Deep transition architectures have been successfully used for language modeling, but not for NMT so far.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7588302195072174}]}, {"text": "We evaluate these architectures, both alone and in combination, varying the connection scheme between the different components and their depth over the different dimensions, measuring the performance of the different configurations on the WMT news translation task.", "labels": [], "entities": [{"text": "WMT news translation task", "start_pos": 239, "end_pos": 264, "type": "TASK", "confidence": 0.9091089069843292}]}, {"text": "Related work includes that of, who have performed an exploration of NMT architectures in parallel to our work.", "labels": [], "entities": []}, {"text": "Their experiments, which are largely orthogonal to ours, focus on embedding size, RNN cell type (GRU vs. LSTM), network depth (defined according to the architecture of ), attention mechanism and beam size.", "labels": [], "entities": []}, {"text": "recently proposed a NMT architecture based on convolutions over fixed-sized windows rather than RNNs, and they reported results for different model depths and attention mechanism configurations.", "labels": [], "entities": []}, {"text": "A similar feedforward architecture which uses multiple pervasive attention mechanisms rather than convolutions was proposed by, who also report results for different model depths.", "labels": [], "entities": []}], "datasetContent": [{"text": "All experiments were performed with Nematus (Sennrich et al., 2017b), following in their choice of preprocessing and hyperparameters.", "labels": [], "entities": []}, {"text": "For experiments with deep models, we increase the depth by a factor of 4 compared to the baseline for most experiments; in preliminary experiments, we observed diminishing returns for deeper models.", "labels": [], "entities": []}, {"text": "We trained on the parallel English-German training data of WMT-2017 news translation task, using newstest2013 as validation set.", "labels": [], "entities": [{"text": "WMT-2017 news translation task", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.8159098476171494}]}, {"text": "We used early-stopping on the validation cross-entropy and selected the best model based on validation BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9777846336364746}]}, {"text": "We report cross-entropy (CE) on newstest2013, training speed (on a single Titan X (Pascal) GPU), and the number of parameters.", "labels": [], "entities": [{"text": "cross-entropy (CE)", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7851397395133972}, {"text": "newstest2013", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9519534111022949}]}, {"text": "For translation quality, we report case-sensitive, detokenized BLEU, measured with mteval-v13a.pl, on newstest2014, newstest2015, and newstest2016.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9743607640266418}, {"text": "newstest2014", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.9535681009292603}, {"text": "newstest2015", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.8987323045730591}, {"text": "newstest2016", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.9609892964363098}]}, {"text": "We release the code under an open source license, including it in the official Nematus repository.", "labels": [], "entities": [{"text": "Nematus repository", "start_pos": 79, "end_pos": 97, "type": "DATASET", "confidence": 0.919287770986557}]}, {"text": "The configuration files needed to replicate our experiments are available in a separate repository.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Layer normalization results. English\u2192German WMT17 data.", "labels": [], "entities": [{"text": "Layer normalization", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6286483108997345}, {"text": "WMT17 data", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.8047254085540771}]}, {"text": " Table 2: Deep encoder results. English\u2192German WMT17 data. Parameters and speed are highlighted  for the deep recurrent models.", "labels": [], "entities": [{"text": "WMT17 data", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.8250608146190643}, {"text": "Parameters", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9939942955970764}]}, {"text": " Table 3: Deep decoder results. English\u2192German WMT17 data. Parameters and speed are highlighted  for the deep recurrent models.", "labels": [], "entities": [{"text": "WMT17 data", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.8133855760097504}, {"text": "Parameters", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.993775486946106}]}, {"text": " Table 4: Deep encoder-decoder results. English\u2192German WMT17 data. Transition depth 4/2 means 4  in the base RNN of the stack and 2 in the higher RNNs. The last two models are large and their results  are highlighted separately.", "labels": [], "entities": [{"text": "German WMT17 data", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.6635524034500122}, {"text": "Transition depth 4/2", "start_pos": 67, "end_pos": 87, "type": "METRIC", "confidence": 0.9274737477302551}]}]}