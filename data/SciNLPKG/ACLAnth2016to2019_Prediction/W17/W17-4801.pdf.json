{"title": [{"text": "Findings of the 2017 DiscoMT Shared Task on Cross-lingual Pronoun Prediction", "labels": [], "entities": [{"text": "DiscoMT Shared Task", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7231525182723999}, {"text": "Cross-lingual Pronoun Prediction", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6600061555703481}]}], "abstractContent": [{"text": "We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction.", "labels": [], "entities": [{"text": "DiscoMT 2017 shared task", "start_pos": 69, "end_pos": 93, "type": "DATASET", "confidence": 0.7654385417699814}, {"text": "cross-lingual pronoun prediction", "start_pos": 97, "end_pos": 129, "type": "TASK", "confidence": 0.6720501979192098}]}, {"text": "The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence.", "labels": [], "entities": []}, {"text": "We further provided a lem-matized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the target-language lemmata.", "labels": [], "entities": []}, {"text": "The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the entire document.", "labels": [], "entities": []}, {"text": "We offered four subtasks, each fora different language pair and translation direction: English-to-French, English-to-German, German-to-English, and Spanish-to-English.", "labels": [], "entities": []}, {"text": "Five teams participated in the shared task, making submissions for all language pairs.", "labels": [], "entities": []}, {"text": "The evaluation results show that all participating teams outperformed two strong n-gram-based language model-based baseline systems by a sizable margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pronoun translation poses a problem for machine translation (MT) as pronoun systems do not map well across languages, e.g., due to differences in gender, number, case, formality, or humanness, as well as because of language-specific restrictions about where pronouns maybe used.", "labels": [], "entities": [{"text": "Pronoun translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9133115708827972}, {"text": "machine translation (MT)", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.8488254129886628}]}, {"text": "For example, when translating the English it into French an MT system needs to choose between il, elle, and cela, while translating the same pronoun into German would require a choice between er, sie, and es.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.912551999092102}]}, {"text": "This is hard as selecting the correct pronoun may need discourse analysis as well as linguistic and world knowledge.", "labels": [], "entities": []}, {"text": "Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb's morphology, rendering a subject pronoun or noun phrase redundant.", "labels": [], "entities": []}, {"text": "Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source.", "labels": [], "entities": []}, {"text": "Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and.", "labels": [], "entities": [{"text": "Pronoun translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8814806342124939}, {"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9881540536880493}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 88, "end_pos": 125, "type": "TASK", "confidence": 0.8198936978975931}]}, {"text": "Phrase-based SMT () was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, ().", "labels": [], "entities": [{"text": "Phrase-based SMT", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6790027320384979}, {"text": "Neural Machine Translation", "start_pos": 95, "end_pos": 121, "type": "TASK", "confidence": 0.6937204400698344}]}], "datasetContent": [{"text": "While in 2015 we used macro-averaged F 1 as an official evaluation measure, this year we followed the setup of 2016, where we switched to macroaveraged recall, which was also recently adopted by some other competitions, e.g., by.", "labels": [], "entities": [{"text": "F 1", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9208299815654755}, {"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9659609794616699}]}, {"text": "Moreover, as in 2015 and 2016, we also report accuracy as a secondary evaluation measure (but we abandon F 1 altogether).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9995973706245422}, {"text": "F 1", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9947153925895691}]}, {"text": "Macro-averaged recall ranges in, where a value of 1 is achieved by the perfect classifier, 8 and a value of 0 is achieved by the classifier that misclassifies all examples.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.994809627532959}]}, {"text": "The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier.", "labels": [], "entities": []}, {"text": "The advantage of macro-averaged recall over accuracy is that it is more robust to class imbalance.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9800966382026672}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9993485808372498}]}, {"text": "For instance, the accuracy of the majorityclass classifier maybe much higher than 1/C if the test dataset is imbalanced.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995145797729492}]}, {"text": "Thus, one cannot interpret the absolute value of accuracy (e.g., is 0.7 a good or a bad value?) without comparing it to a baseline that must be computed for each specific test dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8437493443489075}]}, {"text": "In contrast, for macro-averaged recall, it is clear that a value of, e.g., 0.7, is well above both the majority-class and the random baselines, which are both always 1/C (e.g., 0.5 with two classes, 0.33 with three classes, etc.).", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9069371223449707}]}, {"text": "Similarly to accuracy, standard F 1 and macro-averaged F 1 are both sensitive to class imbalance for the same reason; see Sebastiani (2015) for more detail and further discussion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9993457198143005}]}], "tableCaptions": [{"text": " Table 2: Statistics about the 2017 test datasets.", "labels": [], "entities": [{"text": "2017 test datasets", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.6957833568255106}]}, {"text": " Table 3: TED talks for testing: English\u2192French and English\u2192German.", "labels": [], "entities": []}, {"text": " Table 4: TED talks for testing: German\u2192English and Spanish\u2192English.", "labels": [], "entities": []}, {"text": " Table 5: Results for German\u2192English.", "labels": [], "entities": []}, {"text": " Table 6: Results for English\u2192German.", "labels": [], "entities": []}, {"text": " Table 7: Results for Spanish\u2192English.", "labels": [], "entities": []}, {"text": " Table 8: Results for English\u2192French.", "labels": [], "entities": []}, {"text": " Table 9: Recall for each class and system for German\u2192English.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9929144382476807}]}, {"text": " Table 10: Recall for each class and system for English\u2192German. In the test dataset, there were no  instances of the pronoun class man, and thus this class is not included in the table.", "labels": [], "entities": []}, {"text": " Table 11: Recall for each class and system for Spanish\u2192English.", "labels": [], "entities": [{"text": "Recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9908702969551086}]}, {"text": " Table 12: Recall for each class and system for English\u2192French.", "labels": [], "entities": [{"text": "Recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9928185939788818}]}]}