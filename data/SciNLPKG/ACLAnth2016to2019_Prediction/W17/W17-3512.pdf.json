{"title": [{"text": "Proceedings of The 10th International Natural Language Generation conference", "labels": [], "entities": [{"text": "International Natural Language Generation", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.6200338304042816}]}], "abstractContent": [{"text": "There has been continuous growth in the volume and ubiquity of video material.", "labels": [], "entities": []}, {"text": "It has become essential to define video semantics in order to aid the searchability and retrieval of this data.", "labels": [], "entities": []}, {"text": "We present a framework that produces textual descriptions of video, based on the visual semantic content.", "labels": [], "entities": []}, {"text": "Detected action classes rendered as verbs, participant objects converted to noun phrases, visual properties of detected objects rendered as adjectives and spatial relations between objects rendered as prepositions.", "labels": [], "entities": []}, {"text": "Further, in cases of zero-shot action recognition, a language model is used to infer a missing verb, aided by the detection of objects and scene settings.", "labels": [], "entities": [{"text": "zero-shot action recognition", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6377212007840475}]}, {"text": "These extracted features are converted into textual descriptions using a template-based approach.", "labels": [], "entities": []}, {"text": "The proposed video descriptions framework evaluated on the NLDHA dataset using ROUGE scores and human judgment evaluation.", "labels": [], "entities": [{"text": "NLDHA dataset", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9873528778553009}, {"text": "ROUGE", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9446109533309937}]}], "introductionContent": [{"text": "The field of computer vision has advanced to detect humans, identify their activities, or to discriminate between a large number of object classes and assign them attributes.", "labels": [], "entities": [{"text": "computer vision", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7186831682920456}]}, {"text": "The outcome is usually a compact semantic representation that encodes activities associated with object categories.", "labels": [], "entities": []}, {"text": "Such representations could be easily processed and interpreted by automatic systems.", "labels": [], "entities": []}, {"text": "However, the natural way to convey this kind of information to humans is through natural language.", "labels": [], "entities": []}, {"text": "Thus, this paper addresses the issue of producing textual descriptions for human activities in videos.", "labels": [], "entities": []}, {"text": "This task has a range of applications, such as human-computer/robot interaction, video summarising, indexing and retrieval.", "labels": [], "entities": [{"text": "video summarising", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.6454961895942688}, {"text": "indexing", "start_pos": 100, "end_pos": 108, "type": "TASK", "confidence": 0.962011992931366}]}, {"text": "Furthermore, translation between visual video content and language provides a solid foundation for understanding relations between vision and linguistics, as they are the closest modalities to interact with humans.", "labels": [], "entities": [{"text": "translation between visual video content", "start_pos": 13, "end_pos": 53, "type": "TASK", "confidence": 0.7945744395256042}]}, {"text": "Generating textual descriptions of visual content is an intriguing task that requires a combination of two major research aspects: visual recognition approaches and natural language generation (NLG) techniques.", "labels": [], "entities": [{"text": "Generating textual descriptions of visual content", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8973920146624247}, {"text": "visual recognition", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.720139890909195}, {"text": "natural language generation (NLG)", "start_pos": 165, "end_pos": 198, "type": "TASK", "confidence": 0.8188451727231344}]}, {"text": "To generate descriptions for videos and images, a template-based approach is a powerful tool though one which needs to be manually identified).", "labels": [], "entities": []}, {"text": "An alternative approach is to retrieve descriptive sentences from a training corpus based on visual similarity, or to utilise externally textual-based corpora to help rank the visual detections).", "labels": [], "entities": []}, {"text": "The most relevant researches to us are the ( and ().", "labels": [], "entities": []}, {"text": "Both of these approaches identify high-level features (HLFs) such as humans, chairs, and so forth, and generate textual descriptions using a template-based approach.", "labels": [], "entities": []}, {"text": "( propose a method that relies on treating a video as a sequence of frames, and performs image detection for each frame independently, to identify HLFs without exploiting the temporal domain.", "labels": [], "entities": [{"text": "image detection", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.683337464928627}]}, {"text": "Alternatively, () have used a dataset with simple video settings where only one action is performed.", "labels": [], "entities": []}, {"text": "Consequently, their natural language descriptions consist of one sentence.", "labels": [], "entities": []}, {"text": "In contrast, this study focuses on generating de-scriptions of human activities in videos sequences at a shot-based level, relying mainly on visual detections.", "labels": [], "entities": []}, {"text": "Specifically, objects tracks and their visual attributions are extracted from each shot, along with their spatial and temporal relations.", "labels": [], "entities": []}, {"text": "In cases of zero-shot action recognition, where no verb (action class) is assigned fora given track, the detected objects classes are used to mine the relative verb from web-scale textual corpora via incorporated text-mined likelihoods.", "labels": [], "entities": [{"text": "zero-shot action recognition", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6788250307242075}]}, {"text": "Structuring videos at shot-level enables us to utilise the temporal information associated with video data.", "labels": [], "entities": []}, {"text": "Finally, the set of detected HLFs will be used to generate the final description for the video using a template-based approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the evaluation procedure of our video description framework on the NLDHA dataset introduced in.", "labels": [], "entities": [{"text": "NLDHA dataset", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9810589849948883}]}, {"text": "First, a brief overview of the baseline approach used to provide a comparison with our system is presented.", "labels": [], "entities": []}, {"text": "Next, the results of quantitative evaluation with the ROUGE Metric, along with qualitative human judgements, are discussed.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.8878730535507202}]}, {"text": "The complexity of evaluating video textual descriptions comes from the fact that defining the criteria is a challenging task.", "labels": [], "entities": []}, {"text": "To evaluate our method, we examine the metrics commonly used for this purpose in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7930189967155457}]}, {"text": "These metrics include the BLEU (bilingual evaluation understudy) () and ROUGE (Recall Oriented Understudy for Gisting Evaluation)) metrics, among others.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9985671043395996}, {"text": "ROUGE", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9921293258666992}]}, {"text": "The BLEU score calculates precision on a word basis or n-grams, and for this reason is not suitable for our task of lingual video description, as has already suggest by (Mitchell et al., 2012) and (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9728825390338898}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9940661191940308}, {"text": "lingual video description", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.6861988504727682}]}, {"text": "By contrast, ROUGE score is an n-gram recall oriented measure of the information coverage of human annotation references compared to automatic summaries produced by a system.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9836373329162598}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9710044264793396}]}, {"text": "A higher ROUGE score denotes a higher degree of match between them.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9857656359672546}, {"text": "match", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.8668302893638611}]}, {"text": "In general, a score of '1' indicates a perfect match whereas a score close to '0' means the match occurs in only a small portion of the data.", "labels": [], "entities": []}, {"text": "Four different ROUGE scores are used in this experiment, ROUGE-1 (unigram) recall is the perfect option to compare descriptions based on predicted keywords only ().", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9707842469215393}, {"text": "ROUGE-1", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9861640334129333}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.5825536847114563}]}, {"text": "ROUGE-2 (bigram) and ROUGE-SU4 (skip-4 bi-gram) scores are best to evaluate lingual video descriptions for coherence and fluency, whereas ROUGE-L scores depend on the longest common subsequence.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8613579869270325}, {"text": "ROUGE-SU4", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.946557879447937}]}, {"text": "ROUGE metrics are chosen for this study following () who used it to evaluate lingual video summarisation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8655462861061096}, {"text": "lingual video summarisation", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.610334058602651}]}, {"text": "present the average ROUGE scores achieved between the automatic descriptions produced by the baseline and our system, averaged overall twelve different human action categories, with respect to manual annotations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9957558512687683}]}, {"text": "Manual annotations tend to be subjective as they depend on the annota-90   tors perception and understanding.", "labels": [], "entities": []}, {"text": "Moreover, this subjectivity might be affected by personal education level, interests, background and experiences.", "labels": [], "entities": []}, {"text": "As a result, the ROUGE metric inevitably penalises many automatically generated sentences where these do not match the manual annotations, despite being technically correct.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9676671624183655}]}, {"text": "Clearly, the best results were obtained by ROUGE-1, as our method involves an extended language vocabulary compared to the baseline.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9766829609870911}]}, {"text": "This richness comes from a varied set of verbs included along with their scene setting, especially when the language model is involved for the case of zeroshot action recognition.", "labels": [], "entities": [{"text": "zeroshot action recognition", "start_pos": 151, "end_pos": 178, "type": "TASK", "confidence": 0.6594135661919912}]}, {"text": "(e.g.When 'person' and 'TV' are detected in the scene without a connected verb, the language model will infer the verb 'watch' to complete the sentence.)", "labels": [], "entities": []}, {"text": "Additionally, ROUGE-L results confirm the efficiency of our approach as  it captures similarity at sentence-level between the automatic generated descriptions and hand annotations.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9555409550666809}]}, {"text": "There is also an observable improvement for ROUGE-2 and ROUGE-SU4.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.6614082455635071}]}, {"text": "This is not surprising since attributes (such as adjectives and prepositions) and co-reference enhance the quality of description by generating richer and less verbose descriptions.", "labels": [], "entities": []}, {"text": "However, this kind of improvement in quality does not usually contribute considerably to the ROUGE score, which is based on n-gram comparisons.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9855130612850189}]}, {"text": "The ROUGE metrics produce only a rough estimate of the informativeness of an automatically produced summary, as it does not consider other significant aspects, such as readability or overall responsiveness.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8174059987068176}]}, {"text": "To evaluate these types of aspects there is an urgent need for manual evaluation.", "labels": [], "entities": []}, {"text": "For this task Amazon Mechanical Turk was used to collect human judgements of automatic video descriptions.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 14, "end_pos": 36, "type": "DATASET", "confidence": 0.8794230421384176}, {"text": "collect human judgements of automatic video descriptions", "start_pos": 49, "end_pos": 105, "type": "TASK", "confidence": 0.6412787352289472}]}, {"text": "We follow () and asked 10 Turk workers to rate video descriptions generated by the baseline and our description.", "labels": [], "entities": []}, {"text": "Each worker watched each video and rated the description on a scale of 1 to 5, where 5 means 'perfect description', and 1 indicates 'bad description'.", "labels": [], "entities": []}, {"text": "The description rating was based on three different criteria: grammar, correctness, and relevance.", "labels": [], "entities": [{"text": "relevance", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9483794569969177}]}, {"text": "For both the correctness and relevance aspects, the video was displayed with its description.", "labels": [], "entities": [{"text": "correctness", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9469788074493408}]}, {"text": "The correctness evaluates to what extent the textual description depicted the video semantic content, while the relevance rates if the sentence captures the most salient actions and objects.", "labels": [], "entities": [{"text": "correctness", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9611787796020508}, {"text": "relevance", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9882683157920837}]}, {"text": "For the grammar correctness, only lingual descriptions were presented to the worker, without the video, to evaluate the sentence.", "labels": [], "entities": []}, {"text": "shows the results of human evaluation of both the baseline and our approach.", "labels": [], "entities": []}, {"text": "It can be observed that our system improves on the baseline in all three aspects.", "labels": [], "entities": []}, {"text": "However, the relevance score significantly outperforms the baseline with margin of 1.61.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9707420468330383}, {"text": "margin", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9895267486572266}]}, {"text": "This indicates that our approach is able to describe much more semantic video content, especially in terms of activities, attributes and scene setting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: ROUGE scores calculated for the baseline and our", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9953588843345642}]}, {"text": " Table 3: Human evaluation for the baseline and our approach,", "labels": [], "entities": []}]}