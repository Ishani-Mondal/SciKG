{"title": [{"text": "Learning to Compose Spatial Relations with Grounded Neural Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Language is compositional: we can generate and interpret novel sentences by having a notion of meaning of their individual parts.", "labels": [], "entities": []}, {"text": "Spatial descriptions are grounded in perceptional representations but their meaning is also defined by what neighbouring words they co-occur with.", "labels": [], "entities": [{"text": "Spatial descriptions", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8991720974445343}]}, {"text": "In this paper we examine how language models conditioned on perceptual features can capture the semantics of composed phrases as well as of individual words.", "labels": [], "entities": []}, {"text": "We generate a synthetic dataset of spatial descriptions referring to perceptual scenes and examine how grounded language models built with deep neural networks can account for compositionality of descriptions-by evaluating how the learned language models can deal with novel grounded composed descriptions and novel grounded decomposed descriptions , constituents previously not seen in isolation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing and reasoning with linguistic meaning is a central task in computational linguistics.", "labels": [], "entities": [{"text": "Representing and reasoning with linguistic meaning", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.839612623055776}]}, {"text": "Here two kinds of meaning representations are used: (i) probabilistic language models and (ii) meaning representations grounded in other, typically perceptual information.", "labels": [], "entities": []}, {"text": "Recently, there have been several approaches in deep learning that deal with both, either independently or together.", "labels": [], "entities": []}, {"text": "The main goal of probabilistic language models is to estimate a probability distribution of sequences of words based on observable samples from language production, typically by estimating conditional probabilities of words with a categorical distribution.", "labels": [], "entities": []}, {"text": "This gives language models means for representing words as sequences with a measure of likelihood for each sequence.", "labels": [], "entities": []}, {"text": "Neural language models perform this objective by parametrising a probability density function with parametric representations of words and functions which compose words into phrases (.", "labels": [], "entities": []}, {"text": "The gradient based learning in neural networks turns the modelling problem into an optimisation problem, minimising the error or distance between a model prediction and an observable data over a list of parameters: 1.", "labels": [], "entities": []}, {"text": "parameters representing words with feature vectors known as word embeddings; 2.", "labels": [], "entities": []}, {"text": "parameters of functions composing word features into a structure; 3.", "labels": [], "entities": []}, {"text": "parameters of projections from final composed representations to categorical probabilities which in sequential models are the next word predictions.", "labels": [], "entities": []}, {"text": "There have been many attempts to show that the learned word embeddings in vector spaces are good representations of meaning.", "labels": [], "entities": []}, {"text": "Basing the argument on the distributional hypothesis, if a probabilistic model of words is conditioned on their context words (i.e. skip-grams or bag-of-words), the word embeddings must encode semantic information by having learned distances in vector spaces which correspond to semantic similarity scores obtained through relatedness tests performed by native speakers.", "labels": [], "entities": []}, {"text": "These representations were extended to word compositions by considering different compositional functions as vector manipulations).", "labels": [], "entities": []}, {"text": "Our notion of composition in a language model is broader than this: it involves (1) distributional models of words estimated from word sequences as well as (2) their grounding into representations of physical space.", "labels": [], "entities": []}, {"text": "This extends the Montague's notion of compositionality.", "labels": [], "entities": []}, {"text": "Lexical representations and their compositions are not dependent on meaning postulates and lexicalised constraints but rather perceptual evidence which is (probabilistically) associated with them.; define language grounding as a process of relating words with an agent's perception.", "labels": [], "entities": []}, {"text": "The ambiguity and vagueness of grounded meanings as well as of syntactic structures suggest that the connection between language and perception is gradient and therefore probabilistic.", "labels": [], "entities": []}, {"text": "The main approaches to probabilistic models of grounded language are probabilistic learning of grounded language and grammar, classifiers, and feature representations in perceptual space such as colour.", "labels": [], "entities": []}, {"text": "Our proposal is inline with all three approaches.", "labels": [], "entities": []}, {"text": "A grounded language model is a language model conditioned by perceptual representations that it refers to.", "labels": [], "entities": []}, {"text": "Ideally, the model should capture how each constituent in the composed phrase relates to some perceptual representations.", "labels": [], "entities": []}, {"text": "For example, in an image captioning task, a grounded language model estimates a conditional probability of a word sequence w 1:T given some image feature c that the words refers to.", "labels": [], "entities": [{"text": "image captioning task", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.8176765243212382}]}, {"text": "A general way to model word sequences is to use the chain rule as follows.", "labels": [], "entities": []}, {"text": "The model can generate phrases and sentences step-by-step by predicting the next word in a sequence: Pr(w t |w 1:t , c) The parametrisation of vision and language is often done by combining word-embeddings with multimodal embeddings ().", "labels": [], "entities": [{"text": "Pr", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9774866700172424}]}, {"text": "In the state of the art models for image captioning with encoder-decoder architecture, the encoder module is trained under the assumption that grounded words only denote features in subareas of an image, e.g bounding boxes) and pixel-wise mapping with attention models (.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.744602620601654}]}, {"text": "Another example of a visually grounded language model is a model that is used to demonstrate the compositionality of colour descriptions in () where linguistic descriptions are associated with areas of the colour space.", "labels": [], "entities": []}, {"text": "Similar to, each observed instance is a colour term paired with a colour code but instead of considering each description as a lexical entry, phrases are captured by a grounded language model as in Equation 1.", "labels": [], "entities": []}, {"text": "The qualitative human evaluation of how newly composed colour words by this model refer to the colour space suggest that language models can capture compositionality through gradient learning used with neural networks.", "labels": [], "entities": []}, {"text": "In this paper, we followup and extend the work of (.", "labels": [], "entities": []}, {"text": "We focus on recurrent neural language models of sequences of words conditioned by encoded locations that these words refer to in visual scenes.", "labels": [], "entities": []}, {"text": "Hence, we are interested in grounded semantic composition that is not only captured by probabilistic models of words given their context words, but also by models of their relatedness to perceptual representations.", "labels": [], "entities": []}, {"text": "An important and novel question we investigate is what these models are learning: to what degree the representations of meaning (both collocational from vector spaces and grounded in perception) are interpretable and therefore compositional in the sense of.", "labels": [], "entities": []}, {"text": "We focus on one domain of grounded meaning: spatial descriptions of various length and their grounding in spatial templates of.", "labels": [], "entities": []}, {"text": "In particular we try to answer the following questions: (1) To what extent are the language models that have been learned grounded in spatial representations?", "labels": [], "entities": []}, {"text": "(2) Is it possible to generate new, previously unseen grounded composed spatial descriptions from observing their words only in other grounded composed phrases?", "labels": [], "entities": []}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the creation of an artificial dataset of composed spatial templates and the associated descriptions based on the experimental work of.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our neural network model which we use for training our grounded language model.", "labels": [], "entities": []}, {"text": "Section 4 describes an evaluation of the learned representations compared to the original representations the system was learning from.", "labels": [], "entities": []}, {"text": "Finally, Section 5 points to conclusions and further work.", "labels": [], "entities": []}, {"text": "The code and results are available at https://github.com/GU-CLASP/ spatial-composition.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to train a grounded language model we require samples of language use paired with locations they are referring to.", "labels": [], "entities": []}, {"text": "Considering the rationality of speakers and their observers, the frequency of each co-occurring utterance-location corresponds to the appropriateness of such utterance as a description of that location.", "labels": [], "entities": []}, {"text": "One complication of judging the appropriateness of spatial terms this way is that they are not only depended on the location they describe but also on other properties of the situation such as the agreed frame of reference, object shape, and the function of the landmark and the target objects involved, etc..", "labels": [], "entities": []}, {"text": "However, these properties will not be considered in the present study.", "labels": [], "entities": []}, {"text": "performed several psychological experiments related to the geometric apprehension of spatial relations.", "labels": [], "entities": []}, {"text": "For example, they collected acceptability ratings (1-9) fora set of spatial relations per different locations of the target object in a 7 \u00d7 7 grid relative to the landmark object in the centre (3, 3).", "labels": [], "entities": []}, {"text": "The acceptability scores were collected from 32 informants through random presentation and then averaged per location.", "labels": [], "entities": []}, {"text": "The matrix of average acceptability scores per description is called a spatial template and represents the appropriateness of each location in the process of interpreting that spatial relation.", "labels": [], "entities": []}, {"text": "They collect spatial templates for the following spatial relations: right of, left of, below, under, over, above, near to, next to, far from, and away from which we also apply in our work.", "labels": [], "entities": []}, {"text": "Furthermore, in order to be able to explore the limits of the language models for learning compositions, we extend this vocabulary with a few additional words.", "labels": [], "entities": []}, {"text": "We describe how we used them to synthesise the composed spatial templates for our training data in the following section.", "labels": [], "entities": []}, {"text": "We evaluate the learning of composed grounded phrases by examining to what degree the spatial templates produced by the learned model correspond to the original spatial templates that were used in generating the training data, how successful is the learning with different kinds of compositions, and what is the effect of adding distractor words.", "labels": [], "entities": []}, {"text": "We ran two experiments, (1) on a simple synthetic dataset containing short phrases where all words are grounded in locations, and (2) on a synthetic dataset generated with five additional grammar rules from Equation 11, introducing words without spatial grounding or distractor words.", "labels": [], "entities": []}, {"text": "We test the learning of compositional phrases by training a language model on phrases produced by individual composition types as well as all composition types in both synthetic datasets.", "labels": [], "entities": []}, {"text": "A comparison of the predicted spatial templates with the original spatial templates with Spearman's rank correlation coefficient (Equation 5) in shows that there is high correlation between them.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 89, "end_pos": 128, "type": "METRIC", "confidence": 0.6276991307735443}, {"text": "Equation 5)", "start_pos": 130, "end_pos": 141, "type": "METRIC", "confidence": 0.8730718890825907}]}, {"text": "We report the average Spearman's \u03c1 and their median p-values for statistical significance.: For each type of compositional phrases we calculate the average Spearman's rank correlation coefficient (\u03c1) between the predicted spatial templates and the templates used to generate the training data.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient (\u03c1)", "start_pos": 156, "end_pos": 199, "type": "METRIC", "confidence": 0.785427026450634}]}, {"text": "The median p-value of \u03c1 of all trained models is < 0.001.", "labels": [], "entities": []}, {"text": "The column Untrained indicates the performance of the model with a random initialisation of weights.", "labels": [], "entities": [{"text": "Untrained", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9544323682785034}]}, {"text": "For both Experiment 1 and 2 we created two variations: (1) learning of novel grounded compositions, where different proportions of AND-phrases and OR-phrases are omitted from the dataset and therefore hidden from the learner; (2) learning of novel single words from grounded compositions, where proportions of single-word instances are omitted from the dataset and their representations can only be learned from their occurrence in composed phrases with other words.", "labels": [], "entities": []}, {"text": "In all experiments we holdout 10% of the dataset for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 53, "end_pos": 63, "type": "TASK", "confidence": 0.9601947665214539}]}, {"text": "In Experiment 1 we iterated the training over 64 epochs using a batch size 8.", "labels": [], "entities": []}, {"text": "In Experiment 2, using a batch size 256, we stopped learning iterations before 1024 epochs if the validation loss became equal to the training loss.", "labels": [], "entities": []}, {"text": "In this experiment the training data is generated for single spatial words, AND-compositions, ORcompositions, and negated phrases without additional distractor words save \"and\", \"either\", \"or\", and \"not\".", "labels": [], "entities": []}, {"text": "In Experiment 2 we train and measure the performance of the model on grounded descriptions which also include non-grounded distractor words, for example: \"the ball is not left of the box\" or \"it is above and right of the object\".", "labels": [], "entities": []}, {"text": "The words such as \"ball\", \"object\", \"box\", \"it\" and \"is\" provide no contribution to the grounded meaning (location).", "labels": [], "entities": []}, {"text": "In this dataset the number of possible composed phrases increases from 200 to 1,000.", "labels": [], "entities": []}, {"text": "Algorithm 1 in Section 2.2 ensures that in the 1,000 possible phrases the same number of instances is generated as before, now per each of the five permutation rules introducing distractors.", "labels": [], "entities": []}, {"text": "The held-out proportions of spatial descriptions are created before Algorithm 1 is applied so permutations including these are not generated.", "labels": [], "entities": []}, {"text": "In Experiment 3, we examine how the amount of training corresponds to the groundedness of expressions in spatial templates.", "labels": [], "entities": []}, {"text": "In particular, we examine the learning curve across several epochs at which more of the same data is presented incrementally to the learner and how well does the currently learned model corresponds to the target spatial templates.", "labels": [], "entities": []}, {"text": "Typically, the performance of the learner at each epoch is estimated by a loss function, here the cross-entropy (log-loss).", "labels": [], "entities": []}, {"text": "We compare the loss at each epoch with the average Spearman's \u03c1 between the predicted templates and the original templates for 110 possible combinations of descriptions from Experiment 1 (excluding OR-phrases).", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.9656767845153809}]}, {"text": "Here, we only run the experiment with 20% omission of the dataset.", "labels": [], "entities": []}, {"text": "shows how average \u03c1 corresponds to the learning progress.", "labels": [], "entities": []}, {"text": "The figure shows that even after the training and the validation loss are only slightly decreasing between epochs the groundedness is increasing at a higher rate.", "labels": [], "entities": [{"text": "validation", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.9582313299179077}]}, {"text": "This can be explained by the fact that the network is not only predicting locations but also sequences of descriptions which adds a further complexity to learning which is reflected in the loss.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: For each type of compositional phrases we calculate the average Spearman's rank correlation  coefficient (\u03c1) between the predicted spatial templates and the templates used to generate the training  data. The median p-value of \u03c1 of all trained models is < 0.001. The column Untrained indicates the  performance of the model with a random initialisation of weights.", "labels": [], "entities": [{"text": "Spearman's rank correlation  coefficient (\u03c1)", "start_pos": 74, "end_pos": 118, "type": "METRIC", "confidence": 0.892535112798214}]}, {"text": " Table 2: Spearman's \u03c1 for held-out proportions of phrases up to 80% have a median p-value < 0.001 and  p-value > 0.05 for higher proportions.", "labels": [], "entities": []}, {"text": " Table 3 with Table 2.", "labels": [], "entities": []}]}