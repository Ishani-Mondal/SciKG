{"title": [{"text": "Lexical Chains meet Word Embeddings in Document-level Statistical Machine Translation", "labels": [], "entities": [{"text": "Document-level Statistical Machine Translation", "start_pos": 39, "end_pos": 85, "type": "TASK", "confidence": 0.6278533563017845}]}], "abstractContent": [{"text": "The phrase-based Statistical Machine Translation (SMT) approach deals with sentences in isolation, making it difficult to consider discourse context in translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.797845130165418}]}, {"text": "This poses a challenge for ambiguous words that need discourse knowledge to be correctly translated.", "labels": [], "entities": []}, {"text": "We propose a method that benefits from the semantic similarity in lexical chains to improve SMT output by integrating it in a document-level decoder.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 92, "end_pos": 102, "type": "TASK", "confidence": 0.9229375720024109}]}, {"text": "We focus on word embeddings to deal with the lexical chains, contrary to the traditional approach that uses lexical resources.", "labels": [], "entities": []}, {"text": "Experimental results on German\u2192English show that our method produces correct translations in up to 88% of the changes, improving the translation in 36%-48% of them over the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current phrase-based Statistical Machine Translation (SMT) systems translate sentences in a document independently (, ignoring document context.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.7742629994948705}]}, {"text": "This sentence-level approach causes wrong translations when discourse knowledge is needed.", "labels": [], "entities": []}, {"text": "Therefore, many methods that integrate discourse features have been proposed to improve lexical choice.", "labels": [], "entities": []}, {"text": "Documents area set of sentences that function as a unit.", "labels": [], "entities": []}, {"text": "When we translate at document-level we take into account document properties that help to improve the quality of the translation, not only locally, but also in the context of the document.", "labels": [], "entities": []}, {"text": "Coherence and cohesion are terms that describe properties of texts.", "labels": [], "entities": []}, {"text": "Coherence concerns the semantic meaningfulness of the text, whereas cohesion has to do with relating the sentences through reference, ellipsis, substitution, conjunction, and the use of semantically-similar words.", "labels": [], "entities": []}, {"text": "Often, these words are related sequentially in the document, defining the topic of the text segment that they cover.", "labels": [], "entities": []}, {"text": "These sequences of words are lexical chains, and they have been successfully used in research areas such as information retrieval and document summarization (.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.8380407691001892}, {"text": "document summarization", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.682135820388794}]}, {"text": "However, they have received little attention in Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.8698026418685914}]}, {"text": "introduce a method to detect lexical chains using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9720557928085327}]}, {"text": "The method first builds a representation of all words in the document and all their senses, creating semantic links such as synonym, hypernym, hyponym, and sibling between them.", "labels": [], "entities": []}, {"text": "It then uses the semantic links to disambiguate each word and builds the lexical chains accordingly.", "labels": [], "entities": []}, {"text": "The performance of the method is evaluated on a sense disambiguation task.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.6643707007169724}]}, {"text": "Indeed, lexical chains help to disambiguate the sense of polysemic words by looking at the words in the chain.", "labels": [], "entities": []}, {"text": "Despite the problems of word senses, it shows the potential that lexical chains have to improve the lexical choice of words with multiple translations in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.856688916683197}]}, {"text": "In this paper, we present a method that uses word embeddings instead of lexical resources to detect the lexical chains in the source and also to maintain their semantic similarity on the target side.", "labels": [], "entities": []}, {"text": "We focus on the German\u2192English translation and integrate our model into the documentlevel SMT decoder Docent ().", "labels": [], "entities": [{"text": "documentlevel SMT decoder Docent", "start_pos": 76, "end_pos": 108, "type": "DATASET", "confidence": 0.6732872575521469}]}, {"text": "We perform a manual evaluation of the output, which shows that our method improves the translation over the baseline, with a tendency to consistently translate the words in the chain.", "labels": [], "entities": []}, {"text": "Furthermore, experimental results reveal that the use of word embeddings in lexical chain detection outperforms lexical resources on the translation task.", "labels": [], "entities": [{"text": "lexical chain detection", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7066872715950012}]}], "datasetContent": [{"text": "In this section, we present the results obtained through the combination of lexical chain detection (using word embeddings and GermaNet) and the LCTM.", "labels": [], "entities": [{"text": "lexical chain detection", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.6564306418100992}, {"text": "LCTM", "start_pos": 145, "end_pos": 149, "type": "DATASET", "confidence": 0.8220805525779724}]}, {"text": "The LCTM takes into account the relevance (i.e. strength) of every lexical chain to compute the overall score.", "labels": [], "entities": [{"text": "relevance", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9647683501243591}]}, {"text": "We also perform a third experiment that ignores this fact to assess its impact in the translation quality.", "labels": [], "entities": []}, {"text": "To do so, we develop a model that behaves like the LCTM, except that it assigns the maximum strength value (i.e. 1.0) to all lexical chains.", "labels": [], "entities": [{"text": "LCTM", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8050704002380371}]}, {"text": "We refer to this new model in the following as LCTM base . The baseline BLEU scores () of the test sets newstest2010, newstest2011, Chain politik \u2192 politischer Input ich bin ein neuling in der prager politik Ref.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9943207502365112}]}, {"text": "i'm a novice in prague politics Base.", "labels": [], "entities": []}, {"text": "i am a newcomer in the prague policy LC i am a newcomer in the prague politics Chain erkl\u00e4rt \u2192 meint \u2192 meint Input \"hier geht niemand vor gericht\", meint . .", "labels": [], "entities": []}, {"text": "\"nobody will sue them here,\" said.", "labels": [], "entities": []}, {"text": "\"here is no one in court\", . .", "labels": [], "entities": []}, {"text": "LC \"here is no one in court\", says.", "labels": [], "entities": []}, {"text": "more than it spends on salaries.", "labels": [], "entities": []}, {"text": "more than they for wage donations.", "labels": [], "entities": []}, {"text": "more than they for pay donations.", "labels": [], "entities": []}, {"text": "In the last example, the presented method incorrectly translates lohn into pay, despite the context given by the lexical chain: eh\u00f6ht (\"increase\") and lohnerh\u00f6hungen (\"wage increases\"). and newstest2013 are 12.44, 12.18, and 17.64, respectively.", "labels": [], "entities": [{"text": "newstest2013", "start_pos": 190, "end_pos": 202, "type": "DATASET", "confidence": 0.8814340829849243}]}, {"text": "The results of the experiments show between 20 to 30 translation changes in every test set due to lexical chains.", "labels": [], "entities": []}, {"text": "We observe that the translation changes are often correct although they do not use the same terms as in the reference.", "labels": [], "entities": []}, {"text": "Therefore, the fluctuations in BLEU scores are small (\u00b10.1), and so BLEU does not provide sufficient insight into the performance.", "labels": [], "entities": [{"text": "fluctuations", "start_pos": 15, "end_pos": 27, "type": "METRIC", "confidence": 0.9610297679901123}, {"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9943398833274841}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9949270486831665}]}, {"text": "We then perform a manual evaluation to assess the results of the experiments.", "labels": [], "entities": []}, {"text": "The annotation is carried out by two annotators who judge the quality of the translation changes due to the lexical chains.", "labels": [], "entities": []}, {"text": "Specifically, the annotators obtain for each translation change the source sentence, the baseline (i.e. the translation ignoring lexical chains), the translation produced by the method we want to evaluate, and the reference.", "labels": [], "entities": []}, {"text": "They then anno-: Manual evaluation results of the presented method (1) compared to using GermaNet for lexical chain detection.", "labels": [], "entities": [{"text": "lexical chain detection", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.6334753334522247}]}, {"text": "The analysis shows the percentage of correct (+), wrong translations (-), and the improvement over the baseline (++).", "labels": [], "entities": [{"text": "correct", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9798888564109802}]}, {"text": "There area total of 20 to 30 translation changes in every test set due to the lexical chains.", "labels": [], "entities": []}, {"text": "We observe that the method (1) outperforms the approach that uses GermaNet (2).", "labels": [], "entities": []}, {"text": "It also performs better than the method that ignores length, density, and repetition for the computation of the strength of each lexical chain in the overall score (3).", "labels": [], "entities": [{"text": "repetition", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.99845290184021}]}, {"text": "tate whether the word that changes due to lexical chains is better than the one produced by the baseline, equally good or worse.", "labels": [], "entities": []}, {"text": "The Cohen's Kappa coefficient of inter-rater agreement between the two annotators is 0.77.", "labels": [], "entities": []}, {"text": "We then compute from the annotations the percentage of incorrect and good translations and the improvement over the baseline.", "labels": [], "entities": []}, {"text": "shows the results of the manual evaluation.", "labels": [], "entities": []}, {"text": "We observe that the combination of lexical chain detection using word embeddings with our LCTM performs best.", "labels": [], "entities": [{"text": "lexical chain detection", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.6947212219238281}]}, {"text": "In particular, 81%-88% of the changes are correct translations, and among them, 36%-48% are improvements over the baseline.", "labels": [], "entities": []}, {"text": "Only 12%-19% of the changes are incorrect.", "labels": [], "entities": [{"text": "incorrect", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9625657200813293}]}, {"text": "With GermaNet to detect lexical chains, the correctness decreases between 10% and 26%.", "labels": [], "entities": [{"text": "correctness", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9980760812759399}]}, {"text": "Word embeddings may work better than lexical resources as they capture contextual information from the text, without relying on whether is defined in a resource.", "labels": [], "entities": []}, {"text": "In those cases, where the resource does not provide a relation for two given words such as in idiomatic or metaphoric uses, the lexical chain cannot benefit from them.", "labels": [], "entities": []}, {"text": "The parameters length, density, and repetition have an impact on translation when using them to compute the strength of each lexical chain in the overall LCTM score.", "labels": [], "entities": [{"text": "length", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.961571216583252}, {"text": "repetition", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9979643821716309}, {"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9500046968460083}]}, {"text": "We see that the correctness of the translation output decreases approximately by 20% in all test sets when using the LCTM base (i.e. the model that gives the highest strength value to all lexical chains, ignoring the mentioned parameters) instead of the LCTM.", "labels": [], "entities": []}, {"text": "Furthermore, the percentage of the improvements over the baseline decrease by half.", "labels": [], "entities": []}, {"text": "Some translation examples using our method are illustrated in.", "labels": [], "entities": []}, {"text": "In the first example, the ambiguous German noun Politik gets correctly translated into politics.", "labels": [], "entities": []}, {"text": "Politik is connected to politischer (\"political\") in the lexical chain, and therefore politics is semantically more related to political than policy.", "labels": [], "entities": []}, {"text": "Our method is also good at enforcing the translation of all words in the lexical chain, since an untranslated word will decrease the score of the translated lexical chain, and accordingly, the overall LCTM score (see Example 2).", "labels": [], "entities": [{"text": "LCTM score", "start_pos": 201, "end_pos": 211, "type": "METRIC", "confidence": 0.7414354383945465}]}, {"text": "In the last example, the method produces a wrong translation of the German word lohn (\"wage\", \"salary\"), whereas the baseline translates it correctly.", "labels": [], "entities": []}, {"text": "The word lohn is linked to erh\u00f6ht (\"increase\") and lohnerh\u00f6hungen (\"wage increases\") in the lexical chain.", "labels": [], "entities": [{"text": "erh\u00f6ht", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9852340817451477}]}, {"text": "Both words provide good context for the translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9684404730796814}]}, {"text": "However, our method incorrectly translates it into pay, whereas the baseline translates it correctly into wage.", "labels": [], "entities": []}, {"text": "In the third example, we observe that the method produces a different but equally good translation compared to the baseline.", "labels": [], "entities": []}, {"text": "In the lexical chain, the German word Rakete is linked to another occurrence of the same word that is translated into missile.", "labels": [], "entities": []}, {"text": "Since the highest similarity score is obtained when both translations are the same, our method encourages consistency, translating both into missile.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.9715975821018219}, {"text": "consistency", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9674854874610901}]}, {"text": "Consistency is possible since we assume that there is only a unique sense per word in each document ().", "labels": [], "entities": []}, {"text": "illustrates the benefits and issues of consistent translation.", "labels": [], "entities": [{"text": "consistent translation", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.5072451382875443}]}, {"text": "These are special cases, where the word in the lexical chain is linked only to other occurrences of the same word.", "labels": [], "entities": []}, {"text": "In the first example, we observe that the baseline translates the wrong sense of the word wahl (i.e. choice).", "labels": [], "entities": []}, {"text": "Here, wahl is linked to another oc-: These examples show how the presented method behaves when a word in the lexical chain is linked to the same word in the text.", "labels": [], "entities": []}, {"text": "In the first example, the German word wahl is linked to another occurrence of wahl in the text.", "labels": [], "entities": []}, {"text": "The later is correctly translated into election, and therefore the LCTM gets a higher score when the first sentence is translated into the same term.", "labels": [], "entities": [{"text": "LCTM", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.6728407740592957}]}, {"text": "This produces an improvement over the baseline that wrongly translates it into choice.", "labels": [], "entities": []}, {"text": "In the second example, both senses of the word verh\u00e4ltnis occur in the same document, forcing the first occurrence to be incorrectly translated.", "labels": [], "entities": []}, {"text": "currence of the same word in the lexical chain, which is translated into the other sense election.", "labels": [], "entities": []}, {"text": "Since the method obtains the highest score when the translations are the same, it either encourages both occurrences to be translated into election or choice.", "labels": [], "entities": []}, {"text": "The LCTM score competes with other models such as language and translation model.", "labels": [], "entities": []}, {"text": "The overall score when using the translation choice is then lower than when using election due to the other models, since choice does not fit in the local context of the other sentence.", "labels": [], "entities": []}, {"text": "In the second example, however, the method translates the wrong sense of verh\u00e4ltnis.", "labels": [], "entities": [{"text": "verh\u00e4ltnis", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.8899971842765808}]}, {"text": "That is because the two senses of the word verh\u00e4ltnis (\"ratio\" and \"relationship\") are in the same document.", "labels": [], "entities": [{"text": "verh\u00e4ltnis (\"ratio", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.8095985452334086}]}, {"text": "This fact violates the one-sense-per-discourse hypothesis, and when the only context provided by the lexical chain is the word itself, the method cannot disambiguate the senses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Manual evaluation results of the presented method (1) compared to using GermaNet for lexical  chain detection", "labels": [], "entities": [{"text": "lexical  chain detection", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.716129998366038}]}]}