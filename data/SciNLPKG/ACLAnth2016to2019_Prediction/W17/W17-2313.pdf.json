{"title": [{"text": "Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second-Order Vectors", "labels": [], "entities": [{"text": "Improving Correlation with Human Judgments", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8994322419166565}]}], "abstractContent": [{"text": "Vector space methods that measure semantic similarity and relatedness often rely on distributional information such as co-occurrence frequencies or statistical measures of association to weight the importance of particular co-occurrences.", "labels": [], "entities": []}, {"text": "In this paper, we extend these methods by incorporating a measure of semantic similarity based on a human curated taxon-omy into a second-order vector representation.", "labels": [], "entities": []}, {"text": "This results in a measure of semantic relatedness that combines both the con-textual information available in a corpus-based vector space representation with the semantic knowledge found in a biomedical ontology.", "labels": [], "entities": []}, {"text": "Our results show that incorporating semantic similarity into a second order co-occurrence matrices improves correlation with human judgments for both similarity and relatedness, and that our method compares favorably to various different word embedding methods that have recently been evaluated on the same reference standards we have used.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measures of semantic similarity and relatedness quantify the degree to which two concepts are similar (e.g., lung-heart) or related (e.g., lungbronchitis).", "labels": [], "entities": []}, {"text": "Semantic similarity can be viewed as a special case of semantic relatedness -to be similar is one of many ways that a pair of concepts maybe related.", "labels": [], "entities": []}, {"text": "The automated discovery of groups of semantically similar or related terms is critical to improving the retrieval () and clustering ( of biomedical and clinical documents, and the development of biomedical terminologies and ontologies).", "labels": [], "entities": [{"text": "automated discovery of groups of semantically similar or related terms", "start_pos": 4, "end_pos": 74, "type": "TASK", "confidence": 0.6929675042629242}]}, {"text": "There is along history in using distributional methods to discover semantic similarity and relatedness (e.g.,).", "labels": [], "entities": []}, {"text": "These methods are all based on the distributional hypothesis, which holds that two terms that are distributionally similar (i.e., used in the same context) will also be semantically similar).", "labels": [], "entities": []}, {"text": "Recently word embedding techniques such as word2vec () have become very popular.", "labels": [], "entities": []}, {"text": "Despite the prominent role that neural networks play in many of these approaches, at their core they remain distributional techniques that typically start with a word byword co-occurrence matrix, much like many of the more traditional approaches.", "labels": [], "entities": []}, {"text": "However, despite these successes distributional methods do not perform well when data is very sparse (which is common).", "labels": [], "entities": []}, {"text": "One possible solution is to use second-order co-occurrence vectors.", "labels": [], "entities": []}, {"text": "In this approach the similarity between two words is not strictly based on their co-occurrence frequencies, but rather on the frequencies of the other words which occur with both of them (i.e., second order co-occurrences).", "labels": [], "entities": []}, {"text": "This approach has been shown to be successful in quantifying semantic relatedness (.", "labels": [], "entities": [{"text": "quantifying semantic relatedness", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.8536576827367147}]}, {"text": "However, while more robust in the face of sparsity, second-order methods can result in significant amounts of noise, where contextual information that is overly general is included and does not contribute to quantifying the semantic relatedness between the two concepts.", "labels": [], "entities": []}, {"text": "Our goal then is to discover methods that automatically reduce the amount of noise in a secondorder co-occurrence vector.", "labels": [], "entities": []}, {"text": "We achieve this by incorporating pairwise semantic similarity scores derived from a taxonomy into our second-order vectors, and then using these scores to select only the most semantically similar co-occurrences (thereby reducing noise).", "labels": [], "entities": []}, {"text": "We evaluate our method on two datasets that have been annotated in multiple ways.", "labels": [], "entities": []}, {"text": "One has been annotated for both similarity and relatedness, and the other has been annotated for relatedness by two different types of experts (medical doctors and medical coders).", "labels": [], "entities": [{"text": "similarity", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.985051155090332}]}, {"text": "Our results show that integrating second order co-occurrences with measures of semantic similarity increases correlation with our human reference standards.", "labels": [], "entities": []}, {"text": "We also compare our result to a number of other studies which have applied various word embedding methods to the same reference standards we have used.", "labels": [], "entities": []}, {"text": "We find that our method often performs at a comparable or higher level than these approaches.", "labels": [], "entities": []}, {"text": "These results suggest that our methods of integrating semantic similarity and relatedness values have the potential to improve performance of purely distributional methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted our experiments using the freely available open source software package UMLS::Similarity () version 1.47 2 . This package takes as input two terms (or UMLS concepts) and returns their similarity or relatedness using the measures discussed in Section 2.", "labels": [], "entities": []}, {"text": "Correlation between the similarity measures and human judgments were estimated using Spearman's Rank Correlation (\u03c1).", "labels": [], "entities": [{"text": "Spearman's Rank Correlation (\u03c1)", "start_pos": 85, "end_pos": 116, "type": "METRIC", "confidence": 0.7109609544277191}]}, {"text": "Spearman's measures the statistical dependence between two variables to assess how well the relationship between the rankings of the variables can be described using a monotonic function.", "labels": [], "entities": []}, {"text": "We used Fisher's r-to-z transformation to calculate the significance between the correlation results.", "labels": [], "entities": [{"text": "significance", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.954535186290741}]}, {"text": "shows the Spearman's Rank Correlation between the human scores from the four reference standards and the scores from the various measures of similarity introduced in Section 2.", "labels": [], "entities": [{"text": "Spearman's Rank Correlation", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.8038763850927353}]}, {"text": "Each class of measure is followed by the scores obtained when integrating our second order vector approach with these measures of semantic similarity.", "labels": [], "entities": []}, {"text": "These results show that using a threshold cutoff of 2 obtains the highest correlation for the UMN-SRS dataset, and that a threshold cutoff of 4 obtains the highest correlation for the MiniMayoSRS dataset.", "labels": [], "entities": [{"text": "UMN-SRS dataset", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.9592104554176331}, {"text": "MiniMayoSRS dataset", "start_pos": 184, "end_pos": 203, "type": "DATASET", "confidence": 0.9835691452026367}]}, {"text": "All of the results show an increase in correlation with human judgments when incorporating a threshold cutoff overall of the original     measures.", "labels": [], "entities": []}, {"text": "The increase in the correlation for the UMNSRS tagged for similarity is statistically significant (p \u2264 0.05), however this is not the case for the UMNSRS tagged for relatedness nor for the MiniMayoSRS data.", "labels": [], "entities": [{"text": "MiniMayoSRS data", "start_pos": 189, "end_pos": 205, "type": "DATASET", "confidence": 0.9719768762588501}]}, {"text": "Similarly, shows the results of applying the threshold parameter (T) on each of the reference standards using the faith measure.", "labels": [], "entities": [{"text": "threshold parameter (T)", "start_pos": 45, "end_pos": 68, "type": "METRIC", "confidence": 0.871579909324646}]}, {"text": "Although, unlike res whose scores are greater than or equal to 0 without an upper limit, the faith measure returns scores between 0 and 1 (inclusive).", "labels": [], "entities": []}, {"text": "Therefore, here a threshold of 0 indicates that all of the bigrams were included in the similarity matrix; and a threshold of 0.1 indicates that only the bigram pairs with a similarity score greater than 0.1 were included.", "labels": [], "entities": []}, {"text": "The results show an increase inaccuracy for all of the datasets except for the MiniMayoSRS tagged for physicians.", "labels": [], "entities": []}, {"text": "The increase in the results for the UMNSRS tagged for similarity and the MayoSRS is statistically significant (p \u2264 0.05).", "labels": [], "entities": [{"text": "similarity", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9607006907463074}, {"text": "MayoSRS", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.7454527020454407}]}, {"text": "This is not the case for the UMNSRS tagged for relatedness nor the MiniMayoSRS.", "labels": [], "entities": [{"text": "MiniMayoSRS", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.9619114995002747}]}, {"text": "Overall, these results indicate that including only those bigrams that have a sufficiently high similarity score increases the correlation results with human judgments, but what quantifies as sufficiently high varies depending on the dataset and measure.", "labels": [], "entities": [{"text": "correlation", "start_pos": 127, "end_pos": 138, "type": "METRIC", "confidence": 0.9529926180839539}]}], "tableCaptions": [{"text": " Table 1: Spearman's Correlation Results  UMNSRS MiniMayoSRS  Resident  MD  Coder  sim  rel  relatedness  Path  path  0.52 0.28 0.35  0.45  wup  0.50 0.24 0.39  0.51  pks  0.49 0.25 0.38  0.50  zhong  0.50 0.25 0.42  0.50", "labels": [], "entities": [{"text": "UMNSRS MiniMayoSRS  Resident  MD  Coder  sim  rel", "start_pos": 42, "end_pos": 91, "type": "DATASET", "confidence": 0.8922524281910488}]}, {"text": " Table 2: Threshold Correlation with vector-res  UMNSRS MiniMayoSRS  T # bigrams sim  rel  MD  coder  0  850,959 0.58 0.41 0.58  0.65  1  166,003 0.56 0.39 0.60  0.67  2  65,502 0.64 0.47 0.56  0.62  3  27,744 0.60 0.46 0.62  0.71  4  10,991 0.56 0.43 0.75  0.76  5  3,305 0.26 0.16 0.36  0.36", "labels": [], "entities": [{"text": "UMNSRS MiniMayoSRS  T # bigrams sim  rel  MD  coder  0  850,959", "start_pos": 49, "end_pos": 112, "type": "DATASET", "confidence": 0.8404150009155273}]}, {"text": " Table 4: Comparison with Previous Work", "labels": [], "entities": []}]}