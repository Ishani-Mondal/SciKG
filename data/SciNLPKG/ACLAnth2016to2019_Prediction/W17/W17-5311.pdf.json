{"title": [{"text": "LCT-MALTA's Submission to RepEval 2017 Shared Task", "labels": [], "entities": [{"text": "LCT-MALTA's Submission to RepEval 2017 Shared Task", "start_pos": 0, "end_pos": 50, "type": "DATASET", "confidence": 0.5767182148993015}]}], "abstractContent": [{"text": "We present in this paper our team LCT-MALTA's submission to the RepEval 2017 Shared Task on natural language inference.", "labels": [], "entities": [{"text": "RepEval 2017 Shared Task on natural language inference", "start_pos": 64, "end_pos": 118, "type": "TASK", "confidence": 0.6080124564468861}]}, {"text": "Our system is a simple system based on a standard BiLSTM architecture , using as input GloVe word embed-dings augmented with further linguistic information.", "labels": [], "entities": []}, {"text": "We use max pooling on the BiLSTM outputs to obtain embeddings for sentences.", "labels": [], "entities": [{"text": "BiLSTM outputs", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8421635329723358}]}, {"text": "On both the matched and the mismatched test sets, our system clearly beats the shared task's BiLSTM baseline model.", "labels": [], "entities": []}], "introductionContent": [{"text": "The RepEval 2017 Shared Task aims to evaluate fixed-length vector representations (or embeddings) of sentences on the basis of a natural language understanding task, viz.", "labels": [], "entities": []}, {"text": "natural language inference (NLI), also known as recognising textual entailments.", "labels": [], "entities": [{"text": "natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.789733832081159}]}, {"text": "Given two sentences, the first being the premise and the second the hypothesis, the goal of NLI is to train a classifier to predict whether the relation of the hypothesis to the premise is one of entailment, contradiction or a neutral relation.", "labels": [], "entities": []}, {"text": "The training and test data for this 3-way classification task at RepEval 2017 are drawn from the Multi-Genre NLI, or MultiNLI corpus (see for details).", "labels": [], "entities": [{"text": "MultiNLI corpus", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.918119877576828}]}, {"text": "Task participants are provided with both training and development datasets, where parts of the development data match the training data in terms of genre, topic etc.", "labels": [], "entities": []}, {"text": "(referred to as matched examples) and other parts do not (referred to as mismatched examples).", "labels": [], "entities": []}, {"text": "This paper presents Team LCT-MALTA's submission to the shared task.", "labels": [], "entities": []}, {"text": "In line with previous research, we obtain a single vector which is the combined representation of both the premise and the hypothesis and feed it into a Multilayer Perceptron (MLP) for the actual 3-way classification.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with BiLSTM-based sentence encoders including and excluding our enhanced word embeddings as well as in combination with max pooling and average pooling.", "labels": [], "entities": []}, {"text": "We use L2 regularization and set dropout rate to 0.1 to prevent overfitting.", "labels": [], "entities": []}, {"text": "The models are trained in 10 epoches using Adam optimizer with learning rate 10 \u22123 . The models having the best performance on development set are selected to evaluate on test set.", "labels": [], "entities": []}, {"text": "Furthermore, we implemented two systems presented in literature, viz.", "labels": [], "entities": []}, {"text": "()'s selfattentive embeddings approach and: System performances on MultiNLI dataset in order of ascending accuracy scores. system.", "labels": [], "entities": [{"text": "MultiNLI dataset", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.9619394242763519}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9381542205810547}]}, {"text": "In their original work, use 30 hops of attention for each sentence.", "labels": [], "entities": []}, {"text": "In our implementation of their algorithm, we deemed 10 hops to be sufficient due to the limited length of the sentences in our MultiNLI database.", "labels": [], "entities": [{"text": "MultiNLI database", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.9412526786327362}]}, {"text": "The results of our experiments are shown in Table 1, in order of ascending accuracy scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9366880059242249}]}, {"text": "In our experiments, our final submission, i.e. the BiLSTM-encoder with enhanced word embeddings and max pooling, as described in section 3, performed best, obtaining accuracy scores of 70.8 and 71.1 on the matched and mismatched dev set, respectively.", "labels": [], "entities": [{"text": "BiLSTM-encoder", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.8404715657234192}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9992018342018127}]}, {"text": "Its final results on the full shared task test set are 70.7 for matched and 70.8 for mismatched data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: System performances on MultiNLI dataset in order of ascending accuracy scores.", "labels": [], "entities": [{"text": "MultiNLI dataset", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9674778878688812}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9451231956481934}]}]}