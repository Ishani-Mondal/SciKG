{"title": [{"text": "An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.7180567185084025}]}], "abstractContent": [{"text": "Training of neural machine translation (NMT) models usually uses mini-batches for efficiency purposes.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.8567564984162649}]}, {"text": "During the mini-batched training process, it is necessary to pad shorter sentences in a mini-batch to be equal in length to the longest sentence therein for efficient computation.", "labels": [], "entities": []}, {"text": "Previous work has noted that sorting the corpus based on the sentence length before making mini-batches reduces the amount of padding and increases the processing speed.", "labels": [], "entities": []}, {"text": "However, despite the fact that mini-batch creation is an essential step in NMT training, widely used NMT toolkits implement disparate strategies for doing so, which have not been empirically validated or compared.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.9113501310348511}]}, {"text": "This work investigates mini-batch creation strategies with experiments over two different datasets.", "labels": [], "entities": []}, {"text": "Our results suggest that the choice of a mini-batch creation strategy has a large effect on NMT training and some length-based sorting strategies do not always work well compared with simple shuffling.", "labels": [], "entities": [{"text": "NMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9583540558815002}]}], "introductionContent": [{"text": "Mini-batch training is a standard practice in largescale machine learning.", "labels": [], "entities": []}, {"text": "In recent implementations of neural networks, the efficiency of loss and gradient calculation is greatly improved by minibatching due to the fact that combining training examples into batches allows for fewer but larger operations that can take advantage of the parallelism allowed by modern computation architectures, particularly GPUs.", "labels": [], "entities": []}, {"text": "* This work is done while the author was at Nara Institute of In some cases, such as the case of processing images, mini-batching is straightforward, as the inputs in all training examples take the same form.", "labels": [], "entities": [{"text": "Nara Institute", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9283489286899567}]}, {"text": "However, in order to perform mini-batching in the training of neural machine translation (NMT) or other sequence-to-sequence models, we need to pad shorter sentences to be the same length as the longest sentences to account for sentences of variable length in each mini-batch.", "labels": [], "entities": [{"text": "training of neural machine translation (NMT)", "start_pos": 50, "end_pos": 94, "type": "TASK", "confidence": 0.7979112342000008}]}, {"text": "To help prevent wasted calculation due to this padding, it is common to sort the corpus according to the sentence length before creating minibatches (, because putting sentences that have similar lengths in the same mini-batch will reduce the amount of padding and increase the per-word computation speed.", "labels": [], "entities": []}, {"text": "However, we can also easily imagine that this grouping of sentences together may affect the convergence speed and stability, and the performance of the learned models.", "labels": [], "entities": [{"text": "stability", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9447081685066223}]}, {"text": "Despite this fact, no previous work has explicitly examined how mini-batch creation affects the learning of NMT models.", "labels": [], "entities": []}, {"text": "Various NMT toolkits include implementations of different strategies, but they have neither been empirically validated nor compared.", "labels": [], "entities": []}, {"text": "In this work, we attempt to fill this gap by surveying the various mini-batch creation strategies that are in use: sorting by length of the source sentence, target sentence, or both, as well as making mini-batches according to the number of sentences and the number of words.", "labels": [], "entities": []}, {"text": "We empirically compare their efficacy on two translation tasks and find that some strategies in wide use are not necessarily optimal for reliably training models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted NMT experiments with the strategies presented above to examine their effects on NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 93, "end_pos": 105, "type": "TASK", "confidence": 0.9233689606189728}]}, {"text": "In this section, we used lamtram 8 as a NMT toolkit.", "labels": [], "entities": []}, {"text": "We carried out the Japanese-English translation experiments with ASPEC-JE corpus.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.6583865880966187}, {"text": "ASPEC-JE corpus", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9036909341812134}]}, {"text": "We used Adam () (\u03b1 = 0.001) as the learning algorithm and tried the two sorting algorithms: SHUFFLE which is the best sorting method on previous experiments and TRG SRC which is the default sorting method used by the lamtram toolkit.", "labels": [], "entities": [{"text": "SHUFFLE", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.8973511457443237}, {"text": "TRG SRC", "start_pos": 161, "end_pos": 168, "type": "TASK", "confidence": 0.5100264102220535}]}, {"text": "Normally, lamtram creates minibatches based on the number of target words contained in each mini-batch, but we changed it to fix the mini-batch size to 64 sentences because we find that larger mini-batch size seems to be better in the previous experiments.", "labels": [], "entities": []}, {"text": "Other experimental settings are the same as described in the Section 4.1.", "labels": [], "entities": []}, {"text": "shows the transition of negative log likelihoods using lamtram.", "labels": [], "entities": []}, {"text": "We can seethe tendency of the training curves are similar to the (a), the combination with SHUFFLE drops negative log likelihood faster than the TRG SRC one.", "labels": [], "entities": [{"text": "SHUFFLE", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9758498072624207}, {"text": "negative log likelihood", "start_pos": 105, "end_pos": 128, "type": "METRIC", "confidence": 0.5730548004309336}, {"text": "TRG SRC", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.6465011984109879}]}, {"text": "In the previous experiments, we conducted the experiments with only one NMT toolkit, so the results maybe dependent on the particular implementation provided therein.", "labels": [], "entities": []}, {"text": "To ensure that these results generalize to other toolkits with different default parameters, we conducted the experiments with another NMT toolkit.", "labels": [], "entities": []}, {"text": "From this experiments, we could verify that our experimental results in the Section 4 do not rely on the toolkit and we think the observed behavior will generalize to other toolkits and implementations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of sentences in the corpus", "labels": [], "entities": []}]}