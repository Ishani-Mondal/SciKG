{"title": [{"text": "Ensemble and Reranking: Using Multiple Models in the NICT-2 Neural Machine Translation System at WAT2017", "labels": [], "entities": [{"text": "NICT-2 Neural Machine Translation", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.8401744514703751}, {"text": "WAT2017", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.31557539105415344}]}], "abstractContent": [{"text": "In this paper, we describe the NICT-2 neu-ral machine translation system evaluated at WAT2017.", "labels": [], "entities": [{"text": "NICT-2 neu-ral machine translation", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.6689688563346863}, {"text": "WAT2017", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.8411739468574524}]}, {"text": "This system uses multiple models as an ensemble and combines models with opposite decoding directions by reranking (called bi-directional rerank-ing).", "labels": [], "entities": []}, {"text": "In our experimental results on small data sets, the translation quality improved when the number of models was increased to 32 in total and did not saturate.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9435030221939087}]}, {"text": "In the experiments on large data sets, improvements of 1.59-3.32 BLEU points were achieved when six-model ensembles were combined by the bi-directional reranking.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9947734475135803}]}], "introductionContent": [{"text": "This paper presents the NICT-2 machine translation system evaluated at WAT2017 (.", "labels": [], "entities": [{"text": "NICT-2 machine translation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6212381521860758}, {"text": "WAT2017", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.7904362082481384}]}, {"text": "This system is a basic encoderdecoder with an attention mechanism.", "labels": [], "entities": []}, {"text": "This methodology is known to achieve high translation quality, even when using a single model.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9672557711601257}]}, {"text": "It is also known that better quality can be achieved by utilizing multiple models.", "labels": [], "entities": []}, {"text": "In this paper, we use as many models as possible and attempt to improve the translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9617014527320862}]}, {"text": "There are two major approaches that use multiple models: ensemble and reranking (e.g.,).", "labels": [], "entities": []}, {"text": "The ensemble approach independently encodes and decodes input sentences by multiple models and averages the word distributions output from the decoder (c.f., Sec. 2.1).", "labels": [], "entities": []}, {"text": "The reranking approach first creates an n-best list of translations using a model A, rescores it using another model B, and selects the highest scoring translation (c.f., Sec. 2.2).", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform Japanese-English translation experiments using small data (with approximately 200k sentences) to clarify characteristics of the ensemble and the bi-directional reranking approaches.", "labels": [], "entities": []}, {"text": "Corpora: The corpora used here are the ASPEC data sets listed in.", "labels": [], "entities": [{"text": "ASPEC data sets", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.829079290231069}]}, {"text": "From these training sets, we acquired the byte-pair encoding rules, which generate approximately 50k sub-word types per language, and used sentences in which the number of sub-words is equal to or less than 80.", "labels": [], "entities": []}, {"text": "Translation System: The other settings such as the translation system, preprocessing, and postprocessing are the same as those in Section 3.", "labels": [], "entities": []}, {"text": "shows a summary of the settings.", "labels": [], "entities": []}, {"text": "In this section, we show the results of Ja-En, EnJa, Ja-Zh, and Zh-Ja translation of the ASPEC task.", "labels": [], "entities": [{"text": "ASPEC task", "start_pos": 89, "end_pos": 99, "type": "TASK", "confidence": 0.8850347399711609}]}], "tableCaptions": [{"text": " Table 4: WAT2017 Official Scores (Ja-En Pair of ASPEC).  Note: The Japanese scores are based on the JUMAN segmenter.", "labels": [], "entities": [{"text": "WAT2017", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.5929522514343262}, {"text": "Official Scores", "start_pos": 18, "end_pos": 33, "type": "METRIC", "confidence": 0.43111760914325714}, {"text": "Ja-En Pair of ASPEC)", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.7402865946292877}, {"text": "JUMAN segmenter", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.8780609369277954}]}, {"text": " Table 5: WAT2017 Official Scores (Ja-Zh Pair of ASPEC).  Note: The Japanese and Chinese scores are based on the JUMAN and Stanford (CTB Model)  segmenters, respectively.", "labels": [], "entities": [{"text": "WAT2017 Official Scores", "start_pos": 10, "end_pos": 33, "type": "DATASET", "confidence": 0.5539542138576508}, {"text": "Ja-Zh Pair of ASPEC)", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.7615454435348511}, {"text": "JUMAN and Stanford (CTB Model)  segmenters", "start_pos": 113, "end_pos": 155, "type": "DATASET", "confidence": 0.7855060212314129}]}]}