{"title": [{"text": "Beam Search Strategies for Neural Machine Translation", "labels": [], "entities": [{"text": "Beam Search Strategies", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8356008728345236}, {"text": "Neural Machine Translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.7711617747942606}]}], "abstractContent": [{"text": "The basic concept in Neural Machine Translation (NMT) is to train a large Neu-ral Network that maximizes the translation performance on a given parallel corpus.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.7779846886793772}]}, {"text": "NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7910258173942566}]}, {"text": "The current beam search strategy generates the target sentence word byword from left-to-right while keeping a fixed amount of active candidates at each time step.", "labels": [], "entities": []}, {"text": "First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best.", "labels": [], "entities": []}, {"text": "Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one.", "labels": [], "entities": []}, {"text": "The latter one can be avoided by increasing the beam size until no performance improvement can be observed.", "labels": [], "entities": []}, {"text": "While you can reach better performance, this has the drawback of a slower decoding speed.", "labels": [], "entities": []}, {"text": "In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores.", "labels": [], "entities": []}, {"text": "We speedup the original decoder by up to 43% for the two language pairs German\u2192English and Chinese\u2192English without losing any translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to the fact that Neural Machine Translation (NMT) is reaching comparable or even better performance compared to the traditional statistical machine translation (SMT) models, it has become very popular in the recent years).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.8079074621200562}, {"text": "statistical machine translation (SMT)", "start_pos": 132, "end_pos": 169, "type": "TASK", "confidence": 0.8264361321926117}]}, {"text": "With the recent success of NMT, attention has shifted towards making it more practical.", "labels": [], "entities": [{"text": "NMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.7540926933288574}]}, {"text": "One of the challenges is the search strategy for extracting the best translation fora given source sentence.", "labels": [], "entities": []}, {"text": "In NMT, new sentences are translated by a simple beam search decoder that finds a translation that approximately maximizes the conditional probability of a trained NMT model.", "labels": [], "entities": []}, {"text": "The beam search strategy generates the translation word byword from left-to-right while keeping a fixed number (beam) of active candidates at each time step.", "labels": [], "entities": [{"text": "beam search", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8274393081665039}]}, {"text": "By increasing the beam size, the translation performance can increase at the expense of significantly reducing the decoder speed.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9499046802520752}]}, {"text": "Typically, there is a saturation point at which the translation quality does not improve anymore by further increasing the beam.", "labels": [], "entities": []}, {"text": "The motivation of this work is two folded.", "labels": [], "entities": []}, {"text": "First, we prune the search graph, thus, speedup the decoding process without losing any translation quality.", "labels": [], "entities": []}, {"text": "Secondly, we observed that the best scoring candidates often share the same history and often come from the same partial hypothesis.", "labels": [], "entities": []}, {"text": "We limit the amount of candidates coming from the same partial hypothesis to introduce more diversity without reducing the decoding speed by just using a higher beam.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the German\u2192English translation task, we train an NMT system based on the WMT 2016 training data (Bojar et al., 2016) (3.9M parallel sentences).", "labels": [], "entities": [{"text": "German\u2192English translation task", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6278712034225464}, {"text": "WMT 2016 training data", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.9351798743009567}]}, {"text": "For the Chinese\u2192English experiments, we use an NMT system trained on 11 million sentences from the BOLT project.", "labels": [], "entities": [{"text": "BOLT project", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8069113790988922}]}, {"text": "In all our experiments, we use our in-house attention-based NMT implementation which is similar to (.", "labels": [], "entities": []}, {"text": "For German\u2192English, we use sub-word units extracted by byte pair encoding () instead of words which shrinks the vocabulary to 40k sub-word symbols for both source and target.", "labels": [], "entities": []}, {"text": "For Chinese\u2192English, we limit our vocabularies to be the top 300K most frequent words for both source and target language.", "labels": [], "entities": []}, {"text": "Words not in these vocabularies are converted into an unknown token.", "labels": [], "entities": []}, {"text": "During translation, we use the alignments (from the attention mechanism) to replace the unknown tokens either with potential targets (obtained from an IBM Model-1 trained on the parallel data) or with the source word itself (if no target was found) (.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9720451831817627}]}, {"text": "We use an embedding dimension of 620 and fix the RNN GRU layers to be of 1000 cells each.", "labels": [], "entities": []}, {"text": "For the training procedure, we use SGD to update model  parameters with a mini-batch size of 64.", "labels": [], "entities": []}, {"text": "The training data is shuffled after each epoch.", "labels": [], "entities": []}, {"text": "We measure the decoding speed by two numbers.", "labels": [], "entities": []}, {"text": "First, we compare the actual speed relative to the same setup without any pruning.", "labels": [], "entities": [{"text": "speed", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9799630045890808}]}, {"text": "Secondly, we measure the average fan out per time step.", "labels": [], "entities": []}, {"text": "For each time step, the fan out is defined as the number of candidates we expand.", "labels": [], "entities": []}, {"text": "Fan out has an upper bound of the size of the beam, but can be decreased either due to early stopping (we reduce the beam every time we predict a end-of-sentence symbol) or by the proposed pruning schemes.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.9278774559497833}]}, {"text": "For each pruning technique, we run the experiments with different pruning thresholds and chose the largest threshold that did not degrade the translation performance based on a selection set.", "labels": [], "entities": []}, {"text": "In, you can seethe German\u2192English translation performance and the average fan out per sentence for different beam sizes.", "labels": [], "entities": [{"text": "fan out", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9498663544654846}]}, {"text": "Based on this experiment, we decided to run our pruning experiments for beam size 5 and 14.", "labels": [], "entities": []}, {"text": "The German\u2192English results can be found in.", "labels": [], "entities": []}, {"text": "By using the combination of all pruning techniques, we can speedup the decoding process by 13% for beam size 5 and by 43% for beam size 14 without any drop in performance.", "labels": [], "entities": []}, {"text": "The relative pruning technique is the best working one for beam size 5 whereas the absolute pruning technique works best fora beam size 14.", "labels": [], "entities": []}, {"text": "In the decoding speed with different relative pruning threshold for beam size 5 are illustrated.", "labels": [], "entities": []}, {"text": "Setting the threshold higher than 0.6 hurts the translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.966813862323761}]}, {"text": "A nice side effect is that it has become possible to decode without any fix beam size when we apply pruning.", "labels": [], "entities": []}, {"text": "Nevertheless, the decoding speed drops while the translation performance did not change.", "labels": [], "entities": [{"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9551050066947937}]}, {"text": "Further, we looked at the number of search errors introduced by our pruning schemes (number of times we prune the best scoring hypothesis).", "labels": [], "entities": []}, {"text": "5% of the sentences change due to search errors for beam size 5 and 9% of the sentences change for beam size 14 when using all four pruning techniques together.", "labels": [], "entities": []}, {"text": "The Chinese\u2192English translation results can be found in.", "labels": [], "entities": []}, {"text": "We can speedup the decoding process by 10% for beam size 5 and by 24% for beam size 14 without loss in translation quality.", "labels": [], "entities": []}, {"text": "In addition, we measured the number of search errors introduced by pruning the search.", "labels": [], "entities": []}, {"text": "Only 4% of the sentences change for beam size 5, whereas 22% of the sentences change for beam size 14.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results German\u2192English: relative pruning(rp), absolute pruning(ap), relative local pruning(rpl)  and maximum candidates per node(mc). Average fan out is the average number of candidates we keep at  each time step during decoding.", "labels": [], "entities": [{"text": "absolute pruning(ap)", "start_pos": 56, "end_pos": 76, "type": "METRIC", "confidence": 0.8147406935691833}, {"text": "relative local pruning(rpl)", "start_pos": 78, "end_pos": 105, "type": "METRIC", "confidence": 0.7264437824487686}, {"text": "Average fan out", "start_pos": 144, "end_pos": 159, "type": "METRIC", "confidence": 0.8700739741325378}]}]}