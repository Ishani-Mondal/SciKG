{"title": [{"text": "Exploring Cross-Lingual Transfer of Morphological Knowledge In Sequence-to-Sequence Models", "labels": [], "entities": [{"text": "Exploring Cross-Lingual Transfer of Morphological Knowledge", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7489047000805537}]}], "abstractContent": [{"text": "Multi-task training is an effective method to mitigate the data sparsity problem.", "labels": [], "entities": []}, {"text": "It has recently been applied for cross-lingual transfer learning for paradigm completion-the task of producing inflected forms of lemmata-with sequence-to-sequence networks.", "labels": [], "entities": [{"text": "cross-lingual transfer learning", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.8474824825922648}, {"text": "paradigm completion-the task of producing inflected forms of lemmata-with sequence-to-sequence networks", "start_pos": 69, "end_pos": 172, "type": "TASK", "confidence": 0.8097529248757795}]}, {"text": "However, it is still vague how the model transfers knowledge across languages, as well as if and which information is shared.", "labels": [], "entities": []}, {"text": "To investigate this, we propose a set of data-dependent experiments using an existing encoder-decoder recurrent neural network for the task.", "labels": [], "entities": []}, {"text": "Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about language and morphology can be transferred.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural sequence-to-sequence models define the state of the art for paradigm completion, the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in using one of the non-empty fields.", "labels": [], "entities": [{"text": "paradigm completion", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7295954823493958}]}, {"text": "However, those models are in general very datahungry, and do not reach good performances in low-resource settings.", "labels": [], "entities": []}, {"text": "Therefore, propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language).", "labels": [], "entities": [{"text": "paradigm completion", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.6840046048164368}]}, {"text": "This is achieved by a form of multi-task learning -they train an encoder-decoder model simultaneously on training examples for both languages.", "labels": [], "entities": []}, {"text": "While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still  remain largely obscure.", "labels": [], "entities": []}, {"text": "Several possibilities exist: (i) learning of target tag specific word transformations from the high-resource language (trans); (ii) training of the character language model of the decoder (LM); (iii) learning a bias to copy a large part of the input (copy), since members of the same paradigm mostly share the same stem; (iv) a general regularization effect obtained by multitask training (reg).", "labels": [], "entities": [{"text": "learning of target tag specific word transformations", "start_pos": 33, "end_pos": 85, "type": "TASK", "confidence": 0.6379539157663073}]}, {"text": "In this work, we intend to shed light on the way cross-lingual transfer learning for paradigm completion with an encoder-decoder model works, and will especially focus on the role of the character and tag embeddings.", "labels": [], "entities": [{"text": "cross-lingual transfer learning", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7698214848836263}, {"text": "paradigm completion", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7287134677171707}]}, {"text": "In particular we aim at answering the following questions: (i) What does the neural model learn from the tags of a highresource language for the tags of a low-resource language?", "labels": [], "entities": []}, {"text": "(ii) Is sharing an alphabet important for the transfer?", "labels": [], "entities": []}, {"text": "(iii) How much of the transfer learning can be reduced to a regularization effect achieved by multi-task learning?", "labels": [], "entities": []}, {"text": "For our analysis, we present a set of detailed experiments for the target language Spanish.", "labels": [], "entities": []}, {"text": "Source languages are either members of the Romance language family, French, Italian, Portuguese) of different levels of similarity to Spanish, cf., or an unrelated language (Arabic).", "labels": [], "entities": []}, {"text": "We show which parts of the information are learned from the characters or tags and discuss where sequences of letters or tags from a second language contribute to or restrain performance on the paradigm comple-", "labels": [], "entities": []}], "datasetContent": [{"text": "Letter cipher (l-ciph).", "labels": [], "entities": []}, {"text": "Let C = C low \u222a C high be the union of the sets of all characters in the alphabets of the low-resource language and the high-resource language, respectively.", "labels": [], "entities": []}, {"text": "We define a bijective cipher function f ciph : C \u2192 C, mapping each character to a different character, chosen at random.", "labels": [], "entities": []}, {"text": "Then, we apply this function to the elements of the input and output words in the high-resource language and train the model on this modified data.", "labels": [], "entities": []}, {"text": "The low-resource samples in train, dev and test remain unchanged.", "labels": [], "entities": []}, {"text": "We expect this to have the following effects: (i) languages do not share affixes anymore; (ii) as we use the same embeddings for the changed and unchanged characters, the model might learn wrong affixes for tags; (iii) an incorrect character language model could be learned; and (iv) a general bias to copy should remain unchanged.", "labels": [], "entities": []}, {"text": "Tag cipher (t-ciph).", "labels": [], "entities": []}, {"text": "We further consider the union of the sets of all morphological tags existing in the low-and high-resource languages: T = T low \u222a T high . We define a bijective cipher function f ciph : T \u2192 T . We then apply this function to all tags in the high-resource language input and train anew model.", "labels": [], "entities": []}, {"text": "The low-resource examples in train, dev and test are not changed.", "labels": [], "entities": []}, {"text": "We expect this to: (i) disturb the learning of correspondences between target tags and output characters; (ii) not influence anything else.", "labels": [], "entities": []}, {"text": "Language-dependent letter embeddings (lemb).", "labels": [], "entities": []}, {"text": "We now use different embeddings for the characters of the two languages.", "labels": [], "entities": []}, {"text": "This corresponds to a setting where the source and target languages do not share the same vocabulary.", "labels": [], "entities": []}, {"text": "This should result in: (i) making it impossible for the model to learn which affixes have to be produced for which tag, maybe resulting in benefits for more distant and worse performance for extremely close languages; and (ii) transfer of the decoder's character language model getting impossible.", "labels": [], "entities": []}, {"text": "Language-dependent tag embedding (t-emb).", "labels": [], "entities": []}, {"text": "Additionally, we also experiment with different embeddings for the morphological tags in different languages.", "labels": [], "entities": []}, {"text": "We expect the following to happen: (i) the model can learn a character language model in the output, which might be good for related and bad for more distant languages; (ii) it should not be possible for the model to learn a correspondence between tags and characters in the output sequence; and (iii) the model cannot get information about tags in the low-resource language from the high-resource language's examples.", "labels": [], "entities": []}, {"text": "We additionally perform two last experiments: Language-dependent letter embeddings with separation symbol (l-emb-sep).", "labels": [], "entities": []}, {"text": "This is the same as l-emb, but we introduce anew separation symbol SEP between the tags and the characters, solving the problem that it is not clear where the tag ends and the word starts.", "labels": [], "entities": [{"text": "SEP", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.5263032913208008}]}, {"text": "We expect equal or better performance than for l-emb.", "labels": [], "entities": []}, {"text": "Language-dependent tag embedding with separation symbol (t-emb-sep).", "labels": [], "entities": []}, {"text": "This is equivalent to t-emb, but we again insert anew separation symbol SEP between the tags and the input word's characters.", "labels": [], "entities": [{"text": "SEP", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.8512712121009827}]}, {"text": "We expect equal or better performance than for t-emb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results for all experiments and all high-resource source languages. ES denotes experiments  without transfer. 50 and 200 are the numbers of low-resource training examples. All results are averaged  over 5 training runs, standard deviation in parenthesis.", "labels": [], "entities": [{"text": "ES", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9867637753486633}]}]}