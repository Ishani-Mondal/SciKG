{"title": [], "abstractContent": [{"text": "This paper describes the joint submission of the QT21 projects for the English\u2192Latvian translation task of the EMNLP 2017 Second Conference on Machine Translation (WMT 2017).", "labels": [], "entities": [{"text": "English\u2192Latvian translation task of the EMNLP 2017 Second Conference on Machine Translation (WMT 2017)", "start_pos": 71, "end_pos": 173, "type": "TASK", "confidence": 0.800913370317883}]}, {"text": "The submission is a system combination which combines seven different statistical machine translation systems provided by the different groups.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6213585038979849}]}, {"text": "The systems are combined using either RWTH's system combination approach, or USFD's consensus-based system-selection approach.", "labels": [], "entities": [{"text": "RWTH", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.908943235874176}, {"text": "USFD", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9562440514564514}]}, {"text": "The final submission shows an improvement of 0.5 BLEU compared to the best single system on newstest2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9996820688247681}]}], "introductionContent": [{"text": "Quality Translation 21 (QT21) is a European machine translation research project with the aim of substantially improving statistical and machine learning based translation models for challenging languages and low-resource scenarios.", "labels": [], "entities": [{"text": "Quality Translation 21 (QT21)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7438796559969584}, {"text": "machine translation research", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.7780014077822367}]}, {"text": "Members of the QT21 project have jointly built a combined statistical machine translation system, in order to achieve high-quality machine translation from English into Latvian.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.613996148109436}, {"text": "machine translation from English into Latvian", "start_pos": 131, "end_pos": 176, "type": "TASK", "confidence": 0.8002866009871165}]}, {"text": "Core components of the QT21 combined system for the WMT 2017 shared task for machine translation of news 1 are seven individual English\u2192Latvian translation engines which have been setup by different project partners.", "labels": [], "entities": [{"text": "WMT 2017 shared task", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.5443417355418205}, {"text": "machine translation of news 1", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.8539771676063538}]}, {"text": "The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH's open source statistical machine translation toolkit ().", "labels": [], "entities": [{"text": "Jane, RWTH", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.8436743418375651}, {"text": "statistical machine translation", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.7715383966763815}]}, {"text": "The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs.", "labels": [], "entities": []}, {"text": "As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since only one development set was provided we split the given development set into two parts: newsdev2017/1 and newsdev2017/2.", "labels": [], "entities": []}, {"text": "The first part was used as development set while the second part was our internal test set.", "labels": [], "entities": []}, {"text": "The single systems and the system combintaion are optimized for the newsdev2017/1 set.", "labels": [], "entities": [{"text": "newsdev2017/1 set", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9514501839876175}]}, {"text": "The single system scores in show that the KIT system is the strongest single system closely followed by the UEDIN NMT system.", "labels": [], "entities": [{"text": "KIT system", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.700946182012558}, {"text": "UEDIN NMT system", "start_pos": 108, "end_pos": 124, "type": "DATASET", "confidence": 0.9181061585744222}]}, {"text": "The rescoreing of the UEDIN NMT nbest lists by KIT showed only a small improvement on newstest2017.", "labels": [], "entities": [{"text": "UEDIN NMT nbest lists", "start_pos": 22, "end_pos": 43, "type": "DATASET", "confidence": 0.9104513227939606}, {"text": "KIT", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.6390097737312317}]}, {"text": "The system combination of all these systems showed an improvement of 1.1 BLEU on newsdev2017/2 and 0.5 BLEU on official test set, newstest2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9990046620368958}, {"text": "newsdev2017/2", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9369394779205322}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9985791444778442}, {"text": "official test set", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.7213354309399923}, {"text": "newstest2017", "start_pos": 130, "end_pos": 142, "type": "DATASET", "confidence": 0.609546959400177}]}, {"text": "shows a comparison between all systems by scoring the translation output against each other in TER and BLEU.", "labels": [], "entities": [{"text": "TER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9950241446495056}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9967371821403503}]}, {"text": "We see that the outputs of the two best performing systems KIT and UEDIN are very close.", "labels": [], "entities": [{"text": "KIT", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.6931188106536865}, {"text": "UEDIN", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.5330512523651123}]}, {"text": "In order to get some insight regarding the quality of the morphological correctness of the outputs produced by the systems involved in the combina-: Comparison of system outputs against each other, generated by computing BLEU and TER on the system translations for newstest2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9976609945297241}, {"text": "TER", "start_pos": 230, "end_pos": 233, "type": "METRIC", "confidence": 0.9825257062911987}]}, {"text": "One system in a pair is used as the reference, the other as candidate translation; we report the average over both directions.", "labels": [], "entities": []}, {"text": "The USFD system is similar to the \"Consensus-based System-selection Beer\" in: Sentence group evaluation with Entropy (C-set).", "labels": [], "entities": [{"text": "USFD", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8473473787307739}, {"text": "Sentence group evaluation", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.863864024480184}]}, {"text": "tion, we ran the evaluation method introduced in).", "labels": [], "entities": []}, {"text": "The evaluation of the morphological competence of a machine translation system is performed on an automatically produced test suite.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7132741957902908}]}, {"text": "For each source test sentence from a monolingual corpus (the base), one (or several) variant(s) are generated, containing exactly one difference with the base, focusing on a specific target lexeme of the base.", "labels": [], "entities": []}, {"text": "These variants differ on a feature that is expressed morphologically in the target, such as the person, number or tense of a verb; or the number or case of a noun or an adjective.", "labels": [], "entities": []}, {"text": "This artificial test set is then translated with a machine translation system.", "labels": [], "entities": []}, {"text": "The machine translation system is deemed correct if the translations of the base and variant differ in the same way as their respective source.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7005095183849335}]}, {"text": "Another setup focuses on a word in the base sentence and produces variants containing antonyms and synonyms of this word.", "labels": [], "entities": []}, {"text": "The expected translation is then synonyms and antonyms bearing the same morphological features as the initial word.", "labels": [], "entities": []}, {"text": "There are three types of contrasts implying different sorts of evaluation: \u2022 A: We check whether the morphological feature inserted in the source sentence has been translated (eg. plural number of a noun).", "labels": [], "entities": [{"text": "A", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9621504545211792}]}, {"text": "Accuracy for all morphological features is averaged overall sentences.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9873803853988647}]}, {"text": "\u2022 B: We focus on various agreement phenomena by checking whether a given morphological feature is present in both words that need to agree (eg. case of two nouns).", "labels": [], "entities": [{"text": "B", "start_pos": 2, "end_pos": 3, "type": "METRIC", "confidence": 0.9544815421104431}]}, {"text": "Accuracy is computed here as well.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9952632188796997}]}, {"text": "\u2022 C: We test the consistency of morphological choices over lexical variation (eg. synonyms and antonyms all having the same tense) and measure the success based on the average normalized entropy of morphological features in the set of target sentences.", "labels": [], "entities": []}, {"text": "The results for the A-set are shown in and reflect the adequacy of an output towards the source, or the quantity of morphological information that has been well conveyed from the source.", "labels": [], "entities": []}, {"text": "Certain morphological features indicate rather low contrasts between statistical and neural systems (verb tense and pronoun gender), which shows the relevance of SMT systems in the combination.", "labels": [], "entities": [{"text": "SMT", "start_pos": 162, "end_pos": 165, "type": "TASK", "confidence": 0.9862457513809204}]}, {"text": "Sets B and C are more forcused on target monolingual phenomena, such as agreement, and assess the level of fluency of a system output.", "labels": [], "entities": []}, {"text": "Here, the observed contrasts between statistical and neural systems are far more obvious: all B-set SMT scores are below 50%, whereas NMT scores are always above.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9304488897323608}]}, {"text": "Here again, the superior performance of KIT is noticed, at least for sets A and B.", "labels": [], "entities": [{"text": "KIT", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.571273922920227}]}, {"text": "As for the C-set, LIMSI factored, KIT and UEDIN show a comparable high confidence in their morphology predictions across lexical variety.", "labels": [], "entities": [{"text": "LIMSI", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9416872262954712}, {"text": "KIT", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.5869204998016357}, {"text": "UEDIN", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.8681958317756653}]}], "tableCaptions": [{"text": " Table 1: Results of the individual systems for the English\u2192Latvian task. BLEU [%] and TER [%] scores  are case-sensitive.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9996261596679688}, {"text": "TER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9960488677024841}]}, {"text": " Table 2: USFD rescoring and combination experiments English\u2192Latvian task. BLEU [%] and TER [%]  scores are case-sensitive.", "labels": [], "entities": [{"text": "USFD rescoring", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.44405728578567505}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9996439218521118}, {"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9953436255455017}]}, {"text": " Table 3: Comparison of system outputs against each other, generated by computing BLEU and TER  on the system translations for newstest2017. One system in a pair is used as the reference, the other as  candidate translation; we report the average over both directions. The USFD system is similar to the  \"Consensus-based System-selection Beer\" in", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9990401864051819}, {"text": "TER", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9884917736053467}, {"text": "USFD", "start_pos": 273, "end_pos": 277, "type": "DATASET", "confidence": 0.9010965824127197}]}, {"text": " Table 2. The upper-right half lists BLEU [%] scores, the  lower-left half TER [%] scores.", "labels": [], "entities": [{"text": "BLEU [%] scores", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.9332701762517294}, {"text": "TER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9983963370323181}]}, {"text": " Table 4: Sentence pair evaluation (A-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.922028124332428}]}, {"text": " Table 5: Sentence pair evaluation (B-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9335419535636902}]}, {"text": " Table 6: Sentence group evaluation with Entropy (C-set).", "labels": [], "entities": [{"text": "Sentence group evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.9094459811846415}]}]}