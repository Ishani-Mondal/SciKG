{"title": [{"text": "Connecting the Dots: Towards Human-Level Grammatical Error Correction", "labels": [], "entities": [{"text": "Human-Level Grammatical Error Correction", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.6530278325080872}]}], "abstractContent": [{"text": "We build a grammatical error correction (GEC) system primarily based on the state-of-the-art statistical machine translation (SMT) approach, using task-specific features and tuning, and further enhance it with the modeling power of neural network joint models.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.7635180354118347}, {"text": "statistical machine translation (SMT)", "start_pos": 93, "end_pos": 130, "type": "TASK", "confidence": 0.8346974154313406}]}, {"text": "The SMT-based system is weak in generalizing beyond patterns seen during training and lacks gran-ularity below the word level.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9882259964942932}]}, {"text": "To address this issue, we incorporate a character-level SMT component targeting the misspelled words that the original SMT-based system fails to correct.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9258010983467102}, {"text": "SMT-based", "start_pos": 119, "end_pos": 128, "type": "TASK", "confidence": 0.9638886451721191}]}, {"text": "Our final system achieves 53.14% F 0.5 score on the benchmark CoNLL-2014 test set, an improvement of 3.62% F 0.5 over the best previous published score.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9851728081703186}, {"text": "CoNLL-2014 test set", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.957332968711853}, {"text": "F 0.5", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9736600816249847}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) is the task of correcting various textual errors including spelling, grammar, and collocation errors.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8070242752631506}]}, {"text": "The phrase-based statistical machine translation (SMT) approach is able to achieve state-of-theart performance on GEC.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 4, "end_pos": 54, "type": "TASK", "confidence": 0.7165835584912982}, {"text": "GEC", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.9245501160621643}]}, {"text": "In this approach, error correction is treated as a machine translation task from the language of \"bad English\" to the language of \"good English\".", "labels": [], "entities": [{"text": "error correction", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7165464609861374}]}, {"text": "SMT-based systems do not rely on language-specific tools and hence they can be trained for any language with adequate parallel data (i.e., erroneous and corrected sentence pairs).", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9836222529411316}]}, {"text": "They are also capable of correcting complex errors which are difficult for classifier systems that target specific error types.", "labels": [], "entities": []}, {"text": "The generalization of SMT-based GEC systems has been shown to improve further by adding neural network models.", "labels": [], "entities": [{"text": "SMT-based GEC", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.8511154055595398}]}, {"text": "Though SMT provides a strong framework for GEC, the traditional word-level SMT is weak in generalizing beyond patterns seen in the training data (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9747301936149597}, {"text": "GEC", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.971209704875946}, {"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.6310459971427917}]}, {"text": "This effect is particularly evident for spelling errors, since a large number of misspelled words produced by learners are not observed in the training data.", "labels": [], "entities": []}, {"text": "We propose improving the SMT approach by adding a character-level SMT component to a word-level SMT-based GEC system, with the aim of correcting misspelled words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9945112466812134}, {"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8303959369659424}, {"text": "SMT-based GEC", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.6841942965984344}]}, {"text": "Our word-level SMT-based GEC system utilizes task-specific features described in.", "labels": [], "entities": [{"text": "SMT-based GEC", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.8226847052574158}]}, {"text": "We show in this paper that performance continues to improve further after adding neural network joint models (NNJMs), as introduced in).", "labels": [], "entities": []}, {"text": "NNJMs can leverage the continuous space representation of words and phrases and can capture a larger context from the source sentence, which enables them to make better predictions than traditional language models).", "labels": [], "entities": []}, {"text": "The NNJM is further improved using the regularized adaptive training method described in) on a higher quality training dataset, which has a higher errorper-sentence ratio.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9242448210716248}]}, {"text": "In addition, we add a characterlevel SMT component to generate candidate corrections for misspelled words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8096237778663635}]}, {"text": "These candidate corrections are rescored with n-gram language model features to prune away non-word candidates and select the candidate that best fits the context.", "labels": [], "entities": []}, {"text": "Our final system outperforms the best prior published system when evaluated on the benchmark CoNLL-2014 test set.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 93, "end_pos": 112, "type": "DATASET", "confidence": 0.9651558995246887}]}, {"text": "For better replicability, we release our source code and model files publicly at https://github.com/nusnlp/ smtgec2017.", "labels": [], "entities": [{"text": "replicability", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.9653749465942383}]}], "datasetContent": [{"text": "The parallel data for training our word-level SMT system consist of two corpora: the", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8665493130683899}]}], "tableCaptions": [{"text": " Table 1: Results of incremental addition of fea- tures and components.", "labels": [], "entities": []}, {"text": " Table 2: Comparison on the CoNLL-2014 test set.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9789868593215942}]}, {"text": " Table 3: Results on the JFLEG corpus.", "labels": [], "entities": [{"text": "JFLEG corpus", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9587146043777466}]}, {"text": " Table 4: Performance on spelling error correction.", "labels": [], "entities": [{"text": "spelling error correction", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.6281649072964987}]}]}