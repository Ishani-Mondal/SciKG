{"title": [{"text": "Sentence-level quality estimation by predicting HTER as a multi-component metric", "labels": [], "entities": [{"text": "Sentence-level quality estimation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8596393465995789}, {"text": "predicting HTER", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.6847244501113892}]}], "abstractContent": [{"text": "This submission investigates alternative machine learning models for predicting the HTER score on the sentence level.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.6229849308729172}]}, {"text": "Instead of directly predicting the HTER score, we suggest a model that jointly predicts the amount of the 4 distinct post-editing operations, which are then used to calculate the HTER score.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.6355759054422379}]}, {"text": "This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.7171732485294342}]}, {"text": "Without any feature exploration, a multi-layer perceptron with 4 outputs yields small but significant improvements over the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) is the evaluation method that aims at employing machine learning in order to predict some measure of quality given a Machine Translation (MT) output.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8026914060115814}, {"text": "Machine Translation (MT) output", "start_pos": 141, "end_pos": 172, "type": "TASK", "confidence": 0.8466466168562571}]}, {"text": "A commonly-used subtask of QE refers to the learning of automatic metrics.", "labels": [], "entities": []}, {"text": "These metrics produce a continuous score based on the comparison between the MT output and a reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9016543626785278}]}, {"text": "When the reference is a minimal post-edition of the MT output, the quality score produced is intuitively more objective and robust as compared to other QE subtasks, where the quality score is assigned directly by the annotators.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.8100653886795044}]}, {"text": "In that case, the score is a direct reflection of the changes that need to take place in order to fix the translation.", "labels": [], "entities": []}, {"text": "HTER) is the most commonly used metric as it directly represents the least required post-editing effort.", "labels": [], "entities": [{"text": "HTER)", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7205092310905457}]}, {"text": "In order to predict the results of an automatic metric, QE approaches use machine learning to predict a model that associates a feature vector with the single quality score.", "labels": [], "entities": []}, {"text": "In this case the statistical model treats the automatic metric as a black box, in the sense that no particular knowledge about the exact calculation of the quality score is explicitly included in the model.", "labels": [], "entities": []}, {"text": "In this submission we aim to partially break this black-box.", "labels": [], "entities": []}, {"text": "We explore the idea of creating a QE model that does not directly predict the single HTER score, but it jointly predicts the 4 components of the metric, which are later used for computing the single score.", "labels": [], "entities": []}, {"text": "This way, the structure of the model can be aware of the distinct factors that comprise the final quality score and also potentially learn the interactions between them.", "labels": [], "entities": []}, {"text": "Hence, the focus of this submission will remain on machine learning and there will not be exploration in terms of features.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly introduce previous work, in Section 3 we provide details about the method, whereas the experiment results are given in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 we describe the models submitted at the shared-task and we explain why they differ from our best models.", "labels": [], "entities": []}, {"text": "Finally, in Section 6 we present the conclusions and some ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we present the experiments of testing the various machine learning approaches on the development set.", "labels": [], "entities": []}, {"text": "After the decisions were taken based on the development set, the models were also applied on the test-sets and the respective results are also given.", "labels": [], "entities": []}, {"text": "The performance of the models is measured with Pearson's rho, as this is the basic metric of the WMT17 Shared Task.", "labels": [], "entities": [{"text": "Pearson's rho", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.6564216613769531}, {"text": "WMT17 Shared Task", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.5145657062530518}]}, {"text": "A test on statistical significance for comparisons is performed with bootstrap re-sampling over 1000 samples.", "labels": [], "entities": []}, {"text": "The 4 types of post-editing operations were re-computed with TERCOM on the exact way that the workshop organizers computed the HTER scores.", "labels": [], "entities": [{"text": "TERCOM", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9744762182235718}]}, {"text": "1 Similar to the baseline, features values are standardized by removing the mean and scaled to unit variance.", "labels": [], "entities": []}, {"text": "Since the experiment is focusing on machine learning, for German-English only the baseline features are used.", "labels": [], "entities": []}, {"text": "For English-German, we additionally performed preliminary experiments with the feature-set from Avramidis (2017) including 94 features that improved QE performance for translating into German, generated with the software Qualitative.", "labels": [], "entities": []}, {"text": "The addition of these features did not result into any improvements, so we are not reporting their results during the development phase (see Section 5 for more details).", "labels": [], "entities": []}, {"text": "The code for training quality estimation models was based on the software Quest++ (Specia et al., 2015) and Scikit-learn (Pedregosa et al., 2011) ver.", "labels": [], "entities": []}, {"text": "0.7.25 was downloaded from http: //www.cs.umd.edu/ \u02dc snover/tercom.", "labels": [], "entities": []}, {"text": "The scripts used for running the experiments can be found at https: //github.com/lefterav/MLP4.", "labels": [], "entities": [{"text": "MLP4", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.8072590231895447}]}, {"text": "The development results concerning the presented methods are given below in this section.", "labels": [], "entities": []}, {"text": "The model approaches are tested for both language directions, whereas the experiments on the normalization of the predictions and ML optimization are run only for German-English and these observations are applied to the models for EnglishGerman.", "labels": [], "entities": [{"text": "ML optimization", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.9518774747848511}]}], "tableCaptions": [{"text": " Table 1: Pearson rho correlation against golden  labels concerning the 4 different approaches for  predicting HTER for German-English. (*) indi- cates significant improvement (\u03b1 = 0.05) over the  SVM baseline (**) significant improvement over  all models", "labels": [], "entities": [{"text": "Pearson rho correlation", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.9275508721669515}, {"text": "predicting HTER", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7413191497325897}]}, {"text": " Table 2: Performance of the 4 different approaches  for predicting HTER for English-German (*) indi- cates a significant improvement (\u03b1 = 0.1) over the  baseline", "labels": [], "entities": [{"text": "predicting HTER", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7945986986160278}]}, {"text": " Table 3: Performance improvements by introduc- ing rounding and cut-off for the predicted post- editing operations (German-English)", "labels": [], "entities": [{"text": "introduc- ing rounding", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.699227586388588}]}, {"text": " Table 4: Experimentation with different optimiza- tion measures for defining the perceptron hyperpa- rameters (German-English model)", "labels": [], "entities": []}, {"text": " Table 6. All SVMs  have an RBF kernel and all MLPs are optimized  with adam as a solver. It is noteworthy that for  German-English a network topology with multiple  hidden layers performed better, which is an indica- tion that the mapping between features and labels  in this language pair is much more complex than  the one for German-English.", "labels": [], "entities": []}, {"text": " Table 6: Hyperparameters and network topology  after the optimization of the MLP models", "labels": [], "entities": []}, {"text": " Table 7: Scores for the submitted models and  for their corrected versions after the submission  (German-English)", "labels": [], "entities": []}, {"text": " Table 8: Scores for the submitted models and  for their corrected versions after the submission  English-German", "labels": [], "entities": []}]}