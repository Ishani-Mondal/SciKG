{"title": [{"text": "Fully Delexicalized Contexts for Syntax-Based Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings induced from large amounts of unannotated text area key resource for many NLP tasks.", "labels": [], "entities": []}, {"text": "Several recent studies have proposed extensions of the basic distributional semantics approach where words form the context of other words, adding features from e.g. syntactic dependencies.", "labels": [], "entities": []}, {"text": "In this study, we look in a different direction, exploring models that leave words out entirely, instead basing the context representation exclusively on syntactic and morphological features.", "labels": [], "entities": []}, {"text": "Remarkably, we find that the resulting vectors still capture clear semantic aspects of words in addition to syntactic ones.", "labels": [], "entities": []}, {"text": "We assess the properties of the vectors using both intrinsic and extrinsic evaluations, demonstrating in a multilingual parsing experiment using 55 treebanks that fully delexicalized syntax-based word representations give a higher average parsing performance than conventional word2vec embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent resurgence of interest in neural methods for natural language processing involves a particular focus on neural approaches to inducing representations of words from large text corpora based on distributional semantics approaches ().", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.6711001793543497}]}, {"text": "The methods introduced by and implemented in their popular word2vec tool have been proven both effective and a good foundation for further exploration.", "labels": [], "entities": []}, {"text": "In addition to representing word contexts as sliding windows of words in linear sequence, recent work has included efforts of building the word vectors using dependency-based approaches (, where the context is based on nearby words in the syntactic tree.", "labels": [], "entities": []}, {"text": "In this paper, we set out to study dependencybased contexts further, exploring word embeddings derived from fully delexicalized syntactic contexts, and in particular the degree to which models induced using such context representations are dependent on word forms.", "labels": [], "entities": []}], "datasetContent": [{"text": "We next present the sources of the unannotated texts and their syntactic analyses used as input and the methods and resources applied to create word embeddings and evaluate them.", "labels": [], "entities": []}, {"text": "Our primary evaluation is based on dependency parsing, where we evaluate parsing accuracy using different pre-trained word embeddings during parser training.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8143861293792725}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9088073372840881}]}, {"text": "We use the UDPipe pipeline 4 for tokenizing, tagging, lemmatizing and parsing Universal Treebanks (.", "labels": [], "entities": []}, {"text": "In all experiments, we use system parameters optimized on baseline models separately for each treebank, keeping the parameters fixed in the comparative evaluations of the different word representations.", "labels": [], "entities": []}, {"text": "We note that any possible bias introduced by this parameter selection strategy would favour the baseline model rather than one using the delexicalized syntax-based representations proposed here.", "labels": [], "entities": []}, {"text": "Parsing results are reported for all UD v2.0 treebanks in the CoNLL 2017 Shared Task release 6 that have a separate development set which can be used for testing and raw data for training embeddings.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8966477513313293}, {"text": "UD v2.0 treebanks", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.7513330380121866}, {"text": "CoNLL 2017 Shared Task release 6", "start_pos": 62, "end_pos": 94, "type": "DATASET", "confidence": 0.8934779067834219}]}, {"text": "Of the 64 treebanks in the release, 9 do not fulfill these criteria (French-ParTUT, GalicianTreeGal, Irish, Kazakh, Latin, Slovenian-SST, Ukrainian and Uyghur do not have development data, Gothic does not have raw data) and are not included in the evaluation.", "labels": [], "entities": []}, {"text": "Models are trained on the training section of a treebank and tested on the development section.", "labels": [], "entities": []}, {"text": "The results for the intrinsic evaluation based on the comparison of word pair similarity ranking with human judgments on 13 datasets are summarized in.", "labels": [], "entities": []}, {"text": "The correlations seen for the word2vec embeddings are inline with those for previously released representations generated using the algorithm (e.g.), confirming that the texts used to induce these representations are appropriate for generating highquality word embeddings.", "labels": [], "entities": []}, {"text": "The results for the delexicalized syntax-based embeddings are, as expected, much lower and far from competitive on any of the datasets.", "labels": [], "entities": []}, {"text": "Nevertheless, the correlations remain positive in all 13 evaluations, providing support for the proposition that delexicalized contexts representations can identify similarities in word meaning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Intrinsic evaluation results. The numbers of found pairs are identical for the two methods.", "labels": [], "entities": []}, {"text": " Table 4: Parsing results for Conll 2017 shared task UD treebanks using different pretrained word em- beddings. Green colour identifies treebanks where the performance of delexicalized syntax-based em- beddings is higher than standard word2vec embeddings and the difference to the baseline model is  positive.", "labels": [], "entities": [{"text": "Conll 2017 shared task UD treebanks", "start_pos": 30, "end_pos": 65, "type": "DATASET", "confidence": 0.8623968859513601}]}, {"text": " Table 5: Bootstrapping results for Finnish syntax- based embeddings.", "labels": [], "entities": []}]}