{"title": [{"text": "Detecting mentions of pain and acute confusion in Finnish clinical text", "labels": [], "entities": [{"text": "Detecting mentions of pain and acute confusion", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7742079240935189}]}], "abstractContent": [{"text": "We study and compare two different approaches to the task of automatic assignment of predefined classes to clinical free-text narratives.", "labels": [], "entities": []}, {"text": "In the first approach this is treated as a traditional mention-level named-entity recognition task, while the second approach treats it as a sentence-level multi-label classification task.", "labels": [], "entities": [{"text": "mention-level named-entity recognition task", "start_pos": 55, "end_pos": 98, "type": "TASK", "confidence": 0.6746379658579826}, {"text": "sentence-level multi-label classification task", "start_pos": 141, "end_pos": 187, "type": "TASK", "confidence": 0.690550297498703}]}, {"text": "Performance comparison across these two approaches is conducted in the form of sentence-level evaluation and state-of-the-art methods for both approaches are evaluated.", "labels": [], "entities": []}, {"text": "The experiments are done on two data sets consisting of Finnish clinical text, manually annotated with respect to the topics pain and acute confusion.", "labels": [], "entities": []}, {"text": "Our results suggest that the mention-level named-entity recognition approach outperforms sentence-level classification overall, but the latter approach still manages to achieve the best prediction scores on several annotation classes.", "labels": [], "entities": [{"text": "mention-level named-entity recognition", "start_pos": 29, "end_pos": 67, "type": "TASK", "confidence": 0.6204358637332916}, {"text": "sentence-level classification", "start_pos": 89, "end_pos": 118, "type": "TASK", "confidence": 0.6835611015558243}]}], "introductionContent": [{"text": "In relation to patient care in hospitals, clinicians document the administrated care on a regular basis.", "labels": [], "entities": []}, {"text": "The documented information is stored as clinical notes in electronic health record (EHR) systems.", "labels": [], "entities": []}, {"text": "In many countries and hospital districts, a substantial portion of the information that clinicians document concerning patient status, performed interventions, thoughts, uncertainties and plans are written in a narrative manner using (natural) free text.", "labels": [], "entities": []}, {"text": "This means that much of the patient information is only found in free-text form, as opposed to structured or coded information (c.f.", "labels": [], "entities": []}, {"text": "* These authors contributed equally.", "labels": [], "entities": []}, {"text": "standardized terminology, medications and diagnosis codes).", "labels": [], "entities": []}, {"text": "When it comes to information retrieval, management and secondary use, having the computer automatically identify and extract information from health records related to a given query or topic is desirable.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7398267090320587}]}, {"text": "This could, for example, be information about pain treatment given to a patient, or a patient group.", "labels": [], "entities": []}, {"text": "Although free text is easy to produce by humans and allows for great flexibility and expressibility, it is challenging to have computers automatically classify and extract information from such text.", "labels": [], "entities": []}, {"text": "The use of computers to automatically extract, label and structure information in free text is referred to as information extraction (, with named-entity recognition as a sub-task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7571576535701752}]}, {"text": "Due to the complexity of free text, this task is commonly approached using manually annotated text as training data for machine learning algorithms (see e.g.).", "labels": [], "entities": []}, {"text": "We present an ongoing work towards automated annotation of text, i.e. labelling with pre-defined classes/entity types, by first having the computer learn from a set of manually annotated clinical notes.", "labels": [], "entities": []}, {"text": "The annotations concern two topics relevant to clinical care: Pain and Acute Confusion.", "labels": [], "entities": [{"text": "Acute Confusion", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.8375458121299744}]}, {"text": "To get a better insight into these topics and how this is being documented, two separate data sets have been manually annotated, one for each topic.", "labels": [], "entities": []}, {"text": "For each of the two topics, a set of classes has been initially identified that reflect the information which the domain experts are interested in.", "labels": [], "entities": []}, {"text": "An example sentence demonstrating the annotations is presented in.", "labels": [], "entities": []}, {"text": "The ultimate aim of this annotation work is to achieve improved documentation, assessment, handling and treatment of pain and acute confusion in hospitals.", "labels": [], "entities": [{"text": "assessment, handling and treatment of pain and acute confusion", "start_pos": 79, "end_pos": 141, "type": "TASK", "confidence": 0.5802607506513595}]}, {"text": "Now we want to investigate how to best train the computer to automatically detect and annotate mentions of these topics in new, unseen text by exploring various machine learning methods.", "labels": [], "entities": []}, {"text": "We address this by testing and comparing two different overall approaches: \u2022 Named-entity recognition (NER), where we have the computer attempt to detect the mention-level annotation boundaries.", "labels": [], "entities": [{"text": "Named-entity recognition (NER)", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.78205486536026}]}, {"text": "\u2022 Sentence classification (SC), where we have the computer attempt to label sentences based on the contained annotations.", "labels": [], "entities": [{"text": "Sentence classification (SC)", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.8291128754615784}]}, {"text": "The motivation for comparing these two approaches is that: (a) the experts are satisfied with having the computer identify and extract information on sentence level; and (b) we hypothesize that several classes, in particular those reflecting the more complex concepts, are easier for the computer to identify when approached as a sentence classification task.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 330, "end_pos": 358, "type": "TASK", "confidence": 0.7916248043378195}]}, {"text": "Further, we are not aware of any other work where a similar comparison has been reported.", "labels": [], "entities": []}, {"text": "The methods and algorithms that we explore are based on state-of-the-art machine learning methods for NER and SC.", "labels": [], "entities": []}], "datasetContent": [{"text": "Below (Section 3.1 and 3.2) we describe the methods, algorithm implementations and hyper parameters used in the two approaches, i.e., namedentity recognition (NER) and sentence classification (SC).", "labels": [], "entities": [{"text": "namedentity recognition (NER)", "start_pos": 134, "end_pos": 163, "type": "TASK", "confidence": 0.8059422433376312}, {"text": "sentence classification (SC)", "start_pos": 168, "end_pos": 196, "type": "TASK", "confidence": 0.8393958985805512}]}, {"text": "In the Results section, Section 4, we compare the scores achieved by these two ap- proaches for each of the two topics (i.e. pain and acute confusion).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mention-level evaluation of the tested  NER approaches on the test sets of the Pain and  Acute confusion corpora. The reported numbers  are micro-averaged over the various classes.", "labels": [], "entities": [{"text": "NER", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9562065601348877}]}, {"text": " Table 2: Micro-averaged F-scores for the dif- ferent approaches on the test sets of the pain and  acute confusion data sets. NERsuite was used to  produce the NER scores.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.757560133934021}, {"text": "acute confusion data sets", "start_pos": 99, "end_pos": 124, "type": "DATASET", "confidence": 0.6426818445324898}, {"text": "NERsuite", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.6531899571418762}]}, {"text": " Table 3: Counts showing the number of classes  that the various approaches performed best at pre- dicting.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of SC and NER for sentence classification, for pain corpus test set, evaluated on  micro-averaged F1-scores.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7500210106372833}, {"text": "F1-scores", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9066458344459534}]}]}