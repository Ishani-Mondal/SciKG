{"title": [{"text": "The WebNLG Challenge: Generating Text from RDF Data", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9018481969833374}]}], "abstractContent": [{"text": "The WebNLG challenge consists in mapping sets of RDF triples to text.", "labels": [], "entities": []}, {"text": "It provides a common benchmark on which to train, evaluate and compare \"microplanners\", i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation , aggregation, lexicalisation, surface real-isation and sentence segmentation.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 202, "end_pos": 233, "type": "TASK", "confidence": 0.664125382900238}, {"text": "sentence segmentation", "start_pos": 290, "end_pos": 311, "type": "TASK", "confidence": 0.747704267501831}]}, {"text": "In this paper , we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Previous Natural Language Generation (NLG) challenges have focused on surface realisation (), referring expression generation ( and content selection.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.797512282927831}, {"text": "surface realisation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7341394126415253}, {"text": "referring expression generation", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.8089168270428976}, {"text": "content selection", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7207731455564499}]}, {"text": "In contrast, the WebNLG challenge focuses on microplanning, that subtask of NLG which consists in mapping a given content to a text verbalising this content.", "labels": [], "entities": []}, {"text": "Microplanning is a complex choice problem involving several subtasks referred to in the literature as referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation.", "labels": [], "entities": [{"text": "expression generation", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7326202839612961}, {"text": "sentence segmentation", "start_pos": 188, "end_pos": 209, "type": "TASK", "confidence": 0.7202244400978088}]}, {"text": "For instance, given the WebNLG data unit shown in (1a), generating the text in (1b) involves choosing to lexicalise the JOHN E BLAHA entity only once (referring expression generation), lexicalising the OCCUPATION property as the phrase worked as (lexicalisation), using PP coordination to avoid repeating the word born (aggregation) and verbalising the three triples by a single complex sentence including an apposition, a PP coordination and a transitive verb construction (sentence segmentation and surface realisation).", "labels": [], "entities": [{"text": "WebNLG data unit", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9618407487869263}, {"text": "BLAHA", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.586940348148346}, {"text": "OCCUPATION", "start_pos": 202, "end_pos": 212, "type": "METRIC", "confidence": 0.8591930270195007}]}], "datasetContent": [{"text": "The WebNLG challenge includes both an automatic and a human-based evaluation.", "labels": [], "entities": []}, {"text": "Due to time constraints, only the results of the automatic evaluation are presented in this paper.", "labels": [], "entities": []}, {"text": "The results of the humanbased evaluation will be provided on the WebNLG website 2 in October 2017.", "labels": [], "entities": [{"text": "WebNLG website 2", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9844917058944702}]}, {"text": "Three automatic metrics were used to evaluate the participating systems: \u2022 BLEU-4 3 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9986780285835266}]}, {"text": "BLEU scores were computed using up to three references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9745503664016724}]}, {"text": "\u2022 METEOR (v1.5) 4 (Denkowski and Lavie, 2014); \u2022 TER).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9833322763442993}, {"text": "TER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9681289792060852}]}, {"text": "For statistical significance testing, we followed the bootstrapping algorithm described in ().", "labels": [], "entities": [{"text": "statistical significance testing", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.810960590839386}]}, {"text": "To assess the ability of the participating systems to generalise to out of domain data, the test dataset consists of two sets of roughly equal size: a test set containing inputs created for entities belonging to DBpedia categories that were seen in the training data (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, City, and WrittenWork), and a test set containing inputs extracted for entities belonging to 5 unseen categories (Athlete, Artist, MeanOfTransportation, CelestialBody, Politician).", "labels": [], "entities": []}, {"text": "We call the first type of data seen categories, the second, unseen categories.", "labels": [], "entities": []}, {"text": "Correspondingly, we report results for 3 datasets: the seen category dataset, the unseen category dataset and the total test set made of both the seen and the unseen category datasets.", "labels": [], "entities": []}, {"text": "gives more detailed statistics about the number of properties, objects and subject entities occurring in each test set.", "labels": [], "entities": []}, {"text": "Participants were requested to submit tokenised and lowercased texts.", "labels": [], "entities": []}, {"text": "To ensure consistency between submissions, we pre-processed the submitted results one more time to double-check that those requirements were fullfilled.", "labels": [], "entities": [{"text": "double-check", "start_pos": 99, "end_pos": 111, "type": "METRIC", "confidence": 0.9723513126373291}]}, {"text": "As teams used different strategies of tokenisation, we had to modify submissions using our own scripts.", "labels": [], "entities": []}, {"text": "In particular, all punctuation signs were separated from alphanumeric sequences (e.g. a two-token group 65.6 feet was modified to a four-token 65 . 6 feet).", "labels": [], "entities": []}, {"text": "Moreover, we converted both references and submission outputs to the ASCII character set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Some Statistics about the WebNLG Dataset", "labels": [], "entities": [{"text": "WebNLG Dataset", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9866615235805511}]}, {"text": " Table 3: Test data statistics on properties, objects and subjects", "labels": [], "entities": []}, {"text": " Table 4: Vocabulary size in tokens.", "labels": [], "entities": []}, {"text": " Table 5: Results for all categories. Lines between systems indicate a difference in scores which is statistically significant (p <", "labels": [], "entities": []}, {"text": " Table 6: Results for seen categories.", "labels": [], "entities": []}, {"text": " Table 7: Results for unseen categories.", "labels": [], "entities": []}]}