{"title": [{"text": "An Error-Oriented Approach to Word Embedding Pre-Training", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a novel word embedding pre-training approach that exploits writing errors in learners' scripts.", "labels": [], "entities": []}, {"text": "We compare our method to previous models that tune the embeddings based on script scores and the discrimination between correct and corrupt word contexts in addition to the generic commonly-used embeddings pre-trained on large corpora.", "labels": [], "entities": []}, {"text": "The comparison is achieved by using the afore-mentioned models to bootstrap a neural network that learns to predict a holistic score for scripts.", "labels": [], "entities": []}, {"text": "Furthermore, we investigate augmenting our model with error corrections and monitor the impact on performance.", "labels": [], "entities": []}, {"text": "Our results show that our error-oriented approach outperforms other comparable ones which is further demonstrated when training on more data.", "labels": [], "entities": []}, {"text": "Additionally , extending the model with corrections provides further performance gains when data sparsity is an issue.", "labels": [], "entities": []}], "introductionContent": [{"text": "Assessing students' writing plays an inherent pedagogical role in the overall evaluation of learning outcomes.", "labels": [], "entities": [{"text": "Assessing students' writing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8735094666481018}]}, {"text": "Traditionally, human graders are required to mark essays, which is cost-and timeinefficient, especially with the growing numbers of students.", "labels": [], "entities": []}, {"text": "Moreover, the evaluation process is subjective, which leads to possible variations in the awarded scores when more than one human assessor is employed.", "labels": [], "entities": []}, {"text": "To remedy this, the automated assessment (AA) of writing has been motivated in order to automatically evaluate writing competence and hence not only reduce grader workload, but also bypass grader inconsistencies as only one system would be responsible for the assessment.", "labels": [], "entities": [{"text": "automated assessment (AA) of writing", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.574717253446579}]}, {"text": "Numerous AA systems have been developed for research purposes or deployed for commercial use, including Project Essay Grade (PEG), e-Rater (), Intelligent Essay Assessor (IEA) and Bayesian Essay Test Scoring sYstem (BETSY)) among others.", "labels": [], "entities": [{"text": "Bayesian Essay Test Scoring sYstem (BETSY", "start_pos": 180, "end_pos": 221, "type": "TASK", "confidence": 0.6560612065451485}]}, {"text": "They employ statistical approaches that exploit a wide range of textual features.", "labels": [], "entities": []}, {"text": "A recent direction of research has focused on applying deep learning to the AA task in order to circumvent the heavy feature engineering involved in traditional systems.", "labels": [], "entities": [{"text": "AA task", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9285654425621033}]}, {"text": "Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN).", "labels": [], "entities": []}, {"text": "They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest 1 , which contains essays written by middle-school English speaking students.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP) dataset", "start_pos": 29, "end_pos": 78, "type": "DATASET", "confidence": 0.4590040110051632}]}, {"text": "On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features).", "labels": [], "entities": []}, {"text": "The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recourse to handcrafted features.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.7770561575889587}]}, {"text": "However, other AA datasets pose a challenge to neural models and they still fail to beat state-of-the-art methods when evaluated on these sets.", "labels": [], "entities": []}, {"text": "An example of such datasets is the First Certificate in English (FCE) set where applying a rank preference Support Vector Machine (SVM) trained on various lexical and grammatical features achieved the best results).", "labels": [], "entities": [{"text": "First Certificate in English (FCE) set", "start_pos": 35, "end_pos": 73, "type": "DATASET", "confidence": 0.5472062304615974}]}, {"text": "This motivates further investigation into neural networks to determine what minimum useful information they can utilize to enhance their predictive power.", "labels": [], "entities": []}, {"text": "Initializing neural models with contextually rich word embeddings pre-trained on large corpora ( has been used to feed the networks with meaningful embeddings rather than random initialization.", "labels": [], "entities": []}, {"text": "Those embeddings are generic and widely employed in Natural Language Processing (NLP) tasks, yet few attempts have been made to learn more task-specific embeddings.", "labels": [], "entities": []}, {"text": "For instance, developed score-specific word embeddings (SSWE) to address the AA task on the ASAP dataset.", "labels": [], "entities": [{"text": "AA", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9679650664329529}, {"text": "ASAP dataset", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.7765996158123016}]}, {"text": "Their embeddings are constructed by ranking correct ngrams against their \"noisy\" counterparts, in addition to capturing words' informativeness measured by their contribution to the overall score of the essay.", "labels": [], "entities": []}, {"text": "We propose a task-specific approach to pre-train word embeddings, utilized by neural AA models, in an error-oriented fashion.", "labels": [], "entities": []}, {"text": "Writing errors are strong indicators of the quality of writing competence and good predictors for the overall script score, especially in scripts written by language learners, which is the case for the FCE dataset.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 202, "end_pos": 213, "type": "DATASET", "confidence": 0.9662823677062988}]}, {"text": "For example, the Spearman's rank correlation coefficient between the FCE script scores and the ratio of errors is \u22120.63 which is indicative of the importance of errors in writing evaluation: ratio of errors = number of erroneous script words script length This correlation could even be higher if error severity is accounted for as some errors could be more serious than others.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 17, "end_pos": 56, "type": "METRIC", "confidence": 0.6934971928596496}, {"text": "FCE script scores", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.773725688457489}]}, {"text": "Therefore, it seems plausible to exploit writing errors and integrate them into AA systems, as was successfully done by and, but not by capturing this information directly in word embeddings in a neural AA model.", "labels": [], "entities": []}, {"text": "Our pre-training model learns to predict a score for each ngram based on the errors it contains and modifies the word vectors accordingly.", "labels": [], "entities": []}, {"text": "The idea is to arrange the embedding space in away that discriminates between \"good\" and \"bad\" ngrams based on their contribution to writing errors.", "labels": [], "entities": []}, {"text": "Bootstrapping the assessment neural model with those learned embeddings could help detect wrong patterns in writing which should improve its accuracy of predicting the script's holistic score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9987195730209351}]}, {"text": "We implement a CNN as the AA model and compare its performance when initialized with our embeddings, tuned based on natural writing errors, to the one obtained when bootstrapped with the SSWE, proposed by, that relies on random noisy contexts and script scores.", "labels": [], "entities": []}, {"text": "Furthermore, we implement another version of our model that augments ngram errors with their corrections and investigate the effect on performance.", "labels": [], "entities": []}, {"text": "Additionally, we compare the aforementioned pre-training approaches to the commonly used embeddings trained on large corpora (Google or Wikipedea).", "labels": [], "entities": []}, {"text": "The results show that our approach outperforms other initialization methods and augmenting the model with error corrections helps alleviate the effects of data sparsity.", "labels": [], "entities": []}, {"text": "Finally, we further analyse the pre-trained representations and demonstrate that our embeddings are better at detecting errors which is inherent for AA.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our error-oriented approaches to the SSWE model as well as generic pre-trained models commonly used to initialize  prompts asking the learner to write either an article, a letter, a report, a composition or a short story.", "labels": [], "entities": [{"text": "initialize  prompts asking the learner to write either an article, a letter, a report, a composition or a short story", "start_pos": 114, "end_pos": 231, "type": "Description", "confidence": 0.7190886787746263}]}, {"text": "We apply script-level evaluation by concatenating the two answers and using a special answer end token to separate the answers in the same script.", "labels": [], "entities": []}, {"text": "The writing errors committed in the scripts are manually annotated using a taxonomy of 80 error types together with suggested corrections.", "labels": [], "entities": []}, {"text": "An example of error annotations is: The problems started <e type=\"RT\"> <i>in</i><c>at</c></e> the box office.", "labels": [], "entities": []}, {"text": "where <i></i> is the error, <c></c> is the suggested correction and the error type \"RT\" refers to \"replace preposition\".", "labels": [], "entities": []}, {"text": "For error-oriented models, a word is considered an error if it occurs inside an error tag and the correction is retrieved according to the correction tag.", "labels": [], "entities": []}, {"text": "We train the models on the released public FCE dataset which contains 1, 141 scripts for training and 97 scripts for testing.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9615591466426849}]}, {"text": "In order to examine the effects of training with extra data, we conduct experiments where we augment the public set with additional FCE scripts and refer to this extended version as FCE ext , which contains 9, 822 scripts.", "labels": [], "entities": [{"text": "FCE ext", "start_pos": 182, "end_pos": 189, "type": "DATASET", "confidence": 0.8923992216587067}]}, {"text": "We report the results of both datasets on the released test set.", "labels": [], "entities": []}, {"text": "The public FCE dataset is divided into 1, 061 scripts for training and 80 for development while for FCE ext , 8, 842 scripts are used for training and 980 are held out for development.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9456092715263367}, {"text": "FCE ext", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.7877830862998962}]}, {"text": "The only data preprocessing employed is word tokenization which is achieved using the Robust Accurate Statistical Parsing (RASP) system).", "labels": [], "entities": [{"text": "word tokenization", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7582377791404724}]}, {"text": "Hyperparameter tuning is done for each model separately.", "labels": [], "entities": []}, {"text": "The SSWE, ESWE and ECSWE models are initialized with GloVe (d wrd = 50) vectors, trained for 20 epochs and the learning rate is set to 0.01.", "labels": [], "entities": [{"text": "ESWE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.606572687625885}, {"text": "ECSWE", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.472204327583313}, {"text": "GloVe (d wrd = 50) vectors", "start_pos": 53, "end_pos": 79, "type": "METRIC", "confidence": 0.9193627536296844}]}, {"text": "For SSWE, \u03b1 is set to 0.1, batch size to 128, the number of randomly gen-  erated counterparts per ngram to 20 and the size of hidden layer to 100.", "labels": [], "entities": []}, {"text": "9 For the AA network, initialized with any of the 5 models, h is set to 100, and learning rate to 0.001 when training on public FCE and 0.0001 on FCE ext . The sizes used for error and script filters are shown in.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 81, "end_pos": 94, "type": "METRIC", "confidence": 0.9830546975135803}, {"text": "FCE", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.8769873976707458}, {"text": "FCE ext", "start_pos": 146, "end_pos": 153, "type": "DATASET", "confidence": 0.9006555080413818}]}, {"text": "All the networks are optimized using Stochastic Gradient Descent (SGD).", "labels": [], "entities": []}, {"text": "The AA system is regularized with L2 regularization with rate = 0.0001 and trained for 50 epochs during which performance is monitored on the dev sets.", "labels": [], "entities": [{"text": "AA", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9925671815872192}]}, {"text": "Finally, the AA model with the best mean square error over the dev sets is selected.", "labels": [], "entities": [{"text": "AA", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9830821752548218}]}], "tableCaptions": [{"text": " Table 1: Error and script refer to their filter sizes.  For each of the 5 pre-training models on the two  datasets, the error filter size is displayed (if ap- plicable) along with the script filter size used in  the AA network initialized with the embeddings  on the left. FCE refers to the public FCE.", "labels": [], "entities": []}, {"text": " Table 2: AA results when bootstrapped from different word embeddings and trained on public FCE. The  bold values indicate the best results.", "labels": [], "entities": [{"text": "AA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9992532134056091}, {"text": "FCE", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.8469513058662415}]}, {"text": " Table 3: AA results when bootstrapped from different word embeddings and trained on the extended  FCE version (FCE ext ). The bold values indicate the best results.", "labels": [], "entities": [{"text": "AA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9991710186004639}, {"text": "FCE version (FCE ext )", "start_pos": 99, "end_pos": 121, "type": "DATASET", "confidence": 0.8867871562639872}]}, {"text": " Table 4: AP results of the random baseline and  SSWE and EWE models when trained on public  and extended FCE sets and tested on the respective  dev sets. The AP is calculated with respect to the  error class.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9873096346855164}, {"text": "EWE", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.6785787343978882}, {"text": "AP", "start_pos": 159, "end_pos": 161, "type": "METRIC", "confidence": 0.9854515194892883}]}]}