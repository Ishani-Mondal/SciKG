{"title": [], "abstractContent": [{"text": "We describe here our approaches and results on the WAT 2017 shared translation tasks.", "labels": [], "entities": [{"text": "WAT 2017 shared translation tasks", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.7441188812255859}]}, {"text": "Motivated by the good results we obtained with Neural Machine Translation in the previous shared task, we continued to explore this approach this year, with incremental improvements in models and training methods.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7423011064529419}]}, {"text": "We focused on the AS-PEC dataset and could improve the state-of-the-art results for Chinese-to-Japanese and Japanese-to-Chinese translations.", "labels": [], "entities": [{"text": "AS-PEC dataset", "start_pos": 18, "end_pos": 32, "type": "DATASET", "confidence": 0.7979665100574493}]}], "introductionContent": [{"text": "This paper describes our experiments for the WAT 2017 shared translation task.", "labels": [], "entities": [{"text": "WAT 2017 shared translation task", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.9153126001358032}]}, {"text": "For more details refer to the overview paper (.", "labels": [], "entities": []}, {"text": "This translation task contains several subtasks, but we focused on the ASPEC dataset, for the Japanese-English and Japanese-Chinese language pairs.", "labels": [], "entities": [{"text": "translation task", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.89217808842659}, {"text": "ASPEC dataset", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.8904188275337219}]}, {"text": "Following upon our findings during WAT 2016 ( ) that our Neural Machine Translation system yielded significantly better results than our Example-Based Machine Translation system, we only experimented with NMT this year.", "labels": [], "entities": [{"text": "WAT 2016", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.7509598433971405}, {"text": "Neural Machine Translation", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6549838781356812}, {"text": "Example-Based Machine Translation", "start_pos": 137, "end_pos": 170, "type": "TASK", "confidence": 0.5564596255620321}]}, {"text": "Our improvements are actually quite incremental, with only small changes in the model architectures, model sizes, training and decoding approaches.", "labels": [], "entities": []}, {"text": "Together, these small changes, however, allow us to improve over our past year's results by several BLEU points, leading to the best official results for the Japanese-Chinese pair.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9994956254959106}]}, {"text": "In terms of pairwise human evaluation scores we have the best official results for all language directions except for English to Japanese.", "labels": [], "entities": []}, {"text": "Our JPO adequacy scores are also within 1% of the best score for these language directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "From the point of view of human pairwise evaluation, our system achieved the best translation quality for all the subtasks except for En \u2192 Ja.", "labels": [], "entities": []}, {"text": "From the point of view of automatic BLEU evaluation, we obtained the best results for the two directions of the Japanese-Chinese dataset, but not for the Japanese-English dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9846816062927246}, {"text": "Japanese-Chinese dataset", "start_pos": 112, "end_pos": 136, "type": "DATASET", "confidence": 0.7069201171398163}, {"text": "Japanese-English dataset", "start_pos": 154, "end_pos": 178, "type": "DATASET", "confidence": 0.7258532345294952}]}, {"text": "In the case of JPO Adequacy scores we rank 2nd for the three language directions for which we had ranked first in term of pairwise evaluation.", "labels": [], "entities": []}, {"text": "But because the difference inadequacy score with respect to the first system is by less than 1%, it might not be statistcially significant.", "labels": [], "entities": []}, {"text": "For Japanese to Chinese we noticed that we had a higher percentage of translations which were rated as perfect compared to the other systems.In general the number of translations with the lowest scores (with a rating of 1) are much lower when compared to last years results which is a clear indication of progress.", "labels": [], "entities": []}, {"text": "It is interesting to note that these results reveal a certain discrepancy between BLEU and human evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.997844934463501}]}, {"text": "In particular, for Japanese-to-English, although our submission was significantly below some other submissions in term of BLEU, it ended up being given a higher score by human evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9982700347900391}]}, {"text": "It somehow confirms that BLEU is not always a clear indicator of translation quality, maybe especially fora language like Japanese that has free word order.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9989055395126343}, {"text": "translation quality", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7847296595573425}]}, {"text": "Moreover, there are questions on the reliability of BLEU when the BLEU scores are for w, p w \u2208 top B (prob) do \u25b7 top B return the B words with highest probability if w = EOS then 11: add (lp + log(p w ), t) to f inished: Official automatic and human evaluation results of our NMT systems for the ASPEC subtasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9861339926719666}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.996618390083313}]}, {"text": "The scores in bold are the best compared to the scores of the other systems.", "labels": [], "entities": []}, {"text": "For JPO adequacy, rank marked by a * indicates the score was within 1% of the best and therefore the difference might not be statistically significant.", "labels": [], "entities": []}, {"text": "This hints that it might not be a good idea to use training procedures that directly optimize BLEU, something that was already mentioned in (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9955536723136902}]}, {"text": "We also performed additional experiments for En \u2192 Ja after the official submission deadline which we describe in the following subsection.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official automatic and human evaluation results of our NMT systems for the ASPEC subtasks.  The scores in bold are the best compared to the scores of the other systems. For JPO adequacy, rank  marked by a * indicates the score was within 1% of the best and therefore the difference might not be  statistically significant.", "labels": [], "entities": [{"text": "ASPEC subtasks", "start_pos": 85, "end_pos": 99, "type": "DATASET", "confidence": 0.7903040945529938}]}, {"text": " Table 2: Automatic evaluation results of system combination for English to Japanese. These results  represent the SOTA in terms of BLEU and AM-FM.", "labels": [], "entities": [{"text": "SOTA", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.3814459443092346}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9892459511756897}, {"text": "AM-FM", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.9400475025177002}]}]}