{"title": [{"text": "Modeling Target-Side Inflection in Neural Machine Translation", "labels": [], "entities": [{"text": "Modeling Target-Side Inflection", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8212741216023763}, {"text": "Neural Machine Translation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.642690896987915}]}], "abstractContent": [{"text": "NMT systems have problems with large vocabulary sizes.", "labels": [], "entities": []}, {"text": "Byte-pair encoding (BPE) is a popular approach to solving this problem, but while BPE allows the system to generate any target-side word, it does not enable effective generalization over the rich vocabulary in morphologically rich languages with strong inflec-tional phenomena.", "labels": [], "entities": [{"text": "Byte-pair encoding (BPE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7723125040531158}]}, {"text": "We introduce a simple approach to overcome this problem by training a system to produce the lemma of a word and its morphologically rich POS tag, which is then followed by a deter-ministic generation step.", "labels": [], "entities": []}, {"text": "We apply this strategy for English-Czech and English-German translation scenarios, obtaining improvements in both settings.", "labels": [], "entities": []}, {"text": "We furthermore show that the improvement is not due to only adding explicit morphological information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has recently become the new state of the art.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8550064563751221}]}, {"text": "Despite a large body of recent research, NMT still remains a relatively unexplored territory.", "labels": [], "entities": [{"text": "NMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6279995441436768}]}, {"text": "In this work, we focus on one of these less studied areas, namely target-side morphology.", "labels": [], "entities": []}, {"text": "NMT systems typically produce outputs word-by-word and at each step, they evaluate the probability of all possible target words.", "labels": [], "entities": []}, {"text": "When translating to morphologically rich languages, due to the large size of target-side vocabularies, NMT systems run into scalability issues and struggle with vocabulary coverage.", "labels": [], "entities": []}, {"text": "Byte-pair encoding (BPE,) is currently perhaps the most successful approach to addressing these problems.", "labels": [], "entities": [{"text": "Byte-pair encoding (BPE", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.781612403690815}]}, {"text": "However, while BPE allows the system to generate any target-side word (possibly as a concatenation of smaller segments), it does not enable effective generalization over the many different surface forms possible fora single lemma, which had been shown to be useful in phrase-based SMT.", "labels": [], "entities": [{"text": "BPE", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.6101934909820557}, {"text": "SMT", "start_pos": 281, "end_pos": 284, "type": "TASK", "confidence": 0.7141317129135132}]}, {"text": "We propose a simple two-step approach to achieve morphological generalization in NMT.", "labels": [], "entities": [{"text": "morphological generalization", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.750400722026825}]}, {"text": "In the first step, we use an encoder-decoder NMT system with attention and BPE () to generate a sequence of interleaving morphological tags and lemmas.", "labels": [], "entities": [{"text": "BPE", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9972215890884399}]}, {"text": "In the second step, we use a morphological generator to produce the final inflected output.", "labels": [], "entities": []}, {"text": "This decomposition addresses all three of the problems outlined above: \u2022 the presence of lemmas allows the system to model different inflections jointly and better capture lexical correspondence with the source, \u2022 morphological information is explicit and allows the system to easily learn target-side morpho-syntactic patterns including agreement, \u2022 unseen surface forms can be generated simply by combining a known lemma and a known tag.", "labels": [], "entities": []}, {"text": "While simple, the approach is very effective and leads to significant improvements in translation quality in a medium-resource setting for EnglishCzech translation.", "labels": [], "entities": [{"text": "EnglishCzech translation", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.7576761245727539}]}, {"text": "Similarly, experiments in an English-German setting lead to improved translation results and also show that the proposed strategy can be applied to other language pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.9492209553718567}]}], "datasetContent": [{"text": "In this section, we describe our experiments with English-Czech and English-German translation.", "labels": [], "entities": [{"text": "English-German translation", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.6972496211528778}]}], "tableCaptions": [{"text": " Table 2: The most frequent fragments on word  ends after BPE from the German surface data.", "labels": [], "entities": [{"text": "BPE", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.5224647521972656}, {"text": "German surface data", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.8292046387990316}]}, {"text": " Table 3: Overview of vocabulary size in the Ger- man TED data (BPE: Byte Pair Encoding).", "labels": [], "entities": [{"text": "Ger- man TED data", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.7709776520729065}]}, {"text": " Table 4: Sizes of English-Czech corpora.", "labels": [], "entities": []}, {"text": " Table 5: English-Czech: BLEU scores of NMT  system variants.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9964534044265747}]}, {"text": " Table 6: English-Czech: BLEU scores of systems  with larger parallel training data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9970034956932068}]}, {"text": " Table 7: English-German: lowercased BLEU for  two test sests (1363 and 1305 sentences).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9777995944023132}]}, {"text": " Table 8: English-German: lowercased BLEU for  newstest'16 (2169 sentences) trained on 250k and  500k sentences news-mix data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9833223819732666}]}]}