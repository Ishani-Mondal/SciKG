{"title": [{"text": "Structured Generation of Technical Reading Lists", "labels": [], "entities": []}], "abstractContent": [{"text": "Learners need to find suitable documents to read and prioritize them in an appropriate order.", "labels": [], "entities": []}, {"text": "We present a method of automatically generating reading lists, selecting documents based on their pedagogical value to the learner and ordering them using the structure of concepts in the domain.", "labels": [], "entities": []}, {"text": "Resulting reading lists related to computational linguistics were evaluated by advanced learners and judged to be near the quality of those generated by domain experts.", "labels": [], "entities": []}, {"text": "We provide an open-source implementation of our method to enable future work on reading list generation.", "labels": [], "entities": [{"text": "reading list generation", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.6887847781181335}]}], "introductionContent": [{"text": "More scientific and technical literature is instantly accessible than ever before, but this means that it can also be harder than ever to determine what sequence of documents would be most helpful fora learner to read.", "labels": [], "entities": []}, {"text": "Standard information retrieval tools, e.g., a search engine, will find documents that are highly relevant, but they will not return documents about concepts that must be learned first, and they will not identify which documents are appropriate fora particular user.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7676719129085541}]}, {"text": "Learners would greatly benefit from an automated approximation of the sort of personalized reading list an expert tutor would create for them.", "labels": [], "entities": []}, {"text": "We have developed TechKnAcq -short for Technical Knowledge Acquisition -to automatically construct this kind of pedagogically useful reading list for technical subjects.", "labels": [], "entities": []}, {"text": "Presented with only a \"core corpus\" of technical material that represents the subject understudy, without any additional semantic annotation, TechKnAcq generates a reading list in response to a simple query.", "labels": [], "entities": []}, {"text": "For instance, given a corpus of documents related to natural language processing, a reading list can be generated for the query \"machine translation.\"", "labels": [], "entities": [{"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7492602169513702}]}, {"text": "The reading list should be similar to what a PhD student might be given by her advisor: it should include prerequisite subjects that need to be understood before attempting to learn material about the query, and it should be tailored to the individual needs of the student.", "labels": [], "entities": []}, {"text": "To generate such a reading list, we first infer the conceptual structure of the domain from the core corpus.", "labels": [], "entities": []}, {"text": "We then expand this corpus to include a greater amount of relevant, pedagogically useful documents, and we relate concepts to one another and to the individual documents in a concept graph structure.", "labels": [], "entities": []}, {"text": "Using this graph and a model of the learner's expertise, we generate personalized reading lists for the user's queries.", "labels": [], "entities": []}, {"text": "In the following sections, we describe these steps and then evaluate the resulting reading lists for several concepts in computational linguistics, compared to reading lists generated by domain experts.", "labels": [], "entities": []}], "datasetContent": [{"text": "To enable comparison to an existing gold standard, we evaluated TechKnAcq on the domain of computational linguistics and natural language processing.", "labels": [], "entities": []}, {"text": "Our evaluation covers 16 topics: For eight topics, we evaluate the expert-generated Jardine (2014) gold standard (JGS) reading lists and reading lists generated by TechKnAcq for the same topics.", "labels": [], "entities": [{"text": "Jardine (2014) gold standard (JGS) reading lists", "start_pos": 84, "end_pos": 132, "type": "DATASET", "confidence": 0.6450857818126678}]}, {"text": "We additionally evaluated reading lists generated by TechKnAcq for eight topics of central importance in the domain, sampled from the list of \"Major evaluations and tasks\" on the Wikipedia article on natural language processing.2 In this section, we describe the generation of a concept graph for the evaluation domain, the evaluation methodology and participants, and the results.", "labels": [], "entities": []}, {"text": "As our core corpus, we used the ACL Anthology, which consists of PDFs -many of them scanned -of conference and workshop papers and journal articles.", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9639557003974915}]}, {"text": "There have been multiple attempts to produce machine-readable versions of the corpus, but all suffer from problems of text quality and extraction coverage.", "labels": [], "entities": []}, {"text": "We used the December 2016 release of the ACL Anthology Network corpus (, which includes papers published through 2014.", "labels": [], "entities": [{"text": "ACL Anthology Network corpus", "start_pos": 41, "end_pos": 69, "type": "DATASET", "confidence": 0.9365210384130478}]}, {"text": "We automatically and manually enhanced this corpus by adding missing text, removing documents not primarily written in English and ones with only abstracts, and joining words split across lines.", "labels": [], "entities": []}, {"text": "After running the corpus expansion method described in Section 2.1, the corpus includes: \u2022 22,084 papers from the ACL Anthology \u2022 1,949 encyclopedia articles from Wikipedia \u2022 1,172 book chapters from ScienceDirect \u2022 114 tutorials retrieved from the Web The concept graph was generated using a 300-topic LDA model, defined over bigrams.", "labels": [], "entities": []}, {"text": "Names were manually assigned to 238 topics, and 62 topics that could not be assigned a name were excluded from the concept graph.", "labels": [], "entities": []}, {"text": "We recruited 33 NLP researchers to take part in the evaluation, primarily from an online mailing list for the computational linguistics community.", "labels": [], "entities": []}, {"text": "Participants were required to have institutional affiliations and expertise in NLP.", "labels": [], "entities": []}, {"text": "In the evaluation, participants were presented with the reading lists3 and asked to change the order of documents to the order they would recommend a novice in NLP to read, i.e., ensuring that the first documents require limited knowledge and the documents that follow are predicated on the ones that came before.", "labels": [], "entities": []}, {"text": "The participants could also remove documents from the reading list and suggest new documents be added in any position.", "labels": [], "entities": []}, {"text": "By tracking changes in the reading lists, we can measure how many entries had to be changed for the list to be satisfactory.", "labels": [], "entities": []}, {"text": "Three sets of reading lists were evaluated.", "labels": [], "entities": []}, {"text": "The first two were comparable lists, consisting of expertgenerated lists, and their TechKnAcq counterparts.", "labels": [], "entities": []}, {"text": "Together, these constitute the \"comparison\" set.", "labels": [], "entities": []}, {"text": "The third set consisted of additional TechKnAcqgenerated reading lists; this constitutes the \"standalone\" set.", "labels": [], "entities": []}, {"text": "In addition to this edit-based evaluation, for the stand-alone set participants were asked to rate their agreement with statements about read- The order in which TechKnAcq and JGS reading lists were presented was randomized and counterbalanced to control for order effects.", "labels": [], "entities": [{"text": "TechKnAcq", "start_pos": 162, "end_pos": 171, "type": "DATASET", "confidence": 0.9455501437187195}]}, {"text": "ing lists generated by TechKnAcq fora qualitative measure of a reading list's pedagogical value.", "labels": [], "entities": []}, {"text": "The similarity of TechKnAcq reading lists to expertgenerated ones in terms of pedagogical value was assessed based on the changes participants made to the lists -the fewer documents that were moved, deleted, or added, the better the participant considered the reading list.", "labels": [], "entities": []}, {"text": "The total number of changes to a reading list was measured using edit distance, but we are also interested specifically in the stability of document positions, the number of documents deleted, and the number of documents added to the reading lists.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.8035227954387665}]}, {"text": "Edit distance One of the most natural ways to compute how much a participant modified a given reading list overall is to use edit distance.", "labels": [], "entities": []}, {"text": "This is a method of computing the fewest edit operations necessary to turn one sequence into another, classically applied to spell-checking.", "labels": [], "entities": []}, {"text": "The operations are insertion, deletion, and substitution of an item.", "labels": [], "entities": []}, {"text": "So, for instance, if the participant removes a paper and adds another in the same location in the reading list, she has performed a substitution, with an edit distance of one.", "labels": [], "entities": []}, {"text": "If she then moves a paper from the end of the reading list to the beginning, that is a deletion from the old location followed by an insertion.", "labels": [], "entities": []}, {"text": "A limitation of edit distance is that it does not take into account the length of the sequence being modified.", "labels": [], "entities": []}, {"text": "E.g., along reading list that is mostly considered to be good may have the same number of edits as a shorter reading list that is much worse.", "labels": [], "entities": []}, {"text": "As such, we also normalized the edit distance scores by dividing by the length of the original reading list.", "labels": [], "entities": [{"text": "edit distance scores", "start_pos": 32, "end_pos": 52, "type": "METRIC", "confidence": 0.8615099191665649}]}, {"text": "For the comparable set, the average edit distance was 0.22 for an expert reading list and 0.33 fora TechKnAcq-generated one.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9324351847171783}]}, {"text": "The edit distance for TechKnAcq reading lists for the stand-alone set was 0.38.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.958541065454483}, {"text": "TechKnAcq reading lists", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.9200512766838074}]}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "List stability One indicator of reading list quality is how stable a list is, i.e., whether a document changes position within a list.", "labels": [], "entities": []}, {"text": "This is computed as the number of documents whose absolute position in the reading list has changed, not including documents that were added (written in) by the participants.", "labels": [], "entities": []}, {"text": "The mean level of stability for reading lists is given in maximums are also reported, with TechKnAcq scoring a minimum of zero more often, indicating that participants left these lists unchanged more often than the expert (JGS) lists.", "labels": [], "entities": []}, {"text": "Note that, unlike for edit distance, some changes to reading lists, such as moving the first document to the end, have an outsize effect on the stability score compared with others, like swapping the first and last documents.", "labels": [], "entities": [{"text": "stability score", "start_pos": 144, "end_pos": 159, "type": "METRIC", "confidence": 0.9651476144790649}]}, {"text": "This indicator is also sensitive to list length -the longer the list, the more potential there is for changes within the list.", "labels": [], "entities": []}, {"text": "For the comparison set, the average stability for TechKnAcq reading lists, normalized by length, is 0.70 vs 0.69 for expert-generated reading lists, indicating a similar level of document movement.", "labels": [], "entities": []}, {"text": "Deletions Fewer deletions signals a judgment that the reading list contents are appropriate.", "labels": [], "entities": []}, {"text": "presents the mean number of deletions.", "labels": [], "entities": []}, {"text": "When deletions are normalized by reading list length, there are fewer (0.16) for expert-generated reading lists than for for TechKnAcq (0.23) on the comparison set.", "labels": [], "entities": []}, {"text": "While the stability scores were similar for the comparison set, the deletions suggest that TechKnAcq does worse at selecting documents than experts do.", "labels": [], "entities": []}, {"text": "This maybe a limitation of computing relevance using a coarse-grained topic model or it may reflect that TechKnAcq includes more documents for concept dependencies than the participants felt necessary.", "labels": [], "entities": []}, {"text": "Additions Participants were encouraged to add any documents they felt belonged in the reading list that were not present.", "labels": [], "entities": []}, {"text": "However, this was relatively labor-intensive, requiring the participant to either remember or lookup relevant papers and then enter information about them.", "labels": [], "entities": []}, {"text": "As such, relatively few documents were added.", "labels": [], "entities": []}, {"text": "Statistics for additions are given in, but the rate with which documents were added is similar for TechKnAcq and expertgenerated reading lists.", "labels": [], "entities": []}, {"text": "Qualitative For reading lists generated for the stand-alone set, participants qualitatively evaluated whether they were appropriate to use in a pedagogical setting.", "labels": [], "entities": []}, {"text": "They were asked to rate their agreement with these statements on a scale from 1 (strongly disagree) to 7 (strongly agree): 1.", "labels": [], "entities": []}, {"text": "This reading list is complete.", "labels": [], "entities": []}, {"text": "2. This is a good reading list fora PhD student.", "labels": [], "entities": []}, {"text": "3. I would use this reading list in one of my classes.", "labels": [], "entities": []}, {"text": "4. I would send this reading list to a colleague of mine.", "labels": [], "entities": []}, {"text": "5. This is a good reading list fora master's student.", "labels": [], "entities": []}, {"text": "6. I could come up with a more complete reading list than the one provided.", "labels": [], "entities": []}, {"text": "7. If a PhD read the articles in this reading list in order, they would master the concepts.", "labels": [], "entities": []}, {"text": "Cronbach's \u03b1 was calculated for each set of questions; high values (\u03b1 > .8) indicate that each set of items were internally consistent, and closely related as a set).", "labels": [], "entities": []}, {"text": "Thus, we averaged these ratings (with responses to Statement 6 inverted) fora composite measure of the pedagogical value of each reading list.", "labels": [], "entities": []}, {"text": "Results indicate that, on average, the reading lists have moderate-to-high potential.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Changes to document positions in expert and TechKnAcq reading lists, for the comparison and", "labels": [], "entities": [{"text": "TechKnAcq reading lists", "start_pos": 54, "end_pos": 77, "type": "DATASET", "confidence": 0.8901336391766866}]}, {"text": " Table 2: Number of documents participants deleted from expert and TechKnAcq reading lists, for the  comparison and stand-alone sets. Lower numbers indicate better document selection. Norm is the mean  number of deletions normalized by dividing by the reading list length to allow comparison across lists.", "labels": [], "entities": [{"text": "TechKnAcq reading lists", "start_pos": 67, "end_pos": 90, "type": "DATASET", "confidence": 0.8750715851783752}, {"text": "Norm", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9900010824203491}]}, {"text": " Table 3: Number of documents participants added to expert and TechKnAcq reading lists, for the comparison  and stand-alone sets. Lower numbers indicate better original reading lists. Norm is the mean number of  additions normalized by dividing by the reading list length to allow comparison across lists.", "labels": [], "entities": [{"text": "TechKnAcq reading lists", "start_pos": 63, "end_pos": 86, "type": "DATASET", "confidence": 0.8922837773958842}, {"text": "Norm", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9925762414932251}]}]}