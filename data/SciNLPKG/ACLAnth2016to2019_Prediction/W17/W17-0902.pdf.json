{"title": [{"text": "A Consolidated Open Knowledge Representation for Multiple Texts", "labels": [], "entities": [{"text": "Open Knowledge Representation", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6703300476074219}]}], "abstractContent": [{"text": "We propose to move from Open Information Extraction (OIE) ahead to Open Knowledge Representation (OKR), aiming to represent information conveyed jointly in a set of texts in an open text-based manner.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.7885017295678457}, {"text": "Open Knowledge Representation (OKR)", "start_pos": 67, "end_pos": 102, "type": "TASK", "confidence": 0.7632833222548167}]}, {"text": "We do so by consolidating OIE extractions using entity and predicate coreference, while modeling information containment between coreferring elements via lexical entailment.", "labels": [], "entities": [{"text": "OIE extractions", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.8313658237457275}]}, {"text": "We suggest that generating OKR structures can be a useful step in the NLP pipeline, to give semantic applications an easy handle on consolidated information across multiple texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language understanding involves identifying, classifying, and integrating information about events and other propositions mentioned in text.", "labels": [], "entities": [{"text": "Natural language understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6907005508740743}]}, {"text": "While much effort has been invested in generic methods for analyzing single sentences and detecting the propositions they contain, little thought and effort has been put into the integration step: how to systematically consolidate and represent information contributed by propositions originating from multiple texts.", "labels": [], "entities": []}, {"text": "Consolidating such information, which is typically both complementary and partly overlapping, is needed to construct multi-document summaries, to combine evidence when answering questions that cannot be answered based on a single sentence, and to populate a knowledge base while relying on multiple pieces of evidence (see fora motivating * Corresponding author example).", "labels": [], "entities": []}, {"text": "Yet, the burden of integrating information across multiple texts is currently delegated to downstream applications, leading to various partial solutions in different application domains.", "labels": [], "entities": []}, {"text": "This paper suggests that a common consolidation step and a corresponding knowledge representation should be part of the \"standard\" semantic processing pipeline, to be shared by downstream applications.", "labels": [], "entities": []}, {"text": "Specifically, we pursue an Open Knowledge Representation (OKR) framework that captures the information expressed jointly in multiple texts while relying solely on the terminology appearing in those texts, without requiring pre-defined external knowledge resources or schemata.", "labels": [], "entities": [{"text": "Open Knowledge Representation (OKR)", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.804010291894277}]}, {"text": "As we focus in this work on investigating an open representation paradigm, our proposal follows and extends the Open Information Extraction (OIE) approach.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 112, "end_pos": 145, "type": "TASK", "confidence": 0.7442182302474976}]}, {"text": "We do that by first extracting textual predicate-argument tuples, each corresponding to an individual proposition mention.", "labels": [], "entities": []}, {"text": "We then merge these mentions by accounting for proposition coreference, an extended notion of event coreference.", "labels": [], "entities": []}, {"text": "This process yields consolidated propositions, each corresponding to a single fact, or assertion, in the described scenario.", "labels": [], "entities": []}, {"text": "Similarly, entity coreference links are used to establish reference to real-world entities.", "labels": [], "entities": []}, {"text": "Taken together, our proposed representation encodes information about events and entities in the real world, similarly to what is expected from structured knowledge representations.", "labels": [], "entities": []}, {"text": "Yet, being an open textbased representation, we record the various lexical terms used to describe the scenario.", "labels": [], "entities": []}, {"text": "Further, we model information redundancy and containment among these terms through lexical entailment.", "labels": [], "entities": [{"text": "information redundancy", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7408816814422607}]}, {"text": "In this paper we specify our proposed represen-: A sample of news headlines, illustrating the need for information consolidation.", "labels": [], "entities": [{"text": "information consolidation", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.7304131239652634}]}, {"text": "Two mentions of the same proposition, for which event coreference holds, are highlighted, with the predicate in bold and the arguments underlined.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.6859074831008911}]}, {"text": "Some information is redundant, but maybe described at different granularity levels; for example, different mentions describe the interception target as a plane and as a jet, where jet entails plane and is accordingly more informative.", "labels": [], "entities": []}, {"text": "tation, while specifying the involved annotation sub-tasks from which our structures are composed.", "labels": [], "entities": [{"text": "tation", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.8243886828422546}]}, {"text": "We then describe our annotated dataset, of news headline tweets about 27 news stories, which is the first to be jointly annotated for all our required sub-tasks.", "labels": [], "entities": []}, {"text": "We also provide initial predicted baseline results for each of the sub-tasks, pointing at the limitations of current state of the art.", "labels": [], "entities": []}, {"text": "1 Overall, our main contribution is in proposing to create a consolidated representation for the information contained in multiple texts, and in specifying how such representation can be created based on entity and event coreference and lexical entailment.", "labels": [], "entities": []}, {"text": "An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures.", "labels": [], "entities": []}, {"text": "We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (, while exploiting similar principles to those proposed here.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the formal definition of our OKR structures, we compiled a corpus with gold annotations of our 5 subtasks (listed in).", "labels": [], "entities": [{"text": "OKR structures", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.8290527164936066}]}, {"text": "As outlined in the previous section, our structures follow deterministically from these annotations.", "labels": [], "entities": []}, {"text": "Specifically, we make use of the news-related tweets collected in the Twitter Event Detection Dataset (, which clusters tweets from major news networks and other sources discussing the same event (for example, the grounding of a Syrian plane by the Turkish government).", "labels": [], "entities": [{"text": "Twitter Event Detection Dataset", "start_pos": 70, "end_pos": 101, "type": "DATASET", "confidence": 0.5994597375392914}]}, {"text": "We chose to annotate news related tweets in this first dataset for several reasons: (1) they represent self contained assertions, (2) they tend to be relatively factual and succinct, and (3) by looking at several news sources we can obtain a corpus with high redundancy, which our representation aims to address.", "labels": [], "entities": []}, {"text": "We note that while this dataset exhibits a limited amount of linguistic complexity, making it suitable fora first investigation, it still represents a very practical use case of consolidating information in a large stream of tweets about a news story.", "labels": [], "entities": [{"text": "consolidating information in a large stream of tweets about a news story", "start_pos": 178, "end_pos": 250, "type": "TASK", "confidence": 0.6082753563920656}]}, {"text": "This annotation serves two main purposes.", "labels": [], "entities": []}, {"text": "First, it validates the feasibility of our annotation scheme in terms of annotator requirements, training and agreement.", "labels": [], "entities": []}, {"text": "Second, to the best of our knowledge, this is the first time these core NLP annotations are annotated in parallel over the same texts.", "labels": [], "entities": []}, {"text": "Following, this annotation has the potential of becoming a useful resource spurring future research into joint prediction of these annotations.", "labels": [], "entities": []}, {"text": "For instance, predicate argument structures may benefit from co-reference signals, and entity extraction systems may exploit signals from lexical entailment.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7458315193653107}]}, {"text": "Overall, we annotated 1257 tweets from 27 clusters.", "labels": [], "entities": []}, {"text": "We release the dataset both in full OKR format, as well as ECB-like \"light\" format, containing only the annotated co-reference chains.", "labels": [], "entities": [{"text": "OKR", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.799514889717102}]}, {"text": "Overall corpus statistics are depicted in.", "labels": [], "entities": []}, {"text": "An analysis of the annotations reveals interesting and unique characteristics of our annotated corpus.", "labels": [], "entities": []}, {"text": "First, the part of speech distribution of entities and predicates shows that our corpus captures information beyond the current focus on verb-centric applications and corpora in NLP.", "labels": [], "entities": []}, {"text": "Namely, our corpus contains avast number of nonverbal predications (67%), and a relatively large number of adjectival entities, owing to the fact that our structure annotates concepts such as \"northern\" or \"Syrian\" as entities in an implicit relation.", "labels": [], "entities": []}, {"text": "Second, the average number of unique lemmas per entity and proposition chains (2.00 and 2.24, respectively) shows that our corpus exhibits a fair amount of non-trivial lexical variability.", "labels": [], "entities": []}, {"text": "Finally, roughly 96% of our entailment graphs (entity and proposition) form a connected component.", "labels": [], "entities": []}, {"text": "This data provides an interesting potential for investigating and modeling lexical inference relations within coreference chains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Twitter dataset statistics. Distinct lemma  terms per proposition chain were calculated only  on explicit propositions. Average number of el- ements per argument chain measures how many  distinct entities or propositions were part of the  same argument.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8492908477783203}, {"text": "Average number of el- ements", "start_pos": 130, "end_pos": 158, "type": "METRIC", "confidence": 0.9226838946342468}]}, {"text": " Table 3: Entity and Predicate distribution across  part of speech tags: nouns, verbs, adjectives, non- lexicalized (implicit) and all others.", "labels": [], "entities": []}]}