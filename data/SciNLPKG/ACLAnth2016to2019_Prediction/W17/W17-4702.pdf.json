{"title": [{"text": "Improving Word Sense Disambiguation in Neural Machine Translation with Sense Embeddings", "labels": [], "entities": [{"text": "Improving Word Sense Disambiguation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9069823324680328}, {"text": "Neural Machine Translation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6306079030036926}]}], "abstractContent": [{"text": "Word sense disambiguation is necessary in translation because different word senses often have different translations.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6181159118811289}]}, {"text": "Neural machine translation models learn different senses of words as part of an end-to-end translation task, and their capability to perform word sense disambiguation has so far not been quantified.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.672883411248525}, {"text": "word sense disambiguation", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.6389055649439493}]}, {"text": "We exploit the fact that neural translation models can score arbitrary translations to design a novel cross-lingual word sense disam-biguation task that is tailored towards evaluating neural machine translation models.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7141502201557159}]}, {"text": "We present a test set of 7,200 lexical ambiguities for German\u2192English, and 6,700 for German\u2192French, and report baseline results.", "labels": [], "entities": []}, {"text": "With 70% of lexical ambiguities correctly disambiguated, we find that word sense disambiguation remains a challenging problem for neural machine translation , especially for rare word senses.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7210265795389811}, {"text": "neural machine translation", "start_pos": 130, "end_pos": 156, "type": "TASK", "confidence": 0.6831267674763998}]}, {"text": "To improve word sense disambiguation in neural machine translation, we experiment with two methods to integrate sense em-beddings.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7410038908322653}, {"text": "neural machine translation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7524007161458334}]}, {"text": "Ina first approach we pass sense embeddings as additional input to the neural machine translation system.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7438668211301168}]}, {"text": "For the second experiment, we extract lexical chains based on sense embeddings from the document and integrate this information into the NMT model.", "labels": [], "entities": []}, {"text": "While a base-line NMT system disambiguates frequent word senses quite reliably, the annotation with both sense labels and lexical chains improves the neural models' performance on rare word senses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantically ambiguous words present a special challenge to machine translation systems: in order to produce a correct sentence in the target language, the system has to decide which meaning is accurate in the given context.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7253079861402512}]}, {"text": "Errors in lexical choice can lead to wrong or even incomprehensible translations.", "labels": [], "entities": []}, {"text": "However, quantitatively assessing errors of this type is challenging, since automatic metrics such as BLEU () do not provide a sufficiently detailed analysis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9975281357765198}]}, {"text": "Several ways of evaluating lexical choice for machine translation have been proposed in previous work.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7852783799171448}]}, {"text": "Cross-lingual lexical choice tasks have been created for the evaluation of word sense disambiguation (WSD) systems (, and have been applied to the evaluation of MT systems.", "labels": [], "entities": [{"text": "evaluation of word sense disambiguation (WSD)", "start_pos": 61, "end_pos": 106, "type": "TASK", "confidence": 0.7337508909404278}, {"text": "MT", "start_pos": 161, "end_pos": 163, "type": "TASK", "confidence": 0.9531383514404297}]}, {"text": "evaluate lexical choice in a blank-filling task, where the translation of an ambiguous source word is blanked from the reference translation, and an MT system is tested as to whether it can predict it.", "labels": [], "entities": []}, {"text": "In all these tasks, a word-level translation (or set of translations) is defined as the gold label.", "labels": [], "entities": [{"text": "word-level translation (or set of translations)", "start_pos": 22, "end_pos": 69, "type": "TASK", "confidence": 0.654479194432497}]}, {"text": "A major problem is that an MT system will be punished for producing a synonym, paraphrase, or inflected variant of the predefined gold label.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9866863489151001}]}, {"text": "We thus propose a more constrained task where an MT system has to select one out of a predefined set of translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9770445823669434}]}, {"text": "Neural machine translation (NMT) (Sutskever et al.,; has recently emerged as the new state of the art in machine translation, producing top-ranked systems in recent shared tasks (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8061056882143021}, {"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7723086178302765}]}, {"text": "The strengths and weaknesses of NMT have been the subject of recent research, and previous studies involving human analysis have consistently found NMT to be more fluent than phrase-based SMT (, but results in terms of adequacy are more mixed.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 175, "end_pos": 191, "type": "TASK", "confidence": 0.519940048456192}]}, {"text": "report improvements in lexical choice based on HTER matches on the lemma-level, while found no clear improvement in a direct assessment of adequacy.", "labels": [], "entities": []}, {"text": "perform an error annotation in which the number of lexical choice errors even increases slightly by reranking a syntaxbased statistical machine translation system with an NMT model.", "labels": [], "entities": [{"text": "syntaxbased statistical machine translation", "start_pos": 112, "end_pos": 155, "type": "TASK", "confidence": 0.5649324655532837}]}, {"text": "We aim to allow fora large-scale, reproducible method of assessing the capability of an NMT model to perform lexical disambiguation.", "labels": [], "entities": []}, {"text": "NMT systems cannot only be used to generate translations of a source sentence, but also to assign a probability P (T |S) for any given pair of a source sentence Sand a target sentence T . We use this feature to create a test set with artificially introduced lexical disambiguation errors.", "labels": [], "entities": []}, {"text": "Comparing the scores of an NMT model on these contrastive translations to the score of the reference allows us to assess how well the model can distinguish different senses in ambiguous words.", "labels": [], "entities": []}, {"text": "We have created two test sets for the language pairs German-English and German-French with about 6,500 and 6,700 sentence pairs respectively.", "labels": [], "entities": []}, {"text": "Based on the performance of state-of-theart NMT systems on these test sets, we discuss the capability of NMT to perform lexical disambiguation.", "labels": [], "entities": []}, {"text": "Furthermore, we present two methods to improve word sense disambiguation in neural machine translation by allowing the model to learn sense-specific word embeddings.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.7362717390060425}, {"text": "neural machine translation", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.7364645004272461}]}, {"text": "Both methods are based on an external word sense disambiguation.", "labels": [], "entities": []}, {"text": "While the first method passes sense labels as additional input to an NMT system, the second is motivated by the hypothesis that document-level context is valuable for disambiguation.", "labels": [], "entities": []}, {"text": "We model this context via lexical chains, i.e. sequences of semantically-similar words in a given text that express the topic of the segment they cover in a condensed form.", "labels": [], "entities": []}, {"text": "Our method is inspired by, who present an approach to build English lexical chains automatically using WordNet and evaluate its performance on a sense disambiguation task.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9655899405479431}, {"text": "sense disambiguation task", "start_pos": 145, "end_pos": 170, "type": "TASK", "confidence": 0.7763505081335703}]}, {"text": "Instead of WordNet, we use sense embeddings in order to determine the similarity between the words in a document and thus find and annotate the lexical chains.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9276377558708191}]}, {"text": "Experimental results show the potential of lexical chains at disambiguating word senses.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present an evaluation with two basic neural MT systems, trained with Nematus ( , using byte pair encoding (BPE) on both source and target side ().", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.964504063129425}, {"text": "byte pair encoding (BPE)", "start_pos": 90, "end_pos": 114, "type": "METRIC", "confidence": 0.6866951982180277}]}, {"text": "For both the German-English and the German-French experiments, we train a model on 2.1 million sentence pairs from Europarl (v7) and News Commentary (v11).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9878944754600525}]}, {"text": "We use these corpora because they contain document boundaries, which is a requirement for the lexical chains experiments.", "labels": [], "entities": []}, {"text": "We present further results for models that use additional source-side features, a) the sense labels themselves and b) lexical chains.", "labels": [], "entities": []}, {"text": "The feature is   given its own embedding space, and the model can thus learn sense-specific embeddings.", "labels": [], "entities": []}, {"text": "If a word is segmented into multiple subword units by BPE, the additional input feature of the word is repeated for each unit.", "labels": [], "entities": [{"text": "BPE", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.7683024406433105}]}, {"text": "Vocabulary size for all models is 90,000.", "labels": [], "entities": [{"text": "Vocabulary size", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9122018814086914}]}, {"text": "We train the models fora week, using Adam () to update the model parameters on minibatches of the size 80.", "labels": [], "entities": []}, {"text": "Every 10,000 minibatches, we validate our model on a held out development set via BLEU and perplexity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9987817406654358}]}, {"text": "The maximum length of the sentences is 50.", "labels": [], "entities": []}, {"text": "The total size of the embedding layer is 500 for both the baseline and the system trained with additional input features, and the dimension of the hidden layer is 1024.", "labels": [], "entities": []}, {"text": "For the experiments with additional input features, we divide the embedding size equally among the features.", "labels": [], "entities": []}, {"text": "Conceivably, keeping the dimensionality of the word embedding constant and adding more parameters for additional features would result in better performance, but we wanted to rule out that any performance improvements are solely due to an increase in model size.", "labels": [], "entities": []}, {"text": "To assess a model's capability to distinguish different meanings of ambiguous words, we let it assign a score to the reference translation and to the artificially created contrastive translations.", "labels": [], "entities": []}, {"text": "If the score of the reference translation is higher than the scores of all contrastive translations, this counts as a correct decision.", "labels": [], "entities": []}, {"text": "As shows, both the German\u2192French and the German\u2192English baseline model achieve an accuracy of 0.70 on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9994614720344543}]}, {"text": "We also report accuracy of a smaller-scale human evaluation, in which two human annotators (one per language pair) were asked to identify the correct translation fora random sample of the test set (N=100-150).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996851682662964}]}, {"text": "The annotation was performed purely on sentencelevel, without any document context, and shows that some ambiguities are even hard fora human to resolve without context.", "labels": [], "entities": []}, {"text": "Consider the sentence pairs in for such an example.", "labels": [], "entities": []}, {"text": "We speculate that both humans and MT systems should be able to resolve more ambiguities with wider context.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9711687564849854}]}, {"text": "Even with only sentence-level information, the gap between human and NMT performance is sizeable, between 23 and 26 percentage points.", "labels": [], "entities": []}, {"text": "An important indicator of how well a word sense is translated by NMT is its frequency in the training data.", "labels": [], "entities": []}, {"text": "illustrates the relationship between the frequency of a word sense in the training data (both absolute and relative to the frequency of the source word) and the accuracy the model achieves on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9979621171951294}]}, {"text": "There is a high correlation between word sense frequency and accuracy: for German\u2192English, Spearman's \u03c1 is 0.75 for the correlation between accuracy and absolute frequency, and 0.77 for the correlation between accuracy and relative frequency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9988947510719299}, {"text": "Spearman's \u03c1", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.6214506526788076}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.992348313331604}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9969406127929688}]}, {"text": "For German\u2192French, \u03c1 is 0.58 for both.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 19, "end_pos": 20, "type": "METRIC", "confidence": 0.9526646137237549}]}, {"text": "It is unsurprising that the most frequent word sense is preferred by the model, and that accuracy for it is high.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9994329810142517}]}, {"text": "We hence want to highlight performance on rarer word senses.", "labels": [], "entities": []}, {"text": "shows the word sense accuracy of the NMT models grouped by frequency classes and the number of senses in each class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9901754260063171}]}, {"text": "All models achieve close to 100% accuracy on words that occur more than 10,000 times in the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9991540908813477}]}, {"text": "For the rare senses however, the NMT models are much less reliable: for word senses seen 0-20 times in training, the baseline accuracy is between 31-49%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9909225106239319}]}, {"text": "The annotation of the source side with sense labels improves the accuracy on the test set by 0.43% for German\u2192English, while the lexical chains does not improve the model on average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9996322393417358}]}, {"text": "On the other hand for German\u2192French, the lexical chains result in an improvement of 0.6%, but the annotation with sense labels does not lead to a better score on the test set on average.", "labels": [], "entities": []}, {"text": "As shown in, there is little room for improvement for frequent word senses, and sense labels and lexical chains show the strongest improvements over the baseline for the less frequent word senses.", "labels": [], "entities": []}, {"text": "contains the average BLEU scores on the newstest 2009-2013 test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9986037611961365}, {"text": "newstest 2009-2013 test sets", "start_pos": 40, "end_pos": 68, "type": "DATASET", "confidence": 0.9743267446756363}]}], "tableCaptions": [{"text": " Table 2: Word sense disambiguation accuracy", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7533742984135946}]}, {"text": " Table 3: Accuracy of word sense prediction by frequency of word sense in training set (  *  number of senses", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.996933102607727}, {"text": "word sense prediction", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7846105694770813}]}, {"text": " Table 5: Average BLEU scores on newstest 2009- 2013", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9352905750274658}, {"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9120035767555237}, {"text": "newstest 2009- 2013", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.9730625301599503}]}]}