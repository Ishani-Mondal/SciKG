{"title": [{"text": "Not All Dialogues are Created Equal: Instance Weighting for Neural Conversational Models", "labels": [], "entities": [{"text": "Instance Weighting", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8179768919944763}]}], "abstractContent": [{"text": "Neural conversational models require substantial amounts of dialogue data to estimate their parameters and are therefore usually learned on large corpora such as chat forums, Twitter discussions or movie subtitles.", "labels": [], "entities": []}, {"text": "These corpora are, however, often challenging to work with, notably due to their frequent lack of turn segmentation and the presence of multiple references external to the dialogue itself.", "labels": [], "entities": []}, {"text": "This paper shows that these challenges can be mitigated by adding a weighting model into the neural architecture.", "labels": [], "entities": []}, {"text": "The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling.", "labels": [], "entities": [{"text": "dialogue modelling", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.9166666865348816}]}, {"text": "At training time, these sample weights are included into the empirical loss to be minimised.", "labels": [], "entities": []}, {"text": "Evaluation results on retrieval-based models trained on movie and TV subtitles demonstrate that the inclusion of such a weighting model improves the model performance on unsupervised metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "The development of conversational agents (such as mobile assistants, chatbots or interactive robots) is increasingly based on data-driven methods aiming to infer conversational patterns from dialogue data.", "labels": [], "entities": []}, {"text": "One major trend in the last recent years is the emergence of neural conversation models (.", "labels": [], "entities": []}, {"text": "These neural models can be directly estimated from raw (non-annotated) dialogue corpora, allowing them to be deployed with a limited amount of domain-specific knowledge and feature engineering.", "labels": [], "entities": []}, {"text": "Due to their large parameter space, the estimation of neural conversation models requires considerable amounts of dialogue data.", "labels": [], "entities": [{"text": "estimation", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.9641385674476624}]}, {"text": "They are therefore often trained on conversations collected from various online resources, such as Twitter discussions () online chat logs (, movie scripts (DanescuNiculescu-Mizil and Lee, 2011) and movie and TV subtitles (.", "labels": [], "entities": []}, {"text": "Although these corpora are undeniably useful, they also face some limitations from a dialogue modelling perspective.", "labels": [], "entities": [{"text": "dialogue modelling", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8386868834495544}]}, {"text": "First of all, several dialogue corpora, most notably those extracted from subtitles, do not include any explicit turn segmentation or speaker identification.", "labels": [], "entities": [{"text": "speaker identification", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.737029641866684}]}, {"text": "In other words, we do not know whether two consecutive sentences are part of the same dialogue turn or were uttered by different speakers.", "labels": [], "entities": []}, {"text": "The neural conversation model may therefore inadvertently learn responses that remain within the same dialogue turn instead of starting anew turn.", "labels": [], "entities": []}, {"text": "Furthermore, these dialogues contain multiple references to named entities (in particular, person names such as fictional characters) that are specific to the dialogue in question.", "labels": [], "entities": []}, {"text": "These named entities should ideally not be part of the conversation model, since they often draw on an external context that is absent from the inputs provided to the conversation model.", "labels": [], "entities": []}, {"text": "For instance, the mention of character names in a movie is associated with a visual context (for instance, the characters appearing in a given scene) that is not captured in the training data.", "labels": [], "entities": []}, {"text": "Finally, a substantial portion of the utterances observed in these corpora is made of neutral, commonplace responses (\"Perhaps\", \"I don't know\", \"Err\", ...) that can be used inmost conversational situations but fall short of creating meaningful and engaging conversations with human users ().", "labels": [], "entities": [{"text": "Err", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9846569895744324}]}, {"text": "The present paper addresses these limitations by adding a weighting model to the neural architecture.", "labels": [], "entities": []}, {"text": "The purpose of this model is to associate each context, response example pair to a numerical weight that reflects the intrinsic \"quality\" of each example.", "labels": [], "entities": []}, {"text": "The instance weights are then included in the empirical loss to minimise when learning the parameters of the neural conversation model.", "labels": [], "entities": []}, {"text": "The weights are themselves computed via a neural model learned from dialogue data.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that the use of instance weights improves the performance of neural conversation models on unsupervised metrics.", "labels": [], "entities": []}, {"text": "Human evaluation results are, however, inconclusive.", "labels": [], "entities": []}, {"text": "The rest of this paper is as follows.", "labels": [], "entities": []}, {"text": "The next section presents a brief overview of existing work on neural conversation models.", "labels": [], "entities": []}, {"text": "Section 3 provides a description of the instance weighting approach.", "labels": [], "entities": []}, {"text": "Section 4 details the experimental validation of the proposed model, using both unsupervised metrics and a human evaluation of the selected responses.", "labels": [], "entities": []}, {"text": "Finally, Section 5 discusses the advantages and limitations of the approach, and Section 6 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The approach is evaluated on the basis of retrievalbased neural models trained on English-language subtitles from (.", "labels": [], "entities": []}, {"text": "Three alternative models are evaluated:  Training data for the conversation models The dataset used for training the three retrieval models is the English-language portion of the OpenSubtitles corpus of movie and TV subtitles (.", "labels": [], "entities": [{"text": "OpenSubtitles corpus of movie and TV subtitles", "start_pos": 179, "end_pos": 225, "type": "DATASET", "confidence": 0.8962416989462716}]}, {"text": "The full dataset is composed of 105 445 subtitles and 95.5 million utterances, each utterance being associated with a start and end time (in milliseconds).", "labels": [], "entities": []}, {"text": "The utterances from all datasets were tokenised, lemmatised and POS-tagged using the spaCy NLP library . We also ran the named entity recogniser 1 https://spacy.io/ from the same library to extract named entities.", "labels": [], "entities": [{"text": "spaCy NLP library", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.9115700125694275}]}, {"text": "Since the person names mentioned in movies and theatre plays typically refer to fictional characters, we replaced their occurrences by tags, one distinct tag per entity.", "labels": [], "entities": []}, {"text": "For instance, the pair: Dana: Frank, do you think you could give me a hand with these bags?", "labels": [], "entities": []}, {"text": "Frank: I'm not a doorman, Miss Barrett.", "labels": [], "entities": []}, {"text": "I'm a building superintendent. is simplified as: Dana: <person1>, do you think you could give me a hand with these bags?", "labels": [], "entities": []}, {"text": "Frank: I'm not a doorman, <person2>.", "labels": [], "entities": []}, {"text": "I'm a building superintendent.", "labels": [], "entities": []}, {"text": "Named entities of locations and numbers are also replaced by similar tags.", "labels": [], "entities": []}, {"text": "To account for the turn structure, turn boundaries were annotated with a <newturn> tag.", "labels": [], "entities": []}, {"text": "The vocabulary is capped to 25 000 words determined from their frequency in the training corpus.", "labels": [], "entities": []}, {"text": "Tokens not covered in this vocabulary are replaced by <unknown>.", "labels": [], "entities": []}, {"text": "To further investigate the potential of this weighting strategy for neural conversational models, we conducted a human evaluation of the responses generated by the two neural models included in the evaluation.", "labels": [], "entities": []}, {"text": "We collected human judgements on context, response pairs using a crowdsourcing platform.", "labels": [], "entities": []}, {"text": "We extracted 115 random contexts from the Cornell Movie Dialogs corpus and used four distinct strategies to generate dialogue responses: a random predictor (used to identify the lower bound), the two Dual Encoder models (both without and with instance weights), and expert responses (used to identify the upper bound).", "labels": [], "entities": [{"text": "Cornell Movie Dialogs corpus", "start_pos": 42, "end_pos": 70, "type": "DATASET", "confidence": 0.9625196903944016}]}, {"text": "The expert responses were manually authored by two human annotators.", "labels": [], "entities": []}, {"text": "The resulting 460 context, response pairs were evaluated by 8 distinct human judges each (920 ratings per model).", "labels": [], "entities": []}, {"text": "The human judges were asked to rate the consistency between context and response on a 5-points scale, from Inconsistent to Consistent.", "labels": [], "entities": [{"text": "consistency", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9594066143035889}]}, {"text": "In total, 118 individuals participated in the crowdsourced evaluation.", "labels": [], "entities": []}, {"text": "The results of this human evaluation are presented in.", "labels": [], "entities": []}, {"text": "There is unfortunately no statistically significant difference between the baseline Dual Encoder (M = 2.97, SD = 1.27) and the one combined with the weighting model (M = 3.04, SD = 1.27), as established by a Wilcoxon rank-sum test, W (1838) = 410360, p = 0.23.", "labels": [], "entities": [{"text": "410360", "start_pos": 243, "end_pos": 249, "type": "DATASET", "confidence": 0.7088945508003235}]}, {"text": "These inconclusive results are probably due to the very low agreement between the evaluation participants (Krippendorff's \u03b1 for continuous variable = 0.36).", "labels": [], "entities": []}, {"text": "The fact that the lower and upper bounds are only separated by 2 standard deviations confirms the difficulty for the raters to discriminate between responses.", "labels": [], "entities": []}, {"text": "We hypothesise that the nature of the corpus, which is heavily dependent on an external context (the movie scenes), makes it particularly difficult to assess the consistency of the responses.", "labels": [], "entities": [{"text": "consistency", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.964509904384613}]}, {"text": "Some examples of responses produced by the two Dual Encoder models illustrate the improvements brought by the weighting model.", "labels": [], "entities": []}, {"text": "In (1), the baseline Dual Encoder selected a turn continuation rather than a reply, while the second model avoids this pitfall.", "labels": [], "entities": []}, {"text": "Both (1) and (2) also show that the dual encoder with instance weighting tends to select utterances with fewer named entities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the 3 retrieval models on the two test sets, namely the Cornell Movie Dialogs  Dataset and the smaller dataset of theatre plays, using the Recall 10 @i metric.", "labels": [], "entities": [{"text": "Cornell Movie Dialogs  Dataset", "start_pos": 81, "end_pos": 111, "type": "DATASET", "confidence": 0.9698165506124496}]}]}