{"title": [], "abstractContent": [{"text": "The WMT17 Neural Machine Translation Training Task aims to test various methods of training neural machine translation systems.", "labels": [], "entities": [{"text": "WMT17 Neural Machine Translation Training Task", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.8143681287765503}, {"text": "training neural machine translation", "start_pos": 83, "end_pos": 118, "type": "TASK", "confidence": 0.7495927661657333}]}, {"text": "We describe the AFRL submission, including preprocessing and its knowledge distillation framework.", "labels": [], "entities": [{"text": "AFRL submission", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.7973517179489136}, {"text": "knowledge distillation", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7126746624708176}]}, {"text": "Teacher systems are given factors for domain, case, and subword location.", "labels": [], "entities": []}, {"text": "Student systems are given multiple teachers' output and a sub-selected set of the training data designed to match the target domain.", "labels": [], "entities": []}, {"text": "Numerical results indicate that the student systems surpass the teachers in translation quality and that this benefit comes directly from the inclusion of the teachers' output.", "labels": [], "entities": [{"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9515978097915649}]}], "introductionContent": [{"text": "This paper describes our development of systems for the WMT17 Neural Machine Translation (NMT) Training Task.", "labels": [], "entities": [{"text": "WMT17 Neural Machine Translation (NMT) Training Task", "start_pos": 56, "end_pos": 108, "type": "TASK", "confidence": 0.8309431672096252}]}, {"text": "This task tests methods of adjusting the NMT training process, with a fixed size and format for the final English-to-Czech system.", "labels": [], "entities": [{"text": "NMT training process", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7234989404678345}, {"text": "format", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.961793065071106}]}, {"text": "A large (approx. 50 million line) general-domain (mostly subtitles) bilingual corpus is provided as a training set.", "labels": [], "entities": []}, {"text": "A domain is provided for each line of this corpus.", "labels": [], "entities": []}, {"text": "News text, the application domain, composes about 0.5% of the corpus (see, column \"Given\").", "labels": [], "entities": []}, {"text": "A subword expansion to be used is explicitly provided as well.", "labels": [], "entities": []}, {"text": "We preprocess the training data to standardize some punctuation and character encoding differences.", "labels": [], "entities": []}, {"text": "We filter the data to remove some lines of foreign languages and little information, approximately 5% of the training data.", "labels": [], "entities": []}, {"text": "We follow a teacher-student (aka knowledge distillation) paradigm for this task).", "labels": [], "entities": [{"text": "knowledge distillation)", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.800122082233429}]}, {"text": "We train ten replicate systems larger than the final system, based on all the training data available.", "labels": [], "entities": []}, {"text": "These systems are aware of different factors (domain, case, subword location) for each subword, allowing them to use this information to learn finer details of translation.", "labels": [], "entities": []}, {"text": "They also produce different outputs, based on randomness in training.", "labels": [], "entities": []}, {"text": "We translate the entire news-domain training corpus with all replicate systems.", "labels": [], "entities": [{"text": "news-domain training corpus", "start_pos": 24, "end_pos": 51, "type": "DATASET", "confidence": 0.752347727616628}]}, {"text": "These outputs are added to the most applicable training data as another set of references, and the final NMT systems are trained from this decimated and augmented training set.", "labels": [], "entities": []}, {"text": "We choose to resist making many changes to the given systems, in order to provide useful a posteriori comparisons.", "labels": [], "entities": []}, {"text": "To this end, we use: \u2022 only neuralmonkey, or branches thereof, for NMT \u2022 the given data only \u2022 alterations to given 4GB and 8GB configurations only.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Validation set BLEU scores of intermedi- ate, factored 8GB teacher systems and final student  systems. Scores are computed internally by neu- ralmonkey. Starred systems are submission sys- tems.  System  Replicate Score  Teacher  0  17.19  Teacher  1  16.98  Teacher  2  16.96  Teacher  3  17.07  Teacher  4  17.10  Teacher  5  17.01  Teacher  6  17.09  Teacher  7  16.82  Teacher  8  16.80  Teacher  9  17.14  Student 4GB  0  17.47   * Student 4GB  1  17.58   * Student 8GB  0  18.15  Student 8GB  1  18.05", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9555424451828003}]}, {"text": " Table 3: Validation set BLEU scores of 4GB  systems trained using different data. Scores are  computed internally by neuralmonkey. \"Dup- News\" and \"DupTeach\" training was halted after  one week, since negligible improvement over \"Se- lected\" was found.  System-Replicate 4-day 7-day 14-day  Given-0  14.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9970736503601074}]}]}