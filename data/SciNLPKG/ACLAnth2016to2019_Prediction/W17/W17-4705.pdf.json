{"title": [{"text": "Evaluating the morphological competence of Machine Translation Systems", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7955173552036285}]}], "abstractContent": [{"text": "While recent changes in Machine Translation state-of-the-art brought translation quality a step further, it is regularly acknowledged that the standard automatic metrics do not provide enough insights to fully measure the impact of neural models.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8448866307735443}]}, {"text": "This paper proposes anew type of evaluation focused specifically on the morphological competence of a system with respect to various grammatical phenomena.", "labels": [], "entities": []}, {"text": "Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast.", "labels": [], "entities": []}, {"text": "This methodology is used to compare several systems submitted at WMT'17 for English into Czech and Latvian.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.8636425137519836}]}], "introductionContent": [{"text": "It is nowadays unanimously recognized that Machine Translation (MT) engines based on the neural encoder-decoder architecture with attention ( ) constitute the new state-of-the-art in statistical MT, at least for open-domain tasks).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8471461057662963}, {"text": "MT", "start_pos": 195, "end_pos": 197, "type": "TASK", "confidence": 0.6664044260978699}]}, {"text": "The previous phrase-based (PBMT) architectures were complex and hard to diagnose, and Neural MT (NMT) systems, which dispense with any sort of symbolic representation of the learned knowledge, are probably worse in this respect.", "labels": [], "entities": []}, {"text": "Furthermore, the steady progress of MT engines makes automatic metrics such as BLEU () or METEOR () less appropriate to evaluate and compare modern NMT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9646840691566467}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9989180564880371}, {"text": "METEOR", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9882304668426514}]}, {"text": "To better understand the strength and weaknesses of these new architectures, it is thus necessary to investigate new, more focused, evaluation procedures.", "labels": [], "entities": []}, {"text": "Error analysis protocols, as proposed eg. by; Popovi\u00b4cPopovi\u00b4c and Ney (2011) for PBMT, are obvious candidates for such studies and have been used eg. in (.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6490078717470169}]}, {"text": "Recently, various new proposals have been put forward to better diagnose neural models, notably by;, who focus respectively on the syntactic competence of Neural Language Models (NLMs) or of NMT; and by;, who resuscitate an old tradition of designing test suites.", "labels": [], "entities": []}, {"text": "Inspired by these (and other) works (see \u00a7 4), we propose in this paper anew evaluation scheme aimed at specifically assessing the morphological competence of MT engines translating from English into a Morphologically Rich Language (MRL).", "labels": [], "entities": [{"text": "MT engines translating from English into a Morphologically Rich Language (MRL)", "start_pos": 159, "end_pos": 237, "type": "TASK", "confidence": 0.8266779596988971}]}, {"text": "Morphology poses two main types of problems in MT: (a) morphological variation in the source increases the occurrence of Out-ofVocabulary (OOV) source tokens, the translation of which is difficult to coin; (b) morphological variation in the target forces the MT to generate forms that have not been seen in training.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9935281872749329}, {"text": "MT", "start_pos": 259, "end_pos": 261, "type": "TASK", "confidence": 0.9277374744415283}]}, {"text": "Morphological complexity is alo often associated to more flexible word orderings, which is mostly a problem when translating from a MRL . Reducing these issues is a legitimate and important goal for many language pairs.", "labels": [], "entities": []}, {"text": "Our method for measuring the morphological competence of MT systems (detailed in \u00a7 2) is mainly based on the analysis of minimal pairs, each representing a contrast that is expressed syntactically in English and morphologically in the MRL.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9774925112724304}, {"text": "MRL", "start_pos": 235, "end_pos": 238, "type": "DATASET", "confidence": 0.8099736571311951}]}, {"text": "By comparing the automatic translations of these pairs, it is then possible to approximately assess whether a given MT system has succeeded in generating the correct word form, carrying the proper morphological marks.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9607062339782715}]}, {"text": "In \u00a7 3, we illustrate the potential of our evaluation protocol in a large-scale comparison of multiple MT engines having participated to the WMT'17 News Transla-tion tasks for the pairs English-Czech and EnglishLatvian.", "labels": [], "entities": [{"text": "WMT'17 News Transla-tion tasks", "start_pos": 141, "end_pos": 171, "type": "DATASET", "confidence": 0.9104696363210678}]}, {"text": "We finally relate our protocol to conventional metrics ( \u00a7 4), and conclude in \u00a7 5 by discussing possible extensions of this methodology, for instance to other (sets of) language pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have run the presented morphological evaluation 6 on several systems among which some were submitted at WMT'17.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 107, "end_pos": 113, "type": "DATASET", "confidence": 0.9721489548683167}]}, {"text": "The description of the latter can be found in the proceedings of the Second Conference on Machine Translation (2017a).", "labels": [], "entities": [{"text": "Machine Translation (2017a)", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.7888917922973633}]}, {"text": "We briefly summarize the types of systems included in the English-to-Czech study: \u2022 Phrase-based systems: The Moses baseline was trained on WMT'17 data and was not submitted at WMT'17.", "labels": [], "entities": [{"text": "Moses baseline", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.8572130501270294}, {"text": "WMT'17 data", "start_pos": 140, "end_pos": 151, "type": "DATASET", "confidence": 0.9748001098632812}, {"text": "WMT'17", "start_pos": 177, "end_pos": 183, "type": "DATASET", "confidence": 0.9835251569747925}]}, {"text": "UFAL Chimera 7 was submitted at WMT'16 and is described in ().", "labels": [], "entities": [{"text": "UFAL Chimera 7", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7694340944290161}, {"text": "WMT'16", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.923835277557373}]}, {"text": "\u2022 Word based NMT: NMT words is a system trained on WMT'17 parallel data with a target vocabulary of 80k tokens.", "labels": [], "entities": [{"text": "WMT'17 parallel data", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.837352991104126}]}, {"text": "It was not submitted at WMT'17 and is used for contrast.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.9151965379714966}, {"text": "contrast", "start_pos": 47, "end_pos": 55, "type": "TASK", "confidence": 0.9840598106384277}]}, {"text": "\u2022 where we report BLEU, BEER) and CharacTER (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9953953623771667}, {"text": "BEER", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9957448840141296}, {"text": "CharacTER", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.8047507405281067}]}, {"text": "We also computed a morphology accuracy for these systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9342750310897827}]}, {"text": "Using output-to-reference alignments produced by METEOR on lemmas, we: Scores of the English-to-Czech WMT'17 submissions on the official test set.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.6454991698265076}, {"text": "WMT'17", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.6355811357498169}, {"text": "official test set", "start_pos": 128, "end_pos": 145, "type": "DATASET", "confidence": 0.7543390989303589}]}, {"text": "checked whether aligned words shared the same form.", "labels": [], "entities": []}, {"text": "Our assumption is that two different forms associated to the same lemma correspond to two different inflections of the same lexeme, which allows us to locate positions that likely correspond to morphological errors.", "labels": [], "entities": []}, {"text": "lists the results for the A-set tests, which evaluate the morphological adequacy of the output wrt.", "labels": [], "entities": []}, {"text": "The last column provides the mean of all scores for one system.", "labels": [], "entities": []}, {"text": "We can note that all BPE-based NMT systems have a much higher performance than the phrase-based systems.", "labels": [], "entities": []}, {"text": "We explain the poor performance of the word-based NMT system by the use of a too small closed vocabulary: over the 18,500 sentences of the test suite, 12,016 unknown words were produced by this system.", "labels": [], "entities": []}, {"text": "However, when it comes to predicting the morphology of closed class words, this systems performs much better: the accuracy computed for pronoun gender and number is similar to the ones of best BPE-based systems.", "labels": [], "entities": [{"text": "predicting the morphology of closed class words", "start_pos": 26, "end_pos": 73, "type": "TASK", "confidence": 0.8409958396639142}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9990845918655396}]}, {"text": "As opposed to nouns and verbs (open classes), the set of pronouns in Czech is quite small; having observed all their inflections, the word-based system is in a better position to convey the target form.", "labels": [], "entities": []}, {"text": "Despite important differences in automatic metric scores between UEDIN NMT system and LIMSI FNMT, we see that the latter always outperforms the former, except for the feminine pronoun prediction.", "labels": [], "entities": [{"text": "automatic metric scores", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.8961572845776876}, {"text": "UEDIN NMT system", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.7813166975975037}, {"text": "LIMSI FNMT", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.6500203013420105}]}, {"text": "The overall morphological accuracies show that UEDIN NMT provides more similar word forms with the reference translation, but these global scores fail to show the higher adequacy performance of LIMSI FNMT highlighted in the A-set.", "labels": [], "entities": [{"text": "UEDIN NMT", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.7954347133636475}]}, {"text": "The results of the B-set evaluation for Czech are in and are an estimate of the morphological fluency of the output.", "labels": [], "entities": [{"text": "B-set", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.940997838973999}]}, {"text": "We observe here again that morphological phenomena such as agreement are better modeled by sequence-to-sequence models using BPE segmentation than phrase-based or word-based NMT systems.", "labels": [], "entities": [{"text": "BPE segmentation", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.651703268289566}]}, {"text": "The overall best performance of UEDIN and UFAL NMT has to be noted, since both outperform systems that explicitly model target morphology.", "labels": [], "entities": [{"text": "UEDIN", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8112192153930664}]}, {"text": "The results for the C-set for English-to-Czech are shown in.", "labels": [], "entities": []}, {"text": "We now observe that factored systems are less sensitive to lexical variations and make more stable morphological predictions.", "labels": [], "entities": []}, {"text": "The differences with the entropy values computed for the phrase-based systems are spectacular, especially for verbal morphology.", "labels": [], "entities": []}, {"text": "We understand this poor performance for phrase-based systems as a consequence of the initial assumption those systems rely on: the concatenation of phrases to constitute an output sentence does not help to provide a single morphological prediction in slightly various contexts.", "labels": [], "entities": []}, {"text": "As an attempt to evaluate the error margin of our evaluation results, we have run a manual check of our evaluation measures.", "labels": [], "entities": [{"text": "error margin", "start_pos": 30, "end_pos": 42, "type": "METRIC", "confidence": 0.9748603403568268}]}, {"text": "For this, we have taken all 500 sentence pairs reflecting past tense (A-set), as well as case (pronouns to nouns in B-set), and took translations from different systems randomly.", "labels": [], "entities": []}, {"text": "We report on cases where the modification of the source created a \"bad\" (meaningless or ungrammatical) variant, as well as sample translations erroneously considered successful or unsuccessful.", "labels": [], "entities": []}, {"text": "For past tense, we observe a low quantity of false positive (1.6%) and false negative (0.4%).", "labels": [], "entities": []}, {"text": "The ratio of bad sources is quite low as well (3%), and is mostly related to cases where a word was given the wrong analysis in the first place, such as a noun labeled by the PoS-tagger as a verb, which was then turned into a past form.", "labels": [], "entities": []}, {"text": "For pronouns to nouns, there are nearly no bad source sentences (0.2%): the transformation of pronouns into noun phrases is quite easy and safe.", "labels": [], "entities": []}, {"text": "While false positive labels are lower (0.2%), there is a higher amount of false positive (4.4%), which was mainly due to our word-based NMT system that generates many unknown words and presents important differences between base and variant: several adjectives and nouns, not corresponding to the ones we generated in the source sentence, could then be considered during the evaluation.", "labels": [], "entities": []}, {"text": "For English-to-Latvian, we have represented the same types of systems as for Czech, with an additional hybrid system.", "labels": [], "entities": []}, {"text": "The scores and mor-: Sentence pair evaluation for English-to-Czech (B-set).", "labels": [], "entities": [{"text": "mor", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9521620869636536}]}, {"text": "phological accuracies of the systems submitted at WMT'17 are in.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.8981342315673828}, {"text": "WMT'17", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.831256628036499}]}, {"text": "\u2022 Phrase-based systems: The Moses baseline was trained on WMT'17 data and TILDE PBMT was provided by TILDE 11 and is described in ).", "labels": [], "entities": [{"text": "Moses baseline", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9071868360042572}, {"text": "WMT'17 data", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9437867701053619}, {"text": "TILDE PBMT", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.8223520517349243}]}, {"text": "These systems did not take part in the official WMT'17 evaluation campaign.", "labels": [], "entities": [{"text": "WMT'17 evaluation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.754034161567688}]}, {"text": "\u2022 Word-based NMT: NMT words is a system trained on WMT'17 parallel data with a 80K target vocabulary.", "labels": [], "entities": [{"text": "WMT'17 parallel data", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.8448271552721659}]}, {"text": "It was not submitted at WMT'17 and is used here as a contrast.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.9158464074134827}]}, {"text": "\u2022 BPE-based NMT: LIMSI NMT () is based on NMTPY and UEDIN NMT (Sennrich et al., 2017a) on Nematus.", "labels": [], "entities": []}, {"text": "\u2022 NMT modeling target morphology: LIMSI FNMT ( and LIUM FNMT ( ) use a factored output predicting words and PoS.", "labels": [], "entities": [{"text": "NMT modeling target morphology", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.8586423844099045}]}, {"text": "\u2022 Hybrid system: TILDE hybrid is an ensemble of NMT models using a PBMT to process rare and unknown words.", "labels": [], "entities": []}, {"text": "It was submitted at WMT'17 (  The results for the A-set evaluation are in Table 9.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.9449306726455688}]}, {"text": "Compared to the previous Czech evaluation, there is a less clear difference between phrase-based and NMT systems based on BPE.", "labels": [], "entities": []}, {"text": "Indeed, TILDE hybrid has the best mean performance and is only 5 points above our Moses baseline.", "labels": [], "entities": [{"text": "TILDE", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.8265506625175476}]}, {"text": "A possible reason for that situation is the lower amount of parallel data available for English-Latvian, compared to English-Czech.", "labels": [], "entities": []}, {"text": "We notice that there is no significant difference between the two NMT systems and LIMSI FNMT.", "labels": [], "entities": [{"text": "LIMSI FNMT", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.6625618934631348}]}, {"text": "With this language pair, word-based NMT performs significantly worse than all other systems on all morphological features, which is confirmed by the fluency evaluation in.", "labels": [], "entities": []}, {"text": "Here, the factored systems tend to have a better verbal fluency, whereas NMT systems perform better on nominal agreement: LIMSI FNMT has the best mean score, but is only 0.2 points above UEDIN NMT.", "labels": [], "entities": [{"text": "LIMSI FNMT", "start_pos": 122, "end_pos": 132, "type": "METRIC", "confidence": 0.6915156841278076}, {"text": "UEDIN NMT", "start_pos": 187, "end_pos": 196, "type": "DATASET", "confidence": 0.8720529079437256}]}, {"text": "The best system, TILDE hybrid, is now 21.1 points above the Moses baseline, which again seems to be the main reason for such high overall morphological accuracy in. confirms the higher performance of NMT and factored NMT systems, with a clear advantage for TILDE hybrid, which has the best accuracy in terms of fluency, like in the previous Table 10, which tends to show some correlation between both types of tests.: Sentence pair evaluation for English-to-Latvian (A-set).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9478937983512878}, {"text": "accuracy", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.9869834184646606}]}, {"text": "When it comes to morphological correction of the output, our evaluation clearly shows the superiority of BPE-based NMT systems over phrasebased ones.", "labels": [], "entities": []}, {"text": "On the other hand, while we observed that factored models obtain a higher performance in terms of adequacy, NMT models are still very close to them in terms of fluency.", "labels": [], "entities": []}, {"text": "Finally, factored models, as well as TILDE hybrid, clearly showed more confidence in their predictions through slight lexical variations.", "labels": [], "entities": [{"text": "TILDE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.7460975646972656}]}], "tableCaptions": [{"text": " Table 4: Scores of the English-to-Czech WMT'17  submissions on the official test set.", "labels": [], "entities": [{"text": "official test set", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.786025325457255}]}, {"text": " Table 5: Sentence pair evaluation for English-to-Czech (A-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9065601229667664}]}, {"text": " Table 6: Sentence pair evaluation for English-to-Czech (B-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8976038694381714}]}, {"text": " Table 7: Sentence group evaluation for English-to-Czech with Entropy (C-set).", "labels": [], "entities": [{"text": "Sentence group evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8841493527094523}]}, {"text": " Table 8: Scores of the English-to-Latvian  WMT'17 submissions on the official test  set.", "labels": [], "entities": [{"text": "official test  set", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.7937087217966715}]}, {"text": " Table 9: Sentence pair evaluation for English-to-Latvian  (A-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9105179707209269}]}, {"text": " Table 10: Sentence pair evaluation for English-to-Latvian (B-set).", "labels": [], "entities": [{"text": "Sentence pair evaluation", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.9100624720255533}]}, {"text": " Table 11: Sentence group evaluation for English-to-Latvian with Entropy (C-set).", "labels": [], "entities": [{"text": "Sentence group evaluation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.88889213403066}]}]}