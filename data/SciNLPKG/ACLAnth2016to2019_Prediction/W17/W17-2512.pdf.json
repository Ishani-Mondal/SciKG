{"title": [{"text": "Overview of the Second BUCC Shared Task: Spotting Parallel Sentences in Comparable Corpora", "labels": [], "entities": [{"text": "BUCC Shared Task", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.5651187698046366}, {"text": "Spotting Parallel Sentences", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.8661253452301025}]}], "abstractContent": [{"text": "This paper presents the BUCC 2017 shared task on parallel sentence extraction from comparable corpora.", "labels": [], "entities": [{"text": "BUCC 2017 shared task", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.8626562505960464}, {"text": "parallel sentence extraction from comparable corpora", "start_pos": 49, "end_pos": 101, "type": "TASK", "confidence": 0.7153407831986746}]}, {"text": "It recalls the design of the datasets, presents their final construction and statistics and the methods used to evaluate system results.", "labels": [], "entities": []}, {"text": "13 runs were submitted to the shared task by 4 teams, covering three of the four proposed language pairs: French-English (7 runs), German-English (3 runs), and Chinese-English (3 runs).", "labels": [], "entities": []}, {"text": "The best F-scores as measured against the gold standard were 0.84 (German-English), 0.80 (French-English), and 0.43 (Chinese-English).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9946337938308716}]}, {"text": "Because of the design of the dataset, in which not all gold parallel sentence pairs are known, these are only minimum values.", "labels": [], "entities": []}, {"text": "We examined manually a small sample of the false negative sentence pairs for the most precise French-English runs and estimated the number of parallel sentence pairs not yet in the provided gold standard.", "labels": [], "entities": []}, {"text": "Adding them to the gold standard leads to revised estimates for the French-English F-scores of at most +1.5pt.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.6568756103515625}]}, {"text": "This suggests that the BUCC 2017 datasets provide a reasonable approximate evaluation of the parallel sentence spotting task.", "labels": [], "entities": [{"text": "BUCC 2017 datasets", "start_pos": 23, "end_pos": 41, "type": "DATASET", "confidence": 0.9810725649197897}, {"text": "parallel sentence spotting task", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.6711424738168716}]}], "introductionContent": [{"text": "Shared tasks and the associated datasets have proved their worth as a driving force in a number of subfields of Natural Language Processing.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 112, "end_pos": 139, "type": "TASK", "confidence": 0.6269479393959045}]}, {"text": "However, very few shared tasks were organized on the topic of comparable corpora.", "labels": [], "entities": []}, {"text": "Therefore, we endeavored to design and organize shared tasks as companions of the BUCC workshop series on Building and Using Comparable Corpora.", "labels": [], "entities": [{"text": "BUCC workshop series", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.9046168327331543}]}, {"text": "The First BUCC Shared Task () tackled the detection of comparable documents across languages.", "labels": [], "entities": [{"text": "BUCC Shared Task", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.4743511776129405}]}, {"text": "The Second BUCC Shared Task, 1 presented here, addresses the detection of parallel sentences across languages in nonaligned, monolingual corpora.", "labels": [], "entities": [{"text": "BUCC Shared Task", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.5555063883463541}, {"text": "detection of parallel sentences across languages in nonaligned, monolingual corpora", "start_pos": 61, "end_pos": 144, "type": "TASK", "confidence": 0.7146714763207869}]}, {"text": "Let us recall the overall goals, design and principles of this task, which were introduced in (.", "labels": [], "entities": []}, {"text": "A bottleneck in statistical machine translation is the scarceness of parallel resources for many language pairs and domains.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.6994146307309469}]}, {"text": "Previous research has shown that this bottleneck can be reduced by utilizing parallel portions found within comparable corpora ().", "labels": [], "entities": []}, {"text": "These are useful for many purposes, including automatic terminology extraction and the training of statistical MT systems.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.8429730236530304}, {"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.7529389262199402}]}, {"text": "However, past work relied on metainformation, such as the publication date of news articles or inter-language links in Wikipedia documents, to help select promising sentence pairs before examining them more thoroughly.", "labels": [], "entities": []}, {"text": "It is therefore difficult to separate the heuristic part of the methods that deals with this meta-information in clever ways from the cross-language part of the methods that deals with translation and comparability issues.", "labels": [], "entities": []}, {"text": "We consider that the latter type of methods is more fundamental and wanted to focus on its evaluation.", "labels": [], "entities": []}, {"text": "We thus designed a task in which no meta-information is available on the relation between the two monolingual corpora in which pairs of translated sentences are to be found.", "labels": [], "entities": []}, {"text": "In) we showed the difference of this task to PAN's cross-language plagiarism detection, SemEval's cross-language semantic text similarity, and WMT's bilingual document alignment.", "labels": [], "entities": [{"text": "cross-language plagiarism detection", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.6639809310436249}, {"text": "bilingual document alignment", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.5483831862608591}]}, {"text": "The present paper reports the actual organization of the task as a companion to the BUCC 2017 workshop.", "labels": [], "entities": [{"text": "BUCC 2017 workshop", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.9241796731948853}]}, {"text": "We describe the final method we used to prepare bilingual corpora in four language pairs: Chinese-English, French-English, GermanEnglish, and Russian-English (Section 2), the evaluation method (Section 3), the participants' systems (Section 4), the results they obtained (Section 5), and conclude (Section 6).", "labels": [], "entities": [{"text": "conclude", "start_pos": 288, "end_pos": 296, "type": "METRIC", "confidence": 0.9844735860824585}]}], "datasetContent": [{"text": "Given two sentence-split monolingual corpora, participant systems were expected to identify pairs of sentences that are translations of each other.", "labels": [], "entities": []}, {"text": "Each team was allowed to submit up to three runs per language pair.", "labels": [], "entities": []}, {"text": "Evaluation was performed using balanced Fscore.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.924905002117157}]}, {"text": "In the results of a system, a true positive TP is a pair of sentences that is present in the gold standard and a false positive FP is a pair of sentences that is not present in the gold standard.", "labels": [], "entities": [{"text": "FP", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.978457510471344}]}, {"text": "A false negative FN is a pair of sentences present in the gold standard but absent from system results.", "labels": [], "entities": [{"text": "FN", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.6456130743026733}]}, {"text": "Precision, Recall and F1-score were then computed using the usual formulas.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9935838580131531}, {"text": "Recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9965031147003174}, {"text": "F1-score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9994087219238281}]}, {"text": "Of note, this evaluation is performed on the synthetic corpus presented above, using the inserted parallel sentence pairs as the gold standard.", "labels": [], "entities": []}, {"text": "Therefore it does not take into account the possible existence of true parallel pairs present in the monolingual corpora beyond the inserted sentence pairs.", "labels": [], "entities": []}, {"text": "By avoiding aligned Wikipedia articles, the construction of the corpus attempted to reduce the likelihood of such sentence pairs, but indeed it did not suppress it altogether.", "labels": [], "entities": []}, {"text": "For these reasons we also performed a limited experiment in which human judges evaluated selected samples of the system results.", "labels": [], "entities": []}, {"text": "The assessment of each sentence pair was performed according to the guidelines of the SemEval 2016 cross-language sentence similarity task ().", "labels": [], "entities": [{"text": "SemEval 2016 cross-language sentence similarity task", "start_pos": 86, "end_pos": 138, "type": "TASK", "confidence": 0.8105700214703878}]}, {"text": "We present here the evaluation results for the submitted runs for each language in turn.", "labels": [], "entities": []}, {"text": "As explained above, these results are based on the artificially inserted translation pairs.", "labels": [], "entities": []}, {"text": "In each table we show the precision, recall and F1-score of each run in percentages.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9998273849487305}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9995361566543579}, {"text": "F1-score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9996471405029297}]}, {"text": "Because this synthetic dataset represents an approximation of areal task, there is no point in computing precise scores: we round the computed percentages to the nearest integer.", "labels": [], "entities": []}, {"text": "Additionally, we observed that some participants took into account the prior probability of translation pairs in the training datasets.", "labels": [], "entities": []}, {"text": "Given that the test dataset was announced to be generated in the same way as the training dataset, they targeted a number of translation pairs in the test that was consistent with this prior probability.", "labels": [], "entities": []}, {"text": "We therefore display this number of translation pairs in the tables too.", "labels": [], "entities": []}, {"text": "Three teams submitted runs on the FrenchEnglish (fr-en) language pair.", "labels": [], "entities": [{"text": "FrenchEnglish (fr-en) language pair", "start_pos": 34, "end_pos": 69, "type": "DATASET", "confidence": 0.9440097113450369}]}, {"text": "In addition to these runs, presents the minimum, maximum, median, mean and standard deviation for each measure.", "labels": [], "entities": []}, {"text": "The initial JUNLP submission had a bug which was fixed a couple of days later; we show the results of the fixed submission in italics, but did not include it in the additional statistics.", "labels": [], "entities": [{"text": "JUNLP submission", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.8862497210502625}]}, {"text": "The VIC results confirm the strategy described in run name sys n P (%) R (%) F1: Evaluation of fr-en runs (n gold=9,043) () by which they optimized VIC1 for F1-score, VIC2 for precision, and VIC3 for recall; the results for German also display the same pattern.", "labels": [], "entities": [{"text": "F1", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9938333034515381}, {"text": "VIC1", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.8947106003761292}, {"text": "F1-score", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9803794622421265}, {"text": "precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9897068738937378}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9982650876045227}]}, {"text": "The three runs RALI2, RALI1 and RALI3 produce an increasing number of candidate pairs, resulting in a decrease in precision; this leads to an increase in recall only for RALI1, but always to a decrease in F1-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9994276165962219}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9994161128997803}, {"text": "F1-score", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9985172152519226}]}, {"text": "Reasons for the lower precisions and (to a lesser extent) recalls of the RALI results are proposed in, including the handling of numbers (improved in their later experiments) and the selection of negative training examples.", "labels": [], "entities": [{"text": "precisions", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9986897110939026}, {"text": "recalls", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9967315196990967}, {"text": "RALI", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.5517226457595825}]}, {"text": "Only one team submitted runs on the GermanEnglish (de-en) language pair, therefore we do not report min, max and other statistics.", "labels": [], "entities": [{"text": "GermanEnglish (de-en) language pair", "start_pos": 36, "end_pos": 71, "type": "DATASET", "confidence": 0.9144560595353445}, {"text": "min", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9584670662879944}]}, {"text": "The results are displayed in.", "labels": [], "entities": []}, {"text": "The precisions and run name sys n P (%) R (%) F1 (%) VIC1.: Evaluation of de-en runs (n gold=9,550) F1-scores obtained by VIC for German-English are higher than those they obtained for FrenchEnglish, with similar recalls.", "labels": [], "entities": [{"text": "precisions", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9986435770988464}, {"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9807881116867065}, {"text": "VIC1.", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.6145860552787781}, {"text": "F1-scores", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9885643124580383}]}, {"text": "The only difference in the two corpora in terms of statistics is that the German-English dataset was more balanced in its numbers of monolingual sentences, but other differences linked to the intrinsic properties of German and French or to the resources used to train the system for these two languages are likely to have an effect too.", "labels": [], "entities": [{"text": "German-English dataset", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.7343417555093765}]}, {"text": "One team submitted runs on the ChineseEnglish (zh-en) language pair, therefore we do not report min, max and other statistics.", "labels": [], "entities": [{"text": "ChineseEnglish (zh-en) language pair", "start_pos": 31, "end_pos": 67, "type": "DATASET", "confidence": 0.8568185667196909}, {"text": "min", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9375624060630798}]}, {"text": "The results are displayed in  Zweigenbaum, 2017), zNLP3 was optimized for precision: this is confirmed by its results on the test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9992297887802124}]}, {"text": "Overall, the results are lower than the best runs on the fr-en and de-en datasets.", "labels": [], "entities": [{"text": "fr-en and de-en datasets", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.760707676410675}]}, {"text": "Various hypotheses can be proposed to account for this difference, including the different types and sizes of the resources used for translation in VIC and zNLP, the specific methods used in the two systems, and differences in intrinsic language properties.", "labels": [], "entities": []}, {"text": "Were we to know which 'natural' translation pairs existed in the test datasets beyond the translation pairs we inserted, would the results be very different?", "labels": [], "entities": []}, {"text": "We did not have resources to perform an extensive human evaluation to answer this question, therefore we designed a minimal experiment on the French-English language pair.", "labels": [], "entities": []}, {"text": "In the VIC and RALI runs, we selected the run with the best precision and randomly drew 20-pair samples.", "labels": [], "entities": [{"text": "VIC", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.5791149139404297}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9952530860900879}]}, {"text": "To check agreement, the first two 20-pair samples were scored by a second French native speaker.", "labels": [], "entities": []}, {"text": "Besides, in a few situations, the first judge was sometimes unsure whether to give a score or the next higher score.", "labels": [], "entities": []}, {"text": "In these situations, he entered two alternate scores: this created a second series of judgments which differed only in a few places.", "labels": [], "entities": []}, {"text": "Altogether, five batches were examined: three for VIC and two for RALI, and for each batch, we had two series of judgments.", "labels": [], "entities": [{"text": "VIC", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.43249276280403137}, {"text": "RALI", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.5878214240074158}]}, {"text": "For VIC, we sampled 60 sentence pairs from the 978 false positives of the most precise run, Run 2.", "labels": [], "entities": [{"text": "VIC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6476413011550903}]}, {"text": "Out of these sentence pairs, 3-5 were considered as perfect translations (grade 5) and an additional 8-13 were judged as near-perfect translations (grade 4).", "labels": [], "entities": []}, {"text": "From this we computed four increasingly lenient evaluations based upon the minimum and maximum numbers of perfect translations (5 min, 5 max) and upon the minimum and maximum numbers of perfect or near-perfect translations (4-5 min, 4-5 max).", "labels": [], "entities": []}, {"text": "We converted these counts into percentages of the examined false positives that were judged as true translations (T%FP).", "labels": [], "entities": [{"text": "T%FP)", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9159426093101501}]}, {"text": "We then extrapolated these percentages to the whole set of false positives to obtain the number of humanjudged true positives that should be added to the automatically evaluated true positives (+TP).", "labels": [], "entities": []}, {"text": "We used this additional number to recompute the true positives and the associated precision (P').", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9821978211402893}]}, {"text": "Recall cannot be recomputed this way, because to estimate the recall for both automatic and 'natural' translation pairs, we would need to draw a sample from the full test corpus, and given the low prevalence of 'natural' translation pairs, this sample should be quite large.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9991304278373718}]}, {"text": "For information we also recomputed the F1-score (F1', still without changing the recall).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9992465972900391}, {"text": "F1", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9895601272583008}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9989598989486694}]}, {"text": "We observe that precision is reevaluated with an increase of up to 4pt, whereas F1-score gains up to 1.5pt.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9997656941413879}, {"text": "F1-score", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9989952445030212}]}, {"text": "This difference cannot be ignored fora precise evaluation, but does not bring drastic changes to the overall conclusions of the shared task.", "labels": [], "entities": []}, {"text": "For RALI, we sampled 40 sentence pairs from the 41,865 false positives of the most precise run, Run 2.", "labels": [], "entities": [{"text": "RALI", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.842216968536377}]}, {"text": "Out of these sentence pairs, none was considered as perfect translations nor near-perfect translations (most were related though).", "labels": [], "entities": []}, {"text": "This is consistent with the fact that RALI2's precision was seven times lower than that of VIC2: a much larger sample might be needed to evidence 'natural' translation pairs in RALI2's output.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9874087572097778}, {"text": "VIC2", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.8102847337722778}]}, {"text": "This limited experiment suggests that 'natural' translation pairs are much less frequent in the French-English test set than our artificially inserted translation pairs (or that the VIC2 system is much better at spotting the inserted translation pairs than 'natural' translation pairs): shows that out of 7569 sentence pairs proposed by VIC2, 87% were inserted translation pairs and between 0.6% and 4% were 'natural' translation pairs.", "labels": [], "entities": [{"text": "French-English test set", "start_pos": 96, "end_pos": 119, "type": "DATASET", "confidence": 0.749249130487442}, {"text": "VIC2", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.9199979901313782}]}, {"text": "This would extrapolate to a rate of less than 5% of 'natural' translation pairs among the total translation pairs in the corpus.", "labels": [], "entities": []}, {"text": "An important limitation of this experiment is that it examined only a limited sample of sentence pairs, which entails large confidence intervals around the reported values.", "labels": [], "entities": []}, {"text": "To compute these confidence intervals, we would need to know more or to make hypotheses about the distribution of 'natural' translation pairs not only in the systemreturned sets of sentence pairs, but also outside these sets, which would require more time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics: number of monolingual sentences (fr, en) and of parallel pairs (gold) for each  split and each language pair. The fr column stands for the non-English language in each pair.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of fr-en runs (n gold=9,043)", "labels": [], "entities": []}, {"text": " Table 4. The precisions and", "labels": [], "entities": [{"text": "precisions", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9992595314979553}]}, {"text": " Table 5. According to (Zhang and", "labels": [], "entities": []}, {"text": " Table 5: Evaluation of zh-en runs (n gold=1,896)", "labels": [], "entities": []}, {"text": " Table 6: Re-evaluation of precision for VIC's  Run 2. 'T%FP' is the percentage of human- assessed good translations in the false positives.", "labels": [], "entities": [{"text": "Re-evaluation", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9595462083816528}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.990727424621582}, {"text": "VIC's  Run", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.7342777252197266}, {"text": "T%FP'", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.850962683558464}]}]}