{"title": [{"text": "The Effect of Translationese on Tuning for Statistical Machine Translation", "labels": [], "entities": [{"text": "Translationese", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9767624735832214}, {"text": "Statistical Machine Translation", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.7648685574531555}]}], "abstractContent": [{"text": "We explore how the translation direction in the tuning set used for statistical machine translation affects the translation results.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6356554826100668}]}, {"text": "We explore this issue for three language pairs.", "labels": [], "entities": []}, {"text": "While the results on different metrics are somewhat conflicting, using tuning data translated in the same direction as the translation systems tends to give the best length ratio and Meteor scores for all language pairs.", "labels": [], "entities": [{"text": "length ratio", "start_pos": 166, "end_pos": 178, "type": "METRIC", "confidence": 0.9279273748397827}, {"text": "Meteor", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.920120894908905}]}, {"text": "This tendency is confirmed in a small human evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translationese is a term that is used to describe the special characteristics of translated texts, as opposed to originally authored tests.", "labels": [], "entities": [{"text": "Translationese", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9528511166572571}]}, {"text": "Translations are different from original texts, which can be due both to influences from the source language and as a result of the translation process itself.", "labels": [], "entities": [{"text": "Translations", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.952284574508667}]}, {"text": "For instance, texts that are translated tends to have shorter sentences and a lower type/token ratio than original texts, and explicitate information, for instance by using more cohesive markers than in original texts.", "labels": [], "entities": []}, {"text": "Several studies have shown that it is possible to use text classification techniques to distinguish between original and translated texts with high accuracy (, further supporting that there is a clear difference between original and translated texts.", "labels": [], "entities": [{"text": "text classification", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7496001124382019}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.997368574142456}]}, {"text": "However, the domain of the text interacts to a high degree with translationese identification.", "labels": [], "entities": [{"text": "translationese identification", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.9526119828224182}]}, {"text": "Translationese has been shown to have an effect in relation to the training of statistical machine translation (SMT) systems, where the best results are seen when the texts used for training the SMT system have been translated in the same direction as that of the SMT system.", "labels": [], "entities": [{"text": "Translationese", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9407650232315063}, {"text": "statistical machine translation (SMT)", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.7822971592346827}, {"text": "SMT", "start_pos": 195, "end_pos": 198, "type": "TASK", "confidence": 0.9556686282157898}, {"text": "SMT", "start_pos": 264, "end_pos": 267, "type": "TASK", "confidence": 0.9546447992324829}]}, {"text": "This has been shown both for the translation model (TM) ( and for the language model (LM) for which it is better to use translated than original texts).", "labels": [], "entities": []}, {"text": "It works nearly as well to use predicted translationese as known translationese, both for the LM and TM (.", "labels": [], "entities": []}, {"text": "It has also been noted that the original language of the test sentences influences the Bleu score of translations.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9749292135238647}]}, {"text": "Besides the data used for the LM and TM, another important text for SMT training is the data used for tuning.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.9190831780433655}, {"text": "tuning", "start_pos": 102, "end_pos": 108, "type": "TASK", "confidence": 0.9572964906692505}]}, {"text": "The tuning set is used for tuning, or optimizing, the log-linear feature weights of the models, such as TM, LM, and reordering models.", "labels": [], "entities": []}, {"text": "It is small compared to the other training data, and usually contains a couple of thousands of sentences, as opposed to millions of sentences for the LM and TM.", "labels": [], "entities": []}, {"text": "It is supposed to be representative of the test set.", "labels": [], "entities": []}, {"text": "To the best of our knowledge the effect of translationese has not previously been studied with respect to the tuning set.", "labels": [], "entities": [{"text": "translationese", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.9608485102653503}]}, {"text": "We investigate the effect of the translation direction in the tuning text.", "labels": [], "entities": []}, {"text": "We explore this for translation between English on one side, and German, French, and Czech on the other side, for the news domain.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9712968468666077}]}, {"text": "There is a tendency that tuning in the same direction as the SMT system performs best, especially as measured by length ratio and Meteor.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.986401379108429}, {"text": "length ratio", "start_pos": 113, "end_pos": 125, "type": "METRIC", "confidence": 0.9002005755901337}, {"text": "Meteor", "start_pos": 130, "end_pos": 136, "type": "DATASET", "confidence": 0.9730865955352783}]}], "datasetContent": [{"text": "To facilitate presentation we will use the abbreviations O for original texts and T for translated texts, and the term foreign to represent either of the languages German, French, and Czech.", "labels": [], "entities": [{"text": "O", "start_pos": 57, "end_pos": 58, "type": "METRIC", "confidence": 0.9139081239700317}]}, {"text": "In much of the work on translationese, with the exception of, only Bleu () has been used for evaluation.", "labels": [], "entities": [{"text": "translationese", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.9682156443595886}, {"text": "Bleu", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.921340823173523}]}, {"text": "Bleu has its limitations though, and to give a somewhat more thorough evaluation we also show results on Meteor (Denkowski and Lavie, 2010) and TER ().", "labels": [], "entities": [{"text": "Meteor", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9369341135025024}, {"text": "TER", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.6359521746635437}]}, {"text": "These metrics capture somewhat different aspects of MT quality.", "labels": [], "entities": [{"text": "MT quality", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.8312892019748688}]}, {"text": "Bleu is mainly based on the precision of n-grams up to length 4, and thus rewards local fluency highly.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9769105911254883}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9991728663444519}]}, {"text": "Meteor is based on a weighted F-score on unigrams, with a matching step that consider word forms, stems, synonyms (for English), and paraphrases with different weights for content and function words, and a fragmentation score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9913697242736816}]}, {"text": "It is thus less sensitive than Bleu to allowable linguistic variation.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.9289100766181946}]}, {"text": "Meteor is also tuned for different target languages, to increase correlation with human evaluation scores.", "labels": [], "entities": []}, {"text": "TER is an extension of the Levenshtein distance, with the addition of a shift operation to account for movement.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9841307997703552}, {"text": "Levenshtein distance", "start_pos": 27, "end_pos": 47, "type": "METRIC", "confidence": 0.6189734041690826}]}, {"text": "Like Bleu, TER only considers exact word form matches.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.7427326440811157}, {"text": "TER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.6372252702713013}]}, {"text": "We also give the length ratio (LR), counted as the number of words, of the translation hypothesis relative to the reference text.", "labels": [], "entities": [{"text": "length ratio (LR)", "start_pos": 17, "end_pos": 34, "type": "METRIC", "confidence": 0.9756381869316101}]}, {"text": "In addition we perform a small human evaluation on a sample of segments for German\u2192English translation.", "labels": [], "entities": [{"text": "German\u2192English translation", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.5288305208086967}]}, {"text": "For each setting, we randomly picked 100 segments of length 10-15 words.", "labels": [], "entities": []}, {"text": "One annotator compared the output from two systems for overall quality.", "labels": [], "entities": []}, {"text": "Using only short segments can introduce a bias, since they might not be representative for all segments (Stymne and Ahrenberg, 2012), but it has the trade-off of being much faster and more consistent.", "labels": [], "entities": []}, {"text": "shows the results on the O\u2192T test set.", "labels": [], "entities": [{"text": "O\u2192T test set", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.570356810092926}]}, {"text": "The scores are obviously different for the different language pairs, which are due to both differences between the languages, differences in the use of training data and factors in the SMT systems, and for from-foreign, different test sets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 185, "end_pos": 188, "type": "TASK", "confidence": 0.9864502549171448}]}], "tableCaptions": [{"text": " Table 1: Ratio of foreign to English words for sets  with different original language.", "labels": [], "entities": []}, {"text": " Table 2: Metric scores and length ratio on the O\u2192T test set.", "labels": [], "entities": [{"text": "Metric", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9179182648658752}, {"text": "length ratio", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9750974476337433}, {"text": "O\u2192T test set", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.5779624342918396}]}, {"text": " Table 3: Metric scores and length ratio on the mixed test set.", "labels": [], "entities": [{"text": "Metric", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9342698454856873}, {"text": "length ratio", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9718950390815735}, {"text": "mixed test set", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.7136626640955607}]}, {"text": " Table 4: Human comparison of O\u2192T and T\u2192O  tuning for German-English O\u2192T test set.", "labels": [], "entities": [{"text": "German-English O\u2192T test set", "start_pos": 54, "end_pos": 81, "type": "DATASET", "confidence": 0.6364770382642746}]}, {"text": " Table 5: Human comparison of O\u2192T and custom  tuning for German-English mixed test set.", "labels": [], "entities": [{"text": "German-English mixed test set", "start_pos": 57, "end_pos": 86, "type": "DATASET", "confidence": 0.7329735830426216}]}]}