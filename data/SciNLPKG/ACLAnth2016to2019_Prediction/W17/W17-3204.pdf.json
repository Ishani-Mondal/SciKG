{"title": [{"text": "Six Challenges for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.8601039250691732}]}], "abstractContent": [{"text": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.666788250207901}, {"text": "word alignment", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.7825458645820618}, {"text": "beam search", "start_pos": 148, "end_pos": 159, "type": "TASK", "confidence": 0.7704156935214996}]}, {"text": "We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.6245114728808403}]}], "introductionContent": [{"text": "Neural machine translation has emerged as the most promising machine translation approach in recent years, showing superior performance on public benchmarks ( ) and rapid adoption in deployments by, e.g.,), Systran (, and WIPO.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.779745856920878}, {"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7278149873018265}, {"text": "WIPO", "start_pos": 222, "end_pos": 226, "type": "DATASET", "confidence": 0.9328537583351135}]}, {"text": "But there have also been reports of poor performance, such as the systems built under low-resource conditions in the DARPA LORELEI program.", "labels": [], "entities": [{"text": "DARPA LORELEI program", "start_pos": 117, "end_pos": 138, "type": "DATASET", "confidence": 0.6290908058484396}]}, {"text": "In this paper, we examine a number of challenges to neural machine translation (NMT) and give empirical results on how well the technology currently holds up, compared to traditional statistical machine translation (SMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.8375680247942606}, {"text": "statistical machine translation (SMT)", "start_pos": 183, "end_pos": 220, "type": "TASK", "confidence": 0.8058704336484274}]}, {"text": "We find that: 1.", "labels": [], "entities": []}, {"text": "NMT systems have lower quality out of domain, to the point that they completely sacrifice adequacy for the sake of fluency.", "labels": [], "entities": []}, {"text": "2. NMT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings, but better performance in highresource settings.", "labels": [], "entities": []}, {"text": "3. NMT systems that operate at the sub-word level (e.g. with byte-pair encoding) perform better than SMT systems on extremely lowfrequency words, but still show weakness in translating low-frequency words belonging to highly-inflected categories (e.g. verbs).", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9702591300010681}]}, {"text": "4. NMT systems have lower translation quality on very long sentences, but do comparably better up to a sentence length of about 60 words.", "labels": [], "entities": []}, {"text": "5. The attention model for NMT does not always fulfill the role of a word alignment model, but may in fact dramatically diverge.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7335023134946823}]}, {"text": "6. Beam search decoding only improves translation quality for narrow beams and deteriorates when exposed to a larger search space.", "labels": [], "entities": [{"text": "Beam search decoding", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.9002264936765035}]}, {"text": "We note a 7th challenge that we do not examine empirically: NMT systems are much less interpretable.", "labels": [], "entities": []}, {"text": "The answer to the question of why the training data leads these systems to decide on specific word choices during decoding is buried in large matrices of real-numbered values.", "labels": [], "entities": []}, {"text": "There is a clear need to develop better analytics for NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8763580322265625}]}, {"text": "Other studies have looked at the comparable performance of NMT and SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9854200482368469}]}, {"text": "considered different linguistic categories for English-German and compared different broad aspects such as fluency and reordering for nine language directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use common toolkits for neural machine translation (Nematus) and traditional phrase-based statistical machine translation (Moses) with common data sets, drawn from WMT and OPUS.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.7572221954663595}, {"text": "phrase-based statistical machine translation", "start_pos": 80, "end_pos": 124, "type": "TASK", "confidence": 0.6154557019472122}, {"text": "WMT", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.917807400226593}, {"text": "OPUS", "start_pos": 175, "end_pos": 179, "type": "DATASET", "confidence": 0.8575778603553772}]}], "tableCaptions": [{"text": " Table 1: Corpora used to train domain-specific  systems, taken from the OPUS repository. IT  corpora are GNOME, KDE, PHP, Ubuntu, and  OpenOffice.", "labels": [], "entities": [{"text": "OPUS repository", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9359816610813141}]}, {"text": " Table 2: Breakdown of the first 100 tokens that  were unobserved in training or observed once in  training, by hand-annotated category.", "labels": [], "entities": []}]}