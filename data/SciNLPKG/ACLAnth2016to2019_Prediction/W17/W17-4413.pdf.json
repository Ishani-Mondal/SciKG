{"title": [], "abstractContent": [{"text": "We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers.", "labels": [], "entities": []}, {"text": "Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options.", "labels": [], "entities": []}, {"text": "Our method addresses these problems by lever-aging a large corpus of domain-specific text and a small set of existing questions.", "labels": [], "entities": []}, {"text": "It produces model suggestions for document selection and answer distractor choice which aid the human question generation process.", "labels": [], "entities": [{"text": "document selection", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7819597125053406}, {"text": "answer distractor choice", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8615008989969889}, {"text": "human question generation", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.6291995545228323}]}, {"text": "With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions.", "labels": [], "entities": []}, {"text": "1 We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions.", "labels": [], "entities": []}, {"text": "When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9992732405662537}]}], "introductionContent": [{"text": "The construction of large, high-quality datasets has been one of the main drivers of progress in NLP.", "labels": [], "entities": []}, {"text": "The recent proliferation of datasets for textual entailment, reading comprehension and Question Answering (QA)) has allowed for advances on these tasks, particularly with neural models ( * Work done while at the Allen Institute for Artificial Intelligence.", "labels": [], "entities": [{"text": "Question Answering (QA))", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.8444388031959533}]}, {"text": "1 Dataset available at http://allenai.org/data.", "labels": [], "entities": []}, {"text": "These recent datasets cover broad and general domains, but progress on these datasets has not translated into similar improvements in more targeted domains, such as science exam QA.", "labels": [], "entities": [{"text": "science exam QA", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.4652279814084371}]}, {"text": "Science exam QA is a high-level NLP task which requires the mastery and integration of information extraction, reading comprehension and commonsense reasoning.", "labels": [], "entities": [{"text": "Science exam QA", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.5253794193267822}, {"text": "information extraction", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.7447741031646729}, {"text": "commonsense reasoning", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.837031900882721}]}, {"text": "Consider, for example, the question \"With which force does the moon affect tidal movements of the oceans?\".", "labels": [], "entities": []}, {"text": "To solve it, a model must possess an abstract understanding of natural phenomena and apply it to new questions.", "labels": [], "entities": []}, {"text": "This transfer of general and domain-specific background knowledge into new scenarios poses a formidable challenge, one which modern statistical techniques currently struggle with.", "labels": [], "entities": []}, {"text": "Ina recent Kaggle competition addressing 8 th grade science questions (, the highest scoring systems achieved only 60% on a multiple choice test, with retrieval-based systems far outperforming neural systems.", "labels": [], "entities": []}, {"text": "A major bottleneck for applying sophisticated statistical techniques to science QA is the lack of large in-domain training sets.", "labels": [], "entities": []}, {"text": "Creating a large, multiple choice science QA dataset is challenging, since crowd workers cannot be expected to have domain expertise, and questions can lack relevance and diversity in structure and content.", "labels": [], "entities": []}, {"text": "Furthermore, poorly chosen answer distractors in a multiple choice setting can make questions almost trivial to solve.", "labels": [], "entities": []}, {"text": "The first contribution of this paper is a general method for mitigating the difficulties of crowdsourcing QA data, with a particular focus on multiple choice science questions.", "labels": [], "entities": []}, {"text": "The method is broadly similar to other recent work But Coriolis makes them blow northeast to southwest or the reverse in the Northern Hemisphere.", "labels": [], "entities": []}, {"text": "The winds blow northwest to southeast or the reverse in the southern hemisphere.", "labels": [], "entities": []}, {"text": "Summary Changes of state are examples of phase changes, or phase transitions.", "labels": [], "entities": []}, {"text": "All phase changes are accompanied by changes in the energy of a system.", "labels": [], "entities": []}, {"text": "Changes from a moreordered state to a less-ordered state (such as a liquid to a gas) are endothermic.", "labels": [], "entities": []}, {"text": "Changes from a less-ordered state to a moreordered state (such as a liquid to a solid) are always exothermic.", "labels": [], "entities": []}, {"text": "All radioactive decay is dangerous to living things, but alpha decay is the least dangerous.", "labels": [], "entities": []}, {"text": "workers a passage of text and having them ask a question about it.", "labels": [], "entities": []}, {"text": "However, unlike previous dataset construction tasks, we (1) need domainrelevant passages and questions, and (2) seek to create multiple choice questions, not directanswer questions.", "labels": [], "entities": []}, {"text": "We use a two-step process to solve these problems, first using a noisy classifier to find relevant passages and showing several options to workers to select from when generating a question.", "labels": [], "entities": []}, {"text": "Second, we use a model trained on real science exam questions to predict good answer distractors given a question and a correct answer.", "labels": [], "entities": []}, {"text": "We use these predictions to aid crowd workers in transforming the question produced from the first step into a multiple choice question.", "labels": [], "entities": []}, {"text": "Thus, with our methodology we leverage existing study texts and science questions to obtain new, relevant questions and plausible answer distractors.", "labels": [], "entities": []}, {"text": "Consequently, the human intelligence task is shifted away from a purely generative task (which is slow, difficult, expensive and can lack diversity in the outcomes when repeated) and reframed in terms of a selection, modification and validation task (being faster, easier, cheaper and with content variability induced by the suggestions provided).", "labels": [], "entities": []}, {"text": "The second contribution of this paper is a dataset constructed by following this methodology.", "labels": [], "entities": []}, {"text": "With a total budget of $10,415, we collected 13,679 multiple choice science questions, which we call SciQ.", "labels": [], "entities": []}, {"text": "shows the first four training examples in SciQ.", "labels": [], "entities": []}, {"text": "This dataset has a multiple choice version, where the task is to select the correct answer using whatever background information a system can find given a question and several answer options, and a direct answer version, where given a passage and a question a system must predict the span within the passage that answers the question.", "labels": [], "entities": []}, {"text": "With experiments using recent state-ofthe-art reading comprehension methods, we show that this is a useful dataset for further research.", "labels": [], "entities": []}, {"text": "Interestingly, neural models do not beat simple information retrieval baselines on the multiple choice version of this dataset, leaving room for research on applying neural models in settings where training examples number in the tens of thousands, instead of hundreds of thousands.", "labels": [], "entities": []}, {"text": "We also show that using SciQ as an additional source of training data improves performance on real 4 th and 8 th grade exam questions, proving that our method successfully produces useful in-domain training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our method for crowdsourcing science exam questions.", "labels": [], "entities": [{"text": "crowdsourcing science exam questions", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.8020351529121399}]}, {"text": "The method is a two-step process: first we present a set of candidate passages to a crowd worker, letting the worker choose one of the passages and ask a question about it.", "labels": [], "entities": []}, {"text": "Second, another worker takes the question and answer generated in the first step and produces three distractors, aided by a model trained to predict good answer distractors.", "labels": [], "entities": []}, {"text": "The end result is a multiple choice science question, consisting of a question q, a passage p, a correct answer a \u21e4 , and a set of distractors, or incorrect answer options, {a 0 }.", "labels": [], "entities": []}, {"text": "Some example questions are shown in Figure 1.", "labels": [], "entities": []}, {"text": "The remainder of this section elaborates on the two steps in our question generation process.", "labels": [], "entities": [{"text": "question generation process", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.8457473715146383}]}, {"text": "SciQ has a total of 13,679 multiple choice questions.", "labels": [], "entities": []}, {"text": "We randomly shuffled this dataset and split it into training, validation and test portions, with 1000 questions in each of the validation and test portions, and the remainder in train.", "labels": [], "entities": []}, {"text": "In we show the distribution of question and answer lengths in the data.", "labels": [], "entities": []}, {"text": "For the most part, questions and answers in the dataset are relatively short, though there are some longer questions.", "labels": [], "entities": []}, {"text": "Each question also has an associated passage used when generating the question.", "labels": [], "entities": []}, {"text": "Because the multiple choice question is trivial to answer when given the correct passage, the multiple choice version of SciQ does not include the passage; systems must retrieve their own background knowledge when answering the question.", "labels": [], "entities": []}, {"text": "Because we have the associated passage, we additionally created a direct-answer version of SciQ, which has the passage and the question, but no answer options.", "labels": [], "entities": []}, {"text": "A small percentage of the passages were obtained from unreleasable texts, so the direct answer version of SciQ is slightly smaller, with 10481 questions in train, 887 in dev, and 884 in test.", "labels": [], "entities": []}, {"text": "We created a crowdsourcing task with the following setup: A person was presented with an original science exam question and a crowdsourced question.", "labels": [], "entities": []}, {"text": "The instructions were to choose which of the two questions was more likely to be the real exam question.", "labels": [], "entities": []}, {"text": "We randomly drew 100 original questions and 100 instances from the SciQ training set and presented the two options in random order.", "labels": [], "entities": [{"text": "SciQ training set", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.7632671793301901}]}, {"text": "People identified the science exam question in 55% of the cases, which falls below the significance level of p=0.05 under a null hypothesis of a random guess 4 .", "labels": [], "entities": [{"text": "significance", "start_pos": 87, "end_pos": 99, "type": "METRIC", "confidence": 0.9652233123779297}]}], "tableCaptions": [{"text": " Table 2: Test set accuracy of existing models on  the multiple choice version of SciQ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9851856827735901}]}, {"text": " Table 3: Model accuracies on real science ques- tions validation set when trained on 4 th / 8 th grade  exam questions alone, and when adding SciQ.", "labels": [], "entities": []}]}