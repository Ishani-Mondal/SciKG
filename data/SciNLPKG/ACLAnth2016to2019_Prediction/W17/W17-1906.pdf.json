{"title": [{"text": "Supervised and unsupervised approaches to measuring usage similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "Usage similarity (USim) is an approach to determining word meaning in context that does not rely on a sense inventory.", "labels": [], "entities": [{"text": "Usage similarity (USim)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6641324996948242}]}, {"text": "Instead, pairs of usages of a target lemma are rated on a scale.", "labels": [], "entities": []}, {"text": "In this paper we propose un-supervised approaches to USim based on embeddings for words, contexts, and sentences , and achieve state-of-the-art results over two USim datasets.", "labels": [], "entities": [{"text": "USim datasets", "start_pos": 161, "end_pos": 174, "type": "DATASET", "confidence": 0.6872585564851761}]}, {"text": "We further consider supervised approaches to USim, and find that although they outperform unsu-pervised approaches, they are unable to generalize to lemmas that are unseen in the training data.", "labels": [], "entities": [{"text": "USim", "start_pos": 45, "end_pos": 49, "type": "TASK", "confidence": 0.8908685445785522}]}], "introductionContent": [], "datasetContent": [{"text": "We evaluate our methods on two USim datasets representing two different text types: ORIGINAL, the USim dataset of, and TWIT-TER from.", "labels": [], "entities": [{"text": "USim datasets", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.843284010887146}, {"text": "ORIGINAL", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9541081786155701}, {"text": "USim dataset", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9448322951793671}, {"text": "TWIT-TER", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9626909494400024}]}, {"text": "Both USim datasets contain pairs of sentences; each sentence in each pair includes a usage of a particular target lemma.", "labels": [], "entities": [{"text": "USim datasets", "start_pos": 5, "end_pos": 18, "type": "DATASET", "confidence": 0.8966241776943207}]}, {"text": "Each sentence pair is rated on a scale of 1-5 for how similar in meaning the usages of the target words are in the two sentences.", "labels": [], "entities": []}, {"text": "ORIGINAL consists of sentences from McCarthy and, which were drawn from a web corpus).", "labels": [], "entities": [{"text": "ORIGINAL", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5276750922203064}]}, {"text": "This dataset contains 34 lemmas, including nouns, verbs, adjectives, and adverbs.", "labels": [], "entities": []}, {"text": "Each lemma is the target word in 10 sentences.", "labels": [], "entities": []}, {"text": "For each lemma, sentence pairs (SPairs) are formed based on all pairwise comparisons, giving 45 SPairs per lemma.", "labels": [], "entities": []}, {"text": "Annotations were provided by three native English speakers, with the average taken as the final gold standard similarity.", "labels": [], "entities": [{"text": "Annotations", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.922855794429779}]}, {"text": "Ina small number of cases the annotators were unable to judge similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9605320692062378}]}, {"text": "removed these SPairs from the dataset, resulting in a total of 1512 SPairs.", "labels": [], "entities": []}, {"text": "TWITTER contains SPairs for ten nouns from ORIGINAL.", "labels": [], "entities": [{"text": "TWITTER", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9048641920089722}, {"text": "SPairs", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.5592431426048279}]}, {"text": "In this case the \"sentences\" are in fact tweets.", "labels": [], "entities": []}, {"text": "55 SPairs are provided for each noun.", "labels": [], "entities": [{"text": "SPairs", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.5057669878005981}]}, {"text": "Unlike ORIGINAL, the SPairs are not formed on the basis of all pairwise comparisons amongst a smaller set of sentences.", "labels": [], "entities": [{"text": "SPairs", "start_pos": 21, "end_pos": 27, "type": "TASK", "confidence": 0.9701752662658691}]}, {"text": "This dataset was annotated via crowd sourcing and carefully cleaned to remove outlier annotations.", "labels": [], "entities": []}, {"text": "Following and we evaluate our systems by calculating Spearman's rank correlation coefficient between the gold standard similarities and the predicted similarities.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 53, "end_pos": 92, "type": "METRIC", "confidence": 0.620506078004837}]}, {"text": "This enables direct comparison of our results with those reported in these previous studies.", "labels": [], "entities": []}, {"text": "We evaluate our supervised approaches using two cross-validation methodologies.", "labels": [], "entities": []}, {"text": "In the first case we apply 10-fold cross-validation, randomly partitioning all SPairs for all lemmas in a given dataset.", "labels": [], "entities": []}, {"text": "Using this approach, the test data fora given fold consists of SPairs for target lemmas that were seen in the training data.", "labels": [], "entities": [{"text": "SPairs", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9067525267601013}]}, {"text": "To determine how well our methods generalize to unseen lemmas, we consider a second cross-validation setup in which we partition the SPairs in a given dataset by lemma.", "labels": [], "entities": []}, {"text": "Here the test data fora given fold consists of SPairs for one lemma, and the training data consists of SPairs for all other lemmas.", "labels": [], "entities": []}, {"text": "We first consider the unsupervised approach using word2vec fora variety of window sizes and number of dimensions.", "labels": [], "entities": []}, {"text": "All correlations are significant (p < 0.05).", "labels": [], "entities": []}, {"text": "On both ORIGINAL and TWITTER, fora given number of dimensions, as the window size is increased, \u03c1 increases.", "labels": [], "entities": []}, {"text": "Embeddings for larger window sizes tend to better capture semantics, whereas embeddings for smaller window sizes tend to better reflect syntax (: Spearman's \u03c1 on each dataset using the unsupervised method, and supervised methods with cross-validation folds based on random sampling across all lemmas (All) and holding out individual lemmas (Lemma), for each embedding approach.", "labels": [], "entities": []}, {"text": "The best \u03c1 for each experimental setup, on each dataset, is shown in boldface.", "labels": [], "entities": []}, {"text": "Significant correlations (p < 0.05) are indicated with *.", "labels": [], "entities": []}, {"text": "more-semantic embeddings given by larger window sizes appear to be better-suited to the task of predicting USim.", "labels": [], "entities": [{"text": "predicting USim", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.7721009850502014}]}, {"text": "For a given window size, a higher number of dimensions also tends to achieve higher \u03c1.", "labels": [], "entities": []}, {"text": "The best parameter settings for our unsupervised approach using word2vec embeddings achieve higher correlations, 0.286 and 0.300, on ORIGI-NAL and TWITTER, respectively. and both report drastic variation in performance for different settings of the number of topics in their models.", "labels": [], "entities": [{"text": "ORIGI-NAL", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.8698828220367432}]}, {"text": "We also observe some variation with respect to parameter settings; however, any of the parameter settings considered achieves a higher correlation than on ORIGINAL.", "labels": [], "entities": []}, {"text": "For TWITTER, parameter settings with W \u2265 5 and D \u2265 100 achieve a correlation comparable to, or greater than, the best reported by We now consider the unsupervised approach, using the other embeddings.", "labels": [], "entities": []}, {"text": "Based on the previous findings for word2vec, we only consider this model with W = 8 and D = 300 here.", "labels": [], "entities": []}, {"text": "Results are shown in in the column labeled \"Unsupervised\".", "labels": [], "entities": []}, {"text": "For ORIGINAL, context2vec performs best (and indeed outperforms word2vec for all parameter settings considered).", "labels": [], "entities": []}, {"text": "This result demonstrates that approaches to predicting USim that explicitly embed the context of a target word can outperform approaches based on averaging word embeddings (i.e., word2vec and GloVe) or embedding sentences (skip-thoughts).", "labels": [], "entities": [{"text": "predicting USim", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.6887330710887909}]}, {"text": "This result is particularly strong because we consider a range of parameter settings for word2vec, but only used the default settings for context2vec.", "labels": [], "entities": []}, {"text": "8 Word2vec does however perform best on TWITTER.", "labels": [], "entities": [{"text": "8 Word2vec", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.6713943183422089}, {"text": "TWITTER", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9020082354545593}]}, {"text": "The relatively poor performance of context2vec and skip-thoughts here could be due to differences between the text types these embedding models were trained on and the evaluation data.", "labels": [], "entities": []}, {"text": "GloVe performs poorly, even though it was trained on tweets for these experiments, but that it performs less well than word2vec is consistent with the findings for ORIGINAL.", "labels": [], "entities": []}, {"text": "Turning to the supervised approach, we first consider results for cross-validation based on randomly partitioning all SPairs in a dataset (column \"All\" in).", "labels": [], "entities": []}, {"text": "The best correlation on TWIT-TER (0.384) is again achieved using word2vec, while the best correlation on ORIGINAL (0.434) is obtained with skip-thoughts.", "labels": [], "entities": [{"text": "TWIT-TER", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8195216655731201}, {"text": "ORIGINAL (0.434)", "start_pos": 105, "end_pos": 121, "type": "METRIC", "confidence": 0.9299396276473999}]}, {"text": "The difference in performance amongst the various embedding approaches is, however, somewhat less here than in the unsupervised setting.", "labels": [], "entities": []}, {"text": "For each embedding approach, and each dataset, the correlation in the supervised setting is better than that in the unsupervised setting, suggesting that if labeled training data is available, supervised approaches can give substantial improvements over unsupervised approaches to predicting USim.", "labels": [], "entities": [{"text": "predicting USim", "start_pos": 281, "end_pos": 296, "type": "TASK", "confidence": 0.8404806554317474}]}, {"text": "However, this experimental setup does not show the extent to which the supervised approach is able to generalize to previously-unseen lemmas.", "labels": [], "entities": []}, {"text": "The column labeled \"Lemma\" in shows results for the supervised approach for crossvalidation using lemma-based partitioning.", "labels": [], "entities": []}, {"text": "In these experiments, the test data consists of usages of a target lemma that was not seen as a target lemma during training.", "labels": [], "entities": []}, {"text": "For each dataset, the correlations achieved here for each type of embedding are lower than those of the corresponding unsupervised method, with the exception of GloVe.", "labels": [], "entities": []}, {"text": "In the case of ORIGINAL, the higher correlation for GloVe relative to the unsupervised setup appears to be largely due to improved performance on adverbs.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9594837427139282}]}, {"text": "Nevertheless, for each dataset, the correlations achieved by GloVe are still lower than those of the best unsupervised method on that dataset.", "labels": [], "entities": [{"text": "correlations", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.953295886516571}]}, {"text": "These results demonstrate that the supervised approach generalizes poorly to new lemmas.", "labels": [], "entities": []}, {"text": "This negative result indicates an important direction for future work -identifying strategies to training supervised approaches to predicting USim that generalize to unseen lemmas.", "labels": [], "entities": [{"text": "predicting USim", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.792007178068161}]}], "tableCaptions": [{"text": " Table 1: Spearman's \u03c1 on each dataset using  the unsupervised approach with word2vec embed- dings trained using several settings for the number  of dimensions (D) and window size (W ). The best  \u03c1 for each dataset is shown in boldface.", "labels": [], "entities": []}]}