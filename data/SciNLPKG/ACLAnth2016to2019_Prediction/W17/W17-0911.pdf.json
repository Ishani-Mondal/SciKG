{"title": [{"text": "An RNN-based Binary Classifier for the Story Cloze Test", "labels": [], "entities": [{"text": "Story Cloze", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9238107204437256}]}], "abstractContent": [{"text": "The Story Cloze Test consists of choosing a sentence that best completes a story given two choices.", "labels": [], "entities": []}, {"text": "In this paper we present a system that performs this task using a supervised binary classifier on top of a recurrent neural network to predict the probability that a given story ending is correct.", "labels": [], "entities": []}, {"text": "The classifier is trained to distinguish correct story endings given in the training data from incorrect ones that we artificially generate.", "labels": [], "entities": []}, {"text": "Our experiments evaluate different methods for generating these negative examples, as well as different embedding-based representations of the stories.", "labels": [], "entities": []}, {"text": "Our best result obtains 67.2% accuracy on the test set, outperforming the existing top baseline of 58.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9997465014457703}]}], "introductionContent": [{"text": "Automatically predicting \"what happens next\" in a story is an emerging AI task, situated at the point where natural language processing meets commonsense reasoning research.", "labels": [], "entities": [{"text": "Automatically predicting \"what happens next\" in a story", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8207417458295823}]}, {"text": "Story understanding began as classic AI planning research, e.g.), and has evolved with the shift to data-driven AI approaches by which large sets of stories can be analyzed from text.", "labels": [], "entities": [{"text": "Story understanding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8700726628303528}]}, {"text": "A barrier to this research has been the lack of standard evaluation schemes for benchmarking progress.", "labels": [], "entities": []}, {"text": "The new Story Cloze Test addresses this need through a binary-choice evaluation format: given the beginning sentences of a story, the task is to choose which of two given sentences best completes the story.", "labels": [], "entities": []}, {"text": "The cloze framework also * This research was conducted at his previous affiliation, Tohoku University.", "labels": [], "entities": [{"text": "Tohoku University", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.9631259441375732}]}, {"text": "provides training stories (referred to here as the ROC corpus) in the same domain as the evaluation items.", "labels": [], "entities": [{"text": "ROC corpus", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.782379686832428}]}, {"text": "Mostafazadeh et al. details the crowdsourced authoring process for this dataset.", "labels": [], "entities": []}, {"text": "Ultimately the training data consists of 97,027 fivesentence stories.", "labels": [], "entities": []}, {"text": "The separate cloze test has 3742 items (divided equally between validation and test sets) each containing the first four sentences of a story with a correct and incorrect ending to choose from.", "labels": [], "entities": []}, {"text": "In the current paper, we describe a set of approaches for performing the Story Cloze Test.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.7025673786799113}]}, {"text": "Our best result obtains 67.2% accuracy on the test set, outperforming Mostafazadeh et al.'s best baseline of 58.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9996955394744873}]}, {"text": "We first report two additional unsupervised baselines used in other narrative prediction tasks.", "labels": [], "entities": [{"text": "narrative prediction tasks", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.7912616431713104}]}, {"text": "We then describe our supervised approach, which uses a recurrent neural network (RNN) with a binary classifier to distinguish correct story endings from artificially generated incorrect endings.", "labels": [], "entities": []}, {"text": "We compare the performance of this model when alternatively trained on different story encodings and different strategies for generating incorrect endings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained a classifier for each type of negative ending and additionally for each type of embedding, shown in.", "labels": [], "entities": []}, {"text": "For each correct example, we generated multiple incorrect examples.", "labels": [], "entities": []}, {"text": "We found that setting the number of negative samples per positive example near 6 produced the best results on the validation set for all configurations, so we kept this number consistent across experiments.", "labels": [], "entities": []}, {"text": "The exception is the Backward method, which can only generate one of the first four sentences in each story.", "labels": [], "entities": []}, {"text": "For each generation method, the negative samples were kept the same across runs of the model with different embeddings, rather than re-sampling for each run.", "labels": [], "entities": []}, {"text": "After discovering that our best validation results came from the random endings, we also evaluated combinations of these endings with the other types to see if they could further boost the model's performance.", "labels": [], "entities": []}, {"text": "The samples used by these combinedmethod experiments were a subset of the negative samples generated for the single-method results.", "labels": [], "entities": []}, {"text": "shows the accuracy of all unsupervised and supervised models on both the validation and test sets, with the best test result within each group in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995198249816895}]}, {"text": "Among the unsupervised models, the AveMax model with the GoogleNews embeddings (55.2% test accuracy) performs comparably to Mostafazadeh et al.'s word2vec similarity model (53.9%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9209654331207275}]}, {"text": "The PMI approach performs at the same level as the current best baseline of 58.5%, and the counts from the ROC stories are just as effective (59.9%) as those from the much larger blog corpus (59.1%).", "labels": [], "entities": [{"text": "ROC stories", "start_pos": 107, "end_pos": 118, "type": "DATASET", "confidence": 0.9536908268928528}]}, {"text": "The best test result using the GoogleNews word embeddings (61.5%) was slightly better than that of the ROC word embeddings (58.8%).", "labels": [], "entities": [{"text": "ROC word embeddings", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.8192227681477865}]}, {"text": "Among the single-method results, the word embeddings were outperformed by the best result of the skipthought embeddings (63.2%), suggesting that the skip-thought model may capture more information about a sentence than simply averaging its word embeddings.", "labels": [], "entities": []}, {"text": "For this reason we skipped evaluating the word embeddings for the combined-ending experiments.", "labels": [], "entities": []}, {"text": "One caveat to this is the smaller size of the word embeddings relative to the skipthought vectors.", "labels": [], "entities": []}, {"text": "While it is unusual for word2vec embeddings to have more than a thousand dimensions, to be certain that the difference in performance was not due to the difference in dimensionality, we performed an ad-hoc evaluation of word embeddings that were the same size as the ROC sentence vectors (2400 nodes).", "labels": [], "entities": []}, {"text": "We computed these vectors from the ROC corpus in the same way described in Section 2, and applied them to our best-performing data configuration (Rand-3 + Back-1 + Near-1 + LM-1).", "labels": [], "entities": [{"text": "ROC corpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.8680061995983124}]}, {"text": "The result (57.9%) was still lower than that produced by the cor-: Accuracy on the Story Cloze Test responding ROC sentence embeddings (66.1%), supporting our idea that the skip-thought embeddings area better sentence representation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.99726402759552}, {"text": "Story Cloze Test", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.7833652496337891}]}, {"text": "Interestingly, though the BookCorpus sentence vectors obtained the best result overall (67.2%), they performed on average the same as the ROC ones (mean accuracy of 61.1% versus 61.3%, respectively), despite that the former have more dimensions (4800) and were trained on several more stories.", "labels": [], "entities": [{"text": "BookCorpus sentence vectors", "start_pos": 26, "end_pos": 53, "type": "DATASET", "confidence": 0.9144170085589091}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9019734263420105}]}, {"text": "This might suggest it helps to model the unique genre of stories contained in the ROC corpus for this task.", "labels": [], "entities": [{"text": "ROC corpus", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.9280582368373871}]}, {"text": "The best results in terms of data generation incorporate the Random endings, suggesting that for many of the items in the Story Cloze Test, the correct ending is the one that is more semantically similar to the context.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 122, "end_pos": 138, "type": "DATASET", "confidence": 0.7768713633219401}]}, {"text": "Not surprisingly, the Backward endings have limited effect on their own (best result 56%), but they boost the performance of the Random endings when combined (best result 66.9%).", "labels": [], "entities": [{"text": "Backward", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9289175868034363}]}, {"text": "We expected that the Nearest-Ending and LM endings would have an advantage over the Random endings, but our results didn't show this.", "labels": [], "entities": []}, {"text": "The best result for the Nearest-Ending method was 62.1% compared to 63.2% produced by the Random endings.", "labels": [], "entities": []}, {"text": "The LM endings fared particularly badly on their own (best result 54.4%).", "labels": [], "entities": [{"text": "LM endings", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8439077138900757}]}, {"text": "We noticed the LM seemed to produce very similar endings across different stories, which possibly influenced this result.", "labels": [], "entities": []}, {"text": "The best result overall (67.2%) was produced by the model that sampled from all four types of endings, though it was only trivially higher than the best result for the combined Random and Backward endings (66.9%).", "labels": [], "entities": []}, {"text": "Still, we see opportunity in the technique of using generative methods to expand the training set.", "labels": [], "entities": []}, {"text": "We only generated incorrect endings in this work, but ideally this approach could generate correct endings as well, given that a story has multiple possible correct endings.", "labels": [], "entities": []}, {"text": "It is possible that the small size of the ROC corpus limited our current success with this idea, so in the future we plan to pursue this using a much larger story dataset.", "labels": [], "entities": [{"text": "ROC corpus", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.8876668512821198}]}], "tableCaptions": []}