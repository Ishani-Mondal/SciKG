{"title": [{"text": "The Role of Conversation Context for Sarcasm Detection in Online Interactions", "labels": [], "entities": [{"text": "Sarcasm Detection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.9447774291038513}]}], "abstractContent": [{"text": "Computational models for sarcasm detection have often relied on the content of utterances in isolation.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9430851638317108}]}, {"text": "However, speaker's sarcastic intent is not always obvious without additional context.", "labels": [], "entities": []}, {"text": "Focusing on social media discussions, we investigate two issues: (1) does modeling of conversation context help in sarcasm detection and (2) can we understand what part of conversation context triggered the sarcastic reply.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.8705103099346161}]}, {"text": "To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the sarcastic response.", "labels": [], "entities": []}, {"text": "1 We show that the conditional LSTM network (Rockt\u00e4schel et al., 2015) and LSTM networks with sentence level attention on context and response outper-form the LSTM model that reads only the response.", "labels": [], "entities": []}, {"text": "To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "It has been argued that sarcasm, or verbal irony, is a type of interactional phenomenon with specific perlocutionary effects on the hearer, such as to break their pattern of expectation.", "labels": [], "entities": []}, {"text": "Thus, to be able to detect speakers' sarcastic intent it is necessary (even if maybe not sufficient) to consider their utterances in the larger conversation context.", "labels": [], "entities": []}, {"text": "Consider the Twitter conversation example in.", "labels": [], "entities": []}, {"text": "Without the context of UserA's We use response and reply interchangeably.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the effect of conversation context (c) on labeling a reply (r) as sarcastic or not sarcastic, we consider two binary classification tasks.", "labels": [], "entities": []}, {"text": "We refer to sarcastic instances as Sand non-sarcastic instances as NS.", "labels": [], "entities": []}, {"text": "In the first task, classification is performed using the reply in isolation (S r vs. NS r task).", "labels": [], "entities": [{"text": "classification", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.9632290005683899}]}, {"text": "In the second, the classification considers both the reply and its context (S c+r vs. NS c+r task).", "labels": [], "entities": []}, {"text": "We experiment with two types of computational models: Support Vector Machines (SVM) with linguistically-motivated discrete features (used as baseline; SVM bl ), and approaches using distributed representations.", "labels": [], "entities": []}, {"text": "For the latter we use the Long short-term Memory (LSTM) Networks) that have been shown to be successful in various NLP tasks, such as constituency parsing, language modeling (), machine translation () and textual entailment).", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.892960399389267}, {"text": "language modeling", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.7860421240329742}, {"text": "machine translation", "start_pos": 178, "end_pos": 197, "type": "TASK", "confidence": 0.8371260464191437}, {"text": "textual entailment", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.7356842458248138}]}, {"text": "We present these models in the next subsections.", "labels": [], "entities": []}, {"text": "We designed an Amazon Mechanical Turk task (for brevity, MTurk) framed as follow: Given a pair of context c and a sarcastic reply r from the discussion forum dataset, identify one or more sentences inc that may trigger the sarcastic reply r.", "labels": [], "entities": []}, {"text": "Turkers could select one or more sentences from the context c, including the entire context.", "labels": [], "entities": []}, {"text": "From the test data, we select examples with context length between three to seven sentences since for longer posts the task will be too complicated for the Turkers.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 156, "end_pos": 163, "type": "DATASET", "confidence": 0.9036784768104553}]}, {"text": "We provided a definition of sarcasm and a few examples to the Turkers.", "labels": [], "entities": [{"text": "definition of sarcasm", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.6419689456621805}, {"text": "Turkers", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9265718460083008}]}, {"text": "We also explained how to carryout the task with the help of a few context/reply pairs.", "labels": [], "entities": []}, {"text": "Each HIT contains only one task and five Turkers were allowed to attempt each HIT (a total of 85 HITS).", "labels": [], "entities": []}, {"text": "Turkers with reasonable quality (i.e., more than 95% of acceptance rate with experience of over 8,000 HITs) were selected and paid seven cents per task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results for the discussion forum dataset (bold are best scores)", "labels": [], "entities": [{"text": "discussion forum dataset", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.6450720032056173}]}, {"text": " Table 3: Experimental results for Twitter dataset (bold are best scores)", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9737308621406555}]}]}