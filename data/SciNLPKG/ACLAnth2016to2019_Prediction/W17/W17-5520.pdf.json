{"title": [{"text": "Interaction Quality Estimation Using Long Short-Term Memories", "labels": [], "entities": [{"text": "Interaction Quality Estimation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9139315883318583}]}], "abstractContent": [{"text": "For estimating the Interaction Quality (IQ) in Spoken Dialogue Systems (SDS), the dialogue history is of significant importance.", "labels": [], "entities": [{"text": "Interaction Quality (IQ)", "start_pos": 19, "end_pos": 43, "type": "METRIC", "confidence": 0.7938997268676757}]}, {"text": "Previous works included this information manually in the form of precom-puted temporal features into the classification process.", "labels": [], "entities": []}, {"text": "Here, we employ a deep learning architecture based on Long Short-Term Memories (LSTM) to extract this information automatically from the data, thus estimating IQ solely by using current exchange features.", "labels": [], "entities": []}, {"text": "We show that it is thereby possible to achieve competitive results as in a scenario where manually optimized temporal features have been included .", "labels": [], "entities": []}], "introductionContent": [{"text": "The increasing complexity of Spoken Dialogue Systems (SDS) and the requirements that come with this progress made automatized recognition and modeling of user states crucial to ensure natural and user adaptive interaction.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems (SDS)", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.7742329935232798}]}, {"text": "User Satisfaction (US) is one important part of such a state.", "labels": [], "entities": [{"text": "User Satisfaction (US)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6550973951816559}]}, {"text": "On the dialogue level (i.e. after the interaction is complete), it provides a measure for the interaction and allows to compare different SDS () or to learn appropriate dialogue strategies).", "labels": [], "entities": []}, {"text": "However, if US is available in each turn, it can also be used for user adaptation).", "labels": [], "entities": [{"text": "user adaptation", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7573149800300598}]}, {"text": "In the scope of this work we focus on the Interaction Quality (IQ) as a turn-wise approach to US and propose a deep learning architecture to estimate it solely using exchange parameters . In doing so, we show that with the proposed approach, An exchange is a system turn followed by a user turn.", "labels": [], "entities": []}, {"text": "manually optimized, pre-computed temporal information (as employed in previous work) is no longer required.", "labels": [], "entities": []}, {"text": "Diverse approaches for estimating the US were already proposed, including n-gram models and Hidden Markov Models) in different scenarios.", "labels": [], "entities": [{"text": "estimating the US", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7810879747072855}]}, {"text": "Although the results were above the random baseline, the respective improvement was only minor.", "labels": [], "entities": []}, {"text": "As it was discussed by, one difficulty of this task lies in the subjective nature of US since it depends on the appreciation of the user.", "labels": [], "entities": [{"text": "US", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.8282952308654785}]}, {"text": "IQ is a more objective approach to US that relies on the rating of experts instead of users (  and thus closes the gap between subjective valuation and objective criteria.", "labels": [], "entities": [{"text": "IQ", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.47895824909210205}, {"text": "US", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.8252951502799988}]}, {"text": "The respective rating is given on a scale between 1 (extremely unsatisfied) and five (satisfied) after listening to audio records of the dialogue in question.", "labels": [], "entities": []}, {"text": "A detailed study on the correlation between the IQ and a measure of the real US was provided by  and various approaches including Hidden Markov Models (, Support Vector Machines ( , Ordinal Regression (El and Recurrent Neural Networks ( have been employed to estimate the IQ from exchange parameters.", "labels": [], "entities": []}, {"text": "Although the results show a significant improvement to alternative approaches, the classification relies in each case on precomputed features modeling the dialogue history (so called temporal features).", "labels": [], "entities": []}, {"text": "Despite the good results, using temporal features requires insight into the correlations between the dialogue history and the IQ score as the timespan covered by the temporal information significantly influences the outcome ().", "labels": [], "entities": [{"text": "IQ", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.5706112384796143}]}, {"text": "The required knowledge about this correlation is usually not accessible and likely to be domain dependent thus rendering the respective approaches inflexible.", "labels": [], "entities": []}, {"text": "In contrast, we employ a deep learning classifier to extract the required temporal information automatically and show that in doing so it is possible to achieve competitive results by only using exchange level parameters.", "labels": [], "entities": []}, {"text": "In addition, we show that findings of previous works regarding the optimal amount of temporal information to be included maybe retrieved in our approach by slightly varying the input sequences.", "labels": [], "entities": []}, {"text": "Finally, the usability of our proposed architecture in real-life scenarios is discussed by looking at the percentage of usable IQ guesses.", "labels": [], "entities": []}, {"text": "The remainder of this paper is as follows: In Section 2 we discuss the LSTM based neural network architecture followed by a discussion of the employed data in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents the experiments and results and we close with a brief conclusion and outlook in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss the results of the employed classifier in estimating the IQ for the annotated LEGO corpus.", "labels": [], "entities": [{"text": "LEGO corpus", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.8707599937915802}]}, {"text": "To distinguish the contribution of the parameters derived from different SDS instances to the IQ, three feature sets were employed that consisted of features assigned to the ASR, the DM and both: ASR: ASRRecognitionStatus (string, status of the ASR), Modality (string, input modality of the user, either speech or dtmf ), ExMo (string, expected modality of the user input, either speech, dtmf, both or none), ASRConfidence (float, confidence score of the ASR), Barged-In?", "labels": [], "entities": [{"text": "ASRConfidence (float, confidence score", "start_pos": 409, "end_pos": 447, "type": "METRIC", "confidence": 0.6409145991007487}]}, {"text": "(boolean, true if system was interrupted by the user), UnExMo?", "labels": [], "entities": []}, {"text": "(boolean, true if the actual input modality did not match the expected one), WPUT (integer, words per user turn), UTD (float, utterance turn duration) DM: ActivityType (string, type of activity), RoleName (string, function of the system turn), RePromt?", "labels": [], "entities": [{"text": "WPUT", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.8864364624023438}, {"text": "UTD", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9120985865592957}]}, {"text": "(boolean, true if the current turn is a repromt), WPST (integer, words per system turn), DD (float, dialogue duration), RoleIndex (integer, tries necessary to get a desired response from the user) Parameters that are either constant or task-related were discarded, including the two features from the NLU.", "labels": [], "entities": [{"text": "WPST", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.8045050501823425}]}, {"text": "To represent all parameters as a numerical input vector, non-numerical features were encoded in a one-hot vector.", "labels": [], "entities": []}, {"text": "As in previous work, we used 10-fold cross validation to evaluate the outcomes.", "labels": [], "entities": []}, {"text": "The results are compared in terms of Unweighted Average Recall 2 (UAR), Cohen's (linearly weighted) Kappa and Spearman's Rho) to the ones achieved by with the best window size n = 9, the full feature set and a Support Vector Machine (SVM).", "labels": [], "entities": [{"text": "Unweighted Average Recall 2 (UAR)", "start_pos": 37, "end_pos": 70, "type": "METRIC", "confidence": 0.9599342686789376}]}, {"text": "Our results as well as the baseline value are shown in.", "labels": [], "entities": []}, {"text": "For all three measures, the results with the full feature set are competitive to the baseline.", "labels": [], "entities": []}, {"text": "Whereas the UAR is slightly below the reference value, \u03ba and \u03c1 show a small improvement.", "labels": [], "entities": [{"text": "UAR", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9951752424240112}]}, {"text": "The results for the two subsets are visibly below the baseline for both UAR and \u03ba whereas the DM value of \u03c1 equals the respective reference value.", "labels": [], "entities": [{"text": "UAR", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9608922004699707}]}, {"text": "Moreover, the DM features yield better results than the ASR features and thus contribute more to the overall IQ value, The arithmetic average of all class-wise recalls.: The results of the LSTM approach in comparison to the SVM baseline (, including the number of handcrafted temporal features in use (#TF) for each scenario.", "labels": [], "entities": [{"text": "IQ", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9035202264785767}]}, {"text": "which is inline with the outcomes of previous work . It is stressed that none of the feature sets employed for the LSTM uses handcrafted temporal features nor needs them.", "labels": [], "entities": []}, {"text": "Thus, we conclude that our approach is indeed capable of extracting the required temporal information automatically.", "labels": [], "entities": []}, {"text": "In addition, we investigate the temporal information extracted by the trained classifier by measuring the impact of one system-user exchange on following estimates.", "labels": [], "entities": []}, {"text": "This allows a comparison of the extracted information in the herein discussed scenario with the manually set window size in previous work.", "labels": [], "entities": []}, {"text": "To this end, we replaced the input vector of the second system-user exchange e 2 in each dialogue ., e i L ) of the corpus {D 1 , ..., D M } by the input associated with one out of 20 randomly picked exchanges e jr (j \u2208 {1, . .", "labels": [], "entities": []}, {"text": ", M }) with assigned IQ value of 1.", "labels": [], "entities": [{"text": "IQ value", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9731829166412354}]}, {"text": "The modified dialogues\u02dcD were then fed through a trained model of the 10-fold cross validation and the results were compared to the ones achieved with the original data by computing the sum of the absolute errors of each class.", "labels": [], "entities": []}, {"text": "This was repeated for all 20 random picks and all 10 models (we employed different random picks for each model).", "labels": [], "entities": []}, {"text": "The mean of this error overall dialogues, all trained models and all random picks for the replaced exchange was determined and is shown as a function of the systemuser exchange number in.", "labels": [], "entities": []}, {"text": "This error indicates the impact one exchange has on the IQ estimate of following exchanges.", "labels": [], "entities": []}, {"text": "We see that from exchange number 9 to exchange number 12 the error clearly decreases.", "labels": [], "entities": [{"text": "error", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9940516352653503}]}, {"text": "A comparison with the referenced work shows that this drop is in the same range as the optimal window size n = 9 (that would correspond to exchange number 11).", "labels": [], "entities": []}, {"text": "Therefore the impact of the exchange in question is decreased in the same range as in a scenario were this impact is controlled manually.", "labels": [], "entities": []}, {"text": "This indicates that similar temporal information that was employed therein is automatically extracted by our architecture.", "labels": [], "entities": []}, {"text": "In many classification scenarios, the classes are not ordered which means that in the case of a wrong guess it is irrelevant which class was chosen.", "labels": [], "entities": []}, {"text": "However, as the IQ is an ordered scale, the distance of the wrong guess to the real class is of interest, especially in view of the application.", "labels": [], "entities": []}, {"text": "We therefore compute the amount of guesses in which the classification was wrong only by one point (e.g. an instant of IQ 1 classified as IQ 2 or vice versa).", "labels": [], "entities": []}, {"text": "This percentage \u03b4 can be derived directly from the confusion matrix C as with N the number of total entries of C and K the number of classes, i.e. the dimension of C.", "labels": [], "entities": []}, {"text": "Adding this value to the Accuracy (ACC) gives a percentage of usable guesses of the classifier.", "labels": [], "entities": [{"text": "Accuracy (ACC)", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.9560319930315018}]}, {"text": "The results for the architecture used in this work and the best feature set (ASR + DM) are ACC=0.57 and \u03b4=0.37, resulting in a sum of 0.94.", "labels": [], "entities": [{"text": "ASR + DM)", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.7502063661813736}, {"text": "ACC", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9850550889968872}]}, {"text": "In other words, considering a real-life scenario, 94% of the classifiers guesses could be used, for example for user adaptation.", "labels": [], "entities": [{"text": "user adaptation", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.8172124028205872}]}, {"text": "Again, these results are compared to the ones achieved with a SVM and the setup of (Ultes et al., 2017b) with a sum of 0.91.", "labels": [], "entities": []}, {"text": "Evidently, the deep learning classifier outperforms the SVM approach in this metric.", "labels": [], "entities": []}], "tableCaptions": []}