{"title": [{"text": "Cost Weighting for Neural Machine Translation Domain Adaptation", "labels": [], "entities": [{"text": "Neural Machine Translation Domain Adaptation", "start_pos": 19, "end_pos": 63, "type": "TASK", "confidence": 0.8236256837844849}]}], "abstractContent": [{"text": "In this paper, we propose anew domain adaptation technique for neural machine translation called cost weighting, which is appropriate for adaptation scenarios in which a small in-domain data set and a large general-domain data set are available.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6742297609647115}]}, {"text": "Cost weighting incorporates a domain classifier into the neural machine translation training algorithm, using features derived from the encoder representation in order to distinguish in-domain from out-of-domain data.", "labels": [], "entities": [{"text": "Cost weighting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8381485641002655}, {"text": "neural machine translation training", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.7862218767404556}]}, {"text": "Classifier probabilities are used to weight sentences according to their domain similarity when updating the parameters of the neural translation model.", "labels": [], "entities": []}, {"text": "We compare cost weighting to two traditional domain adaptation techniques developed for statistical machine translation: data selection and sub-corpus weighting.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.7173275550206503}, {"text": "data selection", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.7970826327800751}]}, {"text": "Experiments on two large-data tasks show that both the traditional techniques and our novel proposal lead to significant gains, with cost weighting outper-forming the traditional methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The performance of data-driven machine translation techniques depends heavily on the degree of domain match between training and test data, where \"domain\" indicates a particular combination of factors such as genre, topic, national origin, dialect, or author's or publication's style.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7142037749290466}]}, {"text": "Training data varies significantly across domains, and cross-domain translations are unreliable, so performance can often be improved by adapting the MT system to the test domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 150, "end_pos": 152, "type": "TASK", "confidence": 0.9254705905914307}]}, {"text": "Domain adaptation (DA) techniques for SMT systems have been widely studied.", "labels": [], "entities": [{"text": "Domain adaptation (DA)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8487782657146454}, {"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9960541725158691}]}, {"text": "Approaches include self-training, data selection, data weighting, context-based DA, and topic-based DA, etc.", "labels": [], "entities": [{"text": "data selection", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7564731538295746}]}, {"text": "We review these techniques in the next section.", "labels": [], "entities": []}, {"text": "Sequence-to-sequence learning ( has achieved great success on machine translation tasks recently (, and is often referred to as Neural Machine Translation (NMT).", "labels": [], "entities": [{"text": "Sequence-to-sequence learning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9235890805721283}, {"text": "machine translation tasks", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.7972651322682699}, {"text": "Neural Machine Translation (NMT)", "start_pos": 128, "end_pos": 160, "type": "TASK", "confidence": 0.8300954898198446}]}, {"text": "NMT usually adopts the encoder-decoder framework: it first encodes a source sentence into context vector(s), then decodes its translation token-by-token, selecting from the target vocabulary.", "labels": [], "entities": []}, {"text": "Attention based NMT () dynamically generates context vectors for each target position, and focuses on the relevant source words when generating a target word.", "labels": [], "entities": []}, {"text": "Domain adaptation for NMT is still anew research area, with only a small number of relevant publications.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8185128569602966}, {"text": "NMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8255172967910767}]}, {"text": "adapted an NMT model trained on general domain data with further training (fine-tuning) on in-domain data only.", "labels": [], "entities": []}, {"text": "This was called the continue model by, who propose an ensemble method that combines the continue model with the original model.", "labels": [], "entities": []}, {"text": "propose a method called mixed fine tuning, which combines fine tuning and multi domain NMT.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew domain adaptation method for NMT called cost weighting, in which a domain classifier and sequence-tosequence translation model are trained simultaneously.", "labels": [], "entities": [{"text": "sequence-tosequence translation", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6708706766366959}]}, {"text": "The domain classifier is trained on indomain and general domain data, and provides an estimate of the probability that each sentence in the training data is in-domain.", "labels": [], "entities": []}, {"text": "The cost incurred for each sentence is weighted by the probability of it being in-domain.", "labels": [], "entities": []}, {"text": "This biases the sequenceto-sequence model toward in-domain data, resulting in improved translation performance on an indomain test set.", "labels": [], "entities": []}, {"text": "We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods.", "labels": [], "entities": [{"text": "SMT domain adaptation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.9476449688275655}, {"text": "data selection", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.7199713885784149}, {"text": "corpus weighting", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.651795893907547}]}, {"text": "Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement.", "labels": [], "entities": []}, {"text": "The cost weighting scheme has the additional advantage of being integrated with sequence-to-sequence training.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU scores for ensembled baseline  and domain adapted systems, which include aver- age weighting (\"avg weighting\"), corpus weight- ing (\"crp weighting\") ensemble, ensembled cross- entropy based data selection (\"DS xent\"), semi- supervised CNN based data selection (\"DS ss- cnn\"), and cost weighting based systems. */**  means the result is significantly better than the  baseline at p < 0.05 or p < 0.01 level, respec- tively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9951417446136475}]}]}