{"title": [{"text": "Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets", "labels": [], "entities": []}], "abstractContent": [{"text": "There has been a good amount of progress in sentiment analysis over the past 10 years, including the proposal of new methods and the creation of benchmark datasets.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9675300121307373}]}, {"text": "In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task.", "labels": [], "entities": []}, {"text": "Accordingly, it is hard to understand how well a certain model generalizes across different tasks and datasets.", "labels": [], "entities": []}, {"text": "In this paper, we contribute to this situation by comparing several models on six different benchmarks, which belong to different domains and additionally have different levels of granularity (binary, 3-class, 4-class and 5-class).", "labels": [], "entities": []}, {"text": "We show that Bi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are particularly good at fine-grained sentiment tasks (i. e., with more than two classes).", "labels": [], "entities": []}, {"text": "Incorporating sentiment information into word em-beddings during training gives good results for datasets that are lexically similar to the training data.", "labels": [], "entities": []}, {"text": "With our experiments, we contribute to a better understanding of the performance of different model architec-tures on different data sets.", "labels": [], "entities": []}, {"text": "Consequently, we detect novel state-of-the-art results on the SenTube datasets.", "labels": [], "entities": [{"text": "SenTube datasets", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9061196744441986}]}], "introductionContent": [{"text": "The task of analyzing private states expressed by an author in text, such as sentiment, emotion or affect, can give us access to a wealth of hidden information to analyze product reviews (), political views (, or to identify potentially dangerous activity on the Internet (.", "labels": [], "entities": []}, {"text": "The first approaches in this field of research depended on the use of words at a symbolic level (unigrams, bigrams, bag-of-words features), where generalizing to new words was difficult (;.", "labels": [], "entities": []}, {"text": "Current state-of-the-art methods rely on features extracted in an unsupervised manner, mainly through one of the existing pre-trained word embedding approaches).", "labels": [], "entities": []}, {"text": "These approaches represent words as some function of their contexts, enabling machine learning algorithms to generalize over tokens that have similar representations, arguably giving them an advantage over previous symbolic approaches.", "labels": [], "entities": []}, {"text": "In order to evaluate state-of-the-art models (both symbolic and embedding-based), different datasets are used.", "labels": [], "entities": []}, {"text": "However, it is not clear that a model that performs well on one certain dataset will transfer well to other datasets with different properties.", "labels": [], "entities": []}, {"text": "The work we describe in this paper aims at discovering if there are certain models that generally perform better or if there are certain models that are better adapted to certain kinds of datasets.", "labels": [], "entities": []}, {"text": "Ultimately, the goal of this paper is to contribute to the current situation by supporting the choice of a method for novel domains and datasets, based on properties of the task at hand.", "labels": [], "entities": []}, {"text": "Our main contributions are, therefore, comparing seven approaches to sentiment analysis on six benchmark datasets . We show that \u2022 bidirectional LSTMs perform well across datasets, \u2022 both LSTMs and bidirectional LSTMs are particularly good at fine-grained sentiment tasks, \u2022 and embeddings trained jointly for semantics and sentiment perform well on datasets that are similar to the training data.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8699540793895721}]}], "datasetContent": [{"text": "We choose to evaluate the approaches presented in Section 2.1 on a number of different datasets from different domains, which also have differing levels of granularity of class labels.", "labels": [], "entities": []}, {"text": "The Stanford Sentiment Treebank and SemEval 2013 shared-task dataset have already been used as benchmarks for some of the approaches mentioned in Section 2.1.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.84344349304835}, {"text": "SemEval 2013 shared-task dataset", "start_pos": 36, "end_pos": 68, "type": "DATASET", "confidence": 0.6083997264504433}]}, {"text": "shows which approaches have been tested on which datasets and gives an overview of the statistics for each dataset.", "labels": [], "entities": []}, {"text": "The SenTube datasets () are texts that are taken from YouTube comments regarding automobiles and tablets.", "labels": [], "entities": [{"text": "SenTube datasets", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8882000744342804}]}, {"text": "These comments are normally directed towards a commercial or a video that contains information about the product.", "labels": [], "entities": []}, {"text": "We take only those comments that have some polarity towards the target product in the video.", "labels": [], "entities": []}, {"text": "For the automobile dataset (SenTube-A), this gives a 3381/225/903 training, validation, and test split.", "labels": [], "entities": [{"text": "automobile dataset", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.6926993578672409}]}, {"text": "For the tablets dataset (SenTube-T) the splits are 4997/333/1334.", "labels": [], "entities": [{"text": "tablets dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8779577910900116}]}, {"text": "These are annotated for positive, negative, and neutral sentiment.", "labels": [], "entities": []}, {"text": "We compare seven approaches, five of which fall into the categories mentioned in Section 2, as well as two baselines.", "labels": [], "entities": []}, {"text": "The models and parameters are described in Section 3.1.", "labels": [], "entities": []}, {"text": "We test these models on the benchmark datasets mentioned in Section 2.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of datasets. Train, Dev., and Test refer to the number of examples for each subsection  of a dataset. The number of labels corresponds to the annotation scheme, where: two is positive and  negative; three is positive, neutral, negative; four is strong positive, positive, negative, strong negative;  five is strong positive, positive, neutral, negative, strong negative.", "labels": [], "entities": [{"text": "Test", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9720894694328308}]}, {"text": " Table 4: \u03c7 2 statistics comparing the frequency of  the following emoticons over the different datasets,  :), :(, :-), :-(, :D, =). The difference in frequency  of emoticons between the SemEval and SenTube  datasets is not significant (p > 0.05), while for SST  and OpeNER it is (p < 0.05).", "labels": [], "entities": [{"text": "SenTube  datasets", "start_pos": 199, "end_pos": 216, "type": "DATASET", "confidence": 0.8242904543876648}, {"text": "OpeNER", "start_pos": 267, "end_pos": 273, "type": "DATASET", "confidence": 0.8552143573760986}]}]}