{"title": [{"text": "The Good, the Bad, and the Disagreement: Complex ground truth in rhetorical structure analysis", "labels": [], "entities": [{"text": "rhetorical structure analysis", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.7581788003444672}]}], "abstractContent": [{"text": "We present a proposal to analyze disagreement in Rhetorical Structure Theory annotation which takes into account what we consider \"legitimate\" disagreements.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory annotation", "start_pos": 49, "end_pos": 87, "type": "TASK", "confidence": 0.8246274143457413}]}, {"text": "In rhetorical analysis, as in many other pragmatic annotation tasks, a certain amount of disagreement is to be expected, and it is important to distinguish true mistakes from legitimate disagreements due to different possible interpretations of the structure and intention of a text.", "labels": [], "entities": [{"text": "rhetorical analysis", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.9220296740531921}]}, {"text": "Using different sets of annotations in German and En-glish, we present an analysis of such possible disagreements, and propose an under-specified representation that captures the disagreements.", "labels": [], "entities": []}], "introductionContent": [{"text": "The past ten years have seen continuous interest in RST-oriented discourse parsing, which aims at automatically deriving a complete and well-formed tree representation over coherence relations assigned to adjacent spans of text.", "labels": [], "entities": [{"text": "RST-oriented discourse parsing", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.9362040559450785}]}, {"text": "For various downstream applications (e.g., summarization, essay scoring), such a complete structure is more useful than the purely localized assignment of individual relations, as it is done in PDTB-style analysis (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9907197952270508}, {"text": "essay scoring", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.7328906357288361}]}, {"text": "At the same time, it is well known that RST parsing is difficult, and furthermore, it is more difficult to achieve good human agreement on RST trees, as compared to PDTB annotation.", "labels": [], "entities": [{"text": "RST parsing", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9552465379238129}]}, {"text": "This latter problem has not been in the spotlight of attention, though, while the computational linguistics community developed a series of parsing approaches over the years (.", "labels": [], "entities": []}, {"text": "Part of the reason for the focus on dataoriented automatic parsing is the availability of the RST Discourse Treebank (), a corpus large enough to supply training/test data in supervised machine learning (ML).", "labels": [], "entities": [{"text": "RST Discourse Treebank", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.6426661113897959}]}, {"text": "The central thesis of our paper is that the fundamental questions of RST annotation and agreement deserve to be re-opened.", "labels": [], "entities": [{"text": "RST annotation", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.9554517865180969}]}, {"text": "With powerful ML and parsing technology in place, it is timely to give more attention to the nature of the underlying data, and to its descriptive and theoretical adequacy.", "labels": [], "entities": [{"text": "ML and parsing", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.681190570195516}]}, {"text": "Our claim is that the \"single ground truth asssumption\" is essentially invalid for an annotation task such as rhetorical structure, which inevitably includes a fair amount of subjective decisions on the part of the annotator.", "labels": [], "entities": [{"text": "single ground truth asssumption", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6835778653621674}]}, {"text": "As we will emphasize later, we regard this not as a fault of Rhetorical Structure Theory (), but as a reality to accept, shared with labelling of other pragmatic phenomena, such as speech acts or presuppositions.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.7383957405885061}]}, {"text": "Specifically, we will argue that a certain amount of ambiguity is to be regarded as part of the \"gold standard\" or \"ground truth\".", "labels": [], "entities": []}, {"text": "At the same time, it is clear that RST annotation is not a matter of \"anything goes\".", "labels": [], "entities": [{"text": "RST annotation", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.9347680509090424}]}, {"text": "So, the central challenge in our view is to differentiate between good and bad disagreement: Two annotators may legitimately disagree on some part of the analysis, when both alternatives are inline with the annotation guidelines, and they arise from, for instance, different background knowledge.", "labels": [], "entities": []}, {"text": "This needs to be kept separate from disagreement with a not-so-well-educated annotator who misread the guidelines and thus sometimes makes analysis decisions that should not be regarded as legitimate.", "labels": [], "entities": []}, {"text": "Our overall project has two parts: Teasing apart the two types of disagreement, and adequately representing the space of legitimate alternative analyses.", "labels": [], "entities": []}, {"text": "In this paper we focus on the first task and", "labels": [], "entities": []}], "datasetContent": [{"text": "The most popular method to measure agreement Marcu (2000) computes precision and recall with four factors: Elementary Discourse Units (EDUs), units linked with relations (Spans), nucleus or satellite status (Nuclearity), and relation label (Relation).", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9989376664161682}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9992444515228271}, {"text": "Elementary Discourse Units (EDUs)", "start_pos": 107, "end_pos": 140, "type": "METRIC", "confidence": 0.8051749169826508}, {"text": "relation label (Relation)", "start_pos": 225, "end_pos": 250, "type": "METRIC", "confidence": 0.789336884021759}]}, {"text": "One problem with this method is that it measures twice the same type of decision: Whether the units are linked (Span), and the status of each unit as nucleus or satellite.", "labels": [], "entities": []}, {"text": "This problem is extensively discussed by.", "labels": [], "entities": []}, {"text": "Another problem with this type of evaluation is that it is just quantitative, that is, it does not distinguish between different types of disagreements and their \"quality\".", "labels": [], "entities": []}, {"text": "We believe that on the one hand there are true mistakes in discourse annotation, maybe due to lack of experience in annotation, carelessness, or any other human factor.", "labels": [], "entities": []}, {"text": "We also believe, however, that other differences in annotation maybe considered legitimate disagreement, i.e., annotations that are both valid from a theoretical point of view.", "labels": [], "entities": []}, {"text": "This is particularly the casein argumentative texts, where the analysis hinges on how the annotator perceives the writer's intentions.", "labels": [], "entities": []}, {"text": "Those may not be equally clear to annotators in argumentative texts, as they are more subjective than descriptive text types.", "labels": [], "entities": []}, {"text": "In particular, what we find with inter-annotator agreement studies, is that (i) spans are relatively easy to identify; (ii) nuclearity increases complexity and leads to disagreements; and, most importantly, (iii) relation assignment seems particularly difficult.", "labels": [], "entities": [{"text": "relation assignment", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.8847394585609436}]}, {"text": "We propose that some of the more finegrained distinctions among relations may not be relevant in all cases and all uses of RST trees.", "labels": [], "entities": [{"text": "RST trees", "start_pos": 123, "end_pos": 132, "type": "TASK", "confidence": 0.8906022310256958}]}, {"text": "Thus, an underspecified representation of spans and nuclearity, plus reliably annotated relations, maybe sufficient in many cases.", "labels": [], "entities": []}, {"text": "We propose such a representation in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Precision and recall for expert versus stu- dent annotation (GE1-GL1)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9972997307777405}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9991329312324524}]}, {"text": " Table 4: Pairwise annotator agreement (%) on re- lations (German study, 5 texts)", "labels": [], "entities": [{"text": "re- lations", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.5588521758715311}]}, {"text": " Table 5: Percent agreement of two annotator pairs  (English study, 7 texts)", "labels": [], "entities": [{"text": "Percent agreement", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8473467528820038}]}, {"text": " Table 6: Chance-corrected agreement of two an- notator pairs (English study, 7 texts); for each  group, line 1 provides fixed marginal kappa, line  2 free marginal kappa", "labels": [], "entities": []}, {"text": " Table 7: Precision and recall for expert versus stu- dent annotation (EE1-EL1)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9984239339828491}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9991043210029602}, {"text": "EE1-EL1", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.861388623714447}]}]}