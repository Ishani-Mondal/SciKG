{"title": [{"text": "On Integrating Discourse in Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6846670061349869}]}], "abstractContent": [{"text": "As the quality of Machine Translation (MT) improves, research on improving discourse in automatic translations becomes more viable.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.8754999279975891}]}, {"text": "This has resulted in an increase in the amount of work on discourse in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9684162139892578}]}, {"text": "However many of the existing models and metrics have yet to integrate these insights.", "labels": [], "entities": []}, {"text": "Part of this is due to the evaluation methodology, based as it is largely on matching to a single reference.", "labels": [], "entities": []}, {"text": "At a time when MT is increasingly being used in a pipeline for other tasks, the semantic element of the translation process needs to be properly integrated into the task.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9775837659835815}]}, {"text": "Moreover, in order to take MT to another level, it will need to judge output not based on a single reference translation , but based on notions of fluency and of adequacy-ideally with reference to the source text.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9919843077659607}]}], "introductionContent": [{"text": "Despite the fact that discourse has long been recognised as a crucial part of translation, when it comes to Statistical Machine Translation (SMT), discourse information has been mostly neglected.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 108, "end_pos": 145, "type": "TASK", "confidence": 0.8233473499615988}]}, {"text": "Recently increasing amounts of effort have been going into addressing discourse explicitly in MT, with research covering lexical cohesion, discourse connectives, discourse relations), pronoun prediction and negation.", "labels": [], "entities": [{"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9861288070678711}, {"text": "pronoun prediction", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.7615523934364319}, {"text": "negation", "start_pos": 207, "end_pos": 215, "type": "TASK", "confidence": 0.9658167362213135}]}, {"text": "Considerable progress was made in the field of SMT over the past two decades, culminating in models which give surprisingly good output given the limited amount of crosslingual information they have.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9920516014099121}]}, {"text": "Neural Machine Translation (NMT) models are now the most performant, to the extent that in the past year they have been the best performing at WMT (, and although deeper than the linguistically superficial SMT, to evaluate progress we need to be able to measure the extent to which these models successfully integrate discourse.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.755683089296023}, {"text": "WMT", "start_pos": 143, "end_pos": 146, "type": "TASK", "confidence": 0.5964594483375549}, {"text": "SMT", "start_pos": 206, "end_pos": 209, "type": "TASK", "confidence": 0.8200864195823669}]}, {"text": "Besides the difficulty of the task, one of the issues preventing progress is alack of understanding regarding the problem: what is the purpose of translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 146, "end_pos": 157, "type": "TASK", "confidence": 0.9497514963150024}]}, {"text": "In order to fulfil its role, MT needs to capture and transfer the communicative message of the Source Text (ST) into the Target Text (TT).", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9782121181488037}]}, {"text": "While MT cannot be expected to assess the pragmatics, in terms of the intended effect on the target audience of the Source Language (SL) and ensuring a corresponding effect on the target audience of the Target Language (TL), there is a basic communicative intent in terms of the semantics which has to surely betaken into account in evaluation, if we are to move beyond stringing together phrase matches.", "labels": [], "entities": [{"text": "MT", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9868647456169128}]}, {"text": "Despite agreement on the shortcomings of BLEU (), for example ( , the standard metrics are still based on comparison to a single reference translation, which is inflexible (requiring a professional translation for every text automatically translated), and is also unrealistic as a text can be translated many ways, all of them valid.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9863727688789368}]}, {"text": "We would also argue that it does not incentivise the integration of deeper linguistic elements.", "labels": [], "entities": []}, {"text": "In the next section (Section 2) we give a brief survey of recent work on Discourse in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.808406412601471}]}, {"text": "We then describe the constraints of SMT architecture (Section 3), followed by a brief description of the translation process from the human translator's perspective (Section 4) and a review of the limitations of the current evaluation paradigm (Section 6).", "labels": [], "entities": [{"text": "SMT architecture", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.9262904822826385}, {"text": "translation process", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8920785188674927}]}], "datasetContent": [{"text": "Current evaluation methods Hardmeier (2012) already touches on the problem of current evaluation methods.", "labels": [], "entities": []}, {"text": "In particular, he mentions the shortcomings of ngram-based metrics and the issue of sentence level evaluation, where much of discourse is document level: \"However, it could be argued that the metric evaluation in the shared task itself was biased since the document-level human scores evaluated against were approximated by averaging human judgments of sentences seen out of context, so it is unclear to what extent the evaluation of a document-level score can be trusted.\"", "labels": [], "entities": [{"text": "sentence level evaluation", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.6397912700970968}]}, {"text": "It has to be pointed out that that human evaluation is also not at document level.", "labels": [], "entities": []}, {"text": "The problems with BLEU are well illustrated in research by , proving that optimizing by BLEU scores can actually lead to a drop in quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9936665892601013}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9948474168777466}]}, {"text": "However, another major problem is the fact that the evaluation of MT output is still largely based on comparison to a single reference or gold standard translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9836843013763428}]}, {"text": "A reference, or gold standard translation, is one version.", "labels": [], "entities": []}, {"text": "A text can be translated in many ways, all of which will reflect the translator's interpretation of what the ST is saying.", "labels": [], "entities": []}, {"text": "To constrain the measure of correctness to a single reference is only consulting one interpretation of the ST.", "labels": [], "entities": []}, {"text": "There could be equally good (or better) examples of MT output which are not being scored as highly as they should, simply because they employ a different lexical choice.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9854119420051575}]}, {"text": "Recently, there has also been a trend towards totally ignoring the ST during evaluation of WMT submissions, where 'human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation' (.", "labels": [], "entities": [{"text": "WMT submissions", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.8294713497161865}]}, {"text": "So human assessors are asked to rate a given translation by how close it is to the reference translation, with no regard to the source text.", "labels": [], "entities": []}, {"text": "The process is treated as a monolingual direct assessment of translation fluency and adequacy.", "labels": [], "entities": [{"text": "translation fluency", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.9418658912181854}]}, {"text": "We would argue that surely adequacy should be based on how well the meaning of the ST has been transferred to the TT, and that to ignore the ST (simply relying on the one rendering of it) is to lose that direct dependency.", "labels": [], "entities": []}, {"text": "Whereas a proper measure of adequacy is whether the translation captures and transfers the semantics from ST to TT.", "labels": [], "entities": []}, {"text": "Moreover, the human assessment of the output has recently become 'researcher based judgments only'-which is also problematic, in that the researchers in question are not generally trained in translation, and some are monolingual.", "labels": [], "entities": []}, {"text": "This means that they will not necessarily capture discourse information, such as the implicit discourse relations of the reference translation, for example, and know to look for them in the MT output.", "labels": [], "entities": []}, {"text": "Not knowing the source language means that you cannot assess the correctness of the output if it alters from the reference.", "labels": [], "entities": []}, {"text": "Moving forward As mentioned by, 'there is a consensus in the MT com-munity that more discourse-aware metrics need to be proposed for this area to move forward'.", "labels": [], "entities": [{"text": "MT com-munity", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.7617841064929962}]}, {"text": "In terms of evaluation in training, one novel idea is the use of post edits in evaluation-this can be seen as more informative and reliable feedback, if done by a human translator, and can be directly used to improve the system.", "labels": [], "entities": []}, {"text": "Post edits could also form the basis of test items.", "labels": [], "entities": []}, {"text": "Both Popovi\u00b4cPopovi\u00b4c (2017); directly or indirectly touch on the issue of evaluation.", "labels": [], "entities": []}, {"text": "As part of her analysis Popovi\u00b4cPopovi\u00b4c (2017) attempts to classify the type of errors made by each system.", "labels": [], "entities": []}, {"text": "A most constructive development, introduces a test suite which while it is common and invaluable in software engineering, is not widespread for this domain.", "labels": [], "entities": []}, {"text": "With the suite of tests they aim to cover different phenomena, and how the systems handle them, saying they aim to focus on new insights not on how well the systems match the reference ().", "labels": [], "entities": []}, {"text": "In the past there have been examples of unit testing for evaluation of MT quality, in particular () who developed theirs for evaluation of different MT systems before financial outlay.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9781384468078613}, {"text": "MT", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9658240675926208}]}, {"text": "Nevertheless, a substantial amount of the logic is still valid: evaluating the strengths and weaknesses of output from various MT systems, with tests focussing on specific aspects (syntactic, lexical ambiguity etc) for particular language pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9708734750747681}]}, {"text": "Ina more general vein, develop test suites for NLP in their Test Suites For Natural Language Processing work, for the general evaluation of NLP systems.", "labels": [], "entities": []}, {"text": "Their test suites aimed to be reusable, focused on particular phenomena and consisted of a database which could identify test items covering specific phenomena.", "labels": [], "entities": []}, {"text": "Similarly, the MT community could potentially develop relevant tests in github, with agreement on format and peer reviews.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9352595806121826}]}, {"text": "This type of method could easily be adopted as a means of evaluation in the context of WMT tasks, and besides being much more informative, would help to pinpoint strengths and weaknesses, leading to more focussed progress.", "labels": [], "entities": [{"text": "WMT tasks", "start_pos": 87, "end_pos": 96, "type": "TASK", "confidence": 0.9328293204307556}]}, {"text": "Existing test suites, such as the ones developed by and, could be integrated and added to, giving a more comprehensive and linguistically-based evaluation of system submissions.", "labels": [], "entities": []}, {"text": "Unit tests can be added to by interested parties, with peer reviewing if appropriate.", "labels": [], "entities": []}, {"text": "The resulting suite could eventually cover a whole host of discourse aspects, and an indication therefore of how different systems perform, and where there is work to be done.", "labels": [], "entities": []}, {"text": "The concept is not new, and could build on previous initiatives and experience, such as () to ensure it is adaptable yet robust, providing a baseline for progress in particular aspects of discourse.", "labels": [], "entities": []}], "tableCaptions": []}