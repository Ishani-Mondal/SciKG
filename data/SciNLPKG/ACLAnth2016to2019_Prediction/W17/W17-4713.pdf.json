{"title": [{"text": "Multi-Domain Neural Machine Translation through Unsupervised Adaptation", "labels": [], "entities": [{"text": "Multi-Domain Neural Machine Translation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5967638939619064}]}], "abstractContent": [{"text": "We investigate the application of Neu-ral Machine Translation (NMT) under the following three conditions posed by real-world application scenarios.", "labels": [], "entities": [{"text": "Neu-ral Machine Translation (NMT)", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.745036726196607}]}, {"text": "First, we operate with an input stream of sentences coming from many different domains and with no predefined order.", "labels": [], "entities": []}, {"text": "Second, the sentences are presented without domain information.", "labels": [], "entities": []}, {"text": "Third, the input stream should be processed by a single generic NMT model.", "labels": [], "entities": []}, {"text": "To tackle the weaknesses of current NMT technology in this unsupervised multi-domain setting, we explore an efficient instance-based adaptation method that, by exploiting the similarity between the training instances and each test sentence , dynamically sets the hyperparame-ters of the learning algorithm and updates the generic model on-the-fly.", "labels": [], "entities": []}, {"text": "The results of our experiments with multi-domain data show that local adaptation outperforms not only the original generic NMT system, but also a strong phrase-based system and even single-domain NMT models specifically optimized on each domain and applicable only by violating two of our afore-mentioned assumptions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The progress towards a more pervasive integration of machine translation (MT) into industrial translation workflows has to confront two interconnected problems.", "labels": [], "entities": [{"text": "machine translation (MT) into industrial translation workflows", "start_pos": 53, "end_pos": 115, "type": "TASK", "confidence": 0.7821369237369962}]}, {"text": "On one side, MT technology should be able to guarantee a high level of flexibility to deliver good-quality output in a wide range of use scenarios (language combinations, genres, domains).", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9879254102706909}]}, {"text": "On the other side, the infrastructures required to reach this objective should be scalable enough to enable the industrial deployment of MT at reasonable cost.", "labels": [], "entities": [{"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.9774921536445618}]}, {"text": "The first problem is a well known one in (statistical) MT: regardless of the paradigm adopted, performance is bounded by the similarity between training and test data.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.8840267658233643}]}, {"text": "The scenario addressed in this paper, in which the input stream comes from a variety of different domains, is atypical example where models trained on generic parallel corpora suffer from data diversity.", "labels": [], "entities": []}, {"text": "Indeed, processing sentences from diverse domains becomes more and more difficult when the distance from the training instances increases.", "labels": [], "entities": []}, {"text": "The more the domains a system is exposed to, the higher the chance to experience drops in translation quality under unseen conditions.", "labels": [], "entities": []}, {"text": "To cope with this issue, MT systems should be flexible enough to adapt to a variety of linguistic differences (e.g. lexical, structural) between different data points.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9912133812904358}]}, {"text": "The second problem is more practical: in absence of flexible models, multi-domain translation scenarios call for infrastructures based on multiple specialised systems, each of which is tuned to maximise performance in a given domain.", "labels": [], "entities": [{"text": "multi-domain translation", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6766816973686218}]}, {"text": "This solution, however, has two evident drawbacks: i) domain-specific models can only be invoked by input sentences presented along with domain information, so that each instance is processed by the right model, and ii) each time anew domain has to be covered, anew dedicated model has to be trained with domain-specific data.", "labels": [], "entities": []}, {"text": "In realworld application scenarios, however, translation requests rarely come with domain information, the notion of domain is per se fuzzy, domain-specific data can be hard to acquire and, most importantly, architectures' costs and scalability are of utmost concern.", "labels": [], "entities": [{"text": "translation requests", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.907988578081131}]}, {"text": "When maintenance costs and architecture scalability come into play, a preferable solution would be to rely on one single model, capable to adapt on-the-fly to input streams of diverse data, without any supervision.", "labels": [], "entities": []}, {"text": "Neural machine translation (, which has recently become the dominant approach in MT, is not immune to the aforementioned problems.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7719231247901917}, {"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9955142140388489}]}, {"text": "Domain drifts are in fact hard to manage by NMT, due to its inherent characteristics.", "labels": [], "entities": [{"text": "Domain drifts", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7253249436616898}]}, {"text": "Different from the phrase-based paradigm, in which training data is explicitly memorised and used in the form of basic translation constituents (phrases), NMT generates a more implicit representation of the data, by compressing and distributing the information over its internal parameters.", "labels": [], "entities": []}, {"text": "Moreover, given the amount of data and time required for training, high flexibility and fast adaptation capabilities of single generic models become key requirements to unleash NMT's potential in industry applications.", "labels": [], "entities": []}, {"text": "To pursue these objectives, we investigate the application of an unsupervised method to adapt on-the-fly a generic NMT model (M g ) and improve translation quality for an input stream of diverse, multi-domain sentences presented in random order.", "labels": [], "entities": []}, {"text": "Our approach is based on a retrieval mechanism that, given an input sentence q, extracts from the pool of parallel data the top (source, target) pairs in terms of similarity between the source and q.", "labels": [], "entities": []}, {"text": "The retrieved pairs are then used to fine-tune the model (M q ), which is then applied to translate q.", "labels": [], "entities": []}, {"text": "Finally, the adapted model is reset to the original parameters (M g ), the next input sentence is read, and soon.", "labels": [], "entities": []}, {"text": "In order to learn more efficiently from the retrieved set, we introduce a dynamic method that, based on the similarity between the test sentence and the retrieved pairs, decides about the hyperparameters to be used by the learning algorithm (i.e. learning rate and number of epochs).", "labels": [], "entities": []}, {"text": "In our experiments with multi-domain data, we observe significant improvements by our approach over the generic NMT system, a strong generic phrase-based MT system, and also specialised NMT models fine-tuned on each domain using domain-specific data.", "labels": [], "entities": []}, {"text": "In particular, by dynamically setting the model hyperparameters, our solution is able to outperform the strong pool of domain-specific NMT systems by +2.8 BLEU scores, in overall.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.999468982219696}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the English side of the train- ing corpora.", "labels": [], "entities": [{"text": "English side of the train- ing corpora", "start_pos": 28, "end_pos": 66, "type": "DATASET", "confidence": 0.9678841382265091}]}, {"text": " Table 2: Statistics of the English side of the test  corpora.", "labels": [], "entities": [{"text": "English side of the test  corpora", "start_pos": 28, "end_pos": 61, "type": "DATASET", "confidence": 0.9422400891780853}]}, {"text": " Table 3: Comparison of the performance of generic PBMT and NMT, domain-specific oracles, and the  adaptive NMT systems on the dev corpora in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9949486255645752}]}]}