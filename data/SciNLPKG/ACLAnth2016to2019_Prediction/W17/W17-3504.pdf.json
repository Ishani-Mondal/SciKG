{"title": [{"text": "Content Selection for Real-time Sports News Construction from Commentary Texts", "labels": [], "entities": []}], "abstractContent": [{"text": "We study the task of constructing sports news report automatically from live commentary and focus on content selection.", "labels": [], "entities": [{"text": "constructing sports news report", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.7042202427983284}]}, {"text": "Rather than receiving every piece of text of a sports match before news construction, as in previous related work, we novelly verify the feasibility of a more challenging setting to generate news report on the flyby treating live text input as a stream.", "labels": [], "entities": []}, {"text": "We design scoring functions to address different requirements of the task and use stream substitution for sentence selection.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7370485961437225}]}, {"text": "Experiments suggest that our proposed framework can already produce comparable results compared with previous work that relies on a supervised learning-to-rank model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Live text commentary services are available on the web and are becoming increasingly popular for sports fans who do not have access to live video streams due to copyright reasons.", "labels": [], "entities": []}, {"text": "Some people may also prefer live texts on portable devices.", "labels": [], "entities": []}, {"text": "The emergence of live texts has produced huge amount of text commentary data.", "labels": [], "entities": []}, {"text": "Currently there exists very few studies about utilizing this rich data source.", "labels": [], "entities": []}, {"text": "On the other hand, manually-written sports news for game reporting usually share the same information and vocabulary as live texts for the corresponding sports game.", "labels": [], "entities": []}, {"text": "Sports news and commentary texts can be treated as two different sources of descriptions for the same sports events.", "labels": [], "entities": []}, {"text": "It is tempting to investigate whether we can utilize the huge amount of live texts to automatically generate sports news for sports game reporting.", "labels": [], "entities": [{"text": "sports game reporting", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.6945550243059794}]}, {"text": "Building an automatic sports news generation system will largely relax the burden of sports news editors, making them free from repetitive efforts for writing while producing sports news more efficiently and covering more sports games.", "labels": [], "entities": [{"text": "sports news generation", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.6809411843617758}]}, {"text": "As a promising starting point, one recent study ( successfully demonstrated that it is technically feasible to generate sports news from given live text commentary scripts.", "labels": [], "entities": []}, {"text": "They treat the task as a special kind of document summarization and adapt supervised learning-to-rank models to learn preference for which sentences should be extracted for construction.", "labels": [], "entities": []}, {"text": "However, sports news providers demand more on automatic generation, from a practical point of view.", "labels": [], "entities": [{"text": "automatic generation", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.6931817680597305}]}, {"text": "Taking this to the extreme, a sports news reporter typically starts writing early following the game proceeding, without even having seen an entire game played to the final minute.", "labels": [], "entities": []}, {"text": "Manually written match reports usually get uploaded within a few minutes after the game, which is rather speedy.", "labels": [], "entities": []}, {"text": "An automatic writer should likewise avoid long wait times until the game finished before the writing procedure.", "labels": [], "entities": []}, {"text": "A more natural way to view the problem is to treat commentary texts as stream data, which come in to the system one by one as input.", "labels": [], "entities": []}, {"text": "Unfortunately, previously used strategies cannot fulfill such requirements.", "labels": [], "entities": []}, {"text": "In this work we proposed a simple framework as a response to stream data requirements.", "labels": [], "entities": []}, {"text": "By studying the properties of the task, we design scoring schemes to address different aspects of the problem.", "labels": [], "entities": []}, {"text": "To extract the subset of commentary texts that maximize the score when the data come in stream, while considering a possible overall length budget constraint, we design an efficient stream substitution algorithm that requires only single pass of data, based on a priority queue implementation.", "labels": [], "entities": []}, {"text": "The overall framework forms a rather simple, efficient, practical approach that produces results comparable to the supervised learning-to-rank framework used by previous studies that involves rather heavy feature engineering, as shown on areal world dataset containing Chinese commentary texts.", "labels": [], "entities": [{"text": "areal world dataset containing Chinese commentary texts", "start_pos": 238, "end_pos": 293, "type": "DATASET", "confidence": 0.8012470773288182}]}], "datasetContent": [{"text": "Similar to the evaluation for traditional summarization tasks, we use the ROUGE metrics ( to automatically evaluate the quality of produced summaries given the gold-standard reference news.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.9309638738632202}, {"text": "ROUGE", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9863781929016113}]}, {"text": "The ROUGE metrics measure summary quality by counting the precision, recall and F-score of overlapping units, such as n-grams and skip grams, between a candidate summary and the reference summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.7925663590431213}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9994787573814392}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9963489770889282}, {"text": "F-score", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9943925738334656}]}, {"text": "Specifically, we report the F-scores of the following metrics in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based) and ROUGE-SU4 (based on skip bigrams with a maximum skip distance of 4).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9957834482192993}, {"text": "ROUGE-1", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9696000218391418}, {"text": "ROUGE-2", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.8832897543907166}, {"text": "ROUGE-SU4", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.8291300535202026}]}, {"text": "Note that the ROUGE scores are computed for each document set, and then the scores are averaged.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9878195524215698}]}, {"text": "We use the ROUGE-1.5.5 toolkit to perform the evaluation.", "labels": [], "entities": [{"text": "ROUGE-1.5.5", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.8579389452934265}]}, {"text": "Note that the results are slightly different with those reported in ().", "labels": [], "entities": []}, {"text": "As we understand, in that work the ROUGE overlaps are calculated based on a rather weak word segmentation tool that breaks many named entities into separated characters or subwords, which boosts the ROUGE quantities slightly larger than expected and may incorrectly reflect the preference between each other.", "labels": [], "entities": [{"text": "ROUGE overlaps", "start_pos": 35, "end_pos": 49, "type": "METRIC", "confidence": 0.8168813288211823}, {"text": "word segmentation", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7545376718044281}]}, {"text": "The ROUGE distance between system outputs and gold standard manually written news, which should be treated as an upper bound, is somewhat close.", "labels": [], "entities": [{"text": "ROUGE distance", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.974551647901535}]}, {"text": "In this work the evaluation is based on another popular Chinese word segmentation toolkit called Jieba, 5 that performs word segmentation results with satisfactory level of accuracy, when provided external sports dictionary.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7012304663658142}, {"text": "word segmentation", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.7622970342636108}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9980393052101135}]}, {"text": "We also conduct manual evaluation in this study.", "labels": [], "entities": []}, {"text": "Specifically, we use the pyramid method () and modified pyramid scores as described in () to manually evaluate the summaries generated by different methods.", "labels": [], "entities": []}, {"text": "We randomly sample 20 games from the data set and manually annotate facts on the gold-standard news.", "labels": [], "entities": []}, {"text": "The annotated facts are mostly describing specific events happened during the game.", "labels": [], "entities": []}, {"text": "Each fact is treated as a Summarization Content Unit (SCU) ().", "labels": [], "entities": []}, {"text": "The number of occurrences for each SCU in the gold-standard news is regarded as the weight of this SCU.", "labels": [], "entities": [{"text": "gold-standard news", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.7479877769947052}]}, {"text": "Two types of scores for peers were computed from the peer annotations.", "labels": [], "entities": []}, {"text": "Both scores area ratio of the sum of the weights of the SCUs found in the generated summary (OBServed) to the sum for an ideal gold-standard news (MAXimum).", "labels": [], "entities": [{"text": "area", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.979332447052002}]}, {"text": "If the number of SCUs of a given weight i that occur in a summary is O i , the sum of the weights of all the SCUs in a 5 https://github.com/fxsjy/jieba/ summary is: In the original pyramid scoring, the number of SCUs used in computing MAX is the same as the number used to compute OBS.", "labels": [], "entities": []}, {"text": "The score is defined as the ratio OBS/M AX.", "labels": [], "entities": [{"text": "OBS/M AX", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7664567232131958}]}, {"text": "Ina more commonly used modified score, M AX M is computed instead of MAX using the average number of SCUs found in all goldstandard news and the score is defined as the ratio OBS/M AX M . This modified version avoids assigning high scores to summaries that have retrieved very few SCUs.", "labels": [], "entities": [{"text": "M AX M", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.8416132728258768}, {"text": "MAX", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9317688941955566}]}, {"text": "We conform to the modified version during pyramid evaluation.", "labels": [], "entities": [{"text": "pyramid evaluation", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7903554141521454}]}], "tableCaptions": [{"text": " Table 5: Example output for the Everton vs Man City game", "labels": [], "entities": [{"text": "Everton vs Man City game", "start_pos": 33, "end_pos": 57, "type": "DATASET", "confidence": 0.7684147953987122}]}, {"text": " Table 6: Score ablation results", "labels": [], "entities": [{"text": "Score ablation", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8228058516979218}]}, {"text": " Table 4: Evaluation results of different approaches", "labels": [], "entities": []}]}