{"title": [{"text": "Classifier Stacking for Native Language Identification", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.7020575602849325}]}], "abstractContent": [{"text": "This paper reports our contribution (team WLZ) to the NLI Shared Task 2017 (essay track).", "labels": [], "entities": [{"text": "NLI Shared Task 2017 (essay track)", "start_pos": 54, "end_pos": 88, "type": "DATASET", "confidence": 0.8025626912713051}]}, {"text": "We first extract lexical and syntactic features from the essays, perform feature weighting and selection, and train linear support vector machine (SVM) classi-fiers each on an individual feature type.", "labels": [], "entities": []}, {"text": "The output of base classifiers, as probabilities for each class, are then fed into a multilayer perceptron to predict the native language of the author.", "labels": [], "entities": []}, {"text": "We also report the performance of each feature type, as well as the best features of a type.", "labels": [], "entities": []}, {"text": "Our system achieves an accuracy of 86.55%, which is among the best performing systems of this shared task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9997475743293762}]}], "introductionContent": [{"text": "Native language identification (NLI) is the task of determining an author's native language (L1) based on their writings in a second language (L2).", "labels": [], "entities": [{"text": "Native language identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8108619848887125}]}, {"text": "NLI works under the assumption that an author's L1 will dispose them towards particular language production patterns in their L2, as influenced by their native language.", "labels": [], "entities": []}, {"text": "This relates to crosslinguistic influence (CLI), a key topic in the field of second language acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages.", "labels": [], "entities": [{"text": "crosslinguistic influence (CLI)", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.6053388357162476}, {"text": "second language acquisition (SLA)", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.8205967942873637}]}, {"text": "The identification of L1-specific features has been used to study language transfer effects in second-language acquisition (, which is useful for developing pedagogical material, teaching methods, L1-specific instructions and generating learner feedback that is tailored to their native language.", "labels": [], "entities": []}, {"text": "The first NLI shared task was held in 2013, and the winner team reported an accuracy of 83.6% on the test data using an SVM classifier with over 400,000 unique features consisting of lexical and POS n-grams occurring in at least two texts in the training set (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9994483590126038}]}, {"text": "In addition to n-gram features, other researchers have also explored syntactic features) and the use of string kernels ().", "labels": [], "entities": []}, {"text": "All NLI shared tasks to date have been based on L2 English data, but NLI research has been extended to at least six other non-English languages.", "labels": [], "entities": []}, {"text": "In addition to using the written responses, a recent trend has been the use of speech transcripts and audio features for dialect identification . The combination of transcripts and acoustic features has also provided good results for dialect identification (.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.7673391997814178}, {"text": "dialect identification", "start_pos": 234, "end_pos": 256, "type": "TASK", "confidence": 0.7855507731437683}]}, {"text": "Following this trend, the 2016 Computational Paralinguistics Challenge () also included an NLI task based on the spoken response.", "labels": [], "entities": []}, {"text": "The NLI Shared Task 2017 attempts to combine these approaches by including a written response (essay) and a spoken response (speech transcript and ivector acoustic features) for each subject.", "labels": [], "entities": []}, {"text": "The task also allows for the fusion of all features.", "labels": [], "entities": []}, {"text": "Ensemble methods using multiple classifiers have proven to be one of the most successful approaches for the task of NLI (, and researchers have reported better results using stacking than a single classifier in other text classification tasks (e.g.,.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 217, "end_pos": 242, "type": "TASK", "confidence": 0.798275093237559}]}, {"text": "In this work we present a stacking model using lexical and syntactic features for NLI Shared Task 2017 ( ), report the performance of different feature types, and show the best features in each type.", "labels": [], "entities": [{"text": "NLI Shared Task 2017", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.7322789132595062}]}, {"text": "The features we use in the final model include character/word/stem n-grams, function word n-grams, and dependency parses.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.8005024492740631}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Total number of features, selected number of features, and accuracy of each feature type. CV:  10-fold cross validation on Train; Dev: trained on Train, tested on Dev; Test: trained on Train and Dev,  tested on Test. Best performance of a feature group on Test is in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9995285272598267}]}, {"text": " Table 2: Final results using different combining  methods. Trained on Train and Dev, tested on Test.", "labels": [], "entities": []}, {"text": " Table 3: Detailed evaluation of our best perform- ing system. Trained on Train and Dev, tested on  Test. Best and worst F1 in bold and italics.", "labels": [], "entities": [{"text": "Test", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.9080075621604919}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9974339604377747}]}]}