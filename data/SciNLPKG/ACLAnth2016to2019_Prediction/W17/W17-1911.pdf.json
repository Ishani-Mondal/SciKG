{"title": [{"text": "Elucidating Conceptual Properties from Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we introduce a method of identifying the components (i.e. dimensions) of word embeddings that strongly signifies properties of a word.", "labels": [], "entities": []}, {"text": "By elucidating such properties hidden in word em-beddings, we could make word embed-dings more interpretable, and also could perform property-based meaning comparison.", "labels": [], "entities": []}, {"text": "With the capability, we can answer questions like \"To what degree a given word has the property cuteness?\" or \"In what perspective two words are similar?\".", "labels": [], "entities": []}, {"text": "We verify our method by examining how the strength of property-signifying components correlates with the degree of proto-typicality of a target word.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modeling the meaning of words has long been studied and served as a basis for almost every kind of NLP tasks.", "labels": [], "entities": [{"text": "Modeling the meaning of words", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8872506141662597}]}, {"text": "Most recent word modeling techniques are based on neural networks, and the word representations produced by such techniques are called word embeddings, which are usually lowdimensional, dense vectors of continuous-valued components.", "labels": [], "entities": [{"text": "word modeling", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.7997671663761139}]}, {"text": "Although word embeddings have been proved for their usefulness in many tasks, the question of what are represented in them is understudied.", "labels": [], "entities": []}, {"text": "Recent studies report empirical evidence that indicates word embeddings may reflect some property information of a target word.", "labels": [], "entities": []}, {"text": "Learning the properties of a word would be helpful because many NLP tasks can be related to \"finding words that possess similar properties\", which include finding synonyms, named entity recognition (NER).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 173, "end_pos": 203, "type": "TASK", "confidence": 0.7844928850730261}]}, {"text": "Without a method for explicating what properties are contained in embeddings, however, researchers have mostly focused on improving the performance in wellknown semantic benchmark tasks (e.g. SimLex-999) as away to find better embeddings.", "labels": [], "entities": []}, {"text": "Performing well in such benchmark tasks is valuable but provides little help in understanding the inside of the black box.", "labels": [], "entities": []}, {"text": "For instance, it is not possible to answer to questions like \"To what degree a given word has the property cuteness?\".", "labels": [], "entities": []}, {"text": "One way to solve this problem is to elucidate properties that are encoded in word embeddings and associate them with task performances.", "labels": [], "entities": []}, {"text": "With the capability, we cannot only enhance our understanding of word embeddings but also make it easier to make comparisons among heterogeneous word embedding models in more coherent ways.", "labels": [], "entities": []}, {"text": "Our immediate goal in this paper is to show the feasibility of explicating properties contained in word embeddings.", "labels": [], "entities": []}, {"text": "Our research can be seen as an attempt to increase the interpretability of word embeddings.", "labels": [], "entities": []}, {"text": "It is inline with an attempt to provide a humanunderstandable explanation for complex machine learning models, with which we can gain enough confidence to use them in decision-making processes.", "labels": [], "entities": []}, {"text": "There has been a line of work devoted to identifying components that are important for performing various NLP tasks such as sentiment analysis or NER.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.938224196434021}, {"text": "NER", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9348490238189697}]}, {"text": "Those works are analogous to ours in that they try to inspect the role of the components in word embedding.", "labels": [], "entities": []}, {"text": "However, they just attempt to identify key features for specific tasks rather than elucidating properties.", "labels": [], "entities": []}, {"text": "In contrast, our question is \"what comprises word embeddings?\" not \"what components are important for performing well in a specific task?\"", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiment dealing with typicality of concepts, we needed both (pre-trained) word embeddings and a dataset that encodes typicality scores of concepts to a set of categories.", "labels": [], "entities": [{"text": "typicality of concepts", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8789760073026022}]}, {"text": "Below we describe two datasets we used in our experiment: HyperLex and Non-Negative Sparse Embedding (NNSE).", "labels": [], "entities": []}, {"text": "Embedding (NNSE) One desirable quality we wanted from the word embeddings to be used in our experiment is that there should be clear contrast between informative and non-informative components.", "labels": [], "entities": []}, {"text": "In ordinary dense word embeddings, usually every component is filled with a non-zero value.", "labels": [], "entities": []}, {"text": "The Non-Negative Sparse Embedding (NNSE) () fulfills the condition in the sense that insignificant components are set to zero.", "labels": [], "entities": []}, {"text": "The NNSE component values falling between 0 and 1 (non-negative) are generated by applying the non-negative sparse coding algorithm) to ordinary word embeddings (e.g. word2vec).", "labels": [], "entities": []}, {"text": "HyperLex is a dataset and evaluation resource that quantifies the extent of the semantic category membership.", "labels": [], "entities": []}, {"text": "A total of 2,616 concept pairs are included in the dataset, and the strength of category membership is given by native English speakers and recorded in graded manner.", "labels": [], "entities": []}, {"text": "This graded category membership can be interpreted as a 'typicality score' (1-10).", "labels": [], "entities": [{"text": "typicality score", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.9682885408401489}]}, {"text": "Some samples are shown in  3 Experiment", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The size of selected categories", "labels": [], "entities": []}, {"text": " Table 3: SIG-PROPS of each category. The strings  in \"Comp. ID\" column are the component IDs  (c1-c300). \"Avg.\" column indicates the average  value of the component across all the concepts un- der that category.", "labels": [], "entities": []}, {"text": " Table 4: Corr(SIG-PROPS, typicality): instrument", "labels": [], "entities": [{"text": "Corr", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9959372282028198}, {"text": "typicality", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.8728232979774475}]}, {"text": " Table 5: Corr(SIG-PROPS, typicality): animal", "labels": [], "entities": [{"text": "Corr", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9962108135223389}, {"text": "SIG-PROPS", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8052380681037903}, {"text": "typicality", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.89351487159729}]}, {"text": " Table 6: Corr(SIG-PROPS, typicality): bird", "labels": [], "entities": [{"text": "Corr", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9977447986602783}, {"text": "SIG-PROPS", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.838573157787323}, {"text": "typicality", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9152566194534302}]}, {"text": " Table 7: Corr(SIG-PROPS, typicality): food", "labels": [], "entities": [{"text": "Corr", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9967441558837891}, {"text": "SIG-PROPS", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9189559817314148}, {"text": "typicality", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9358601570129395}]}, {"text": " Table 8: Corr(SIG-PROPS, typicality): fruit", "labels": [], "entities": [{"text": "Corr", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9954158067703247}, {"text": "SIG-PROPS", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.7446203827857971}, {"text": "typicality", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.8845282793045044}]}]}