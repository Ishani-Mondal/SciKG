{"title": [{"text": "Character and Subword-Based word Representation for Neural Language Modeling prediction", "labels": [], "entities": [{"text": "Character and Subword-Based word Representation", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.5832770526409149}, {"text": "Neural Language Modeling", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7013609011967977}]}], "abstractContent": [{"text": "Most of neural language models use different kinds of embeddings for word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7772375345230103}]}, {"text": "While word embeddings can be associated to each word in the vocabulary or derived from characters as well as fac-tored morphological decomposition, these word representations are mainly used to parametrize the input, i.e. the context of prediction.", "labels": [], "entities": []}, {"text": "This work investigates the effect of using subword units (character and factored morphological decomposition) to build output representations for neural language modeling.", "labels": [], "entities": [{"text": "neural language modeling", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.6857033173243204}]}, {"text": "We present a case study on Czech, a morphologically-rich language , experimenting with different input and output representations.", "labels": [], "entities": []}, {"text": "When working with the full training vocabulary, despite unstable training, our experiments show that augmenting the output word representations with character-based embed-dings can significantly improve the performance of the model.", "labels": [], "entities": []}, {"text": "Moreover, reducing the size of the output look-up table, to let the character-based embeddings represent rare words, brings further improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most of neural language models, such as n-gram models ( are word based and rely on the definition of a finite vocabulary V.", "labels": [], "entities": []}, {"text": "Therefore, a look-up table maps each wordw \u2208 V to a vector of real features, and is stored in a matrix.", "labels": [], "entities": []}, {"text": "While this approach yields significant improvement fora variety of tasks and languages, see for instance) in speech recognition and () in machine translation, it induces several limitations.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.843157947063446}, {"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.8013220131397247}]}, {"text": "For morphologically-rich languages, like Czech or German, the lexical coverage is still an important issue, since there is a combinatorial explosion of word forms, most of which are hardly observed on training data.", "labels": [], "entities": []}, {"text": "On the one hand, growing the look-up table is not a solution, since it would increase the number of parameters without having enough training examples fora proper estimation.", "labels": [], "entities": []}, {"text": "On the other hand, rare words can be replaced by a special token.", "labels": [], "entities": []}, {"text": "This acts as a word class merging very different words without any distinction, while using different word classes to handle outof-vocabulary words (OOVs)) does not really solve this issue, since rare words are difficult to classify.", "labels": [], "entities": []}, {"text": "Moreover, for most inflected or agglutinative forms, as well as for compound words, the word structure is overlooked, wasting parameters for modeling forms that could be more efficiently handled byword decomposition into subwords units.", "labels": [], "entities": []}, {"text": "Using subword units, whether they are built via a different supervised method with embedded language knowledge, or from the training data, has been attempted many times, especially for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.8532514274120331}]}, {"text": "The main goal is to reduce the OOV rate.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9746378660202026}]}, {"text": "While most of them were focused on a specific language, () is a representative example of such a model applied to several morphologically-rich languages.", "labels": [], "entities": []}, {"text": "One of the first occurrences of general language models integrating morphological features to represent words are the factored language model and its neural version ().", "labels": [], "entities": []}, {"text": "Input words are represented by their embedding, plus several other features, some of which include morphemes.", "labels": [], "entities": []}, {"text": "To alleviate the impact of OOVs,) used morphological features for class-based predictions when input words are unknown, obtaining state-of-the-art 1 results on English.", "labels": [], "entities": []}, {"text": "More recently, several types of language models represent words as function of subwords units: using a recursive structure (, or an additive one.", "labels": [], "entities": []}, {"text": "Quite a lot of work has been made on language models that extract features directly from the character sequence, whether they use character n-grams (, or characters composed by a convolutional layer) or a Bi-LSTM layer (.", "labels": [], "entities": []}, {"text": "This avoids using an external morphological analyser.", "labels": [], "entities": []}, {"text": "We can note that these types of models have also been applied with success to several other task, including learning word representations (, POS tagging (, Named entity recognition (), Parsing ( and Machine translation.", "labels": [], "entities": [{"text": "learning word representations", "start_pos": 108, "end_pos": 137, "type": "TASK", "confidence": 0.7032986879348755}, {"text": "POS tagging", "start_pos": 141, "end_pos": 152, "type": "TASK", "confidence": 0.8249126076698303}, {"text": "Named entity recognition", "start_pos": 156, "end_pos": 180, "type": "TASK", "confidence": 0.6049475570519766}, {"text": "Parsing", "start_pos": 185, "end_pos": 192, "type": "TASK", "confidence": 0.9821239113807678}, {"text": "Machine translation", "start_pos": 199, "end_pos": 218, "type": "TASK", "confidence": 0.85598224401474}]}, {"text": "Recently, an exhaustive summary of previous work on word representation by composing subword units was presented in.", "labels": [], "entities": [{"text": "word representation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7645228207111359}]}, {"text": "This work also compares the types of subword unit, how they are composed, and their impact on various morphological typologies.", "labels": [], "entities": []}, {"text": "While recurrent neural networks have shown excellent performances for character-level language modeling, the results of such models are usually worse than those that use word-level prediction, since they have to consider afar longer history of tokens to be able to predict the next one correctly.", "labels": [], "entities": [{"text": "character-level language modeling", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.6802770694096884}, {"text": "word-level prediction", "start_pos": 170, "end_pos": 191, "type": "TASK", "confidence": 0.7211639881134033}]}, {"text": "However, more recent work) seems to obtain very satisfactory results with a supplementary word-level layer that allows a better processing of the longer history.", "labels": [], "entities": []}, {"text": "Our work focuses on replacing output word embeddings by representations built from subwords.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, such a model has only been proposed in, which evaluates the use of convolutional and LSTM layers to build word representations for outputs words.", "labels": [], "entities": []}, {"text": "They allow the model to trade size against perplexity, since their model performs worse than the classic softmax approach, but with far less parameters.", "labels": [], "entities": []}, {"text": "We first propose to study the training of a language model which augments or completely replaces output words representations with character-based representations.", "labels": [], "entities": []}, {"text": "We compare the effect of different architectures, as well as the effect of different input representations.", "labels": [], "entities": []}, {"text": "Our results show that: \u2022 When evaluating perplexity on the full training vocabulary, using an augmented output representation improves the model performance.", "labels": [], "entities": []}, {"text": "\u2022 Not using the look-up table for rare words also improves the model performance.", "labels": [], "entities": []}, {"text": "Finally, we describe a short experiment with factoring the output predictions using a morphological analysis, which we believe could lead to a facilitated word generation when combined with reinflexion models.", "labels": [], "entities": [{"text": "word generation", "start_pos": 155, "end_pos": 170, "type": "TASK", "confidence": 0.7424312233924866}]}, {"text": "Our paper is organized as follows: Section 2 describes the general architecture of the language model, and of the representations used, as well as its training, Section 3 presents the experiments and Section 4 gives our results and discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are carried out on Czech, a morphologically rich language using the different criteria described in section 2.2.", "labels": [], "entities": []}, {"text": "Language models are evaluated with perplexity: overall sequences in the testing data.", "labels": [], "entities": []}, {"text": "Perplexity is computed fora fixed output vocabulary V, which allows to compare models using the same output vocabulary.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9537374377250671}]}, {"text": "However, we can't evaluate model performance on out-of-vocabulary words, since those are to be classified as the unknown token in V.", "labels": [], "entities": []}, {"text": "Our models are implemented with Tensorflow (.", "labels": [], "entities": []}, {"text": "We use the Adam algorithm () with an initial learning rate of 5 * 10 \u22124 for training, over a maximum of 10 epochs, with a batch size of 128 sequences.", "labels": [], "entities": []}, {"text": "However, since the training is often unstable, the model backtracks to the last checkpoint if it does not improve its performance on validation data after 1/10 of an epoch, and stop training after 10 unsuccessful loadings in a row.", "labels": [], "entities": []}, {"text": "To avoid overfitting, we use dropout with probability 0.5 on recurrent layers, and L2 regularization on feedforward layers.", "labels": [], "entities": []}, {"text": "We use two hidden layers, and choose our embeddings dimensions in order to obtain, for each type of representation, an embedding dimension of 150.", "labels": [], "entities": []}, {"text": "In the case of the CNN, we used filters of 3, 5 and 7 characters, of dimension 30, 50, and 70.", "labels": [], "entities": []}, {"text": "Whether we use NCE, blackOut, or importance sampling, we draw k = 500 noise samples by batch.", "labels": [], "entities": [{"text": "NCE", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.6847347617149353}]}, {"text": "For all experiments, we report the perplexity on test data at the end of training.", "labels": [], "entities": []}, {"text": "Results presented in tables 5, 6, 7 are the average of the results obtained on 5 models, and the standard deviation.", "labels": [], "entities": [{"text": "standard", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9642422795295715}]}], "tableCaptions": [{"text": " Table 2: Vocabulary sizes for different frequency  thresholds", "labels": [], "entities": []}, {"text": " Table 3: Vocabulary sizes for subword units", "labels": [], "entities": []}, {"text": " Table 5: Average test perplexities obtained when  training 5 models with target sampling, for various  input/output representations. Results in bold are  the best models for a given output representation.", "labels": [], "entities": []}, {"text": " Table 6: Test perplexity averaged on 5 models  trained with target sampling, for various input rep- resentations and output word look-up table sizes.", "labels": [], "entities": []}, {"text": " Table 7: Test perplexities averaged on 5 models on  lemmas with a multiple objectives cost function.  Results are given for various input/output repre- sentations. In bold are the best models for a given  output representation.", "labels": [], "entities": []}]}