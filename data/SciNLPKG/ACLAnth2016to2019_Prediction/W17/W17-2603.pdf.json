{"title": [{"text": "Machine Comprehension by Text-to-Text Neural Question Generation", "labels": [], "entities": [{"text": "Neural Question Generation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6620562473932902}]}], "abstractContent": [{"text": "We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers.", "labels": [], "entities": []}, {"text": "We show how to train the model using a combination of supervised and reinforcement learning.", "labels": [], "entities": []}, {"text": "After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality.", "labels": [], "entities": []}, {"text": "Most notably, one of these rewards is the performance of a question-answering system.", "labels": [], "entities": []}, {"text": "We motivate question generation as a means to improve the performance of question answering systems.", "labels": [], "entities": [{"text": "question generation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7985186576843262}, {"text": "question answering", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7637978792190552}]}, {"text": "Our model is trained and evaluated on the recent question-answering dataset SQuAD.", "labels": [], "entities": [{"text": "question-answering dataset SQuAD", "start_pos": 49, "end_pos": 81, "type": "DATASET", "confidence": 0.7249188224474589}]}], "introductionContent": [{"text": "People ask questions to improve their knowledge and understanding of the world.", "labels": [], "entities": []}, {"text": "Questions can be used to access the knowledge of others or to direct one's own information-seeking behavior.", "labels": [], "entities": []}, {"text": "Here we study the generation of natural-language questions by machines, based on text passages.", "labels": [], "entities": []}, {"text": "This task is synergistic with machine comprehension (MC), which pursues the understanding of written language by machines at a near-human level.", "labels": [], "entities": [{"text": "machine comprehension (MC)", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6854907393455505}]}, {"text": "Because most human knowledge is recorded in text, this would enable transformative applications.", "labels": [], "entities": []}, {"text": "Many machine comprehension datasets have been released recently.", "labels": [], "entities": []}, {"text": "These generally comprise (document, question, answer) triples (, where the goal is to predict an answer, conditioned on a document and question.", "labels": [], "entities": []}, {"text": "The availability of large * Equal contribution.", "labels": [], "entities": []}, {"text": "\u2020 Supported by funding from Maluuba.", "labels": [], "entities": [{"text": "Maluuba", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.959564745426178}]}, {"text": "Text Passage in 1066 1,2 , duke william ii 3 of normandy conquered england killing king harold ii at the battle of hastings.", "labels": [], "entities": []}, {"text": "the invading normans and their descendants 4 replaced the anglo-saxons as the ruling class of england.", "labels": [], "entities": []}, {"text": "Questions Generated by our System 1) when did the battle of hastings take place?", "labels": [], "entities": []}, {"text": "2) in what year was the battle of hastings fought?", "labels": [], "entities": []}, {"text": "3) who conquered king harold ii at the battle of hastings?", "labels": [], "entities": []}, {"text": "4) who became the ruling class of england?: Examples of conditional question generation given a context and an answer from the SQuAD dataset, using the scheme referred to as R PPL + QA below.", "labels": [], "entities": [{"text": "conditional question generation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.661002109448115}, {"text": "SQuAD dataset", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.915526956319809}]}, {"text": "Bold text in the passage indicates the answers used to generate the numbered questions.", "labels": [], "entities": []}, {"text": "labeled datasets has spurred development of increasingly advanced models for question answering (QA) from text (.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.8510204195976258}]}, {"text": "In this paper we reframe the standard MC task: rather than answering questions about a document, we teach machines to ask questions.", "labels": [], "entities": []}, {"text": "Our work has several motivations.", "labels": [], "entities": []}, {"text": "First, we believe that posing appropriate questions is an important aspect of information acquisition in intelligent systems.", "labels": [], "entities": [{"text": "information acquisition", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7606726288795471}]}, {"text": "Second, learning to ask questions may improve the ability to answer them.", "labels": [], "entities": []}, {"text": "demonstrated that having students devise questions before reading can increase scores on subsequent comprehension tests.", "labels": [], "entities": []}, {"text": "Third, answering the questions inmost existing QA datasets is an extractive task -it requires selecting some span of text within the document -while question asking is comparatively abstractive -it requires generation of text that may not appear in the document.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.7155593037605286}, {"text": "question asking", "start_pos": 149, "end_pos": 164, "type": "TASK", "confidence": 0.7613162994384766}]}, {"text": "Fourth, asking good questions involves skills beyond those used to answer them.", "labels": [], "entities": []}, {"text": "For instance, in existing QA datasets, atypical (document, question) pair specifies a unique answer.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.7144807428121567}]}, {"text": "Conversely, atypical (document, answer) pair maybe associated with multiple questions, since a valid question can be formed from any information or relations which uniquely specify the given answer.", "labels": [], "entities": []}, {"text": "Finally, a mechanism to ask informative questions about documents (and eventually answer them) has many practical applications, e.g.: generating training data for question answering, synthesising frequently asked question (FAQ) documentation, and automatic tutoring systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.7852564454078674}, {"text": "synthesising frequently asked question (FAQ) documentation", "start_pos": 183, "end_pos": 241, "type": "TASK", "confidence": 0.7472465336322784}]}, {"text": "We adapt the sequence-to-sequence approach of for generating questions, conditioned on a document and answer: first we encode the document and answer, then output question words sequentially with a decoder that conditions on the document and answer encodings.", "labels": [], "entities": []}, {"text": "We augment the standard encoder-decoder approach with several modifications geared towards the question generation task.", "labels": [], "entities": [{"text": "question generation task", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8207228183746338}]}, {"text": "During training, in addition to maximum likelihood for predicting questions from (document, answer) tuples, we use policy gradient optimization to maximize several auxiliary rewards.", "labels": [], "entities": [{"text": "predicting questions from (document, answer) tuples", "start_pos": 55, "end_pos": 106, "type": "TASK", "confidence": 0.8093942403793335}]}, {"text": "These include a language-modelbased score for fluency and the performance of a pretrained question-answering model on generated questions.", "labels": [], "entities": []}, {"text": "We show quantitatively that policy gradient increases the rewards earned by generated questions attest time, and provide examples to illustrate the qualitative effects of different training schemes.", "labels": [], "entities": []}, {"text": "To our knowledge, we present the first end-to-end, text-to-text model for question generation.", "labels": [], "entities": [{"text": "question generation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.8135121166706085}]}], "datasetContent": [{"text": "We conducted our experiments on the SQuAD dataset for machine comprehension, a large-scale, human-generated corpus of (document, question, answer) triples.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.7573979198932648}]}, {"text": "Documents are paragraphs from 536 high-PageRank Wikipedia articles covering a variety of subjects.", "labels": [], "entities": []}, {"text": "Questions are posed by crowdworkers in natural language and answers are spans of text in the related paragraph highlighted by the same crowdworkers.", "labels": [], "entities": []}, {"text": "There are 107,785 question-answer pairs in total, including 87,599 training instances and 10,570 development instances.", "labels": [], "entities": []}, {"text": "We use several automatic evaluation metrics to judge the quality of generated questions with respect to the ground-truth questions from the dataset.", "labels": [], "entities": []}, {"text": "We are undertaking a large-scale human evaluation to determine how these metrics align with human judgments.: Automatic metrics on SQuAD's dev set.", "labels": [], "entities": [{"text": "SQuAD's dev set", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.9077323973178864}]}, {"text": "NLL is the negative log-likelihood.", "labels": [], "entities": [{"text": "NLL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6990332007408142}]}, {"text": "BLEU and F1 are computed with respect to the ground-truth questions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9873569011688232}, {"text": "F1", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9989843964576721}]}, {"text": "QA is the F1 obtained by the MPCM model answers to generated questions and PPL is the perplexity computed with the question language model (LM) (lower is better).", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9329354763031006}, {"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9977953433990479}]}, {"text": "PG denotes policy gradient training.", "labels": [], "entities": [{"text": "policy gradient training", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.847191333770752}]}, {"text": "The bottom two lines report performance on ground-truth questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Automatic metrics on SQuAD's dev set.  NLL is the negative log-likelihood. BLEU and F1  are computed with respect to the ground-truth ques- tions. QA is the F1 obtained by the MPCM model  answers to generated questions and PPL is the per- plexity computed with the question language model  (LM) (lower is better). PG denotes policy gradient  training. The bottom two lines report performance  on ground-truth questions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9988893866539001}, {"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9966122508049011}, {"text": "F1", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.9654372334480286}]}, {"text": " Table 4: Comparison of questions from different reward combinations on the same text and answer.", "labels": [], "entities": []}]}