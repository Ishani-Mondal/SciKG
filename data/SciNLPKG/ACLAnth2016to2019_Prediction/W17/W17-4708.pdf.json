{"title": [{"text": "Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.6863066554069519}]}], "abstractContent": [{"text": "Linguistic resources such as part-of-speech (POS) tags have been extensively used in statistical machine translation (SMT) frameworks and have yielded better performances.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) frameworks", "start_pos": 85, "end_pos": 133, "type": "TASK", "confidence": 0.8044519254139492}]}, {"text": "However, usage of such linguistic annotations in neural machine translation (NMT) systems has been left under-explored.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.8413941661516825}]}, {"text": "In this work, we show that multi-task learning is a successful and a easy approach to introduce an additional knowledge into an end-to-end neural attentional model.", "labels": [], "entities": []}, {"text": "By jointly training several natural language processing (NLP) tasks in one system, we are able to leverage common information and improve the performance of the individual task.", "labels": [], "entities": []}, {"text": "We analyze the impact of three design decisions in multi-task learning: the tasks used in training, the training schedule, and the degree of parameter sharing across the tasks, which is defined by the network architecture.", "labels": [], "entities": []}, {"text": "The experiments are conducted for an German to English translation task.", "labels": [], "entities": [{"text": "German to English translation task", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.6863119781017304}]}, {"text": "As additional linguistic resources, we exploit POS information and named-entities (NE).", "labels": [], "entities": []}, {"text": "Experiments show that the translation quality can be improved by up to 1.5 BLEU points under the low-resource condition.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9553629159927368}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9990218877792358}]}, {"text": "The performance of the POS tag-ger is also improved using the multi-task learning scheme.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, there has been a dramatic change in the state-of-the-art techniques for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8245787918567657}]}, {"text": "Ina traditional method, often the best performance is achieved by using a complicated combination of several statistical models, which are individually trained.", "labels": [], "entities": []}, {"text": "For example, POS information was shown to be very helpful to model word reordering between languages, as shown in.", "labels": [], "entities": [{"text": "word reordering between languages", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.8067317381501198}]}, {"text": "While the recent development of end-to-end trained neural models () showed significant gains over traditional approaches, they are often trained only on the parallel data in an end-to-end fashion.", "labels": [], "entities": []}, {"text": "In most cases, therefore, they do not facilitate other knowledge sources.", "labels": [], "entities": []}, {"text": "When parallel data is sparse, exploiting other knowledge sources can be crucial for performance.", "labels": [], "entities": []}, {"text": "Two techniques to integrate the additional resources are well studied.", "labels": [], "entities": []}, {"text": "In one technique, we train a tool on the additional resources (e.g. POS tagger) and then annotate the parallel data using this tool.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.6666324436664581}]}, {"text": "This technique has been applied extensively in SMT systems (e.g.) as well as in some NMT systems (e.g. ). The second technique would be to use the annotated data directly to train the model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9942860007286072}]}, {"text": "The goal of this work is to integrate the additional linguistic resources directly into neural models, in order to achieve better performance.", "labels": [], "entities": []}, {"text": "To do so, we build a multi-task model and train several NLP tasks jointly.", "labels": [], "entities": []}, {"text": "We use an attention-based sequence-tosequence model for all tasks.", "labels": [], "entities": []}, {"text": "Experiments show that we are able to improve the performance on the German to English machine translation task measured in BLEU, BEER and CharacTER.", "labels": [], "entities": [{"text": "German to English machine translation task", "start_pos": 68, "end_pos": 110, "type": "TASK", "confidence": 0.5528654456138611}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9988922476768494}, {"text": "BEER", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9979256391525269}, {"text": "CharacTER", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.5058277249336243}]}, {"text": "Furthermore, we analyze three important decisions when designing multi-task models.", "labels": [], "entities": []}, {"text": "First, we investigated the influence of secondary tasks.", "labels": [], "entities": []}, {"text": "Also, we analyze the influence of training schedule, e.g. whether we need to adjust it in order to get the best performance on the target task.", "labels": [], "entities": []}, {"text": "And finally, we evaluated the amount of parameter sharing enforced by different model architectures.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are (1) that we show multi-task learning is possible within attention-based sequence-to-sequence models, which are state-of-the-art in machine translation and that we analyze the influence of three main design decisions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.7818830907344818}]}], "datasetContent": [{"text": "We conduct experiments using the multi-task approach on three different tasks: machine translation from German to English, German finegrained POS tagging and German NE tagging.", "labels": [], "entities": [{"text": "machine translation from German to English", "start_pos": 79, "end_pos": 121, "type": "TASK", "confidence": 0.8725001911322275}, {"text": "German finegrained POS tagging", "start_pos": 123, "end_pos": 153, "type": "TASK", "confidence": 0.4567267671227455}, {"text": "NE tagging", "start_pos": 165, "end_pos": 175, "type": "TASK", "confidence": 0.651564359664917}]}, {"text": "As briefly mentioned in Section 1, multi-task approach can be helpful when data is sparse.", "labels": [], "entities": []}, {"text": "In order to simulate this, we deploy only German to English TED data for the translation task.", "labels": [], "entities": [{"text": "translation", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9826319217681885}]}, {"text": "The machine translation output is evaluated with BLEU (), BEER () and CharacTER (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6558213382959366}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9992380142211914}, {"text": "BEER", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9989186525344849}, {"text": "CharacTER", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9725183844566345}]}, {"text": "For the POS tags, we report error rates on the small label set as well as on the large label set.", "labels": [], "entities": [{"text": "error", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9434208869934082}]}, {"text": "The results of the initial experiments on the machine translation tasks are shown in.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.8360720872879028}]}, {"text": "The table displays the performance on the validation set and on both test sets.", "labels": [], "entities": []}, {"text": "For all experiments, we first show the BLEU score, then the BEER score and finally the characTER.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9856364727020264}, {"text": "BEER score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9898841977119446}]}, {"text": "First, we show the results of the baseline neural MT system trained on the parallel data (single task).", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9381653070449829}]}, {"text": "As mentioned in the beginning, we simulated a low-resource condition in these experiments by only using the data from TED, which are roughly 185K sentences.", "labels": [], "entities": [{"text": "TED", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.7998170852661133}]}, {"text": "We evaluated models that are trained both on the translation and POS tagging task.", "labels": [], "entities": [{"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9756175875663757}, {"text": "POS tagging task", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.8454527457555135}]}, {"text": "Although the POS data is out-of-domain and significantly smaller than the parallel training data for the translation task (ca.", "labels": [], "entities": [{"text": "POS data", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.6883354634046555}, {"text": "translation task", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.9216510653495789}]}, {"text": "20% of the size), we see improvements for all three architectures consistently in three metrics.", "labels": [], "entities": []}, {"text": "The BLEU scores is improved by more than 1 point and the characTER is reduced by more than 1.5 points.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9620163142681122}, {"text": "characTER", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.8906172513961792}]}, {"text": "The BEER metric score is improved by more than a half point on both sets.", "labels": [], "entities": [{"text": "BEER metric score", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9425417383511862}]}, {"text": "Ina more detailed look at this task, we see that the model sharing the most (shrd Dec) performs better than the baseline, but worse than the other two.", "labels": [], "entities": [{"text": "shrd Dec)", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9523398478825887}]}, {"text": "Therefore, we can conclude that it is helpful to separate the tasks when the components work on different types of data.", "labels": [], "entities": []}, {"text": "Whether it is helpful to share the attention layer (shrd Att) or not (shrd Enc) is not clear from this experiment.", "labels": [], "entities": [{"text": "attention layer (shrd Att)", "start_pos": 35, "end_pos": 61, "type": "METRIC", "confidence": 0.6501097033421198}]}, {"text": "Therefore, we concentrate on these two architectures in the following experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Results of multi-task learning architectures on the machine translation task  (BLEU/BEER/characTER)", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.7634232739607493}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9978942275047302}, {"text": "BEER", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.8234376311302185}]}, {"text": " Table 2: Impact of the training schedule in the machine translation task (BLEU/BEER/characTER)", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.846368690331777}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9985517859458923}, {"text": "BEER", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.8064144849777222}]}, {"text": " Table 3: Results of different multi-task architectures on the POS task", "labels": [], "entities": []}]}