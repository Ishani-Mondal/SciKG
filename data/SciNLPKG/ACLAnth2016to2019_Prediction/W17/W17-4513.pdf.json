{"title": [{"text": "A Pilot Study of Domain Adaptation Effect for Neural Abstractive Summarization", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.740910530090332}, {"text": "Neural Abstractive Summarization", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.6656387746334076}]}], "abstractContent": [{"text": "We study the problem of domain adaptation for neural abstractive summarization.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7097939848899841}, {"text": "neural abstractive summarization", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.7086187303066254}]}, {"text": "We make initial efforts in investigating what information can be transferred to anew domain.", "labels": [], "entities": []}, {"text": "Experimental results on news stories and opinion articles indicate that neural summariza-tion model benefits from pre-training based on extractive summaries.", "labels": [], "entities": []}, {"text": "We also find that the combination of in-domain and out-of-domain setup yields better summaries when in-domain data is insufficient.", "labels": [], "entities": [{"text": "summaries", "start_pos": 85, "end_pos": 94, "type": "TASK", "confidence": 0.966959536075592}]}, {"text": "Further analysis shows that, the model is capable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style fora target domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent text summarization research moves towards producing abstractive summmaries, which better emulates human summarization process and produces more concise summaries).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.6208200007677078}]}, {"text": "Built on the success of sequenceto-sequence learning with encoder-decoder neural networks (), there has been growing interest in utilizing this framework for generating abstractive summaries (;.", "labels": [], "entities": []}, {"text": "The end-to-end learning framework circumvents efforts in feature engineering and template construction as done in previous work (;), by directly learning to detect summary-worthy content as well as generate fluent sentences.", "labels": [], "entities": [{"text": "template construction", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.7190946191549301}]}, {"text": "Nevertheless, training such systems requires large amounts of labeled data, which creates a big hurdle for new domains where training data is scant and expensive to acquire.", "labels": [], "entities": []}, {"text": "Consequently, we raise the following research questions:  \u2022 domain adaptation: whether we can leverage available out-of-domain abstracts or extractive summaries to help train a neural summarization system fora new domain?", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.748235821723938}]}, {"text": "\u2022 transferable component: what information is transferable and what are the limitations?", "labels": [], "entities": []}, {"text": "In this paper, we attempt to shed some light on the above questions by investigating neural summarization on two types of documents with major difference: news stories and opinion articles from The New York Times Annotated Corpus.", "labels": [], "entities": [{"text": "neural summarization", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.5908574759960175}, {"text": "The New York Times Annotated Corpus", "start_pos": 194, "end_pos": 229, "type": "DATASET", "confidence": 0.6557508260011673}]}, {"text": "Sample articles and human written abstracts are shown in.", "labels": [], "entities": []}, {"text": "We select a reasonably simple task on generating short news summary for multi-paragraph documents.", "labels": [], "entities": []}, {"text": "We first investigate the effect of parameter initialization via pre-training on extractive summaries.", "labels": [], "entities": [{"text": "parameter initialization", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6239122003316879}]}, {"text": "A large-scale dataset consisting of 1 million article-extract pairs is collected from The New York Times for use.", "labels": [], "entities": [{"text": "The New York Times", "start_pos": 86, "end_pos": 104, "type": "DATASET", "confidence": 0.9567278176546097}]}, {"text": "Experimental results show that this step improves summarization performance measured by ROUGE) and BLEU ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9699423909187317}, {"text": "ROUGE", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9954877495765686}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9992170333862305}]}, {"text": "We then treat news stories as source domain and opinion articles as target domain, and make initial tries for understanding the feasibility of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.808503657579422}]}, {"text": "Importantly, by testing on opinion article summarization, the model leveraging data from both source and target domains yields better performance than in-domain trained model when in-domain training data is rare.", "labels": [], "entities": [{"text": "opinion article summarization", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6299678087234497}]}, {"text": "Furthermore, we interpret the learned model to understand what information is transferred to anew domain.", "labels": [], "entities": []}, {"text": "In general, a model trained on out-of-domain data can learn to detect summaryworthy content, but may not match the generation style in the target domain.", "labels": [], "entities": []}, {"text": "Concretely, we observe that the model trained on news domain pays similar amount of attention to summary-worthy content (i.e., words reused by human abstracts) when tested on news and opinion articles.", "labels": [], "entities": []}, {"text": "On the other hand, human writers tend to employ new words unseen from the input when constructing opinion abstracts.", "labels": [], "entities": []}, {"text": "End-to-end evaluation results imply that the model trained on out-of-domain data fails to capture this aspect.", "labels": [], "entities": []}, {"text": "The above observations suggest that the neural summarization model learns to 1) identify salient content, and 2) generate summaries with a style as in the training data.", "labels": [], "entities": []}, {"text": "The first element might be transferable to anew domain, while not so much for the second.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our primary data source is The New York Times Annotated Corpus (Sandhaus, 2008) (henceforth called NYT-annotated).", "labels": [], "entities": [{"text": "The New York Times Annotated Corpus (Sandhaus, 2008)", "start_pos": 27, "end_pos": 79, "type": "DATASET", "confidence": 0.8020139905539426}, {"text": "NYT-annotated", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.9260843396186829}]}, {"text": "Compared with other commonly used dataset for abstractive summarization, NYT-annotated has more variation in its abstracts, such as paraphrase and generalization.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.5327870547771454}, {"text": "NYT-annotated", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.902012288570404}]}, {"text": "It also comes with other human labels we could use to characterize the type of articles.", "labels": [], "entities": []}, {"text": "The whole dataset consists of 1.8 million articles, of which 650,000 are annotated with human constructed abstracts.", "labels": [], "entities": []}, {"text": "Articles longer than 15 tokens and abstracts longer than 10 tokens are extracted for use in our study (as in).", "labels": [], "entities": []}, {"text": "The resulting dataset are further separated into two types based on their taxonomy tags 1 : NEWS stories and OPINION articles.", "labels": [], "entities": [{"text": "NEWS stories", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.8326479196548462}]}, {"text": "We believe these two types of documents are different enough in terms of topics, summary style, and lexical level language use, that they could be treated as different domains for our study.", "labels": [], "entities": []}, {"text": "We collected 100,824 We also make use of the section tag, such as Business, Sports, Arts, to calculate the topic distribution for these two domains.", "labels": [], "entities": []}, {"text": "About 57% of the documents of NEWS are about Sports, whereas more than 78% documents of OPINION are about Arts.", "labels": [], "entities": [{"text": "NEWS", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9385656714439392}, {"text": "OPINION", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.6583349108695984}]}, {"text": "We also observe different levels of subjectivity based on the percentage of strong subjective words taken from MPQA lexicon ().", "labels": [], "entities": [{"text": "MPQA lexicon", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9199382066726685}]}, {"text": "On average 4.1% of the tokens in OPINION articles are strong subjective, compared to 2.9% for NEWS stories.", "labels": [], "entities": [{"text": "NEWS stories", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.7795864045619965}]}, {"text": "This shows the topics and word usage are essentially different between these two domains.", "labels": [], "entities": [{"text": "word usage", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.6931872367858887}]}, {"text": "Here we characterize the difference between NEWS and OPINION by analyzing the distribution of word types in abstracts and how often human reuse words from input text to construct the summaries.", "labels": [], "entities": []}, {"text": "Overall, 81.3% of the words in NEWS abstracts are reused from input, compared with 75.8% for OPINION.", "labels": [], "entities": []}, {"text": "The distribution for words of different part-ofspeech is displayed on the left of, which shows that there are relatively more Nouns in OPINION.", "labels": [], "entities": [{"text": "OPINION", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.8704057931900024}]}, {"text": "In the same figure, we display the percentage of words in abstract that are reused from input, which suggests that human tends to reuse more nouns and verbs for NEWS abstracts.", "labels": [], "entities": [{"text": "NEWS abstracts", "start_pos": 161, "end_pos": 175, "type": "TASK", "confidence": 0.6879406273365021}]}, {"text": "Furthermore, the distribution of Named Entities words and subjective words in abstracts are depicted in.", "labels": [], "entities": []}, {"text": "We further collect lead paragraphs and article descriptions for 1,435,735 articles from The New York Times API 2 . About 71% of these descriptions are the first sentences in the lead paragraphs, and thus can be considered as extractive summaries.", "labels": [], "entities": [{"text": "The New York Times API 2", "start_pos": 88, "end_pos": 112, "type": "DATASET", "confidence": 0.9195999801158905}]}, {"text": "About one million lead paragraph and description pairs are retained for pre-training 3 (henceforth NYTextract).", "labels": [], "entities": [{"text": "NYTextract", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.9181345701217651}]}, {"text": "We randomly divide NYTannotated into training (75%), validation (15%), and test (10%) for both news and opinion.", "labels": [], "entities": [{"text": "NYTannotated", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.6761439442634583}]}, {"text": "We use automatic evaluation on recall-oriented ROUGE) and precision-oriented BLEU ().", "labels": [], "entities": [{"text": "recall-oriented ROUGE", "start_pos": 31, "end_pos": 52, "type": "METRIC", "confidence": 0.8956326246261597}, {"text": "precision-oriented", "start_pos": 58, "end_pos": 76, "type": "METRIC", "confidence": 0.9975065588951111}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.7773185968399048}]}, {"text": "We consider ROUGE-2 which measures bigram recall, and ROUGE-L which takes into account the longest common subsequence.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9264612197875977}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9429706931114197}, {"text": "ROUGE-L", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9464650750160217}]}, {"text": "We also evaluate on BLEU which measures precision up to bigrams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.998472273349762}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9993128776550293}]}], "tableCaptions": [{"text": " Table 1: Evaluation based on ROUGE-2 (R-2),", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9732826352119446}]}, {"text": " Table 2: Comparison of generated (Gen) and missed", "labels": [], "entities": []}, {"text": " Table 3: Attention distribution on different word cate-", "labels": [], "entities": []}]}