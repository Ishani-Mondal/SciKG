{"title": [], "abstractContent": [], "introductionContent": [{"text": "This has been a momentous year for the BEA Workshop.", "labels": [], "entities": [{"text": "BEA Workshop", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.5635398626327515}]}, {"text": "In its 12th year, the BEA workshop is, for the first time, being held in conjunction with EMNLP.", "labels": [], "entities": [{"text": "BEA", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.7104510068893433}, {"text": "EMNLP", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.965584397315979}]}, {"text": "In addition, the workshop is being sponsored by the newly formed Special Interest Group: SIG EDU.", "labels": [], "entities": [{"text": "SIG EDU", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.6911177933216095}]}, {"text": "Since the first workshop in 1997, BEA has become the leading venue for sharing and publishing innovative work that uses NLP to develop educational applications.", "labels": [], "entities": [{"text": "BEA", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.6694781184196472}]}, {"text": "The consistent interest and growth of the workshop has clear ties to challenges in education.", "labels": [], "entities": []}, {"text": "The research presented at the workshop highlights advances in the technology and the maturity of the field of NLP in education.", "labels": [], "entities": []}, {"text": "The capabilities serve as a response to educational challenges and are poised to support the needs of a variety of stakeholders, including educators, learners, parents, and administrators.", "labels": [], "entities": []}, {"text": "NLP capabilities now support an array of learning domains, including writing, speaking, reading, and mathematics.", "labels": [], "entities": []}, {"text": "In the writing and speech domains, automated writing evaluation (AWE) and speech assessment applications, respectively, are commercially deployed in high-stakes assessment and instructional settings, including Massive Open Online Courses (MOOCs).", "labels": [], "entities": [{"text": "automated writing evaluation (AWE)", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7141986091931661}]}, {"text": "We also see widelyused commercial applications for plagiarism detection and peer review and explosive growth of mobile applications for game-based applications for instruction and assessment.", "labels": [], "entities": [{"text": "plagiarism detection", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.748406857252121}, {"text": "peer review", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.662830114364624}]}, {"text": "The current educational and assessment landscape continues to foster a strong interest and high demand that pushes the state of the art in AWE capabilities to expand the analysis of written responses to writing genres other than those traditionally found in standardized assessments, especially writing tasks requiring use of sources and argumentative discourse.", "labels": [], "entities": []}, {"text": "Steady growth in the development of NLP-based applications for education has prompted an increased number of workshops that typically focus on a single subfield.", "labels": [], "entities": []}, {"text": "In BEA, we make an effort to have papers from many subfields, for example, tools for automated scoring, automated test-item generation, curriculum development, evaluation of text, dialogue, evaluation of genres beyond essays, feedback studies, and grammatical error correction.", "labels": [], "entities": [{"text": "BEA", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.3812160789966583}, {"text": "automated test-item generation", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.6964321533838908}, {"text": "grammatical error correction", "start_pos": 248, "end_pos": 276, "type": "TASK", "confidence": 0.6073565781116486}]}, {"text": "This year we received a record 62 submissions, and accepted 9 papers as oral presentations and 25 as poster presentation and/or demos, for an overall acceptance rate of 55 percent.", "labels": [], "entities": []}, {"text": "Each paper was reviewed by three members of the Program Committee who were believed to be most appropriate for each paper.", "labels": [], "entities": []}, {"text": "We continue to have a very strong policy to deal with conflicts of interest.", "labels": [], "entities": []}, {"text": "First, we made a concerted effort to not assign papers to reviewers to evaluate if the paper had an author from their institution.", "labels": [], "entities": []}, {"text": "Second, with respect to the organizing committee, authors of papers for which there was a conflict of interest recused themselves from the discussions.", "labels": [], "entities": []}, {"text": "While the field is growing, we do recognize that there is a core group of institutions and researchers who work in this area.", "labels": [], "entities": []}, {"text": "With a higher acceptance rate, we were able to include papers from a wider variety of topics and institutions.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.8792348504066467}]}, {"text": "The papers accepted were selected on the basis of several factors, including the relevance to a core educational problem space, the novelty of the approach or domain, and the strength of the research.", "labels": [], "entities": []}, {"text": "The accepted papers were highly diverse -an indicator of the growing variety of foci in this field.", "labels": [], "entities": []}, {"text": "We continue to believe that the workshop framework designed to introduce work in progress and new ideas needs to be revived, and we hope that we have achieved this with the breadth and variety of research accepted for this workshop, a brief description of which is presented below.", "labels": [], "entities": []}, {"text": "The BEA12 workshop has presentations on Automated Writing Evaluation (AWE), item generation, readability, dialogue and annotation/database schemas, among others: AWE Written Assessments: Whereas much work in scoring at BEA focuses on learner language, Horbach et al. score essays written by proficient native German speakers in a complex writing task.", "labels": [], "entities": [{"text": "BEA12", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8767415881156921}, {"text": "item generation", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.6929765194654465}, {"text": "AWE", "start_pos": 162, "end_pos": 165, "type": "METRIC", "confidence": 0.9067268371582031}]}, {"text": "Madnani et al. look at scoring for content in science, math, language arts and social studies.", "labels": [], "entities": []}, {"text": "Rei looks at detecting off-topic essay responses to visual prompts.", "labels": [], "entities": [{"text": "detecting off-topic essay responses to visual prompts", "start_pos": 13, "end_pos": 66, "type": "TASK", "confidence": 0.7507644380841937}]}, {"text": "Riordan et al. examine neural architectures for scoring responses to short answer questions.", "labels": [], "entities": [{"text": "scoring responses to short answer questions", "start_pos": 48, "end_pos": 91, "type": "TASK", "confidence": 0.7392603556315104}]}, {"text": "Finally, looking at the bigger picture, Burstein et al. explore the relations between AWE and broader educational outcomes.", "labels": [], "entities": []}, {"text": "Domain-Specific AWE: Three papers look at assessments in specific subject domains.", "labels": [], "entities": []}, {"text": "For language learning, Tolmachev and Kurohashi extract exemplar sentences to accompany flash cards.", "labels": [], "entities": [{"text": "language learning", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7342696189880371}]}, {"text": "Tack et al. investigate the feasibility of automated learner English assessment in the CEFR (European) framework.", "labels": [], "entities": [{"text": "automated learner English assessment", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.6097968518733978}, {"text": "CEFR (European) framework", "start_pos": 87, "end_pos": 112, "type": "DATASET", "confidence": 0.9047466278076172}]}, {"text": "In the science domain, Nadeem and Ostendorf look at language-based mapping of science assessment items to skills.", "labels": [], "entities": []}, {"text": "Dialogue: There are two papers on dialogue, but with very different topics.", "labels": [], "entities": []}, {"text": "In the first, Lugini and Litman predict specificity in classroom discussions.", "labels": [], "entities": []}, {"text": "In the second, Jin et al. develop a system for interpreting questions in a virtual patient dialogue system.", "labels": [], "entities": []}, {"text": "This year, the workshop is hosting a Shared Task on Native Language Identification 2 (NLI).", "labels": [], "entities": [{"text": "Shared Task on Native Language Identification 2 (NLI)", "start_pos": 37, "end_pos": 90, "type": "TASK", "confidence": 0.6517961651086808}]}, {"text": "NLI is the process of automatically identifying the native language (L1) of a non-native speaker based solely on language that he or she produces in another language.", "labels": [], "entities": []}, {"text": "Two previous shared tasks on NLI have been organized in which the task was to identify the native language of non-native speakers of English based on essays and spoken responses to a standardized assessment of academic English proficiency.", "labels": [], "entities": []}, {"text": "The first shared task 3 was based on the essays only and was also held with the BEA workshop in 2013.", "labels": [], "entities": [{"text": "BEA workshop in 2013", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.8919521272182465}]}, {"text": "Three years later, Computational Paralinguistics Challenge 4 at Interspeech 2016 hosted a sub-challenge on identifying the native language based solely on the spoken responses.", "labels": [], "entities": [{"text": "Computational Paralinguistics Challenge 4 at Interspeech 2016", "start_pos": 19, "end_pos": 80, "type": "TASK", "confidence": 0.5688014328479767}]}, {"text": "This year's shared task combines the inputs from the two previous tasks.", "labels": [], "entities": []}, {"text": "There are three tracks: NLI on the essay only, NLI on the speech response only, and NLI using both responses from a test taker.", "labels": [], "entities": []}, {"text": "19 teams competed in the NLI shared task, with 17 presenting their systems during the poster session.", "labels": [], "entities": []}, {"text": "A summary report of the shared task (Malmasi et al.) will be presented orally.", "labels": [], "entities": []}, {"text": "We wish to thank everyone who showed interest and submitted a paper, all of the authors for their contributions, the members of the Program Committee for their thoughtful reviews, and everyone who is attending this workshop.", "labels": [], "entities": []}, {"text": "We would especially like to thank our sponsors: at the Gold Level, Turnitin | LightSide, Grammarly and Duolingo; at the Silver level, Educational Testing Service (ETS), Pacific Metrics, National Board of Medical Examiners (NBME), and iLexIR; at the Bronze level, Cognii.", "labels": [], "entities": [{"text": "Turnitin", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.8963263630867004}, {"text": "LightSide", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.6332669854164124}, {"text": "Grammarly", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.902570366859436}, {"text": "Duolingo", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.6263114213943481}, {"text": "Cognii", "start_pos": 263, "end_pos": 269, "type": "DATASET", "confidence": 0.8889971971511841}]}, {"text": "Their contributions help fund workshop extras, such as the dinner which is a great social and networking event, especially for students.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}