{"title": [{"text": "The BURCHAK corpus: a Challenge Data Set for Interactive Learning of Visually Grounded Word Meanings", "labels": [], "entities": [{"text": "BURCHAK corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8864245116710663}, {"text": "Interactive Learning of Visually Grounded Word Meanings", "start_pos": 45, "end_pos": 100, "type": "TASK", "confidence": 0.6280724278518132}]}], "abstractContent": [{"text": "We motivate and describe anew freely available human-human dialogue data set for interactive learning of visually grounded word meanings through osten-sive definition by a tutor to a learner.", "labels": [], "entities": []}, {"text": "The data has been collected using a novel, character-by-character variant of the DiET chat tool (Healey et al., 2003; Mills and Healey, submitted) with a novel task, where a Learner needs to learn invented visual attribute words (such as \"burchak\" for square) from a tutor.", "labels": [], "entities": []}, {"text": "As such, the text-based interactions closely resemble face-to-face conversation and thus contain many of the linguistic phenomena encountered in natural, spontaneous dialogue.", "labels": [], "entities": []}, {"text": "These include self-and other-correction, mid-sentence continuations, interruptions, overlaps, fillers, and hedges.", "labels": [], "entities": [{"text": "mid-sentence continuations", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7549089193344116}]}, {"text": "We also present a generic n-gram framework for building user (i.e. tutor) simulations from this type of incremental data, which is freely available to researchers.", "labels": [], "entities": []}, {"text": "We show that the simulations produce outputs that are similar to the original data (e.g. 78% turn match similarity).", "labels": [], "entities": [{"text": "78% turn match similarity", "start_pos": 89, "end_pos": 114, "type": "METRIC", "confidence": 0.6704306244850159}]}, {"text": "Finally, we train and evaluate a Reinforcement Learning dialogue control agent for learning visually grounded word meanings, trained from the BURCHAK corpus.", "labels": [], "entities": [{"text": "Reinforcement Learning dialogue control", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.805406853556633}, {"text": "learning visually grounded word meanings", "start_pos": 83, "end_pos": 123, "type": "TASK", "confidence": 0.6529862582683563}, {"text": "BURCHAK corpus", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.7555937767028809}]}, {"text": "The learned policy shows comparable performance to a rule-based system built previously.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identifying, classifying, and talking about objects and events in the surrounding environment are key capabilities for intelligent, goal-driven systems that interact with other humans and the exter-  nal world (e.g. robots, smart spaces, and other automated systems).", "labels": [], "entities": [{"text": "Identifying, classifying, and talking about objects and events in the surrounding environment", "start_pos": 0, "end_pos": 93, "type": "TASK", "confidence": 0.5953605494328907}]}, {"text": "To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions (.", "labels": [], "entities": [{"text": "generation of Natural Language (NL) descriptions of images", "start_pos": 128, "end_pos": 186, "type": "TASK", "confidence": 0.7790910184383393}]}, {"text": "Another strand of work has focused on incremental reference resolution in a model where word meaning is modeled as classifiers (the so-called Words-As-Classifiers model ().", "labels": [], "entities": [{"text": "incremental reference resolution", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7111513018608093}]}, {"text": "However, none of this prior work focuses on how concepts/word meanings are learned and adapted in interactive dialogue with a human, the most common setting in which robots, home automation devices, smart spaces etc.", "labels": [], "entities": []}, {"text": "operate, and, indeed the richest resource that such devices could exploit for adaptation overtime to the idiosyncrasies of the language used by their users.", "labels": [], "entities": []}, {"text": "Though recent prior work has focused on the problem of learning visual groundings in interaction with a tutor (see e.g. (), it has made use of hand-constructed, synthetic dialogue examples that thus lack in variation, and many of the characteristic, but consequential phenomena observed in naturalistic dialogue (see below).", "labels": [], "entities": []}, {"text": "Indeed, to our knowledge, there is no existing data set of real human-human dialogues in this domain, suitable for training multimodal conversational agents that perform the task of actively learning visual concepts from a human partner in natural, spontaneous dialogue.", "labels": [], "entities": []}, {"text": "(c) Overlapping Natural, spontaneous dialogue is inherently incremental (, and thus gives rise to dialogue phenomena such as self-and other-corrections, continuations, unfinished sentences, interruptions and overlaps, hedges, pauses and fillers.", "labels": [], "entities": []}, {"text": "These phenomena are interactionally and semantically consequential, and contribute directly to how dialogue partners coordinate their actions and the emergent semantic content of their conversation.", "labels": [], "entities": []}, {"text": "They also strongly mediate how a conversational agent might adapt to their partner overtime.", "labels": [], "entities": []}, {"text": "For example, self-interruption, and subsequent selfcorrection (see example in table 1.b) as well as hesitations/fillers (see example in table 1.e) aren't simply noise and are used by listeners to guide linguistic processing); similarly, while simultaneous speech is the bane of dialogue system designers, interruptions and subsequent continuations (see examples in table 1.c and 1.d) are performed deliberately by speakers to demonstrate strong levels of understanding.", "labels": [], "entities": []}, {"text": "Despite this importance, these phenomena are excluded in many dialogue corpora, and glossed over/removed by state of the art speech recognisers (e.g.) and Google's web-based ASR (; see fora comparison).", "labels": [], "entities": []}, {"text": "One reason for this is that naturalistic spoken interaction is excessively expensive and timeconsuming to transcribe and annotate on a level of granularity fine-grained enough to reflect the strict time-linear nature of these phenomena.", "labels": [], "entities": []}, {"text": "In this paper, we present anew dialogue data set -the BURCHAK corpus -collected using anew incremental variant of the DiET chat-tool ( , which enables character-by-character, text-based interaction between pairs of participants, and which circumvents all transcription effort as all this data, including all timing information at the character level is automatically recorded.", "labels": [], "entities": [{"text": "BURCHAK corpus", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9088008403778076}]}, {"text": "The chat-tool is designed to support, elicit, and record at a fine-grained level, dialogues that resemble the face-to-face setting in that turns are: (1) constructed and displayed incrementally as they are typed; (2) transient; (3) potentially overlapping as participants can type at the same time; (4) not editable, i.e. deletion is not permitted -see Sec.", "labels": [], "entities": []}, {"text": "Thus, we have been able to collect many of the important phenomena mentioned above that arise from the inherently incremental nature of language processing in dialogue -see table 1.", "labels": [], "entities": []}, {"text": "Having presented the data set, we then goon to introduce a generic n-gram framework for building user simulations for either task-oriented or non-task-oriented dialogue systems from this dataset, or others constructed using the same tool.", "labels": [], "entities": []}, {"text": "We apply this framework to train a robust user model that is able to simulate the tutor's behaviour to interactively teach (visual) word meanings to a Reinforcement Learning dialogue agent.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed user simulation based on the turn-level evaluation metrics by, in which evaluation is done on a turn-byturn basis.", "labels": [], "entities": []}, {"text": "Evaluation is done based on the cleaned up corpus (see Section 4).", "labels": [], "entities": []}, {"text": "We investigate the performance of the user model on two levels: the utterance level and the action level.", "labels": [], "entities": []}, {"text": "The evaluation is done by comparing the distribution of the predicted actions or utterances with the actual distributions in the data.", "labels": [], "entities": []}, {"text": "We report two measures: the Accuracy and Kullback-Leibler Divergence (cross-entropy) to quantify how closely the simulated user responses resemble the real user shows the results: the user simulation on both utterance and action levels achieves good performance.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995015859603882}, {"text": "Kullback-Leibler Divergence", "start_pos": 41, "end_pos": 68, "type": "METRIC", "confidence": 0.8601663410663605}]}, {"text": "The action-based user model, on a more abstract level, would likely be better as it is less sparse, and produces more variation in the resulting utterances.", "labels": [], "entities": []}, {"text": "Ongoing work involves using BURCHAK to train a word-by-word incremental tutor simulation, capable of generating all the incremental phenomena identified earlier.", "labels": [], "entities": [{"text": "BURCHAK", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9049432873725891}]}, {"text": "To compare the performance of the rule-based system and the trained RL-based system in the interactive learning process, we follow all experi-ment setup, including visual data-set and crossvalidation method.", "labels": [], "entities": []}, {"text": "We also follow the evaluation metrics provided by (2016b) : Overall Performance Ratio (R perf ) to measures the trade-offs between the cost to the tutor and the accuracy of the learned meanings, i.e. the classifiers that ground our colour and shape concepts.", "labels": [], "entities": [{"text": "Overall Performance Ratio (R perf )", "start_pos": 60, "end_pos": 95, "type": "METRIC", "confidence": 0.871963884149279}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9948689937591553}]}, {"text": "i.e. the increase inaccuracy per unit of the cost, or equivalently the gradient of the curve in We seek dialogue strategies that maximise this.", "labels": [], "entities": []}, {"text": "The cost C tutor measure reflects the effort needed by a human tutor in interacting with the system.", "labels": [], "entities": []}, {"text": "point out that a comprehensive teachable system should learn as autonomously as possible, rather than involving the human tutor too frequently.", "labels": [], "entities": []}, {"text": "The result shows that the RL-based learning agent achieves a comparable performance with the rule-based system.", "labels": [], "entities": []}, {"text": "shows an example dialogue between the learned concept learning agent and the tutor simulation, where the user model simulates the tutor behaviour (T) for the learning tasks.", "labels": [], "entities": []}, {"text": "In this example, the utterance produced by the simulation involves two incremental phenomena, i.e. a selfcorrection and a continuation, though note that these have not been produced on a word-by-word level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation of The User Simulation on  both Utterance and Act levels", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6591521352529526}, {"text": "Utterance", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9130450487136841}]}]}