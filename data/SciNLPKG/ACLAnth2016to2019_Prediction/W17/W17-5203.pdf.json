{"title": [{"text": "Annotation, Modelling and Analysis of Fine-Grained Emotions on a Stance and Sentiment Detection Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "There is a rich variety of data sets for sentiment analysis (viz., polarity and subjec-tivity classification).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9503579139709473}]}, {"text": "For the more challenging task of detecting discrete emotions following the definitions of Ekman and Plutchik, however, there are much fewer data sets, and notably no resources for the social media domain.", "labels": [], "entities": [{"text": "detecting discrete emotions", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.8439708550771078}]}, {"text": "This paper contributes to closing this gap by extending the SemEval 2016 stance and sentiment dataset with emotion annotation.", "labels": [], "entities": [{"text": "SemEval 2016 stance", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.737544854482015}]}, {"text": "We (a) analyse annotation reliability and annotation merging ; (b) investigate the relation between emotion annotation and the other annotation layers (stance, sentiment); (c) report modelling results as a baseline for future work.", "labels": [], "entities": [{"text": "annotation merging", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6788656711578369}]}], "introductionContent": [{"text": "Emotion recognition is a research area in natural language processing concerned with associating words, phrases or documents with predefined emotions from psychological models.", "labels": [], "entities": [{"text": "Emotion recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9283694624900818}]}, {"text": "Discrete emotion recognition assigns categorial emotions), namely Anger, Anticipation, Disgust, Fear, Joy, Sadness, Surprise und Trust.", "labels": [], "entities": [{"text": "Discrete emotion recognition", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.632506807645162}]}, {"text": "Compared to the very active area of sentiment analysis, whose goal is to recognize the polarity of text (e. g., positive, negative, neutral, mixed), few resources are available for discrete emotion analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.9453935623168945}, {"text": "discrete emotion analysis", "start_pos": 181, "end_pos": 206, "type": "TASK", "confidence": 0.666982372601827}]}, {"text": "Emotion analysis has been applied to several domains, including tales (), blogs) and microblogs).", "labels": [], "entities": [{"text": "Emotion analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.931134968996048}]}, {"text": "The latter in particular provides a major data source in the form of user messages from platforms such as Twitter (Costa et al., * We thank Marcus Hepting, Chris Krauter, Jonas Vogelsang, Gisela Kollotzek for annotation and discussion.) which contain semi-structured information (hashtags, emoticons, emojis) that can be used as weak supervision for training classifiers.", "labels": [], "entities": []}, {"text": "The classifier then learns the association of all other words in the message with the \"self-labeled\" emotion (.", "labels": [], "entities": []}, {"text": "While this approach provides a practically feasible approximation of emotions, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations.", "labels": [], "entities": []}, {"text": "In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion.", "labels": [], "entities": []}, {"text": "With this paper, we contribute manual emotion annotation fora publicly available Twitter data set.", "labels": [], "entities": [{"text": "Twitter data set", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.8186564048131307}]}, {"text": "We annotate the SemEval 2016 Stance Data set () which provides sentiment and stance information and is popular in the research community.", "labels": [], "entities": [{"text": "SemEval 2016 Stance Data set", "start_pos": 16, "end_pos": 44, "type": "DATASET", "confidence": 0.7775426864624023}]}, {"text": "It therefore enables further research on the relations between sentiment, emotions, and stances.", "labels": [], "entities": []}, {"text": "For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could contribute to stance detection.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 163, "end_pos": 179, "type": "TASK", "confidence": 0.9686009883880615}]}, {"text": "An additional feature of our resource is that we do not only provide a \"majority annotation\" as is usual.", "labels": [], "entities": []}, {"text": "We do define a well-performing aggregated annotation, but additionally provide the individual labels of each of our six annotators.", "labels": [], "entities": []}, {"text": "This enables further research on differences in the perception of emotions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: A selection of resources for sentiment analysis (on Twitter, 1-7) and emotion analysis (in  general, 8-12). Annotation refers to the following annotation schemes:", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.9716687202453613}, {"text": "emotion analysis", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7549407482147217}]}, {"text": " Table 2: Corpus Statistics. The threshold t mea- sures that a fraction of more than t annotators la- beled the respective emotion (e. g., t=0.0: at least  one annotator t=0.99: all annotators). Overall num- ber of tweets: 4,868.", "labels": [], "entities": [{"text": "num- ber of tweets", "start_pos": 203, "end_pos": 221, "type": "METRIC", "confidence": 0.9186978816986084}]}, {"text": " Table 3: Kappa Statistics for all pairs of annotators.", "labels": [], "entities": [{"text": "Kappa Statistics", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8187237977981567}]}, {"text": " Table 4: Tweet Counts (above diagonal) and odds ratio (below diagonal) for cooccurring annotations for  all classes in the corpus (emotions based on aggregated annotation, t=0.0).", "labels": [], "entities": [{"text": "Tweet Counts", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9305551052093506}, {"text": "odds ratio", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9885085225105286}]}, {"text": " Table 5: Tweet Counts (above diagonal) and odds ratio (below diagonal) for cooccurring annotations for  all classes in the corpus (emotions based on majority annotation, t=0.5).", "labels": [], "entities": [{"text": "Tweet Counts", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9374972283840179}, {"text": "odds ratio", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9873951375484467}]}, {"text": " Table 6: Results of linear and neural models for labels from the aggregated annotation (t=0.0). For the  neural models, we report the average of five runs and standard deviation in brackets. Best F 1 for each  emotion shown in boldface.", "labels": [], "entities": []}, {"text": " Table 7: Results of the BiLSTM for different voting thresholds. We report average results for each emotion  over 5 runs (standard deviations are included in parenthesis).", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.6004419922828674}]}]}