{"title": [{"text": "End-to-End Information Extraction without Token-Level Supervision", "labels": [], "entities": [{"text": "End-to-End Information Extraction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5670173863569895}]}], "abstractContent": [{"text": "Most state-of-the-art information extraction approaches rely on token-level labels to find the areas of interest in text.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7113792151212692}]}, {"text": "Unfortunately , these labels are time-consuming and costly to create, and consequently, not available for many real-life IE tasks.", "labels": [], "entities": [{"text": "IE tasks", "start_pos": 121, "end_pos": 129, "type": "TASK", "confidence": 0.9283028244972229}]}, {"text": "To make matters worse, token-level labels are usually not the desired output, but just an intermediary step.", "labels": [], "entities": []}, {"text": "End-to-end (E2E) models , which take raw text as input and produce the desired output directly, need not depend on token-level labels.", "labels": [], "entities": []}, {"text": "We propose an E2E model based on pointer networks, which can be trained directly on pairs of raw input and output text.", "labels": [], "entities": []}, {"text": "We evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT movie corpus and compare to neural baselines that douse token-level labels.", "labels": [], "entities": [{"text": "ATIS data set", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9445811708768209}, {"text": "MIT restaurant corpus", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.9108880162239075}, {"text": "MIT movie corpus", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9272765119870504}]}, {"text": "We achieve competitive results, within a few percentage points of the baselines, showing the feasibility of E2E information extraction without the need for token-level labels.", "labels": [], "entities": [{"text": "E2E information extraction", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.674033542474111}]}, {"text": "This opens up new possibilities, as for many tasks currently addressed by human extractors, raw input and output data are available, but not token-level labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Humans spend countless hours extracting structured machine readable information from unstructured information in a multitude of domains.", "labels": [], "entities": [{"text": "extracting structured machine readable information from unstructured information", "start_pos": 29, "end_pos": 109, "type": "TASK", "confidence": 0.8332033231854439}]}, {"text": "Promising to automate this, information extraction (IE) is one of the most sought-after industrial applications of natural language processing.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.869888174533844}, {"text": "natural language processing", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6544957260290781}]}, {"text": "However, despite substantial research efforts, in practice, many applications still rely on manual effort to extract the relevant information.", "labels": [], "entities": []}, {"text": "One of the main bottlenecks is a shortage of the data required to train state-of-the-art IE models, which rely on sequence tagging ().", "labels": [], "entities": [{"text": "IE", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.985665500164032}]}, {"text": "Such models require sufficient amounts of training data that is labeled at the token-level, i.e., with one label for each word.", "labels": [], "entities": []}, {"text": "The reason token-level labels are in short supply is that they are not the intended output of human IE tasks.", "labels": [], "entities": [{"text": "IE tasks", "start_pos": 100, "end_pos": 108, "type": "TASK", "confidence": 0.9261994361877441}]}, {"text": "Creating token-level labels thus requires an additional effort, essentially doubling the work required to process each item.", "labels": [], "entities": []}, {"text": "This additional effort is expensive and infeasible for many production systems: estimates put the average cost fora sentence at about 3 dollars, and about half an hour annotator time (.", "labels": [], "entities": []}, {"text": "Consequently, state-of-the-art IE approaches, relying on sequence taggers, cannot be applied to many real life IE tasks.", "labels": [], "entities": [{"text": "IE", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.986782431602478}, {"text": "IE tasks", "start_pos": 111, "end_pos": 119, "type": "TASK", "confidence": 0.9206184446811676}]}, {"text": "What is readily available in abundance and at no additional costs, is the raw, unstructured input and machine-readable output to a human IE task.", "labels": [], "entities": [{"text": "IE task", "start_pos": 137, "end_pos": 144, "type": "TASK", "confidence": 0.9255979061126709}]}, {"text": "Consider the transcription of receipts, checks, or business documents, where the input is an unstructured PDF and the output a row in a database (due date, payable amount, etc).", "labels": [], "entities": [{"text": "transcription of receipts, checks, or business documents", "start_pos": 13, "end_pos": 69, "type": "TASK", "confidence": 0.7834500339296129}]}, {"text": "Another example is flight bookings, where the input is a natural language request from the user, and the output a HTTP request, sent to the airline booking API.", "labels": [], "entities": [{"text": "flight bookings", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.8034080266952515}]}, {"text": "To better exploit such existing data sources, we propose an end-to-end (E2E) model based on pointer networks with attention, which can be trained end-to-end on the input/output pairs of human IE tasks, without requiring token-level annotations.", "labels": [], "entities": []}, {"text": "We evaluate our model on three traditional IE data sets.", "labels": [], "entities": [{"text": "IE data sets", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9250647823015848}]}, {"text": "Note that our model and the baselines are competing in two dimensions.", "labels": [], "entities": []}, {"text": "The first is cost and applicability.", "labels": [], "entities": []}, {"text": "The baselines require tokenlevel labels, which are expensive and unavailable for many real life tasks.", "labels": [], "entities": []}, {"text": "Our model does not re- quire such token-level labels.", "labels": [], "entities": []}, {"text": "Given the time and money required for these annotations, our model clearly improves over the baselines in this dimension.", "labels": [], "entities": []}, {"text": "The second dimension is the accuracy of the models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995856881141663}]}, {"text": "Here we show that our model is competitive with the baseline models on two of the data sets and only slightly worse on the last data set, all despite fewer available annotations.", "labels": [], "entities": []}, {"text": "Contributions We present an E2E IE model with attention that does not depend on costly token-level labels, yet performs competitively with neural baseline models that rely on tokenlevel labels.", "labels": [], "entities": []}, {"text": "By saving both time and money at comparable performance, our model presents a viable alternative for many real-life IE needs.", "labels": [], "entities": [{"text": "IE", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9868481755256653}]}, {"text": "Code is available at github.com/rasmusbergpalm/e2e-ierelease", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Micro average F1 scores on the E2E data  sets. Results that are significantly better (p <  0.05) are highlighted in bold.", "labels": [], "entities": [{"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.8999989628791809}, {"text": "E2E data  sets", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.9918192823727926}]}]}