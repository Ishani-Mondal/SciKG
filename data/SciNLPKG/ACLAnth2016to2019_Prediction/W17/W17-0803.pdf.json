{"title": [{"text": "Crowdsourcing discourse interpretations: On the influence of context and the reliability of a connective insertion task", "labels": [], "entities": [{"text": "Crowdsourcing discourse interpretations", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7072863380114237}, {"text": "connective insertion task", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.7573200563589731}]}], "abstractContent": [{"text": "Traditional discourse annotation tasks are considered costly and time-consuming, and the reliability and validity of these tasks is in question.", "labels": [], "entities": [{"text": "validity", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9638962745666504}]}, {"text": "In this paper, we investigate whether crowdsourcing can be used to obtain reliable discourse relation annotations.", "labels": [], "entities": []}, {"text": "We also examine the influence of context on the reliability of the data.", "labels": [], "entities": [{"text": "reliability", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9605998396873474}]}, {"text": "The results of the crowdsourced connective insertion task showed that the majority of the inserted connectives converged with the original label.", "labels": [], "entities": [{"text": "crowdsourced connective insertion task", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.7593186348676682}]}, {"text": "Further, the distribution of inserted connectives revealed that multiple senses can often be inferred fora single relation.", "labels": [], "entities": []}, {"text": "Regarding the presence of context, the results show no significant difference in distributions of insertions between conditions overall.", "labels": [], "entities": []}, {"text": "However , a by-item comparison revealed several characteristics of segments that determine whether the presence of context makes a difference in annotations.", "labels": [], "entities": []}, {"text": "The findings discussed in this paper can betaken as preliminary evidence that crowd-sourcing can be used as a valuable method to obtain insights into the sense(s) of relations .", "labels": [], "entities": []}], "introductionContent": [{"text": "In order to study discourse coherence, researchers need large amounts of discourse-annotated data, and these data need to be reliable and valid.", "labels": [], "entities": []}, {"text": "However, manually coding coherence relations is a difficult task that is prone to individual variation.", "labels": [], "entities": []}, {"text": "Because the task requires a large amount of time and resources, researchers try to find a balance between obtaining reliable data and sparing resources.", "labels": [], "entities": []}, {"text": "This has led to the standard practice of using two trained, expert annotators to code data.", "labels": [], "entities": []}, {"text": "Not only is this procedure time-consuming and therefore costly, it also raises questions regarding the reliability and validity of the data.", "labels": [], "entities": [{"text": "reliability", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9775069952011108}, {"text": "validity", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9124715924263}]}, {"text": "When using trained, expert annotators, they may agree because they share implicit knowledge and know the purpose of the research well, rather than because they are carefully following instructions.", "labels": [], "entities": []}, {"text": "therefore notes that the more annotators participate in the process and the less expert they are, the more likely they can ensure the reliability of the data.", "labels": [], "entities": [{"text": "reliability", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.9826887845993042}]}, {"text": "In this paper, we investigate how useful crowdsourcing can be in obtaining discourse annotations.", "labels": [], "entities": []}, {"text": "We present an experiment in which subjects were asked to insert (\"drag and drop\") a connecting phrase from a pre-defined list between the two segments of coherence relations.", "labels": [], "entities": []}, {"text": "By employing non-trained, non-expert (also referred as na\u00a8\u0131vena\u00a8\u0131ve) subjects to code the data, large amounts of data can be coded in a short period of time, and it is ensured that the obtained annotations are independent and do not rely on implicit expert knowledge.", "labels": [], "entities": []}, {"text": "Instead, the task allows us to tap into the na\u00a8\u0131vena\u00a8\u0131ve subjects' interpretations directly.", "labels": [], "entities": []}, {"text": "However, crowdsourcing has rarely been used to obtain discourse relation annotations.", "labels": [], "entities": []}, {"text": "This could be due to the nature of crowdsourcing: Typically, crowdsourced tasks are small and intuitive tasks.", "labels": [], "entities": []}, {"text": "Under these conditions, crowdsourced annotators -unlike expert annotators or in-lab na\u00a8\u0131vena\u00a8\u0131ve annotators -cannot be asked to code according to a specific framework because this would require them to study manuals.", "labels": [], "entities": []}, {"text": "Therefore, rather than asking for relation labels, we ask them to insert a connective from a predefined list.", "labels": [], "entities": []}, {"text": "In order to ensure that these connectives are not ambiguous, we chose connectives based on a classification of connective substitutability by.", "labels": [], "entities": []}, {"text": "We investigate how reliable the obtained annotations are by comparing them to expert annotations from two existing corpora.", "labels": [], "entities": []}, {"text": "Moreover, we examine the effect of the design of the task on the reliability of the data.", "labels": [], "entities": [{"text": "reliability", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.978521466255188}]}, {"text": "Researchers agree that discourse relations should be supplied with linguistic context in order to be annotated reliably but there are no clear guidelines for how much context is needed.", "labels": [], "entities": []}, {"text": "The current contribution experimentally examines the influence of context on the interpretation of a discourse relation, with a specific focus on whether there is an interaction between characteristics of the segment and the presence of context.", "labels": [], "entities": [{"text": "interpretation of a discourse relation", "start_pos": 81, "end_pos": 119, "type": "TASK", "confidence": 0.7907123684883117}]}, {"text": "The contributions of this paper include the following: \u2022 We evaluate anew crowdsourcing method to elicit discourse interpretations and obtain discourse annotations, showing that such a task has the potential to function as a reliable alternative to traditional annotation methods.", "labels": [], "entities": []}, {"text": "\u2022 The distributions of inserted connectives per item reveal that, often, annotators converged on two or three dominant interpretations, rather than one single interpretation.", "labels": [], "entities": []}, {"text": "We also found that this distribution is replicable with high reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9833360910415649}]}, {"text": "This is evidence that relations can have multiple senses.", "labels": [], "entities": []}, {"text": "\u2022 We show that the presence of context led to higher annotator agreement when (i) the first segment of a relation refers to an entity or event in the context, or introduces important background information; (ii) the first segment consists of a deranked subordinate clause attaching to the context; or (iii) the context sentence following the relation expands on the second argument of the relation.", "labels": [], "entities": []}, {"text": "This knowledge can be used in the design of discourse relation annotation tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Percentage agreement between the origi- nal label and the dominant response per condition.", "labels": [], "entities": []}]}