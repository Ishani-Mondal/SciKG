{"title": [{"text": "Speech-and Text-driven Features for Automated Scoring of English Speaking Tasks", "labels": [], "entities": [{"text": "Automated Scoring of English Speaking Tasks", "start_pos": 36, "end_pos": 79, "type": "TASK", "confidence": 0.841907282670339}]}], "abstractContent": [{"text": "We consider the automatic scoring of a task for which both the content of the response as well the pronunciation and fluency are important.", "labels": [], "entities": [{"text": "pronunciation", "start_pos": 99, "end_pos": 112, "type": "METRIC", "confidence": 0.9706839323043823}]}, {"text": "We combine features from a text-only content scoring system originally designed for written responses with several categories of acoustic features.", "labels": [], "entities": []}, {"text": "Although adding any single category of acoustic features to the text-only system on its own does not significantly improve performance, adding all acoustic features together does yield a small but significant improvement.", "labels": [], "entities": []}, {"text": "These results are consistent for responses to open-ended questions and to questions focused on some given source material.", "labels": [], "entities": []}], "introductionContent": [{"text": "English language proficiency assessments designed to evaluate speaking ability often include tasks that require the test takers to speak for one or two minutes on a particular topic.", "labels": [], "entities": [{"text": "English language proficiency assessments", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6640903428196907}]}, {"text": "These responses are then evaluated by a human rater in terms of how well the test takers addressed the question as well as the general proficiency of their speech.", "labels": [], "entities": []}, {"text": "Therefore, a system designed to automatically score such responses should combine NLP components aimed at evaluating the content of the response as well as text-based aspects of speaking proficiency such as vocabulary and grammar, and speech-processing components aimed at evaluating fluency and pronunciation.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the automatic scoring of such spoken responses collected as part of a large-scale assessment of English speaking ability.", "labels": [], "entities": []}, {"text": "Our corpus contains responses to two types of questions -both administered as part of the same speaking ability task -that we will refer to as \"source-based\" and \"general\".", "labels": [], "entities": []}, {"text": "For sourcebased questions, test-takers are expected to use the provided materials (e.g., a reading passage) as a basis for their response and, therefore, good responses are likely to have similar content.", "labels": [], "entities": []}, {"text": "In contrast, general questions are more open-ended such as \"What is your favorite food and why?\" and, therefore, the content of such responses can vary greatly across test takers.", "labels": [], "entities": []}, {"text": "In total, our corpus contains over 150,000 spoken responses to 147 different questions, both source-based and general.", "labels": [], "entities": []}, {"text": "We focus our system on two dimensions of proficiency: content, that is how well the test-taker addressed the task, and delivery (pronunciation and fluency).", "labels": [], "entities": []}, {"text": "To evaluate the content of a spoken response, we use features from an existing content-scoring NLP system developed for written responses that uses the textual characteristics of the response to produce a score.", "labels": [], "entities": []}, {"text": "We apply this system to the 1-best ASR (automatic speech recognition) hypotheses for the spoken responses.", "labels": [], "entities": [{"text": "ASR (automatic speech recognition)", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.5714462051788965}]}, {"text": "To evaluate the fluency and pronunciation of the speech in the response, we use features from an existing speech-scoring system that capture information relevant to spoken language proficiency and cannot be obtained just from the ASR hypothesis.", "labels": [], "entities": [{"text": "ASR", "start_pos": 230, "end_pos": 233, "type": "TASK", "confidence": 0.9154635071754456}]}, {"text": "We compare the contributions of several types of features: speech rate, pausing patterns, pronunciation measures based on acoustic model scores and ASR confidence scores as well as more complex features that capture timing patterns and other prosodic properties of the response.", "labels": [], "entities": [{"text": "ASR confidence scores", "start_pos": 148, "end_pos": 169, "type": "METRIC", "confidence": 0.9290536244710287}]}, {"text": "We combine the two types of features (textdriven and speech-driven) and compare the performance of this model to two baseline models, each using only one type of features.", "labels": [], "entities": []}, {"text": "All models are evaluated by comparing the scores obtained from that model to the scores assigned by human raters to the same responses.", "labels": [], "entities": []}, {"text": "We hypothesize that: \u2022 Given the characteristics of the two types of questions, the model with only text-driven features will exhibit better performance for source-based questions as opposed to general ones.", "labels": [], "entities": []}, {"text": "\u2022 Since human raters reward how well the response addresses the question as well as higher spoken proficiency, the combined model that uses both text-driven features (for content) & speech-driven features (for proficiency) will perform better than the individual text-only and speech-only models.", "labels": [], "entities": []}, {"text": "We find that our results generally meet our expectations but interestingly the improvement in performance by combining text-driven & speechdriven features -while significant -is not as large as we had expected, i.e., the combination does not add much over the text-driven features.", "labels": [], "entities": []}, {"text": "We conclude by discussing possible reasons for this observation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Total number of responses for each ques- tion type; the average, median, min and max num- ber of responses per question; the average number  of words in responses to each question computed  based on ASR hypotheses.", "labels": [], "entities": [{"text": "max num- ber", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.8805183619260788}]}, {"text": " Table 2: The five sets of speech features used in  this study along with the number of features in  each group and the average correlations with hu- man score across all features and questions (Pear- son's r).", "labels": [], "entities": [{"text": "hu- man score", "start_pos": 146, "end_pos": 159, "type": "METRIC", "confidence": 0.7501034885644913}, {"text": "Pear- son's r", "start_pos": 195, "end_pos": 208, "type": "DATASET", "confidence": 0.84864581823349}]}, {"text": " Table 3: Average R 2 achieved by different mod- els on different types of questions (N =99 for gen- eral questions and N =48 for source-based ques- tions).", "labels": [], "entities": [{"text": "Average R 2", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.902726431687673}]}]}