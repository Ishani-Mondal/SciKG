{"title": [{"text": "A vis-\u00e0-vis evaluation of MT paradigms for linguistically distant languages", "labels": [], "entities": [{"text": "MT paradigms", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.9319483637809753}]}], "abstractContent": [{"text": "Neural Machine Translation is emerging as the de facto standard for Machine Translation across the globe.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7846429546674093}, {"text": "Machine Translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8682940602302551}]}, {"text": "Statistical Machine Translation has been the state-of-the-art for translation among Indian languages.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7308528621991476}]}, {"text": "This paper probes into the effectiveness of NMT for Indian languages and compares the strengths and weaknesses of NMT with SMT through a visa -vis qualitative estimation on different linguistic parameters.", "labels": [], "entities": [{"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.9548490047454834}]}, {"text": "We compare the outputs of both models for the languages English, Malayalam and Hindi; and test them on various linguistic parameters.", "labels": [], "entities": []}, {"text": "We conclude that NMT works better inmost of the settings, however there is still immense scope for the betterment of accuracy for translation of Indian Languages.", "labels": [], "entities": [{"text": "NMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.546966016292572}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9987541437149048}, {"text": "translation of Indian Languages", "start_pos": 130, "end_pos": 161, "type": "TASK", "confidence": 0.9034827798604965}]}, {"text": "We describe the challenges faces especially when dealing with languages from different language families.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "A key aspect in developing efficient MT systems is addressing the issue of effective metrics for automatic evalution of translations, since manual evaluation is expensive and timeconsuming.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9951395988464355}]}, {"text": "There has been significant interest in this area, both in terms of development as well as evaluation of MT metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9925096035003662}]}, {"text": "The Workshop on Statistical Machine Transla-tion) and the NIST Metrics for Machine Translation 2008 Evaluation 1 have both collected human judgement data to evaluate a wide spectrum of metrics.", "labels": [], "entities": [{"text": "NIST Metrics for Machine Translation 2008 Evaluation 1", "start_pos": 58, "end_pos": 112, "type": "DATASET", "confidence": 0.7615837603807449}]}, {"text": "However, the problem of reordering has not been addressed much so far.", "labels": [], "entities": [{"text": "reordering", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.9761267900466919}]}, {"text": "The primary evalutaion metrics which exist currently for scoring translations are BLEU, METEOR, RIBES and NIST.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.998213529586792}, {"text": "METEOR", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9602885246276855}, {"text": "RIBES", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9766298532485962}, {"text": "NIST", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.9281842112541199}]}, {"text": "BLEU () measures the number of overlapping n-grams in a given translation when compared to a reference translation, giving higher scores to sequential words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9872469902038574}]}, {"text": "METEOR) scores translations using alignments based on exact, stem, synonym, and paraphrase matches between words and phrases.", "labels": [], "entities": []}, {"text": "RIBES () is based on rank correlation coefficients modified with precision.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6204909682273865}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9975687861442566}]}, {"text": "NIST) is a variation of BLEU; where instead of treating all n-grams equally, weightage is given on how informative a particular n-gram is.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8893198370933533}, {"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9976206421852112}]}, {"text": "We report the BLEU score as a measure to test accuracy for the 110 NMT systems to maintain brevity.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9702638983726501}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996658563613892}]}, {"text": "However, for the language-pair English -> Hindi; we report all of the above scores.", "labels": [], "entities": []}, {"text": "We also describe the challenges in evaluating MT accuracy keeping this language pair in consideration, however it should be noted that the same or similar challenges are faced when dealing with other language pairs as well.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.993800938129425}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9062755703926086}]}, {"text": "We use the MT-Eval Toolkit 1 to calculate all these metrics.", "labels": [], "entities": [{"text": "MT-Eval Toolkit 1", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.8469261328379313}]}, {"text": "It can be noted that most of the abovementioned metrics employ some concept of word-order as well as word similarity using ngrams to score translations, which makes evaluating Hindi translations a tedious task.", "labels": [], "entities": []}, {"text": "In addition to this, there exists a many-to-many mapping of vocabulary between English and Hindi which makes all of these scoring mechanisms less effective.", "labels": [], "entities": []}, {"text": "For example, both translations shown in are valid.", "labels": [], "entities": []}, {"text": "However; since the current MT metrics rely heavily on lexical choice, there is no mechanism which takes into account the phenomena described above, which is which is quite common in Indic languages like Hindi.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9788313508033752}]}, {"text": "Hence, in addition to the metric scores, we also show sample examples with their descriptions in the following section, in order to demonstrate translation quality in a more comprehensive manner.", "labels": [], "entities": []}, {"text": "Since the evaluation metrics do not capture how well different linguistic phenomena are handled by our model, we perform a manual investigation and error analysis with the help of linguists.", "labels": [], "entities": []}, {"text": "In order to have a clear insight of NMT performance as compared to SMT on various aspects, we do a side-by-side comparison of the output sentences generated by the SMT and the NMT models respectively.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.939795196056366}]}, {"text": "The linguists were asked to identify the strengths and weaknesses of NMT and SMT by ranking 200 output sentences produced by the respective models in terms of the following parameters: \u2022 Word order  We show the results in.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9645940065383911}]}, {"text": "It can be observed from that SMT produces about twice as more errors in word order and almost thrice as more errors in syntactic and morphological structures and agreement than NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9904748797416687}]}, {"text": "Thus the NMT model is able to perform significantly better than SMT for these phenomena.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9911755323410034}]}, {"text": "This results in much more fluent translations produced by the NMT model -making it a better choice inmost scenarios.", "labels": [], "entities": []}, {"text": "At the same time, the errors made in terms of lexical choice are much more in NMT than SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9731835722923279}]}, {"text": "NMT also produces slightly greater number of errors in terms of missing or additional phrases.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.708646833896637}]}, {"text": "On deeper investigation, it is made clear that a majority of the lexical choice errors are due to the noise present in the training data.", "labels": [], "entities": []}, {"text": "This leads to the insight that NMT is more prone to greater sensitivity to training noise than SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9674540162086487}]}, {"text": "To summarize, NMT performs better than SMT inmost linguistic aspects, particularly in the presence of a high quality training corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9881684184074402}]}], "tableCaptions": [{"text": " Table 3: Results of SMT and NMT on the  ILCI test set", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9920126795768738}, {"text": "NMT", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.53813236951828}, {"text": "ILCI test set", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9564924836158752}]}]}