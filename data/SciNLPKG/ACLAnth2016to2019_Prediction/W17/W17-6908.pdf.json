{"title": [{"text": "Bigger does not mean better! We prefer specificity", "labels": [], "entities": [{"text": "specificity", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.7478017210960388}]}], "abstractContent": [{"text": "This paper studies the applicability of word2vec to the task of extracting similar words from small, domain-specific data.", "labels": [], "entities": []}, {"text": "Results show that, even though the general tendency of the community is to focus on using more and more data, the specificity of the corpus has much more influence on word2vec results than its size.", "labels": [], "entities": []}, {"text": "Actually, when the goal is to automatically detect similar words that are domain specific, it is necessary to have a corpus that correctly represents the use of those specific words more than to have huge amounts of data unrelated to the targeted language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dealing with the automatic extraction of related terms is a trending topic on Natural Language Processing (NLP) area.", "labels": [], "entities": [{"text": "automatic extraction of related terms", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.7933104872703552}]}, {"text": "From synonym extraction, ontology creation or automatic gazetteer building, this is a challenging task approached by many in many publications and shared tasks.", "labels": [], "entities": [{"text": "synonym extraction", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.939566433429718}, {"text": "ontology creation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7546960115432739}, {"text": "gazetteer building", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7753822505474091}]}, {"text": "This paper presents a set of experiments on finding similar words in very specific domains.", "labels": [], "entities": []}, {"text": "This work is framed on a bigger project on performing classification of customer reviews for different companies in the Customer Relationship Management (CRM) domain.", "labels": [], "entities": [{"text": "classification of customer reviews", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.8564828336238861}, {"text": "Customer Relationship Management (CRM) domain", "start_pos": 120, "end_pos": 165, "type": "TASK", "confidence": 0.7912267531667437}]}, {"text": "To enrich the classification system, a taxonomy that assigns a semantic tag to the terms that are relevant to the domain was developed.", "labels": [], "entities": []}, {"text": "Currently, this is a manual work that is very time consuming, especially given that CRM domain is in fact a combination of sub-domains, or business sectors.", "labels": [], "entities": [{"text": "CRM domain", "start_pos": 84, "end_pos": 94, "type": "TASK", "confidence": 0.8932500183582306}]}, {"text": "This means that every time the data from a company operating in anew sector is to be treated, the taxonomy needs to be enriched to cover the terms specific to the new sector.", "labels": [], "entities": []}, {"text": "Doing that manually is demanding in time and resources, and it is difficult to assure a good coverage, so the present study explores how to automatize this step.", "labels": [], "entities": []}, {"text": "Thus, this paper proposes to use a small existing taxonomy developed by hand, and to automatically enrich it with terms that are semantically similar to the words already present in the taxonomy, we call them the seed words.", "labels": [], "entities": []}, {"text": "This paper presents an approach to extract related terms in domain-specific corpora by using distributional hypothesis, which permits to extract words which share similar contexts and consequently same senses.", "labels": [], "entities": []}, {"text": "Specifically, word2vec () caught our attention because of its impressive performances in semantic extraction tasks in many works of NLP.", "labels": [], "entities": [{"text": "semantic extraction tasks", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.8007407387097677}]}, {"text": "The key point of this study is the very reduced size of the domain-specific corpus.", "labels": [], "entities": []}, {"text": "Even though it is a limitation for this kind of tool, these experiments show that size is not the only parameter that matters.", "labels": [], "entities": []}, {"text": "Indeed, in the present case, we obtained better results with a small, specific corpus than with a huge, general domain amount of words.", "labels": [], "entities": []}], "datasetContent": [{"text": "Word2Vec can be used with two architectures: Skip Gram Negative Sampling (SGNS) and Continuous Bag of Words (CBOW), both based on a prediction system that works with a neural network where words are represented by vectors.", "labels": [], "entities": []}, {"text": "In the present case, experiments were conducted with SGNS architecture that is, according to, better for semantic relations.", "labels": [], "entities": []}, {"text": "Word2vec disposes of several parameters that can be adapted to improve the results such as the window size, the layer size or the number of iteration on the corpus.", "labels": [], "entities": []}, {"text": "For the present experiments, it was decided to use a window size of 2 words, 400 dimensions for the vectors and 5 iterations on the corpus.", "labels": [], "entities": []}, {"text": "Several experiments were performed to study the performances and limitations of word2vec when applied to CRM domain.", "labels": [], "entities": []}, {"text": "For each configuration, the list of closest neighbors for seed words was generated with word2vec.", "labels": [], "entities": []}, {"text": "Different tests were conducted with different thresholds between 0.1 and 0.6, meaning that all words which are above the threshold are considered neighbors of the seed word (thus, semantically close).", "labels": [], "entities": []}, {"text": "The different configurations performances were computed by using the gold-standard, over which was calculated micro-averaged Precision, Recall and F1.", "labels": [], "entities": [{"text": "Precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9697234034538269}, {"text": "Recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9950007796287537}, {"text": "F1", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.9987326264381409}]}], "tableCaptions": [{"text": " Table 2. Comparison between the best results of Word2vec and Random Indexing", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9591355323791504}]}]}