{"title": [{"text": "Semantic Composition via Probabilistic Model Theory", "labels": [], "entities": [{"text": "Semantic Composition", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8706598579883575}]}], "abstractContent": [{"text": "Semantic composition remains an open problem for vector space models of semantics.", "labels": [], "entities": [{"text": "Semantic composition", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8777903914451599}]}, {"text": "In this paper, we explain how the probabilistic graphical model used in the framework of Functional Distri-butional Semantics can be interpreted as a probabilistic version of model theory.", "labels": [], "entities": []}, {"text": "Building on this, we explain how various semantic phenomena can be recast in terms of conditional probabilities in the graphical model.", "labels": [], "entities": []}, {"text": "This connection between formal semantics and machine learning is helpful in both directions: it gives us an explicit mechanism for modelling context-dependent meanings (a challenge for formal semantics), and also gives us well-motivated techniques for composing distributed representations (a challenge for distributional semantics).", "labels": [], "entities": []}, {"text": "We present results on two datasets that go beyond word similarity, showing how these semantically-motivated techniques improve on the performance of vector models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models of semantics are popular in NLP, as they are easy to work with, can be trained on unannotated corpora, and are useful in many tasks.", "labels": [], "entities": []}, {"text": "They can be trained in multiple ways, including count methods and neural embedding methods ().", "labels": [], "entities": []}, {"text": "Furthermore, they allow a natural and computationally efficient measure of similarity, in the form of cosine similarity.", "labels": [], "entities": []}, {"text": "However, even if we can train models that produce good similarity scores, a vector space does not provide natural operations for other aspects of meaning.", "labels": [], "entities": []}, {"text": "How can vectors be composed to form semantic representations for larger phrases?", "labels": [], "entities": []}, {"text": "Can we say that one vector implies another?", "labels": [], "entities": []}, {"text": "How do we capture how meanings vary according to context?", "labels": [], "entities": []}, {"text": "An overview of existing approaches to these questions is given in \u00a72, but these issues do not have clear solutions.", "labels": [], "entities": []}, {"text": "In contrast, the framework of Functional Distributional Semantics (Emerson and Copestake, 2016) (henceforth E&C) aims to overcome such issues, not by extending a vector space model, but by learning a different kind of representation.", "labels": [], "entities": [{"text": "Functional Distributional Semantics (Emerson and Copestake, 2016)", "start_pos": 30, "end_pos": 95, "type": "TASK", "confidence": 0.7004056334495544}]}, {"text": "Each predicate is represented not by a vector, but by a function, which forms part of a probabilistic graphical model.", "labels": [], "entities": []}, {"text": "In \u00a73, we build on the description given by E&C, and explain how this graphical model can in fact be viewed as encapsulating a probabilistic version of model theory.", "labels": [], "entities": []}, {"text": "With this connection, we can naturally transfer concepts informal semantics to this probabilistic framework, and we culminate in \u00a73.5 by showing how generalised quantifiers can be interpreted in our probabilistic model.", "labels": [], "entities": []}, {"text": "In \u00a74, we look at how to naturally derive context-dependent representations, and further, how these representations can be used for certain kinds of inference and semantic composition.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 163, "end_pos": 183, "type": "TASK", "confidence": 0.7472617924213409}]}, {"text": "In \u00a75, we turn to using the model in practice, and evaluate on three tasks.", "labels": [], "entities": []}, {"text": "Firstly, we look at lexical similarity, to show it is competitive with vector-based models.", "labels": [], "entities": []}, {"text": "Secondly, we consider the dataset produced by, which measures the similarity of verbs in the context of a specific subject and object.", "labels": [], "entities": []}, {"text": "Finally, we consider the RELPRON dataset produced by, which requires matching individual nouns to short phrases including relative clauses.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9208961725234985}]}, {"text": "Our aim is to show that, not only does the connection with formal semantics give us well-motivated techniques to tackle these disparate datasets, but this also leads to improvements in performance.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Spearman rank correlation with average annotator judgements, for SimLex-999 (SL) noun and  verb subsets, SimVerb-3500, MEN, and WordSim-353 (WS) similarity and relatedness subsets. Note  that we would like to have a low score for WS Rel (which measures relatedness, rather than similarity).", "labels": [], "entities": [{"text": "WS Rel", "start_pos": 240, "end_pos": 246, "type": "TASK", "confidence": 0.3801869750022888}]}, {"text": " Table 2: Spearman rank correlation with average annotator judgements, on the GS2011 dataset, and  mean average precision on the RELPRON development and test sets. For RELPRON, the Word2Vec  model was trained on a larger training set, so that we can directly compare with Rimell et al.'s results.  For GS2011, the ensemble model uses SVO Word2Vec, while for RELPRON, it uses normal Word2Vec.", "labels": [], "entities": [{"text": "GS2011 dataset", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9914281964302063}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.8508930802345276}, {"text": "RELPRON development and test sets", "start_pos": 129, "end_pos": 162, "type": "DATASET", "confidence": 0.6671728909015655}, {"text": "GS2011", "start_pos": 302, "end_pos": 308, "type": "DATASET", "confidence": 0.899836003780365}]}]}