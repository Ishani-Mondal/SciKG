{"title": [{"text": "A BiLSTM-based System for Cross-lingual Pronoun Prediction", "labels": [], "entities": [{"text": "Cross-lingual Pronoun Prediction", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.667731781800588}]}], "abstractContent": [{"text": "We describe the Uppsala system for the 2017 DiscoMT shared task on cross-lingual pronoun prediction.", "labels": [], "entities": [{"text": "Uppsala", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9583622217178345}, {"text": "DiscoMT shared task", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.4803816576798757}, {"text": "cross-lingual pronoun prediction", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.7154220938682556}]}, {"text": "The system is based on a lower layer of BiLSTMs reading the source and target sentences respectively.", "labels": [], "entities": []}, {"text": "Classification is based on the BiLSTM representation of the source and target positions for the pronouns.", "labels": [], "entities": []}, {"text": "In addition we enrich our system with dependency representations from an external parser and character representations of the source sentence.", "labels": [], "entities": []}, {"text": "We show that these additions perform well for German and Span-ish as source languages.", "labels": [], "entities": []}, {"text": "Our system is competitive and is in first or second place for all language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-lingual pronoun prediction is a classification approach to directly estimate the translation of a pronoun, without generating a full translation of the segment containing the pronoun.", "labels": [], "entities": [{"text": "Cross-lingual pronoun prediction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.691866377989451}]}, {"text": "The task is restricted to pronouns at subject positions only and it is defined as a \"fill-in-the-gap-task\": given an input text and a translation with placeholders, replace the placeholders with pronouns.", "labels": [], "entities": []}, {"text": "Word alignment links of the placeholders to the source sentence are also given.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6256687939167023}]}, {"text": "This setting allows to analyze both the source and the target languages to create features, potentially providing the means to understand the different aspects involved in pronoun translation.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.7508638501167297}]}, {"text": "First formalized by, the approach was introduced as a shared task at the DiscoMT 2015.", "labels": [], "entities": [{"text": "DiscoMT 2015", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.8836212158203125}]}, {"text": "In 2016, the shared task included more language pairs and lemmatized target data (.", "labels": [], "entities": []}, {"text": "This year's edition src me ayudan a ser escuchada \"me help 3.Pers.Pl to be heard\" trg REPLACE help me to be heard pos PRON VERB PRON PART AUX VERB ref They help me to be heard also features lemmatized target data and it includes the Spanish-English language pair, which introduces pro-drops or null subjects to the task.", "labels": [], "entities": [{"text": "REPLACE", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.980562686920166}]}, {"text": "These refer to omitted subject pronouns whose interpretation is recovered through the verb's morphology, as shown in.", "labels": [], "entities": []}, {"text": "Given the success of neural networks for crosslingual pronoun classification, we wanted to explore this type of system architecture.", "labels": [], "entities": [{"text": "crosslingual pronoun classification", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.6891087790330251}]}, {"text": "Our system is based on BiLSTMs enhanced with information about the source pronoun, the pronoun's syntactic head dependency and character-level representations of the source words.", "labels": [], "entities": []}, {"text": "Our system ranked first for EnglishGerman, with 10 percentage points of macro recall ahead of the second best team.", "labels": [], "entities": [{"text": "EnglishGerman", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9549902081489563}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9653546810150146}]}, {"text": "For the other three language pairs, the system obtained the second best macro recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9853417873382568}]}, {"text": "In addition, our system reached the highest accuracy for three out of the four language pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9993624091148376}]}], "datasetContent": [{"text": "We use only the training data provided by the shared task ().", "labels": [], "entities": []}, {"text": "1 For development data, we concatenate all available development data for each language pair.", "labels": [], "entities": []}, {"text": "Test data is the official shared task test data.", "labels": [], "entities": [{"text": "Test data", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7386580407619476}]}, {"text": "For training data we either concatenate all available training data, or use only the in-domain IWSLT data, which contains TED talks.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.915763646364212}]}, {"text": "In addition, we perform experiments with a very simple domain adaptation technique in the spirit of, but applying it to different domains instead of to different languages.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7190371602773666}]}, {"text": "We first train models on all available data, then continue training these models for additional epochs using only in-domain IWSLT data.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 124, "end_pos": 134, "type": "DATASET", "confidence": 0.8839585185050964}]}, {"text": "While the source side sentences are regular inflected words, the target side sentences are given as lemmas with POS-tags.", "labels": [], "entities": []}, {"text": "In order to utilize richer representations for the source side we tag and parse the source data.", "labels": [], "entities": []}, {"text": "For English and German we use Mate Tools and for Spanish we use UD-Pipe (.", "labels": [], "entities": []}, {"text": "To achieve a flat representation, we represent each source word by its word form, POStag and the dependency label for its head (e.g. woman|NOUN|SBJ, false|JJ|NMOD).", "labels": [], "entities": [{"text": "POStag", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.8211304545402527}]}, {"text": "After parsing, all input words and lemmas are lowercased, and all numerals are replaced by a single token.", "labels": [], "entities": []}, {"text": "We give results on two metrics, macro-recall (macro-R) and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.998806357383728}]}, {"text": "Macro-R is the official shared task metric.", "labels": [], "entities": []}, {"text": "It gives the average recall for each pronoun class, thus giving the same importance to rare classes as to common classes.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993410706520081}]}, {"text": "We also give unofficial accuracy scores, to give a more balanced view of system performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9453292489051819}]}, {"text": "All scores are given on both the official test data and dev data.", "labels": [], "entities": [{"text": "official test data", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.6915571987628937}]}, {"text": "First we performed experiments to evaluate the different components of our network, using only IWSLT data.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.8999079465866089}]}, {"text": "These experiments are run for 100 epochs with proportional sampling and 10% of the training data in each epoch.", "labels": [], "entities": []}, {"text": "shows the results on development data and shows the results on test data.", "labels": [], "entities": []}, {"text": "We can note a marked difference in performance for English as a target language on the one hand, and English as a source language on the other hand, which interestingly mirrors previous results with an SVM classifier.", "labels": [], "entities": []}, {"text": "With German or Spanish as the source, nearly all the components are useful, and discarding them all results in a large performance drop on both metrics.", "labels": [], "entities": []}, {"text": "Using the source pronoun head in the MLP was highly useful for German, but not used for Spanish, where the source pronoun is already encoded in the verb.", "labels": [], "entities": [{"text": "MLP", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.7388257384300232}]}, {"text": "When English is the source language, we see little effect of any component; some of them even hurt performance slightly.", "labels": [], "entities": []}, {"text": "The all system did give slightly better scores than the none system even in this direction, though, so we decided to use the all components system for all languages in our submission.", "labels": [], "entities": []}, {"text": "For our main experiments, we used all training data and different sampling schemes.", "labels": [], "entities": []}, {"text": "For the equal and proportional sampling schemes we used samples containing 10% of the data and ran the system for 72 hours, which resulted in 36-66 epochs, depending on the language pair and sampling scheme.", "labels": [], "entities": []}, {"text": "When domain adaptation is used, we ran an additional 100 epochs with the same settings but only IWSLT data, as a final step.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7466203272342682}, {"text": "IWSLT data", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.8320048451423645}]}, {"text": "For offline sampling, we precomputed 500 samples per training file, and ran 860-1204 epochs.", "labels": [], "entities": []}, {"text": "shows the results of these experiments for development and test data.", "labels": [], "entities": []}, {"text": "Using all data and proportional sampling improves over using only IWSLT, but to different degrees for the different language pairs.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.7196941375732422}]}, {"text": "Overall we see that for several language pairs the scores are quite different on dev and test data.", "labels": [], "entities": []}, {"text": "For English-German, macro-R on testis higher, which can be explained by the missing rare class man in the test data.", "labels": [], "entities": []}, {"text": "For German-English macro-R is lower on test, which can be explained by our system failing to predict   the very few instances of two rare classes.", "labels": [], "entities": []}, {"text": "For Spanish-English, the scores on both metrics are overall lower for all classes in test, for which we can see no clear explanation.", "labels": [], "entities": []}, {"text": "We expected to see a trade-off between macro-R and accuracy for the equal sampling compared with the other sampling methods, like for who used weighted loss.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9996480941772461}]}, {"text": "For the dev data we see clearly higher macro-R with equal sampling, but, less of a difference for accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9991766810417175}]}, {"text": "For the test data with domain adaption, though, scores on both metrics are either better or similar with equal sampling compared to the other sampling methods.", "labels": [], "entities": []}, {"text": "This means that the system with equal sampling performs strongly on both metrics, contrary to our expectations, making it clearly the best choice for this task.", "labels": [], "entities": []}, {"text": "We believe that one partial reason for this could be that we choose the best epoch based on the average of the two metrics.", "labels": [], "entities": []}, {"text": "Domain adaptation improved the results slightly inmost cases on dev data.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7756940126419067}]}, {"text": "On the test data, we also saw improvements or stable results inmost cases, the exceptions being proportional sampling for English-French and Spanish-English, where we saw a small drop in results.", "labels": [], "entities": []}, {"text": "We also note that all of our systems are considerably better than the shared task LM-based baseline, shown in, on both metrics.", "labels": [], "entities": []}, {"text": "For our shared task submission we used the system with equal sampling and domain adaptation as our primary system, bold in, since it had the best macro-R scores on the development set.", "labels": [], "entities": []}, {"text": "We used the system with proportional sampling with domain adaptation as our secondary system, italic in.", "labels": [], "entities": []}, {"text": "Our systems perform well in the shared task, achieving first and second places for both macro-R and accuracy in all cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.99931800365448}]}, {"text": "Our primary systems have high scores on both macro-R and accuracy, in contrast to most other systems in the shared task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9994148015975952}]}], "tableCaptions": [{"text": " Table 2: Development results with different system settings, training with IWSLT data, and proportional  sampling. Scores are Macro-R and accuracy.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.8757134974002838}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9997023940086365}]}, {"text": " Table 3: Test results with different system settings, training with IWSLT data, and proportional sampling.  Scores are Macro-R and accuracy.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.8738548159599304}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9996368885040283}]}, {"text": " Table 4: Final development results on all training data with different types of sampling, with and without  domain adaptation (DA). Scores are Macro-R and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9997325539588928}]}, {"text": " Table 5: Final test results on all training data with different types of sampling, with and without domain  adaptation (DA). The last line shows the official shared task baseline scores. Scores are Macro-R and  accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9997474551200867}]}]}