{"title": [{"text": "Living a discrete life in a continuous world: Reference in cross-modal entity tracking", "labels": [], "entities": []}], "abstractContent": [{"text": "Reference is a crucial property of language that allows us to connect linguistic expressions to the world.", "labels": [], "entities": []}, {"text": "Modeling it requires handling both continuous and discrete aspects of meaning.", "labels": [], "entities": []}, {"text": "Data-driven models excel at the former, but struggle with the latter, and the reverse is true for symbolic models.", "labels": [], "entities": []}, {"text": "This paper (a) introduces a concrete referential task to test both aspects, called cross-modal entity tracking; (b) proposes a neural network architecture that uses external memory to build an entity library inspired in the DRSs of DRT, with a mechanism to dynamically introduce new referents or add information to referents that are already in the library.", "labels": [], "entities": [{"text": "cross-modal entity tracking", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.6159396370251974}]}, {"text": "Our model shows promise: it beats traditional neural network architectures on the task.", "labels": [], "entities": []}, {"text": "However, it is still outperformed by Memory Networks, another model with external memory.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language combines discrete and continuous facets, as exemplified by the phenomenon of reference: When we refer to an object in the world with the noun phrase the mug I bought, we use content words such as mug, which are notoriously fuzzy or vague in their meaning;) and are best modeled through continuous means.", "labels": [], "entities": []}, {"text": "Once the referent for the mug has been established, however, it becomes a linguistic entity that we can manipulate in a largely discrete fashion, retrieving it and updating it with new information as needed (Remember the mug I bought?", "labels": [], "entities": []}, {"text": "My brother stole it!.", "labels": [], "entities": []}, {"text": "Put differently, managing reference requires two distinct abilities: 1.", "labels": [], "entities": [{"text": "managing reference", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8998474478721619}]}, {"text": "The ability to categorize, that is, to recognize that different entities are equivalent with regard to some concept of interest (e.g. two mugs, two instances of the \"things to take on a camping trip\" category;.", "labels": [], "entities": []}, {"text": "This implies being able to aggregate seemingly diverse objects.", "labels": [], "entities": []}, {"text": "2. The ability to individuate, that is, to keep entities distinct even if they are similar with regard to many attributes (e.g. two pieces of pink granite that were collected in different national parks).", "labels": [], "entities": []}, {"text": "This implies being able to keep seemingly similar things apart.", "labels": [], "entities": []}, {"text": "Data-driven, continuous models are very good at categorizing, but not at individuating, and the reverse holds for symbolic models.", "labels": [], "entities": []}, {"text": "Our long-term research goal is to build a continuous computational model of reference that emulates discrete referential mechanisms such as those defined in DRT (; here we present initial work towards that goal, with two specific contributions.", "labels": [], "entities": []}, {"text": "Our first contribution is an experimental task (and associated dataset), cross-modal entity tracking, that tests the ability of computational models to refer successfully in a setting where they are required to both categorize and individuate entities.", "labels": [], "entities": [{"text": "cross-modal entity tracking", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.6000040968259176}]}, {"text": "The task presents different entities (represented by pictures) repeatedly, each time with a different, linguistically conveyed attribute (e.g. a given mug is presented once with the attribute bought and once with stolen).", "labels": [], "entities": []}, {"text": "The category label (\"mug\") is not given at exposure time.", "labels": [], "entities": []}, {"text": "The task is to choose the picture of the entity that corresponds to a linguistic query that combines category information with attribute information (e.g. simulating \"the mug that was bought and stolen\"), among the set of all the entities presented in a given sequence.", "labels": [], "entities": []}, {"text": "The sequences in each datapoint of our dataset contain confounders that make the task challenging: Other entities with the same category but only one matching attribute (e.g. a different mug that was bought and stored), and other entities with the same attributes but a different category (e.g. a chair that was bought and stolen).", "labels": [], "entities": []}, {"text": "Therefore, the task requires models to 1) correctly categorize entities, recognizing which images belong to the category in the query (something that is hard for symbolic models), 2) individuate and track them, being able to distinguish among different entities based on visual and linguistic cues provided at different time steps (something that is hard for continuous models).", "labels": [], "entities": []}, {"text": "In DRT terms (, each entity exposure either introduces anew discourse referent or updates the representation of an old referent with new information.", "labels": [], "entities": []}, {"text": "To solve the task successfully, the model needs to decide, for each incoming exposure, whether to aggregate it with a previously known referent (in DRT, this means introducing an equation between two referents), or to treat it as anew referent.", "labels": [], "entities": []}, {"text": "Our second contribution is a neural network architecture with a module for referent representations: DIstributed model of REference, DIRE.", "labels": [], "entities": []}, {"text": "DIRE uses the concept of external memory from deep learning ( to build an entity library for an exposure sequence that conceptually corresponds to the set of DRT discourse referents, using similarity-based reasoning on distributed representations to decide between aggregating and initializing entity representations.", "labels": [], "entities": []}, {"text": "In contrast to symbolic implementations of DRT, which manipulate discourse referents on the basis of manually specified algorithms, DIRE learns to make these decisions directly from observing reference acts using end-to-end training.", "labels": [], "entities": []}, {"text": "We see our paper as a first, modest step in the direction of data-driven learning of DRT-like behavior, and are of course still far from learning anything resembling a fully fledged DRT system.", "labels": [], "entities": [{"text": "DRT-like behavior", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7512111961841583}]}], "datasetContent": [{"text": "representations are given by 400-dimensional cbow embeddings from, trained on about 2.8 billion tokens of raw text.", "labels": [], "entities": []}, {"text": "We map to a 1K-dimensional multimodal space.", "labels": [], "entities": []}, {"text": "The parameters of DIRE are estimated by stochastic gradient descent with 0.09 learning rate, 10 minibatch size, 0.5 dropout probability, and maximally 150 epochs (here and below, hyperparameter values as in.", "labels": [], "entities": [{"text": "DIRE", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.47476157546043396}]}, {"text": "As competitors, we train standard feed-forward (FF) and recurrent (RNN) networks which have no external memory, using two 300-dimensional hidden layers and sigmoid nonlinearities.", "labels": [], "entities": []}, {"text": "We also implement the related Memory Network model (.", "labels": [], "entities": []}, {"text": "Like DIRE, MemN controls a memory structure, but stores each input exposure separately in the memory.", "labels": [], "entities": []}, {"text": "At the same time, MemN can perform multiple \"hops\" at query time.", "labels": [], "entities": []}, {"text": "Each hop consists in soft-retrieving a vector from the memory, where the probing vector is the sum of the input query vector and the vector retrieved in the previous hop (null for the first hop).", "labels": [], "entities": []}, {"text": "Conceptually, DIRE attempts to merge different instances of the same entity at input processing time, whereas MemN stores each piece of input separately and aggregates relevant information at query time.", "labels": [], "entities": []}, {"text": "MemN can thus use the query to guide the search for relevant information.", "labels": [], "entities": [{"text": "MemN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9344696402549744}]}, {"text": "At the same time, it does not optimize the way in which it stores information in memory.", "labels": [], "entities": []}, {"text": "Another difference with DIRE is that MemN uses two sets of mapping matrices: One to derive the vectors used at query time, the other for the vectors used for retrieval.", "labels": [], "entities": []}, {"text": "We employ the same hyperparameters for MemN (also multimodal vector size) as for our model.", "labels": [], "entities": [{"text": "MemN", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7263100743293762}]}, {"text": "shows that DIRE outperforms the standard networks (FF and RNN) by a large margin, confirming the importance of a discrete memory structure for reference tracking.", "labels": [], "entities": [{"text": "DIRE", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.7317317724227905}, {"text": "reference tracking", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.8818569481372833}]}, {"text": "If we make the MemN architecture completely comparable to our model (with one matrix and one hop, MemN-1m-1h), our model achieves higher results (0.64 for DIRE-1m, 0.59 for MemN-1m-1h), which indicates that the basic architecture of the model holds promise.", "labels": [], "entities": [{"text": "DIRE-1m", "start_pos": 155, "end_pos": 162, "type": "METRIC", "confidence": 0.7643044590950012}]}, {"text": "However, MemN outperforms DIRE when using two matrices, two hops (0.67 MemN-2m-1h/MemN-1m-2h vs. 0.65 DIRE-2m), or both (0.69 MemN-2m-2h).", "labels": [], "entities": []}, {"text": "For MemN, this seems to be the upper bound, as increasing to three hops greatly harms results (see last row).", "labels": [], "entities": [{"text": "MemN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7880169153213501}]}], "tableCaptions": [{"text": " Table 1: Tracking results (accuracy on test set).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9991745352745056}]}]}