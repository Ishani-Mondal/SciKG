{"title": [{"text": "An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation", "labels": [], "entities": [{"text": "Attention-Based Neural Machine Translation", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.6213346570730209}]}], "abstractContent": [{"text": "Recently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.8525477647781372}]}, {"text": "However, as it computes a score function for the encoder states in all positions at each decoding step, the attention model greatly increases the computational complexity.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the adequate vision span of attention models in the context of machine translation, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8047027587890625}]}, {"text": "The term \"vi-sion span\" means a window of the en-coder states considered by the attention model in one step.", "labels": [], "entities": []}, {"text": "In our experiments, we found that the average window size of vision span can be reduced by over 50% with modest loss inaccuracy on English-Japanese and German-English translation tasks.", "labels": [], "entities": [{"text": "German-English translation tasks", "start_pos": 152, "end_pos": 184, "type": "TASK", "confidence": 0.744083305199941}]}], "introductionContent": [{"text": "In recent years, recurrent neural networks have been successfully applied in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8018733561038971}]}, {"text": "In many major language pairs, Neural Machine Translation (NMT) has already outperformed conventional Statistical Machine Translation (SMT) models (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.8073747257391611}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 101, "end_pos": 138, "type": "TASK", "confidence": 0.8013265430927277}]}, {"text": "NMT models are generally composed of an encoder and a decoder, which is also known as encoder-decoder framework).", "labels": [], "entities": []}, {"text": "The encoder creates a vector representation of the input sentence, whereas the decoder generates the translation from this single vector.", "labels": [], "entities": []}, {"text": "This simple encoder-decoder model suffers from along backpropagation path; thus, adversely affected by long input sequences.", "labels": [], "entities": []}, {"text": "In recent NMT models, soft attention mechanism () has been a key extension to ensure high performance.", "labels": [], "entities": []}, {"text": "In each decoding step, the attention model computes alignment weights for all the encoder states.", "labels": [], "entities": []}, {"text": "Then a context vector, which is a weighted summarization of the encoder states is computed and fed into the decoder as input.", "labels": [], "entities": []}, {"text": "In contrast to the aforementioned simple encoder-decoder model, the attention mechanism can greatly shorten the backpropagation path.", "labels": [], "entities": []}, {"text": "Although the attention mechanism provides NMT models with a boost in performance, it also significantly increases the computational burden.", "labels": [], "entities": []}, {"text": "As the attention model has to compute the alignment weights for all the encoder states in each step, the decoding process becomes timeconsuming.", "labels": [], "entities": []}, {"text": "Even worse, recent researches in NMT prefer to separate the texts into subwords () or even characters (, which means massive encoder states have to be considered in the attention model at each step, thereby resulting in increasing computational cost.", "labels": [], "entities": [{"text": "NMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9032395482063293}]}, {"text": "On the other hand, the attention mechanism is becoming more complicated.", "labels": [], "entities": []}, {"text": "For example, the NMT model with recurrent attention modeling () maintains a dynamic memory of attentions for every encoder states, which is updated in each decoding step.", "labels": [], "entities": []}, {"text": "In this paper, we study the adequate vision span in the context of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7939623594284058}]}, {"text": "Here, the term \"vision span\" means a window of encoder states considered by the attention model in one step.", "labels": [], "entities": []}, {"text": "We examine the minimum window size of an attention model have to consider in each step while maintaining the translation quality.", "labels": [], "entities": []}, {"text": "For this purpose, we propose a novel attention framework which we refer to as Flexible Attention in this paper.", "labels": [], "entities": [{"text": "Flexible", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.921941339969635}]}, {"text": "The proposed attention framework tracks the center of attention in each decoding step, and pre- Figure 1: (a) An example English-Japanese sentence pair with long-range reordering (b) The vision span predicted by the proposed Flexible Attention at each step in English-Japanese translation task dict an adequate vision span for the next step.", "labels": [], "entities": []}, {"text": "In the test time, the encoder states outside of this range are omitted in the computation of score function.", "labels": [], "entities": []}, {"text": "Our proposed attention framework is based on simple intuition.", "labels": [], "entities": []}, {"text": "For most language pairs, the translations of words inside a phrase usually remain together.", "labels": [], "entities": []}, {"text": "Even the translation of a small chunk usually does not mix with the translation of other words.", "labels": [], "entities": [{"text": "translation of a small chunk", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.8124246835708618}]}, {"text": "Hence, information about distant words is basically unnecessary when translating locally.", "labels": [], "entities": []}, {"text": "Therefore, we argue that computing the attention overall positions in each step is redundant.", "labels": [], "entities": []}, {"text": "However, attending to distant positions remains important when dealing with long-range reordering.", "labels": [], "entities": []}, {"text": "In(a), we show an example sentence pair with long-range reordering, where the positions of the first three words have monotone alignments, but the fourth word is aligned to distant target positions.", "labels": [], "entities": []}, {"text": "If we can predict whether the next word to translate is in a local position, the amount of redundant computation in the attention model can be safely reduced by controlling the window size of vision span dynamically.", "labels": [], "entities": []}, {"text": "This motivated us to propose a flexible attention framework which predicts the minimum required vision span according to the context (See).", "labels": [], "entities": []}, {"text": "We evaluated our proposed Flexible Attention by comparing with the conventional attention mechanism, and Local Attention () which puts attention on a fixed-size window.", "labels": [], "entities": [{"text": "Flexible Attention", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8183202147483826}]}, {"text": "We focus on comparing the minimum window size of vision span these models can achieve without hurting the performance too much.", "labels": [], "entities": []}, {"text": "Note that as the window size determines the number of times the score function is evaluated, reducing the window size leads to the reduction of score computation.", "labels": [], "entities": []}, {"text": "We select English-Japanese and German-English language pairs for evaluation as they consist of languages with different word orders, which means the attention model cannot simply look at a local range constantly and translate monotonically.", "labels": [], "entities": []}, {"text": "Through empirical evaluation, we found with Flexible Attention, the average window size is reduced by 56% for EnglishJapanese task and 64% for German-English task, with modest loss of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9978125095367432}]}, {"text": "The reduction rate also achieves 46% for character-based NMT models.", "labels": [], "entities": [{"text": "reduction", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9895380735397339}]}, {"text": "Our contributions can be summarized as three folds: 1.", "labels": [], "entities": []}, {"text": "We empirically confirmed that the conventional attention mechanism performs a significant amount of redundant computation.", "labels": [], "entities": []}, {"text": "Although attending globally is necessary when dealing with long-range reordering, a small vision span is sufficient when translating locally.", "labels": [], "entities": []}, {"text": "The results may provide insights for future research on more efficient attentionbased NMT models.", "labels": [], "entities": []}, {"text": "2. The proposed Flexible Attention provides a general framework for reducing the amount of score computation according to the context, which can be combined with other expensive attention models of which computing for all positions in each step is costly.", "labels": [], "entities": []}, {"text": "3. We found that reducing the amount of computation in the attention model can benefit the decoding speed on CPU, but not GPU.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we focus on evaluating our proposed attention models by measuring the minimum average window size of vision span it can achieve with a modest performance loss 2 . In de-tail, we measure the average number of the encoder states considered when computing the score function in Equation 3.", "labels": [], "entities": []}, {"text": "Note that as we decode using Beam Search algorithm , the value of window size is further averaged over the number of hypotheses considered in each step.", "labels": [], "entities": []}, {"text": "For the conventional attention mechanism, as all positions have to be considered in each step, the average window size equals to the average sentence length of the testing data.", "labels": [], "entities": []}, {"text": "Following, we refer to the conventional attention mechanism as Global Attention in experiments.", "labels": [], "entities": []}, {"text": "We evaluate our models on English-Japanese and German-English translation task.", "labels": [], "entities": [{"text": "German-English translation", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.6626344621181488}]}, {"text": "As translating these language pairs requires long-range reordering, the proposed Flexible Attention has to correctly predict when the reordering happens and look at distant positions when necessary.", "labels": [], "entities": [{"text": "Flexible Attention", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7839376330375671}]}, {"text": "The training data of En-Ja task is based on ASPEC parallel corpus (), which contains 3M sentence pairs, whereas the test data contains 1812 sentences, which have 24.4 words on average.", "labels": [], "entities": [{"text": "ASPEC parallel corpus", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.7151153087615967}]}, {"text": "We select 1.5M sentence pairs according to the automatically calculated matching scores, which are provided along with the ASPEC corpus.", "labels": [], "entities": [{"text": "ASPEC corpus", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.7843462824821472}]}, {"text": "For De-En task, we use the WMT'15 training data consisting of 4.5M sentence pairs.", "labels": [], "entities": [{"text": "WMT'15 training data", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.8866447607676188}]}, {"text": "The WMT'15 test data (newstest2015) contains 2169 pairs, which have 20.7 words on average.", "labels": [], "entities": [{"text": "WMT'15 test data (newstest2015)", "start_pos": 4, "end_pos": 35, "type": "DATASET", "confidence": 0.8485669692357382}]}, {"text": "We preprocess the En-Ja corpus with \"tokenizer.perl\" for English side, and Kytea tokenizer (Neubig et al., 2011) for Japanese side.", "labels": [], "entities": []}, {"text": "The preprocessing procedure for De-En corpus is similar to, except we did not filter sentence pairs with language detection.", "labels": [], "entities": [{"text": "language detection", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7005961239337921}]}, {"text": "The vocabulary size are cropped to 80k and 40k for En-Ja NMT models, whereas 50k for De-En NMT models.", "labels": [], "entities": []}, {"text": "The OOV words are replaced with a \"UNK\" symbol.", "labels": [], "entities": [{"text": "OOV", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8239116072654724}]}, {"text": "Long sentences with more than 50 words on either the source or target side are removed from the training set, resulting in 1.3M and 3.8M training pairs for En-Ja and De-En task respectively.", "labels": [], "entities": []}, {"text": "We use mini-batch in our training procedure, where each batch contains 64 data samples.", "labels": [], "entities": []}, {"text": "All sentence pairs are firstly sorted according to their length before we group them into batches.", "labels": [], "entities": []}, {"text": "After which, the order of the mini-batches is shuffled.: Evaluation results on English-Japanese and German-English translation task.", "labels": [], "entities": [{"text": "German-English translation task", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.7415584524472555}]}, {"text": "This table provides a comparison of the minimum window size of vision span the models can achieve with a modest loss of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9951469302177429}]}, {"text": "We evaluate the attention models to determine the minimum window size they can achieve with a modest loss of accuracy (0.5 development BLEU) compared to Flexible Attention with \u03c4 = \u221e.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9990932941436768}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9990074038505554}]}, {"text": "The results we obtained are summarized in.", "labels": [], "entities": []}, {"text": "The scores of Global Attention (conventional attention model) and Local Attention () are listed for comparison.", "labels": [], "entities": []}, {"text": "For Local Attention, we found a window size of 21 gives the best performance for En-Ja and De-En tasks.", "labels": [], "entities": []}, {"text": "In this setting, Local Attention achieves an average window of 18.4 words in En-Ja task and 15.7 words in De-En task, as some sentences in the test corpus have fewer than 21 words.", "labels": [], "entities": []}, {"text": "For Flexible Attention, we search a good \u03c4 among (0.8, 1.0, 1.2, 1.4, 1.6) on a development corpus so that the development BLEU(%) does not degrade more than 0.5 compared to \u03c4 = \u221e.", "labels": [], "entities": [{"text": "Flexible Attention", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8808881640434265}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9959502220153809}]}, {"text": "Finally, \u03c4 = 1.2 is selected for both language pairs in our experiments.", "labels": [], "entities": []}, {"text": "We can see from the results that Flexible Attention can achieve comparable scores even consider only half of the encoder states in each step.", "labels": [], "entities": [{"text": "Flexible Attention", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.72707799077034}]}, {"text": "After fine-tuning, our proposed attention model further reduces 56% of the vision span for En-Ja task and 64% for De-En task.", "labels": [], "entities": []}, {"text": "The high reduction rate confirms that the conventional attention model performs massive redundant computation.", "labels": [], "entities": [{"text": "reduction rate", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.956304281949997}]}, {"text": "With Flexible Attention, redundant score computation can be efficiently cut down according to the context.", "labels": [], "entities": []}, {"text": "Interestingly, the NMT models using Flexible Attention without the threshold improves the translation accuracy by a small margin, which may indi- cates that the quality of attention is improved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9232735633850098}]}], "tableCaptions": [{"text": " Table 1: Evaluation results on English-Japanese and German-English translation task. This table pro- vides a comparison of the minimum window size of vision span the models can achieve with a modest  loss of accuracy.", "labels": [], "entities": [{"text": "German-English translation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.6939156651496887}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9950369000434875}]}, {"text": " Table 2: Evaluation results with character-based  English-Japanese NMT models", "labels": [], "entities": []}]}