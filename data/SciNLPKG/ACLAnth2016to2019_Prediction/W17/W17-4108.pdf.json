{"title": [{"text": "Character-based recurrent neural networks for morphological relational reasoning", "labels": [], "entities": [{"text": "relational reasoning", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.6745065152645111}]}], "abstractContent": [{"text": "We present a model for predicting word forms based on morphological relational reasoning with analogies.", "labels": [], "entities": [{"text": "predicting word forms", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8439046144485474}]}, {"text": "While previous work has explored tasks such as morphological inflection and reinflection, these models rely on an explicit enumeration of morphological features, which may not be available in all cases.", "labels": [], "entities": []}, {"text": "To address the task of predicting a word form given a demo relation (a pair of word forms) and a query word, we devise a character-based recurrent neural network architecture using three separate encoders and a decoder.", "labels": [], "entities": []}, {"text": "We also investigate a multiclass learning setup, where the prediction of the relation type label is used as an auxiliary task.", "labels": [], "entities": []}, {"text": "Our results show that the exact form can be predicted for English with an accuracy of 94.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9995427131652832}]}, {"text": "For Swedish, which has a more complex morphology with more inflec-tional patterns for nouns and verbs, the accuracy is 89.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9997665286064148}]}, {"text": "We also show that using the auxiliary task of learning the relation type speeds up convergence and improves the prediction accuracy for the word generation task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9712222814559937}, {"text": "word generation task", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.8276134729385376}]}], "introductionContent": [{"text": "Recently, a number of papers have been published that use character-level neural models as away to address the inherent drawbacks of traditional models that represent words as atomic symbols.", "labels": [], "entities": []}, {"text": "This offers a number of advantages: the vocabulary in a character-based model can be much smaller, as it only needs to represent a finite and fairly small alphabet, and as long as the characters are in the alphabet, no words will be outof-vocabulary (OOV).", "labels": [], "entities": []}, {"text": "Character-level models can capture distributional properties, not only of frequent words but also of words that occur rarely.", "labels": [], "entities": []}, {"text": "This type of model needs no tokenization, freeing the system from one source of errors.", "labels": [], "entities": []}, {"text": "Character-level neural models have been applied in several NLP tasks, ranging from relatively basic tasks such as text categorization ( and language modeling ( to complex prediction tasks such as translation (.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8008508086204529}, {"text": "language modeling", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.7466039657592773}, {"text": "translation", "start_pos": 196, "end_pos": 207, "type": "TASK", "confidence": 0.9549611806869507}]}, {"text": "In particular, character-based neural models are attractive because they can take sub-word units, such as the morphology, into account.", "labels": [], "entities": []}, {"text": "Morphological analysis and prediction models using character-based recurrent neural networks have recently become popular, as evidenced by their complete dominance at the SIGMORPHON shared task on morphological reinflection (.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9073425829410553}, {"text": "SIGMORPHON shared task", "start_pos": 171, "end_pos": 193, "type": "TASK", "confidence": 0.5775358279546102}]}, {"text": "However, in these models, including the top-performing system in the shared task, an explicit feature representation of the morphological inflection needs to be provided as an input.", "labels": [], "entities": []}, {"text": "These features represent number, gender, case, tense, aspect, etc.", "labels": [], "entities": []}, {"text": "In this paper, we take anew approach to predicting word forms that bypasses the need for an explicit representation of morphological features.", "labels": [], "entities": [{"text": "predicting word forms", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.8638290762901306}]}, {"text": "We present a model that learns morphological analogy relations between words: given a demo relation R demo = (w 1 , w 2 ), represented as a pair of words w 1 and w 2 , and a query word q, can we apply the same relation as represented by R demo to the query word, and arrive at the correct target t?", "labels": [], "entities": []}, {"text": "The task maybe illustrated with a simple example: see is to sees as eat is to what?", "labels": [], "entities": []}, {"text": "The relation in the example above is trivial on a superficial level, as the model just needs to add an s to the query word.", "labels": [], "entities": []}, {"text": "However, the analogy task is more challenging in the general case.", "labels": [], "entities": [{"text": "analogy task", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.8872454464435577}]}, {"text": "The model needs to take into account that words belong to groups whose inflectional patterns are different -morphological paradigms.", "labels": [], "entities": []}, {"text": "For instance, if we consider the past tense instead of the present in the example above, the relation is more complex: see is to saw as eat is to what?", "labels": [], "entities": []}, {"text": "The model also needs to pickup general patterns that cut across paradigms, including phonological processes such as umlaut and vowel harmony, as well as orthographic quirks such as the rule in English that turns y into ie in certain contexts.", "labels": [], "entities": []}, {"text": "The fact that our model does not rely on explicit features makes it applicable in scenarios where features are unavailable, such as when working with under-resourced languages.", "labels": [], "entities": []}, {"text": "However, since the model is trained using a weaker signal than in the traditional feature-based scenario, it needs to learn a latent representation from the analogies that plays the same role as the morphological features otherwise would.", "labels": [], "entities": []}, {"text": "This makes the task more challenging to learn, and we compare the training time of a purely feature-free model to one where features are available during training as an auxiliary prediction task in a multi-task learning setup.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section explains the setup of the empirical study of our model.", "labels": [], "entities": []}, {"text": "How it is designed, trained, and evaluated.", "labels": [], "entities": []}, {"text": "The model was trained and evaluated on words in English and Swedish.", "labels": [], "entities": []}, {"text": "For each class, 200 words were used for validation, and 200 for testing.", "labels": [], "entities": []}, {"text": "For Swedish, words were extracted from SALDO ().", "labels": [], "entities": [{"text": "SALDO", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9057365655899048}]}, {"text": "In the Swedish data, 64,460 nouns, 12,507 adjectives, and 7,764 verbs were used for training.", "labels": [], "entities": [{"text": "Swedish data", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8870321214199066}]}, {"text": "The same size of validation and test sets were used.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the model, the datasets were split into training, validation, and test sets.", "labels": [], "entities": []}, {"text": "Where nothing else is specified, reported numbers are prediction accuracy.", "labels": [], "entities": [{"text": "prediction", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.8727615475654602}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8508478999137878}]}, {"text": "This is the frac-  88.0%: Prediction accuracy of the proposed model trained using both English and Swedish simultaneously.", "labels": [], "entities": [{"text": "frac-  88.0", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9310157100359598}, {"text": "Prediction", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9863826036453247}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.742230236530304}]}, {"text": "Column labels here denotes test dataset: English, Swedish, and combined.", "labels": [], "entities": []}, {"text": "tion of predictions that were exactly matching the target words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Prediction accuracy of the proposed  model using different hidden sizes. Column la- bels denote training set: the English & Swedish  model were simultaneously trained on both lan- guages, and has no explicit signal about the lan- guage it is seeing, the other columns show results  for models trained on only one language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.865473747253418}]}, {"text": " Table 3: Prediction accuracy of the proposed  model trained using both English and Swedish si- multaneously. Column labels here denotes test  dataset: English, Swedish, and combined.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9283544421195984}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8813013434410095}]}]}