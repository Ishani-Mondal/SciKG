{"title": [{"text": "Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines", "labels": [], "entities": [{"text": "Semantic Vector Encoding", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7257339954376221}, {"text": "Similarity Search", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.935001015663147}]}], "abstractContent": [{"text": "Vector representations and vector space modeling (VSM) play a central role in modern machine learning.", "labels": [], "entities": [{"text": "Vector representations", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8757921755313873}, {"text": "vector space modeling (VSM)", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.8071414232254028}]}, {"text": "We propose a novel approach to 'vector similarity search-ing' over dense semantic representations of words and documents that can be deployed on top of traditional inverted-index-based fulltext engines, taking advantage of their robustness, stability, scalability and ubiq-uity.", "labels": [], "entities": [{"text": "vector similarity search-ing", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.6930185159047445}]}, {"text": "We show that this approach allows the indexing and querying of dense vectors in text domains.", "labels": [], "entities": []}, {"text": "This opens up exciting avenues for major efficiency gains, along with simpler deployment, scaling and monitoring.", "labels": [], "entities": []}, {"text": "The end result is a fast and scal-able vector database with a tunable trade-off between vector search performance and quality, backed by a standard fulltext engine such as Elasticsearch.", "labels": [], "entities": []}, {"text": "We empirically demonstrate its querying performance and quality by applying this solution to the task of semantic searching over a dense vector representation of the entire English Wikipedia.", "labels": [], "entities": [{"text": "semantic searching", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7600485980510712}, {"text": "English Wikipedia", "start_pos": 173, "end_pos": 190, "type": "DATASET", "confidence": 0.6455845385789871}]}], "introductionContent": [{"text": "The vector space model ( of representing documents in high-dimensional vector spaces has been validated by decades of research and development.", "labels": [], "entities": []}, {"text": "Extensive deployment of inverted-index-based information retrieval (IR) systems has led to the availability of robust open source IR systems such as Sphinx, Lucene or its popular, horizontally scalable extensions of Elasticsearch and Solr.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.8042973041534424}, {"text": "Lucene", "start_pos": 157, "end_pos": 163, "type": "DATASET", "confidence": 0.9069889783859253}]}, {"text": "Representations of document semantics based solely on first order document-term statistics, such as TF-IDF or Okapi BM25, are limited in their expressiveness and search recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9790052175521851}]}, {"text": "Today, approaches based on distributional semantics and deep learning allow the construction of semantic vector space models representing words, sentences, paragraphs or even whole documents as vectors in highdimensional spaces.", "labels": [], "entities": []}, {"text": "The ubiquity of semantic vector space modeling raises the challenge of efficient searching in these dense, high-dimensional vector spaces.", "labels": [], "entities": [{"text": "semantic vector space modeling", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.6295563280582428}]}, {"text": "We would naturally want to take advantage of the design and optimizations behind modern fulltext engines like Elasticsearch so as to meet the scalability and robustness demands of modern IR applications.", "labels": [], "entities": []}, {"text": "This is the research challenge addressed in this paper.", "labels": [], "entities": []}, {"text": "The rest of the paper describes novel ways of encoding dense vectors into text documents, allowing the use of traditional inverted index engines, and explores the trade-offs between IR accuracy and speed.", "labels": [], "entities": [{"text": "IR", "start_pos": 182, "end_pos": 184, "type": "TASK", "confidence": 0.9740099310874939}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.8597378134727478}, {"text": "speed", "start_pos": 198, "end_pos": 203, "type": "METRIC", "confidence": 0.9593415260314941}]}, {"text": "Being motivated by pragmatic needs, we describe the results of experiments carried out on real datasets measured on concrete, practical software implementations.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our method, we used ScaleText () based on Elasticsearch ( as our fulltext IR system.", "labels": [], "entities": []}, {"text": "The evaluation dataset was the whole of the English Wikipedia consisting of 4,181,352 articles.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9536711573600769}]}, {"text": "Results of the quality evaluation are summarized in.", "labels": [], "entities": []}, {"text": "The results of our method in different settings are put side by side with the results of the brute-force naive search and with Elasticsearch's native More Like This (MLT) search.", "labels": [], "entities": []}, {"text": "For the MLT results, the max_query_terms Elasticsearch parameter is reported in the best column since both the semantics and the impact on the search speed are similar.", "labels": [], "entities": [{"text": "MLT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.7001511454582214}]}, {"text": "illustrates the impact of feature value filtering and the number of retrieved search candidates from Elasticsearch (page size) on its accuracy.", "labels": [], "entities": [{"text": "feature value filtering", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.6297073662281036}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9980196952819824}]}, {"text": "It can be seen that avg. diff.", "labels": [], "entities": [{"text": "avg. diff.", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9582212269306183}]}, {"text": "decreases logarithmically with the page size.", "labels": [], "entities": []}, {"text": "The results improve all the way up to 640 search results (the maximum value we have tried), which is expected as this increases the size of the subset E that is consequently ordered (re-ranked) in phase 2 with the precise but more costly exact algorithm.", "labels": [], "entities": []}, {"text": "Increasing the size of E increases the chance of the inclusion of relevant results.", "labels": [], "entities": [{"text": "size of E", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8616355657577515}]}, {"text": "The shape of the curve suggests that there would only be a slight improvement inaccuracy, and this would beat the cost of a substantial drop in performance.", "labels": [], "entities": []}, {"text": "The impact of including only a limited number of features with the highest absolute value (see Section 2.2.2), is rather low.", "labels": [], "entities": []}, {"text": "This is an excellent result with regard to performance as it means we may effectively sparsify the query vector with very little impact on search quality.", "labels": [], "entities": []}, {"text": "We observe little difference between no filtering (searching by   all 400 encoded features) and trimming to only the 90 best query vector values.", "labels": [], "entities": [{"text": "trimming", "start_pos": 96, "end_pos": 104, "type": "TASK", "confidence": 0.9482187628746033}]}, {"text": "However, trimming to as low as 6 values results in a significant increase in avg. diff.", "labels": [], "entities": [{"text": "trimming", "start_pos": 9, "end_pos": 17, "type": "TASK", "confidence": 0.9110556840896606}, {"text": "avg. diff.", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.8047141581773758}]}, {"text": "Our methods scores above the MLT baseline in all of the followed metrics, even with aggressive high-pass filtering and with a small page size.", "labels": [], "entities": [{"text": "MLT baseline", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.6294179856777191}]}, {"text": "We expect that similar setup parameters will work similarly, at least for general multi-topic text datasets.", "labels": [], "entities": []}, {"text": "Its behaviour fora dramatically different dataset, such as images instead of texts, or without normalized feature ranges, cannot be directly inferred and remains to be investigated in our future work.", "labels": [], "entities": []}, {"text": "However, we expect our speed optimization methods to be applicable in some form, with the concrete parameters to be validated on the particular dataset and algorithmic setup.", "labels": [], "entities": []}, {"text": "Selected results of our speed evaluation are summarized in.", "labels": [], "entities": [{"text": "speed", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9477057456970215}]}, {"text": "For clarity, we selected only the configurations using 400 LSA features and 48 Elasticsearch shards, as this setup turned out to provide the optimal performance on the Elasticsearch cluster and dataset we used according to the quality evaluation (see Section 4.1), where the same parameters were used.", "labels": [], "entities": []}, {"text": "The speed of the native Elasticsearch MLT search is summarized in.", "labels": [], "entities": []}, {"text": "The speed is comparable to our method when high-pass filtering is involved.) with four parallel queries on the same Elasticsearch cluster configuration shows that at the expense of doubling the response time, we are able to answer four requests in parallel.", "labels": [], "entities": []}, {"text": "The best results in are located in the bottom right corner where the precision is high and the response time is low.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9997705817222595}, {"text": "response time", "start_pos": 95, "end_pos": 108, "type": "METRIC", "confidence": 0.9540965259075165}]}, {"text": "For our dataset and algorithm (LSA with 400 features), the best overall results are represented by the largest gray dots, i.e. retrieving 320 vectors from Elasticsearch while filtering the query vector to roughly 90 values via trimming with a threshold of 0.05.", "labels": [], "entities": []}, {"text": "To achieve the optimal results, we suggest retrieving as large a set of candidates (Elasticsearch page size, E) as the response-time constraints allow, as the page size seems to have significantly lower influence on the response time compared to trimming, while having a significant positive effect on accuracy.", "labels": [], "entities": [{"text": "trimming", "start_pos": 246, "end_pos": 254, "type": "TASK", "confidence": 0.9532306790351868}, {"text": "accuracy", "start_pos": 302, "end_pos": 310, "type": "METRIC", "confidence": 0.9973680377006531}]}, {"text": "Our experiments were done on the Wikipedia dataset.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.9773348569869995}]}, {"text": "Wikipedia is a multi-topic dataset -articles are on a wide variety of different topics using different keywords (names of people, places and things, etc) and notation (text only articles, articles on mathematics using formulae, articles on chemistry using different formulae, etc) are included.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9187813401222229}]}, {"text": "This provides enough room for the machine learning algorithms to build features that reflect these unique markers of particular topics and makes particular features significantly irrelevant for particular documents in the dataset.", "labels": [], "entities": []}, {"text": "For general multi-topic text datasets, we recommend trimming features values by their absolute value below 5% of the maximum (i.e. between \u22120.05 and 0.05 in our experiments).", "labels": [], "entities": []}, {"text": "Trimming more feature tokens decreases the precision with almost no influence on the response times, while keeping more feature tokens in the index has almost no positive effect on the precision but slows down the search significantly.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9994545578956604}, {"text": "precision", "start_pos": 185, "end_pos": 194, "type": "METRIC", "confidence": 0.9987227320671082}]}], "tableCaptions": [{"text": " Table 4: Results of speed evaluation using  the native Elasticsearch More Like This (MLT)  search (no parallel queries) using 48 shards.  max_query_terms is the maximum number of  query terms per query that were selected by Elastic- search. ES avg./std. is the average and standard  deviation of the number of seconds per request  Elasticsearch took.", "labels": [], "entities": [{"text": "ES", "start_pos": 242, "end_pos": 244, "type": "METRIC", "confidence": 0.9955215454101562}]}]}