{"title": [{"text": "Centroid-based Text Summarization through Compositionality of Word Embeddings", "labels": [], "entities": [{"text": "Centroid-based Text Summarization", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5252601404984792}]}], "abstractContent": [{"text": "The textual similarity is a crucial aspect for many extractive text summarization methods.", "labels": [], "entities": [{"text": "extractive text summarization", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.6452160179615021}]}, {"text": "A bag-of-words representation does not allow to grasp the semantic relationships between concepts when comparing strongly related sentences with no words in common.", "labels": [], "entities": []}, {"text": "To overcome this issue , in this paper we propose a centroid-based method for text summarization that exploits the compositional capabilities of word embeddings.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7345110476016998}]}, {"text": "The evaluations on multi-document and multilingual datasets prove the effectiveness of the continuous vector representation of words compared to the bag-of-words model.", "labels": [], "entities": []}, {"text": "Despite its simplicity, our method achieves good performance even in comparison to more complex deep learning models.", "labels": [], "entities": []}, {"text": "Our method is unsupervised and it can be adopted in other summarization tasks.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.9272432625293732}]}], "introductionContent": [{"text": "The goal of text summarization is to produce a shorter version of a source text by preserving the meaning and the key contents of the original text.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7073144912719727}]}, {"text": "This is a very complex problem since it requires to emulate the cognitive capacity of human beings to generate summaries.", "labels": [], "entities": []}, {"text": "Thus, text summarization poses open challenges in both natural language understanding and generation.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7494380474090576}, {"text": "natural language understanding", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.6725643475850424}]}, {"text": "Due to the difficulty of this task, research work in the literature focused on the extractive aspect of summarization, where the generated summary is a selection of relevant sentences from a document (or a set of documents) in a copy-paste fashion.", "labels": [], "entities": [{"text": "summarization", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.9898852109909058}]}, {"text": "A good extractive summarization method must satisfy and optimize both coverage and diversity properties, where the selected sentences should cover a sufficient amount of topics from the original source text, avoiding the redundancy of information in the summary.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.5654915571212769}]}, {"text": "The diversity property is fundamental especially fora multi-document summarization.", "labels": [], "entities": []}, {"text": "For instance in a news aggregator, a selection of too similar sentences may compromise the quality of the generated summary.", "labels": [], "entities": []}, {"text": "An extractive method should define a sentence representation model, a technique for assigning a score to each sentence in the original source and a ranking module to properly select the most relevant sentences by relying on a similarity function.", "labels": [], "entities": []}, {"text": "Following this vision, several summarization methods proposed in the literature use the bag of words (BOW) as representation model for the sentence scoring and selection modules).", "labels": [], "entities": [{"text": "BOW", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9138156175613403}]}, {"text": "Despite their proven effectiveness, these methods rely heavily on the notion of similarity between sentences, and a BOW representation is often not suitable to grasp the semantic relationships between concepts when comparing sentences.", "labels": [], "entities": [{"text": "BOW", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.8283106088638306}]}, {"text": "For example, taking into account the following two sentences \"Syd leaves Pink Floyd\" and \"Barrett abandons the band\", in the BOW model their vector (sparse) representations result orthogonal since they have no words in common, nonetheless the two sentences are strongly related.", "labels": [], "entities": []}, {"text": "In attempt to solve this issue, in this work we propose a novel and simple extractive summarization method based on the geometric meaning of the centroid vector of a (multi) document by taking advantage of compositional properties of the word embeddings (.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.5315629541873932}]}, {"text": "Empirically, we prove the effectiveness of word embeddings with a fair comparison to the BOW representation by limiting, as much as possible, the parameters and the complexity of the method.", "labels": [], "entities": []}, {"text": "Surprisingly, the results achieved from our method on the gold standard DUC-2004 dataset are comparable, and in some cases better, to those obtained using a more complex sentence representations coming from the deep learning models.", "labels": [], "entities": [{"text": "gold standard DUC-2004 dataset", "start_pos": 58, "end_pos": 88, "type": "DATASET", "confidence": 0.6936945766210556}]}, {"text": "In the following section we provide a brief description of word embeddings and text summarization methods.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.6811433285474777}]}, {"text": "The centroid-based summarization method that uses word embeddings is described in Section 3, followed by experimental results in Section 4.", "labels": [], "entities": []}, {"text": "Final remarks and a discussion about our future plans are reported in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.8557521402835846}]}], "datasetContent": [{"text": "In this section we describe the benchmarks conducted on two text summarization tasks.", "labels": [], "entities": [{"text": "text summarization tasks", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7689777414004008}]}, {"text": "The main goal is to compare the centroid-based method using two different representations (bag-of-words and word embeddings).", "labels": [], "entities": []}, {"text": "In Section 4.1 and in Section 4.2 we report the experimental results carried out on Multi-Document and Multilingual Single Document summarization tasks, respectively.", "labels": [], "entities": [{"text": "Multilingual Single Document summarization tasks", "start_pos": 103, "end_pos": 151, "type": "TASK", "confidence": 0.6303086936473846}]}, {"text": "Marc Hodler, a senior member of the International Olympic Committee executive board, alleged malpractices in the voting for the 1996 Atlanta Games, 2000 Sydney Olympics and 2002 Salt Lake Games.", "labels": [], "entities": [{"text": "International Olympic Committee executive board", "start_pos": 36, "end_pos": 83, "type": "TASK", "confidence": 0.6692934274673462}]}, {"text": "The IOC, meanwhile, said it was prepared to investigate allegations made by Hodler of bribery in the selection of Olympic host cities.", "labels": [], "entities": [{"text": "IOC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.870373547077179}]}, {"text": "The issue of vote-buying came to the fore in Lausanne because of the recent disclosure of scholarship payments made to six relatives of IOC members by Salt Lake City officials during their successful bid to play host to the 2002 Winter Games.", "labels": [], "entities": []}, {"text": "Marc Hodler, a senior member of the International Olympic Committee executive board, alleged malpractices in the voting for the 1996 Atlanta Games, 2000 Sydney Olympics and 2002 Salt Lake Games.", "labels": [], "entities": [{"text": "International Olympic Committee executive board", "start_pos": 36, "end_pos": 83, "type": "TASK", "confidence": 0.6692934274673462}]}, {"text": "The IOC, meanwhile, said it was prepared to investigate allegations made by Hodler of bribery in the selection of Olympic host cities.", "labels": [], "entities": [{"text": "IOC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.870373547077179}]}, {"text": "Saying \"if we have to clean, we will clean,\" Juan Antonio Samaranch responded on Sunday to allegations of corruption in the Olympic bidding process by declaring that IOC members who were found to have accepted bribes from candidate cities could be expelled.", "labels": [], "entities": []}, {"text": "The International Olympic Committee has ordered a top-level investigation into the payment of nearly dlrs 400,000 in scholarships to relatives of IOC members by the Salt Lake group which won the bid for the 2002 Winter Games.", "labels": [], "entities": []}, {"text": "The mayor of the Japanese city of Nagano, site of the 1998 Winter Olympics, denied allegations that city officials bribed members of the International Olympic Committee to win the right to host the games.", "labels": [], "entities": []}, {"text": "Swiss IOC executive board member Marc Hodler said Sunday he might be thrown out of the International Olympic Committee for making allegations of corruption within the Olympic movement.", "labels": [], "entities": [{"text": "International Olympic Committee", "start_pos": 87, "end_pos": 118, "type": "DATASET", "confidence": 0.8327251474062601}]}, {"text": "MSS task, we performed the tuning of parameters using only the training set.", "labels": [], "entities": []}, {"text": "To find the best topic and similarity threshold parameters we run a grid search as explained in Section 4.1.", "labels": [], "entities": []}, {"text": "The grid search is performed for each language separately using both BOW and skip-gram representations.", "labels": [], "entities": [{"text": "BOW", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9529635906219482}]}, {"text": "The parameter configurations are inline with those of the previous experiment on DUC-2004.", "labels": [], "entities": [{"text": "DUC-2004", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9688160419464111}]}, {"text": "In detail, the topic thresholds are in the range [0.1, 0.2] using the BOW model and in the range [0.3, 0.5] using word embeddings.", "labels": [], "entities": [{"text": "BOW", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8346850275993347}]}, {"text": "While, the similarity thresholds are slightly higher w.r.t. the multi-document experiment: about 0.7 and 0.95 for BOW and skip-gram, respectively.", "labels": [], "entities": [{"text": "similarity thresholds", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.9725235998630524}, {"text": "BOW", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9015117287635803}]}, {"text": "This is due to the fact that too similar sentences are rare, especially with well-written documents as Wikipedia articles.", "labels": [], "entities": []}, {"text": "The best parameters configuration for each language is used to generate summaries for the documents in the test set.", "labels": [], "entities": []}, {"text": "Also for this task, each document is preprocessed with the sentences segmentation and stopwords removal, without stemming.", "labels": [], "entities": []}, {"text": "We adopt the same automatic evaluation metrics used by the participating systems in MSS 2015 task: ROUGE-1, -2, -SU4 14 . ROUGE-SU4 computes the score between the generated and human summaries considering the overlap of the skip-bigrams of 4 as well as the unigrams.", "labels": [], "entities": [{"text": "MSS 2015 task", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.5251781841119131}, {"text": "ROUGE-1", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9935705661773682}]}, {"text": "Finally, the generated summary for each document must comply with a specific length constraint (rather than using a unique length limit for the whole collection  from the previous evaluation on DUC-2004.", "labels": [], "entities": [{"text": "DUC-2004", "start_pos": 194, "end_pos": 202, "type": "DATASET", "confidence": 0.9732291102409363}]}], "tableCaptions": [{"text": " Table 2: The most relevant sentences of the Donkey Kong article selected with the centroid-based sum- marization method using word embeddings. For each sentence are reported the related position ID in  the document and the similarity score computed between sentence and centroid embeddings. The words  that compose the centroid vector are marked in bold. The most similar words to the centroid ones are  reported in italic.", "labels": [], "entities": []}, {"text": " Table 4: ROUGE scores without topic threshold.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.994763970375061}, {"text": "topic threshold", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.9842238426208496}]}, {"text": " Table 6: Summaries of the cluster d30038 in DUC-2004 dataset using the centroid-based summarization  method with different sentence representations.", "labels": [], "entities": [{"text": "DUC-2004 dataset", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9847416281700134}]}, {"text": " Table 8: ROUGE-1, -2 scores (%) on MultiLing MSS 2015 dataset for five different languages.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9953346848487854}, {"text": "MultiLing MSS 2015 dataset", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.6918115839362144}]}]}