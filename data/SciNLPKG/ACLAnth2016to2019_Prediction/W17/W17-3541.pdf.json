{"title": [{"text": "Neural Response Generation for Customer Service based on Personality Traits", "labels": [], "entities": [{"text": "Neural Response Generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8868301113446554}]}], "abstractContent": [{"text": "We present a neural response generation model that generates responses conditioned on a target personality.", "labels": [], "entities": []}, {"text": "The model learns high level features based on the target personality, and uses them to update its hidden state.", "labels": [], "entities": []}, {"text": "Our model achieves performance improvements in both perplexity and BLEU scores over a base-line sequence-to-sequence model, and is validated by human judges.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9991888403892517}]}], "introductionContent": [{"text": "Automated conversational agents are becoming popular for various tasks, such as personal assistants, shopping assistants, or as customer service agents.", "labels": [], "entities": []}, {"text": "Automated agents benefit from adapting their personality according to the task at hand or to the customer ().", "labels": [], "entities": []}, {"text": "Thus, it is desirable for automated agents to be capable of generating responses that express a target personality.", "labels": [], "entities": []}, {"text": "Personality is defined as a set of traits which represent durable characteristics of a person.", "labels": [], "entities": []}, {"text": "Many models of personality exist while the most common one is the Big Five model , including: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.", "labels": [], "entities": [{"text": "Agreeableness", "start_pos": 137, "end_pos": 150, "type": "METRIC", "confidence": 0.973450243473053}]}, {"text": "These traits were correlated with linguistic choices including lexicon and syntax.", "labels": [], "entities": []}, {"text": "In this paper we study how to encode personality traits as part of neural response generation for conversational agents.", "labels": [], "entities": [{"text": "neural response generation", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.7746676007906595}]}, {"text": "Our approach builds upon a sequence-to-sequence (SEQ2SEQ) architecture () by adding an additional Example 1 Customer: Why isn't your service working???", "labels": [], "entities": []}, {"text": "Consensus-agent: We are aware of the issue and are working to fix it.", "labels": [], "entities": []}, {"text": "Agreeableness-agent: We're hereto help!", "labels": [], "entities": [{"text": "Agreeableness-agent", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9286389350891113}]}, {"text": "Are you getting any error messages or codes?", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model is designed to generate text conditioned on a target set of personality traits.", "labels": [], "entities": []}, {"text": "Specifically, we verified its performance in a scenario of customer service.", "labels": [], "entities": []}, {"text": "For our experiments we utilized the dataset presented in (, which exhibits a large variety of customer service properties.", "labels": [], "entities": []}, {"text": "This dataset is a collection of 1M conversations over customer service Twitter channels of 62 different brands which cover a large variety of product categories.", "labels": [], "entities": []}, {"text": "Several preprocessing steps were performed for our purposes: We first split the data to pairs consisting of a single customer utterance and its corresponding agent response.", "labels": [], "entities": []}, {"text": "We removed pairs containing non-English sentences.", "labels": [], "entities": []}, {"text": "We further removed pairs for agents that participated in less than 30 conversation pairs, so we would have sufficient data for each agent to extract their personality traits (see below).", "labels": [], "entities": []}, {"text": "This resulted in 87.5K conversation pairs in total including 633 different agents (138 \u00b1 160 pairs per agent on average).", "labels": [], "entities": []}, {"text": "Following () we used BLEU () for evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9988275170326233}]}, {"text": "Besides BLEU scores, we also report perplexity as an indicator of model capability.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9984740614891052}]}, {"text": "For implementation details, refer to Appendix A. Results.", "labels": [], "entities": [{"text": "Appendix A. Results", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.7185448010762533}]}, {"text": "We experimented with two different settings to measure our model's performance.", "labels": [], "entities": []}, {"text": "Warm Start: In the first experiment, data for each agent in the dataset was split between training, validation and test data sets with a fraction of 80%/10%/10%, respectively.", "labels": [], "entities": []}, {"text": "We then extracted the agents' personality traits using an external service (described in Appendix B), from the training data for each agent.", "labels": [], "entities": []}, {"text": "These personality traits values are then used during the model training as the values for the personality vector p.", "labels": [], "entities": []}, {"text": "In this setting, since all the agents that appear in the test data appear also in the training data, we can also test the performance of (, which learns a persona vector for each agent in the training data.", "labels": [], "entities": []}, {"text": "The results in table 1 show that the standard SEQ2SEQ model achieved the lowest performance in terms of both perplexity and BLEU score while the competing models which learn a representation for the agents achieved higher performance.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9864447116851807}]}, {"text": "The PERSONA-BASED model achieved similar perplexity but higher BLEU score than our model.", "labels": [], "entities": [{"text": "PERSONA-BASED", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.580162763595581}, {"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9765983819961548}]}, {"text": "This is reasonable since PERSONA-BASED is not restricted to personality based features.", "labels": [], "entities": [{"text": "PERSONA-BASED", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.8048170208930969}]}, {"text": "However, this model cannot generate content for agents which do not appear in the training data, and thus, it is limited.", "labels": [], "entities": []}, {"text": "Cold Start: In our second experiment, we split the dataset such that 10% of the agents only formed the validation and test sets (half of each agent's examples for each set).", "labels": [], "entities": []}, {"text": "Data for the other 90% of the agents formed the training set.", "labels": [], "entities": []}, {"text": "In this setting, data for agents in the test set does not appear in the training set.", "labels": [], "entities": []}, {"text": "These agents represent new personality distributions we would like to generate responses for.", "labels": [], "entities": []}, {"text": "Note that, we extracted target personality traits for agents in the training set using their training data, or, for agents in the test set, using validation data.", "labels": [], "entities": []}, {"text": "In this setting, it is not possible to test the PERSONA-BASED model since no representation is learned during training for agents in the test set.", "labels": [], "entities": [{"text": "PERSONA-BASED", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.8502296209335327}]}, {"text": "Thus, we only compare our model to the baseline SEQ2SEQ model.", "labels": [], "entities": []}, {"text": "shows that, in this setting, we get better performance by utilizing personality based representation: our model achieves a relative 6.7% decrease in perplexity, and a 46% relative improvement in BLEU score.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 149, "end_pos": 159, "type": "METRIC", "confidence": 0.9644035696983337}, {"text": "BLEU score", "start_pos": 195, "end_pos": 205, "type": "METRIC", "confidence": 0.9847448468208313}]}, {"text": "Results from both experiments demonstrate that we can better model the linguistic variation in agent responses by conditioning on target personality traits.", "labels": [], "entities": []}, {"text": "We conducted a human evaluation of our PERSONALITY-BASED model using a crowd-sourcing service.", "labels": [], "entities": []}, {"text": "This evaluation measures whether the responses generated by our model are correlated with the target personality traits.", "labels": [], "entities": []}, {"text": "We focused on two personality traits from the Big Five model that are important to customer service: agreeableness and conscientiousness).", "labels": [], "entities": []}, {"text": "We extracted 60 customer utterances from the validation set of the cold start setting described above.", "labels": [], "entities": []}, {"text": "We selected customer utterances that convey a negative sentiment, since re-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Warm start performance.", "labels": [], "entities": [{"text": "Warm start", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8774456977844238}]}]}