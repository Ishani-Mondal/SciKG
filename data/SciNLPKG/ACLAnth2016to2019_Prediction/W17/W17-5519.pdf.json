{"title": [{"text": "A surprisingly effective out-of-the-box char2char model on the E2E NLG Challenge dataset", "labels": [], "entities": [{"text": "E2E NLG Challenge dataset", "start_pos": 63, "end_pos": 88, "type": "DATASET", "confidence": 0.9588666260242462}]}], "abstractContent": [{"text": "We train a char2char model on the E2E NLG Challenge data, by exploiting \"out-of-the-box\" the recently released tf-seq2seq framework, using some of the standard options of this tool.", "labels": [], "entities": [{"text": "E2E NLG Challenge data", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.9616411179304123}]}, {"text": "With minimal effort, and in particular without delex-icalization, tokenization or lowercasing, the obtained raw predictions, according to a small scale human evaluation, are excellent on the linguistic side and quite reasonable on the adequacy side, the primary downside being the possible omissions of semantic material.", "labels": [], "entities": []}, {"text": "However, in a significant number of cases (more than 70%), a perfect solution can be found in the top-20 predictions, indicating promising directions for solving the remaining issues.", "labels": [], "entities": []}], "introductionContent": [{"text": "Very recently, researchers at Heriot-Watt University proposed the E2E NLG Challenge and released a dataset consisting of 50K (MR, RF) pairs, MR being a slot-value Meaning Representation of a restaurant, RF (human ReFerence) being a natural language utterance rendering of that representation.", "labels": [], "entities": []}, {"text": "The utterances were crowd-sourced based on pictorial representations of the MRs, with the intention of producing more natural and diverse utterances compared to the ones directly based on the original MRs ().", "labels": [], "entities": []}, {"text": "Most of the RNN-based approaches to Natural Language Generation (NLG) that we are aware of, starting with (, generate the output word-by-word, and resort to special delexicalization or copy mechanisms ( to * Previously Xerox Research Centre Europe.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.7935825288295746}, {"text": "Xerox Research Centre Europe", "start_pos": 219, "end_pos": 247, "type": "DATASET", "confidence": 0.8806822896003723}]}, {"text": "1 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ handle rare or unknown words, for instance restaurant names or telephone numbers.", "labels": [], "entities": []}, {"text": "One exception is (, who employed a char-based seq2seq model where the input MR is simply represented as a character sequence, and the output is also generated char-by-char; this approach avoids the rare word problem, as the character vocabulary is very small.", "labels": [], "entities": []}, {"text": "While () used an additional finite-state mechanism to guide the production of well-formed (and input-motivated) character sequences, the performance of their basic char2char model was already quite good.", "labels": [], "entities": []}, {"text": "We further explore how a recent out-of-the box seq2seq model would perform on E2E NLG Challenge, when used in a char-based mode.", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9328953425089518}]}, {"text": "We choose attention-based tfseq2seq framework provided by authors of) (which we detail in next section).", "labels": [], "entities": []}, {"text": "Using some standard options provided by this framework, and without any pre-or postprocessing (not even tokenization or lowercasing), we obtained results on which we conducted a small-scale human evaluation on one hundred MRs, involving two evaluators.", "labels": [], "entities": []}, {"text": "This evaluation, on the one hand, concentrated on the linguistic quality, and on the other hand, on the semantic adequacy of the produced utterances.", "labels": [], "entities": []}, {"text": "On the linguistic side, vast majority of the predictions were surprisingly grammatically perfect, while still being rather diverse and natural.", "labels": [], "entities": []}, {"text": "In particular, and contrary to the findings of () (on a different dataset), our char-based model never produced non-words.", "labels": [], "entities": []}, {"text": "On the adequacy side, we found that the only serious problem was the tendency (in about half of the evaluated cases) of the model to omit to render one (rarely two) slot(s); on the other end, it never hallucinated, and very rarely duplicated, material.", "labels": [], "entities": []}, {"text": "To try and assess the potential value of a simple re-ranking technique (which we did not implement at this stage, but the approach of) and more recently the \"inverted generation\" technique of could be used), we generated (using the beam-search option of the framework) 20-best utterances for each MR, which the evaluators scanned towards finding an \"oracle\", i.e. a generated utterance considered as perfect not only from the grammatical but also from the adequacy viewpoint.", "labels": [], "entities": []}, {"text": "An oracle was found in the first position in around 50% of the case, otherwise among the 20 positions in around 20% of the cases, and not at all inside this list in the remaining 30% cases.", "labels": [], "entities": []}, {"text": "On the basis of these experiments and evaluations we believe that there remains only a modest gap towards a very reasonable NLG seq2seq model for the E2E NLG dataset.", "labels": [], "entities": [{"text": "E2E NLG dataset", "start_pos": 150, "end_pos": 165, "type": "DATASET", "confidence": 0.9420398871103922}]}], "datasetContent": [{"text": "() explain the protocol followed for crowdsourcing the E2E NLG Challenge dataset.", "labels": [], "entities": [{"text": "E2E NLG Challenge dataset", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.9351081848144531}]}, {"text": "Slightly different from the description in the article, there are two additional slots in the dataset: 'kidsFriendly' and 'children-friendly' which seem to be alternates for 'familyFriendly'.", "labels": [], "entities": []}, {"text": "Thus, there are in total 10 slots (in decreasing order of frequency of being mentioned in the dataset MRs): name (100%), food (83%), customer rating (68%), priceRange (68%), area (60%), eatType (51%), near (50%), familyFriendly (25%), kidsFriendly (19%), children-friendly (19%).", "labels": [], "entities": [{"text": "priceRange", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.9064951539039612}]}, {"text": "Also, the number of active slots in the MRs varies as: 3 (5%), 4 (17%), 5 (19%), 6 (19%), 7 (16%), 8 (4%).", "labels": [], "entities": [{"text": "MRs", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.6572480201721191}]}, {"text": "The human evaluations were performed by two annotators on the top 20 predictions of the previously discussed model, for the first 100 MRs of the devset, using the following metrics: 1, 1=Prediction better than RF, 0=Prediction at par with RF, -1=RF better than prediction).", "labels": [], "entities": [{"text": "Prediction", "start_pos": 187, "end_pos": 197, "type": "METRIC", "confidence": 0.9528673887252808}, {"text": "Prediction", "start_pos": 216, "end_pos": 226, "type": "METRIC", "confidence": 0.9479404091835022}, {"text": "RF", "start_pos": 246, "end_pos": 248, "type": "METRIC", "confidence": 0.9899420738220215}]}], "tableCaptions": [{"text": " Table 1: BLEU scores on devset with different configuration: varying the depth of both encoder and decoder RNNs, type of  cell unit, different beam width and length penalty. (Results reported for only a single experiment with training and prediction.)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9956921935081482}]}, {"text": " Table 2: Human annotations for 100 samples using different metrics defined in Sec. 4. O (Omission), A (Addition), R (Repe- tition) and G (Grammar) are on binary scale. Naturalness is measured as (2/1/0) and Oracle as (1/0/-1). Predictions were also  judged against the reference on a scale of (1/0/-1).", "labels": [], "entities": [{"text": "O", "start_pos": 87, "end_pos": 88, "type": "METRIC", "confidence": 0.9427122473716736}]}, {"text": " Table 3: Human annotations for different slots using beam- width 20. 'Or@1' represents the presence of an 'oracle' at  first position while 'Or' represents the presence of 'Oracle'  (desirable) in the top-20 predictions. Cases where no oracle  was found are marked as 'No Or'.", "labels": [], "entities": []}]}