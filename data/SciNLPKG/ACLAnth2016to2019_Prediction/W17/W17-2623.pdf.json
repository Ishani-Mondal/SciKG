{"title": [{"text": "NewsQA: A Machine Comprehension Dataset", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9152854084968567}]}], "abstractContent": [{"text": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.9430138468742371}]}, {"text": "Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text in the articles.", "labels": [], "entities": []}, {"text": "We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning.", "labels": [], "entities": []}, {"text": "Analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual en-tailment.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9451780915260315}, {"text": "word matching", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.7250695377588272}]}, {"text": "We measure human performance on the dataset and compare it to several strong neural models.", "labels": [], "entities": []}, {"text": "The performance gap between humans and machines (13.3% F1) indicates that significant progress can be made on NewsQA through future research.", "labels": [], "entities": [{"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9974836707115173}, {"text": "NewsQA", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.9563931226730347}]}, {"text": "The dataset is freely available on-line.", "labels": [], "entities": []}], "introductionContent": [{"text": "Almost all human knowledge is recorded in the medium of text.", "labels": [], "entities": []}, {"text": "As such, comprehension of written language by machines, at a near-human level, would enable abroad class of artificial intelligence applications.", "labels": [], "entities": []}, {"text": "In human students we evaluate reading comprehension by posing questions based on a text passage and then assessing a student's answers.", "labels": [], "entities": []}, {"text": "Such comprehension tests are objectively gradable and may measure a range of important abilities, from basic understanding to causal reasoning to inference (.", "labels": [], "entities": []}, {"text": "To teach literacy to machines, the research community has taken a similar approach with machine comprehension (MC).", "labels": [], "entities": [{"text": "machine comprehension (MC)", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.705522334575653}]}, {"text": "Recent years have seen the release of a host of MC datasets.", "labels": [], "entities": [{"text": "MC datasets", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.8118441998958588}]}, {"text": "Generally, these consist of (document, question, answer) triples to be used in a supervised learning framework.", "labels": [], "entities": []}, {"text": "Existing datasets vary in size, difficulty, and collection methodology; however, as pointed out by, most suffer from one of two shortcomings: those that are designed explicitly to test comprehension () are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning () are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly).", "labels": [], "entities": []}, {"text": "More recently, proposed SQuAD, a dataset that overcomes these deficiencies as it contains crowdsourced natural language questions.", "labels": [], "entities": []}, {"text": "In this paper, we present a challenging new largescale dataset for machine comprehension: NewsQA.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 90, "end_pos": 96, "type": "DATASET", "confidence": 0.962265133857727}]}, {"text": "It contains 119,633 natural language questions posed by crowdworkers on 12,744 news articles from CNN.", "labels": [], "entities": []}, {"text": "In SQuAD, crowdworkers are tasked with both asking and answering questions given a paragraph.", "labels": [], "entities": []}, {"text": "In contrast, NewsQA was built using a collection process designed to encourage exploratory, curiosity-based questions that may better reflect realistic information-seeking behaviors.", "labels": [], "entities": []}, {"text": "Particularly, a set of crowdworkers were tasked to answer questions given a summary of the article, i.e. the CNN article highlights.", "labels": [], "entities": [{"text": "CNN article highlights", "start_pos": 109, "end_pos": 131, "type": "DATASET", "confidence": 0.9053948720296224}]}, {"text": "A separate set of crowdworkers selects answers given the full article, which consist of word spans in the corresponding articles.", "labels": [], "entities": []}, {"text": "This gives rise to interesting patterns such as questions that may not be answerable by the original article.", "labels": [], "entities": []}, {"text": "As,, and others have argued, it is important for datasets to be sufficiently challenging to teach models the abilities we wish them to learn.", "labels": [], "entities": []}, {"text": "Thus, inline with, our goal with NewsQA was to construct a corpus of challenging questions that necessitate reasoning-like behaviors-for example, synthesis of information across different parts of an article.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9502570629119873}, {"text": "synthesis of information across different parts of an article", "start_pos": 146, "end_pos": 207, "type": "TASK", "confidence": 0.7509605553415086}]}, {"text": "We designed our collection methodology explicitly to capture such questions.", "labels": [], "entities": []}, {"text": "NewsQA is closely related to the SQuAD dataset: it is crowdsourced, with answers given by spans of text within an article rather than single words or entities, and there are no candidate answers from which to choose.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9165762066841125}, {"text": "SQuAD dataset", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.858055979013443}]}, {"text": "The challenging characteristics of NewsQA that distinguish it from SQuAD are as follows: 1.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.9645823240280151}]}, {"text": "Articles in NewsQA are significantly longer (6x on average) and come from a distinct domain.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.941817581653595}]}, {"text": "2. Our collection process encourages lexical and syntactic divergence between questions and answers.", "labels": [], "entities": [{"text": "syntactic divergence between questions and answers", "start_pos": 49, "end_pos": 99, "type": "TASK", "confidence": 0.8307805359363556}]}, {"text": "3. A greater proportion of questions requires reasoning beyond simple word-and contextmatching.", "labels": [], "entities": []}, {"text": "4. A significant proportion of questions have no answer in the corresponding article.", "labels": [], "entities": []}, {"text": "We demonstrate through several metrics that, consequently, NewsQA offers a greater challenge to existing comprehension models.", "labels": [], "entities": []}, {"text": "Given their similarities, we believe that SQuAD and NewsQA can be used to complement each other, for instance to explore models' ability to transfer across domains.", "labels": [], "entities": []}, {"text": "In this paper we describe the collection methodology for NewsQA, provide a variety of statistics to characterize it and contrast it with previous datasets, and assess its difficulty.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.9644062519073486}]}, {"text": "In particular, we measure human performance and compare it to that of two strong neural-network baselines.", "labels": [], "entities": []}, {"text": "Humans significantly outperform powerful question-answering models, suggesting NewsQA could drive further advances in machine comprehension research.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.8703598976135254}]}], "datasetContent": [{"text": "NewsQA follows in the tradition of several recent comprehension datasets.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9418439865112305}]}, {"text": "These vary in size, difficulty, and collection methodology, and each has its own distinguishing characteristics.", "labels": [], "entities": [{"text": "collection", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.9386966228485107}]}, {"text": "All of our present experiments use the subset of NewsQA with agreed or validated answers (92,549 samples for training, 5,166 for validation, and 5,126 for testing).", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.955394983291626}]}, {"text": "We leave the challenge of identifying the unanswerable questions for future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The variety of answer types appearing in  NewsQA, with proportion statistics and examples.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9821404814720154}]}, {"text": " Table 2: Reasoning mechanisms needed to answer questions. For each we show an example question  with the sentence that contains the answer span. Words relevant to the reasoning type are in bold. The  corresponding proportion in the human-evaluated subset of both NewsQA and SQuAD (1,000 samples  each) is also given.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 264, "end_pos": 270, "type": "DATASET", "confidence": 0.9765399694442749}, {"text": "SQuAD", "start_pos": 275, "end_pos": 280, "type": "DATASET", "confidence": 0.7429531216621399}]}, {"text": " Table 3: Human performance on SQuAD and  NewsQA datasets. The first row is taken from Ra- jpurkar et al. (2016).", "labels": [], "entities": [{"text": "NewsQA datasets", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9442638754844666}]}, {"text": " Table 4: Sentence-level accuracy on standard and  artificially-lengthened SQuAD documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.967082142829895}]}, {"text": " Table 5: Neural model performance on SQuAD and NewsQA. mLSTM results on SQuAD are derived from  our implementation of", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.9528393745422363}]}]}