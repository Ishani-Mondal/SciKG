{"title": [{"text": "Controlling Target Features in Neural Machine Translation via Prefix Constraints", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6862952709197998}]}], "abstractContent": [{"text": "We propose prefix constraints, a novel method to enforce constraints on target sentences in neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.7018142143885294}]}, {"text": "It places a sequence of special tokens at the beginning of target sentence (target prefix), while side constraints (Sennrich et al., 2016) places a special token at the end of source sentence (source suffix).", "labels": [], "entities": []}, {"text": "Prefix constraints can be predicted from source sentence jointly with target sentence, while side constraints must be provided by the user or predicted by some other methods.", "labels": [], "entities": []}, {"text": "In both methods, special tokens are designed to encode arbitrary features on target-side or metatextual information.", "labels": [], "entities": []}, {"text": "We show that prefix constraints are more flexible than side constraints and can be used to control the behavior of neu-ral machine translation, in terms of output length, bidirectional decoding, domain adaptation, and unaligned target word generation .", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.552997479836146}, {"text": "domain adaptation", "start_pos": 195, "end_pos": 212, "type": "TASK", "confidence": 0.7180893868207932}, {"text": "unaligned target word generation", "start_pos": 218, "end_pos": 250, "type": "TASK", "confidence": 0.7296427637338638}]}], "introductionContent": [{"text": "It is difficult to change the behaviors of a current neural machine translation system, because the internal states of the system are represented by vectors of real numbers.", "labels": [], "entities": []}, {"text": "There are no symbols to be manipulated and end-to-end optimization makes it impossible to identify the source of poor performance.", "labels": [], "entities": []}, {"text": "Some studies control the output of the encoder-decoder model, through the use of additional information such as target-side information and meta-textual information.", "labels": [], "entities": []}, {"text": "Target-side information includes politeness (), voice (, sentence * * Currently, Retty Inc.", "labels": [], "entities": []}, {"text": "length (, and target language).", "labels": [], "entities": [{"text": "length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.926102340221405}]}, {"text": "Meta-textual information include dialogue act, user personality ( ), topic , and domain ( Two approaches can be used to provide additional information to the encoder-decoder model, word-level methods and sentence-level methods.", "labels": [], "entities": []}, {"text": "Word-level methods encode the additional information as a vector (embedding) that is input together with a word at each time step of either (or both) encoder and decoder.", "labels": [], "entities": []}, {"text": "Sentence level methods encode the additional information as special tokens.", "labels": [], "entities": []}, {"text": "Side constraints are placed at the end of source sentence (, while our proposal, prefix constraints, is placed at the beginning of target sentence.", "labels": [], "entities": []}, {"text": "The advantage of sentence-level methods over word-level methods is their simplicity in application.", "labels": [], "entities": []}, {"text": "The network structure of the underlying encoder-decoder model does not have to be modified.", "labels": [], "entities": []}, {"text": "The problem with side constraints is that, attest time, additional information must be either specified by the user or automatically predicted by some other method.", "labels": [], "entities": []}, {"text": "As prefix constraints move the special tokens from source to target, they can be predicted by the network jointly with target sentence.", "labels": [], "entities": []}, {"text": "Like side constraints, the user can specify prefix constraints by using prefix-constrained decoding (, which can be implemented by a trivial modification of the decoder.", "labels": [], "entities": []}, {"text": "The following sections start by describing the framework of prefix constraints.", "labels": [], "entities": []}, {"text": "We then show three simple use cases, namely, length control, bidirectional decoding, and domain adaptation.", "labels": [], "entities": [{"text": "length control", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7355971336364746}, {"text": "domain adaptation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7247589379549026}]}, {"text": "We then show a more sophisticated usage of prefix constraints: unaligned target word generation.", "labels": [], "entities": [{"text": "unaligned target word generation", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.6887232065200806}]}], "datasetContent": [{"text": "The experiments used five publicly available Japanese-English parallel corpora, namely IWSLT-2005, KFTT, GVOICES, REUTERS, and TATOEBA, as shown in.", "labels": [], "entities": [{"text": "IWSLT-2005", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.7116782665252686}, {"text": "KFTT", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.5388585925102234}, {"text": "GVOICES", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.8664130568504333}, {"text": "REUTERS", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9893327951431274}, {"text": "TATOEBA", "start_pos": 127, "end_pos": 134, "type": "METRIC", "confidence": 0.991804838180542}]}, {"text": "IWSLT-2005 is a dataset for Japanese-English Tasks of the International Workshop on Spoken Language Translation ().", "labels": [], "entities": [{"text": "IWSLT-2005", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9040767550468445}, {"text": "International Workshop on Spoken Language Translation", "start_pos": 58, "end_pos": 111, "type": "TASK", "confidence": 0.5356864432493845}]}, {"text": "It is available from ALA-GIN 2 . KFTT (Kyoto Free Translation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto 3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs.", "labels": [], "entities": [{"text": "Japanese-English translation task", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.7561081051826477}]}, {"text": "Tatoeba is a collection of multilingual translated example sentences from Tatoeba website.", "labels": [], "entities": [{"text": "Tatoeba is a collection of multilingual translated example sentences from Tatoeba website", "start_pos": 0, "end_pos": 89, "type": "DATASET", "confidence": 0.6373514508207639}]}, {"text": "These last two are available from OPUS (.", "labels": [], "entities": [{"text": "OPUS", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9629838466644287}]}, {"text": "Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for English and RCV2 for other languages) available from NIST ( . The unaligned target word generation experiments used two additional proprietary spoken.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8732101917266846}, {"text": "Reuters RCV1 RCV2 multilingual text categorization test collection data set", "start_pos": 62, "end_pos": 137, "type": "DATASET", "confidence": 0.9196388959884644}, {"text": "NIST", "start_pos": 201, "end_pos": 205, "type": "DATASET", "confidence": 0.959205150604248}, {"text": "unaligned target word generation", "start_pos": 214, "end_pos": 246, "type": "TASK", "confidence": 0.7239450514316559}]}, {"text": "Japanese sentences are normalized by NFKC (a unicode normalization form) and word segmented by MeCab 6 with UniDic.", "labels": [], "entities": [{"text": "NFKC", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8593326210975647}, {"text": "word segmented", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.684545248746872}]}, {"text": "For neural http://www.straightword.jp/ 6 http://taku910.github.io/mecab/ machine translation, we used seq2seq-attn , which implements an attention-based encoder-decoder ( . We used default settings unless otherwise specified.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7240674495697021}]}, {"text": "Translation accuracy is measured by BLEU ().", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9417251348495483}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9026040434837341}, {"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9984728693962097}]}, {"text": "compares side constraints with prefix constraints in terms of length control for IWSLT-2005 dataset.", "labels": [], "entities": [{"text": "IWSLT-2005 dataset", "start_pos": 81, "end_pos": 99, "type": "DATASET", "confidence": 0.980167955160141}]}, {"text": "Baseline is a NMT system trained on the parallel corpus without length tag.", "labels": [], "entities": []}, {"text": "Side Constraints and Prefix Constraints stand for NMT systems trained on the corpus with length tags placed at the end of source sentence and at the begging of target sentence, respectively.", "labels": [], "entities": []}, {"text": "In None, source sentences without length tag are entered into the system attest time.", "labels": [], "entities": []}, {"text": "In Oracle, reference length is encoded as length tag and prefix constrained decoding is used in Prefix Constraints.", "labels": [], "entities": []}, {"text": "In the training for Side Constraints, we mixed tagged sentences and non-tagged sentences to avoid over-fitting to length tag as described in, Prefix Constraints are comparable to or better than Side Constraints in controlling the length of the target sentence if the correct length is known and provided as an oracle.", "labels": [], "entities": []}, {"text": "It is difficult to predict the length of target sentence from source sentence, which lowered the ac-curacy of Prefix Constraints for None.", "labels": [], "entities": [{"text": "ac-curacy", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9946290850639343}]}, {"text": "The accuracy of length prediction for short sentences (less than 10 words) is 97.7%, while that for long sentences (more than or equal to 10 words) is 45.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997584223747253}]}], "tableCaptions": [{"text": " Table 1: Top 50 unaligned target words in IWSLT2005", "labels": [], "entities": [{"text": "IWSLT2005", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.833114743232727}]}, {"text": " Table 3: Comparison between side constraints and  prefix constraints on length control", "labels": [], "entities": []}, {"text": " Table 4: Comparison of target bidirectional  method (Liu et al., 2016) and decoding direction  prediction using prefix constraints", "labels": [], "entities": [{"text": "decoding direction  prediction", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.769025961558024}]}, {"text": " Table 5: BLEU scores for different systems in terms of domain adaptation techniques", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983804225921631}, {"text": "domain adaptation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7357765734195709}]}, {"text": " Table 6: Translation accuracy of prefix constraint  prediction and prefix-constrained decoding", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9624456763267517}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9699705839157104}, {"text": "prefix constraint  prediction", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6933519939581553}]}]}