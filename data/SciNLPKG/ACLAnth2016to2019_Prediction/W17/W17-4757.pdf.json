{"title": [], "abstractContent": [{"text": "This paper presents the results of the WMT17 Neural MT Training Task.", "labels": [], "entities": [{"text": "WMT17 Neural MT Training Task", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.7897396445274353}]}, {"text": "The objective of this task is to explore the methods of training a fixed neural architecture , aiming primarily at the best translation quality and, as a secondary goal, shorter training time.", "labels": [], "entities": []}, {"text": "Task participants were provided with a complete neural machine translation system, fixed training data and the configuration of the network.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.8017412622769674}]}, {"text": "The translation was performed in the English-to-Czech direction and the task was divided into two subtasks of different configurations-one scaled to fit on a 4GB and another on an 8GB GPU card.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.972264289855957}]}, {"text": "We received 3 submissions for the 4GB variant and 1 submission for the 8GB variant ; we provided also our run for each of the sizes and two baselines.", "labels": [], "entities": []}, {"text": "We translated the test set with the trained models and evaluated the outputs using several automatic metrics.", "labels": [], "entities": []}, {"text": "We also report results of the human evaluation of the submitted systems .", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has recently replaced the \"classical statistical machine translation\" and became the dominant research paradigm.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8196892639001211}, {"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7660874327023824}]}, {"text": "A large part of research on NMT is focused on architectural improvements of the neural networks or data preprocessing.", "labels": [], "entities": []}, {"text": "However, in practice, the results of an NMT system depends not only on the architecture of the network, but also on the training techniques used to obtain the parameters.", "labels": [], "entities": []}, {"text": "The goal of NMT Training Task 1 is to compare the results of various training techniques applied to a fixed network architecture.", "labels": [], "entities": [{"text": "NMT Training Task 1", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7814570218324661}]}, {"text": "We provided task participants with the model specification, training and validation data with a fixed way of data preprocessing.", "labels": [], "entities": []}, {"text": "We also listed a few methods as an inspiration for the participants.", "labels": [], "entities": []}, {"text": "These methods included (but were not limited to) the following: Curricula.", "labels": [], "entities": []}, {"text": "The basic idea behind this technique () is inspired by the fact that humans learn more easily when examples are presented in an ordering from trivial to complex ones.", "labels": [], "entities": []}, {"text": "Neural networks could potentially also benefit from such a strategy of increasing task difficulty.", "labels": [], "entities": []}, {"text": "This technique includes modifications of the training data in order to converge faster, or more robustly, towards possibly better local optima.", "labels": [], "entities": []}, {"text": "Data shuffling, reordering, or back-translation, are all techniques that can have a positive impact on the training.", "labels": [], "entities": [{"text": "Data shuffling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7346054315567017}]}, {"text": "There are many optimization algorithms that can be employed to training an NMT model, such as Adadelta or Adam (.", "labels": [], "entities": []}, {"text": "Each method differs in the number of inner trainable parameters and the approach it uses them to perform gradient descent optimization.", "labels": [], "entities": [{"text": "gradient descent optimization", "start_pos": 105, "end_pos": 134, "type": "TASK", "confidence": 0.8411515156428019}]}, {"text": "Better optimization algorithms can improve both the convergence speed and model performance.", "labels": [], "entities": []}, {"text": "A significant improvement in model performance can also be achieved by using variants of the REINFORCE algorithm.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9583577513694763}]}, {"text": "MIXER (, self-critical training (, or minimum risk training) all optimize the model directly to maximize the sentence-level BLEU score) or another sequence-based metric.", "labels": [], "entities": [{"text": "MIXER", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6774734854698181}, {"text": "BLEU score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9589596092700958}]}, {"text": "These methods deal with the exposure bias problem the traditional cross-entropy approach suffers from.", "labels": [], "entities": []}, {"text": "Multi-task training methods improve the model by training it to perform many tasks at once.", "labels": [], "entities": []}, {"text": "show that teaching the model how to parse helps the translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9766674041748047}]}, {"text": "A similar result was achieved by who teach the network to predict the visual features of an image when translating its caption.", "labels": [], "entities": []}, {"text": "The goal of knowledge distillation is to reduce the size of large trained models to smaller models while maintaining the good performance.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.8077627420425415}]}, {"text": "There are two ways for employing this technique.", "labels": [], "entities": []}, {"text": "First, train a large model and then reduce its size by removing unimportant units.", "labels": [], "entities": []}, {"text": "Second, train a large \"teacher\" model (or ensemble of models) and train a smaller \"student\" network on its outputs.", "labels": [], "entities": []}, {"text": "Both of these methods showed promising results, not only in NMT but in the deep learning field in general.", "labels": [], "entities": [{"text": "NMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9378564357757568}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the software and the model architectures.", "labels": [], "entities": []}, {"text": "Details on the used dataset are given in Section 3.", "labels": [], "entities": []}, {"text": "We summarize the submitted systems in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents the results of the submissions and Section 6 discusses them.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "As announced, the official evaluation of the NMT training task is the manual scoring of the systems submitted at the deadline according to the submission instructions.", "labels": [], "entities": [{"text": "NMT training task", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8840758800506592}]}, {"text": "We designed training task so that it was in fact subsumed by the WMT17 News Translation Task (: the training data was a subset of the training data provided for English-to-Czech news task participants and the testset we used the official newstest2017 of WMT.", "labels": [], "entities": [{"text": "WMT17 News Translation Task", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.8138761073350906}, {"text": "WMT", "start_pos": 254, "end_pos": 257, "type": "DATASET", "confidence": 0.9394400715827942}]}, {"text": "All training task submissions can be therefore seen as regular news task submissions, with the additional constraint of a fixed MT system and further constrained training data.", "labels": [], "entities": []}, {"text": "With the help of WMT17 news task organizers, we included the outputs of training task submissions among the MT outputs of other MT systems for the common manual scoring.", "labels": [], "entities": [{"text": "WMT17 news task organizers", "start_pos": 17, "end_pos": 43, "type": "DATASET", "confidence": 0.9345870465040207}, {"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.91050124168396}]}, {"text": "Please see for details on the judgment technique (direct assessment, DA) and its interpretation. is an extract of the official WMT17 news task results, i.e. in, renaming the systems to match the naming in this paper.", "labels": [], "entities": [{"text": "WMT17 news task results", "start_pos": 127, "end_pos": 150, "type": "DATASET", "confidence": 0.9299526214599609}]}, {"text": "The horizontal lines between the systems indicate clusters according to Wilcoxon CUNI-8GB-DOMAIN CUNI-4GB-CURRIC BASELINE-8GB DENISOV-4GB BASELINE-4GB CUNI-4GB-BATCH-DECR: Learning curves for training task submissions (where available).", "labels": [], "entities": [{"text": "Wilcoxon CUNI-8GB-DOMAIN CUNI-4GB-CURRIC BASELINE-8GB DENISOV-4GB BASELINE-4GB CUNI-4GB-BATCH-DECR", "start_pos": 72, "end_pos": 170, "type": "METRIC", "confidence": 0.6961360190595899}]}, {"text": "The 8GB and 4GB baseline runs actually ran much longer, to 300M and 380M training steps, resp.", "labels": [], "entities": []}, {"text": "CUNI-4GB-CURRIC and CUNI-8GB-DOMAIN curves are only continuations and therefore start higher.", "labels": [], "entities": [{"text": "CUNI-4GB-CURRIC", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8986542820930481}]}, {"text": "rank-sum test at p-level p \u2264 0.05, the column \"#\" is the rank of the cluster.", "labels": [], "entities": []}, {"text": "The \"Ave %\" is the average DA score overall evaluated translations by the given system and it reflects the average quality as assessed by human judges against the reference translation on an absolute scale between 0 and 100.", "labels": [], "entities": [{"text": "Ave %\"", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9084876477718353}, {"text": "DA score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.834414541721344}]}, {"text": "The \"Ave z\" first standardizes each annotator's scores and then averages them.", "labels": [], "entities": [{"text": "Ave z\"", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.891089657942454}]}, {"text": "Please seethe original paper fora detailed discussion.", "labels": [], "entities": []}, {"text": "The manual evaluation was affected by an unfortunate omission: namely, the baseline-4GB outputs were not included in the standard batches, among other outputs, but they were scored only later, in annotation batches of their own.", "labels": [], "entities": []}, {"text": "While the direct assessment annotation technique in theory evaluates translation quality on an absolute scale and such evaluations could be in principle comparable among different annotation runs, we see that this does not really work in practice.", "labels": [], "entities": []}, {"text": "It is rather unlikely that the 4GB baseline would be significantly better than the 8GB baseline, also taking into account the big difference in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9829967021942139}]}, {"text": "We thus asked WMT17 news task organizers to remove baseline-4GB from their paper and we do not consider this result in our discussion below.", "labels": [], "entities": [{"text": "WMT17 news task organizers", "start_pos": 14, "end_pos": 40, "type": "DATASET", "confidence": 0.9451902657747269}]}], "tableCaptions": [{"text": " Table 1: Configuration for 4 and 8 GB models.", "labels": [], "entities": []}, {"text": " Table 3: Automatic scores for submissions to the WMT17 NMT Training Task.", "labels": [], "entities": [{"text": "Automatic scores", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.930705189704895}, {"text": "WMT17 NMT Training Task", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.811103105545044}]}, {"text": " Table 4: Manual evaluation of the training task  submissions. For the crossed-out BASELINE-4GB  see the text.", "labels": [], "entities": [{"text": "BASELINE-4GB", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9919724464416504}]}]}