{"title": [{"text": "Cross-lingual parser selection for low-resource language\u0161 language\u0161", "labels": [], "entities": [{"text": "Cross-lingual parser selection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7145137588183085}]}], "abstractContent": [{"text": "In multilingual dependency parsing, transferring delexicalized models provides unmatched language coverage and competitive scores, with minimal requirements.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6160818934440613}]}, {"text": "Still, selecting the single best parser for any target language poses a challenge.", "labels": [], "entities": []}, {"text": "Here, we propose a lean method for parser selection.", "labels": [], "entities": [{"text": "parser selection", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.9761656820774078}]}, {"text": "It offers top performance, and it does so without disadvantaging the truly low-resource languages.", "labels": [], "entities": []}, {"text": "We consistently select appropriate source parsers for our target languages in a realistic cross-lingual parsing experiment.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.6586530357599258}]}], "introductionContent": [{"text": "Treebanks are available for only \u223c1% of the languages spoken in the world today, the resourcerich sources.", "labels": [], "entities": []}, {"text": "One major goal of cross-lingual transfer learning is to provide robust NLP for all the targets, or the remaining \u223c99%.", "labels": [], "entities": [{"text": "cross-lingual transfer learning", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.8263019323348999}]}, {"text": "If we want to parse any language for syntactic dependencies, the only principled method that currently enables it is delexicalized model transfer.", "labels": [], "entities": [{"text": "delexicalized model transfer", "start_pos": 117, "end_pos": 145, "type": "TASK", "confidence": 0.6391821602980295}]}, {"text": "By relying on uniform POS tags only, it offers unprecedented language coverage.", "labels": [], "entities": []}, {"text": "First introduced by, and consolidated by the seminal works of  and, delexicalized parsing is nowadays considered to be a simple baseline.", "labels": [], "entities": [{"text": "delexicalized parsing", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.5723188817501068}]}, {"text": "Recent work promises cross-lingual methods that score almost as high as supervised parsers.", "labels": [], "entities": []}, {"text": "Unfortunately, it also introduces requirements that avast majority of languages cannot meet.", "labels": [], "entities": []}, {"text": "The systems proposed by, e.g., or require: -very large parallel corpora, often in excess of 2M parallel sentences for each language pair, coupled with near-perfect tokenization and sentence splitting; -high-quality sentence and word alignments for all the language pairs, provided by aligners that favor closely related languages; -accurate POS tagging using fully supervised taggers that score \u223c95% on held-out data.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.7547479867935181}, {"text": "POS tagging", "start_pos": 341, "end_pos": 352, "type": "TASK", "confidence": 0.7852803766727448}]}, {"text": "Latest work by, among a few others, shows that in a real-world scenario, where no such unrealistic assumptions are made, delexicalized transfer still constitutes a very competitive choice for multilingual parsing.", "labels": [], "entities": []}, {"text": "Here, we assert that even simple delexicalized parsing might be in need of a reality check.", "labels": [], "entities": []}, {"text": "The idea behind delexicalization is very simple: we omit all lexical features from the parsers, both at training and at runtime, so that they operate on POS sequences only.", "labels": [], "entities": []}, {"text": "All that is then needed to parse an unknown language is a tagger using a uniform POS representation such as the \"universal\" POS tagset by . Delexicalized parsing itself comes in two distinct basic variants: i) multi-source, where we train a single parser by joining multiple delexicalized sourcelanguage treebanks, and ii) single-source, where each source-language treebank contributes a single parser, and then we select the one to use from this pool of parsing models.", "labels": [], "entities": []}, {"text": "Most often, we pick the single-best source parser fora given target language.", "labels": [], "entities": []}, {"text": "The rankings of the candidate source parsers are determined by evaluation on target language test data.", "labels": [], "entities": []}, {"text": "Single-best parsers generally perform better than multi-source parsers.", "labels": [], "entities": []}, {"text": "For example, in their experiment, Agi\u00b4c show that the single-source variant beats multi-source delexicalization in 23/27 languages and scores +3 points higher in UAS on average.", "labels": [], "entities": [{"text": "UAS", "start_pos": 162, "end_pos": 165, "type": "DATASET", "confidence": 0.5544834136962891}]}, {"text": "For fairness, their parsers all work with cross-lingual POS taggers.", "labels": [], "entities": []}, {"text": "However, we argue that single-best source parsing is not realistic.", "labels": [], "entities": [{"text": "single-best source parsing", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.5697968403498331}]}, {"text": "Only in an evaluation framework do we possess prior knowledge of i) which target language we are parsing, and ii) what the source rankings are for the targets.", "labels": [], "entities": []}, {"text": "Single-best parsing thus amounts to an oracle.", "labels": [], "entities": []}, {"text": "By contrast, in the real world, we expect to parse by i) predicting the target language name from the text input at runtime, and by ii) selecting the most appropriate source parser for that language from the parser pool.", "labels": [], "entities": [{"text": "predicting the target language name", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.8268640637397766}]}, {"text": "If the prediction or selection turnout incorrect, we are likely to end up producing a suboptimal parse.", "labels": [], "entities": []}, {"text": "Furthermore, while parsing accuracy is measured on test sets of \u223c1000 sentences on average, the real input can take a much wider size range.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9649441838264465}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9275702834129333}]}, {"text": "This variation in size may challenge the validity of any design choices made on test setlevel only.", "labels": [], "entities": []}, {"text": "The cross-lingual parsing community has largely ignored this problem, focusing instead on test set-based evaluation by proxy.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7048806846141815}]}, {"text": "This, in addition to a list of methodological biases, has spawned a number of complex models incapable of scaling down to real low-resource languages.", "labels": [], "entities": []}, {"text": "How do we single out the best source parser if the language of the input text has to be predicted at runtime?", "labels": [], "entities": []}, {"text": "To answer this question, our paper makes the following contributions: i) We propose a set of methods for matching texts to source parsers.", "labels": [], "entities": []}, {"text": "Our methods are simple, as they rely on nothing but characterbased language identification and typological similarity.", "labels": [], "entities": [{"text": "characterbased language identification", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.611073891321818}]}, {"text": "They consistently find the best parsers for the target languages.", "labels": [], "entities": []}, {"text": "ii) We set aside the test-set granularity assumption.", "labels": [], "entities": []}, {"text": "Instead, we assume that the parser input can vary in size from as little as one sentence.", "labels": [], "entities": []}, {"text": "Our methods prove to be remarkably adaptable to this size variation.", "labels": [], "entities": []}, {"text": "iii) By combining our approaches, our best system even manages to exceed the performance of single-best oracle source parsers.", "labels": [], "entities": []}, {"text": "In our submission, we strive to introduce only the minimal requirements, and to maintain a realistic setup.", "labels": [], "entities": []}, {"text": "For example, in all the experiments, we apply cross-lingual POS taggers for truly low-resource languages.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.7220813930034637}]}, {"text": "By controlling for POS sources, we show how an ingrained bias towards direct supervision of taggers may render any parsing results irrelevant in a low-resource context.", "labels": [], "entities": []}, {"text": "Our code and data are freely available.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our setup, we parse the target texts T with multiple source parsers h S , and we seek to predict the best source parses for all the targets.", "labels": [], "entities": []}, {"text": "We now expose the details of this experiment outline.", "labels": [], "entities": []}, {"text": "We use the Universal Dependencies (UD) treebanks (Nivre et al., 2016) version 1.3. 5 UD currently offers 54 dependency treebanks for 41 different languages.", "labels": [], "entities": []}, {"text": "Since our experiment requires realistic crosslingual POS taggers, we use the freely available collection of training sets by Agi\u00b4c.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.7273357957601547}, {"text": "Agi\u00b4c", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.9012253284454346}]}, {"text": "It is built through low-resource annotation projection over parallel texts from The Watchtower online library (WTC).", "labels": [], "entities": [{"text": "The Watchtower online library (WTC)", "start_pos": 80, "end_pos": 115, "type": "DATASET", "confidence": 0.8332506375653403}]}, {"text": "Thus, we intersect the lan-guages with POS tagging support from WTC with the UD treebanks fora total of 26 languages whose training and testing sets that we proceed to use in the experiment.", "labels": [], "entities": [{"text": "WTC", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.950016438961029}, {"text": "UD treebanks", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.8819769322872162}]}, {"text": "We make use of the English UD development data in hyper-parameter tuning.", "labels": [], "entities": [{"text": "English UD development data", "start_pos": 19, "end_pos": 46, "type": "DATASET", "confidence": 0.8531574010848999}]}, {"text": "For POS tagging, we use a state-of-theart CRF-based tagger MarMoT.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8418090641498566}, {"text": "MarMoT", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.7103888392448425}]}, {"text": "We use Bohnet's (2010) mate-tools graph-based dependency parser.", "labels": [], "entities": []}, {"text": "Both tools are run with their default settings.", "labels": [], "entities": []}, {"text": "In our experiments, we control for the sources of POS tags.", "labels": [], "entities": []}, {"text": "We distinguish i) direct in-language supervision, where the taggers are trained on target language UD training data, from ii) crosslingually predicted POS, where we train the taggers on WTC-projected annotations.", "labels": [], "entities": [{"text": "WTC-projected", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.8074336051940918}]}, {"text": "For training the delexicalized source parsers, we use the following standard features, in reference to the CoNLL 2009 file format: 9 ID, POS, HEAD, and DEPREL).", "labels": [], "entities": [{"text": "CoNLL 2009 file format", "start_pos": 107, "end_pos": 129, "type": "DATASET", "confidence": 0.9658231288194656}, {"text": "POS", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9213007092475891}, {"text": "HEAD", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9733036160469055}, {"text": "DEPREL", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9929360747337341}]}, {"text": "In specific, we don't leverage the UD morphological features (FEATS) as not all languages support them in the 1.3 release.", "labels": [], "entities": [{"text": "FEATS", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9951581358909607}]}, {"text": "We subsample the treebanks for parser training with a ceiling of 10k sentences, so as to avoid the bias towards the largest treebanks such as Czech with 68k training set sentences.", "labels": [], "entities": []}, {"text": "We set the oracle SINGLE-BEST source parsing results as the main reference point for our evaluation.", "labels": [], "entities": [{"text": "oracle SINGLE-BEST source parsing", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.5736675709486008}]}, {"text": "We compare all systems to these scores, as our benchmarking goals are to i) reach SINGLE-BEST performance through best source prediction and to ii) surpass it by weighted reparsing.", "labels": [], "entities": []}, {"text": "We compare our approach to the standard multisource delexicalized parser of McDonald et al.", "labels": [], "entities": []}, {"text": "(2011) (multi-dir in their paper, MULTI here).", "labels": [], "entities": [{"text": "MULTI", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.7892035841941833}]}, {"text": "In training, we uniformly sample from the contributing sources up to 10k sentences.", "labels": [], "entities": []}, {"text": "We collect all single-source parses of target sentences t \u2208 T into a dependency graph.", "labels": [], "entities": []}, {"text": "The graph G t = (V, E) has target tokens as vertices V . The edges (u S , v) \u2208 E originate in the delexicalized source parsers h S , \u2200S.", "labels": [], "entities": []}, {"text": "Following Sagae and Lavie (2006), we can apply directed maximum spanning tree decoding DMST(G t ), resulting in a voted dependency parse fora target sentence t, where each source contributes a unit vote.", "labels": [], "entities": []}, {"text": "Such unit voting presumes that all edges have a weight of 1.", "labels": [], "entities": [{"text": "unit voting presumes", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.7514279981454214}]}, {"text": "We refer to this approach as UNIFORM reparsing.", "labels": [], "entities": [{"text": "UNIFORM reparsing", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.6769754588603973}]}, {"text": "We also experiment with weighing the edges in G t through the distance measures KL-POS, WALS, and COM-BINED: The weights in turn depend on the granularity, as varying sizes of T influence the similarity estimates coming from KL-POS and WALS.", "labels": [], "entities": [{"text": "WALS", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.8241474628448486}, {"text": "COM-BINED", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9355913996696472}]}, {"text": "We tune the softmax temperature to \u03c4 = 0.2 for both KL-POS and WALS by using the English UD development data.", "labels": [], "entities": [{"text": "English UD development data", "start_pos": 81, "end_pos": 108, "type": "DATASET", "confidence": 0.8359621614217758}]}, {"text": "For simplicity, we fix \u03bb K = \u03bb W = 0.5, \u03bb L = 0 without tuning, i.e., in the COMBINED system we give equal weight to KL-POS and WALS.", "labels": [], "entities": []}, {"text": "We exclude LANG-ID from reparsing as it is subsumed by WALS.", "labels": [], "entities": [{"text": "reparsing", "start_pos": 24, "end_pos": 33, "type": "TASK", "confidence": 0.9537005424499512}]}, {"text": "Our experiment assumes the variability of input size in sentences.", "labels": [], "entities": []}, {"text": "We use the full UD test sets for all 26 languages.", "labels": [], "entities": [{"text": "UD test sets", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.8370275696118673}]}, {"text": "However, we vary the sample size or granularity gin best source prediction.", "labels": [], "entities": []}, {"text": "It is implemented as a moving window over the test sets, with sizes of 1 to 100.", "labels": [], "entities": []}, {"text": "The experiment workflow is condensed in Algorithm 1.", "labels": [], "entities": [{"text": "Algorithm", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9423035979270935}]}, {"text": "It shows how we arrive at best source predictions and reparsed trees fora target sample T . In the algorithm sketch, we assume g = |T |, i.e., the granularity is implied by the sample size, but further we provide results for varying g.", "labels": [], "entities": []}, {"text": "Any edge weighting in reparsing is made internal to DMST.", "labels": [], "entities": [{"text": "DMST", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8879714608192444}]}], "tableCaptions": [{"text": " Table 1: Summary UAS parsing scores for all  26 languages, over two underlying sources of POS  tags. Gray: highest scores grouped by POS and  method. \u00b1: 95% confidence intervals. g: sample  size (granularity) associated with the best score.", "labels": [], "entities": [{"text": "UAS parsing", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.8368645906448364}]}, {"text": " Table 2: Parsing target languages using source language weighting. We report changes in UAS over  the SINGLE-BEST delexicalized parsers. POS tags are provided by cross-lingual taggers. Bold: the  best system for a given language, separate for source selection and reparsing, excluding COMBINED.  Underlined: COMBINED systems that match or beat the other respective weighting methods. Gray:  Best overall average score.", "labels": [], "entities": []}]}