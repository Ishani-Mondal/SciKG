{"title": [{"text": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?", "labels": [], "entities": [{"text": "Image Caption Generator", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.8172868092854818}]}], "abstractContent": [{"text": "In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary 'generation' component.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.6971532106399536}]}, {"text": "This view suggests that the image features should be 'injected' into the RNN.", "labels": [], "entities": [{"text": "RNN", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8832265138626099}]}, {"text": "This is in fact the dominant view in the literature.", "labels": [], "entities": []}, {"text": "Alternatively, the RNN can instead be viewed as only encoding the previously generated words.", "labels": [], "entities": []}, {"text": "This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be 'merged' with the image features at a later stage.", "labels": [], "entities": []}, {"text": "This paper compares these two architectures.", "labels": [], "entities": []}, {"text": "We find that, in general, late merging outper-forms injection, suggesting that RNNs are better viewed as encoders, rather than generators.", "labels": [], "entities": []}], "introductionContent": [{"text": "Image captioning () has emerged as an important testbed for solutions to the fundamental AI challenge of grounding symbolic or linguistic information in perceptual data).", "labels": [], "entities": [{"text": "Image captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7575058937072754}]}, {"text": "Most captioning systems focus on what refer to as concrete conceptual descriptions, that is, captions that describe what is strictly within the image, although recently, there has been growing interest in moving beyond this, with research on visual question-answering () and imagegrounded narrative generation ( among others.", "labels": [], "entities": [{"text": "imagegrounded narrative generation", "start_pos": 275, "end_pos": 309, "type": "TASK", "confidence": 0.6351368427276611}]}, {"text": "Approaches to image captioning can be divided into three main classes (): 1.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.7548903226852417}]}, {"text": "Systems that rely on computer vision techniques to extract object detections and features from the source image, using these as input to an NLG stage ().", "labels": [], "entities": []}, {"text": "The latter is roughly akin to the microplanning and realisation modules in the well-known NLG pipeline architecture).", "labels": [], "entities": []}, {"text": "2. Systems that frame the task as a retrieval problem, where a caption, or parts thereof, is identified by computing the proximity/relevance of strings in the training data to a given image.", "labels": [], "entities": []}, {"text": "This is done by exploiting either a unimodal (; Mason and Charniak, ) or multimodal () space.", "labels": [], "entities": []}, {"text": "Many retrieval-based approaches rely on neural models to handle both image features and linguistic information ().", "labels": [], "entities": []}, {"text": "3. Systems that also rely on neural models, but rather than performing partial or wholesale caption retrieval, generate novel captions using a recurrent neural network (RNN), usually along short-term memory (LSTM).", "labels": [], "entities": [{"text": "caption retrieval", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.8062467277050018}]}, {"text": "Typically, such models use image features extracted from a pre-trained convolutional neural network (CNN) such as the VGG CNN) to bias the RNN towards sampling terms from the vocabulary in such away that a sequence of such terms produces a caption that is relevant to the image (.", "labels": [], "entities": [{"text": "VGG CNN", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.9526044130325317}]}, {"text": "This paper focuses on the third class.", "labels": [], "entities": []}, {"text": "The key property of these models is that the CNN image features are used to condition the predictions of the best caption to describe the image.", "labels": [], "entities": []}, {"text": "However, this can be done in different ways and the role of the RNN depends in large measure on the mode in which CNN and RNN are combined.", "labels": [], "entities": []}, {"text": "It is quite typical for RNNs to be viewed as 'generators'.", "labels": [], "entities": []}, {"text": "For example, suggest that 'the RNN is trained to generate the next word [of a caption]', a view also expressed by.", "labels": [], "entities": []}, {"text": "A similar position has also been taken in work focusing on the use of RNNs as language models for generation.", "labels": [], "entities": []}, {"text": "However, an alternative view is possible, whereby the role of the RNN can bethought of as primarily to encode sequences, but not directly to generate them.", "labels": [], "entities": []}, {"text": "These two views can be associated with different architectures for neural caption generators, which we discuss below and illustrated in.", "labels": [], "entities": [{"text": "neural caption generators", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7789016962051392}]}, {"text": "In one class of architectures, image features are directly incorporated into the RNN during the sequence encoding process.", "labels": [], "entities": []}, {"text": "In these models, it is natural to think of the RNN as the primary generation component of the image captioning system, making predictions conditioned by the image.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7466102242469788}]}, {"text": "A different architecture keeps the encoding of linguistic and perceptual features separate, merging them in a later multimodal layer, at which point predictions are made.", "labels": [], "entities": []}, {"text": "In this type of model, the RNN is functioning primarily as an encoder of sequences of word embeddings, with the visual features merged with the linguistic features in a later, multimodal layer.", "labels": [], "entities": []}, {"text": "This multimodal layer is the one that drives the generation process since the RNN never sees the image and hence would not be able to direct the generation process.", "labels": [], "entities": []}, {"text": "While both architectural alternatives have been attested in the literature, their implications have not, to our knowledge, been systematically discussed and comparatively evaluated.", "labels": [], "entities": []}, {"text": "In what follows, we first discuss the distinction between the two architectures (Section 2) and then present some experiments comparing the two (Sections 3 and 4).", "labels": [], "entities": []}, {"text": "Our conclusion is that grounding language generation in image data is best conducted in an architecture that first encodes the two modalities separately, before merging them to predict captions.", "labels": [], "entities": [{"text": "grounding language generation", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7518664399782816}]}], "datasetContent": [{"text": "To evaluate the performance of the inject and merge architectures, and thus the roles of the RNN, we trained and evaluated them on the Flickr8k (Hodosh et al., 2013) and Flickr30k () datasets of image-caption pairs.", "labels": [], "entities": [{"text": "Flickr8k", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.8103731870651245}, {"text": "Flickr30k () datasets", "start_pos": 170, "end_pos": 191, "type": "DATASET", "confidence": 0.7752250631650289}]}, {"text": "For the purposes of these experiments, we used the version of the datasets distributed by . The dataset splits are identical to that used by: Flickr8k is split into 6,000 images for training, 1,000 for validation, and 1,000 for testing whilst Flickr30k is split into 29,000 images for training, 1,014 images for validation, and 1,000 images for testing.", "labels": [], "entities": [{"text": "Flickr8k", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.7542673349380493}, {"text": "Flickr30k", "start_pos": 243, "end_pos": 252, "type": "DATASET", "confidence": 0.9149558544158936}]}, {"text": "Each image  Tokens with frequency lower than a threshold in the training set were replaced with the 'unknown' token.", "labels": [], "entities": []}, {"text": "In our experiments we varied the threshold between 3 and 5 in order to measure the performance of each model as vocabulary size changes.", "labels": [], "entities": []}, {"text": "For thresholds of 3, 4, and 5, this gives vocabulary sizes of 2,539, 2,918, and 3,478 for Flickr8k and 7,415, 8,275, 9,584 and for Flickr30k.", "labels": [], "entities": [{"text": "Flickr8k", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9089179039001465}, {"text": "Flickr30k", "start_pos": 131, "end_pos": 140, "type": "DATASET", "confidence": 0.9774370193481445}]}, {"text": "The hidden state and the cell state always have the same size.", "labels": [], "entities": []}, {"text": "In the experiments, this basic neural language model is used as apart of two different architectures: In the inject architecture, the image vector is concatenated with each of the word vectors in a caption.", "labels": [], "entities": []}, {"text": "In the merge architecture, it is only concatenated with the final LSTM state.", "labels": [], "entities": []}, {"text": "The layer sizes of the embedding, LSTM state, and projected image vector were also varied in the experiments in order to measure the effect of increasing the capacity of the networks.", "labels": [], "entities": []}, {"text": "The layer sizes used are 128, 256, and 512.", "labels": [], "entities": []}, {"text": "The details of the architectures used in the experiments are illustrated in.", "labels": [], "entities": []}, {"text": "Training was performed using the Adam optimisation algorithm) with default hyperparameters and a minibatch size of 50 captions.", "labels": [], "entities": []}, {"text": "The cost function used was sum crossentropy.", "labels": [], "entities": []}, {"text": "Training was carried outwith an early stopping criterion which terminated training as soon as performance on the validation data started to deteriorate (validation performance is measured after each training epoch).", "labels": [], "entities": []}, {"text": "Initialization of weights was done using Xavier initialization and biases were set to zero.", "labels": [], "entities": []}, {"text": "Each architecture was trained three separate times; the results reported below are averages over these three separate runs.", "labels": [], "entities": []}, {"text": "To evaluate the trained models we generated captions for images in the test set using beam search with abeam width of 3 and a clipped maximum length of 20 words.", "labels": [], "entities": []}, {"text": "The MSCOCO evaluation code 3 was used to measure the quality of the captions by using the standard evaluation metrics BLEU-(1,2,3,4) (), METEOR (Banerjee and), CIDEr (, and ROUGE-L ().", "labels": [], "entities": [{"text": "MSCOCO evaluation code 3", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7120427787303925}, {"text": "BLEU-(1,2,3,4)", "start_pos": 118, "end_pos": 132, "type": "METRIC", "confidence": 0.9454133361577988}, {"text": "METEOR", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9843127131462097}, {"text": "ROUGE-L", "start_pos": 173, "end_pos": 180, "type": "METRIC", "confidence": 0.9965993762016296}]}, {"text": "We also calculated the percentage of word types that were actually used in the generated captions out of the vocabulary of available word types.", "labels": [], "entities": []}, {"text": "This measure indicates how well each architecture exploits the vocabulary it is trained on.", "labels": [], "entities": []}, {"text": "The code used for the experiments was implemented with TensorFlow and is available online 4 . reports means and standard deviations over the three runs of all the MSCOCO measures and the vocabulary usage.", "labels": [], "entities": []}, {"text": "Since the point is to compare the effects of the architectures rather than to reach state-of-the-art performance, we do not include results from other published systems in our tables.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the captions generated using the inject and merge architectures. Values are means over  three separately retrained models, together with the standard deviation in parentheses. Legend: Layer -the  layer size used ('x' in", "labels": [], "entities": []}]}