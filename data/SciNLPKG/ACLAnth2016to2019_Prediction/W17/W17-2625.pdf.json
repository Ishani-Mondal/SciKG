{"title": [{"text": "Rethinking Skip-thought: A Neighborhood based Approach", "labels": [], "entities": [{"text": "Rethinking Skip-thought", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7658369839191437}]}], "abstractContent": [{"text": "We study the skip-thought model proposed by Kiros et al.", "labels": [], "entities": []}, {"text": "(2015) with neighborhood information as weak supervision.", "labels": [], "entities": []}, {"text": "More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood.", "labels": [], "entities": []}, {"text": "We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic related-ness, paraphrase detection, and classification benchmarks.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 171, "end_pos": 191, "type": "TASK", "confidence": 0.9108863770961761}]}, {"text": "Both quantitative comparison and qualitative investigation are conducted.", "labels": [], "entities": []}, {"text": "We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks.", "labels": [], "entities": []}, {"text": "In addition, we found that, incorporating an autoencoder path in our model didn't aid our model to perform better , while it hurts the performance of the skip-thought model.", "labels": [], "entities": []}], "introductionContent": [{"text": "We are interested in learning distributed sentence representation in an unsupervised fashion.", "labels": [], "entities": [{"text": "learning distributed sentence representation", "start_pos": 21, "end_pos": 65, "type": "TASK", "confidence": 0.60737644135952}]}, {"text": "Previously, the skip-thought model was introduced by, which learns to explore the semantic continuity within adjacent sentences as supervision for learning a generic sentence encoder.", "labels": [], "entities": []}, {"text": "The skip-thought model encodes the current sentence and then decodes the previous sentence and the next one, instead of itself.", "labels": [], "entities": []}, {"text": "Two independent decoders were applied, since intuitively, the previous sentence and the next sentence should be drawn from 2 different conditional distributions, respectively.", "labels": [], "entities": []}, {"text": "By posing a hypothesis that the adjacent sentences provide the same neighborhood information for learning sentence representation, we first drop one of the 2 decoders, and use only one decoder to reconstruct the surrounding 2 sentences at the same time.", "labels": [], "entities": [{"text": "learning sentence representation", "start_pos": 97, "end_pos": 129, "type": "TASK", "confidence": 0.6411982178688049}]}, {"text": "The empirical results show that our skip-thought neighbor model performs as well as the skip-thought model on 7 evaluation tasks.", "labels": [], "entities": []}, {"text": "Then, inspired by, as they tested the effect of incorporating an autoencoder branch in their proposed FastSent model, we also conduct experiments to explore reconstructing the input sentence itself as well in both our skip-thought neighbor model and the skip-thought model.", "labels": [], "entities": []}, {"text": "From the results, we can tell that our model didn't benefit from the autoencoder path, while reconstructing the input sentence hurts the performance of the skip-thought model.", "labels": [], "entities": []}, {"text": "Furthermore, we conduct an interesting experiment on only decoding the next sentence without the previous sentence, and it gave us the best results among all the models.", "labels": [], "entities": []}, {"text": "Model details will be discussed in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "The large corpus that we used for unsupervised training is the BookCorpus dataset (, which contains 74 million sentences from 7000 books in total.", "labels": [], "entities": [{"text": "BookCorpus dataset", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.9874567091464996}]}, {"text": "All of our experiments were conducted in Torch7 (Collobert et al., 2011).", "labels": [], "entities": [{"text": "Torch7", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.9646188616752625}]}, {"text": "To make the comparison fair, we reimplemented the skip-thought model under the same settings, according to, and the publicly available theano code . We adopted the multi-GPU train-ing scheme from the Facebook implementation of ResNet 2 . We use the ADAM () algorithm for optimization.", "labels": [], "entities": []}, {"text": "Instead of applying the gradient clipping according to the norm of the gradient, which was used in, we directly cutoff the gradient to make it within [\u22121, 1] for stable training.", "labels": [], "entities": []}, {"text": "The dimension of the word embedding and the sentence representation are 620 and 1200. respectively.", "labels": [], "entities": []}, {"text": "For the purpose of fast training, all the sentences were zero-padded or clipped to have the same length.", "labels": [], "entities": []}, {"text": "We compared our proposed skip-thought neighbor model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness, paraphrase detection, question-type classification and 4 benchmark sentiment and subjective datasets.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.7951490581035614}, {"text": "question-type classification", "start_pos": 162, "end_pos": 190, "type": "TASK", "confidence": 0.7731238305568695}]}, {"text": "After unsupervised training on the BookCorpus dataset, we fix the parameters in the encoder, and apply it as a sentence representation extractor on the 7 tasks.", "labels": [], "entities": [{"text": "BookCorpus dataset", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.9854520261287689}, {"text": "sentence representation extractor", "start_pos": 111, "end_pos": 144, "type": "TASK", "confidence": 0.6741746465365092}]}, {"text": "For semantic relatedness, we use the SICK dataset (), and we adopt the feature engineering idea proposed by.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.7846945822238922}]}, {"text": "For a given sentence pair, the encoder computes a pair of representations, denoted as u and v, and the concatenation of the component-wise product u \u00b7 v and the absolute difference |u \u2212 v| is regarded as the feature for the given sentence pair.", "labels": [], "entities": []}, {"text": "Then we train a logistic regression on top of the feature to predict the semantic relatedness score.", "labels": [], "entities": []}, {"text": "The evaluation metrics are Pearsons r, Spearmans \u03c1, and mean squared error M SE.", "labels": [], "entities": [{"text": "Pearsons r", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9815230369567871}, {"text": "Spearmans \u03c1", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9822140038013458}, {"text": "mean squared error M SE", "start_pos": 56, "end_pos": 79, "type": "METRIC", "confidence": 0.9561755895614624}]}, {"text": "The dataset we use for the paraphrase detection is the Microsoft Paraphrase Detection Corpus ().", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.9242243468761444}, {"text": "Microsoft Paraphrase Detection Corpus", "start_pos": 55, "end_pos": 92, "type": "DATASET", "confidence": 0.7629281729459763}]}, {"text": "We follow the same feature engineering idea from to compute a single feature for each sentence pair.", "labels": [], "entities": []}, {"text": "Then we train a logistic regression, and 10-fold cross validation is applied to find the optimal hyperparameter settings.", "labels": [], "entities": []}, {"text": "The 5 classification tasks are question-type classification (TREC) (), movie review sentiment (MR)), customer product reviews (CR) (), subjectivity/objectivity classification (SUBJ) (, and opinion polarity (MPQA) (.", "labels": [], "entities": [{"text": "question-type classification (TREC)", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.7599577128887176}, {"text": "movie review sentiment (MR))", "start_pos": 71, "end_pos": 99, "type": "METRIC", "confidence": 0.5935715585947037}, {"text": "customer product reviews (CR)", "start_pos": 101, "end_pos": 130, "type": "TASK", "confidence": 0.5803154160579046}, {"text": "subjectivity/objectivity classification (SUBJ", "start_pos": 135, "end_pos": 180, "type": "TASK", "confidence": 0.6950127085049947}]}, {"text": "In order to deal with more words besides the words used for training, the same word expansion method, which was introduced by, is applied after training on the BookCorpus dataset.", "labels": [], "entities": [{"text": "BookCorpus dataset", "start_pos": 160, "end_pos": 178, "type": "DATASET", "confidence": 0.9854947030544281}]}, {"text": "The results are shown in, where the model name is given by encoder type -model type -model size.", "labels": [], "entities": []}, {"text": "We tried with three different types of the encoder, denoted as uni-, bi-, and combinein.", "labels": [], "entities": []}, {"text": "The first one is a uni-directional GRU, which computes a 1200-dimension vector as the sentence representation.", "labels": [], "entities": []}, {"text": "The second one is a bi-directional GRU, which computes a 600-dimension vector for each direction, and then the two vectors are concatenated to serve as the sentence representation.", "labels": [], "entities": []}, {"text": "Third, after training the unidirectional model and the bi-directional model, the representation from both models are concatenated together to represent the sentence, denoted as combine-.", "labels": [], "entities": []}, {"text": "In, -N-refers to our skip-thought neighbor model, -N-next-refers to our skip-thought neighbor with only predicting the next sentence, and -skip-refers to the original skip-thought model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The model name is given by encoder type -model type -model size with or without autoencoder  branch +AE. Bold numbers indicate the best results among all models. Without the autoencoder branch,  our skip-thought neighbor models perform as well as the skip-thought models, and our \"next\" models  slightly outperform the skip-thought models. However, with the autoencoder branch, our skip-thought  neighbor models outperform the skip-thought models.", "labels": [], "entities": [{"text": "AE", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.838261604309082}]}, {"text": " Table 2: Evaluation on SICK dataset without nor- malizing the representation. Bold numbers indi- cate the best values among all models.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.7266774475574493}]}]}