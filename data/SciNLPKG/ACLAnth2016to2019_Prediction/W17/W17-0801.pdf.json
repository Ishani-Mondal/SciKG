{"title": [{"text": "Readers vs. Writers vs. Texts: Coping with Different Perspectives of Text Understanding in Emotion Annotation", "labels": [], "entities": [{"text": "Emotion Annotation", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.6690884530544281}]}], "abstractContent": [{"text": "We here examine how different perspectives of understanding written discourse, like the reader's, the writer's or the text's point of view, affect the quality of emotion annotations.", "labels": [], "entities": []}, {"text": "We conducted a series of annotation experiments on two corpora, a popular movie review corpus and a genre-and domain-balanced corpus of standard English.", "labels": [], "entities": []}, {"text": "We found statistical evidence that the writer's perspective yields superior annotation quality overall.", "labels": [], "entities": []}, {"text": "However, the quality one perspective yields compared to the other(s) seems to depend on the domain the utterance originates from.", "labels": [], "entities": []}, {"text": "Our data further suggest that the popular movie review data set suffers from an atypical bimodal distribution which may decrease model performance when used as a training resource.", "labels": [], "entities": [{"text": "movie review data set", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.7126870006322861}]}], "introductionContent": [{"text": "In the past years, the analysis of subjective language has become one of the most popular areas in computational linguistics.", "labels": [], "entities": [{"text": "analysis of subjective language", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.8333772420883179}, {"text": "computational linguistics", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.7565347254276276}]}, {"text": "In the early days, a simple classification according to the semantic polarity (positiveness, negativeness or neutralness) of a document was predominant, whereas in the meantime, research activities have shifted towards a more sophisticated modeling of sentiments.", "labels": [], "entities": []}, {"text": "This includes the extension from only few basic to more varied emotional classes sometimes even assigning real-valued scores, the aggregation of multiple aspects of an opinion item into a composite opinion statement for the whole item (, and sentiment compositionality on sentence level.", "labels": [], "entities": [{"text": "sentiment compositionality", "start_pos": 242, "end_pos": 268, "type": "TASK", "confidence": 0.7578473687171936}]}, {"text": "There is also an increasing awareness of different perspectives one may take to interpret written discourse in the process of text comprehension.", "labels": [], "entities": []}, {"text": "A typical distinction which mirrors different points of view is the one between the writer and the reader(s) of a document as exemplified by utterance (1) below (taken from): (1) Italy defeats France in World Cup Final The emotion of the writer, presumably a professional journalist, can be expected to be more or less neutral, but French or Italian readers may show rather strong (and most likely opposing) emotional reactions when reading this news headline.", "labels": [], "entities": []}, {"text": "Consequently, such finer-grained emotional distinctions must also be considered when formulating instructions for an annotation task.", "labels": [], "entities": []}, {"text": "NLP researchers are aware of this multiperspectival understanding of emotion as contributions often target either one or the other form of emotion expression or mention it as a subject of future work.", "labels": [], "entities": []}, {"text": "However, contributions aiming at quantifying the effect of altering perspectives are rare (see Section 2).", "labels": [], "entities": []}, {"text": "This is especially true for work examining differences in annotation results relative to these perspectives.", "labels": [], "entities": []}, {"text": "Although this is obviously a crucial design decision for gold standards for emotion analytics, we know of only one such contribution.", "labels": [], "entities": [{"text": "emotion analytics", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7642533481121063}]}, {"text": "In this paper, we systematically examine differences in the quality of emotion annotation regarding different understanding perspectives.", "labels": [], "entities": []}, {"text": "Apart from inter-annotator agreement (IAA), we will also look at other quality criteria such as how well the resulting annotations cover the space of possible ratings and check for the representativeness of the rating distribution.", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 11, "end_pos": 42, "type": "METRIC", "confidence": 0.7924436211585999}]}, {"text": "We performed a series of annotation experiments with varying instruc-1 tions and domains of raw text, making this the first study ever to address the impact of text understanding perspective on sentence-level emotion annotation.", "labels": [], "entities": [{"text": "sentence-level emotion annotation", "start_pos": 194, "end_pos": 227, "type": "TASK", "confidence": 0.629971573750178}]}, {"text": "The results we achieved directly influenced the design and creation of EMOBANK, a novel large-scale gold standard for emotion analysis employing the VAD model for affect representation ().", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7479377388954163}, {"text": "affect representation", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.7274246513843536}]}], "datasetContent": [{"text": "Considering Example (1) and our literature review from Section 2, it is obvious that at least the perspective of the writer and the reader of an utterance must be distinguished.", "labels": [], "entities": [{"text": "Example", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.8258950710296631}]}, {"text": "Accordingly, writer emotion refers to how someone feels while producing an utterance, whereas reader emotion relates to how someone feels right after reading or hearing this utterance.", "labels": [], "entities": []}, {"text": "Also taking into account the finding by Mohammad and Turney (2013) that agreement among annotators is higher when asking whether a word is associated with an emotion rather than asking whether it evokes this emotion, we propose to extend the common writer-reader framework by a third category, the text perspective, where no actual person is specified as perceiving an emotion.", "labels": [], "entities": []}, {"text": "Rather, we assume for this perspective that emotion is an intrinsic property of a sentence (or an alternative linguistic unit like a phrase or the entire text).", "labels": [], "entities": []}, {"text": "In the following, we will use the terms WRITER, TEXT and READER to concisely refer to the respective perspectives.", "labels": [], "entities": [{"text": "WRITER", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9658714532852173}, {"text": "TEXT", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9943722486495972}, {"text": "READER", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9947530031204224}]}, {"text": "We collected two data sets, a movie review data set highly popular in sentiment analysis and a balanced corpus of general English.", "labels": [], "entities": [{"text": "movie review data set", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.681097149848938}, {"text": "sentiment analysis", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.9696834981441498}]}, {"text": "In this way, we can estimate the annotation quality resulting from different perspectives, also covering interactions regarding different domains.", "labels": [], "entities": []}, {"text": "The first data set builds upon the corpus originally introduced by.", "labels": [], "entities": []}, {"text": "It consists of about 10k snippets from movie reviews by professional critics collected from the website rottentomatoes.com.", "labels": [], "entities": []}, {"text": "The data was further enriched by who annotated individual nodes in the constituency parse trees according to a 5-point polarity scale, forming the Stanford Sentiment Treebank (SST) which contains 11,855 sentences.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank (SST)", "start_pos": 147, "end_pos": 180, "type": "DATASET", "confidence": 0.8709925413131714}]}, {"text": "Upon closer inspection, we noticed that the SST data have some encoding issues (e.g., Absorbing character study by Andr\u02dcAAndr\u02dc Andr\u02dcA c Turpin .) that are not present in the original Rotten Tomatoes data set.", "labels": [], "entities": [{"text": "SST data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9213050305843353}, {"text": "Absorbing", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9803832173347473}, {"text": "Rotten Tomatoes data set", "start_pos": 183, "end_pos": 207, "type": "DATASET", "confidence": 0.9490561783313751}]}, {"text": "So we decided to replicate the creation of the SST data from the original snippets.", "labels": [], "entities": [{"text": "SST", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.929613471031189}]}, {"text": "Furthermore, we filtered out fragmentary sentences automatically (e.g., beginning with comma, dashes, lowercase, etc.) as well as manually excluded grammatically incomplete and therefore incomprehensible sentences, e.g., \"Or a profit\" or \"Over age 15?\".", "labels": [], "entities": []}, {"text": "Subsequently, a total of 10,987 sentences could be mapped back to SST IDs forming the basis for our experiments (the SST* collection).", "labels": [], "entities": [{"text": "SST* collection", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.7116483549276987}]}, {"text": "To complement our review language data set, a domain heavily focused on in sentiment analysis (, for our second data set, we decided to rely on a genre-balanced corpus.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9096025824546814}]}, {"text": "We chose the Manually Annotated Sub-Corpus (MASC) of the American National Corpus which is already annotated for various linguistic levels (.", "labels": [], "entities": [{"text": "American National Corpus", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.9210335413614908}]}, {"text": "We excluded registers containing spoken, mainly dialogic or non-standard language, e.g., telephone conversations, movie scripts and tweets.", "labels": [], "entities": []}, {"text": "To further enrich this collection of raw data for potential emotion analysis applications, we additionally included the corpus of the SEM-EVAL-2007 Task 14 focusing on Affective Text (SE07;), one of the most important data sets in emotion analysis.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.6976166665554047}, {"text": "SEM-EVAL-2007 Task 14", "start_pos": 134, "end_pos": 155, "type": "TASK", "confidence": 0.6768367290496826}, {"text": "emotion analysis", "start_pos": 231, "end_pos": 247, "type": "TASK", "confidence": 0.7940373122692108}]}, {"text": "This data set already bears annotations according to Ekman's six Basic Emotions (see Section 2) so that the gold standard we ultimately supply already contains a bi-representational part (being annotated according to a dimensional and a categorical model of emotion).", "labels": [], "entities": []}, {"text": "Such a double encoding will easily allow for research on automatically mapping between different emotion formats (.", "labels": [], "entities": []}, {"text": "In order to identify individual sentence in MASC, we relied on the already available annotations.", "labels": [], "entities": []}, {"text": "We noticed, however, that a considerable portion of the sentence boundary annotations were duplicates which we consequently removed (about 5% of the preselected data).", "labels": [], "entities": []}, {"text": "This left us with a total of 18,290 sentences from MASC and 1,250 headlines from SE07.", "labels": [], "entities": [{"text": "MASC", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8909250497817993}, {"text": "headlines", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9509772062301636}, {"text": "SE07", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9255959987640381}]}, {"text": "Together, they form our second data set, MASC*.", "labels": [], "entities": [{"text": "MASC", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.853297233581543}]}], "tableCaptions": [{"text": " Table 1: IAA values obtained on the SST* and the  MASC* data set. r, MAE and RMSE refer to the  respective leave-one-out metric (see Section 3).", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9524881839752197}, {"text": "MASC* data set", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9292638748884201}, {"text": "MAE", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9857262372970581}, {"text": "RMSE", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9842647314071655}]}, {"text": " Table 2: Emotionality results for the SST* and the  MASC* data set.", "labels": [], "entities": [{"text": "SST", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9337781071662903}, {"text": "MASC* data set", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.8806112706661224}]}]}