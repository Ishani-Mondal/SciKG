{"title": [{"text": "Recognizing Textual Entailment in Twitter Using Word Embeddings", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.908822774887085}]}], "abstractContent": [{"text": "In this paper, we investigate the application of machine learning techniques and word embeddings to the task of Recognizing Textual Entailment (RTE) in Social Media.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 112, "end_pos": 148, "type": "TASK", "confidence": 0.8591788113117218}]}, {"text": "We look at a manually labeled dataset (Lendvai et al., 2016) consisting of user generated short texts posted on Twit-ter (tweets) and related to four recent media events (the Charlie Hebdo shooting, the Ottawa shooting, the Sydney Siege, and the German Wings crash) and test to what extent neural techniques and em-beddings are able to distinguish between tweets that entail or contradict each other or that claim unrelated things.", "labels": [], "entities": []}, {"text": "We obtain comparable results to the state of the art in a train-test setting, but we show that, due to the noisy aspect of the data, results plummet in an evaluation strategy crafted to better simulate a real-life train-test scenario .", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to automatically deduce how the meaning of text flows from one sentence to the next is a central part of Natural Language Understanding (NLU) and highly important in many Natural Language Processing tasks (NLP).", "labels": [], "entities": [{"text": "Natural Language Understanding (NLU)", "start_pos": 117, "end_pos": 153, "type": "TASK", "confidence": 0.7680780092875162}, {"text": "Natural Language Processing tasks (NLP)", "start_pos": 183, "end_pos": 222, "type": "TASK", "confidence": 0.6959479025432042}]}, {"text": "Recognizing Textual Entailment (RTE), started as a challenge in 2004 from this very need and reaching its 8th iteration in 2013 at SemEval 1 , falls at the intersection between NLU, NLP, Information Extraction, and Information Retrieval ().", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8436244130134583}, {"text": "Information Extraction", "start_pos": 187, "end_pos": 209, "type": "TASK", "confidence": 0.6977555602788925}, {"text": "Information Retrieval", "start_pos": 215, "end_pos": 236, "type": "TASK", "confidence": 0.7629686594009399}]}, {"text": "Its goal is, given a pair of sentences dubbed text and hypothesis, to determine whether the meaning of either (traditionally, of the hypothesis) entails the meaning of the other, contradicts it, or whether nothing can be said of the relationship between the two sentences.", "labels": [], "entities": []}, {"text": "Here, the notion of entailment and contradiction are not necessarily related to the linguistic notions, where entailment is always explained in contrast with presupposition and sometimes implicature, but are outlined in).", "labels": [], "entities": []}, {"text": "Interest in this task was amplified with the creation of the SNLI corpus () which lead to a few studies using Deep Neural Networks (DNN) ().", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.7425456345081329}]}, {"text": "Previous to this, the Excitement Open Platform 2 was considered the state-of-the-art model.", "labels": [], "entities": []}, {"text": "On the hand, interest in ways to represent text in order to improve performance in text classification (, machine translation ( or question answering tasks () has been rekindled with the introduction of the highly cited word2vec model ( and the avenue of deep neural word embeddings (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.740886464715004}, {"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7227964699268341}, {"text": "question answering tasks", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.7529113789399465}]}, {"text": "Our present research revolved around three questions: \u2022 Can we apply state of the art neural methods created for large datasets or longer texts to small datasets containing very short texts?", "labels": [], "entities": []}, {"text": "\u2022 Will these methods work for fine grained contradictions?", "labels": [], "entities": []}, {"text": "\u2022 Can word embeddings, which were successfully used for word-level semantic tasks, improve performance in tasks pertaining to discourse level semantics?", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Event-based CV results using cosine sim- ilarity and word mover distance on the minority  class and averaged", "labels": [], "entities": []}, {"text": " Table 2: Event Based CV results for 900 dimen- sional vectors", "labels": [], "entities": []}, {"text": " Table 3: Train-Test Split results for LSTM and Lo- gistic Regression", "labels": [], "entities": [{"text": "LSTM", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.5762987732887268}]}]}