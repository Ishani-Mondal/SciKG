{"title": [{"text": "Deep Active Learning for Named Entity Recognition", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.72881551583608}]}], "abstractContent": [{"text": "Deep neural networks have advanced the state of the art in named entity recognition.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7063605586687723}]}, {"text": "However, under typical training procedures , advantages over classical methods emerge only with large datasets.", "labels": [], "entities": []}, {"text": "As a result, deep learning is employed only when large public datasets or a large budget for manually labeling data is available.", "labels": [], "entities": []}, {"text": "In this work, we show that by combining deep learning with active learning, we can outperform classical methods even with a significantly smaller amount of training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past several years, a series of papers have used deep neural networks (DNNs) to advance the state of the art in named entity recognition (NER).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 121, "end_pos": 151, "type": "TASK", "confidence": 0.793192595243454}]}, {"text": "Historically, the advantages of deep learning have been less pronounced when working with small datasets.", "labels": [], "entities": []}, {"text": "For instance, on the popular CoNLL-2003 English dataset, the best DNN model outperforms the best shallow model by only 0.4%, as measured by F1 score, and this is a small dataset containing only 203,621 words.", "labels": [], "entities": [{"text": "CoNLL-2003 English dataset", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.9431560238202413}, {"text": "F1 score", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9847157299518585}]}, {"text": "On the other hand, on the OntoNotes-5.0 English dataset, which contains 1,088,503 words, a DNN model outperforms the best shallow model by 2.24% (.", "labels": [], "entities": [{"text": "OntoNotes-5.0 English dataset", "start_pos": 26, "end_pos": 55, "type": "DATASET", "confidence": 0.901687741279602}]}, {"text": "In this work, we investigate whether we can train DNNs using fewer samples under the active learning framework.", "labels": [], "entities": []}, {"text": "Active learning is the paradigm where we actively select samples to be used during training.", "labels": [], "entities": []}, {"text": "Intuitively, if we are able to select the most informative samples for training, we can vastly reduce the number of samples required.", "labels": [], "entities": []}, {"text": "In practice, we can employ Mechanical Turk or other crowdsourcing platforms to label the samples actively selected by the algorithm.", "labels": [], "entities": []}, {"text": "Reducing sample requirements for training can lower the labeling costs on these platforms.", "labels": [], "entities": []}, {"text": "We present positive preliminary results demonstrating the effectiveness of deep active learning.", "labels": [], "entities": []}, {"text": "We perform incremental training of DNNs while actively selecting samples.", "labels": [], "entities": []}, {"text": "On the standard OntoNotes-5.0 English dataset, our approach matches 99% of the F1 score achieved by the best deep models trained in a standard, supervised fashion despite using only a quarter", "labels": [], "entities": [{"text": "OntoNotes-5.0 English dataset", "start_pos": 16, "end_pos": 45, "type": "DATASET", "confidence": 0.8861508766810099}, {"text": "F1 score achieved", "start_pos": 79, "end_pos": 96, "type": "METRIC", "confidence": 0.9672883947690328}]}], "datasetContent": [{"text": "We use OntoNotes-5.0 English and Chinese data ( for our experiments.", "labels": [], "entities": [{"text": "OntoNotes-5.0 English and Chinese data", "start_pos": 7, "end_pos": 45, "type": "DATASET", "confidence": 0.7350776791572571}]}, {"text": "The training datasets contain 1,088,503 words and 756,063 words respectively.", "labels": [], "entities": []}, {"text": "State-of-the-art models trained the full training sets achieve F1 scores of 86.86 and 75.63 on the test sets.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.977952241897583}]}, {"text": "Comparisons of selection algorithms We empirically compare selection algorithms proposed in Section 3, as well as uniformly random baseline (RAND).", "labels": [], "entities": []}, {"text": "All algorithms start with an identical 1% of original training data and a randomly initialized model.", "labels": [], "entities": []}, {"text": "In each round, every algorithm chooses sentences from the rest of the training data until 20,000 words have been selected, adding this data to its training set.", "labels": [], "entities": []}, {"text": "Then, the algorithm updates its model parameters by stochastic gradient descent on its augmented training dataset for 50 passes.", "labels": [], "entities": []}, {"text": "We evaluate the performance of each algorithm by its F1 score on the test dataset.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9817459285259247}]}, {"text": "All active learning algorithms perform significantly better than the random baseline.", "labels": [], "entities": []}, {"text": "Among active learners, MNLP slightly outperformed others in early rounds.", "labels": [], "entities": [{"text": "MNLP", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.5468831062316895}]}, {"text": "Impressively, active learning algorithms achieve 99% performance of the best deep model trained on full data using only 24.9% of the training data on the English dataset and 30.1% on Chinese.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 154, "end_pos": 169, "type": "DATASET", "confidence": 0.7723641097545624}]}, {"text": "Also, 12.0% and 16.9% of training data were enough for deep active learning algorithms to surpass the performance of the shallow models from trained on the full training data.", "labels": [], "entities": []}, {"text": "Detection of under-explored genres To better understand how active learning algorithms choose informative examples, we designed the following experiment.", "labels": [], "entities": []}, {"text": "The OntoNotes datasets consist of six genres: broadcast conversation (bc), braodcast news (bn), magazine genre (mz), newswire (nw), telephone conversation (tc), weblogs (wb).", "labels": [], "entities": [{"text": "OntoNotes datasets", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9117515385150909}]}, {"text": "We created three training datasets: half-data, which contains random 50% of the original training data, nw-data, which contains sentences only from newswire (51.5% of words in the original data), and no-nw-data, which is the complement of nw-data.", "labels": [], "entities": []}, {"text": "Then, we trained CNN-CNN-LSTM model on each dataset.", "labels": [], "entities": []}, {"text": "The model trained on half-data achieved 85.10 F1, significantly outperforming others trained on biased datasets (no-nwdata: 81.49, nw-only-data: 82.08).", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.990011990070343}]}, {"text": "This showed the importance of good genre coverage in training data.", "labels": [], "entities": []}, {"text": "Then, we analyzed the genre distribution of 1,000 sentences MNLP chose for each model (see).", "labels": [], "entities": []}, {"text": "For no-nw-data, the algorithm chose many more newswire (nw) sentences than it did for unbiased half-data.", "labels": [], "entities": []}, {"text": "On the other hand, it undersampled newswire sentences for nwonly-data and increased the proportion of broadcast news and telephone conversation, which are genres distant from newswire.", "labels": [], "entities": []}, {"text": "Impressively, although we did not provide the genre of sentences to the algorithm, it was able to automatically detect underexplored genres.", "labels": [], "entities": []}], "tableCaptions": []}