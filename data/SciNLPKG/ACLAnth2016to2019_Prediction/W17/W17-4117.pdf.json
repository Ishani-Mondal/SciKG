{"title": [{"text": "What do we need to know about an unknown word when parsing German", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose anew type of subword embedding designed to provide more information about unknown compounds, a major source for OOV words in German.", "labels": [], "entities": []}, {"text": "We present an extrinsic evaluation where we use the compound embeddings as input to a neural dependency parser and compare the results to the ones obtained with other types of embeddings.", "labels": [], "entities": []}, {"text": "Our evaluation shows that adding compound embeddings yields a significant improvement of 2% LAS over using word embeddings when no POS information is available.", "labels": [], "entities": [{"text": "LAS", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9940915703773499}]}, {"text": "When adding POS embeddings to the input, however , the effect levels out.", "labels": [], "entities": []}, {"text": "This suggests that it is not the missing information about the semantics of the unknown words that causes problems for parsing German, but the lack of morphological information for unknown words.", "labels": [], "entities": [{"text": "parsing German", "start_pos": 119, "end_pos": 133, "type": "TASK", "confidence": 0.8697545826435089}]}, {"text": "To augment our evaluation , we also test the new embeddings in a language modelling task that requires both syntactic and semantic information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parsing morphologically rich languages (MRLs) is a challenging task.", "labels": [], "entities": [{"text": "Parsing morphologically rich languages (MRLs)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.9098149963787624}]}, {"text": "One of the main problems is the high proportion of unknown words in the data, due to the high number of different inflected forms.", "labels": [], "entities": []}, {"text": "In some languages, this problem is made even worse by compounding, a highly productive word formation process.", "labels": [], "entities": [{"text": "word formation", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7340848743915558}]}, {"text": "Thus, handling unknown words is crucial for parsing MRLs and especially for German where compounding is a frequent phenomenon.", "labels": [], "entities": [{"text": "parsing MRLs", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9213561713695526}]}, {"text": "While word embeddings area promising way to learn a general representation that captures syntactic and semantic properties of a word, they have not fully solved the sparse data problem.", "labels": [], "entities": []}, {"text": "Recent studies are exploring representations at the subword level that can provide information even for rare and unseen words.", "labels": [], "entities": []}, {"text": "Well-known examples are character and character-ngram-based embeddings (, morphological embeddings (, or byte embeddings (. were the first to integrate character-based embeddings into a syntactic parser and compared the effect for different languages with different levels of morphological richness.", "labels": [], "entities": []}, {"text": "They showed that replacing word embeddings with character-based embeddings can be useful, especially for parsing agglutinative languages.", "labels": [], "entities": [{"text": "parsing agglutinative languages", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.9011305371920267}]}, {"text": "Since then, character-based embeddings have become an ingredient in many parsing systems.", "labels": [], "entities": []}, {"text": "Other work has addressed the compounding problem on the level of word embeddings.", "labels": [], "entities": []}, {"text": "Dima et al. have tried to model compound compositionality for English ( and German.", "labels": [], "entities": []}, {"text": "However, experiments were on the semantic level, and the compounds were restricted to two components only.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, nobody has tried compound embeddings to tackle the unknown word problem in statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8175117373466492}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The percentage of unknown words in the  test data set with respect to different levels of cut- off threshold in the training data. Threshold of 1  means no words in the training data are discarded.", "labels": [], "entities": [{"text": "Threshold", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9995180368423462}]}, {"text": " Table 2: Hyperparameters used in all experiments", "labels": [], "entities": []}, {"text": " Table 3: Results for different input combinations", "labels": [], "entities": []}, {"text": " Table 4: Precision (P) and recall (R) for core gram- matical functions with/without character-based  embeddings. SB: subj, OA: accusative obj, DA:  dative obj, AG: genitive attribute, OG: genitive  obj, PD: predicate.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9289018213748932}, {"text": "recall (R)", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9500174820423126}]}]}