{"title": [{"text": "Target-side Word Segmentation Strategies for Neural Machine Translation", "labels": [], "entities": [{"text": "Target-side Word Segmentation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5632984240849813}, {"text": "Neural Machine Translation", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.7373317281405131}]}], "abstractContent": [{"text": "For efficiency considerations, state-of-the-art neural machine translation (NMT) requires the vocabulary to be restricted to a limited-size set of several thousand symbols.", "labels": [], "entities": [{"text": "state-of-the-art neural machine translation (NMT)", "start_pos": 31, "end_pos": 80, "type": "TASK", "confidence": 0.8396870493888855}]}, {"text": "This is highly problematic when translating into inflected or compounding languages.", "labels": [], "entities": []}, {"text": "A typical remedy is the use of subword units, where words are segmented into smaller components.", "labels": [], "entities": []}, {"text": "Byte pair encoding, a purely corpus-based approach , has proved effective recently.", "labels": [], "entities": [{"text": "Byte pair encoding", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8803086678187052}]}, {"text": "In this paper, we investigate word segmen-tation strategies that incorporate more linguistic knowledge.", "labels": [], "entities": []}, {"text": "We demonstrate that linguistically informed target word seg-mentation is better suited for NMT, leading to improved translation quality on the order of magnitude of +0.5 BLEU and \u22120.9 TER fora medium-scale English\u2192German translation task.", "labels": [], "entities": [{"text": "NMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.7980957627296448}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9986274242401123}, {"text": "TER", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9913538098335266}, {"text": "medium-scale English\u2192German translation task", "start_pos": 193, "end_pos": 237, "type": "TASK", "confidence": 0.6870405028263727}]}, {"text": "Our work is important in that it shows that linguistic knowledge can be used to improve NMT results over results based only on the language-agnostic byte pair encoding vocabulary reduction technique.", "labels": [], "entities": [{"text": "NMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9647844433784485}, {"text": "language-agnostic byte pair encoding vocabulary reduction", "start_pos": 131, "end_pos": 188, "type": "TASK", "confidence": 0.6010786642630895}]}], "introductionContent": [{"text": "Inflection and nominal composition are morphological processes which exist in many natural languages.", "labels": [], "entities": [{"text": "nominal composition", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7112656682729721}]}, {"text": "Machine translation into an inflected language or into a compounding language must be capable of generating words from a large vocabulary of valid word surface forms, or ideally even be open-vocabulary.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7271417379379272}]}, {"text": "In NMT, though, dealing with a very large number of target symbols is expensive in practice.", "labels": [], "entities": []}, {"text": "While, for instance, a standard dictionary of German, a compounding language, may cover 140 000 vocabulary entries, NMT on off-theshelf GPU hardware is nowadays typically only tractable with target vocabularies below 100 000 symbols.", "labels": [], "entities": []}, {"text": "This issue is made worse by the fact that compound words are not a closed set.", "labels": [], "entities": []}, {"text": "More frequently occurring compound words maybe covered in a standard dictionary (e.g., \"Finanztransaktionssteuer\", English: \"financial transaction tax\"), but the compounding process allows for words to be freely joined to form new ones (e.g., \"Finanztransaktionssteuerzahler\", English: \"financial transaction tax payer\"), and compounding is highly productive in a language like German.", "labels": [], "entities": []}, {"text": "Furthermore, a dictionary lists canonical word forms, many of which can have many inflected variants, with morphological variation depending on case, number, gender, tense, aspect, mood, and soon.", "labels": [], "entities": []}, {"text": "The German language has four cases, three grammatical genders, and two numbers.", "labels": [], "entities": []}, {"text": "German exhibits a rich amount of morphological word variations also in the verbal system.", "labels": [], "entities": []}, {"text": "A machine translation system should ideally be able to produce any permissible compound word, and all inflections for each canonical form of all words (including compound words).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.7118876576423645}]}, {"text": "Previous work has drawn on byte pair encoding to obtain a fixed-sized vocabulary of subword units.", "labels": [], "entities": [{"text": "byte pair encoding", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6511693000793457}]}, {"text": "In this paper, we investigate word segmentation strategies for NMT which are linguistically more informed.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7341378778219223}]}, {"text": "Specifically, we explore and empirically compare: \u2022 Compound splitting.", "labels": [], "entities": [{"text": "Compound splitting", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7528331577777863}]}, {"text": "\u2022 Byte pair encoding (BPE).", "labels": [], "entities": [{"text": "Byte pair encoding (BPE)", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.6368027875820795}]}, {"text": "\u2022 Cascaded applications of the above.", "labels": [], "entities": []}, {"text": "Our empirical evaluation focuses on target-language side segmentation, with English\u2192German translation as the application task.", "labels": [], "entities": [{"text": "target-language side segmentation", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.6637269755204519}]}, {"text": "Our proposed approaches improve machine translation quality by up to +0.5 BLEU and \u22120.9 TER, respectively, compared with using plain BPE.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7460627853870392}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9993407130241394}, {"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9913763403892517}]}, {"text": "Advantages of linguistically-informed target word segmentation in NMT are: 1.", "labels": [], "entities": [{"text": "linguistically-informed target word segmentation", "start_pos": 14, "end_pos": 62, "type": "TASK", "confidence": 0.6447505578398705}]}, {"text": "Better vocabulary reduction for practical tractability of NMT, as motivated above.", "labels": [], "entities": [{"text": "vocabulary reduction", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.6170705556869507}]}, {"text": "2. Reduction of data sparsity.", "labels": [], "entities": []}, {"text": "Learning lexical choice is more difficult for rare words that appear in few training samples (e.g., rare compounds), or when a single form from a source language with little inflection (such as English) has many target-side translation options which are morphological variants.", "labels": [], "entities": []}, {"text": "Splitting compounds and separating affixes from stems can ease lexical selection.", "labels": [], "entities": []}, {"text": "3. Better open vocabulary translation.", "labels": [], "entities": [{"text": "open vocabulary translation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7086752851804098}]}, {"text": "With target-side word segmentation, the NMT system can generate sequences of word pieces attest time that have not been seen in this combination in training.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7148838192224503}]}, {"text": "It may produce new compounds, or valid morphological variants that were not present in the training corpus, e.g. by piecing together a stem with an inflectional suffix in anew, but linguistically admissible way.", "labels": [], "entities": []}, {"text": "Using a linguistically informed segmentation should better allow the system to try to learn the linguistic processes of word formation.", "labels": [], "entities": [{"text": "word formation", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.7140747755765915}]}], "datasetContent": [{"text": "We conduct an empirical evaluation using encoder-decoder NMT with attention and gated recurrent units as implemented in Nematus (.", "labels": [], "entities": []}, {"text": "We train and test on English-German Europarl data ().", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8617005050182343}]}, {"text": "The data is tokenized and frequent-cased using scripts from the Moses toolkit ( . Sentences with length >50 after tokenization are excluded from the training corpus, all other sentences (1.7 M) are considered in training under every word segmentation scheme.", "labels": [], "entities": []}, {"text": "We set the amount of merge operations for BPE to 50K.", "labels": [], "entities": [{"text": "BPE", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.761954665184021}]}, {"text": "Corpus statistics of the German data after different preprocessings are given in.", "labels": [], "entities": [{"text": "German data", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.8622079491615295}, {"text": "preprocessings", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.9470843076705933}]}, {"text": "On the English source side, we apply BPE separately, also with 50K merge operations.", "labels": [], "entities": [{"text": "BPE", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8213152885437012}]}, {"text": "For comparison, we build a setup denoted as top 50K voc.", "labels": [], "entities": []}, {"text": "(source & target) where we train on the tokenized corpus without any segmentation, limiting the vocabulary to the 50K most frequent words on each side and replacing rare words by \"UNK\".", "labels": [], "entities": []}, {"text": "Ina setup denoted as suffix + prefix + compound, 50K, we furthermore examine whether BPE can be omitted in a cascaded application of target word segmenters.", "labels": [], "entities": [{"text": "BPE", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9073513150215149}]}, {"text": "Here, we use the top 50K target symbols after suffix, prefix, and compound splitting, but still apply BPE to the English source.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7070886045694351}, {"text": "BPE", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.96510910987854}]}, {"text": "It is important to note that the amount of distinct target symbols in the setups ranges between 43K-46K; 50K for top-50K-voc systems.", "labels": [], "entities": []}, {"text": "There are no massive vocabulary size differences.", "labels": [], "entities": []}, {"text": "We always apply 50K BPE operations.", "labels": [], "entities": [{"text": "BPE", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.784174382686615}]}, {"text": "Minor divergences in the number of types naturally occur amongst the various cascaded segmentations.", "labels": [], "entities": []}, {"text": "The linguistically-informed splitters segment more, resulting in more tokens.", "labels": [], "entities": []}, {"text": "We chose BPE-50K because the vocabulary is reasonably large while training fits onto GPUs with 8 GB of RAM.", "labels": [], "entities": [{"text": "BPE-50K", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.649860680103302}]}, {"text": "Larger vocabularies come at the cost of either more RAM or adjustment of other parameters (e.g., batch size or sentence length limit).", "labels": [], "entities": [{"text": "RAM", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9936782717704773}]}, {"text": "From hyperparameter search over reduced vocabulary sizes we would not expect important insights, so we do not do this.", "labels": [], "entities": []}, {"text": "In all setups the training samples are always the same.", "labels": [], "entities": []}, {"text": "We removed long sentences after tokenization but before segmentation, which affects all setups equally.", "labels": [], "entities": []}, {"text": "No sentences are discarded after that stage (Nematus' maxlen > longest sequence in data).", "labels": [], "entities": []}, {"text": "We configure dimensions of 500 for the embeddings and 1024 for the hidden layer.", "labels": [], "entities": []}, {"text": "We train with the Adam optimizer (), a learning rate of 0.0001, batch size of 50, and dropout with probability 0.2 applied to the hidden layer.", "labels": [], "entities": []}, {"text": "We validate on the test2006 set after every 10 000 updates and do early stopping when the validation cost has not decreased for ten epochs.", "labels": [], "entities": [{"text": "test2006 set", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.8562151193618774}, {"text": "early stopping", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.9376104474067688}]}, {"text": "We evaluate case-sensitive with BLEU () and TER (), computed over postprocessed hypotheses against the raw references with mteval-v13a and tercom.7.25, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9991288781166077}, {"text": "TER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9980472326278687}]}, {"text": "The translation results are reported in: Productivity at open vocabulary translation, measured on test2008 system outputs (after desegmentation) against the vocabulary of the tokenized training data.", "labels": [], "entities": [{"text": "open vocabulary translation", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.6667438944180807}]}, {"text": "splitting does not help because German verb prefixes often radically modify the meaning.", "labels": [], "entities": []}, {"text": "When prefixes are split off, the decoder's embeddings layer may therefore become less effective (as the stem maybe confusable with a completely different word).", "labels": [], "entities": []}, {"text": "We also evaluated casing manually.", "labels": [], "entities": []}, {"text": "Manual inspection of the first fifty #L / #U occurrences in one of the hyptheses reveals that none is misplaced, and casing is always correctly indicated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Target-side training corpus statistics.", "labels": [], "entities": []}, {"text": " Table 6: English\u2192German experimental results  on Europarl (case-sensitive BLEU and TER).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.972016453742981}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9800942540168762}, {"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9872332811355591}]}, {"text": " Table 7: Statistics over words in system outputs for test2008, after desegmentation.", "labels": [], "entities": []}, {"text": " Table 8: Overall types and tokens, measured on  test2008 after desegmentation (hypotheses trans- lations) or after tokenization (reference).", "labels": [], "entities": []}, {"text": " Table 9: Average sentence lengths on test2008.", "labels": [], "entities": []}, {"text": " Table 10: Productivity at open vocabulary transla- tion, measured on test2008 system outputs (after  desegmentation) against the vocabulary of the to- kenized training data.", "labels": [], "entities": []}, {"text": " Table 11: Systems compared against each other in  terms of types found in test2008 hypothesis trans- lations, after desegmentation. (OOV words of out- put of vertical system wrt. vocabulary present in  output of horizontal system.) A: BPE. B: com- pound + BPE. C: suffix + BPE. D: suffix + com- pound + BPE. E: suffix + prefix + compound +  BPE. R: reference translation.", "labels": [], "entities": [{"text": "reference translation", "start_pos": 350, "end_pos": 371, "type": "TASK", "confidence": 0.663473904132843}]}, {"text": " Table 12: Systems compared against each other  in terms of tokens found in test2008 hypothesis  translations, after desegmentation.", "labels": [], "entities": []}, {"text": " Table 13: System outputs (after desegmenta- tion) evaluated against each other with BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9987758994102478}]}]}