{"title": [], "abstractContent": [{"text": "We introduce the Helsinki Neural Machine Translation system (HNMT) and how it is applied in the news translation task at WMT 2017, where it ranked first in both the human and automatic evaluations for English-Finnish.", "labels": [], "entities": [{"text": "Helsinki Neural Machine Translation", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.6086362898349762}, {"text": "news translation task at WMT 2017", "start_pos": 96, "end_pos": 129, "type": "TASK", "confidence": 0.6906221707661947}]}, {"text": "We discuss the success of English-Finnish translations and the overall advantage of NMT over a strong SMT baseline.", "labels": [], "entities": [{"text": "NMT", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8714680075645447}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9557506442070007}]}, {"text": "We also discuss our submissions for English-Latvian, English-Chinese and Chinese-English.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Helsinki Neural Machine Translation system (HNMT) is a full-featured system for neural machine translation, with a particular focus on morphologically rich languages.", "labels": [], "entities": [{"text": "Helsinki Neural Machine Translation system (HNMT)", "start_pos": 4, "end_pos": 53, "type": "TASK", "confidence": 0.7225403860211372}, {"text": "neural machine translation", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.7287085652351379}]}, {"text": "We participated in the WMT 2017 shared task on news translation, obtaining the highest BLEU score for EnglishFinnish translation, while also performing well on English-Latvian and acceptably on EnglishChinese and Chinese-English.", "labels": [], "entities": [{"text": "WMT 2017 shared task", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.7344289422035217}, {"text": "news translation", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.6703860461711884}, {"text": "BLEU score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9835938215255737}, {"text": "EnglishFinnish translation", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.6665104478597641}]}, {"text": "In addition to our participation in the shared task, this paper also details some of the other methods we have implemented and evaluated with HNMT, many of which yielded negative results and were subsequently not used in our submissions for the shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "The outputs of the best SMT and NMT systems were partially reviewed and compared by a profes-sional translator.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9637404680252075}]}, {"text": "The impression of the reviewer was that the perceived quality of NMT far exceeds that of SMT, mainly due to the superior fluency of NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9531965255737305}]}, {"text": "The BLEU scores of the systems also indicate a significant quality difference in favor of NMT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9995145797729492}, {"text": "NMT", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.7896691560745239}]}, {"text": "However, single-reference BLEU scores are known to be unreliable indicators of quality for morphologically complex languages (, and they are also known to favor SMT over other MT methods).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9782686829566956}, {"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.983203649520874}, {"text": "MT", "start_pos": 176, "end_pos": 178, "type": "TASK", "confidence": 0.955141007900238}]}, {"text": "Due to this, it is possible that the BLEU scores, impressive as they are, do not reflect the real qualitative impact of NMT for English-Finnish MT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9990414977073669}, {"text": "MT", "start_pos": 144, "end_pos": 146, "type": "TASK", "confidence": 0.6522634029388428}]}, {"text": "To explore whether single-reference evaluation underestimates NMT quality, a sample of 68 sentences was extracted from the test set.", "labels": [], "entities": []}, {"text": "Both SMT and NMT translations of the sample were postedited with minimal changes to the same quality level as the reference translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.6632323861122131}, {"text": "NMT translations of the sample", "start_pos": 13, "end_pos": 43, "type": "DATASET", "confidence": 0.7458958983421325}]}, {"text": "The minimally edited MT was then used as a TER reference to obtain a more reliable estimate of the MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "DATASET", "confidence": 0.5769050121307373}, {"text": "TER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9621797204017639}, {"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9548085331916809}]}, {"text": "The sample was chosen from sentences where SMT has a sentence-level TER that is at least 10 points lower than the corresponding NMT TER, since such differences can indicate evaluation errors.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9580520391464233}, {"text": "TER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8274878859519958}]}, {"text": "The sample was also restricted to sentences with an SMT TER lower than 40 to reduce postediting workload and filter out low-quality MT.", "labels": [], "entities": [{"text": "SMT TER", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.6768108308315277}, {"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.9593190550804138}]}, {"text": "When postedited MT was used as a reference, total TER/BLEU for the sample changed from 24.7/50.2 to 12.5/76.0 for SMT and from 48.4/25.0 to 18.3/70.5 for NMT.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.48654428124427795}, {"text": "TER/", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9647202491760254}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.714601457118988}, {"text": "SMT", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.8106010556221008}, {"text": "NMT", "start_pos": 154, "end_pos": 157, "type": "DATASET", "confidence": 0.9435912370681763}]}, {"text": "While the score improved for both SMT and NMT, the improvement is clearly much larger for NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9764915108680725}, {"text": "NMT", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.8198748826980591}, {"text": "NMT", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.8182390332221985}]}, {"text": "The test was then repeated for another sample of 68 sentences from the test set, this time selected from the sentences where NMT had lower sentencelevel TER.", "labels": [], "entities": [{"text": "NMT", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.8008137941360474}, {"text": "TER", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9301966428756714}]}, {"text": "The purpose of this sample was to see if evaluation errors affect single-reference scores for SMT to the same extent as for NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9933249354362488}]}, {"text": "With the second sample, total TER/BLEU changed from 58.9/22.1 to 42.5/39.3 for SMT and from 28.2/48.5 to 12.1/77.01 for NMT, so the result was even more favorable for NMT.", "labels": [], "entities": [{"text": "TER/", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9672852158546448}, {"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.6794173121452332}, {"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.8274091482162476}, {"text": "NMT", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.8673097491264343}, {"text": "NMT", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.7468039989471436}]}, {"text": "While the sample size was small, these results strongly suggest that single-reference BLEU scores indeed underestimate NMT quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9804216027259827}]}], "tableCaptions": [{"text": " Table 1: Backtranslated Finnish news data.", "labels": [], "entities": [{"text": "Backtranslated Finnish news data", "start_pos": 10, "end_pos": 42, "type": "DATASET", "confidence": 0.7227265760302544}]}, {"text": " Table 2: Development results with different segmentation strategies for the source language encoder and  the target language decoder and different proportions of backtranslated and parallel data (None = 2.5M  sentences of parallel data + 0 sentences of backtranslated data; Only = 0 + 5.5M; Balanced = 2.5M +  2.5M; All = 2.5M + 5.5M).", "labels": [], "entities": [{"text": "Balanced", "start_pos": 292, "end_pos": 300, "type": "METRIC", "confidence": 0.9907079935073853}]}, {"text": " Table 3: Development results with different en- sembling setups. Each configuration consists of M  models, with SP/M savepoints per model, where  the savepoints may be averaged (+AVG) or in- cluded as equal members in the ensemble (-AVG).", "labels": [], "entities": [{"text": "AVG", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.9720681309700012}, {"text": "AVG", "start_pos": 234, "end_pos": 237, "type": "METRIC", "confidence": 0.9704618453979492}]}, {"text": " Table 6: Backtranslated Latvian news data using  SMT and NMT.", "labels": [], "entities": [{"text": "Backtranslated Latvian news data", "start_pos": 10, "end_pos": 42, "type": "DATASET", "confidence": 0.6152280122041702}, {"text": "NMT", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8867622017860413}]}, {"text": " Table 7: Statistical MT for English-Latvian tested  on newstest 2017 (lowercased BLEU). The offi- cial score in the on-line evaluation system (low- ercased) is surprisingly different from our own  evaluations. The manual evaluation for English- Latvian produced no statistically significant rank- ing.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.7891196608543396}, {"text": "newstest 2017", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9690296053886414}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9929100871086121}, {"text": "statistically significant rank- ing", "start_pos": 266, "end_pos": 301, "type": "METRIC", "confidence": 0.7188845098018646}]}, {"text": " Table 8: HNMT official results on English- Chinese language pair news translation task.", "labels": [], "entities": [{"text": "HNMT", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.743147075176239}, {"text": "English- Chinese language pair news translation task", "start_pos": 35, "end_pos": 87, "type": "TASK", "confidence": 0.6519567146897316}]}]}