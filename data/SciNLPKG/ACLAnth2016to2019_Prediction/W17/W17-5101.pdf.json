{"title": [{"text": "200K+ Crowdsourced Political Arguments fora New Chilean Constitution", "labels": [], "entities": [{"text": "200K", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8454799056053162}, {"text": "Chilean Constitution", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.8192819058895111}]}], "abstractContent": [{"text": "In this paper we present the dataset of 200,000+ political arguments produced in the local phase of the 2016 Chilean constitutional process.", "labels": [], "entities": [{"text": "Chilean constitutional process", "start_pos": 109, "end_pos": 139, "type": "TASK", "confidence": 0.7102760374546051}]}, {"text": "We describe the human processing of this data by the government officials, and the manual tagging of arguments performed by members of our research group.", "labels": [], "entities": []}, {"text": "Afterwards we focus on classification tasks that mimic the human processes, comparing linear methods with neural network architectures.", "labels": [], "entities": []}, {"text": "The experiments show that some of the manual tasks are suitable for automatiza-tion.", "labels": [], "entities": []}, {"text": "In particular, the best methods achieve a 90% top-5 accuracy in a multi-class classification of arguments, and 65% macro-averaged F1-score for tagging arguments according to a three-part argu-mentation model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9985117316246033}, {"text": "F1-score", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9546767473220825}]}], "introductionContent": [{"text": "The current constitution of Chile was written during Pinochet's dictatorship (Political Constitution of the Republic of.", "labels": [], "entities": []}, {"text": "Since the return to democracy in 1990, there has been an increasing pressure to make changes to this constitution.", "labels": [], "entities": []}, {"text": "During 2016, the Chilean government finally decided to begin with a participative process to delineate what anew constitution should consider.", "labels": [], "entities": []}, {"text": "Several aspects of the Chilean process diverged from a classical way of producing anew constitution.", "labels": [], "entities": []}, {"text": "The first phase of the process included small assemblies across the country, big group discussions at the regional level, on-line individual surveys, and soon.", "labels": [], "entities": []}, {"text": "All the generated data was uploaded by the participants using a dedicated Web-site: http://unaconstitucionparachile.cl.", "labels": [], "entities": []}, {"text": "One of the most interesting parts of the process was the local participative phase in which small groups join together in a half-day meeting.", "labels": [], "entities": []}, {"text": "During the meeting the participants had to agree on which are the most important constitutional concepts, writing an argument about why each of these concepts is relevant.", "labels": [], "entities": []}, {"text": "The process produced a dataset of 200,000+ political arguments that was openly published in a raw and anonymous form (General Secretariat, Presidency of.", "labels": [], "entities": []}, {"text": "In this paper we present the curated and tagged dataset of political arguments produced in the local phase of the 2016 Chilean constitutional process, and we analyze it to understand what type of automated reasoning is necessary to classify and tag the components of these arguments.", "labels": [], "entities": [{"text": "Chilean constitutional process", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.6755943596363068}]}, {"text": "We describe the manual processing and tagging performed by the government officials and then by members of our research group.", "labels": [], "entities": []}, {"text": "We consider a three-part argumentation model dividing arguments into policies (e.g., \"The state should provide free education for all\"), facts (e.g., \"Global warming will threaten food security by the middle of the 21st century\"), and values (e.g., \"The pursuit of economic prosperity is less important than the preservation of environmental quality\").", "labels": [], "entities": []}, {"text": "This tagging included the manual parsing, normalization and classification of every single argument in the dataset, and was used as input for the official report of the 2016 process (.", "labels": [], "entities": [{"text": "parsing, normalization and classification", "start_pos": 33, "end_pos": 74, "type": "TASK", "confidence": 0.632578444480896}]}, {"text": "The effort and resources spent in the manual classification and tagging of arguments was considerable, taking months of work.", "labels": [], "entities": [{"text": "manual classification and tagging of arguments", "start_pos": 38, "end_pos": 84, "type": "TASK", "confidence": 0.776591012875239}]}, {"text": "This motivates us to look for ways to automatize at least parts of these tasks.", "labels": [], "entities": []}, {"text": "In particular, one of our motivations is the possibility of adding more arguments from new participant groups, but without the burden of relying on such an expensive and time consuming manual post processing.", "labels": [], "entities": []}, {"text": "We present several baselines on tasks that mimic the human classification and tagging pro-1 cessing.", "labels": [], "entities": []}, {"text": "The first task is a multiclass classification problem of arguments according to the constitutional concept that they are referring to.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.6677607297897339}]}, {"text": "The second task is an automatic tagging of arguments according to our threepart argumentation model.", "labels": [], "entities": []}, {"text": "For these tasks, we compare standard methods, in particular, Logistic Regression, Random Forests and Support Vector Machines, with modern neural-network architectures tailored for natural language processing.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8584415912628174}]}, {"text": "Our baseline methods achieve a good performance thus showing that some of the manual tasks are suitable for automatization.", "labels": [], "entities": []}, {"text": "In particular, our best methods achieve more than 90% top-5 accuracy in the multiclass classification on the first task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9934778213500977}, {"text": "multiclass classification", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.8085610866546631}]}, {"text": "Regarding the second task, we obtain a performance of over 65% macro-averaged F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9541692733764648}]}, {"text": "The data presented in this paper is one of the biggest datasets of arguments written in the Spanish language.", "labels": [], "entities": []}, {"text": "Moreover, to the best of our knowledge, it is the only dataset of its characteristics in the Chilean Spanish dialect.", "labels": [], "entities": [{"text": "Chilean Spanish dialect", "start_pos": 93, "end_pos": 116, "type": "DATASET", "confidence": 0.842211902141571}]}, {"text": "We expect that this dataset plus our baselines can be useful for analyzing political arguments in Spanish beyond the specific tasks that we consider in this paper.", "labels": [], "entities": []}, {"text": "The full dataset is available at https://github.com/ uchile-nlp.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following sections we describe the experiment settings and results for the tasks defined in Section 4.", "labels": [], "entities": []}, {"text": "For Task A and Task B, we compare our methods using accuracy and top-5 accuracy (percentage of cases in which the correct class belongs to the top-5 predictions).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9995032548904419}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9803727865219116}]}, {"text": "Accuracy is useful in our case, given that there are several classes (12 to 44) and the biggest is around 10% of the total instances.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9899094104766846}]}, {"text": "The use of top-5 accuracy allows us to evaluate our models in the scenario of helping humans to quickly determine the class an argument is referring to.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9698371291160583}]}, {"text": "For Task C we use macro-averaged precision, recall and F1-score as metrics due to the class imbalance.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9260876178741455}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9994565844535828}, {"text": "F1-score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9982707500457764}]}], "tableCaptions": [{"text": " Table 1: Examples of constitutional concepts and arguments for the topic \"Rights\" produced during a  SLM. (Arguments were translated from Spanish trying to preserve their original draft and punctuation.)  The final column is the annotation according to the argumentation model.", "labels": [], "entities": [{"text": "SLM", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.6558194756507874}]}, {"text": " Table 3: Arguments with open concepts.", "labels": [], "entities": []}, {"text": " Table 4: Distribution of argument modes resulting  from human annotators.", "labels": [], "entities": []}, {"text": " Table 5: (Task A) Classification results. Top-1 and top-5 accuracy is reported for each baseline and topic.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9904003739356995}]}, {"text": " Table 6: (Task B) Classification results. Top-1 and top-5 accuracy are reported for each baseline and  topic. After each baseline, (c) indicates that only the concept is used as a test instance, and (c, a)  indicates that both the concept and the argument are used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9928107857704163}]}, {"text": " Table 7: (Task C) Classification results. Values  correspond to macro-averaged metrics.", "labels": [], "entities": [{"text": "Classification", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.9141042828559875}]}]}