{"title": [{"text": "Projection-based Coreference Resolution Using Deep Syntax", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.899336189031601}]}], "abstractContent": [{"text": "The paper describes the system for coref-erence resolution in German and Russian, trained exclusively on coreference relations projected through a parallel corpus from English.", "labels": [], "entities": [{"text": "coref-erence resolution", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.949389636516571}]}, {"text": "The resolver operates on the level of deep syntax and makes use of multiple specialized models.", "labels": [], "entities": []}, {"text": "It achieves 32 and 22 points in terms of CoNLL score for Russian and German, respectively.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.8055890500545502}]}, {"text": "Analysis of the evaluation results show that the resolver for Russian is able to preserve 66% of the English resolver's quality in terms of CoNLL score.", "labels": [], "entities": [{"text": "resolver", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.9787580966949463}, {"text": "CoNLL score", "start_pos": 140, "end_pos": 151, "type": "METRIC", "confidence": 0.900814563035965}]}, {"text": "The system was submitted to the Closed track of the COR-BON 2017 Shared task.", "labels": [], "entities": [{"text": "Closed track of the COR-BON 2017 Shared task", "start_pos": 32, "end_pos": 76, "type": "DATASET", "confidence": 0.7764235958456993}]}], "introductionContent": [{"text": "Projection techniques in parallel corpora area popular choice to obtain annotation of various linguistic phenomena in a resource-poor language.", "labels": [], "entities": []}, {"text": "No tools or gold manual labels are required for this language.", "labels": [], "entities": []}, {"text": "Instead, far more easily available parallel corpora are used as a means to transfer the labels to this language from a language, for which such a tool or manual annotation exists.", "labels": [], "entities": []}, {"text": "This paper presents a system submitted to the closed track of the shared task collocated with the Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017).", "labels": [], "entities": [{"text": "Coreference Resolution Beyond OntoNotes (CORBON 2017)", "start_pos": 110, "end_pos": 163, "type": "TASK", "confidence": 0.8380130305886269}]}, {"text": "The task was to build coreference resolution systems for German and Russian without coreference-annotated training data in these languages.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.8908998370170593}]}, {"text": "The only allowed coreference-annotated training data was the English part of the OntoNotes corpus (.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.9103076159954071}]}, {"text": "Alternatively, any publicly available res-1 Details on the shared task are available in its overview paper and at http://corbon.nlp.ipipan.", "labels": [], "entities": []}, {"text": "waw.pl/index.php/shared-task/ olution tool trained on this corpora could be employed.", "labels": [], "entities": []}, {"text": "We adopted and slightly modified an approach previously used by de Souza and Or\u02d8 asan and . Parallel English-German and English-Russian corpora are used to project coreference links that had been automatically resolved on the English side of the corpora.", "labels": [], "entities": []}, {"text": "The projected links then serve as input data for training a resolver.", "labels": [], "entities": []}, {"text": "Unlike the previous works, our coreference resolution system operates on a level of deep syntax.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.8985424935817719}]}, {"text": "The original surface representation of coreference thus must be transferred to this level.", "labels": [], "entities": [{"text": "coreference", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.953498899936676}]}, {"text": "Likewise, coreference relations found by our system must be in the end transformed back to the surface representation, so that they can be evaluated in accordance with the task's requirements.", "labels": [], "entities": []}, {"text": "Our resolver also takes advantage of multiple models, each of them targeting a specific mention type.", "labels": [], "entities": []}, {"text": "According to the official results, we were the only participating team.", "labels": [], "entities": []}, {"text": "Our system achieved 29.40 points and 30.94 points of CoNLL score for German and Russian portion of the official evaluation dataset, respectively.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9465456604957581}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "After introducing related works in Section 2, the paper continues with description of the system and its three main stages (Section 3).", "labels": [], "entities": []}, {"text": "Section 4 lists the training and testing data to enable evaluation of the proposed system in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6, the resolver is analyzed using two different methods.", "labels": [], "entities": [{"text": "resolver", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.974379301071167}]}, {"text": "Finally, we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Raw datasets without manual annotation of coreference are used to train the pipeline described in Section 3.", "labels": [], "entities": []}, {"text": "In contrast, manually annotated datasets are reserved exclusively for evaluation purposes.", "labels": [], "entities": []}, {"text": "shows some basic statistics of the datasets.", "labels": [], "entities": []}, {"text": "We refer to each dataset by its label, which consists of two parts.", "labels": [], "entities": []}, {"text": "The first part denotes the main purpose of the dataset: Train is used for training, Dev for development testing, and Eval for blind evaluation testing.", "labels": [], "entities": [{"text": "Eval", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.5296659469604492}]}, {"text": "The second part indicates the origin of the coreference annotation contained in the dataset: Auto denotes the projected automatic annotation, Off is the official manual annotation provided by the task's organizers, and Add  denotes the additional dataset annotated by the authors of this paper.", "labels": [], "entities": []}, {"text": "We employed the parallel corpora provided by the task's organizers for building the resolver.", "labels": [], "entities": [{"text": "resolver", "start_pos": 84, "end_pos": 92, "type": "TASK", "confidence": 0.9635005593299866}]}, {"text": "Both the English-German and English-Russian corpora come from the NewsCommentary11 collection.", "labels": [], "entities": [{"text": "NewsCommentary11 collection", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.9719549119472504}]}, {"text": "The datasets were provided in a tokenized sentence-aligned format.", "labels": [], "entities": []}, {"text": "We split both corpora into two parts: TrainAuto and DevAuto.", "labels": [], "entities": [{"text": "TrainAuto", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.9171738624572754}, {"text": "DevAuto", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9112305641174316}]}, {"text": "While the former is used for training the models, the latter serves to pick the best values of the learning method's hyperparameters (see Section 3.3).", "labels": [], "entities": []}, {"text": "For evaluation purposes, we used two datasets manually annotated with coreference: DevOff and DevAdd.", "labels": [], "entities": [{"text": "DevOff", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9362291097640991}, {"text": "DevAdd", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.9443372488021851}]}, {"text": "Except for these datasets, a dataset for the final evaluation (EvalOff ) of the shared task was provided by the organizer.", "labels": [], "entities": []}, {"text": "However, the coreference annotation of this dataset has not been published.", "labels": [], "entities": []}, {"text": "Similarly to the raw data, DevOff has been provided by the task's organizers.", "labels": [], "entities": [{"text": "DevOff", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.8716884255409241}]}, {"text": "In fact, both in German and Russian it is represented by a single monolingual document, presumably coming from the News-Commentary11 collection.", "labels": [], "entities": [{"text": "News-Commentary11 collection", "start_pos": 115, "end_pos": 143, "type": "DATASET", "confidence": 0.923039048910141}]}, {"text": "DevAdd dataset consists of the same five documents randomly selected from both the EnglishGerman and English-Russian parallel corpora so that none of these are included in TrainAuto.", "labels": [], "entities": [{"text": "DevAdd dataset", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.966376394033432}]}, {"text": "Coreference relations were annotated on all the three language sides.", "labels": [], "entities": []}, {"text": "The Russian and English sides were labelled by one of this paper's coauthors, who speaks native Russian and fluent  English, and has long experience of annotating anaphoric relations.", "labels": [], "entities": []}, {"text": "The German side was split among three annotators and their outputs were revised by the annotator of the Russian and English part to reach higher consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 145, "end_pos": 156, "type": "METRIC", "confidence": 0.9682400226593018}]}, {"text": "They all followed the annotation guideline published by the organizers.", "labels": [], "entities": []}, {"text": "The reason for creating additional annotated data is that the DevOff set consists only of a thousand words per language, which we found insufficient to reliably assess quality of designed systems.", "labels": [], "entities": [{"text": "DevOff set", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.9267042875289917}]}, {"text": "The English side was labelled to allow for assessing the quality of the projection pipeline over its stages (see Section 6).", "labels": [], "entities": [{"text": "English", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9766581654548645}]}, {"text": "Let us show some notable properties of the German and Russian evaluation data.", "labels": [], "entities": [{"text": "German and Russian evaluation data", "start_pos": 43, "end_pos": 77, "type": "DATASET", "confidence": 0.6469105541706085}]}, {"text": "highlights that the DevAdd sets expectedly contain five times more words than their DevOff counterparts.", "labels": [], "entities": [{"text": "DevAdd sets", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.9512495994567871}]}, {"text": "However, the number of sentences is six times bigger.", "labels": [], "entities": []}, {"text": "This may affect a proportion of individual mention types.", "labels": [], "entities": []}, {"text": "gives a detailed picture of candidate and anaphoric mentions' counts.", "labels": [], "entities": []}, {"text": "Whereas Russian anaphoric NPs account for 75% of all the anaphoric mentions in DevOff, it is only 50% in DevAdd.", "labels": [], "entities": [{"text": "DevOff", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.9499223232269287}, {"text": "DevAdd", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9888997077941895}]}, {"text": "The disproportion appears also between the German datasets.", "labels": [], "entities": [{"text": "German datasets", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.8199745118618011}]}, {"text": "Finally, some of the mention types appear rarely in the DevOff sets.", "labels": [], "entities": [{"text": "DevOff sets", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9718991816043854}]}, {"text": "It especially holds for the Russian DevOff containing alack of reflexive, relative and PPo pronouns.", "labels": [], "entities": []}, {"text": "Conversely, some of the even well-populated types are rarely or never anaphoric (e.g., German demonstrative, reflexive and PPo pronouns).", "labels": [], "entities": []}, {"text": "For both German and Russian, we submitted a single system to the shared task.", "labels": [], "entities": []}, {"text": "Both the systems fulfill the requirements set on the closed track of the task.", "labels": [], "entities": []}, {"text": "To build them we exploited the parallel English-German and English-Russian corpora selected from the News-Commentary11 collection by the task's organizers.", "labels": [], "entities": [{"text": "News-Commentary11 collection", "start_pos": 101, "end_pos": 129, "type": "DATASET", "confidence": 0.911238819360733}]}, {"text": "We present the results in terms of four standard coreference measures: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe () and the CoNLL score ().", "labels": [], "entities": [{"text": "MUC", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9135677814483643}, {"text": "B 3", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9736855030059814}, {"text": "CEAFe", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.942487895488739}, {"text": "CoNLL score", "start_pos": 146, "end_pos": 157, "type": "DATASET", "confidence": 0.5517333596944809}]}, {"text": "The CoNLL score is an average of Fscores of the previous three measures.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.8263687193393707}, {"text": "Fscores", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9996084570884705}]}, {"text": "It was the main score of some previous coreference-related shared tasks, e.g.,, and it remains so for the CORBON 2017 Shared task.", "labels": [], "entities": [{"text": "CORBON 2017 Shared task", "start_pos": 106, "end_pos": 129, "type": "DATASET", "confidence": 0.7283898890018463}]}, {"text": "In, we report the results of evaluating the submitted systems.", "labels": [], "entities": []}, {"text": "Comparison across languages shows very similar performance on the DevOff set.", "labels": [], "entities": [{"text": "DevOff set", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.964648425579071}]}, {"text": "However, evaluation on the larger DevAdd set suggests the Russian resolver performs better.", "labels": [], "entities": [{"text": "DevAdd set", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.9583143591880798}, {"text": "resolver", "start_pos": 66, "end_pos": 74, "type": "TASK", "confidence": 0.48384615778923035}]}, {"text": "Scores on the EvalOff dataset confirms higher quality of the Russian resolver, however, the gap is not so big.", "labels": [], "entities": [{"text": "EvalOff dataset", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.9790564775466919}, {"text": "Russian resolver", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.5710255205631256}]}, {"text": "As the latter dataset is the largest, these results can be considered the most reliable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the datasets used throughout  this work. The last two columns show the number  of tokens in English and in the target language.", "labels": [], "entities": []}, {"text": " Table 3: Distribution of mention types in German  and Russian coreference-annotated datasets. De- nominators show the number of all mention candi- dates while numerators only of the anaphoric ones.", "labels": [], "entities": [{"text": "German  and Russian coreference-annotated datasets", "start_pos": 43, "end_pos": 93, "type": "DATASET", "confidence": 0.6013174474239349}]}, {"text": " Table 5: Results of model ablation. The all line  describes the complete resolver. Every following  line represent an ablated resolver with a model  for a given mention type left out. Differences in  scores are listed in such line.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of the resolvers expressed in terms of Precision, Recall and F-score of some popular  coreference measures.", "labels": [], "entities": [{"text": "Precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.999444305896759}, {"text": "Recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9929336309432983}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9980329871177673}]}]}