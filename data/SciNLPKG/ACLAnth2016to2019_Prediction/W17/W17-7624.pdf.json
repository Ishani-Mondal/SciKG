{"title": [], "abstractContent": [{"text": "The paper illustrates an effective and innovative method for detecting erroneously annotated arcs in gold dependency treebanks based on an algorithm originally developed to measure the reliability of automatically produced dependency relations.", "labels": [], "entities": []}, {"text": "The method permits to significantly restrict the error search space and, more importantly, to reliably identify patterns of systematic recurrent errors which represent dangerous evidence to a parser which tendentially will replicate them.", "labels": [], "entities": []}, {"text": "Achieved results demonstrate effectiveness and reliability of the method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency-based syntactic representations are playing more and more a key role in applications such as machine translation and information extraction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8328944444656372}, {"text": "information extraction", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.8457183241844177}]}, {"text": "If on the one hand current state-ofthe-art approaches to dependency parsing require large training corpora, on the other hand dependency treebanks are very expensive to build in terms of both time and human effort.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8375347852706909}]}, {"text": "The process of developing a treebank can be carried out in different ways, i.e. through: fully manual annotation; semi-automatic annotation, obtained via human editing of the automatic output of relevant NLP tools (e.g. POS taggers, dependency parsers); (semi-)automatic conversion from pre-existing resources.", "labels": [], "entities": [{"text": "POS taggers, dependency parsers)", "start_pos": 220, "end_pos": 252, "type": "TASK", "confidence": 0.6902425835529963}]}, {"text": "If fully manual annotation is time-consuming, costly and prone to inconsistencies even from a single annotator), semi-automatic annotation is faster, less prone to inconsistencies deriving from arbitrary decisions of the single annotator, but is subject to so-called \"anchoring\" effects according to which human decisions are affected by pre-existing values, which include parser errors.", "labels": [], "entities": []}, {"text": "More recently, available resources are more and more the result of a conversion process exploiting already existing annotated corpora: depending on whether conversion is carried out within the same syntactic representation paradigm, approaches can be constituency-to-dependency or operate against dependency-based representations.", "labels": [], "entities": []}, {"text": "Conversion can also be combined with merging and harmonization of different resources (: refers to this case as \"cross-corpus harmonization\".", "labels": [], "entities": []}, {"text": "The conversion approach is particularly significant for less-resourced languages with limited annotated corpora or in the case of multi-lingual resources.", "labels": [], "entities": [{"text": "conversion", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9676783084869385}]}, {"text": "The latter case is exemplified by the Universal Dependencies (UD) initiative, 1 a recent community-driven effort to create cross-linguistically consistent dependency annotated corpora, where 70% of the released treebanks originate from a conversion process and only 29% of them has been manually revised after automatic conversion.", "labels": [], "entities": [{"text": "Universal Dependencies (UD)", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.5172667026519775}]}, {"text": "Whatever strategy is adopted for treebank construction, the resulting annotated corpus unavoidably contains errors.", "labels": [], "entities": [{"text": "treebank construction", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7363967001438141}]}, {"text": "For this reason, the treebank annotation phase is usually followed by another step aimed at detecting and correcting errors.", "labels": [], "entities": []}, {"text": "But treebank validation is as time-consuming as the annotation process: from this, the need follows for methods and techniques to support treebank validation by making the overall task fast and its result consistent and reliable.", "labels": [], "entities": [{"text": "treebank validation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.5512121915817261}]}, {"text": "In principle, treebank validation is concerned with different types of errors.", "labels": [], "entities": []}, {"text": "Following, we distinguish: random errors, which are inherently unpredictable being typically due to annotators' distraction; errors connected with the annotation guidelines, due either to misinterpretation of the guidelines by the annotator, or to constructions not explicitly or comprehensively covered in the annotation guidelines and even errors in the provided guidelines, which are always evolving as long as annotation continues.", "labels": [], "entities": []}, {"text": "To these, conversion errors should be added, i.e. errors due to either erroneous automatic mapping of an original annotation scheme to anew scheme or grey areas in the annotation of specific linguistic constructions.", "labels": [], "entities": []}, {"text": "Whereas random errors are caused by unpredictable decisions by annotators, all other errors types can be classified as systematic and recurrent errors, that are not just determined by chance but are introduced by inaccuracies inherent to the procedure which generated them (automatic pre-annotation or conversion) or gaps in the annotation guidelines.", "labels": [], "entities": []}, {"text": "In this paper, we will mainly focus on systematic and recurrent errors, which we qualify as \"dangerous\" for the fact of providing potentially \"misleading\" evidence to a parser during training, i.e. evidence leading to the replication of errors in the parser output.", "labels": [], "entities": []}, {"text": "In the literature, both pattern-based and statistical approaches have been adopted for carrying out error detection and correction in a rapid and reliable way.", "labels": [], "entities": [{"text": "error detection and correction", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.8021911308169365}]}, {"text": "Relying on the intuition that \"variation in annotation can indicate annotation errors\",) and proposed a variation n-gram detection method where the source of variation is the so-called variation nucleus, i.e. \"a word which has different taggings despite occurring in the same context, in this case surrounded by identical words\".", "labels": [], "entities": [{"text": "variation n-gram detection", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.7286573052406311}]}, {"text": "This methodology has been recently reimplemented and extended by to detect inconsistencies in the UD treebanks.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.8995499014854431}]}, {"text": "The idea that the cases where two \"parsers predict dependencies different from the gold standard\" are \"the most likely candidates when looking for errors\" was experimented by, who trained two parsers based on completely different parsing algorithms to reproduce the training data (i.e. the Penn Treebank).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 290, "end_pos": 303, "type": "DATASET", "confidence": 0.9949061274528503}]}, {"text": "A similar patternbased approach has been also proposed by who complemented their method with a statistical module that, based on contextual features extracted from the Hindi treebank, was in charge of pruning previously identified candidate erroneous dependencies.", "labels": [], "entities": [{"text": "Hindi treebank", "start_pos": 168, "end_pos": 182, "type": "DATASET", "confidence": 0.8564634025096893}]}, {"text": "If all the aforementioned methods exploit corpus-internal evidence to detect inconsistencies within a given treebank, van Noord (2004) and de use external resources, i.e. they rely on the analysis of large automatically parsed corpora external to the treebank under validation.", "labels": [], "entities": []}, {"text": "The underlying idea of these error mining techniques is that sentences with a low parsability score, i.e. sentences which have not received a successful analysis by the parser, very likely contain a parsing error.", "labels": [], "entities": [{"text": "error mining", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.7527673840522766}]}, {"text": "This paper aims at testing the potential of algorithms developed to measure the reliability of automatically produced dependency relations for detecting erroneously annotated arcs in gold treebanks.", "labels": [], "entities": []}, {"text": "In the literature, the result of this type of algorithms varies from a binary classification (correct vs. wrong) as in, to the ranking of dependencies on the basis of a quality score reflecting the reliability and plausibily of the automatic analysis.", "labels": [], "entities": []}, {"text": "Although these algorithms typically work on corpora automatically annotated, they have also been tested against corpora with manually revised (i.e. \"gold\") annotation: in this case, the typical aim is the identification of errors or simply inconsistencies in the annotation.", "labels": [], "entities": []}, {"text": "In this work, we used an algorithm ranking dependencies by reliability, LISCA (Dell', that was applied to a gold treebank to limit the search space for bootstrapping error patterns, i.e. systematic recurring errors (as opposed to random errors).", "labels": [], "entities": [{"text": "LISCA", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9136685132980347}]}, {"text": "Identified error patterns were then projected against the whole corpus.", "labels": [], "entities": []}, {"text": "(2011), here error detection is driven by statistical evidence which, in our approach, is acquired from an external automatically annotated large reference corpus.", "labels": [], "entities": [{"text": "error detection", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8463686108589172}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Occurrences of mark relation in the IUDT newspaper section and erroneously annotated in- stances across the LISCA bins.", "labels": [], "entities": [{"text": "Occurrences", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9795705080032349}, {"text": "IUDT newspaper section", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.9699383576711019}, {"text": "LISCA bins", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.9452956318855286}]}]}