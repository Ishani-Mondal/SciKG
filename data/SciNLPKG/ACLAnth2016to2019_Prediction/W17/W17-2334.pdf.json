{"title": [{"text": "Evaluating Feature Extraction Methods for Knowledge-based Biomedical Word Sense Disambiguation", "labels": [], "entities": [{"text": "Evaluating Feature Extraction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7717177669207255}, {"text": "Knowledge-based Biomedical Word Sense Disambiguation", "start_pos": 42, "end_pos": 94, "type": "TASK", "confidence": 0.6696413934230805}]}], "abstractContent": [{"text": "In this paper, we present an analysis of feature extraction methods via dimension-ality reduction for the task of biomedi-cal Word Sense Disambiguation (WSD).", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7198944091796875}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 126, "end_pos": 157, "type": "TASK", "confidence": 0.7525631586710612}]}, {"text": "We modify the vector representations in the 2-MRD WSD algorithm, and evaluate four dimensionality reduction methods: Word Embeddings using Continuous Bag of Words and Skip Gram, Singular Value Decomposition (SVD), and Principal Component Analysis (PCA).", "labels": [], "entities": []}, {"text": "We also evaluate the effects of vector size on the performance of each of these methods.", "labels": [], "entities": []}, {"text": "Results are evaluated on five standard evaluation datasets (Abbrev.100, Ab-brev.200, Abbrev.300, NLM-WSD, and MSH-WSD).", "labels": [], "entities": [{"text": "Abbrev.100", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.6767991185188293}, {"text": "Ab-brev.200", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.631391704082489}, {"text": "Abbrev.300", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.6556687355041504}]}, {"text": "We find that vector sizes of 100 are sufficient for all techniques except SVD, for which a vector size of 1500 is preferred.", "labels": [], "entities": []}, {"text": "We also show that SVD performs on par with Word Embeddings for all but one dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "W ord Sense Disambiguation (WSD) is the task of automatically identifying the intended sense (or concept) of an ambiguous word based on the context in which the word is used.", "labels": [], "entities": [{"text": "W ord Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8116462911878314}, {"text": "automatically identifying the intended sense (or concept) of an ambiguous word", "start_pos": 48, "end_pos": 126, "type": "TASK", "confidence": 0.6418397632928995}]}, {"text": "Automatically identifying the intended sense of ambiguous words improves the performance of clinical and biomedical applications such as medical coding and indexing for quality assessment, cohort discovery (, and other secondary uses of data such as information retrieval and extraction, and question answering systems).", "labels": [], "entities": [{"text": "Automatically identifying the intended sense of ambiguous words", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.7762089297175407}, {"text": "medical coding", "start_pos": 137, "end_pos": 151, "type": "TASK", "confidence": 0.7385498285293579}, {"text": "cohort discovery", "start_pos": 189, "end_pos": 205, "type": "TASK", "confidence": 0.7178642749786377}, {"text": "information retrieval and extraction", "start_pos": 250, "end_pos": 286, "type": "TASK", "confidence": 0.8223134875297546}, {"text": "question answering", "start_pos": 292, "end_pos": 310, "type": "TASK", "confidence": 0.8614162504673004}]}, {"text": "These capabilities are becoming essential tasks due to the growing amount of information available to researchers, the transition of healthcare documentation towards electronic health records, and the push for quality and efficiency in healthcare.", "labels": [], "entities": []}, {"text": "Previous methods using distributional context vectors have been shown to perform well for the task of WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9365454912185669}]}, {"text": "One problem with distributional vectors is the sparseness of the vectors and noise (defined here as information that does not aid in the discrimination between word senses).", "labels": [], "entities": []}, {"text": "Word embeddings have become an increasingly popular method to reduce the dimensionality of vector representations, and have been shown to be a valuable resource for NLP tasks including WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 185, "end_pos": 188, "type": "TASK", "confidence": 0.7547840476036072}]}, {"text": "Prior to word embeddings, proposed Latent Semantic Indexing (LSI) which reduces dimensionality using the factor analysis technique, singular value decomposition (SVD).", "labels": [], "entities": [{"text": "word embeddings", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7301501333713531}, {"text": "singular value decomposition (SVD)", "start_pos": 132, "end_pos": 166, "type": "TASK", "confidence": 0.75276979804039}]}, {"text": "When performing SVD, some information is lost.", "labels": [], "entities": [{"text": "SVD", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7252970337867737}]}, {"text": "Intuitively the lost information is noise, and its removal causes the similarity and non-similarity between words to be more discernible).", "labels": [], "entities": []}, {"text": "Similar to SVD is principal component analysis.", "labels": [], "entities": [{"text": "principal component analysis", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6751529375712076}]}, {"text": "PCA transforms the vectors into anew basis of principal components, which are created by orthogonal linear combinations of the original features.", "labels": [], "entities": []}, {"text": "Each principal component captures as much variance in the data as possible while maintaining orthogonality.", "labels": [], "entities": []}, {"text": "Dimensionality reduction is performed by removing principal components that capture little variance.", "labels": [], "entities": [{"text": "Dimensionality reduction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9498184621334076}]}, {"text": "In this paper, we evaluate the performance of word embeddings, SVD, and PCA for dimensionality reduction for the task of knowledge-based WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.7837533950805664}]}, {"text": "Explicit vectors are trained on Medline abstracts and performance is evaluated on five reference standards.", "labels": [], "entities": [{"text": "Medline abstracts", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.8894646465778351}]}, {"text": "Specifically, the contributions of this paper are an analysis of: \u2022 Vector Representation: SVD, PCA, and word embeddings using continuous bag of words (CBOW) and skip-gram are evaluated as dimensionality reduction techniques applied to the task of knowledge-based WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 264, "end_pos": 267, "type": "TASK", "confidence": 0.8465467691421509}]}, {"text": "Evaluation is performed on several standard evaluation datasets, and compared against explicit co-occurrence vectors as a baseline.", "labels": [], "entities": []}, {"text": "\u2022 Dimensionality: the dimensionality of the reduced vectors is a parameter, and the value can effect performance.", "labels": [], "entities": []}, {"text": "We evaluate each vector representation's performance at dimensionalities of 100, 200, 500, 1000, and 1500.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate using several standard WSD evaluation datasets which include the following.", "labels": [], "entities": [{"text": "WSD evaluation datasets", "start_pos": 35, "end_pos": 58, "type": "DATASET", "confidence": 0.8305193583170573}]}, {"text": "The Abbrev dataset 3 developed by) contains examples of 300 ambiguous abbreviations found in MEDLINE that were initially presented by ().", "labels": [], "entities": [{"text": "Abbrev dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8249949216842651}, {"text": "MEDLINE", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.8478936553001404}]}, {"text": "The data set was automatically recreated by identifying the abbreviations and longforms (unabbreviated terms) in MEDLINE abstracts, and replacing the long-form in the abstract with its abbreviation.", "labels": [], "entities": [{"text": "MEDLINE abstracts", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.8359944820404053}]}, {"text": "The abbreviations' longforms were manually mapped to concepts in the UMLS by Stevenson, et al.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9244105815887451}]}, {"text": "Each abstract contains approximately 216 words.", "labels": [], "entities": []}, {"text": "The datasets consist of a set of 21 different ambiguous abbreviations for which the number of labeled instances of those abbreviations varies.", "labels": [], "entities": []}, {"text": "Abbrev.100 contains 100 instances, Abbrev.200 contains 200, and Abbrev.300 contains 300 labeled instances.", "labels": [], "entities": []}, {"text": "Two abbreviations contain less than 200 instances, and three abbreviations contain less than 300 instances, and are omitted from Abbrev.200 and Abbrev.300 respectively.", "labels": [], "entities": [{"text": "Abbrev.200", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.9420093894004822}, {"text": "Abbrev.300", "start_pos": 144, "end_pos": 154, "type": "DATASET", "confidence": 0.9231122136116028}]}, {"text": "The average number of long-forms per abbreviation is 2.6 and the average majority sense across all subsets is 70%.", "labels": [], "entities": [{"text": "majority sense", "start_pos": 73, "end_pos": 87, "type": "METRIC", "confidence": 0.6132899820804596}]}, {"text": "The National Library of Medicine's Word Sense Disambiguation (NLM-WSD) dataset 4 developed by) contains 50 frequently occurring ambiguous words from the 1998 MEDLINE baseline.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (NLM-WSD) dataset", "start_pos": 35, "end_pos": 78, "type": "DATASET", "confidence": 0.7534859521048409}, {"text": "MEDLINE baseline", "start_pos": 158, "end_pos": 174, "type": "DATASET", "confidence": 0.7141677886247635}]}, {"text": "Each ambiguous word in the NLM-WSD dataset contains 100 ambiguous instances randomly selected from the abstracts totaling to 5,000 instances.", "labels": [], "entities": [{"text": "NLM-WSD dataset", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.8636323511600494}]}, {"text": "The instances were manually disambiguated by 11 evaluators who assigned the ambiguous word to a concept (CUI) in the UMLS, or assigned the concept as \"None\" if none of the possible concepts described the term.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.8966767191886902}]}, {"text": "The average number of senses per term is 2.3, and the average majority sense is 78%.", "labels": [], "entities": []}, {"text": "We used the following packages to obtain our vector representations: [1] Explicit Representation: We used the Text::NSP packaged developed by ).", "labels": [], "entities": []}, {"text": "We used a windows size of 8, a frequency cutoff of 5, and removed stopwords.", "labels": [], "entities": []}, {"text": "[2] Singular Value Decomposition: We ran the MATLAB R2016b implementation of sparse matrix SVD (svds) on the explicit representation matrix, and used each row of the resulting U matrix as a reduced vector.", "labels": [], "entities": [{"text": "Singular Value Decomposition", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.613895187775294}]}, {"text": "[3] Principal Component Analysis: We centered the explicit representation matrix, and used the MATLAB R2016b implementation of sparse matrix SVD (svds) on the centered matrix to obtain the U and \u03a3 matrices.", "labels": [], "entities": [{"text": "Principal Component Analysis", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.732672115166982}, {"text": "MATLAB R2016b", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.7419136762619019}]}, {"text": "The reduced vectors are obtained from the product of U and \u03a3.", "labels": [], "entities": []}, {"text": "[4] Word Embeddings: We used the word2vec package developed by for the continuous-bag-of-words (CBOW) and skip-gram word embedding models with a window size of 8, a frequency cutoff of 5, and default settings for all other parameters.", "labels": [], "entities": []}, {"text": "We use the Word2vec::Interface package 6 version 0.03 to obtain the disambiguation accuracy for each of the WSD datasets.", "labels": [], "entities": [{"text": "Word2vec::Interface package 6 version 0.03", "start_pos": 11, "end_pos": 53, "type": "DATASET", "confidence": 0.9060018062591553}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9542278051376343}, {"text": "WSD datasets", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.9136074185371399}]}, {"text": "The differences between the means of disambiguation accuracy were tested for statistical significance using pair-wise Students t-test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.873251736164093}]}, {"text": "compares the performance of each vector representation technique, and shows the best results (best among all dimensionalities tested) of each of the vector representations on the WSD datasets.", "labels": [], "entities": [{"text": "WSD datasets", "start_pos": 179, "end_pos": 191, "type": "DATASET", "confidence": 0.9334448873996735}]}, {"text": "Explicit refers to the co-occurrence vector without dimensionality reduction, PCA refers to the principal component analysis representation, SVD refers to singular value decomposition representation, CBOW refers to the word embeddings continuous bag of words representation and SG refers to the word embeddings skip gram representation.", "labels": [], "entities": []}, {"text": "The colored bars show results for individual datasets, and the total length shows the sum of accuracies for all datasets.", "labels": [], "entities": []}, {"text": "The Abbrev.100, Abbrev.200, and Abbrev.300 results show that SVD (0.87/0.84/0.62), CBOW (0.87/0.86/0.62), and SG (0.87/0.84/0.59) obtained a statistically higher overall disambiguation accuracy (p \u2264 0.05) than explicit (0.69/0.70/0.59) and PCA (0.59/0.70/0.59), while the difference between their respective results was not statistically significant.", "labels": [], "entities": [{"text": "Abbrev.100", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.5038787126541138}, {"text": "Abbrev.200", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.5394788384437561}, {"text": "Abbrev.300", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.7746405005455017}, {"text": "SVD", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.7134466171264648}, {"text": "CBOW", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9184236526489258}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.899695634841919}]}, {"text": "The NLM-WSD results also show that SVD (0.61), CBOW (0.65), and SG (0.65) obtained a statistically higher disambiguation accuracy than explicit (0.54) and PCA (0.54), while the difference between their respective results was not statistically significant.", "labels": [], "entities": [{"text": "SVD", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.7033891677856445}, {"text": "CBOW (0.65)", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9084043800830841}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9574811458587646}]}, {"text": "The MSH-WSD results show a statistically significant difference (p \u2264 0.05) between explicit (0.64), PCA (0.64), SVD (0.77), CBOW (0.81), and SG (0.79) except for Explicit and PCA.", "labels": [], "entities": [{"text": "MSH-WSD", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8607617616653442}, {"text": "CBOW", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.7592620849609375}]}, {"text": "shows the p-values between the vector representations for each of the datasets.", "labels": [], "entities": []}, {"text": "To discover an upper bound on dimensionality and performance, we continued to increase the dimensions of SVD up to 3000.", "labels": [], "entities": []}, {"text": "Results are shown in, and indicate that after d = 1500 there are not significant gains inaccuracy, indicating that a dimensionality of 1500 is sufficient for SVD.", "labels": [], "entities": [{"text": "SVD", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.9280962347984314}]}], "tableCaptions": [{"text": " Table 1: The p-values using Student's pairwise t- test. Each table corresponds to a different dataset,  each row and column a different dimensionality  reduction technique.", "labels": [], "entities": []}, {"text": " Table 2: Comparison with Previous Work on MSH-WSD", "labels": [], "entities": [{"text": "MSH-WSD", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8488237261772156}]}]}