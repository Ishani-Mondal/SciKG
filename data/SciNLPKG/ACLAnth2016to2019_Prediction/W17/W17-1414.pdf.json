{"title": [{"text": "Language-Independent Named Entity Analysis Using Parallel Projection and Rule-Based Disambiguation", "labels": [], "entities": [{"text": "Language-Independent Named Entity Analysis", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.5201049372553825}]}], "abstractContent": [{"text": "The 2017 shared task at the Balto-Slavic NLP workshop requires identifying coarse-grained named entities in seven languages, identifying each entity's base form, and clustering name mentions across the multilingual set of documents.", "labels": [], "entities": []}, {"text": "The fact that no training data is provided to systems for building supervised classifiers further adds to the complexity.", "labels": [], "entities": []}, {"text": "To complete the task we first use publicly available parallel texts to project named entity recognition capability from English to each evaluation language.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.6997438669204712}]}, {"text": "We ignore entirely the subtask of identifying non-inflected forms of names.", "labels": [], "entities": [{"text": "identifying non-inflected forms of names", "start_pos": 34, "end_pos": 74, "type": "TASK", "confidence": 0.7601638913154602}]}, {"text": "Finally, we create cross-document entity identi-fiers by clustering named mentions using a procedure-based approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "The LITESABER project at Johns Hopkins University Applied Physics Laboratory is investigating techniques to perform analysis of named entities in low-resource languages.", "labels": [], "entities": []}, {"text": "The tasks we are investigating include: named entity detection and coarse type classification, commonly referred to as named entity recognition (NER); linking of named entities to online databases such as Wikipedia; and clustering of entities across documents.", "labels": [], "entities": [{"text": "named entity detection", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6663290758927664}, {"text": "coarse type classification", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.6608901917934418}, {"text": "named entity recognition (NER)", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.8189886311690012}, {"text": "linking of named entities to online databases such as Wikipedia", "start_pos": 151, "end_pos": 214, "type": "TASK", "confidence": 0.7701338946819305}, {"text": "clustering of entities across documents", "start_pos": 220, "end_pos": 259, "type": "TASK", "confidence": 0.8580472588539123}]}, {"text": "We have applied some of our techniques to the BSNLP 2017 Shared Task.", "labels": [], "entities": [{"text": "BSNLP 2017 Shared Task", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.7644919455051422}]}, {"text": "Specifically, we submitted results in two of the three categories: Named Entity Mention Detection and Classification (or NER), which asks systems to locate mentions of named entities in text and identify their types; and Entity Matching (also known as cross-lingual identification, or cross-document coreference resolution) which asks systems to determine when two entity mentions, either in the same document or in different documents, refer to the same real-world entity.", "labels": [], "entities": [{"text": "Named Entity Mention Detection and Classification (or NER)", "start_pos": 67, "end_pos": 125, "type": "TASK", "confidence": 0.7649822741746902}, {"text": "cross-lingual identification", "start_pos": 252, "end_pos": 280, "type": "TASK", "confidence": 0.7277767658233643}, {"text": "cross-document coreference resolution", "start_pos": 285, "end_pos": 322, "type": "TASK", "confidence": 0.6566291451454163}]}, {"text": "We did not participate in the Name Normalization task, which asks systems to convert each entity mention to its lemmatized form.", "labels": [], "entities": [{"text": "Name Normalization task", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.9072421590487162}]}, {"text": "This paper describes our approach and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We had no collections with ground truth for six of the seven BSNLP languages.", "labels": [], "entities": [{"text": "BSNLP languages", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.8841557204723358}]}, {"text": "To gauge performance, we divided the induced label collection (i.e., the Balto-Slavic side of the parallel collection) into training and test sets.", "labels": [], "entities": []}, {"text": "We then built an SVMLattice tagger using the training set, and applied it to the test set, assuming that the projected tags were entirely accurate.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Digging slightly deeper into these results (Table 3), we see that in general, performance is highest on locations, and lowest for the miscellaneous   The one language for which we have some curated ground truth is Russian.", "labels": [], "entities": []}, {"text": "The LDC collection LDC2016E95 (LORELEI Russian Representative Language Pack) contains, among other things, named entity annotations for 239 Russian documents.", "labels": [], "entities": [{"text": "LDC collection LDC2016E95 (LORELEI Russian Representative Language Pack)", "start_pos": 4, "end_pos": 76, "type": "DATASET", "confidence": 0.8473377108573914}]}, {"text": "We built a named entity recognizer for Russian using the methodology described above, and applied it to 10% of these LDC data.", "labels": [], "entities": [{"text": "LDC data", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.7431752383708954}]}, {"text": "We used the CoNLL evaluation script to score the run.", "labels": [], "entities": [{"text": "CoNLL evaluation script", "start_pos": 12, "end_pos": 35, "type": "DATASET", "confidence": 0.8518799940745035}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Note that the label set for the LDC data is slightly different than the BSNLP label set; in particular, there is no MISC category (although the overall scores count all MISC labels as incorrect).", "labels": [], "entities": [{"text": "LDC data", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.8465432822704315}, {"text": "BSNLP label set", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9516487518946329}]}, {"text": "We note from these results that the tagger is doing much more poorly on ORGs than is suggested by the experiments on projected labels.", "labels": [], "entities": []}, {"text": "Thus, we must view the results on ORGs for the other languages with a degree of skepticism.", "labels": [], "entities": []}, {"text": "Possible reasons include wider variation in organization names than the other categories, the use of acronyms and abbreviations, or greater difficulty in aligning organization names.", "labels": [], "entities": []}, {"text": "Looking at performance by entity type, we see best results for the PER and LOC classes, similar to our findings in   We have not had sufficient time to perform an in-depth analysis of the data.", "labels": [], "entities": [{"text": "PER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.8789954781532288}]}, {"text": "One reason for low performance on ORG and MISC classes maybe that these entity mentions contain more words on average than PER and LOC entities, and our projected alignments maybe less reliable for longer spanning entities.", "labels": [], "entities": []}, {"text": "Additionally, our trained English model is based on the CoNLL dataset, and those tagging guidelines maybe inconsistent with the BSNLP 2017 shared task guidelines.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9751589596271515}, {"text": "BSNLP 2017 shared task guidelines", "start_pos": 128, "end_pos": 161, "type": "DATASET", "confidence": 0.8931945204734802}]}, {"text": "For example, demonyms and nationalities were tagged as MISC in CoNLL,: Per-language entity coreference.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.7926685214042664}]}], "tableCaptions": [{"text": " Table 1: Parallel collection sizes, in words.", "labels": [], "entities": []}, {"text": " Table 2: NER results using projected labels.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9644768834114075}]}, {"text": " Table 3: F 1 Scores for the Four Entity Categories.", "labels": [], "entities": [{"text": "F 1 Scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9596244096755981}]}, {"text": " Table 4. Note that the la- bel set for the LDC data is slightly different than  the BSNLP label set; in particular, there is no  MISC category (although the overall scores count  all MISC labels as incorrect).", "labels": [], "entities": [{"text": "LDC data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.8984892666339874}, {"text": "BSNLP label set", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9168076117833456}]}, {"text": " Table 4: Results on annotated Russian text.", "labels": [], "entities": []}, {"text": " Table 5: NER results for the strict matching con- dition, by language.", "labels": [], "entities": []}, {"text": " Table 6: F 1 scores by type and language for the  trump test set with strict matching.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9653708040714264}]}, {"text": " Table 7: Per-language entity coreference.", "labels": [], "entities": [{"text": "Per-language entity coreference", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6529080073038737}]}]}