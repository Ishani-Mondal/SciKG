{"title": [{"text": "Multi-word Entity Classification in a Highly Multilingual Environment", "labels": [], "entities": [{"text": "Multi-word Entity Classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8488442500432333}]}], "abstractContent": [{"text": "This paper describes an approach for the classification of millions of existing multi-word entities (MWEntities), such as organisation or event names, into thirteen category types, based only on the tokens they contain.", "labels": [], "entities": [{"text": "classification of millions of existing multi-word entities (MWEntities), such as organisation or event names", "start_pos": 41, "end_pos": 149, "type": "TASK", "confidence": 0.8069388340501225}]}, {"text": "In order to classify our very large in-house collection of multilingual MWEntities into an application-oriented set of entity categories, we trained and tested distantly-supervised classifiers in 43 languages based on MWEntities extracted from BabelNet.", "labels": [], "entities": []}, {"text": "The best-performing clas-sifier was the multi-class SVM using a TF.IDF-weighted data representation.", "labels": [], "entities": []}, {"text": "Interestingly , one unique classifier trained on a mix of all languages consistently performed better than classifiers trained for individual languages, reaching an averaged F1-value of 88.8%.", "labels": [], "entities": [{"text": "F1-value", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9992913007736206}]}, {"text": "In this paper, we present the training and test data, including a human evaluation of its accuracy, describe the methods used to train the classi-fiers, and discuss the results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9988698363304138}]}], "introductionContent": [{"text": "Named Entities (NEs) such as persons, organisations, locations or events are crucial bearers of information as they are often the answers to major text understanding questions.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7951824963092804}]}, {"text": "Software to carryout Named Entity Recognition (NER) in free text needs to recognise the relevant strings in text and disambiguate the broad entity types (e.g. Paris Hilton is a person rather than a location), justifying the term Named Entity Recognition and Classification (NERC).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.8339647948741913}, {"text": "Named Entity Recognition and Classification (NERC)", "start_pos": 229, "end_pos": 279, "type": "TASK", "confidence": 0.8454019948840141}]}, {"text": "In this paper we focus on MWEntity classification, thereby placing NERC in the context of the study of MWExpressions.", "labels": [], "entities": [{"text": "MWEntity classification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.885128527879715}, {"text": "NERC", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.7941923141479492}]}, {"text": "Our work is carried out in a highly multilingual environment, and as a result, suitable training corpora are difficult to source.", "labels": [], "entities": []}, {"text": "Motivated by this, in addition to a method of MWEntity classification, we also present a technique for automatically generating a silver-standard annotated resource of 3.8 million entities for use as training data.", "labels": [], "entities": [{"text": "MWEntity classification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.809669703245163}]}, {"text": "This resource incorporates data from 43 different languages, covering multiple language families.", "labels": [], "entities": []}, {"text": "MWEntities are often not translated, so it is rather common to find names from one language in amongst entities from another (e.g. French MWEntity 'institut polytechnique des sciences avanc\u00e9es' found in the Arabic dataset).", "labels": [], "entities": []}, {"text": "It is important to specify that our classification work is exclusively based on internal features of the names; that is, the tokens contained within each MWEntity.", "labels": [], "entities": [{"text": "MWEntity", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.9490607976913452}]}, {"text": "No additional external features were extracted.", "labels": [], "entities": []}, {"text": "This is due in part to the fact that the contexts of our historically accumulated MWEntities are no longer known.", "labels": [], "entities": []}, {"text": "We therefore aim at developing a system that can be widely applied to data sets that do not include, or give access to, such contextual information.", "labels": [], "entities": []}, {"text": "The paper begins with a section on related work (2) and is followed by a section describing the starting point and the objective of our work: the target entity hierarchy (3.1); the set of entities extracted from the BabelNet resource) and the method used for the extraction; and an evaluation of this BabelNet silver-standard including inter-annotator agreement data.", "labels": [], "entities": []}, {"text": "In Section 4, we present the classification methods we tested, i.e. a baseline approach and two variants of Support Vector Machines.", "labels": [], "entities": []}, {"text": "Experiments and results achieved are presented in Section 5, together with a discussion of the results.", "labels": [], "entities": []}, {"text": "We conclude with a short summary and a pointer to future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "A crucial element of our work consisted of evaluating the quality of the automatically generated annotated resource from BabelNet.", "labels": [], "entities": []}, {"text": "To justify its use as a gold (or 'silver') standard resource for this supervised classification task, we conducted a small manual evaluation, shown in, with native speakers of five different languages (German, French, Polish, English and Swedish), evaluating both the quality of the automatic annotations as well as inter-annotator agreement for English across four annotators (one of whom is a native English speaker).", "labels": [], "entities": []}, {"text": "Each annotator was trained on atrial set of 100 randomly extracted English MWEntities, then tested on a further 200 randomly extracted multiword entities for their own native language, and an additional 200 for English.", "labels": [], "entities": []}, {"text": "The annotators were asked to provide two separate sets of annotations: first, the annotators provided 'offline' annotations for each of the entities, selecting from a set of 13 possible entity types (corresponding to the types described in).", "labels": [], "entities": []}, {"text": "The no-guess tag ('NG'): Some example of MWEntities to be annotated. was used when an annotation decision could not be made with certainty, or when an entity appeared to belong in a category not included in the possible list of tags.", "labels": [], "entities": []}, {"text": "Secondly, the annotators were permitted to research their secondary guess 'online'.", "labels": [], "entities": []}, {"text": "For consistency, the BabelNet labels were hidden throughout.", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9206141829490662}]}, {"text": "shows that the 'offline' annotation results are quite heterogeneous among annotators, with a precision between 81.6% and 92.4%, and a recall between 66.5% and 81.0%.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9990369081497192}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9994768500328064}]}, {"text": "On the other hand, the 'online' annotation results are much more homogeneous, for the same language between different annotators, and also across languages: precision varied between 87.6% and 92.5%, and recall between 85.0% and 90.5%.", "labels": [], "entities": [{"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9995344877243042}, {"text": "recall", "start_pos": 203, "end_pos": 209, "type": "METRIC", "confidence": 0.9994920492172241}]}, {"text": "The averaged kappa across the 4 English annotators is 0.848, and among the 200 annotated MWEntities, 159 were annotated with full inter-annotator agreement, including only 6 which differed from the automatically-generated BabelNet annotation (listed in).", "labels": [], "entities": [{"text": "kappa", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9365237355232239}]}, {"text": "10 were annotated with the same type by 3 of 4 annotators.", "labels": [], "entities": []}, {"text": "The remaining 31 MWEntities, where only two annotators agreed,: Manual annotations on 200 MWEntities randomly extracted for 5 languages from the created resource, compared with the best-performing system (right-most column).", "labels": [], "entities": []}, {"text": "highlight the difficulty of the task: some MWEntities are ambiguous, and could easily be annotated with different types.", "labels": [], "entities": []}, {"text": "For example, 'buffalo rochester and pittsburgh railroad' could be annotated both as a company or a facility.", "labels": [], "entities": []}, {"text": "This manual evaluation aims to show that, although the resource we extracted from BabelNet is not perfect, it is consistent enough across annotators and languages to consider it a silver-standard in our experiments.", "labels": [], "entities": []}, {"text": "This section provides a brief discussion of the method of cross-validation used in this work and an overview of the preprocessing carried out on the resource automatically generated from BabelNet, before turning to the experimental method and results of the experiments.", "labels": [], "entities": []}, {"text": "During development, we compared the performance of the two SVM systems, SVM TFIDF and SVM COUNTS.", "labels": [], "entities": [{"text": "SVM", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8247444033622742}, {"text": "TFIDF", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.5355790257453918}, {"text": "SVM", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8383204936981201}, {"text": "COUNTS", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.4203789532184601}]}, {"text": "In line with the expectation that TF.IDF vectorisation would provide more informative features in the task of differentiating between categories, we found SVM TFIDF performed marginally better overall.", "labels": [], "entities": []}, {"text": "In the following full-scale experiments, we therefore will only dis-  Average results across the 43 tested languages, with language-dependent or independent approaches, for the 5 tested percentile thresholds.", "labels": [], "entities": []}, {"text": "cuss the comparison between SVM TFIDF and the baseline approach, COSSIM.", "labels": [], "entities": [{"text": "COSSIM", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.7564055323600769}]}, {"text": "The main task compared the performance of language-dependent and language-independent training for the two classification methods, when applied across 43 languages at 5 different threshold levels (see Section 4.3 for threshold definitions).", "labels": [], "entities": []}, {"text": "The 43 languages correspond mostly to European languages including Russian, plus Arabic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Number of entities extracted from Babel- Net before and after filtering (see Section 5.2).", "labels": [], "entities": [{"text": "Babel- Net", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.7587648034095764}]}, {"text": " Table 5: Manual annotations on 200 MWEntities randomly extracted for 5 languages from the created  resource, compared with the best-performing system (right-most column).", "labels": [], "entities": []}, {"text": " Table 7: Results for some specific languages of the 43 tested, with language-dependent or independent  approaches, with SVM TFIDF method at the 0% percentile threshold.", "labels": [], "entities": []}, {"text": " Table 8: Results for some specific type classes, with language-dependent or independent approaches,  with SVM TFIDF method at the 0% percentile threshold.", "labels": [], "entities": []}]}