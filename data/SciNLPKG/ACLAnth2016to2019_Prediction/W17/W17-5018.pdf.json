{"title": [{"text": "Human and Automated CEFR-based Grading of Short Answers", "labels": [], "entities": [{"text": "Automated CEFR-based Grading of Short Answers", "start_pos": 10, "end_pos": 55, "type": "TASK", "confidence": 0.6193756262461344}]}], "abstractContent": [{"text": "This paper is concerned with the task of automatically assessing the written proficiency level of non-native (L2) learners of English.", "labels": [], "entities": []}, {"text": "Drawing on previous research on automated L2 writing assessment following the Common European Framework of Reference for Languages (CEFR), we investigate the possibilities and difficulties of deriving the CEFR level from short answers to open-ended questions, which has not yet been subjected to numerous studies up to date.", "labels": [], "entities": [{"text": "L2 writing assessment", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.6352663040161133}, {"text": "Common European Framework of Reference for Languages (CEFR)", "start_pos": 78, "end_pos": 137, "type": "DATASET", "confidence": 0.6848466128110886}]}, {"text": "The object of our study is twofold: to examine the intricacy involved with both human and automated CEFR-based grading of short answers.", "labels": [], "entities": [{"text": "CEFR-based grading of short answers", "start_pos": 100, "end_pos": 135, "type": "TASK", "confidence": 0.570014476776123}]}, {"text": "On the one hand, we describe the compilation of a learner corpus of short answers graded with CEFR levels by three certified Cambridge examiners.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9566901922225952}]}, {"text": "We mainly observe that, although the shortness of the answers is reported as undermining a clear-cut evaluation, the length of the answer does not necessarily correlate with inter-examiner disagreement.", "labels": [], "entities": []}, {"text": "On the other hand, we explore the development of a soft-voting system for the automated CEFR-based grading of short answers and draw tentative conclusions about its use in a computer-assisted testing (CAT) setting.", "labels": [], "entities": [{"text": "CEFR-based grading of short answers", "start_pos": 88, "end_pos": 123, "type": "TASK", "confidence": 0.5656220495700837}]}], "introductionContent": [{"text": "The recent years have seen a growth of interest in Automated Writing Evaluation (AWE) for levelling non-native (L2) writing proficiency.", "labels": [], "entities": [{"text": "levelling non-native (L2) writing proficiency", "start_pos": 90, "end_pos": 135, "type": "TASK", "confidence": 0.7434713925634112}]}, {"text": "Among the variety of assessment scales used, a number of studies have focused on levelling the writing proficiency following the Common European Framework of Reference (CEFR)) through a combination of machine learning techniques and linguistic complexity features.", "labels": [], "entities": [{"text": "Common European Framework of Reference (CEFR))", "start_pos": 129, "end_pos": 175, "type": "DATASET", "confidence": 0.7544929459691048}]}, {"text": "One of the often cited benefits for using such assistive systems is that they could increase the effectiveness of large-scale testing procedures where a large panel of examiners are grading amass of responses in a short period of time.", "labels": [], "entities": []}, {"text": "One application that comes to mind is the validation of the required writing skills of a large group of university students.", "labels": [], "entities": []}, {"text": "In this scenario, implementing an expert-only testing procedure is costly for two reasons.", "labels": [], "entities": []}, {"text": "On the one hand, a sufficiently large panel of experts evaluating the same text is needed to guarantee the validity of the evaluation.", "labels": [], "entities": [{"text": "validity", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9685672521591187}]}, {"text": "On the other hand, the large number of students who are participating in the programme makes the procedure even more time-consuming.", "labels": [], "entities": []}, {"text": "Integrating an automated evaluator in the panel of examiners could therefore contribute to an increase in effectiveness of the evaluation procedure.", "labels": [], "entities": []}, {"text": "The present study takes part in a broader project which very aim is to research the possibility of using a computer-assisted setting for evaluating the level of written proficiency in English of non-native university students.", "labels": [], "entities": []}, {"text": "The main idea of the project is to validate whether the students have the writing skills matching the CEFR descriptors of the proficiency level in which they have been placed.", "labels": [], "entities": [{"text": "CEFR descriptors", "start_pos": 102, "end_pos": 118, "type": "DATASET", "confidence": 0.9591443538665771}]}, {"text": "As a follow-up to a more general placement test, the students are queried to write an original short answer (ranging from 30 to 200 words) to an open-ended question, on the basis of which a panel of examiners validate or adapt the CEFR level resulting from the global evaluation.", "labels": [], "entities": []}, {"text": "In this context, we investigated the possibilities of partially automatising the short answer evaluation procedure, which is the general subject of the current paper.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "After a brief review of the previous work on automated grading and the CEFR (Section 2), we will introduce our work on (i) the collection of a CEFR-graded learner corpus of short answers (Section 3) and (ii) the development of an automated grading system through ensemble learning (Section 4).", "labels": [], "entities": [{"text": "CEFR", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.9846241474151611}, {"text": "CEFR-graded learner corpus of short answers", "start_pos": 143, "end_pos": 186, "type": "DATASET", "confidence": 0.9019517302513123}]}, {"text": "In Section 5, we will compare the human and automated grading of short answers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Question types per initial CEFR level", "labels": [], "entities": [{"text": "CEFR", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9149549603462219}]}, {"text": " Table 2: The number of answers collected per ini- tial level and per question type", "labels": [], "entities": []}, {"text": " Table 3: Features selected through a Spearman  rank correlation test and a stability selection pro- cedure. All features are standardised to a Gaussian  scale and their average is reported per assessed  level. Lemma-based indices are marked with L.", "labels": [], "entities": []}, {"text": " Table 4: Inter-examiner agreement scores.", "labels": [], "entities": []}, {"text": " Table 5: Performance of the system compared to a  set of baselines on 10-fold cross-validation.", "labels": [], "entities": []}, {"text": " Table 6: Reliability of replacing one exam- iner with the system. The partial agreement  scores are further broken down into percentages  per human-human agreement (HHA) and human- system agreement (HSA).", "labels": [], "entities": [{"text": "human- system agreement (HSA)", "start_pos": 175, "end_pos": 204, "type": "METRIC", "confidence": 0.6901435213429588}]}]}