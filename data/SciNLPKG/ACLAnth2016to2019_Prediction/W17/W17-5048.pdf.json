{"title": [{"text": "Fusion of Simple Models for Native Language Identification", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6904041767120361}]}], "abstractContent": [{"text": "In this paper we describe the approaches we explored for the 2017 Native Language Identification shared task.", "labels": [], "entities": [{"text": "2017 Native Language Identification shared task", "start_pos": 61, "end_pos": 108, "type": "TASK", "confidence": 0.6913366516431173}]}, {"text": "We focused on simple word and sub-word units avoiding heavy use of hand-crafted features.", "labels": [], "entities": []}, {"text": "Following recent trends, we explored linear and neural networks models to attempt to compensate for the lack of rich feature use.", "labels": [], "entities": []}, {"text": "Initial efforts yielded f1-scores of 82.39% and 83.77% in the development and test sets of the fusion track, and were officially submitted to the task as team L2F.", "labels": [], "entities": [{"text": "f1-scores", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9686588644981384}]}, {"text": "After the task was closed, we carried on further experiments and relied on a late fusion strategy for combining our simple proposed approaches with modifications of the baselines provided by the task.", "labels": [], "entities": []}, {"text": "As expected, the i-vectors based subsystem dominates the performance of the system combinations, and results in the major contributor to our achieved scores.", "labels": [], "entities": []}, {"text": "Our best combined system achieves 90.1% and 90.2% f1-score in the development and test sets of the fusion track, respectively .", "labels": [], "entities": [{"text": "f1-score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.984419047832489}]}], "introductionContent": [{"text": "Native Language Identification (NLI) is the task of identifying a person's native language (L1) based on that person's written or spoken content in a learned language (L2).", "labels": [], "entities": [{"text": "Native Language Identification (NLI) is the task of identifying a person's native language (L1) based on that person's written or spoken content in a learned language (L2)", "start_pos": 0, "end_pos": 171, "type": "Description", "confidence": 0.7055766863482339}]}, {"text": "The task has gained increased interest from various research communities, which led to the first shared task in 2013.", "labels": [], "entities": []}, {"text": "In 2016, a sub-challenge was held at Interspeech () on identifying the native language based on spoken * All authors contributed equally.", "labels": [], "entities": [{"text": "Interspeech", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9150925874710083}]}, {"text": "responses in English, in contrast to the NLI shared task, which was based on written responses.", "labels": [], "entities": []}, {"text": "The NLI Shared Task 2017 is the next instance in this series of shared tasks ( , with the distinction of featuring both written and spoken based responses as available data.", "labels": [], "entities": []}, {"text": "Spoken responses were available in the form of speech transcriptions and i-vectors, not actual audio files.", "labels": [], "entities": []}, {"text": "Systems could compete in three tracks: ESSAYS, where only the provided written essays data could be used; SPEECH, where only the speech transcriptions and possibly i-vectors could be used; and FUSION, where all three datasets could be combined.", "labels": [], "entities": [{"text": "ESSAYS", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.8743667602539062}, {"text": "SPEECH", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9507691860198975}, {"text": "FUSION", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.915847897529602}]}, {"text": "The task provided a single development labeled dataset and two different unlabeled test sets: one for the ESSAYS and SPEECH tracks, and another for the FUSION track.", "labels": [], "entities": [{"text": "ESSAYS", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9019581079483032}, {"text": "SPEECH", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.8064589500427246}, {"text": "FUSION", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.7737390398979187}]}, {"text": "Additionally, each system was allowed to participate in an open or closed sub-track depending on whether any external data was used or not, respectively.", "labels": [], "entities": []}, {"text": "In this paper we describe the approaches we took in the NLI Shared Task 2017, specifically in the FUSION closed track, where we participated as team L2F.", "labels": [], "entities": [{"text": "NLI Shared Task 2017", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.7366853356361389}, {"text": "FUSION closed track", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.846520185470581}]}, {"text": "After having officially submitted a system to the track, we performed further experiments and developed additional systems, including a late fusion one that performs 7 absolute points above the system we submitted.", "labels": [], "entities": []}, {"text": "The best performing systems on a variety of Natural Language Processing (NLP) and Information Retrieval problems, including NLI, are ensembles of complex models that employ a myriad of high-level features.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7139370888471603}]}, {"text": "There are, however, some systems with simple features that are able to surpass complex ensembles, like the previous state of the art in NLI by.", "labels": [], "entities": []}, {"text": "One way of not relying on specially engineered features is to follow the current trend on using Neural Networks (NN) and Deep Learning (DL) techniques (and doing parameter tuning instead).", "labels": [], "entities": []}, {"text": "Although DL approaches have achieved several state of the art results in NLP, this is not the case yet for NLI.", "labels": [], "entities": []}, {"text": "Our line of approach for this task was to benefit from the power of fusion systems while avoiding complex feature engineering and exploring the usefulness of DL techniques.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for the ESSAYS track over the development dataset for the baseline, the langid, the  Naive Bayes with and without BPE, the essay Neural Network (NN-ESSAYS), and the LLR-FUSION  systems. The best result, excluding the LLR-FUSION, is highlighted in bold.", "labels": [], "entities": [{"text": "BPE", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.5509099960327148}]}, {"text": " Table 3: Results for the FUSION track over the development dataset for the baselines, the fusion Neural  Network", "labels": [], "entities": [{"text": "FUSION", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.917027473449707}, {"text": "fusion Neural  Network", "start_pos": 91, "end_pos": 113, "type": "DATASET", "confidence": 0.7529661854108175}]}, {"text": " Table 4: Results over the FUSION test dataset trained only on the ESSAYS for the baseline and our two  best single systems.", "labels": [], "entities": [{"text": "FUSION test dataset", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.8071838219960531}, {"text": "ESSAYS", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9500815272331238}]}, {"text": " Table 5: Results for the FUSION track over the test dataset for the baseline and our two fusion systems.  The emphasized system, NN-FUSION, was the only officially submitted to the task.", "labels": [], "entities": [{"text": "FUSION", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9532079100608826}]}]}