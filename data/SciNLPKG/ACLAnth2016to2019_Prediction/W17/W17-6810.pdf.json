{"title": [{"text": "A constrained graph algebra for semantic parsing with AMRs", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.8520280420780182}, {"text": "AMRs", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.5655444264411926}]}], "abstractContent": [{"text": "When learning grammars that map from sentences to abstract meaning representations (AMRs), one faces the challenge that an AMR can be described in a huge number of different ways using traditional graph algebras.", "labels": [], "entities": [{"text": "learning grammars that map from sentences to abstract meaning representations (AMRs)", "start_pos": 5, "end_pos": 89, "type": "TASK", "confidence": 0.6967354256373185}]}, {"text": "We introduce anew algebra for building graphs from smaller parts, using linguistically motivated operations for combining ahead with a complement or a modifier.", "labels": [], "entities": []}, {"text": "Using this algebra, we can reduce the number of analyses per AMR graph dramatically; at the same time, we show that challenging linguistic constructions can still be handled correctly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsers are systems which map natural-language expressions to formal semantic representations, in away that is learned from data.", "labels": [], "entities": []}, {"text": "Much research on semantic parsing has focused on mapping sentences to Abstract Meaning Representations (AMRs), graphs which represent the predicate-argument structure of the sentences.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7722419500350952}, {"text": "Abstract Meaning Representations (AMRs)", "start_pos": 70, "end_pos": 109, "type": "TASK", "confidence": 0.6331093162298203}]}, {"text": "Such work builds upon the AMRBank (, a corpus in which each sentence has been manually annotated with an AMR.", "labels": [], "entities": [{"text": "AMRBank", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.8163827061653137}]}, {"text": "The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR.", "labels": [], "entities": [{"text": "AMRBank", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.8495808839797974}]}, {"text": "This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.751811683177948}]}, {"text": "In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra, is huge (.", "labels": [], "entities": []}, {"text": "This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8537901937961578}]}, {"text": "Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics.", "labels": [], "entities": []}, {"text": "We propose a novel method to generate a constrained set of derivations directly from an AMR, but without losing linguistically significant phenomena and parses.", "labels": [], "entities": []}, {"text": "To this end we present an apply-modify (AM) graph algebra for combining graphs using operations that reflect the way linguistic predicates combine with complements and adjuncts.", "labels": [], "entities": []}, {"text": "By equipping graphs with annotations that encode argument sharing, AM algebra derivations can model phenomena such as control, raising, and coordination straightforwardly.", "labels": [], "entities": [{"text": "control, raising, and coordination", "start_pos": 118, "end_pos": 152, "type": "TASK", "confidence": 0.671155259013176}]}, {"text": "We describe a method to generate the AM algebra for an AMR in practice, and demonstrate its effectiveness: e.g. for graphs with five nodes, our method reduces the number of candidate terms from 10 17 to just 21 on average.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews some related work and sets the stage for the rest of the paper.", "labels": [], "entities": []}, {"text": "Section 3 briefly reviews the HR algebra and its problems for grammar induction, and then defines the AM algebra.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.8171992599964142}]}, {"text": "In Section 4, we discuss a number of challenging linguistic phenomena and demonstrate that AM algebra derivations can capture the intended compositional derivations of the resulting AMRs.", "labels": [], "entities": [{"text": "AM algebra derivations", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.8712297876675924}]}, {"text": "We explain how to obtain AM algebra derivations for graphs in the AMRBank in Section 5.", "labels": [], "entities": [{"text": "AM algebra derivations", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.820593516031901}, {"text": "AMRBank", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.969716489315033}]}, {"text": "Finally, we show that in practice, we can indeed reduce the set of derivations while achieving high coverage in Section 6.", "labels": [], "entities": [{"text": "coverage", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9951560497283936}]}], "datasetContent": [{"text": "We conclude by analyzing whether the AM algebra achieves our goal of reducing the number of possible terms fora given AMR, compared to the HR algebra.", "labels": [], "entities": []}, {"text": "Both algorithms are implemented and available in the Alto framework . For the HR algebra, we use the setup of: Constants consist of single labeled nodes and single edges, and they are combined using the operations of the HR algebra.", "labels": [], "entities": []}, {"text": "We use an HR algebra with two source names (HR-S2) and one with three source names (HR-S3); this has an impact on the set of graphs that can be analyzed and on the runtime complexity.", "labels": [], "entities": []}, {"text": "For the AM algebra, we use the method of Section 5 with different numbers of allowed COREF sources (AM-C0, AM-C1, AM-C2 for 0, 1, 2 COREF sources respectively).", "labels": [], "entities": []}, {"text": "We use all graphs of the LDC2016E25 training corpus with up to 50 nodes, fora total of 35685 graphs.", "labels": [], "entities": [{"text": "LDC2016E25 training corpus", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.9625774025917053}]}, {"text": "Consider first the coverage of the different graph algebras, i.e. the proportion of graphs of a given size for which we find at least one term, shown in as a function of the graph size.", "labels": [], "entities": []}, {"text": "As expected, coverage goes up as the number of source nodes (for HR) and COREF nodes (for AM) increases.", "labels": [], "entities": [{"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9849531054496765}, {"text": "COREF", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9852079153060913}]}, {"text": "The coverage of AM-C0 is higher than that of HR-S2 because HR-S2 can only analyze graphs of treewidth 1, i.e. without (undirected) cycles, whereas AM-C0 can handle local re-entrancies e.g. from control constructions through the type annotations.", "labels": [], "entities": []}, {"text": "For example, the AMRs in can be decomposed by AM-C0 and HR-S3, but not HR-S2.", "labels": [], "entities": []}, {"text": "The highest coverage is achieved by AM-C2.", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.987532913684845}, {"text": "AM-C2", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9201486706733704}]}, {"text": "We now turn to the (geometric) mean number of terms each algebra assigns to those graphs of a given size that it can analyze.", "labels": [], "entities": []}, {"text": "We find that the AM algebras achieve a dramatic reduction in the number of terms, compared to the HR algebras: Even the high-coverage AM-C2 has much fewer terms than the very low-coverage HR-S2 (note the log-scale on the vertical axis).", "labels": [], "entities": []}, {"text": "As an example, switching from HR-S2 to AM-C0 reduces the number of terms for the graph in from 3584 to 4 (they differ in active vs passive, and order of application).", "labels": [], "entities": []}, {"text": "For 5 nodes, the average for HR-S3 is 10 17 terms, and for AM-C2 just 21.", "labels": [], "entities": []}, {"text": "This reduction has multiple reasons: we can use larger constants in the AM algebra, and the graph-combining operations of the AM algebra are much more constrained.", "labels": [], "entities": []}, {"text": "Further, the type system and carefully chosen set of constants restrict application and modification.", "labels": [], "entities": []}, {"text": "Note that just because an algebra can find some term for an AMR does not necessarily mean that it makes sense from a linguistic perspective (cf..", "labels": [], "entities": []}, {"text": "Conversely, by reducing the set of possible terms, there is a risk that we might throw out the linguistically correct analysis.", "labels": [], "entities": []}, {"text": "By choosing the operations of the AM algebra to match linguistic intuitions about predicate-argument structure, we have reduced this risk.", "labels": [], "entities": []}, {"text": "We leave a precise quantitative analysis, e.g. in the context of grammar induction, for future work.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.8259139657020569}]}, {"text": "We finish by measuring the mean runtimes to compute the decomposition automata.", "labels": [], "entities": []}, {"text": "Once again, we find that the AM algebra solidly outperforms the HR algebra.", "labels": [], "entities": [{"text": "HR algebra", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.8089362382888794}]}, {"text": "The runtimes of HR-S3 are too slow to be useful in practice, whereas even the highest-coverage algebra AM-C2 decomposes even large graphs in seconds.", "labels": [], "entities": []}, {"text": "Moreover, the runtimes for AM-C1 are faster than even for the very low-coverage HR-S2 algebra.", "labels": [], "entities": []}, {"text": "The previously fastest parser for graphs using hyperedge replacement grammars was the one of, which used Interpreted Regular Tree Grammars (IRTGs)) together with the HR algebra.", "labels": [], "entities": []}, {"text": "Because we have seen how to compute decomposition automata for the AM algebra in Section 5, we can do graph parsing with IRTGs over the AM algebra instead.", "labels": [], "entities": [{"text": "graph parsing", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.7349646985530853}]}, {"text": "The fact that decomposition automata for the AM algebra are smaller and faster to compute promises a further speed-up for graph parsing as well, making wide-coverage graph parsing for large graphs feasible.", "labels": [], "entities": [{"text": "graph parsing", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.7050625085830688}, {"text": "wide-coverage graph parsing", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.7323698202768961}]}], "tableCaptions": []}