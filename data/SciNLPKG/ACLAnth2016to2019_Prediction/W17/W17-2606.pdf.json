{"title": [{"text": "Combining Word-Level and Character-Level Representations for Relation Classification of Informal Text", "labels": [], "entities": [{"text": "Relation Classification of Informal Text", "start_pos": 61, "end_pos": 101, "type": "TASK", "confidence": 0.9262831449508667}]}], "abstractContent": [{"text": "Word representation models have achieved great success in natural language processing tasks, such as relation classification.", "labels": [], "entities": [{"text": "Word representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7089648544788361}, {"text": "relation classification", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.92827507853508}]}, {"text": "However, it does not always work on informal text, and the morphemes of some misspelling words may carry important short-distance semantic information.", "labels": [], "entities": []}, {"text": "We propose a hybrid model, combining the merits of word-level and character-level representations to learn better representations on informal text.", "labels": [], "entities": []}, {"text": "Experiments on two dataset of relation classification, SemEval-2010 Task8 and a large-scale one we compile from informal text, show that our model achieves a competitive result in the former and state-of-the-art with the other.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8740833699703217}]}], "introductionContent": [{"text": "Deep learning has made significant progress in natural language processing, and most of approaches treat word representations as the cornerstone.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6979410648345947}]}, {"text": "Though it is effective, word-level representation is inherently problematic: it assumes that each word type has its own vector that can vary independently; most words only occur once in training data and out-of-vocabulary(OOV) words cannot be addressed.", "labels": [], "entities": [{"text": "word-level representation", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.7229534089565277}]}, {"text": "A word may typically include a root and one or more affixes (rock-s, red-ness, quick-ly, run-ning, un-expect-ed), or more than one root in a compound (black-board, rat-race).", "labels": [], "entities": []}, {"text": "It is reasonable to assume that words which share common components(root, prefix, suffix)may be potentially related, while word-level representation considers each word separately.", "labels": [], "entities": []}, {"text": "On the other hand, new words enter English from every area of life, e.g. Chillaxing -Blend of chilling and relaxing, represent taking a break from stressful activities to rest or relax.", "labels": [], "entities": [{"text": "Blend", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.594336986541748}]}, {"text": "Whereas the vocabulary size of word-level model is fixed beforehand, the lack of these word representations may lose important semantic information.", "labels": [], "entities": []}, {"text": "Especially on informal text, the problems of word-level representation will be amplified and hard to ignore.", "labels": [], "entities": [{"text": "word-level representation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7068491131067276}]}, {"text": "Recently, character-level representation, which takes characters as atomic units to derive the embeddings, demonstrates that it can memorize the arbitrary aspects of word orthography.", "labels": [], "entities": [{"text": "character-level representation", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7215132862329483}]}, {"text": "Parameters of these simple model are less, and it will be not ideal when processing long sentence.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9872251152992249}]}, {"text": "Combining word-level and character-level representations attempts to overcome the weaknesses of the two representations.", "labels": [], "entities": []}, {"text": "We utilize a Bidirectional Gated Recurrent Unit (Bi-GRU) () and Convolutional Neural Networks(CNN) to capture twolevel semantic representations respectively.", "labels": [], "entities": []}, {"text": "While character-level information is likely to be drowned out by word-level information if simply connected, we adopt Highway Networks) to balance both.", "labels": [], "entities": []}, {"text": "To evaluate our model, we evaluate on a public benchmark: Task8.", "labels": [], "entities": []}, {"text": "This dataset is small and restricted in their relation types and their syntactic and lexical variations, and it is still unknown whether learning on the range of the specific relation transfers well to informal text.", "labels": [], "entities": []}, {"text": "As such, we introduce a large-scale dataset based on the corpus and queries of TAC-KBP Slot Filling Track () between 2009 to 2014, which contains 48k relation sentences, called KBP-SF48 1 . TAC-KBP corpus comes from newswire, Web, post and discussion forum documents actually comprised of informal content, including language mismatch and spelling errors.", "labels": [], "entities": []}, {"text": "We extract sentences from slots and fillers of Slot Filling Evalua-tion with position indicators to keep the same format as SemEval-2010 Task8.", "labels": [], "entities": []}, {"text": "For instance, the following sentence with two nominals surrounded by position indicators belong to org:founded by relation: Bharara's office brought insider trading charges against <e1>Raj Rajaratnam <e1/>, the co-founder of hedge fund <e2>Galleon Group<e2/>.", "labels": [], "entities": [{"text": "Galleon Group", "start_pos": 240, "end_pos": 253, "type": "DATASET", "confidence": 0.9031665623188019}]}], "datasetContent": [{"text": "We evaluate our model on two dataset.", "labels": [], "entities": []}, {"text": "SemEval-2010 Task8 dataset contains 9 directional relations and an Other class.", "labels": [], "entities": [{"text": "SemEval-2010 Task8 dataset", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.8623013297716776}]}, {"text": "There exist dataset derived from TAC-KBP for relation classification, such as KBP37(20k example for evaluation) collected by.", "labels": [], "entities": [{"text": "TAC-KBP", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.8768444657325745}, {"text": "relation classification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.820780485868454}]}, {"text": "Based on this and more public corpus of resent years, we introduce anew larger scale dataset, called KBP-SF48.", "labels": [], "entities": [{"text": "KBP-SF48", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.9525652527809143}]}, {"text": "There are 48,340 annotated examples distributed among 40 relations(excluding no relation and org:website), including 33,838 sentences for training that consists of 102 unique characters, 9,668 for testing and 4,834 for validation.", "labels": [], "entities": []}, {"text": "Compared to SemEval-2010 Task8, the relation type of KBP-SF48 is designed to build a Knowledge Base from unstructured text, including quite a few informal documents, and the specific nominals that be-45 longs to these relations can be filled in specific slots.", "labels": [], "entities": [{"text": "KBP-SF48", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9018772840499878}]}, {"text": "There exists non-directional and the directional corresponding relations (e.g. per:children & per:parents and org:members & org:member of).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison on KBP-SF48", "labels": [], "entities": [{"text": "KBP-SF48", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.8917509913444519}]}]}