{"title": [{"text": "Supersense Tagging with a Combination of Character, Subword, and Word-level Representations", "labels": [], "entities": [{"text": "Supersense Tagging", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7764442265033722}]}], "abstractContent": [{"text": "Recently, there has been increased interest in utilizing characters or subwords for natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "However, the effect of utilizing character, subword, and word-level information simultaneously has not been examined so far.", "labels": [], "entities": []}, {"text": "In this paper, we propose a model to leverage various levels of input features to improve on the performance of an supersense tagging task.", "labels": [], "entities": [{"text": "supersense tagging task", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.7774155537287394}]}, {"text": "Detailed analysis of experimental results show that different levels of input representation offer distinct characteristics that explain performance discrepancy among different tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, there has been increased interest in using characters or subwords, instead of words, as the basic unit of language feature in natural language processing tasks.", "labels": [], "entities": []}, {"text": "Utilizing subword information has been shown to be very effective for named entity alignment of parallel corpus and named entity recognition (.", "labels": [], "entities": [{"text": "named entity alignment", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.6464105844497681}, {"text": "named entity recognition", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.6348162988821665}]}, {"text": "Some recent advancements were achieved using character or subword features in neural machine translation and language modeling.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.6798677841822306}, {"text": "language modeling", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7180340141057968}]}, {"text": "The main benefit of utilizing features below word-level is the ability to overcome out-ofvocabulary (OOV) and the rare word problems.", "labels": [], "entities": []}, {"text": "When faced with very infrequent or OOV words in the test data, word-level models must resort to replacing them with \"unknown word\" tokens; and in many cases, this discarded information could be vital for understanding certain semantics of the text, hence word-level models could perform poorly when said types of words appear frequently.", "labels": [], "entities": []}, {"text": "Traditionally, words are segmented into subwords using carefully engineered morpheme analyzers ().", "labels": [], "entities": []}, {"text": "Recently, we see arise in popularity of data-driven methods such as employing an efficient encoding scheme of character sequences (e.g. byte-pair encoding ).", "labels": [], "entities": [{"text": "byte-pair encoding", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7428523898124695}]}, {"text": "Words could also be split into individual characters to capture even finer syntactic details.", "labels": [], "entities": []}, {"text": "Subword schemes of varying linguistic granularity offer a trade-off between capturing semantic and syntactic features.", "labels": [], "entities": []}, {"text": "Despite of the success of character or subwordlevel approaches, there has been lack of studies on ways to combine different levels of features, namely character, subword, and word-level features.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, utilization of subword units have not even been applied to supersense tagging yet.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7923585474491119}]}, {"text": "In this paper, we present a novel neural network architecture that incorporates all three types of word-feature units (Section 3).", "labels": [], "entities": []}, {"text": "We conduct experiments on SemCor dataset using our model.", "labels": [], "entities": [{"text": "SemCor dataset", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8178216516971588}]}, {"text": "Then we analyze the optimal combination of the word features for each classs of the 41 supersenses in detail (Section 4.3).", "labels": [], "entities": []}], "datasetContent": [{"text": "Dropout rate was 0.5, stochastic gradient descent (SGD) was used as learning method, and learning rate was 0.005.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 89, "end_pos": 102, "type": "METRIC", "confidence": 0.9759471118450165}]}, {"text": "The gradient clipping is 5.0.", "labels": [], "entities": [{"text": "clipping", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.704257071018219}]}, {"text": "The classification results of SemCor dataset using different combinations of input representations are shown in.", "labels": [], "entities": [{"text": "SemCor dataset", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.7978226840496063}]}, {"text": "We note that in unirepresentation settings the word-level model performs better than the character or subword-level model.", "labels": [], "entities": []}, {"text": "This is presumably because supersense tagging predicts labels for each word.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7948088049888611}]}, {"text": "We also note that when word embeddings are pre-trained, the performance is always improved by the addition of character or subword-level embeddings.", "labels": [], "entities": []}, {"text": "Overall, the best result is obtained when the subword and word embeddings are pre-trained and all embeddings are utilized.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The 41 WordNet supersenses (26 nouns and 15 verbs) and their frequency percentages. Note  the sum of the percentages of the nouns is 100%, and that of the verbs is 100%.", "labels": [], "entities": []}, {"text": " Table 2: The statistics of the SemCor dataset.", "labels": [], "entities": [{"text": "SemCor dataset", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.8370069265365601}]}, {"text": " Table 3: Comparison of character, subword, and  word-level models with/without pre-trained vec- tors.", "labels": [], "entities": []}, {"text": " Table 4: F-score comparison for NER, the most frequent and rarest supersense classification. Bold values  are best cases in with/without pre-trained vectors, respectively. Underlined values represent the cases that  use pre-trained embeddings.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9639195203781128}, {"text": "NER", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.828819215297699}]}]}