{"title": [{"text": "People Daily (14), Sina (14), Xinhua (8) Czech aktu\u00e1ln\u011b.cz (10), blesk.cz (4), blisty.cz (1), den\u00edk.cz (1), iDNES.cz (14), ihned.cz (4), lidovky.cz (8)", "labels": [], "entities": [{"text": "People Daily", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9840562045574188}]}], "abstractContent": [{"text": "This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task.", "labels": [], "entities": [{"text": "WMT17 shared tasks", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.5361811816692352}, {"text": "machine translation (MT) tasks", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.8466608623663584}]}], "introductionContent": [{"text": "We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT) held at EMNLP 2017", "start_pos": 71, "end_pos": 127, "type": "TASK", "confidence": 0.78421870470047}]}, {"text": "This conference builds on eleven previous editions of WMT as workshops and conference.", "labels": [], "entities": []}, {"text": "This year we conducted several official tasks.", "labels": [], "entities": []}, {"text": "We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8822499215602875}]}, {"text": "The Latvian and Chinese translation tasks were new this year.", "labels": [], "entities": [{"text": "Latvian and Chinese translation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.49225352704524994}]}, {"text": "Latvian is a lesser resourced data condition on challenging language pair.", "labels": [], "entities": []}, {"text": "Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages organized alongside the Chinese Workshop on Machine Translation (CWMT).", "labels": [], "entities": [{"text": "Chinese Workshop on Machine Translation (CWMT)", "start_pos": 112, "end_pos": 158, "type": "TASK", "confidence": 0.6609311178326607}]}, {"text": "1 System outputs for each task were evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "The human evaluation (Section 3) involves asking human judges to score sentences output by anonymized systems.", "labels": [], "entities": []}, {"text": "We obtained large numbers of assessments from researchers who contributed evaluations proportional to the number of tasks they entered.", "labels": [], "entities": []}, {"text": "In addition, we used Mechanical Turk to collect further evaluations.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.8443259596824646}]}, {"text": "This year, the official manual evaluation metric is based on judgments of adequacy on a 100-point scale, a method we explored last year with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems.", "labels": [], "entities": []}, {"text": "The quality estimation task (Section 4) this year included three subtasks: sentence-level prediction of post-editing effort scores, word and phraselevel prediction of good/bad labels.", "labels": [], "entities": [{"text": "sentence-level prediction of post-editing effort", "start_pos": 75, "end_pos": 123, "type": "TASK", "confidence": 0.7325461864471435}, {"text": "word and phraselevel prediction of good/bad labels", "start_pos": 132, "end_pos": 182, "type": "TASK", "confidence": 0.7419967187775506}]}, {"text": "Datasets were released with English\u2192German IT translations and German\u2192English Pharmaceutical translations for all subtasks.", "labels": [], "entities": []}, {"text": "The automatic post-editing task (Section 5) examined automatic methods for correcting errors produced by an unknown machine translation system.", "labels": [], "entities": [{"text": "correcting errors produced by an unknown machine translation", "start_pos": 75, "end_pos": 135, "type": "TASK", "confidence": 0.6800427362322807}]}, {"text": "Participants were provided with training triples containing source, target and human post-edits, and were asked to return automatic post-edits for unseen (source, target) pairs.", "labels": [], "entities": []}, {"text": "In this third round, the task focused on correcting English\u2192German translations in the IT domain and German\u2192English translations in the Pharmaceutical domain.", "labels": [], "entities": [{"text": "correcting English\u2192German translations", "start_pos": 41, "end_pos": 79, "type": "TASK", "confidence": 0.8905892491340637}, {"text": "Pharmaceutical domain", "start_pos": 136, "end_pos": 157, "type": "DATASET", "confidence": 0.8835202157497406}]}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation and estimation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8719261288642883}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8308209776878357}, {"text": "machine translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.8069436848163605}]}, {"text": "As before, all of the data, translations, and collected human judgments are publicly available.", "labels": [], "entities": []}, {"text": "We hope these datasets serve as a valuable resource for research into statistical machine translation, automatic evaluation, or prediction of translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6702077289422353}, {"text": "prediction of translation", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.8306145866711935}]}, {"text": "News translations are also available for interactive visualization and comparison of differences between systems at http://wmt.ufal.cz/ using MT-ComparEval ().", "labels": [], "entities": [{"text": "MT-ComparEval", "start_pos": 142, "end_pos": 155, "type": "DATASET", "confidence": 0.8724285960197449}]}], "datasetContent": [{"text": "To assess the quality of the output of the APE systems and produce a ranking based on human judgment, as well as analyze how humans perceive TER/BLEU performance differences between the submitted systems, a human evaluation of the quality of automatic post-edits was carried out using Direct Assessment (DA) (.", "labels": [], "entities": [{"text": "TER", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9619318842887878}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.7606274485588074}, {"text": "Direct Assessment (DA)", "start_pos": 285, "end_pos": 307, "type": "METRIC", "confidence": 0.7929023385047913}]}, {"text": "Since sufficient crowd-sourced workers are available for assessing English on Mechanical Turk, the DA evaluation for German to English was completed via quality-controlled crowdsourcing.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.8813101351261139}]}, {"text": "For English to German, DA judgments were provided by 10 native German speakers from Saarland University, studying language technologies and translation.", "labels": [], "entities": [{"text": "DA", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.7882854342460632}]}, {"text": "This subsection describes the human evaluation procedure and presents the results of the evaluation of participants' primary submissions.", "labels": [], "entities": []}, {"text": "In terms of the News translation task manual evaluation, a total of 151 individual researcher accounts were involved, and 754 turker accounts.", "labels": [], "entities": [{"text": "News translation task", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7884096304575602}]}, {"text": "Researchers in the manual evaluation came from 29 different research groups and contributed judgments of 125,693 translations, while 237,200 translation assessment scores were submitted in total by the crowd.", "labels": [], "entities": []}, {"text": "Under ordinary circumstances, each assessed translation would correspond to a single individual scored segment.", "labels": [], "entities": []}, {"text": "However, since many systems The maximum sentence length with RR was 30 in WMT16.", "labels": [], "entities": [{"text": "RR", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9631315469741821}, {"text": "WMT16", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9635080099105835}]}, {"text": "Numbers do not include the 954 workers on Mechanical Turk who did not pass quality control.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.958529531955719}]}, {"text": "7 Numbers include quality control items for workers who passed quality control but omit the additional 151,200 assessments collected on Mechanical Turk where a worker did not pass quality control.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 136, "end_pos": 151, "type": "DATASET", "confidence": 0.8789595663547516}]}, {"text": "can produce the same output fora particular input sentence, we are often able to take advantage of this and use a single assessment for multiple systems.", "labels": [], "entities": []}, {"text": "This year we only combine human assessments in this way if the string of text belonging to multiple systems is exactly identical.", "labels": [], "entities": []}, {"text": "For example, even small differences in punctuation disqualify the potential combination of similar system outputs into a single human assessment, and this is due to lack of evidence about what kinds of minor differences might impact human evaluation.", "labels": [], "entities": []}, {"text": "shows the numbers of segments for which distinct MT systems participating in the News task produced identical outputs.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9390852451324463}]}, {"text": "English to Czech is the only language pair to include systems that do not belong to the news task, the additional NMT Training task systems, and we include a breakdown of duplicate translations by each task for that language pair in.", "labels": [], "entities": [{"text": "NMT Training task", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.7014812231063843}]}, {"text": "The biggest saving in terms of exact duplicate translations for multiple systems was made in the News task for English to German.", "labels": [], "entities": [{"text": "News task", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.8242324888706207}]}, {"text": "One of the main differences between this year's and previous years' tasks is the considerably larger size of human-labelled datasets made available to participants for training.", "labels": [], "entities": []}, {"text": "Whereas the last year we released a corpus of 12, 000 instances (plus 1, 000 and 2, 000 for development and test, respectively), this year this figure was doubled.", "labels": [], "entities": []}, {"text": "In contrast to last year, we also provide datasets for two language pairs.", "labels": [], "entities": []}, {"text": "The structure used for the data have been the same since WMT15.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9563689827919006}]}, {"text": "Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence.", "labels": [], "entities": []}, {"text": "Post-edits are used to extract labels for the different levels of granularity, which allows using the same datasets for all three QE tasks.", "labels": [], "entities": []}, {"text": "The first dataset contains texts in IT domain translated from English into German.", "labels": [], "entities": []}, {"text": "This is a superset of the last year's data: 11, 000 sentences from the same source were added to the training set.", "labels": [], "entities": []}, {"text": "Their translations were produced using the same statistical MT system and post-edited by professional translators who are native speakers of German.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9262239933013916}]}, {"text": "The dataset statistics are outlined in Table 11.", "labels": [], "entities": []}, {"text": "The second dataset belongs to pharmaceutical domain and provides translations from German into English.", "labels": [], "entities": []}, {"text": "It contains 25, 000 instances for training.", "labels": [], "entities": []}, {"text": "Analogously to the IT dataset, automatic translations were generated with a statistical MT system and post-edited by professional translators.", "labels": [], "entities": [{"text": "IT dataset", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.8447698950767517}]}, {"text": "The dataset statistics are shown in  System performance was evaluated by computing the distance between automatic and human postedits of the machine-translated sentences present in the test set (i.e. for each of the 2, 000 target test sentences).", "labels": [], "entities": []}, {"text": "Similar to last year, this distance was measured in terms of TER and BLEU (casesensitive).", "labels": [], "entities": [{"text": "distance", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9882087707519531}, {"text": "TER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9995612502098083}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9991961121559143}]}, {"text": "TERcom 27 software: lower average TER scores correspond to higher ranks.", "labels": [], "entities": [{"text": "TERcom 27 software", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.7540982762972513}, {"text": "TER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9905259609222412}]}, {"text": "BLEU was computed using the multi-bleu.perl package 28 available in MOSES.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9817201495170593}, {"text": "MOSES", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.901326596736908}]}, {"text": "Direct Assessment, which is described in more detail in Section 3, elicits human assessments of translation adequacy on an analogue rating scale  (0-100), where human assessors are asked to rate how adequately the APE system output expresses the meaning of the human reference translation.", "labels": [], "entities": [{"text": "Direct Assessment", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6017717868089676}, {"text": "analogue rating scale", "start_pos": 123, "end_pos": 144, "type": "METRIC", "confidence": 0.9218792915344238}]}, {"text": "DA scores for systems and segments have been shown to be highly repeatable in self-replication experiments ().", "labels": [], "entities": [{"text": "DA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7118456959724426}]}, {"text": "Thus, DA overcomes the previous challenges associated with lack of reliability of human assessment of MT.", "labels": [], "entities": [{"text": "DA", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9676560163497925}, {"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9744582176208496}]}, {"text": "Since we also have a human post-edit available for each MT output in the test set, to make DA outcomes more informative we also included the human post-edits as a hidden system in the evaluation, which will provide some insight into an achievable DA score fora potential system that achieved human-quality post-editing.", "labels": [], "entities": [{"text": "MT output", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8596952557563782}, {"text": "DA", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9561061859130859}]}, {"text": "Additionally, we included the original MT output without any post-editing as a hidden system to discover the baseline DA score for each language pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9349917769432068}]}, {"text": "When running the APE manual evaluation, it was possible in many cases to take advantage of the fact that multiple systems can produce identical outputs, as was begun in evaluation of the News task in WMT15 (.", "labels": [], "entities": [{"text": "APE manual evaluation", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.5301983853181204}, {"text": "News task in WMT15", "start_pos": 187, "end_pos": 205, "type": "DATASET", "confidence": 0.8378607630729675}]}, {"text": "shows numbers of translations in total for all APE systems, as well as savings in terms of annotation effort that was gained by combining identical system outputs prior to running the evaluation, where, as expected, a substantial saving was made due to the fact that the systems quite often produced the same output.", "labels": [], "entities": [{"text": "APE", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.6985083818435669}]}, {"text": "In terms of human effort involved in carrying out the manual evaluation, Table 33 shows numbers of judgments collected in total for each language pair and number of assessments contributing to the final DA score for APE systems on average.", "labels": [], "entities": [{"text": "APE", "start_pos": 216, "end_pos": 219, "type": "TASK", "confidence": 0.8796247839927673}]}, {"text": "When carrying out a manual evaluation of any kind, it is important to consider the consistency of annotators with the aim of estimating, where the evaluation to be repeated, how likely it would be that the same conclusions would be drawn.: Amount of data (assessments after \"de-collapsing\" multi-system outputs) collected in the WMT17 APE manual evaluation campaign and numbers of assessments per system.", "labels": [], "entities": [{"text": "Amount", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.9723382592201233}, {"text": "WMT17 APE manual evaluation campaign", "start_pos": 329, "end_pos": 365, "type": "DATASET", "confidence": 0.850359320640564}]}, {"text": "carried out, where a darker shade of green signifies a lower p-value and a conclusion made with more certainty.", "labels": [], "entities": [{"text": "certainty", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9613184332847595}]}, {"text": "For this language direction, the ranking produced by DA is slightly different from those based on TER/BLEU.", "labels": [], "entities": [{"text": "DA", "start_pos": 53, "end_pos": 55, "type": "DATASET", "confidence": 0.7258235812187195}, {"text": "TER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.8306226134300232}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.6550085544586182}]}, {"text": "This is not surprising if we consider the close performance results measured with automatic metrics.", "labels": [], "entities": []}, {"text": "With primary submissions compressed in a relatively small TER/BLEU interval, different system orders are in fact likely to emerge also from manual evalua- : DE-EN Wilcoxon rank-sum significance test results for pairs of systems competing in the APE task, where a green cell denotes a significant win for the system in a given row over the system in a given column, at p \u2264 0.05. tion.", "labels": [], "entities": [{"text": "TER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9876091480255127}, {"text": "BLEU interval", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9289610683917999}, {"text": "DE-EN Wilcoxon rank-sum significance test", "start_pos": 157, "end_pos": 198, "type": "METRIC", "confidence": 0.7429261803627014}, {"text": "APE task", "start_pos": 245, "end_pos": 253, "type": "TASK", "confidence": 0.8710538744926453}]}, {"text": "Overall, as shown in, three systems emerge as significantly better than the others.", "labels": [], "entities": []}, {"text": "This ranking is comparable to the one obtained with automatic metrics, although the top two systems (FBK and AMU) are switched, but this is in-line with the human evaluation that showed no significant difference between the two.", "labels": [], "entities": [{"text": "FBK", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9071680307388306}, {"text": "AMU", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.7366164922714233}]}, {"text": "This is also in-line with TER/BLEU rankings, for which the three systems are the only primary systems with TER<20.00 and BLEU>69.00.", "labels": [], "entities": [{"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9684767127037048}, {"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.8945319056510925}, {"text": "TER<20.00", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9487403432528178}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9858578443527222}]}, {"text": "In agreement with the BLEU-based ranking, the JXNU submission ranks in fourth position in its own cluster.", "labels": [], "entities": [{"text": "BLEU-based", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9959926009178162}, {"text": "JXNU submission", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.9119104444980621}]}, {"text": "This represents the main difference with the TERbased ranking (in which it occupies the 6th place), which suggests a higher agreement between DA and BLEU.", "labels": [], "entities": [{"text": "TERbased ranking", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.9256986379623413}, {"text": "DA", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.7680183053016663}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9819528460502625}]}, {"text": "The remaining three systems, which feature rather close TER/BLEU scores, are positioned in the same lower cluster, though in a different order, again with small raw DA score differences.", "labels": [], "entities": [{"text": "TER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9969552755355835}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.8071566820144653}, {"text": "DA score", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9164280295372009}]}, {"text": "Apart from these general considerations, which are difficult to project into conclusive indications about the reliability of our two automatic metrics, two major outcomes are evident.", "labels": [], "entities": []}, {"text": "First, the technology advancement with respect to the 2016 round is also confirmed by DA scores, which indicate that all the systems are significantly better than the \"do-nothing\" baseline (NO POST EDIT).", "labels": [], "entities": [{"text": "DA", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9740405678749084}, {"text": "NO POST EDIT)", "start_pos": 190, "end_pos": 203, "type": "METRIC", "confidence": 0.8498268574476242}]}, {"text": "Last year, in contrast, all participants but one were in the same cluster of the baseline.", "labels": [], "entities": []}, {"text": "The downside is that, despite the significant progress made, APE systems are still far from human quality.", "labels": [], "entities": [{"text": "APE", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9707913398742676}]}, {"text": "Average DA scores indicate that the distance between the top primary submissions and human post-edits is in fact similar to the distance that separates them from the primary submissions in the bottom cluster.", "labels": [], "entities": [{"text": "DA", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9454299211502075}]}, {"text": "Also DA scores confirm the higher difficulty of the German-English task.", "labels": [], "entities": [{"text": "DA", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.9983980059623718}, {"text": "difficulty", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9935235977172852}]}, {"text": "As expected, also in this case human quality is much higher, with a gap that is even larger compared to the distance observed in.", "labels": [], "entities": []}, {"text": "Moreover, while in terms of automatic metrics the improvement over the baseline for the top ranked system was statistically significant, the DA-based ranking places the two primary systems in the same cluster of the baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Total segments prior to sampling for manual evaluation and savings made by combining identical segments (Segs)  produced by multiple MT systems in the News (all language pairs) and NMT Training task (English\u2192Czech only).", "labels": [], "entities": [{"text": "News", "start_pos": 161, "end_pos": 165, "type": "DATASET", "confidence": 0.948466420173645}]}, {"text": " Table 4: Amount of data (assessments after removal of quality control items and \"de-collapsing\" multi-system outputs) col- lected in the WMT17 manual evaluation campaign. The final six rows report summary information from previous years of the  workshop. Note how many rankings we get for Czech language pairs; these include systems from the NMT Training shared  task.", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.982548177242279}, {"text": "WMT17 manual evaluation campaign", "start_pos": 138, "end_pos": 170, "type": "DATASET", "confidence": 0.904003381729126}, {"text": "NMT Training shared  task", "start_pos": 343, "end_pos": 368, "type": "DATASET", "confidence": 0.8814015984535217}]}, {"text": " Table 5: we repeat pairs (expecting a similar  judgement), damage MT outputs (expecting sig- nificantly worse scores) and use references instead  of MT outputs (expecting high scores). Bad refer- ence pairs are created automatically by replacing a  phrase within a given translation with a phrase of  the same length randomly selected from n-grams  extracted from the full test set of reference transla-", "labels": [], "entities": []}, {"text": " Table 6: Number of unique Mechanical Turk workers, (A) those whose scores for bad reference pairs were significantly  different and numbers of unique human assessors in (A) whose scores for exact repeat assessments also showed no significant  difference.", "labels": [], "entities": []}, {"text": " Table 7: Official results of WMT17 News translation task. Systems ordered by standardized mean DA score, though systems  within a cluster are considered tied. Lines between systems indicate clusters according to Wilcoxon rank-sum test at p-level  p \u2264 0.05. Systems with gray background indicate use of resources that fall outside the constraints provided for the shared task.", "labels": [], "entities": [{"text": "WMT17 News translation task", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.7745393514633179}, {"text": "standardized mean DA score", "start_pos": 78, "end_pos": 104, "type": "METRIC", "confidence": 0.8359665721654892}]}, {"text": " Table 8: Pearson correlation (r) between overall DA standardized mean adequacy scores collected via crowd-sourcing (Mturk)", "labels": [], "entities": [{"text": "Pearson correlation (r)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.9546700119972229}, {"text": "DA", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.8874295353889465}]}, {"text": " Table 12. The  Table shows another feature of this dataset: it con- tains much fewer errors than the IT one.", "labels": [], "entities": []}, {"text": " Table 12: Statistics of the German-English dataset.", "labels": [], "entities": [{"text": "German-English dataset", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.9134566187858582}]}, {"text": " Table 13: Official results of the WMT17 Quality Estimation Task 1 for the German-English dataset. The winning submission  is indicated by a \u2022 and is statistically significantly different from all others. Submissions in the grey area are those which are  not significantly different from the baseline.", "labels": [], "entities": [{"text": "WMT17 Quality Estimation Task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6570408567786217}, {"text": "German-English dataset", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.8994136452674866}]}, {"text": " Table 14: Official results of the WMT17 Quality Estimation Task 1 for the English-German dataset. The winning submission  is indicated by a \u2022 and is statistically significantly different from all others. Submissions in the grey area are those which are  not significantly different from the baseline.", "labels": [], "entities": [{"text": "WMT17 Quality Estimation Task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6657874882221222}, {"text": "English-German dataset", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.7816453874111176}]}, {"text": " Table 15: Official results of the WMT17 Quality Estimation Task 2 for the German-English dataset. The winning submissions  are indicated by a \u2022 and are statistically significantly different from all others. Submissions in the grey area are those which are  not significantly different from the baseline.", "labels": [], "entities": [{"text": "WMT17 Quality Estimation Task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6595694422721863}, {"text": "German-English dataset", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.9057199358940125}]}, {"text": " Table 16: Official results of the WMT17 Quality Estimation Task 2 for the English-German dataset. The winning submissions  are indicated by a \u2022 and are statistically significantly different from all others. Submissions in the grey area are those which are  not significantly different from the baseline.", "labels": [], "entities": [{"text": "WMT17 Quality Estimation Task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6668809726834297}, {"text": "English-German dataset", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.7776277959346771}]}, {"text": " Table 17: Additional results of the WMT17 Quality Estimation Task 1 for the German-English dataset: using for the word- level predictions for sentence-level QE, evaluated for scoring. The winning submission is indicated by a \u2022 and is statistically  significantly different from all others. Submissions in the grey area are those which are not significantly different from the  baselines. The word-level systems are denoted with prefix word.", "labels": [], "entities": [{"text": "WMT17 Quality Estimation Task", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.6221773475408554}, {"text": "German-English dataset", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.8740914463996887}]}, {"text": " Table 18: Official results for the WMT17 Quality Estimation Task 3 for the German-English data. The winning submission is  indicated by a \u2022 and is statistically significantly different from all others. The gray area indicates the submissions whose results  are not statistically different from the baseline.", "labels": [], "entities": [{"text": "WMT17 Quality Estimation Task", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.6579142734408379}, {"text": "German-English data", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.8574074804782867}]}, {"text": " Table 19: Official results for the WMT17 Quality Estimation Task 3 for the English-German data. The winning submission is  indicated by a \u2022 and is statistically significantly different from all others. The gray area indicates the submissions whose results  are not statistically different from the baseline.", "labels": [], "entities": [{"text": "WMT17 Quality Estimation Task", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.6573218926787376}, {"text": "English-German data", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.7387189716100693}]}, {"text": " Table 20: Comparison of baseline English-German sys- tems trained on WMT16 and WMT17 datasets (tested on the  WMT16 test set) for all tasks.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.9780682921409607}, {"text": "WMT17 datasets", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.9206992387771606}, {"text": "WMT16 test set", "start_pos": 111, "end_pos": 125, "type": "DATASET", "confidence": 0.9813685218493143}]}, {"text": " Table 21: Comparison of official results of WMT17 and WMT16 sentence-level QE task on the English-German WMT16 test  set. The winning submission is indicated by a \u2022 and is statistically significantly different from all others. WMT16 systems are  highlighted with cyan.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.8987312912940979}, {"text": "WMT16 sentence-level QE task", "start_pos": 55, "end_pos": 83, "type": "DATASET", "confidence": 0.6440513283014297}, {"text": "WMT16 test  set", "start_pos": 106, "end_pos": 121, "type": "DATASET", "confidence": 0.8773425221443176}]}, {"text": " Table 22: Comparison of official results of WMT17 and WMT16 word-level QE task on the English-German WMT16 test  set. Winning submissions are indicated by a \u2022 and are statistically significantly different from all others. WMT16 systems are  highlighted with cyan.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.8894826173782349}, {"text": "WMT16 word-level QE task", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.5870122611522675}, {"text": "WMT16 test  set", "start_pos": 102, "end_pos": 117, "type": "DATASET", "confidence": 0.8838801185290018}]}, {"text": " Table 23: Comparison of official results of WMT17 and WMT16 phrase-level QE task on the English-German WMT16 test  set. The winning submission is indicated by a \u2022 and is statistically significantly different from all others. WMT16 systems are  highlighted with cyan.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.9048413634300232}, {"text": "WMT16 phrase-level QE task", "start_pos": 55, "end_pos": 81, "type": "DATASET", "confidence": 0.6342433393001556}, {"text": "WMT16 test  set", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.8811371326446533}]}, {"text": " Table 25: Repetition Rate (RR) of the WMT15 (English-Spanish, news domain, crowdsourced post-edits), WMT16 (English- German, IT domain, professional post-editors), WMT17 EN-DE (English-German, IT domain, professional post-editors) and  WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data.", "labels": [], "entities": [{"text": "Repetition Rate (RR)", "start_pos": 11, "end_pos": 31, "type": "METRIC", "confidence": 0.9729192733764649}, {"text": "WMT15", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9608333110809326}, {"text": "WMT16", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.9541828632354736}, {"text": "WMT17", "start_pos": 165, "end_pos": 170, "type": "DATASET", "confidence": 0.8937686085700989}, {"text": "WMT17 DE-EN", "start_pos": 237, "end_pos": 248, "type": "DATASET", "confidence": 0.8316898345947266}, {"text": "APE task data", "start_pos": 317, "end_pos": 330, "type": "DATASET", "confidence": 0.7727905909220377}]}, {"text": " Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16,  WMT17 EN-DE and WMT17 DE-EN data.", "labels": [], "entities": [{"text": "Translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.8996244072914124}, {"text": "TER", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9840187430381775}, {"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6960718035697937}, {"text": "TER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9915271401405334}, {"text": "WMT15", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.9308422803878784}, {"text": "WMT16", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.6809380650520325}, {"text": "WMT17 DE-EN data", "start_pos": 121, "end_pos": 137, "type": "DATASET", "confidence": 0.8189661701520284}]}, {"text": " Table 28: Results for the WMT17 APE EN-DE task -av- erage TER (\u2193), BLEU score (\u2191). The  \u2020 indicates a difference  from the MT baseline that is not statistically significant.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.48052605986595154}, {"text": "APE EN-DE task -av- erage TER", "start_pos": 33, "end_pos": 62, "type": "METRIC", "confidence": 0.8314159139990807}, {"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9850910604000092}, {"text": "MT baseline", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.7166659832000732}]}, {"text": " Table 29: Results for the WMT17 APE DE-EN task -av- erage TER (\u2193), BLEU score (\u2191). The  \u2020 indicates differences  from the MT baseline that are not statistically significant.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.45997095108032227}, {"text": "APE DE-EN task -av- erage TER", "start_pos": 33, "end_pos": 62, "type": "METRIC", "confidence": 0.8521723002195358}, {"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9851819574832916}, {"text": "MT baseline", "start_pos": 123, "end_pos": 134, "type": "DATASET", "confidence": 0.7714741230010986}]}, {"text": " Table 30: Number of test sentences modified, improved and deteriorated by each run submitted to the EN-DE task.", "labels": [], "entities": []}, {"text": " Table 31: Number of test sentences modified, improved and deteriorated by each run submitted to the DE-EN task.", "labels": [], "entities": []}, {"text": " Table 32: Total segments prior to sampling for manual eval- uation and savings made by combining identical segments  (Segs) produced by multiple APE systems.", "labels": [], "entities": []}, {"text": " Table 33: Amount of data (assessments after \"de-collapsing\"  multi-system outputs) collected in the WMT17 APE manual  evaluation campaign and numbers of assessments per system.", "labels": [], "entities": [{"text": "Amount", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9600999355316162}, {"text": "WMT17 APE manual  evaluation campaign", "start_pos": 101, "end_pos": 138, "type": "DATASET", "confidence": 0.8439483523368836}]}, {"text": " Table 35: EN-DE DA Human evaluation results showing  average raw DA scores (Ave %) and average standardized  scores (Ave z), lines between systems indicate clusters ac- cording to Wilcoxon rank-sum test at p-level p \u2264 0.05.", "labels": [], "entities": [{"text": "EN-DE DA", "start_pos": 11, "end_pos": 19, "type": "TASK", "confidence": 0.5091800838708878}, {"text": "standardized  scores (Ave z)", "start_pos": 96, "end_pos": 124, "type": "METRIC", "confidence": 0.8756273488203684}]}, {"text": " Table 36: DE-EN DA Human evaluation results showing  average raw DA scores (Ave %) and average standardized  scores (Ave z), lines between systems indicate clusters ac- cording to Wilcoxon rank-sum test at p-level p \u2264 0.05.", "labels": [], "entities": [{"text": "DE-EN DA", "start_pos": 11, "end_pos": 19, "type": "TASK", "confidence": 0.4450046569108963}, {"text": "standardized  scores (Ave z)", "start_pos": 96, "end_pos": 124, "type": "METRIC", "confidence": 0.8765491545200348}]}]}