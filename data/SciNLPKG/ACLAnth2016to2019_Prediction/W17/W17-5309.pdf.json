{"title": [{"text": "Character-level Intra Attention Network for Natural Language Inference", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.686024526755015}]}], "abstractContent": [{"text": "Natural language inference (NLI) is a central problem in language understanding.", "labels": [], "entities": [{"text": "Natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7812402298053106}, {"text": "language understanding", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7577974498271942}]}, {"text": "End-to-end artificial neural networks have reached state-of-the-art performance in NLI field recently.", "labels": [], "entities": []}, {"text": "In this paper, we propose Character-level Intra Attention Network (CIAN) for the NLI task.", "labels": [], "entities": []}, {"text": "In our model, we use the character-level convolutional network to replace the standard word embedding layer, and we use the intra attention to capture the intra-sentence semantics.", "labels": [], "entities": []}, {"text": "The proposed CIAN model provides improved results based on a newly published MNLI corpus.", "labels": [], "entities": [{"text": "MNLI corpus", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.978867918252945}]}], "introductionContent": [{"text": "Natural language inference in natural language processing refers to the problem of determining a directional relation between two text fragments.", "labels": [], "entities": [{"text": "Natural language inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6887644529342651}, {"text": "natural language processing", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6098056534926096}]}, {"text": "Given a sentence pair (premise, hypothesis), the task is to predict whether hypothesis is entailed by premise, hypothesis is contradicted to premise, or whether the relation between premise and hypothesis is neutral.", "labels": [], "entities": []}, {"text": "Recently, the dominating trend of works in natural language processing is based on artificial neural networks, which aims at building deep and complex encoder to transform a sentence into encoded vectors.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6975852847099304}]}, {"text": "For instance, there are recurrent neural network (RNN) based encoders, which recursively concatenate each word with its previous memory, until the whole information of a sentence has been derived.", "labels": [], "entities": []}, {"text": "The most common RNN encoders are Long Short-Term Memory Networks (LSTM; Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit ().", "labels": [], "entities": []}, {"text": "RNNs have surpassed the performance of traditional baselines in many NLP tasks.", "labels": [], "entities": []}, {"text": "There are also convolutional neural network (CNN;) based encoders, which concatenate the sentence information by applying multiple convolving filters over the sentence.", "labels": [], "entities": []}, {"text": "CNNs have achieved state-of-the-art results on various NLP tasks.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the NLI model, the Stanford Natural Language Inference (SNLI;) corpus of 570K sentence pairs was introduced.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI;) corpus", "start_pos": 46, "end_pos": 96, "type": "DATASET", "confidence": 0.7484487891197205}]}, {"text": "It serves as a standard benchmark for NLI task.", "labels": [], "entities": []}, {"text": "However, most of the sentences in SNLI corpus are short and simple, which limit the room for fine-grained comparisons between models.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.865311324596405}]}, {"text": "Currently, a more comprehensive Multi-Genre NLI corpus (MNLI;) of 433K sentence pairs was released, aiming at evaluating large-scale NLI models.", "labels": [], "entities": []}, {"text": "Authors gave out some baseline results accompanied by the publish of MNLI corpus, the BiLSTM model achieves an accuracy of 67.5, and the Enhanced Sequential Inference Model ( achieves an accuracy of 72.4.", "labels": [], "entities": [{"text": "MNLI corpus", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9673498868942261}, {"text": "BiLSTM", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.7709690928459167}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9983624815940857}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9963653087615967}]}, {"text": "Among those encoders for NLI task, most of them use word-level embedding, and initialize the weight of the embedding layer with pre-trained word vectors such as GloVe ().", "labels": [], "entities": []}, {"text": "The pre-trained word vectors helps the encoders to catch richer semantic information.", "labels": [], "entities": []}, {"text": "However, it also has its downside.", "labels": [], "entities": []}, {"text": "As the growth of vocabulary size in the modern corpus, there will be more and more out-of-vocabulary (OOV) words that are not presented in the pre-trained word embedding vector.", "labels": [], "entities": []}, {"text": "As the word-level embedding is blind to subword information (e.g. morphemes), it leads to high perplexities for those OOV words.", "labels": [], "entities": []}, {"text": "In this paper, we use the BiLSTM model from () as the baseline model for the evaluation of the MNLI corpus.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.7393248081207275}, {"text": "MNLI corpus", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.9459174871444702}]}, {"text": "To augment the baseline model, firstly, a character-level convolutional neural network () is applied.", "labels": [], "entities": []}, {"text": "We use the CharCNN to replace the word embedding layer in the baseline model, which will be computed from the characters of corresponding word.", "labels": [], "entities": [{"text": "CharCNN", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.6122980117797852}]}, {"text": "Secondly, the intra attention mechanism introduced by) will be applied, to enhance the model with a richer information of substructures of a sentence.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test set accuracies (%) on MNLI corpus.", "labels": [], "entities": [{"text": "MNLI corpus", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9459414482116699}]}, {"text": " Table 2: Accuracies (%) on matched and mis- matched expert-tagged development data.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9992387294769287}]}]}