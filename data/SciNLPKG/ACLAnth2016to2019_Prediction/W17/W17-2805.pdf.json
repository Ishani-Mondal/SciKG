{"title": [{"text": "Natural Language Grounding and Grammar Induction for Robotic Manipulation Commands", "labels": [], "entities": [{"text": "Robotic Manipulation Commands", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.6329695185025533}]}], "abstractContent": [{"text": "We present a cognitively plausible system capable of acquiring knowledge in language and vision from pairs of short video clips and linguistic descriptions.", "labels": [], "entities": []}, {"text": "The aim of this work is to teach a robot manipula-tor how to execute natural language commands by demonstration.", "labels": [], "entities": []}, {"text": "This is achieved by first learning a set of visual 'concepts' that abstract the visual feature spaces into concepts that have human-level meaning.", "labels": [], "entities": []}, {"text": "Second, learning the mapping/grounding between words and the extracted visual concepts.", "labels": [], "entities": []}, {"text": "Third, inducing grammar rules via a semantic representation known as Robot Control Language (RCL).", "labels": [], "entities": []}, {"text": "We evaluate our approach against state-of-the-art supervised and unsupervised grounding and grammar induction systems, and show that a robot can learn to execute never seen-before commands from pairs of unlabelled linguistic and visual inputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding natural language commands is essential for robotic systems to naturally and effectively interact with humans.", "labels": [], "entities": []}, {"text": "In this paper, we present a framework for learning the linguistic and visual components needed to enable a robot manipulator of executing new natural language commands in a table-top environment.", "labels": [], "entities": []}, {"text": "The learning is divided into three steps: (i) learning of visual concepts, (ii) mapping the words to the extracted visual concepts (i.e. language grounding), and (iii) inducing grammar rules to model the natural language sentences.", "labels": [], "entities": []}, {"text": "Our system updates its knowledge in language and vision incrementally, by processing a pair of inputs at a time.", "labels": [], "entities": []}, {"text": "The input to our system consists of a short video clip of a robot performing a single action, e.g. a pickup or a move action, paired with a natural language command corresponding to the action in the video.", "labels": [], "entities": []}, {"text": "The natural language commands were collected from volunteers and online crowd-sourcing tools such as Amazon Mechanical Turk with minimal amount of supervision or constraints on the language structure which annotators could use.", "labels": [], "entities": []}, {"text": "Generally, supervised language grounding and grammar induction systems learn from sentences that have been manually annotated by a human expert.", "labels": [], "entities": []}, {"text": "As shown in, each word gets annotated with a semantic category (e.g. colour, shape, etc.), and the grammar structure gets annotated using a tree that connects the different words together (e.g. RCL trees) as presented by and.", "labels": [], "entities": []}, {"text": "The manual annotation of data is a labour intensive task that hinders learning from large corpora, and such labels are not necessarily available for all languages.", "labels": [], "entities": []}, {"text": "Therefore, unsupervised grounding and grammar induction systems learn language models from unlabelled/raw linguistic data by exploiting co-occurrences of words in a corpus, which generally performs poorly.", "labels": [], "entities": []}, {"text": "Therefore, in this work we take a different approach to learn words meanings and grammar rules by connecting natural language to extracted visual features from video clips.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of our system using two datasets; a synthetic-world dataset, and anew, simplified real-world dataset of table-top environment.", "labels": [], "entities": []}, {"text": "For the synthetic-world, we use the Train Robots dataset 1 which was designed to develop systems capable of understanding verbal spatial commands described in a natural way.", "labels": [], "entities": [{"text": "Train Robots dataset 1", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.7232315540313721}]}, {"text": "Non-expert users from Amazon Mechanical Turk were asked to annotate appropriate natural language commands to 1000 different scenes.", "labels": [], "entities": []}, {"text": "A total of 4850 commands were collected and later annotated by human experts with appropriate RCL trees.", "labels": [], "entities": [{"text": "RCL trees", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.8463857769966125}]}, {"text": "Examples from this synthetic dataset are shown in.", "labels": [], "entities": []}, {"text": "For the real-world setup, we use a Baxter robot as our test platform and attach a Microsoft Kinect2 sensor to its chest as shown in.", "labels": [], "entities": []}, {"text": "The Kinect2 Train Robots: http://doi.org/10.5518/32 device is used to collect RGBD videos as volunteers controlled the robot arm to perform various manipulation tasks with real objects from the robot's point of view.", "labels": [], "entities": []}, {"text": "The dataset consists of 204 videos with 17, 373 frames in total.", "labels": [], "entities": []}, {"text": "The videos are annotated with 1024 natural language commands (5 per video in average) by a separate group of volunteers 2 . A total of 51 different objects are manipulated in the videos such as basic block shapes, fruit, cutlery, and office supplies.", "labels": [], "entities": []}, {"text": "A detailed description of both datasets is presented in 2.: Number of concepts in A-colour, B-shape, C-location, D-direction, E-distance, and F-action features in both datasets, and G-average number of objects present in each scene.", "labels": [], "entities": [{"text": "G-average", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9834612607955933}]}, {"text": "We evaluated the performance of our technique using two metrics: (i) the ability to correctly ground words to the learned visual concepts using \u03a6, and (ii) the ability to correctly parse previously unseen natural language commands to produce correct RCL trees using the learned grammar G.", "labels": [], "entities": []}, {"text": "To better demonstrate our results in language grounding and grammar induction, we compare our technique with (1) a supervised system that learns from labelled data, and with (2) an unsupervised system that learns from unlabelled linguistic data.", "labels": [], "entities": [{"text": "language grounding", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7552317082881927}, {"text": "grammar induction", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.730581521987915}]}, {"text": "We consider our baseline as the performance of the unsupervised system, i.e. our joint language and vision technique should outperform the unsupervised system that learns from unlabelled linguistic inputs, otherwise there is no benefit of the additional vision component.", "labels": [], "entities": []}, {"text": "On the other hand, an upper bound on performance is the results of the supervised system trained on human labelled (ground-truth) data.", "labels": [], "entities": []}, {"text": "In this section, we evaluate the system's ability to acquire correct groundings for words from parallel pairs of short video clips and linguistic descriptions.", "labels": [], "entities": []}, {"text": "The given task is to learn the partial function \u03a6 : W \u2192 C that maps words w i \u2208 W to their corresponding clusters c j \u2208 C, e.g. the word 'red' should be mapped to the cluster colour-red.", "labels": [], "entities": []}, {"text": "The results for our language grounding experiment are shown in.", "labels": [], "entities": [{"text": "language grounding", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7570500075817108}]}, {"text": "Here, 'our-system' is compared against (1) the supervised semantic tagger) that is trained on human labelled data, and (2) the unsupervised semantic tagger) that is trained on unlabelled linguistic data.", "labels": [], "entities": []}, {"text": "The results are calculated based on the total number of correct tags/groundings assigned to each word in the test fold (four fold cross validation).", "labels": [], "entities": []}, {"text": "Note that for the unsupervised system, the results are calculated based on its ability to cluster words that belong to the same category together, i.e. words that describe colours should be given a unique tag different to those that describe shapes, directions, etc.", "labels": [], "entities": []}, {"text": "Also, we assign new words in the test fold (words that only exist in the test fold) with a function word tag.: The grounding results of (a) supervised, (b) our system, and (c) unsupervised semantic taggers, on both datasets.", "labels": [], "entities": []}, {"text": "Our system is able to correctly ground (85.6%) of the total words in the synthetic, and (81.5%) in the real-world datasets, compared to only (32.9% and 31.2% respectively) using the unsupervised system.", "labels": [], "entities": []}, {"text": "This clearly shows that adding vision inputs produces more correct semantic representations for words, even though both systems use unlabelled data for learning.", "labels": [], "entities": []}, {"text": "Detailed analysis of how the different techniques performed in each feature space is shown in.", "labels": [], "entities": []}, {"text": "Note that distance is not a feature in the synthetic dataset and therefore the corresponding row/column are left empty.", "labels": [], "entities": [{"text": "distance", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.94562166929245}]}, {"text": "In this experiment we test our system's ability to acquire correct grammar rules G from pairs of video clips and unlabelled sentences (with no humanannotated RCL trees).", "labels": [], "entities": []}, {"text": "The learned grammar G is then used to parse new (previously unseen) natural language commands.", "labels": [], "entities": []}, {"text": "We compare our technique with (1) a supervised parser) trained on labelled data, i.e. pairs of sentences and human-annotated RCL trees, and (2) an unsupervised parser () trained on unlabelled sentences, i.e. a corpus of sentences without RCL trees or semantic tags.", "labels": [], "entities": []}, {"text": "The results for (a) our approach, (b) the supervised parser, and (c) the unsupervised grammar induction systems on both datasets are shown in.", "labels": [], "entities": []}, {"text": "The results were calculated based on the number of correctly parsed RCL trees from sentences in the test fold (in the four-fold cross validation).", "labels": [], "entities": []}, {"text": "A score of 1 is given if the parsed sentence completely matches the human annotation, while a partial score in (0, 1) is given if it partially matches the human annotation.", "labels": [], "entities": []}, {"text": "The partial matching is computed by matching subtrees in the both trees divided by the total number of subtrees.", "labels": [], "entities": []}, {"text": "For example, if a tree contains 10 subtrees and only 8 of which has a complete match in labels and links, then we give a score of 0.8 to this tree.", "labels": [], "entities": []}, {"text": "The results in clearly show that our approach outperforms the unsupervised grammar induction system and achieves comparable results to the supervised system by learning from both language and vision as opposed to learning from language alone.", "labels": [], "entities": []}, {"text": "The number of grammar rules generated differs between techniques: our approach generated (139 and 87) grammar rules from the synthetic and real-world datasets respectively, while the supervised system generated (182 and 114) and the unsupervised system generated (45 and 38) grammar rules, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of concepts in A-colour, B-shape,  C-location, D-direction, E-distance, and F-action  features in both datasets, and G-average number of  objects present in each scene.", "labels": [], "entities": [{"text": "G-average", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9720261693000793}]}]}