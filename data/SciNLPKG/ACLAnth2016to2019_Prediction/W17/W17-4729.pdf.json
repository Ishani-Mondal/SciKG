{"title": [], "abstractContent": [{"text": "We describe the neural machine translation system submitted by the University of Rochester to the Chinese-English language pair for the WMT 2017 news translation task.", "labels": [], "entities": [{"text": "WMT 2017 news translation task", "start_pos": 136, "end_pos": 166, "type": "TASK", "confidence": 0.7596074223518372}]}, {"text": "We applied unsuper-vised word and subword segmentation techniques and deep learning in order to address (i) the word segmentation problem caused by the lack of delimiters between words and phrases in Chinese and (ii) the morphological and syntactic differences between Chinese and English.", "labels": [], "entities": [{"text": "word and subword segmentation", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.7244767397642136}, {"text": "word segmentation", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7092045843601227}]}, {"text": "We integrated promising recent developments in NMT, including back-translations, language model reranking, subword splitting and minimum risk tuning.", "labels": [], "entities": [{"text": "NMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9052191376686096}, {"text": "language model reranking", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.6274779339631399}, {"text": "subword splitting", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.8410913646221161}, {"text": "minimum risk tuning", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.5674313406149546}]}], "introductionContent": [{"text": "This paper presents the machine translation (MT) systems submitted by University of Rochester to the WMT 2017 news translation task.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.81757093667984}, {"text": "WMT 2017 news translation task", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.7706474900245667}]}, {"text": "We participated in the Chinese-to-English and Latvian-toEnglish news translation tasks, but will focus on describing the system submitted for the Chineseto-English task.", "labels": [], "entities": [{"text": "Latvian-toEnglish news translation", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.5513834555943807}]}, {"text": "Chinese-to-English is a particularly challenging language pair for corpus-based MT systems due to the task of finding an optimal word segmentation for Chinese sentences as well as other linguistic differences between Chinese and English sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9524897933006287}]}, {"text": "For example the fact that there may exist multiple possible meanings for characters depending on their context and that individual characters can be joined together to build compound words exacerbate the aforementioned segmentation problem.", "labels": [], "entities": []}, {"text": "Additionally, translation performance is also affected by the frequent dropping of subjects and infrequent use of function words in Chinese sentences.", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9809682965278625}]}, {"text": "We used both word-level and morphological feature-based representations of Chinese to deal with data sparsity and reduce the size of the Chinese vocabulary.", "labels": [], "entities": []}, {"text": "We experimented with both subphrase-based and character-based systems.", "labels": [], "entities": []}, {"text": "Both RNN-based and 5-gram language models were trained with data extracted from the English news corpora provided and are used to rerank hypotheses proposed by the decoder.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: in Section 2 we introduce our system and preprocessing methods for the Chinese language.", "labels": [], "entities": []}, {"text": "Our main learning framework training settings are explained in Section 3.", "labels": [], "entities": []}, {"text": "Our NMT, SMT, and submission results are presented in Section 4.", "labels": [], "entities": [{"text": "NMT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.511400580406189}, {"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9422253966331482}]}, {"text": "The paper ends with some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the details of the experimental settings for our system.", "labels": [], "entities": []}, {"text": "We compared the performance of our system to several state-of-the-art algorithms.", "labels": [], "entities": []}, {"text": "Our systems seen that our system outperformed the baselines, whether using words or subwords as the input tokens.", "labels": [], "entities": []}, {"text": "The experiments also showed that the raremorpheme algorithm significantly reduced some potential overfitting, compared to the characterlevel BiRNN.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test Results. Uncased BLEU scores of  the trained models computed over all sentences on  the development and test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9460381269454956}]}]}