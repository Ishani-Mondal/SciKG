{"title": [{"text": "A Simple and Strong Baseline: NAIST-NICT Neural Machine Translation System for WAT2017 English-Japanese Translation Task", "labels": [], "entities": [{"text": "NAIST-NICT Neural Machine Translation", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.8512669801712036}, {"text": "WAT2017 English-Japanese Translation", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.8986627260843912}]}], "abstractContent": [{"text": "This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task.", "labels": [], "entities": [{"text": "NAIST-NICT machine translation", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.8302181363105774}, {"text": "WAT2017 English-Japanese Scientific Paper Translation Task", "start_pos": 85, "end_pos": 143, "type": "TASK", "confidence": 0.8428647617499033}]}, {"text": "The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model.", "labels": [], "entities": []}, {"text": "According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture.", "labels": [], "entities": [{"text": "translation", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9374483823776245}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8859326243400574}]}], "introductionContent": [{"text": "Neural machine translation (NMT) methods became one of the main-stream techniques in current machine translation studies.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7944751679897308}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7240340411663055}]}, {"text": "Previous WAT campaign showed that NMT methods can achieve higher translation accuracy in spite of simple model configurations (.", "labels": [], "entities": [{"text": "WAT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.8278631567955017}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.7827481031417847}]}, {"text": "In this year, we chose the NMT architecture as our translation systems submitted for WAT2017 English-Japanese Scientific Paper Translation Task (.", "labels": [], "entities": [{"text": "WAT2017 English-Japanese Scientific Paper Translation Task", "start_pos": 85, "end_pos": 143, "type": "TASK", "confidence": 0.6595065842072169}]}, {"text": "The main translation model is constructed by an encoder-decoder model enforced by an attention mechanism (.", "labels": [], "entities": []}, {"text": "This paper describes the details of our system, including whole model architecture, training criteria, decoding strategy, and data preparation.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 126, "end_pos": 142, "type": "TASK", "confidence": 0.6845400184392929}]}, {"text": "Results show that our system achieves higher translation accuracy than any systems submitted in previous WAT campaigns.", "labels": [], "entities": [{"text": "translation", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.9353267550468445}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8574850559234619}, {"text": "WAT campaigns", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.9242545664310455}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: JPO adequacy results.", "labels": [], "entities": [{"text": "JPO", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.48115554451942444}]}]}