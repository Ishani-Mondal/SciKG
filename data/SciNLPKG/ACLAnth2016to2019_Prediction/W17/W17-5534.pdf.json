{"title": [{"text": "Adversarial Evaluation for Open-Domain Dialogue Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the potential of adversarial evaluation methods for open-domain dialogue generation systems, comparing the performance of a discriminative agent to that of humans on the same task.", "labels": [], "entities": [{"text": "open-domain dialogue generation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6518704096476237}]}, {"text": "Our results show that the task is hard, both for automated models and humans, but that a discriminative agent can learn patterns that lead to above-chance performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "End-to-end dialogue response generation systems trained to produce a plausible utterance given some limited dialogue context are receiving increased attention.", "labels": [], "entities": [{"text": "dialogue response generation", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.7802056074142456}]}, {"text": "However, for systems dealing with chatbot-style open-dialogue, where task completion is not applicable, evaluating the quality of their responses remains a challenge.", "labels": [], "entities": []}, {"text": "Most current models are evaluated with measures such as perplexity and overlapbased metrics like BLEU, that compare the generated response to the ground-truth response in an actual dialogue.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9939714074134827}]}, {"text": "This kind of measures, however, correlate very weakly or not at all with human judgements on response quality (.", "labels": [], "entities": []}, {"text": "In this paper, we explore a different approach to evaluating open-domain dialogue response generation systems, inspired by the classic Turing Test: measuring the quality of the generated responses on their indistinguishability from human output.", "labels": [], "entities": [{"text": "open-domain dialogue response generation", "start_pos": 61, "end_pos": 101, "type": "TASK", "confidence": 0.6316222175955772}]}, {"text": "This approach has been preliminary explored in recent work under the heading of adversarial evaluation, drawing a parallel with generative adversarial learning ().", "labels": [], "entities": [{"text": "generative adversarial learning", "start_pos": 128, "end_pos": 159, "type": "TASK", "confidence": 0.8239461779594421}]}, {"text": "Here we concentrate on exploring the potential and the limits of such an adversarial evaluation approach by conducting an in-depth analysis.", "labels": [], "entities": []}, {"text": "We implement a discriminative model and train it on the task of distinguishing between actual and \"fake\" dialogue excerpts and evaluate its performance, as well as the feasibility of the task more generally, by conducting an experiment with human judgements.", "labels": [], "entities": []}, {"text": "Results show that the task is hard not only for the discriminative model, but also for human judges.", "labels": [], "entities": []}, {"text": "We then implement a simple chatbot agent for dialogue generation and test the discriminator on this data, again comparing its performance to that of humans on this task.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8689211010932922}]}, {"text": "We show that both humans and the discriminative model can be fooled by the generator in a significant amount of cases.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the performance of our discriminative model, we conduct an experiment with human annotators.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first study of its kind ever conducted.", "labels": [], "entities": []}, {"text": "Previous human evaluation experiments of dialogue generation systems have mostly consisted in asking participants to choose the better response between two options generated by different models or to rate a generated dialogue along several dimensions (.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7975180745124817}]}, {"text": "In contrast, here we present humans with the same task faced by the discriminator: We show them a dialogue passage and ask them to decide whether, given the first one or two utterances of context, the shown continuation is the actual follow-up utterance in the original dialogue or a random response.", "labels": [], "entities": []}, {"text": "The data for this experiment consists of 900 pas-: Accuracy, Precision, Recall, and F-score of discriminator and humans against ground-truth.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9988511800765991}, {"text": "Precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9959893822669983}, {"text": "Recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9922824501991272}, {"text": "F-score", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9996902942657471}]}, {"text": "Inter-annotator agreement among humans and between the discriminator and the human majority class.", "labels": [], "entities": []}, {"text": "sages: 300 randomly selected per dataset, with 50% real and 50% fake dialogues.", "labels": [], "entities": []}, {"text": "We use the CrowdFlower platform to recruit annotators, restricting the pool to English native speakers.", "labels": [], "entities": []}, {"text": "Each item is classified as real or random by three different annotators.", "labels": [], "entities": []}, {"text": "A total of 137 annotators participated in the experiment, with each of them annotating between 10 and 150 items.", "labels": [], "entities": []}, {"text": "We test the discriminator on the same data and compare its performance to the human judgements.", "labels": [], "entities": []}, {"text": "Chance level accuracy for both humans and the discriminator is 50%, namely when real and fake passages are indistinguishable from each other.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.7297557592391968}]}, {"text": "The results are summarised in.", "labels": [], "entities": []}, {"text": "Let us first consider the performance of humans on the task.", "labels": [], "entities": []}, {"text": "We compute inter-annotator agreement using Fleiss \u03c0, suitable for assessing multi-coder annotation tasks.", "labels": [], "entities": [{"text": "Fleiss \u03c0", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9738805592060089}]}, {"text": "Agreement is low: \u03c0 = 0.30 across the 3 corpora, indicating that the task is challenging for humans (there is limited consensus on whether the shown dialogue passages are plausible or not).", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9931656718254089}]}, {"text": "Looking into the human performance with respect to the ground truth, we see similar accuracy scores for Switchboard and MovieTriples, while accuracy is lower for SubTle, where the context consists of one utterance only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9994363188743591}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9994282126426697}]}, {"text": "Across the three datasets, we observe slightly higher F-score for positive instances (real) than negative instances (random).", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9995144605636597}]}, {"text": "For the positive instances, recall is higher than precision, while the opposite is true for negative instances.", "labels": [], "entities": [{"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9998087286949158}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9996689558029175}]}, {"text": "Arguably, this indicates that humans tend to accommodate responses that in fact are random as possible coherent continuations of a dialogue, and will only flag them as fake if they are utterly surprising.", "labels": [], "entities": []}, {"text": "We compute the agreement of the discriminator's predictions and the human majority class over 3 annotators.", "labels": [], "entities": [{"text": "agreement", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9813740849494934}]}, {"text": "For Switchboard, agreement is at chance level (\u03c0 = .07), while for the other two", "labels": [], "entities": [{"text": "agreement", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.991089940071106}, {"text": "chance", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9755282998085022}]}], "tableCaptions": [{"text": " Table 1: Accuracy, Precision, Recall, and F-score of discriminator and humans against ground-truth.  Inter-annotator agreement among humans and between the discriminator and the human majority class.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991509914398193}, {"text": "Precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9968331456184387}, {"text": "Recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.994738757610321}, {"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.999632716178894}]}, {"text": " Table 2: Performance of discriminator and humans against ground-truth for generator experiment. Inter- annotator agreement among humans and between the discriminator and the human majority class.", "labels": [], "entities": []}]}