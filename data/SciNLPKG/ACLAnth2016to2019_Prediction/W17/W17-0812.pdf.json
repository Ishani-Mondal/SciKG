{"title": [{"text": "The BECauSE Corpus 2.0: Annotating Causality and Overlapping Relations", "labels": [], "entities": [{"text": "BECauSE Corpus 2.0", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8898083170255026}]}], "abstractContent": [{"text": "Language of cause and effect captures an essential component of the semantics of a text.", "labels": [], "entities": []}, {"text": "However, causal language is also intertwined with other semantic relations, such as temporal precedence and correlation.", "labels": [], "entities": []}, {"text": "This makes it difficult to determine when causation is the primary intended meaning.", "labels": [], "entities": []}, {"text": "This paper presents BECauSE 2.0, anew version of the BECauSE corpus with exhaustively annotated expressions of causal language, but also seven semantic relations that are frequently co-present with causation.", "labels": [], "entities": [{"text": "BECauSE corpus", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9135677218437195}]}, {"text": "The new corpus shows high inter-annotator agreement, and yields insights both about the linguistic expressions of causation and about the process of annotating co-present semantic relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "We understand our world in terms of causal networks -phenomena causing, enabling, or preventing others.", "labels": [], "entities": []}, {"text": "Accordingly, the language we use is full of references to cause and effect.", "labels": [], "entities": []}, {"text": "In the Penn Discourse Treebank (PDTB;, for example, over 12% of explicit discourse connectives are marked as causal, as are nearly 26% of implicit discourse relationships.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB;", "start_pos": 7, "end_pos": 37, "type": "DATASET", "confidence": 0.937700112660726}]}, {"text": "Recognizing causal assertions is thus invaluable for semanticsoriented applications, particularly in domains such as finance and biology where interpreting these assertions can help drive decision-making.", "labels": [], "entities": [{"text": "Recognizing causal assertions", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8278269171714783}]}, {"text": "In addition to being ubiquitous, causation is often co-present with related meanings such as temporal order (cause precedes effect) and hypotheticals (the if causes the then).", "labels": [], "entities": []}, {"text": "This paper presents the Bank of Effects and Causes Stated Explicitly (BECauSE) 2.0, which offers insight into these overlaps.", "labels": [], "entities": [{"text": "Bank of Effects and Causes Stated Explicitly (BECauSE) 2.0", "start_pos": 24, "end_pos": 82, "type": "TASK", "confidence": 0.5546356385404413}]}, {"text": "As in BECauSE 1.0 (), the corpus contains annotations for causal language.", "labels": [], "entities": [{"text": "BECauSE 1.0", "start_pos": 6, "end_pos": 17, "type": "DATASET", "confidence": 0.7603902518749237}]}, {"text": "It also includes annotations for seven commonly co-present meanings when they are expressed using constructions shared with causality.", "labels": [], "entities": []}, {"text": "To deal with the wide variation in linguistic expressions of causation (see Neeleman and Van de Koot, 2012;, BECauSE draws on the principles of Construction Grammar (CxG;.", "labels": [], "entities": [{"text": "BECauSE", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.7965924739837646}]}, {"text": "CxG posits that the fundamental units of language are constructions -pairings of meanings with arbitrarily simple or complex linguistic forms, from morphemes to structured lexico-syntactic patterns.", "labels": [], "entities": [{"text": "CxG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8779364228248596}]}, {"text": "Accordingly, BECauSE admits arbitrary constructions as the bearers of causal relationships.", "labels": [], "entities": [{"text": "BECauSE", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.7478093504905701}]}, {"text": "As long as there is at least one fixed word, any conventionalized expression of causation can be annotated.", "labels": [], "entities": []}, {"text": "By focusing on causal language -conventionalized expressions of causation -rather than real-world causation, BECauSE largely sidesteps the philosophical question of what is truly causal.", "labels": [], "entities": []}, {"text": "It is not concerned, for instance, with whether there is a real-world causal relationship within flu virus (virus causes flu) or delicious bacon pizza (bacon causes deliciousness); neither is annotated.", "labels": [], "entities": []}, {"text": "Nonetheless, some of the same overlaps and ambiguities that make real-world causation so hard to circumscribe seep into the linguistic domain, as well.", "labels": [], "entities": []}, {"text": "Consider the following examples (with causal constructions in bold, CAUSES in small caps, and effects in italics): (1) After I DRANK SOME WATER, I felt much better.", "labels": [], "entities": [{"text": "I DRANK SOME WATER", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.3913772851228714}]}, {"text": "Each sentence conveys a causal relation, but piggybacks it on a related relation type.", "labels": [], "entities": []}, {"text": "(1) uses a temporal relationship to suggest causality.", "labels": [], "entities": []}, {"text": "(3) employs a correlative construction, and (2) contains elements of both time and correlation in addition to causation.", "labels": [], "entities": []}, {"text": "(4), meanwhile, is framed as bringing something into existence, and (5) suggests both permission and enablement.", "labels": [], "entities": []}, {"text": "Most semantic annotation schemes have required that each token be assigned just one meaning.", "labels": [], "entities": []}, {"text": "BECauSE 1.0 followed this policy, as well, but this resulted in inconsistent handling of cases like those above.", "labels": [], "entities": [{"text": "BECauSE 1.0", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9527880847454071}]}, {"text": "For example, the meaning of let varies from \"allow to happen\" (clearly causal) to \"verbalize permission\" (not causal) to shades of both.", "labels": [], "entities": []}, {"text": "These overlaps made it difficult for annotators to decide when to annotate such cases as causal.", "labels": [], "entities": []}, {"text": "The contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, we present anew version of the BECauSE corpus, which offers several improvements over the original.", "labels": [], "entities": [{"text": "BECauSE corpus", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.934790849685669}]}, {"text": "Most importantly, the updated corpus includes annotations for seven different relation types that overlap with causality: temporal, correlation, hypothetical, obligation/permission, creation/termination, extremity/sufficiency, and context.", "labels": [], "entities": []}, {"text": "Overlapping relations are tagged for any construction that can also be used to express a causal relationship.", "labels": [], "entities": []}, {"text": "The improved scheme yields high inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Second, using the new corpus, we derive intriguing evidence about how meanings compete for linguistic machinery.", "labels": [], "entities": []}, {"text": "Finally, we discuss the issues that the annotation approach does and does not solve.", "labels": [], "entities": []}, {"text": "Our observations suggest lessons for future annotation projects in semantic domains with fuzzy boundaries between categories.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Inter-annotator agreement for the new version of  BECauSE. \u03ba indicates Cohen's kappa; J indicates the average  Jaccard index, a measure of span overlap; and % indicates per- cent agreement of exact matches. Each \u03ba and argument score  was calculated only for instances with matching connectives.  An argument's head was determined automatically by pars- ing the sentence with version 3.5.2 of the Stanford Parser  (", "labels": [], "entities": [{"text": "BECauSE", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.860866904258728}, {"text": "span overlap", "start_pos": 149, "end_pos": 161, "type": "METRIC", "confidence": 0.7939727902412415}, {"text": "Stanford Parser", "start_pos": 406, "end_pos": 421, "type": "DATASET", "confidence": 0.8959425687789917}]}, {"text": " Table 3: Statistics of various combinations of relation types. Note that there are 9 instances of TEMPORAL+CORRELATION and  3 instances of TEMPORAL+CONTEXT. This makes the bottom totals less than the sum of the rows.", "labels": [], "entities": []}]}