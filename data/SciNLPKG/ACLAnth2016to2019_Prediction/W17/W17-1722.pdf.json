{"title": [], "abstractContent": [{"text": "This paper presents a simple method for German compound splitting that combines a basic frequency-based approach with a form-to-lemma mapping to approximate morphological operations.", "labels": [], "entities": [{"text": "German compound splitting", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7300964494546255}]}, {"text": "With the exception of a small set of hand-crafted rules for modeling transitional elements, our approach is resource-poor.", "labels": [], "entities": []}, {"text": "In our evaluation, the simple splitter outperforms a splitter relying on rich morphological resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "In German, as in many other languages, two (or more) words can be combined to form a compound, leading to an infinite amount of new compounds.", "labels": [], "entities": []}, {"text": "For many NLP applications, this productive word formation process presents a problem as compounds often do not appear at all or only infrequently in the training data.", "labels": [], "entities": [{"text": "word formation", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7335537672042847}]}, {"text": "A typical NLP application that benefits from compound handling is statistical machine translation (SMT).", "labels": [], "entities": [{"text": "compound handling", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8331349492073059}, {"text": "statistical machine translation (SMT)", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.8234823346138}]}, {"text": "For example, a compound that does not occur in the training data cannot be translated.", "labels": [], "entities": []}, {"text": "However, the components of a compound often occur in the training data and can be used to translate a previously unseen compound.", "labels": [], "entities": []}, {"text": "Thus, making the parts of a compound accessible through compound splitting when training an SMT system leads to a better lexical coverage and, consequently, to improved translation quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9860390424728394}]}, {"text": "Similarly, in an information retrieval scenario, information about the individual parts of a compound helps to generalize and can thus lead to improved performance.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7843852639198303}]}, {"text": "The basis for successful compound handling in NLP applications is the decomposition of a complex compound into its components.", "labels": [], "entities": [{"text": "compound handling", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7315776348114014}]}, {"text": "This is not a trivial task, as the compound parts are not always just concatenated as in Reis|feld ('rice field'), but are often subject to morphological modifications.", "labels": [], "entities": []}, {"text": "For example, the components can be connected with a transitional element, as the -er in Bilder|buch ('picture book'); or parts of the modifier can be deleted, for example Kirch|turm ('church tower'), where the final -e of the lemma Kirche is deleted.", "labels": [], "entities": []}, {"text": "Furthermore, the modifier components can undergo non-concatenative morphological modifications such as changing a vowel in the word stem (\"Umlautung\"), for example Buch \u2192 B\u00fcch-in B\u00fccher|regal ('book shelf').", "labels": [], "entities": []}, {"text": "To split compounds into meaningful parts, and in particular to obtain a lemmatized representation of the modifier, all these morphological operations need to be considered and modeled accordingly.", "labels": [], "entities": []}, {"text": "There are many approaches for compound splitting, ranging from simple substring operations (e.g.) to linguistically sound splitting approaches relying on high-quality morphological resources (e.g.).", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7711959779262543}, {"text": "linguistically sound splitting", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.716880222161611}]}, {"text": "This paper aims at the \"middle ground\" of this spectrum by combining a minimum amount of linguistic information with corpus-derived statistics.", "labels": [], "entities": []}, {"text": "We present a simple method for compound splitting that makes linguistically informed splitting decisions, but requires only minimal resources.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8790372014045715}, {"text": "linguistically informed splitting", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.6124953230222067}]}, {"text": "It relies on a small set of handcrafted rules to model transitional elements, but all other morphological operations (such as \"Umlautung\") are induced from a mapping of inflected word forms to the word lemma -this can be easily obtained from large partof-speech tagged corpora.", "labels": [], "entities": []}, {"text": "Our approach makes use of the fact that many of the forms that are taken on by compound modifiers are equal to inflected forms (typically plural or genitive forms) and thus can be observed in corpora.", "labels": [], "entities": []}, {"text": "Thus, an explicit modeling of morphological operations for the modifier is often not necessary.", "labels": [], "entities": []}, {"text": "Furthermore, we make use of part-of-speech information fora flat analysis of the compound, as illustrated below: In contrast to morphological resources, which typically involve a large amount of manual work, partof-speech taggers are easily available and cheap to use even for very large corpora.", "labels": [], "entities": [{"text": "partof-speech taggers", "start_pos": 208, "end_pos": 229, "type": "TASK", "confidence": 0.794848620891571}]}, {"text": "We thus consider the presented splitting approach as essentially resource-poor.", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we first outline the splitting method, and then evaluate the splitting quality on a set of more than 51,000 German nominal compounds.", "labels": [], "entities": [{"text": "splitting", "start_pos": 52, "end_pos": 61, "type": "TASK", "confidence": 0.978289783000946}]}, {"text": "Ina comparison with the splitting results obtained with a well-acclaimed splitter relying on a high-quality morphological resource, our simple splitter obtains competitive results.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our splitting method, we analyze the splitting analyses obtained fora gold standard and compare them with a state-of-the-art splitter) relying on the morphological resource SMOR ().", "labels": [], "entities": [{"text": "splitting", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.9715956449508667}]}, {"text": "SMOR is a comprehensive German finite-state morphology covering inflection, derivation and compounding.", "labels": [], "entities": []}, {"text": "As gold standard, we use the binary split compound set developed for GermaNet, containing 51,230 noun compounds.", "labels": [], "entities": [{"text": "GermaNet", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.88657146692276}]}, {"text": "For this task, all words in the test-set should be split into two parts.", "labels": [], "entities": []}, {"text": "To evaluate the splitting results, we use the measures precision and recall as defined in, adapted to the simpler setting of only rating correct vs. wrong splits, without deciding whether a word should be split or not: \u2022 precision:: Comparison of splitting results for \"SMOR Split\" and the presented method.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9990779161453247}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9988743662834167}, {"text": "precision", "start_pos": 221, "end_pos": 230, "type": "METRIC", "confidence": 0.9968909621238708}]}, {"text": "Without the need to decide whether a word should be split, the accuracy of splitting results corresponds to the recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9991326928138733}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9983127117156982}]}, {"text": "Furthermore, we compute the F-score as shows the results of the two systems for the respective best split into two parts.", "labels": [], "entities": [{"text": "F-score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9980233907699585}]}, {"text": "A splitting analysis is counted as correct if both head and modifier are correct (i.e. exact string-match with the reference set).", "labels": [], "entities": []}, {"text": "Part-of-speech tags are not part of the test-set and can thus not be evaluated.", "labels": [], "entities": []}, {"text": "The simple splitting system has a higher total number of correct splittings, and is thus better at recall/accuracy.", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9987277388572693}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.8393078446388245}]}, {"text": "However, the SMOR-based splitting system has a higher precision.", "labels": [], "entities": [{"text": "SMOR-based splitting", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.8380424380302429}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9977731108665466}]}, {"text": "In the combined measure F-score, the simple split system is slightly better.", "labels": [], "entities": [{"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9874064326286316}]}, {"text": "Looking at the number of unsplit compounds, it becomes clear that the SMOR-based system employs a much more conservative splitting approach.", "labels": [], "entities": [{"text": "SMOR-based", "start_pos": 70, "end_pos": 80, "type": "TASK", "confidence": 0.8538749814033508}]}, {"text": "This is due to several factors: First, some word forms are lexicalized in SMOR and thus remain unsplit, for example Abend|Land ('evening country: Occident').", "labels": [], "entities": [{"text": "SMOR", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.5222378969192505}, {"text": "Abend|Land", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.8139113585154215}]}, {"text": "This is often the case for noncompositional compounds, the splitting of which can turnout to be harmful in subsequent applications as their meaning cannot be derived from the parts as is the case with compositional compounds.", "labels": [], "entities": []}, {"text": "Additionally, compounds containing a proper noun as modifier are likely not covered by SMOR's lexicon.", "labels": [], "entities": [{"text": "SMOR", "start_pos": 87, "end_pos": 91, "type": "TASK", "confidence": 0.768720805644989}]}, {"text": "Furtherore, the splitting approach itself is not designed to cover certain types of splittings, for example auf PART fahrt NN ('up|drive: driveway'), as particles cannot occur on their own, as opposed to nouns or verbs.", "labels": [], "entities": []}, {"text": "The decision whether to split or not in such cases depends entirely on the application.", "labels": [], "entities": []}, {"text": "In SMT applications, for example, it is generally assumed that over-splitting does not harm the translation quality, as the system can recover from this by translating split words as a phrase.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9929099678993225}]}, {"text": "Summarizing, we can say that the presented simple splitting approach is competitive with a method relying on a high-quality morphological tool, despite being based only on tagged and lemmatized corpus data in combination with a small set of rules to cover transitional elements.", "labels": [], "entities": [{"text": "simple splitting", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.6666517853736877}]}, {"text": "The results show that the system is robust and nearly always produces a splitting analysis.", "labels": [], "entities": []}, {"text": "This is due to the fact that it is independent of a hand-crafted lexicon, but rather relies on statistics derived from large corpora.", "labels": [], "entities": []}, {"text": "As a result, even compounds containing proper names can be split, for example Beaufort|skala ('Beaufort scale') or Bennett|k\u00e4nguru ('Bennett kangaroo').", "labels": [], "entities": []}, {"text": "Furthermore, by choosing appropriate corpus data, the splitter can be easily adapted to anew domain.", "labels": [], "entities": [{"text": "splitter", "start_pos": 54, "end_pos": 62, "type": "TASK", "confidence": 0.9723535776138306}]}], "tableCaptions": [{"text": " Table 1: Comparison of splitting results for  \"SMOR Split\" and the presented method.", "labels": [], "entities": [{"text": "SMOR Split", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.5394795536994934}]}]}