{"title": [], "abstractContent": [{"text": "This paper describes the SHEF submissions for the three sub-tasks of the Quality Estimation shared task of WMT17, namely: (i) a word-level prediction system using bilexical embeddings, (ii) a phrase-level labelling approach based on the word-level predictions, (iii) a sentence-level prediction system using word em-beddings and handcrafted baseline features.", "labels": [], "entities": [{"text": "SHEF submissions", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7988162040710449}, {"text": "Quality Estimation shared task of WMT17", "start_pos": 73, "end_pos": 112, "type": "TASK", "confidence": 0.6445235212643942}, {"text": "sentence-level prediction", "start_pos": 269, "end_pos": 294, "type": "TASK", "confidence": 0.68343286216259}]}, {"text": "Results are promising for the sentence-level approach, but still very preliminary for the other two levels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) allows the evaluation of Machine Translation (MT) when reference translations are not available.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5734143078327179}, {"text": "Machine Translation (MT)", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8401643514633179}]}, {"text": "It can be used in various ways such as in post-editing (PE) to predict whether or not an automatically generated sentence is worth publishing, editing or it should be retranslated manually.", "labels": [], "entities": []}, {"text": "Word-level predictions can be helpful by highlighting words that cannot be relied upon or should be fixed by post-editors.", "labels": [], "entities": []}, {"text": "More recently, QE at phrase-level has emerged as away of using quality predictions at decoding time in phrase-based Statistical MT (SMT) systems to guide the decoder such as to keep phrases which are predicted as good, and conversely to discard those which are predicted as bad.", "labels": [], "entities": [{"text": "QE", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9500386118888855}, {"text": "phrase-based Statistical MT (SMT)", "start_pos": 103, "end_pos": 136, "type": "TASK", "confidence": 0.7092601805925369}]}, {"text": "QE models are built based on a list of features along with a Machine Learning algorithm for either regression or classification.", "labels": [], "entities": []}, {"text": "These features are usually extracted from the source and target texts or from the MT system that generated the translations.", "labels": [], "entities": []}, {"text": "introduced anew set of features extracted using an unsupervised approach with the use of neural network: continuous-space language model features and word embeddings features.", "labels": [], "entities": []}, {"text": "In our contribution this year we investigate whether we can go beyond engineered features by learning bilexical operators over distributional representations of words in source-target text pairs.", "labels": [], "entities": []}, {"text": "Considering the MT pipeline as a noisy black-box, our motivation is to be able to build QE models to predict if information encoded in the source sentence is preserved in the target sentence after translation.", "labels": [], "entities": [{"text": "MT pipeline", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.867758572101593}]}, {"text": "propose to use wordlevel embeddings to predict the strength of different types of lexical relationships between a pair of words, such as head-modifier relations between noun-adjective pairs.", "labels": [], "entities": []}, {"text": "They designed a supervised framework for learning bilexical operators over distributional representations, based on learning bilinear forms W . We adapted their method to predict the strength of relationship between source and target words.", "labels": [], "entities": []}, {"text": "This problem is formulated as a log-bilinear model, parametrized with W as follows:", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the official task metrics to evaluate our results.", "labels": [], "entities": []}, {"text": "For the word and phrase-level tasks, the metrics are F 1 -BAD and F 1 -OK which correspond to the F 1 scores on both BAD and OK labels, and F 1 -multi which is the product of the two formers.", "labels": [], "entities": [{"text": "F 1 -BAD", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9165105372667313}, {"text": "F 1 -multi", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.9362277835607529}]}, {"text": "For the sentence-level task, the metrics for scoring are Pearson's correlation (primary metric), Mean Average Error (MAE) and Root Mean Squared Error (RMSE), and for ranking, Spearman's rank correlation (primary metric) and DeltaAvg.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 57, "end_pos": 78, "type": "METRIC", "confidence": 0.9579094847043356}, {"text": "Mean Average Error (MAE)", "start_pos": 97, "end_pos": 121, "type": "METRIC", "confidence": 0.9775754908720652}, {"text": "Root Mean Squared Error (RMSE)", "start_pos": 126, "end_pos": 156, "type": "METRIC", "confidence": 0.8365070138658796}, {"text": "DeltaAvg", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.882911205291748}]}], "tableCaptions": [{"text": " Table 2: Results of our word-level predictions.  \u2020 denotes our official submissions to the task using the  l 2 norm and single training set of 2k sentences. The other figures are obtained with mini-batch training  using 500 sentences at the time. In grey are the results of the official baseline of the task.", "labels": [], "entities": []}, {"text": " Table 3: Results of the phrase-level labelling  strategies based upon our word-level QE predic- tions.  \u2020 denotes our official submissions to the task  and \u2022 the results of the other two labelling strate- gies, both using our official submissions to Task  2. The other figures are obtained with the updated  word predictions from Task 2 resulting of the full  batch training. In grey are the results of the official  baseline of the task.", "labels": [], "entities": []}, {"text": " Table 4: Results of QUEST-EMB in the sentence- level QE task. In grey are the results of the official  baseline of the task.", "labels": [], "entities": []}]}