{"title": [{"text": "A Layered Language Model based Hybrid Approach to Automatic Full Diacritization of Arabic", "labels": [], "entities": [{"text": "Automatic Full Diacritization of Arabic", "start_pos": 50, "end_pos": 89, "type": "TASK", "confidence": 0.7684407234191895}]}], "abstractContent": [{"text": "In this paper we present a system for automatic Arabic text diacritization using three levels of analysis granularity in a layered back off manner.", "labels": [], "entities": [{"text": "automatic Arabic text diacritization", "start_pos": 38, "end_pos": 74, "type": "TASK", "confidence": 0.547740750014782}]}, {"text": "We build and exploit di-acritized language models (LM) for each of three different levels of granularity: surface form, morphologically segmented into prefix/stem/suffix, and character level.", "labels": [], "entities": []}, {"text": "For each of the passes, we use Viterbi search to pick the most probable diacriti-zation per word in the input.", "labels": [], "entities": []}, {"text": "We start with the surface form LM, followed by the morphological level, then finally we leverage the character level LM.", "labels": [], "entities": []}, {"text": "Our system outperforms all of the published systems evaluated against the same training and test data.", "labels": [], "entities": []}, {"text": "It achieves a 10.87% WER for complete full diacritization including lexical and syntactic diacritization, and 3.0% WER for lexical diacritization, ignoring syntactic diacritization.", "labels": [], "entities": [{"text": "WER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9989005327224731}, {"text": "WER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9990406632423401}]}], "introductionContent": [{"text": "Most languages have an orthographical system that reflects their phonological system.", "labels": [], "entities": []}, {"text": "Orthographies vary in the way they represent word pronunciations.", "labels": [], "entities": []}, {"text": "Arabic orthography employs an alphabetical system that comprises consonants and vowels.", "labels": [], "entities": []}, {"text": "Short vowels are typically underspecified in the orthography.", "labels": [], "entities": []}, {"text": "When present they appear as diacritical marks.", "labels": [], "entities": []}, {"text": "Moreover, other phonological phenomena are represented with diacritics, such as letter doubling, syllable boundary markers, elongation, etc.", "labels": [], "entities": [{"text": "letter doubling", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7168345153331757}]}, {"text": "In this paper, we are interested in restoring most of these diacritics, making them explicit in the written orthography.", "labels": [], "entities": []}, {"text": "This process is referred to as diacritization/vowelization, or \"tashkeel\" in Arabic.", "labels": [], "entities": []}, {"text": "Absence of these diacritics from the orthography renders the text extremely ambiguous.", "labels": [], "entities": []}, {"text": "Accordingly, the task of diacritization is quite important for many NLP applications such as morphological analysis, text to speech, POS tagging, word sense disambiguation, and machine translation.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.7519412040710449}, {"text": "POS tagging", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.8669074773788452}, {"text": "word sense disambiguation", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.7076598008473715}, {"text": "machine translation", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.7867355644702911}]}, {"text": "Moreover, from a human processing perspective, having the orthography reflect the diacritics explicitly makes for better readability comprehension and pronunciation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The table shows five experiments using A* search using 1, 2, 3, 4, and 5 grams LMs.", "labels": [], "entities": []}, {"text": "And two experiments using Viterbi search because the implementation we have for the Viterbi search supports 2 grams as a maximum size.", "labels": [], "entities": []}, {"text": "The best performance is yielded by the Viterbi algorithm and 2-grams LMs (i.e. 2-grams for WLM, 6-grams for MLM, and 8-grams for CLM).", "labels": [], "entities": []}, {"text": "It yields 6.11% WER for Full diacritization (FULL), corresponding to 2.61% WER for morphological diacritization (MORPH), i.e. by ignoring word final syntactic diacritics.", "labels": [], "entities": [{"text": "WER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.997731626033783}, {"text": "FULL)", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.8762342035770416}, {"text": "WER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9976286292076111}]}, {"text": "compares the performance of our system to the baselines systems.", "labels": [], "entities": []}, {"text": "It shows that our system is outperforming all of the published CER and WER on \"MORPH\" level.", "labels": [], "entities": [{"text": "CER", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.6938860416412354}, {"text": "WER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9741151332855225}, {"text": "MORPH", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9898748397827148}]}, {"text": "On \"FULL\" level, we outperform all of the baselines except (: System performance on DEV.", "labels": [], "entities": [{"text": "FULL", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9875703454017639}, {"text": "DEV", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.9104668498039246}]}, {"text": "The best setup is by using the Viterbi algorithm via 2 grams LMs.", "labels": [], "entities": []}, {"text": "on a deep bidirectional long short-term memory (LSTM) model.", "labels": [], "entities": []}, {"text": "These kinds of models can exploit long-range contexts; which could yield the better performance on the syntactic diacritization level.", "labels": [], "entities": []}, {"text": "It is also worth mentioning that they are using a post-processing correction layer that applies some rules to fix some of the diacritization errors after the LSTM.", "labels": [], "entities": []}, {"text": "It should be highlighted, that in contrast to the previous studies, TEST remained a complete held out data set that was not explored at all during the tuning phase of the system development, where for the previous studies TEST was used as both a development and test set.", "labels": [], "entities": [{"text": "TEST", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.646668016910553}]}], "tableCaptions": [{"text": " Table 1: System performance on DEV. The best setup is by using the Viterbi algorithm via 2 grams LMs.", "labels": [], "entities": []}, {"text": " Table 2: Our System performance against baselines", "labels": [], "entities": []}]}