{"title": [{"text": "Editors: Workshop Programme Exploratory Analysis for Ontology Learning from Social Events on Social Media Streaming in Spanish", "labels": [], "entities": [{"text": "Exploratory Analysis", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7338004112243652}, {"text": "Ontology Learning from Social Events on Social Media Streaming", "start_pos": 53, "end_pos": 115, "type": "TASK", "confidence": 0.712704598903656}]}], "abstractContent": [{"text": "TermBase eXchange (TBX), the ISO standard for the representation and interchange of terminological data, is currently undergoing revision and will for the first time formalize overarching structural constraints regarding the definition and validation of dialects and XML styles.", "labels": [], "entities": [{"text": "representation and interchange of terminological data", "start_pos": 50, "end_pos": 103, "type": "TASK", "confidence": 0.7223402112722397}]}, {"text": "The paper describes the design of an ODD architecture, which allows fora complete specification of presentday TBX.", "labels": [], "entities": [{"text": "TBX", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.4964984357357025}]}], "introductionContent": [{"text": "In this article we look at the use of an ontology as part of a system for annotating and querying ancient Greek tragic texts 1 . This system was designed to support the research carried out by the first author on the dramatic function of religious ritual in ancient Greek tragedy including an analysis of the utilisation of ritual actions by tragic authors in developing tragic plots.", "labels": [], "entities": []}, {"text": "In order to carryout this research it was necessary to create a corpus of annotated texts with the annotation taking into account the most salient phenomena from ancient Greek religion as well as the characteristics of ancient Greek rituals.", "labels": [], "entities": []}, {"text": "The corpus chosen for the annotation comprised all 33 surviving plays by Aeschylus, Sophocles and Euripides, although it does not yet include any of the fragments.", "labels": [], "entities": []}, {"text": "The annotations were carried out by the first author (a specialist in the field) using specialist annotation software and with a tagset which she specifically devised for the purpose.", "labels": [], "entities": []}, {"text": "The annotation software, known as Euporia, was developed through the adoption of a user centred design based on the annotation practices of classicists.", "labels": [], "entities": []}, {"text": "Euporia allows the user to annotate continuous and discontinuous passages of various lengths, and deals with textual and interpretive variants 7 . It is then possible to perform queries on the annotated corpus, searching for all the occurrences of one hashtag or the co-occurrences of two or more hashtags . Once the tragic corpus had been annotated, it became clear that restructuring the tags in the tagset into an ontology would make the annotated corpus even more useful and allow more complex and expressive queries to be made against the text.", "labels": [], "entities": []}, {"text": "We will discuss the design of this ontology in section 3, while in the next section we will look in more detail at the original tagset itself.", "labels": [], "entities": []}], "datasetContent": [{"text": "An interesting use-case for the enrichment of the French biomedical ontologies from SIFR BioPortal with UMLS CUIs is the evaluation of the named entity recognition performance of SIFR Annotator on the Quaero Annotated Corpus ().", "labels": [], "entities": [{"text": "Quaero Annotated Corpus", "start_pos": 201, "end_pos": 224, "type": "DATASET", "confidence": 0.8737468918164571}]}, {"text": "The Quaero corpus is a French-language corpus in the biomedical domain for the evaluation of named entity recognition and normalization.", "labels": [], "entities": [{"text": "Quaero corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8356027007102966}, {"text": "named entity recognition", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.6253478725751241}]}, {"text": "Quaero is more specifically composed of two sub-corpora, EMEA which contains information on marketed drugs and the MEDLINE corpus,which contains titles from PubMed abstract titles.", "labels": [], "entities": [{"text": "Quaero", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8107410073280334}, {"text": "EMEA", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.7852427959442139}, {"text": "MEDLINE corpus,which", "start_pos": 115, "end_pos": 135, "type": "DATASET", "confidence": 0.80362468957901}]}, {"text": "The annotations consist of token or phrase boundaries of identified entities, the corresponding UMLS semantic groups and one or more UMLS CUIs.", "labels": [], "entities": []}, {"text": "A semantic group is a thematic grouping of several semantic types, for example \"Disorder\" or \"Procedures\".", "labels": [], "entities": []}, {"text": "The 10 Semantic Groups are often used as coarse-grained groupings of UMLS Semantic Types ().", "labels": [], "entities": []}, {"text": "The corpus was created by instructing bilingual annotators to annotate the French text with UMLS semantics groups and CUIs based on their English language descriptions and definitions as included in UMLS.", "labels": [], "entities": []}, {"text": "The evaluation of the named entity recognition is bound to the proper recognition of its semantic group: if the token boundaries (NER) or the CUI identified are correct but the semantic group is incorrect, the annotation is counted as incorrect.", "labels": [], "entities": []}, {"text": "This is a confounding factor in the evaluation of NER alone, as the absence of semantic types in a particular ontology will lead to false negatives, although the entity was identified.", "labels": [], "entities": [{"text": "NER", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9122161865234375}]}, {"text": "illustrates the annotations expected in the Quaero corpus.", "labels": [], "entities": [{"text": "Quaero corpus", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9161384105682373}]}, {"text": "The SIFR Annotator proposes a specific output format for the Quaero evaluation and several variants.", "labels": [], "entities": [{"text": "SIFR Annotator", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.793691873550415}, {"text": "Quaero evaluation", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.8030157685279846}]}, {"text": "The quaero output is the direct output of the annotations as they are returned.", "labels": [], "entities": []}, {"text": "The quaerosg format is the same, except that when there are several possible semantic groups, the first is chosen.", "labels": [], "entities": []}, {"text": "The quaeroimg output excludes annotations with ambiguous semantic groups altogether.", "labels": [], "entities": []}, {"text": "Although the interface does not show it, the formats can be used through the format=quaero/quaeroimg/quaerosg option of the REST API.", "labels": [], "entities": []}, {"text": "In this paper, we choose to focus on the toponym resolution process.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9140577912330627}]}, {"text": "In particular, for each document processed, we run our process on their list of annotated toponyms.", "labels": [], "entities": []}, {"text": "Thus, the accuracy measure is the most adapted (Equation 2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994383454322815}, {"text": "Equation", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9576034545898438}]}, {"text": "The datasets we created for this research were extracted from TERMIUM Plus R \ud97b\udf59 , which we will call simply Termium from now on.", "labels": [], "entities": [{"text": "TERMIUM Plus R", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.9385769565900167}]}, {"text": "Termium: Excerpt from a record in Termium.", "labels": [], "entities": []}, {"text": "To create the datasets used in this research, we first had to process the source files containing the open data of Termium in order to reconstruct its records.", "labels": [], "entities": []}, {"text": "The source files are organized by domain, and records belonging to multiple domains are represented by multiple rows, often in different files.", "labels": [], "entities": []}, {"text": "Furthermore, the source files do not (currently) indicate which rows belong to the same record (i.e. using some kind of unique record identifier).", "labels": [], "entities": []}, {"text": "Therefore, data fusion must be performed to reconstruct the records.", "labels": [], "entities": [{"text": "data fusion", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.7561052441596985}]}, {"text": "The principle used to perform the data fusion is that rows that belong to the same record are identical except for the domain fields.", "labels": [], "entities": [{"text": "data fusion", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.788764476776123}]}, {"text": "By merging the rows which are identical except for these fields and combining the values found in these fields, we can reconstruct Termium's records.", "labels": [], "entities": [{"text": "Termium's records", "start_pos": 131, "end_pos": 148, "type": "DATASET", "confidence": 0.8190245429674784}]}, {"text": "Note that there are exceptions to this rule which produce a small quantity of noise (i.e. mismatches between the records shown in the web version of Termium and the index we created using the source files).", "labels": [], "entities": []}, {"text": "Thus the 2 million rows in the source files were aggregated into 1.33 million indexed records, from which we extracted the datasets described below.", "labels": [], "entities": []}, {"text": "We have released this data 2 in order to make it easier to reproduce the results reported in this paper, and more generally, to train a classifier using the fine-grained classification of Termium.", "labels": [], "entities": [{"text": "Termium", "start_pos": 188, "end_pos": 195, "type": "DATASET", "confidence": 0.8096145391464233}]}, {"text": "As illustrated in, records in Termium are all linked to at least one domain, and many are linked to 2 domains (31% of records), 3 domains (8%) or more (1%).", "labels": [], "entities": []}, {"text": "Records can also contain various kinds of textual supports such as a definition or examples which illustrate the use of a term, known as contexts.", "labels": [], "entities": []}, {"text": "Termium contains about 170 000 of these contexts in English and 155 000 in French, not counting duplicates.", "labels": [], "entities": [{"text": "Termium", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7177532315254211}]}, {"text": "These contexts are meant to illustrate the use of a term, but they sometimes also contain definitional or encyclopedic information about the term.", "labels": [], "entities": []}, {"text": "Unlike definitions, which often do not contain the term they define, contexts usually contain an occurrence of one of the synonymous terms on the record, as they are meant to illustrate usage.", "labels": [], "entities": []}, {"text": "We created two gold standard datasets by extracting these contexts and their associated domains from Termium, one in English and one in French.", "labels": [], "entities": [{"text": "Termium", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9276742935180664}]}, {"text": "We will refer to them as the Termium Context (TC) datasets.", "labels": [], "entities": [{"text": "Termium Context (TC) datasets", "start_pos": 29, "end_pos": 58, "type": "DATASET", "confidence": 0.5727115819851557}]}, {"text": "Although the contexts in Termium are supposed to show terms in use, we found out during our experiments that some records contain a context in which none of the record's terms actually occur.", "labels": [], "entities": []}, {"text": "Therefore, we defined a procedure to automatically validate each context by checking if it contained at least one of the terms of the record(s) in which it was found.", "labels": [], "entities": []}, {"text": "The contexts shown in are examples of valid contexts as they contain the terms penalty kick and coup de pied de p\u00e9nalit\u00e9.", "labels": [], "entities": []}, {"text": "About 85% of the  Using the TC datasets (in English and French) described in Section 2, we evaluated the DCVSM as well as five other supervised classification algorithms that have been used for text classification.", "labels": [], "entities": [{"text": "TC datasets", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.7642844319343567}, {"text": "DCVSM", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.9187609553337097}, {"text": "text classification", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.8741067051887512}]}, {"text": "Each short text (instance) from a TC dataset was converted into a bag of words after applying basic preprocessing (tokenization, lemmatization, case-folding, and removal of stop words and punctuation) . Thus, each instance is represented by a feature vector where each value is the frequency of a specific word.", "labels": [], "entities": [{"text": "TC dataset", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.7110207080841064}]}, {"text": "The set of features contains every word that occurs at least twice in the training data.", "labels": [], "entities": []}, {"text": "Word frequencies were optionally weighted using tf-idf, with idf being defined as follows fora given word w: idf(w) = log \u21e3 |D| |Dw| + 1 \u2318 , where Dis the set of contexts used for training, |D| is its size, and D w is the subset of training contexts that contain w.", "labels": [], "entities": []}, {"text": "The five other supervised classification algorithms we tested are: multinomial Naive Bayes (NB), Rocchio classification (RC), softmax regression (SR), k-nearest neighbours (k-NN) and a multi-layer perceptron (MLP).", "labels": [], "entities": []}, {"text": "As noted above, multinomial Naive Bayes is commonly used for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.842196524143219}]}, {"text": "Rocchio classification, like Naive Bayes, is a linear classification algorithm.", "labels": [], "entities": [{"text": "Rocchio classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6147107183933258}]}, {"text": "The DCVSM is similar to Rocchio classification, which involves computing centroids by averaging all the feature vectors belonging to each class, and classifying new instances by assigning them to the class of the nearest centroid.", "labels": [], "entities": []}, {"text": "The DCVSM is different in that the feature vectors belonging to each class are summed rather than being averaged, and then weighted.", "labels": [], "entities": []}, {"text": "Softmax regression (or multinomial logistic regression) is also a linear classification algorithm.", "labels": [], "entities": [{"text": "linear classification", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7202142179012299}]}, {"text": "The softmax classifier was trained using stochastic gradient descent, with a penalty on the L2 norm of the feature weights for regularisation.", "labels": [], "entities": []}, {"text": "The k-NN algorithm and the MLP are non-linear classifiers.", "labels": [], "entities": []}, {"text": "k-NN classifies a given instance based on the classes of the k most similar instances in the training data.", "labels": [], "entities": []}, {"text": "The MLP is also known as a fully connected artificial neural network.", "labels": [], "entities": []}, {"text": "A description of artificial neural networks and the backpropagation algorithm used to train them can be found in, and Bengio (2012) provides a practical guide to training and tuning neural networks.", "labels": [], "entities": []}, {"text": "We tested MLPs containing 1 or 2 hidden layers of exponential linear units ().", "labels": [], "entities": []}, {"text": "The MLP was trained using the Adam algorithm) and regularised using dropout and a max-norm constraint on the incoming weights of all units ().", "labels": [], "entities": [{"text": "MLP", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.508552610874176}]}, {"text": "Each TC dataset was split into 3 subsets of equal size (about 46K instances in English and 41K in French) for training, validation, and testing.", "labels": [], "entities": [{"text": "TC dataset", "start_pos": 5, "end_pos": 15, "type": "DATASET", "confidence": 0.8058006763458252}]}, {"text": "A grid search was used to tune the hyperparameters of each classifier on the validation set (except Naive Bayes, which has no hyperparameters).", "labels": [], "entities": []}, {"text": "Then the best configuration of each classifier was evaluated on the held-out test set.", "labels": [], "entities": []}, {"text": "Each classifier was tuned and tested twice, once using raw word frequencies as input, and once using tf-idf weighted frequencies.", "labels": [], "entities": []}, {"text": "The impact of this weighting will be assessed in the next section.", "labels": [], "entities": []}, {"text": "For the DCVSM, the only hyperparameter is the weighting scheme used to compute the feature weights.", "labels": [], "entities": []}, {"text": "We tested nine different weighting schemes including tf-idf and the simple association measures defined in Evert.", "labels": [], "entities": [{"text": "Evert", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.8306758999824524}]}, {"text": "These association measures compare the observed frequency of (word, context) pairs to their expected frequency in order to measure the strength of their association.", "labels": [], "entities": []}, {"text": "We calculate this expectation using the following equation: where T f i c j , as defined earlier (see Section 3), is the sum, for each feature vector in the training data belonging to class c j , of the value of feature f i . We set all of the association measures to 0 if We optionally apply a log or square root transformation to the output of all the weighting schemes, following.", "labels": [], "entities": []}, {"text": "For Rocchio classification, we tuned the measure used to estimate the distance between a feature vector and the class centroids (euclidean distance or cosine).", "labels": [], "entities": [{"text": "Rocchio classification", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6425095051527023}]}, {"text": "For k-NN, we tuned the number of neighbours (k) and the distance-based weighting of neighbours.", "labels": [], "entities": []}, {"text": "For the softmax classifier, we tuned the learning rate and the L2 penalty coefficient.", "labels": [], "entities": [{"text": "L2 penalty coefficient", "start_pos": 63, "end_pos": 85, "type": "METRIC", "confidence": 0.8678281108538309}]}, {"text": "As for the MLP, we tuned the number of hidden layers (1 or 2), the number of units in each, the learning rate, the number of training iterations (epochs), the dropout probability and the max-norm constraint.", "labels": [], "entities": [{"text": "MLP", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.851417601108551}]}, {"text": "shows the accuracy achieved by each classifier on the test sets in English and French.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995793700218201}]}, {"text": "Accuracy is measured using two different evaluation measures, namely micro-averaged recall at rank 1 (R@1) and recall at rank 5 (R@5).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9884405136108398}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.8896850347518921}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9986129999160767}]}, {"text": "R@1 is simply the percentage of correctly classified instances.", "labels": [], "entities": [{"text": "R@1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9616525371869405}]}, {"text": "This is the measure that was used to tune the models on the validation set.", "labels": [], "entities": []}, {"text": "It only considers the top prediction of a classifier fora given test case, whereas R@5 considers the top five predictions.", "labels": [], "entities": []}, {"text": "In other words, R@5 is the percentage of test cases for which the correct class is among the five most likely classes according to the classifier.", "labels": [], "entities": [{"text": "R@5", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9780393441518148}]}, {"text": "Results are shown as a list of candidates, each one taking the form of a hypernymy chain.", "labels": [], "entities": []}, {"text": "The following is an example of such chains: \u00b4 arbol > planta > entidad > todo Here, the target word is the Spanish noun\u00e1rbolnoun\u00b4noun\u00e1rbol ('tree'), which is automatically linked to its hypernym planta ('plant').", "labels": [], "entities": [{"text": "arbol", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9579678177833557}]}, {"text": "Then, the rest of the links (planta > entidad > todo 'plant > entity > everything') belong to the original structure of the CPA Ontology.", "labels": [], "entities": [{"text": "CPA Ontology", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9632169008255005}]}, {"text": "show a graphic representation of the hypernymy chains for Spanish and French respectively.", "labels": [], "entities": []}, {"text": "In the example of figure 2, azucarera ('sugar bowl') is automatically linked to recipiente ('container') and artefacto ('artifact'), both semantic types of the CPA Ontology such as the rest of the nodes over them.", "labels": [], "entities": [{"text": "CPA Ontology", "start_pos": 160, "end_pos": 172, "type": "DATASET", "confidence": 0.9387241303920746}]}, {"text": "Both links are correct, with different levels of semantic specification.", "labels": [], "entities": []}, {"text": "In, the French word bicyclette ('bike') is also correctly linked to v\u00e9hicule roulant ('vehicle with weels') and v\u00e9hicule ('vehicle'), but the link to roue ('weel') is incorrect (it is actually a meronym).", "labels": [], "entities": []}, {"text": "There are other correct and incorrect links in the structure shown in the figure, which is only apart of the whole net, e.g. the hyponyms linked to bicyclette are correct in the case of ciclo-taxi ('cycle taxi') but incorrect in the rest of the cases.", "labels": [], "entities": []}, {"text": "Regarding evaluation, we made a random sample of 100 nouns for each language and manually checked if the algorithm assigned hypernyms for each of them correctly.", "labels": [], "entities": []}, {"text": "The sample is not stratified by frequency, which is detrimental for performance as most of the randomly selected words are infrequent.", "labels": [], "entities": []}, {"text": "However, we leave for future work the development of an improved evaluation method.", "labels": [], "entities": []}, {"text": "Both for Spanish and French, criteria for precision consisted of considering as correct only those results with links that corresponded to a hypernym-hyponym relation, that is, when the target word could  be correctly linked to the upper node with the expression.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9960249662399292}]}, {"text": "In other words, we say there is a hypernym link between nouns X and Y if we can hold a statement such as \"X is a type of Y\", as in a \"bicyclette is a type of v\u00e9hicule\".", "labels": [], "entities": []}, {"text": "The rest of cases were considered incorrect.", "labels": [], "entities": []}, {"text": "For instance, we marked as incorrect results such as \"termosif\u00f3n ('thermosyphon') is a type of temperature ('temperature')\" for Spanish, or \"instructeur ('instructor') is a type of instruction ('instruction')\" for French.", "labels": [], "entities": []}, {"text": "At this stage of our project, we did not calculate recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9848802089691162}]}, {"text": "Recall could in principle be defined as the number of senses detected per word over the total number of senses that actually exist for such word.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7662110924720764}]}, {"text": "We observed, however, that in the majority of the cases the system was only able to detect the most frequent meanings.", "labels": [], "entities": []}, {"text": "Precision was evaluated taking into account each position of the ranking and the degree of certainty of the algorithm.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9861918687820435}, {"text": "certainty", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9752683639526367}]}, {"text": "The rank of a candidate is given by the integration voting algorithm, thus the best candidate will be in the first position of the rank.", "labels": [], "entities": []}, {"text": "We only considered the first 4 positions in ranking.", "labels": [], "entities": []}, {"text": "shows the intersection of results indicating high probability of success in each ranking position.", "labels": [], "entities": []}, {"text": "If we only consider results ranked in the first position and with high degree of certainty, we obtain 60% precision in the French taxonomy and a 54% in the Spanish taxonomy.", "labels": [], "entities": [{"text": "certainty", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.992496132850647}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9991521835327148}, {"text": "French taxonomy", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.9087227880954742}]}, {"text": "If we ignore the certainty level, results in first position drop to to 51% in French and a 46% in Spanish.", "labels": [], "entities": [{"text": "certainty", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9991488456726074}]}, {"text": "Percentages of precision increase as we consider more positions in the ranking because then the system has more opportunities to find a correct hypernym.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9991893172264099}]}, {"text": "The error analysis indicates that the major part of the errors are cases of semantic relations other than hypernymy.", "labels": [], "entities": []}, {"text": "Typically, we found meronymy relations but also synonymy, co-hyponymy and even hyponymy.", "labels": [], "entities": []}, {"text": "For example, in Spanish, the relation aposento > edificio ('chamber > building') corresponds to meronymy (aposento IS A PART OF edificio), and the same happens in French withglac\u00ec ere > eau ('glacier > water').", "labels": [], "entities": []}, {"text": "Also in the case of French, for the noun produit ('product'), one of the candidates for hypernym is actually a hyponym: oeuvre d'art ('piece of work').", "labels": [], "entities": []}, {"text": "Also incorrect is a result such as copa > vaso ('cup > glass') for Spanish, because the target word and the candidate are co-hyponyms.", "labels": [], "entities": []}, {"text": "Some of the errors are also due to interferences with the lexicographical marks coming from algorithm 1, such as in the case of the Spanish noun pubis ('pubis'), for which the candidate is plural ('plural'), due to the fact that many dictionaries indicate that the plural of this word is irregular.", "labels": [], "entities": []}, {"text": "Problems regarding more general aspects of the methodology are related to the fact that the system does not distinguish between different candidates and different meanings at this stage of the project.", "labels": [], "entities": []}, {"text": "Thus, for example, fora Spanish noun such as taza ('cup'), the system offers 4 candidates: artefacto, vasija, recipiente and leche.", "labels": [], "entities": []}, {"text": "The first three candidates are correct, but they belong to the same meaning of the word, that is, all of them could be considered equivalent hypernyms.", "labels": [], "entities": []}, {"text": "In the case of artefacto, the hypernym is the most general one, but it is correct because a cup is a type of physical object created by humans.", "labels": [], "entities": [{"text": "hypernym", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9402279257774353}]}, {"text": "The other two correct candidates are synonyms and, thus, equivalent and correct hypernyms, being vasija the old-fashion word and recipiente the modern one.", "labels": [], "entities": []}, {"text": "Working on improving all these problems is part of our future work with the taxonomy project.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of tokens by subcorpus: expert (two left-most) and user texts (right)", "labels": [], "entities": []}, {"text": " Table 2: Entry associated information", "labels": [], "entities": [{"text": "Entry associated", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8819848597049713}]}, {"text": " Table 3: Statistics on Geodict", "labels": [], "entities": [{"text": "Geodict", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.6919561624526978}]}, {"text": " Table 3: TC Datasets statistics.", "labels": [], "entities": [{"text": "TC Datasets", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.7485798597335815}]}, {"text": " Table 2. Statistics on the  TC datasets are presented in", "labels": [], "entities": [{"text": "TC datasets", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.7707454264163971}]}, {"text": " Table 3. Contexts contain  about 40-45 tokens on average, which makes these texts  much shorter than those typically used to evaluate text  classification, yet longer than a typical query in informa- tion retrieval. The number of classes (1376 in English,  1342 in French) is also higher than that of other text clas- sification datasets, such as Reuters-21578 3 , which con- tains 118 classes. Thus, this task can be considered a  fine-grained domain classification of short texts.", "labels": [], "entities": [{"text": "text  classification", "start_pos": 135, "end_pos": 155, "type": "TASK", "confidence": 0.6861025989055634}]}, {"text": " Table 4: Micro-averaged R@1 and R@5 on the  test sets in English and French.", "labels": [], "entities": [{"text": "Micro-averaged R@1", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8467248380184174}, {"text": "R@5", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9441398779551188}]}, {"text": " Table 5: Impact of weighting the feature values us- ing tf-idf.", "labels": [], "entities": []}, {"text": " Table 7: Low-recall classes (right) and the classes with which they are most often confused (left). The  frequency of each class is shown in brackets. This is their frequency in the training set (in English).", "labels": [], "entities": []}, {"text": " Table 1: Statistics for the ontologies enriched in CUIs (all UMLS ontologies with a French-language  version): Number of classes, number of classes without CUIs at the beginning, the number of classes  without CUIs at the beginning, the number of CUIs found in labels, the number of CUIs found through  mappings, the number of CUIs found through UMLS codes, the number of classes remaining without  CUIs at the end and the number of ontologies remaining without semantic types at the end.", "labels": [], "entities": []}, {"text": " Table 1: Percentages of precision in the two languages by degree of certainty and rank of the candidate.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9885204434394836}, {"text": "certainty", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9379034042358398}]}]}