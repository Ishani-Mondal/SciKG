{"title": [{"text": "What works and what does not: Classifier and feature analysis for argument mining", "labels": [], "entities": [{"text": "Classifier and feature analysis", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6598215103149414}, {"text": "argument mining", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7277020364999771}]}], "abstractContent": [{"text": "This paper offers a comparative analysis of the performance of different supervised machine learning methods and feature sets on argument mining tasks.", "labels": [], "entities": [{"text": "argument mining tasks", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.8048651814460754}]}, {"text": "Specifically, we address the tasks of extracting argumentative segments from texts and predicting the structure between those segments.", "labels": [], "entities": []}, {"text": "Eight classifiers and different combinations of six feature types reported in previous work are evaluated.", "labels": [], "entities": []}, {"text": "The results indicate that overall best performing features are the structural ones.", "labels": [], "entities": []}, {"text": "Although the performance of classifiers varies depending on the feature combinations and corpora used for training and testing, Random Forest seems to be among the best performing classifiers.", "labels": [], "entities": []}, {"text": "These results build a basis for further development of argument mining techniques and can guide an implementation of argument mining into different applications such as argument based search.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7976496517658234}, {"text": "argument mining", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7190045118331909}]}], "introductionContent": [{"text": "Argument mining refers to the automatic extraction of arguments from natural texts.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7674862146377563}, {"text": "automatic extraction of arguments from natural texts", "start_pos": 30, "end_pos": 82, "type": "TASK", "confidence": 0.8250541772161212}]}, {"text": "An argument consists of a claim (also referred to as the conclusion of the argument) and several pieces of evidence called premises that support or reject the claim (.", "labels": [], "entities": []}, {"text": "As a research area argument mining has seen a rapid progress in the last three-to-five years (.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.855785608291626}]}, {"text": "Current studies report methods for argument mining in legal documents (, persuarespect to two argument mining tasks: (1) identifying argumentative segments in text, i.e. the classification of textual units (usually sentences) into claims, premises or none and (2) the prediction of argument structure, i.e. connecting claims and premises.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7940977811813354}, {"text": "prediction of argument structure", "start_pos": 268, "end_pos": 300, "type": "TASK", "confidence": 0.8734902143478394}]}, {"text": "We re-implement a rich set of features reported by related work and evaluate eight different classification systems.", "labels": [], "entities": []}, {"text": "We perform our investigation on two different well-known corpora: (1) the persuasive essays corpus reported by and the Wikipedia claim and premise data reported by .", "labels": [], "entities": [{"text": "Wikipedia claim and premise data", "start_pos": 119, "end_pos": 151, "type": "DATASET", "confidence": 0.6881320118904114}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F1-scores of 7 classifiers for different feature combinations for the persuasive essay corpus. The  results are shown as X/Y where X refers to the score for the task of detecting argumentative sentences  and Y refers to the score for argument structure prediction task.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9963207244873047}, {"text": "argument structure prediction task", "start_pos": 244, "end_pos": 278, "type": "TASK", "confidence": 0.7634671851992607}]}, {"text": " Table 2: F1-scores of different classifiers on different feature type combinations for the Wikipedia corpus.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985112547874451}, {"text": "Wikipedia corpus", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9346884787082672}]}, {"text": " Table 4: F1-scores of CNN on both persuasive es- say and Wikipedia corpora", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988941550254822}]}]}