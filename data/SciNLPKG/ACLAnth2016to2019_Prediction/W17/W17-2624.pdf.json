{"title": [{"text": "Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in Twitter Streams", "labels": [], "entities": [{"text": "Extrinsic Evaluation of Spatiotemporal Text Representations", "start_pos": 14, "end_pos": 73, "type": "TASK", "confidence": 0.5564680645863215}]}], "abstractContent": [{"text": "Language in social media is a dynamic system, constantly evolving and adapting, with words and concepts rapidly emerging , disappearing, and changing their meaning.", "labels": [], "entities": []}, {"text": "These changes can be estimated using word representations in context , overtime and across locations.", "labels": [], "entities": []}, {"text": "A number of methods have been proposed to track these spatiotemporal changes but no general method exists to evaluate the quality of these representations.", "labels": [], "entities": []}, {"text": "Previous work largely focused on qualitative evaluation , which we improve by proposing a set of visualizations that highlight changes in text representation over both space and time.", "labels": [], "entities": []}, {"text": "We demonstrate usefulness of novel spatiotemporal representations to explore and characterize specific aspects of the corpus of tweets collected from European countries over a two-week period centered around the terrorist attacks in Brussels in March 2016.", "labels": [], "entities": []}, {"text": "In addition, we quantitatively evaluate spatiotemporal representations by feeding them into a downstream classification task-event type prediction.", "labels": [], "entities": []}, {"text": "Thus, our work is the first to provide both intrinsic (qualitative) and extrinsic (quan-titative) evaluation of text representations for spatiotemporal trends.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language in social media presents additional challenges for textual representations.", "labels": [], "entities": []}, {"text": "Being able to represent texts in social media streams requires a methodology with the following properties: 1.", "labels": [], "entities": []}, {"text": "Capable of handling large amounts of data.", "labels": [], "entities": []}, {"text": "2. Ina streaming rather than static fashion.", "labels": [], "entities": []}, {"text": "3. Across many geographic regions.", "labels": [], "entities": []}, {"text": "While there has been some recent work for representing changeover time in embedding spaces, these methods largely did not take into account geographic variation (.", "labels": [], "entities": []}, {"text": "Likewise, papers examining geographic variations of language tend not to examine data temporally.", "labels": [], "entities": []}, {"text": "Although incorporate temporal information, they treat each timestep as a separate corpus, learning unique representations.", "labels": [], "entities": []}, {"text": "We propose two algorithms to learn spatiotemporal text representations from large amounts of social media data and investigate their utility both from a qualitative and quantitative standpoint.", "labels": [], "entities": []}, {"text": "Indeed, the broader question of how to evaluate the quality of an embedding is one which has received a great deal of attention (.", "labels": [], "entities": []}, {"text": "Previous spatial and temporal embedding algorithms have been evaluated primarily with qualitative evidence, investigating the ability of the embedding to capture a small number of known meaning shifts and providing some form of visualization (.", "labels": [], "entities": []}, {"text": "While it is important to capture known changes of interest, without some form of quantitative evaluation it cannot be known whether these embedding methods actually produce good vector spaces.", "labels": [], "entities": []}, {"text": "Because of these issues we not only provide the first spatiotemporal algorithms for learning text embeddings from social media data, but we also evaluate our embedding algorithms through a variety of means.", "labels": [], "entities": []}, {"text": "For qualitative evaluation, we develop a set of novel visualizations 1 which allow us to investigate word representation shifts across space and time.", "labels": [], "entities": []}, {"text": "In particular, we demonstrate that the model captures temporal shifts related to events in our corpus and these shifts vary across distinct countries.", "labels": [], "entities": []}, {"text": "For quantitative evaluation, we estimate the effectiveness of spatiotemporal embeddings through a downstream event-classification task, demonstrating that temporal and spatial algorithms vary in their usefulness.", "labels": [], "entities": []}, {"text": "We choose an extrinsic evaluation task rather than the more standard intrinsic embedding evaluation because of recent work demonstrating weak relationships between intrinsic measures and extrinsic performance ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We make use of three datasets in our experiments.", "labels": [], "entities": []}, {"text": "First, we use a large corpus of European Twitter data captured over two weeks in order to learn text representations across time and space.", "labels": [], "entities": [{"text": "European Twitter data captured", "start_pos": 32, "end_pos": 62, "type": "DATASET", "confidence": 0.7578973025083542}]}, {"text": "For our event classification task, we chose a subset of tweets in the larger corpus which were made by news accounts.", "labels": [], "entities": [{"text": "event classification", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8213425278663635}]}, {"text": "These \"news-worthy\" tweets were then manually annotated for event type.", "labels": [], "entities": []}, {"text": "To leverage the additional available data annotated with real-world events, we train our models on a larger event dataset from Wikipedia and then use transfer learning to apply it to our smaller event data.", "labels": [], "entities": []}, {"text": "We collected a large sample of tweets (with geo-locations and language IDs assigned to each tweet) from 240 countries in 66 languages from Twitter.", "labels": [], "entities": []}, {"text": "Data collection lasted two weeks, beginning on March 15th, 2016 and ending March 29th, 2016.", "labels": [], "entities": [{"text": "Data collection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6497482359409332}]}, {"text": "Tweets were filtered based on geo-location and language tags to include only English-language tweets from a set of 34 European countries that had at least 10,000 English tweets per day in the corpus.", "labels": [], "entities": []}, {"text": "This resulted in a set of 140M tweets we use to learn different types of embeddings.", "labels": [], "entities": []}, {"text": "We selected \"newsworthy\" tweets that discuss real-world events from 400M English tweets generated in 240 countries.", "labels": [], "entities": []}, {"text": "Our criterion for selecting \"news-worthy\" tweets was to only select tweets that contain an action word from the connotation frame lexicon ( and either come from a verified account, from a news account e.g., @bbc, @wsj, or contain the hashtag \"#breaking\" or \"#news\".", "labels": [], "entities": []}, {"text": "We identified 600,000 English subject-predicateobject tuples using SyntaxNet (.", "labels": [], "entities": []}, {"text": "Three annotators labeled event types for all tuples based on two previously defined lists of event categories: the ACE event categories) and those from a related paper on querying event types ().", "labels": [], "entities": []}, {"text": "Because of missing values for the third annotator, we used Krippendorff's alpha to judge inter-annotator agreement (like Fleiss' kappa, Krippendorff's alpha is \u2264 1 with 1 indicating complete agreement and 0 indicating random chance).", "labels": [], "entities": []}, {"text": "This subset of labeled clusters without ties have high interannotator agreement: 0.71 and 0.78, respectively.", "labels": [], "entities": [{"text": "interannotator agreement", "start_pos": 55, "end_pos": 79, "type": "METRIC", "confidence": 0.7002803683280945}]}, {"text": "Finally, we subsampled our \"news-worthy\" tweets to match the 34 European countries in the Brussels dataset.", "labels": [], "entities": [{"text": "Brussels dataset", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.9455197155475616}]}, {"text": "We show the final number of clusters and tweets per event category in.", "labels": [], "entities": []}, {"text": "Wikipedia Event Dataset Given the small size of our Twitter event dataset, we explore additional resources for training an effective event detection model.", "labels": [], "entities": [{"text": "Wikipedia Event Dataset", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.9276037216186523}, {"text": "event detection", "start_pos": 133, "end_pos": 148, "type": "TASK", "confidence": 0.7026747167110443}]}, {"text": "We construct a larger event dataset by scraping the English language Wikipedia Current Events Portal from the time period of January 2010 to October 2016.", "labels": [], "entities": [{"text": "Wikipedia Current Events Portal", "start_pos": 69, "end_pos": 100, "type": "DATASET", "confidence": 0.805513471364975}]}, {"text": "Each event in this portal is described in a short summary and is labeled by date along with a subject heading such as Armed Conflicts and Attacks.", "labels": [], "entities": []}, {"text": "We use these summaries and headings as training data fora neural network to be used for transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.9675708413124084}]}, {"text": "Overall, the Wikipedia event dataset contains 43,098 total event samples and 31 event type classes.", "labels": [], "entities": [{"text": "Wikipedia event dataset", "start_pos": 13, "end_pos": 36, "type": "DATASET", "confidence": 0.9022582372029623}]}, {"text": "For training, we use the 42,906 samples that correspond to the ten most frequent classes within the dataset, approximately 99.5% of the original data.", "labels": [], "entities": []}, {"text": "The distribution of these ten most frequent classes is shown in.", "labels": [], "entities": []}, {"text": "As a baseline, we compare our spatiotemporal embeddings against openly available, pre-trained embeddings -300-dimensional Word2Vec embeddings trained on Google News, and 100-dimensional GloVe embeddings trained on 2 billion tweets.", "labels": [], "entities": []}, {"text": "In addition, we evaluate three simpler classifiers on the the 5-and 10-way event classification problems.", "labels": [], "entities": [{"text": "event classification", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.705368384718895}]}, {"text": "We train logistic regression (LR), linear SVM, and a random forest classifier (RF) with 100 decision trees on TFIDF features from our labeled Twitter dataset, and report micro and macro F1 scores over 10-fold c.v. in below.", "labels": [], "entities": [{"text": "F1", "start_pos": 186, "end_pos": 188, "type": "METRIC", "confidence": 0.9213977456092834}]}], "tableCaptions": [{"text": " Table 1: Distributions of event types in two annotation schemes: A from Doddington et al. (2004) and B  from Metzler et al. (2012).", "labels": [], "entities": []}, {"text": " Table 3: Embedding evaluation results (F1) for event detection task (best performance is marked in bold).  Tweets from specific timesteps and countries make use of relevant temporal and spatial embeddings  where applicable.", "labels": [], "entities": [{"text": "F1)", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9550460278987885}, {"text": "event detection task", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8280191421508789}]}, {"text": " Table 4: Results of baseline models and LSTM  trained on static embeddings. Best performance is  marked in bold.", "labels": [], "entities": []}]}