{"title": [{"text": "Cross-Lingual Pronoun Prediction with Deep Recurrent Neural Networks v2.0", "labels": [], "entities": [{"text": "Cross-Lingual Pronoun Prediction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7034075061480204}]}], "abstractContent": [{"text": "In this paper we present our system in the DiscoMT 2017 Shared Task on Cross-lingual Pronoun Prediction.", "labels": [], "entities": [{"text": "DiscoMT 2017 Shared Task", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.8148998916149139}, {"text": "Cross-lingual Pronoun Prediction", "start_pos": 71, "end_pos": 103, "type": "TASK", "confidence": 0.5890657703081766}]}, {"text": "Our entry builds on our last year's success, our system based on deep recurrent neural networks outperformed all the other systems with a clear margin.", "labels": [], "entities": []}, {"text": "This year we investigate whether different pre-trained word embeddings can be used to improve the neural systems, and whether the recently published Gated Convolutions outperform the Gated Recurrent Units used last year.", "labels": [], "entities": []}], "introductionContent": [{"text": "The DiscoMT 2017 Shared Task on Cross-lingual Pronoun Prediction () concentrates on the difficult task of translating pronouns between languages.", "labels": [], "entities": [{"text": "DiscoMT 2017 Shared Task", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8843296617269516}, {"text": "Cross-lingual Pronoun Prediction", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.5712380309899648}, {"text": "translating pronouns between languages", "start_pos": 106, "end_pos": 144, "type": "TASK", "confidence": 0.8527964353561401}]}, {"text": "For example different gender marking between languages complicates the translation process.", "labels": [], "entities": [{"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9748401641845703}]}, {"text": "This shared task includes three languages and four translation directions: English-French, English-German, GermanEnglish and Spanish-English.", "labels": [], "entities": []}, {"text": "In the target language side selected set of pronouns are substituted with replace token, and the task is then to predict the missing pronoun.", "labels": [], "entities": []}, {"text": "Furthermore, the target side language is not given as running text, but instead in lemma plus part-of-speech tag format, which makes even harder to model the target language.", "labels": [], "entities": []}, {"text": "An example of an English-French sentence pair is given in.", "labels": [], "entities": []}, {"text": "In this paper we describe the pronoun prediction system of the Turku NLP Group.", "labels": [], "entities": [{"text": "pronoun prediction", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7181691080331802}, {"text": "Turku NLP Group", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9361984332402548}]}, {"text": "Our system extends the last year's deep recurrent neural networks based system with word-level embeddings, two layers of Gated Recurrent Units (GRUs) and a softmax layer on top of it to make the final prediction (.", "labels": [], "entities": []}, {"text": "This year Source: That 's how they like to live . Target: ce|PRON\u00eatre|VERce|PRON\u02c6ce|PRON\u00eatre|VER comme|ADV cela|PRON que|PRON REPLACE 3 aimer|VER vivre|VER .|.", "labels": [], "entities": [{"text": "VERce", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.8558810353279114}, {"text": "REPLACE 3 aimer", "start_pos": 126, "end_pos": 141, "type": "METRIC", "confidence": 0.8837411999702454}]}, {"text": "we investigate whether pre-trained word embeddings improve the system performance compared to the random initialization used in the previous system.", "labels": [], "entities": []}, {"text": "We also study whether the recently published Gated Convolution outperforms Gated Recurrent Units.", "labels": [], "entities": []}, {"text": "The network uses both source and target contexts to make the prediction, and no additional data or tools are used beside the data provided by the organizers.", "labels": [], "entities": []}, {"text": "Also our pre-trained word embeddings are trained on the same data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test set results of the variants of the system tested against the test sets.", "labels": [], "entities": []}]}