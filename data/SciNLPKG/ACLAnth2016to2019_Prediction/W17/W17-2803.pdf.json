{"title": [{"text": "Guiding Interaction Behaviors for Multi-modal Grounded Language Learning", "labels": [], "entities": [{"text": "Multi-modal Grounded Language Learning", "start_pos": 34, "end_pos": 72, "type": "TASK", "confidence": 0.5886740460991859}]}], "abstractContent": [{"text": "Multi-modal grounded language learning connects language predicates to physical properties of objects in the world.", "labels": [], "entities": []}, {"text": "Sensing with multiple modalities, such as audio, haptics, and visual colors and shapes while performing interaction behaviors like lifting , dropping, and looking on objects enables a robot to ground non-visual predicates like \"empty\" as well as visual predicates like \"red\".", "labels": [], "entities": []}, {"text": "Previous work has established that grounding in multi-modal space improves performance on object retrieval from human descriptions.", "labels": [], "entities": [{"text": "object retrieval", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.6998005658388138}]}, {"text": "In this work, we gather behavior annotations from humans and demonstrate that these improve language grounding performance by allowing a system to focus on relevant behaviors for words like \"white\" or \"half-full\" that can be understood by looking or lifting, respectively.", "labels": [], "entities": []}, {"text": "We also explore adding modality annotations (whether to focus on audio or haptics when performing a behavior), which improves performance, and sharing information between linguistically related predicates (if \"green\" is a color, \"white\" is a color), which improves grounding recall but at the cost of precision .", "labels": [], "entities": [{"text": "recall", "start_pos": 275, "end_pos": 281, "type": "METRIC", "confidence": 0.9854819774627686}, {"text": "precision", "start_pos": 301, "end_pos": 310, "type": "METRIC", "confidence": 0.9977051615715027}]}], "introductionContent": [{"text": "Connecting human language predicates like \"red\" and \"heavy\" to machine perception is part of the symbol grounding problem, approached in machine learning as grounded language learning.", "labels": [], "entities": []}, {"text": "For many years, grounded language learning has been performed primarily in visual space (;.", "labels": [], "entities": []}, {"text": "Recently, researchers have explored grounding in audio (, haptic (, and multimodal () spaces.", "labels": [], "entities": []}, {"text": "Multimodal grounding allows a system to connect language predicates like \"rattles\", \"empty\", and \"red\" to their audio, haptic, and color signatures, respectively.", "labels": [], "entities": []}, {"text": "Past work has used human-robot interaction to gather language predicate labels for objects in the world (.", "labels": [], "entities": []}, {"text": "Using only human-robot interaction to gather labels, a system needs to learn effectively from only a few examples.", "labels": [], "entities": []}, {"text": "Gathering audio and haptic perceptual information requires doing more than looking at each object.", "labels": [], "entities": []}, {"text": "In past work, multiple interaction behaviors are used to explore objects and add this audio and haptic information.", "labels": [], "entities": []}, {"text": "In this work, we gather annotations on what exploratory behaviors humans would perform to determine whether language predicates apply to a novel object.", "labels": [], "entities": []}, {"text": "A robot could gather such information by asking human users which action would best allow it to test a particular property, e.g. \"To tell whether something is 'heavy' should I look at it or pick it up?\" shows some of the behaviors used by our robot in previous work to perceive objects and their properties.", "labels": [], "entities": []}, {"text": "In this paper, we show that providing a language grounding system with behavior annotation information improves classification performance on whether predicates apply to objects, despite having sparse predicate-object labels.", "labels": [], "entities": []}, {"text": "We additionally explore adding modality annotations (e.g. is a predicate more auditory or more haptic), drawing on previous work in psychology that gathered modality norms for many words.", "labels": [], "entities": []}, {"text": "Finally, we explore using grasp lift lower drop press push: Behaviors the robot used to explore objects.", "labels": [], "entities": []}, {"text": "In addition, the hold behavior (not shown) was performed after the lift behavior by holding the object in place for half a second.", "labels": [], "entities": []}, {"text": "The look behavior (not shown) was also performed for all objects.", "labels": [], "entities": []}, {"text": "word embeddings to help with infrequently seen predicates by sharing information with more common ones (e.g. if \"thin\" is common and \"narrow\" is rare, we can exploit the fact that they are linguistically related to help understand the latter).", "labels": [], "entities": []}], "datasetContent": [{"text": "Previous work provides sparse annotations of 32 household objects () with language predicates derived during an interactive \"I Spy\" game with human users (.", "labels": [], "entities": []}, {"text": "Each predicate p \u2208 P from that work is associated with objects as applying or not applying, based on dialog with human users.", "labels": [], "entities": []}, {"text": "For example, predicate \"red\" applies to several objects and not to others, but for many objects its label is not explicitly known.", "labels": [], "entities": []}, {"text": "Objects are represented by features gathered during several interaction behaviors () as detailed in past work ( . In this work, we focus on improving the language grounding performance of multimodal classifiers that predict whether each predicate p \u2208 P applies to each object o \u2208 O.", "labels": [], "entities": []}, {"text": "In previous work, decisions about a predicate and an object are made for each sensorimotor context (a combination of a behavior and sensory modality) with an SVM using the feature space for that context.", "labels": [], "entities": []}, {"text": "A summary of sensorimotor contexts is given in.", "labels": [], "entities": []}, {"text": "We calculated precision, recall, and f -measure between human labels and predicate decisions when weighting constituent sensorimotor context classifiers by the schemes described above: kappa confidence only (Eq 1, \u03ba), adding behavior annotations (Eq 2, B+\u03ba), adding modality annotations (Eq 3, M+\u03ba), and sharing kappas across predicates using word similarity (Eq 5, W+\u03ba).", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9993513226509094}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9993053674697876}, {"text": "f -measure", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9875746965408325}]}, {"text": "We calculated these metrics for each predicate 4 and averaged scores across all predicates.", "labels": [], "entities": []}, {"text": "We use leave-one-object-out cross validation to obtain performance statistics for each weighting scheme.", "labels": [], "entities": []}, {"text": "gives the results for predicates that have at least 3 positive and 3 negative training object examples.", "labels": [], "entities": []}, {"text": "We observe that adding behavior annotations or modality annotations improves performance over Decisions were made for each testing object and marked corrector incorrect against human labels that object, if available for the predicate.", "labels": [], "entities": []}, {"text": "The trends are similar when considering all predicates, but the scores and differences in performance are lower due to many predicates having only a single positive or negative example.", "labels": [], "entities": []}, {"text": "using kappa confidence alone, as was done in past work.", "labels": [], "entities": []}, {"text": "Sharing kappa confidences across similar predicates based on their embedding cosine similarity improves recall at the cost of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9986454844474792}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9986193180084229}]}, {"text": "Adding behavior annotations helps more than adding modality norms, but we gathered behavior annotations for all predicates, while modality annotations were only available fora subset (about half).", "labels": [], "entities": []}, {"text": "Adding behavior annotations helped the f -measure of predicates like \"pink\", \"green\", and \"half-full\", while adding modality annotations helped with predicates like \"round\", \"white\", and \"empty\".", "labels": [], "entities": [{"text": "f -measure", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.976959228515625}]}, {"text": "Sharing confidences through word similarity helped with some predicates, like \"round\", at the expense of domain-specific meanings of predicates like \"water\".", "labels": [], "entities": []}, {"text": "In the \"I Spy\" paradigm from which these data were gathered, the authors noted that \"water\" correlated with object weight because all of their water bottle objects were partially or completely full (.", "labels": [], "entities": []}, {"text": "Thus, in that domain, \"water\" is synonymous with \"heavy\".", "labels": [], "entities": []}, {"text": "Ina less restricted domain, word similarity may add less real world \"noise\" to the problem.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7248135805130005}]}], "tableCaptions": []}