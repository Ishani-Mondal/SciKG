{"title": [{"text": "LIUM Machine Translation Systems for WMT17 News Translation Task", "labels": [], "entities": [{"text": "LIUM Machine Translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.533536285161972}, {"text": "WMT17 News Translation", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7704127232233683}]}], "abstractContent": [{"text": "This paper describes LIUM submissions to WMT17 News Translation Task for English\u2194German, English\u2194Turkish, English\u2192Czech and English\u2192Latvian language pairs.", "labels": [], "entities": [{"text": "WMT17 News Translation Task", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6892841160297394}]}, {"text": "We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework.", "labels": [], "entities": [{"text": "BPE-based attentive Neural Machine Translation", "start_pos": 9, "end_pos": 55, "type": "TASK", "confidence": 0.8176429986953735}]}, {"text": "Competitive scores were obtained by en-sembling various systems and exploiting the availability of target monolingual corpora for back-translation.", "labels": [], "entities": []}, {"text": "The impact of back-translation quantity and quality is also analyzed for English\u2192Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9949030876159668}]}], "introductionContent": [{"text": "This paper describes LIUM Neural Machine Translation (NMT) submissions to WMT17 News Translation Task for English\u2194German, English\u2194Turkish, English\u2192Czech and English\u2192Latvian language pairs.", "labels": [], "entities": [{"text": "LIUM Neural Machine Translation (NMT) submissions to WMT17 News Translation Task", "start_pos": 21, "end_pos": 101, "type": "TASK", "confidence": 0.7336538067230811}]}, {"text": "We experimented with and without back-translation data for English\u2194German and English\u2194Turkish which are respectively described in Sections 3 and 4.", "labels": [], "entities": []}, {"text": "For the latter pair, we also present an analysis about the impact of back-translation quality and quantity as well as two architectural ablations regarding the initialization and the output of recurrent decoder (Section 3).", "labels": [], "entities": []}, {"text": "Experiments for English\u2192Czech and English\u2192Latvian are performed using Factored NMT (FNMT) () systems.", "labels": [], "entities": []}, {"text": "FNMT is an extension of NMT which aims at simultaneously predicting the canonical form of a word and its morphological information needed to generate the final surface form.", "labels": [], "entities": [{"text": "FNMT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8661030530929565}]}, {"text": "The details and results are presented in section 5.", "labels": [], "entities": []}, {"text": "All submitted systems 1 are trained using the open source nmtpy 2 framework).", "labels": [], "entities": [{"text": "nmtpy 2 framework", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.8217313885688782}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: EN\u2194TR: Underlined and bold scores  represent contrastive and primary submissions re- spectively.", "labels": [], "entities": []}, {"text": " Table 3: Impact of back-translation quantity and  quality for EN\u2192TR: all systems are 3WT, (B0) is  the same as (T4) from", "labels": [], "entities": []}, {"text": " Table 4: Summary of follow-up results for  EN\u2192TR newstest2017: UEDIN is the best  WMT17 matrix entry before deadline while LIUM  is our primary submission", "labels": [], "entities": [{"text": "EN\u2192TR newstest2017", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.47796766459941864}, {"text": "UEDIN", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9504923224449158}, {"text": "LIUM", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9360252022743225}]}, {"text": " Table 5: BLEU scores computed with mteval- v13a.pl for EN\u2194DE systems on newstest2016 and  newstest2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988184571266174}]}, {"text": " Table 6: EN\u2192CS. Bold scores represent primary  submissions. Ensemble(CSn) correspond to the  ensemble of 2 systems CSn trained with different  seeds.", "labels": [], "entities": []}, {"text": " Table 7: EN\u2192LV. Underlined and bold scores  represent contrastive and primary submissions.  Ensemble(Sn) correspond to the ensemble of 2  systems Sn trained with different seeds.", "labels": [], "entities": []}]}