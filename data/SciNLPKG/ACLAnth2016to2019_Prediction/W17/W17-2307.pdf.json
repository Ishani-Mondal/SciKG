{"title": [{"text": "Tackling Biomedical Text Summarization: OAQA at BioASQ 5B", "labels": [], "entities": [{"text": "Tackling Biomedical Text Summarization", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8529626578092575}, {"text": "OAQA", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.5891546607017517}, {"text": "BioASQ 5B", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.719350665807724}]}], "abstractContent": [{"text": "In this paper, we describe our participation in phase B of task 5b of the fifth edition of the annual BioASQ challenge, which includes answering factoid, list, yes-no and summary questions from biomedical data.", "labels": [], "entities": []}, {"text": "We describe our techniques with an emphasis on ideal answer generation, where the goal is to produce a relevant, precise , non-redundant, query-oriented summary from multiple relevant documents.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8534024953842163}]}, {"text": "We make use of extractive summariza-tion techniques to address this task and experiment with different biomedical on-tologies and various algorithms including agglomerative clustering, Maximum Marginal Relevance (MMR) and sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 222, "end_pos": 242, "type": "TASK", "confidence": 0.7992154359817505}]}, {"text": "We propose a novel word embedding based tf-idf similarity metric and a soft positional constraint which improve our system performance.", "labels": [], "entities": []}, {"text": "We evaluate our techniques on test batch 4 from the fourth edition of the challenge.", "labels": [], "entities": []}, {"text": "Our best system achieves a ROUGE-2 score of 0.6534 and ROUGE-SU4 score of 0.6536.", "labels": [], "entities": [{"text": "ROUGE-2 score", "start_pos": 27, "end_pos": 40, "type": "METRIC", "confidence": 0.9852386713027954}, {"text": "ROUGE-SU4 score", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.9878304302692413}]}], "introductionContent": [{"text": "In recent years, there has been a huge surge in the number of biomedical articles being deposited online.", "labels": [], "entities": []}, {"text": "The National Library of Medicine (NLM) provides MEDLINE, a gigantic database of 23 million references to biomedical journal papers.", "labels": [], "entities": [{"text": "National Library of Medicine (NLM)", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.9490721310888018}]}, {"text": "Approximately 200,000 articles 1 from this database have been cited since 2015.", "labels": [], "entities": []}, {"text": "The rapid growth of information in this centralized repository makes it difficult for medical researchers to manually find an exact answer fora question or to summarize the enormous content to answer a query.", "labels": [], "entities": []}, {"text": "The problem of extracting exact answers for factoid questions from this data is being studied extensively, resulting in the development of several techniques including inferencing (), noisy-channel transformation ( and exploitation of resources like WordNet (.", "labels": [], "entities": [{"text": "noisy-channel transformation", "start_pos": 184, "end_pos": 212, "type": "TASK", "confidence": 0.7244043350219727}, {"text": "WordNet", "start_pos": 250, "end_pos": 257, "type": "DATASET", "confidence": 0.9516189098358154}]}, {"text": "However, recent times have also seen an interest in developing ideal answer generation systems which can produce relevant, precise, nonrepetitive and readable summaries for biomedical questions (.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7212818413972855}]}, {"text": "A query based summarization system called \"BioSQUASH\") uses domain specific ontologies like the Unified Medical Language System (UMLS) ( to create a conceptual model for sentence ranking.", "labels": [], "entities": [{"text": "sentence ranking", "start_pos": 170, "end_pos": 186, "type": "TASK", "confidence": 0.7235401570796967}]}, {"text": "Experiments with biomedical ontology based concept expansion and weighting techniques were conducted, where the strength of the semantic relationships between concepts was used as a similarity metric for sentence ranking).", "labels": [], "entities": [{"text": "concept expansion and weighting", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.6602747738361359}]}, {"text": "Similar methods () are used for this task where the difference lies in query similarity ranking methods.", "labels": [], "entities": []}, {"text": "This paper describes our efforts in creating a system that can provide ideal answers for biomedical questions.", "labels": [], "entities": []}, {"text": "More specifically, we develop a system which can answer the kinds of biomedical questions present in the dataset for the BioASQ challenge (, which is a challenge on large-scale biomedical semantic indexing and question answering.", "labels": [], "entities": [{"text": "biomedical semantic indexing", "start_pos": 177, "end_pos": 205, "type": "TASK", "confidence": 0.651087741057078}, {"text": "question answering", "start_pos": 210, "end_pos": 228, "type": "TASK", "confidence": 0.8200222551822662}]}, {"text": "We participate in Phase B of Task 5b (biomedical questionanswering) for the 2016 edition of this challenge comprising of factoid, yes/no, list and summary type questions.", "labels": [], "entities": []}, {"text": "We develop a system for biomedical summarization using MMR and clustering based techniques.", "labels": [], "entities": [{"text": "biomedical summarization", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7821718752384186}]}, {"text": "To answer factoid, list and yes/no questions, we use one of the winning systems () from the 2015 edition of the BioASQ challenge, open-sourced after the conclusion of the challenge 2 . We build on standard techniques such as Maximal Marginal Relevance) and Sentence Compression ( and incorporate domain-specific knowledge using biomedical ontologies such as the UMLS metathesaurus and SNOMEDCT () to build an ideal answer generator for biomedical questions.", "labels": [], "entities": [{"text": "UMLS metathesaurus", "start_pos": 362, "end_pos": 380, "type": "DATASET", "confidence": 0.9075764417648315}]}, {"text": "We also experiment with several similarity metrics such as jaccard similarity and a novel word embedding based tf-idf (w2v tf-idf) similarity metric within our system.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our system on the dataset for test batch 4 of the fourth edition of the challenge and report our system performance on ROUGE-2 and ROUGE-SU4 (, which are the standard metrics used for official evaluation in the BioASQ challenge.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.989669144153595}, {"text": "ROUGE-SU4", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.8631849884986877}, {"text": "BioASQ challenge", "start_pos": 242, "end_pos": 258, "type": "TASK", "confidence": 0.5033443123102188}]}, {"text": "Our best system achieves ROUGE-2 and ROUGE-SU4 scores of 0.6534 and 0.6536 respectively on test batch 4 for task 4b when evaluated on BioASQ Oracle 3 . Various configurations and similarity metrics, granularity and algorithms selection enabled us to secure top 1,2,3 in test batch 4 and top 1,2,3,4 in test batch 5 on automatic evaluation metrics of ROUGE-2 and ROUGE-SU4, from our participation in Task 5b of ideal answer generation.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9665535688400269}, {"text": "BioASQ Oracle 3", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.9069233338038126}, {"text": "answer generation", "start_pos": 416, "end_pos": 433, "type": "TASK", "confidence": 0.7759200036525726}]}, {"text": "The rest of the paper is organized as follows: Section 2 describes the datasets used.", "labels": [], "entities": []}, {"text": "In section 3, we describe our summarization pipeline, while section 4 gives a brief overview of the system used for factoid, list and yes-no questions.", "labels": [], "entities": [{"text": "summarization", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.9693873524665833}]}, {"text": "Section 5 presents the evaluation results of our summarization system and our observations about various system configurations.", "labels": [], "entities": []}, {"text": "Section 6 presents a comparative qualitative error analysis of some of our system configurations.", "labels": [], "entities": []}, {"text": "Section 7 concludes and describes future work in this area.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training data for Phase B of task 5b provides biomedical questions, where each question is associated with question type, urls of relevant PubMed articles and relevant snippets from those articles.", "labels": [], "entities": []}, {"text": "This dataset consists of 1,799 questions.", "labels": [], "entities": []}, {"text": "2 https://github.com/oaqa/bioasq 3 http://participants-area.bioasq.org/oracle/ Though our ideal answer generation system is unsupervised, we use a brief manual inspection of the training data for this edition of the challenge to make an informed choice of hyperparameters for the algorithms used by our system.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8862777650356293}]}, {"text": "To develop an ideal answer generator which can produce query-oriented summaries for each question, we can adopt one of two popular approaches: extractive or abstractive.", "labels": [], "entities": []}, {"text": "Extractive summarization techniques choose sentences from relevant documents and combine them to form a summary.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8673539459705353}]}, {"text": "Abstractive summarization methods use relevant documents to create a semantic representation of the knowledge from these documents and then generate a summary using reasoning and natural language generation techniques.", "labels": [], "entities": []}, {"text": "Brief analysis on a randomly sampled subset from the training data shows us that most of the sentences in the gold ideal answers are present either in the relevant snippets or relevant abstracts of PubMed articles.", "labels": [], "entities": []}, {"text": "Hence we perform extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.611576497554779}]}, {"text": "We also observe an interesting ordering trend among relevant snippets which is used to develop a positional constraint.", "labels": [], "entities": []}, {"text": "Adding this positional constraint to our similarity metrics gives us a slight boost in performance.", "labels": [], "entities": []}, {"text": "We explain the intuition behind this idea in more detail in section 3.1.2.", "labels": [], "entities": []}, {"text": "For evaluation, we use the dataset from test batch 4 of the fourth edition of the BioASQ challenge which consists of 100 questions.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.49630114436149597}]}, {"text": "We experiment with ideal answer generation using various system configurations which differ in similarity metrics, biomedical ontologies, sentence selection algorithms(clustering/MMR) and tiling algorithms used.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8615134358406067}, {"text": "sentence selection", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.6903703510761261}]}, {"text": "The official evaluation for ideal answers includes manual evaluation by biomedical experts in the BioASQ team as well as automatic evaluation via ROUGE scores.", "labels": [], "entities": [{"text": "BioASQ team", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.8489803373813629}, {"text": "ROUGE", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.9913781881332397}]}, {"text": "To present comparable and standardized results, we run our system on the batch 4 dataset for Phase B of task 4b and get our results evaluated via the BioASQ Oracle.", "labels": [], "entities": [{"text": "BioASQ Oracle", "start_pos": 150, "end_pos": 163, "type": "DATASET", "confidence": 0.94013512134552}]}, {"text": "These results are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE scores with different algorithms, ontologies and similarity metrics", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8042418956756592}]}]}