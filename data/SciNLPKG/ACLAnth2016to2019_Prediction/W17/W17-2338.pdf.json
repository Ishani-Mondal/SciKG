{"title": [{"text": "Adapting Pre-trained Word Embeddings For Use In Medical Coding", "labels": [], "entities": [{"text": "Adapting Pre-trained Word Embeddings", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8523633182048798}, {"text": "Medical Coding", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.713418573141098}]}], "abstractContent": [{"text": "Word embeddings area crucial component in modern NLP.", "labels": [], "entities": []}, {"text": "Pre-trained embed-dings released by different groups have been a major reason for their popularity.", "labels": [], "entities": []}, {"text": "However, they are trained on generic corpora , which limits their direct use for domain specific tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to add task specific information to pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Such information can improve their utility.", "labels": [], "entities": []}, {"text": "We add information from medical coding data, as well as the first level from the hierarchy of ICD-10 medical code set to different pre-trained word embed-dings.", "labels": [], "entities": [{"text": "ICD-10 medical code set", "start_pos": 94, "end_pos": 117, "type": "DATASET", "confidence": 0.8046937137842178}]}, {"text": "We adapt CBOW algorithm from the word2vec package for our purpose.", "labels": [], "entities": []}, {"text": "We evaluated our approach on five different pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Both the original word embeddings, and their modified versions (the ones with added information) were used for automated review of medical coding.", "labels": [], "entities": [{"text": "automated review of medical coding", "start_pos": 111, "end_pos": 145, "type": "TASK", "confidence": 0.5947944700717926}]}, {"text": "The modified word em-beddings give an improvement in f-score by 1% on the 5-fold evaluation on a private medical claims dataset.", "labels": [], "entities": [{"text": "f-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9980960488319397}]}, {"text": "Our results show that adding extra information is possible and beneficial for the task at hand.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings area recent addition to an NLP researcher's toolkit.", "labels": [], "entities": []}, {"text": "They are dense, real-valued vector representations of words that capture interesting properties among them.", "labels": [], "entities": []}, {"text": "Word embeddings are learned from raw corpora.", "labels": [], "entities": []}, {"text": "Usually, the larger the corpora, the better is the quality of the embeddings learned.", "labels": [], "entities": []}, {"text": "However, the larger the corpora, the larger is the amount of resources and time needed for their training.", "labels": [], "entities": []}, {"text": "Thus, different groups release their learned embeddings publicly.", "labels": [], "entities": []}, {"text": "Such pre-trained embeddings is a primary reason for the inclusion of word embeddings in mainstream NLP.", "labels": [], "entities": []}, {"text": "However, such pre-trained embeddings are usually learned on generic corpora.", "labels": [], "entities": []}, {"text": "Using such embeddings in a particular domain such as medical domain leads to following problems: \u2022 No embeddings for domain-specific words.", "labels": [], "entities": []}, {"text": "For example, phenacetin is not present in pretrained vectors released by Google.", "labels": [], "entities": []}, {"text": "\u2022 Even those words that do have embeddings, may have a poor quality of the embedding, due to different senses of the words, some of which belonging to different domains.", "labels": [], "entities": []}, {"text": "It is difficult to obtain large amounts of domainspecific data.", "labels": [], "entities": []}, {"text": "However, many NLP applications have benefited from the addition of information from small domain-specific corpus to that obtained from a large generic corpus ().", "labels": [], "entities": []}, {"text": "This raises the following questions: \u2022 Can we use additional domain-specific data to learn the missing embeddings?", "labels": [], "entities": []}, {"text": "\u2022 Can we use additional domain-specific data to improve the quality of already available embeddings?", "labels": [], "entities": []}, {"text": "In this paper, we address the second question: Given pre-trained word embeddings, and domain specific data, we tune the pre-trained word embeddings such that they can achieve better performance.", "labels": [], "entities": []}, {"text": "We tune the embeddings for and evaluate them on an automated review of medical coding.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 provides some background on different notions used later in the paper.", "labels": [], "entities": []}, {"text": "Section 3 motivates our approach through examples.", "labels": [], "entities": []}, {"text": "Section 4 explains our approach in detail.", "labels": [], "entities": []}, {"text": "Section 5 enlists the experimental setup.", "labels": [], "entities": []}, {"text": "Section 6 details the results and analysis, followed by conclusion and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a private medical claims review dataset, which we cannot release publicly due to privacy concerns.", "labels": [], "entities": []}, {"text": "The dataset consists of 280k records, consisting of medical terms along with a code.", "labels": [], "entities": []}, {"text": "Each entry is labeled as accept or reject, depending on whether the entry has correct code, or whether it was sent for recoding.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cosine similarities of pairs of examples from Section 3", "labels": [], "entities": []}, {"text": " Table 2: Average 5-fold cross validation F-score  on automated review of medical coding", "labels": [], "entities": [{"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.923986554145813}]}]}