{"title": [{"text": "Language Based Mapping of Science Assessment Items to Skills", "labels": [], "entities": [{"text": "Language Based Mapping of Science Assessment Items to Skills", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7948138250244988}]}], "abstractContent": [{"text": "Knowledge of the association between assessment questions and the skills required to solve them is necessary for analysis of student learning.", "labels": [], "entities": []}, {"text": "This association, often represented as a Q-matrix, is either hand-labeled by domain experts or learned as latent variables given a large student response data set.", "labels": [], "entities": []}, {"text": "As a means of automating the match to formal standards, this paper uses neural text classification methods , leveraging the language in the standards documents to identify online text fora proxy training task.", "labels": [], "entities": [{"text": "neural text classification", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.7212725679079691}]}, {"text": "Experiments involve identifying the topic and crosscutting concepts of middle school science questions leveraging multi-task training.", "labels": [], "entities": []}, {"text": "Results show that it is possible to automatically build a Q-matrix without student response data and using a modest number of hand-labeled questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "In both traditional and online contexts, fine grain diagnostic information can play a crucial role in employing formative assessment to improve student learning outcomes as observed by, and for creating scalable systems that provide individualized instruction (.", "labels": [], "entities": []}, {"text": "A key requirement for this inference is association of each of the assessment tasks, which we will refer to as questions, with attributes, which are the skills (knowledge, concepts and/or strategies) needed to solve the tasks.", "labels": [], "entities": []}, {"text": "The association of skills to questions is represented as a Q-matrix.", "labels": [], "entities": []}, {"text": "Hand crafted Q-matrices are created by domain experts who label each assessment task with the required skill(s).", "labels": [], "entities": []}, {"text": "While this provides an interpretable matrix for educators, in the sense that the skills are associated with a documented standard or cognitive model, the question annotation process is time consuming and not scalable.", "labels": [], "entities": []}, {"text": "When standards change, the old question annotation is no longer useful.", "labels": [], "entities": []}, {"text": "The cost of question annotation is a key issue with the domain models in intelligent tutoring systems (ITS), which are created by experts for each subject area and grade level, limiting reusability (.", "labels": [], "entities": [{"text": "question annotation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7447326183319092}]}, {"text": "As an alternative, there has been work on automated discovery of an association of (latent) skills to questions using student response data (.", "labels": [], "entities": []}, {"text": "While these unsupervised automated methods can provide a good fit for the student response data, they are limited by the requirement of a large data set of student scores on a given test, which is not available for individual classroom assessments and hard to obtain for standardized testing.", "labels": [], "entities": []}, {"text": "In addition, the latent skills offer limited interpretability for teachers.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.9634925127029419}]}, {"text": "The results cannot easily be used to identify practice questions to help a student improve in areas of weakness.", "labels": [], "entities": []}, {"text": "It was observed in a report by National Research Council (2001) that fine grained diagnostic models are not widely used due to scalability, reusability and/or interpretability issues, which is still a problem today as stated by.", "labels": [], "entities": [{"text": "National Research Council (2001)", "start_pos": 31, "end_pos": 63, "type": "DATASET", "confidence": 0.9285621146361033}]}, {"text": "This work aims to develop interpretable and automatic methods for mapping science assessment tasks to underlying skills by using text classification methods that leverage the language in standards documents and teacher training materials.", "labels": [], "entities": [{"text": "mapping science assessment tasks", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.8030447214841843}, {"text": "text classification", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7203646898269653}]}, {"text": "The experiments here use the Framework for K-12 Science education laid out in the framework by, but the method is designed to work for any well documented standard and the questions used in this study are not explicitly designed for that standard.", "labels": [], "entities": []}, {"text": "Specifically, we look at the core disciplinary ideas (topics) and crosscutting concepts described in the standard as the attributes needed to respond to assessment tasks.", "labels": [], "entities": []}, {"text": "A multi-task convolutional neural network is designed to jointly label topics and concepts.", "labels": [], "entities": []}, {"text": "The greater challenge is in recognizing concepts, for which there is no annotated data available.", "labels": [], "entities": []}, {"text": "A key contribution is in the use of standards documentation to automate training annotation and obtain online text for use as a proxy task in an intermediate training stage.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "2 provides a detailed task description, which is followed in Sec.", "labels": [], "entities": []}, {"text": "3 by an overview of prior text classification work that we build on.", "labels": [], "entities": [{"text": "text classification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7068024575710297}]}, {"text": "Experiment details are provided in Sec.", "labels": [], "entities": []}, {"text": "4 with results in Sec.", "labels": [], "entities": [{"text": "Sec.", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.9200144410133362}]}, {"text": "5. Related work leveraging question text in latent skill learning is discussed in Sec.", "labels": [], "entities": []}, {"text": "6. Findings and open questions are summarized in Sec.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Topic distribution in train and test sets", "labels": [], "entities": []}, {"text": " Table 2: Concept counts in train and test sets", "labels": [], "entities": []}, {"text": " Table 4: Classifier Performance: Topic (accuracy)  and concept (macro F1 score)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9937161803245544}, {"text": "macro F1 score", "start_pos": 65, "end_pos": 79, "type": "METRIC", "confidence": 0.7219887872536978}]}, {"text": " Table 5: Per-Concept Classification Performance", "labels": [], "entities": [{"text": "Per-Concept Classification", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7115761637687683}]}]}