{"title": [{"text": "GradAscent at EmoInt-2017: Character-and Word-Level Recurrent Neural Network Models for Tweet Emotion Intensity Detection", "labels": [], "entities": [{"text": "Tweet Emotion Intensity Detection", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.7522103041410446}]}], "abstractContent": [{"text": "The WASSA 2017 EmoInt shared task has the goal to predict emotion intensity values of tweet messages.", "labels": [], "entities": [{"text": "WASSA 2017 EmoInt shared task", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.5501783788204193}]}, {"text": "Given the text of a tweet and its emotion category (anger, joy, fear, and sadness), the participants were asked to build a system that assigns emotion intensity values.", "labels": [], "entities": []}, {"text": "Emotion intensity estimation is a challenging problem given the short length of the tweets, the noisy structure of the text and the lack of annotated data.", "labels": [], "entities": [{"text": "Emotion intensity estimation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8554567893346151}]}, {"text": "To solve this problem, we developed an ensemble of two neural models, processing input on the character. and word-level with a lexicon-driven system.", "labels": [], "entities": []}, {"text": "The correlation scores across all four emotions are averaged to determine the bottom-line competition metric, and our system ranks place forth in full intensity range and third in 0.5-1 range of intensity among 23 systems at the time of writing (June 2017).", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis of a text reveals information on the degree of positiveness or negativeness of the opinion expressed by the writer.", "labels": [], "entities": [{"text": "Sentiment analysis of a text", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9277330994606018}]}, {"text": "Such information can be useful for providing better services for users) or preventing potentially dangerous situations.", "labels": [], "entities": []}, {"text": "Traditionally the most popular way of sentiment representation is either binary (positive, negative) or multi-class (for example 5 classes: very negative, negative, neutral, positive, very positive).", "labels": [], "entities": [{"text": "sentiment representation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.9440746903419495}]}, {"text": "While being simple, such a scheme looses interpretability and a continuous intensity scale might be preferred.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.9673689007759094}]}, {"text": "Twitter sentiment and emotion intensity detection are still challenging tasks and re- * equal contribution main active areas of research.", "labels": [], "entities": [{"text": "Twitter sentiment", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.568244606256485}, {"text": "emotion intensity detection", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.7097677687803904}]}, {"text": "These difficulties have several reasons: extensive usage of hashtags, slang, abbreviations, and emoticons.", "labels": [], "entities": []}, {"text": "Also, tweets are usually typed on mobile devices which can lead to a substantial amount of typos.", "labels": [], "entities": []}, {"text": "As traditional NLP tools are usually trained on datasets containing clean text, which makes it difficult to use them for tweet analysis.", "labels": [], "entities": [{"text": "tweet analysis", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.7754839658737183}]}, {"text": "Existing approaches for modeling emotion intensity rely heavily on manually constructed lexicons, which contain information about intensity weights for each available word.", "labels": [], "entities": [{"text": "modeling emotion intensity", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7444653511047363}]}, {"text": "The intensity for the whole sentence can be inferred by combining individual scores of words.", "labels": [], "entities": []}, {"text": "While being easily interpretable, such models have several limitations.", "labels": [], "entities": []}, {"text": "Ignoring word order and compositionality of the language is the first issue, which is critical for modeling sequences.", "labels": [], "entities": []}, {"text": "Constructing such lexicons is a labourintensive process, which needs to be carried out continuously due to the constant development of language.", "labels": [], "entities": []}, {"text": "Data-driven approaches like deep neural networks can overcome such limitations, and they have been behind many recent advances in text processing tasks, such as language modeling, machine translation, POS tagging, and classification ().", "labels": [], "entities": [{"text": "language modeling", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7204494923353195}, {"text": "machine translation", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7923168241977692}, {"text": "POS tagging", "start_pos": 201, "end_pos": 212, "type": "TASK", "confidence": 0.8433635532855988}]}, {"text": "The appealing property of such models is their ability to combine feature extraction and classification stages given a sufficient amount of training data.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7280363887548447}]}, {"text": "In this paper, we augment traditional lexiconbased models with two neural network-based models: one with character and one with word input.", "labels": [], "entities": []}, {"text": "Character-level deep neural networks recently showed outstanding results on text understanding tasks such as machine translation) and text classification (.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7981866598129272}, {"text": "machine translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7146527916193008}, {"text": "text classification", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.861334502696991}]}, {"text": "Ina domain-specific task such as predict- ing the emotion intensity of tweets, a characterlevel model can theoretically capture the notion of hashtags, emoticons, or character repetitions, which all are unique to social media.", "labels": [], "entities": [{"text": "predict- ing the emotion intensity of tweets", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.8306403383612633}, {"text": "character repetitions", "start_pos": 166, "end_pos": 187, "type": "TASK", "confidence": 0.7395476400852203}]}, {"text": "The intuition is that a character-level model captures common writing patterns such as punctuations and signaling characters.", "labels": [], "entities": []}, {"text": "A word-level recurrent neural model can incorporate the order of information using distributed representations of words trained on a large amount of text.", "labels": [], "entities": []}, {"text": "Our final model is a weighted average of the scores provided by the baseline, our character-and word-level model.", "labels": [], "entities": []}, {"text": "Our ensemble model achieved forth position in the 0-1 emotion intensity range task and third position in the 0.5-1.0 range task on the public leaderboard (GradAscent team) on CodaLab 1 at the time of writing this paper (June 2017).", "labels": [], "entities": [{"text": "GradAscent team) on CodaLab 1", "start_pos": 155, "end_pos": 184, "type": "DATASET", "confidence": 0.7716352740923563}]}], "datasetContent": [{"text": "The dataset for the WASSA-2017 competition) is comprised of 7097 annotated tweets, classified into 4 categories: joy, anger, fear, and sadness (dataset statistics are presented in).", "labels": [], "entities": [{"text": "WASSA-2017 competition", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6280528008937836}]}, {"text": "For each annotated tweet there is an ID, full text, emotion category, and emotion intensity value.", "labels": [], "entities": []}, {"text": "Emotion intensity is areal value in the range from 0 to 1, where higher value correspond to a higher intensity of the emotion conveyed.", "labels": [], "entities": []}, {"text": "A sample from the EmoInt corpus: 30112 LOVE LOVE LOVE #smile #fun #relaxationiskey joy 0.740, where 30112 is the ID of a tweet, which is labeled as \"joy\" with an intensity of 0.740.", "labels": [], "entities": [{"text": "EmoInt corpus", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.8935382962226868}]}], "tableCaptions": [{"text": " Table 1: WASSA 2017 Emotion Intensity Shared  task dataset statistics.", "labels": [], "entities": [{"text": "WASSA 2017 Emotion Intensity Shared  task", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.7416095832983652}]}, {"text": " Table 3: Effect of different word embedding  initializations for the word-level model: ran- domly initialized, pre-trained GloVe embeddings  on Twitter and Wikipedia.", "labels": [], "entities": []}]}