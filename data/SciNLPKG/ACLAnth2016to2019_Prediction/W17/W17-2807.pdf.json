{"title": [], "abstractContent": [{"text": "As robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variability-for instance, learning to ground symbols in the physical world.", "labels": [], "entities": []}, {"text": "Realistically, this task must cope with small datasets consisting of a particular users' contextual assignment of meaning to terms.", "labels": [], "entities": []}, {"text": "We present a method for processing a raw stream of cross-modal input-i.e., linguistic instructions, visual perception of a scene and a concurrent trace of 3D eye tracking fixations-to produce the segmentation of objects with a correspondent association to high-level concepts.", "labels": [], "entities": []}, {"text": "To test our framework we present experiments in a table-top object manipulation scenario.", "labels": [], "entities": [{"text": "object manipulation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.6889792531728745}]}, {"text": "Our results show our model learns the user's notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Effective and efficient human-robot collaboration requires robots to interpret ambiguous instructions and concepts within a particular context, communicated to them in a manner that feels natural and unobtrusive to the human participant in the interaction.", "labels": [], "entities": []}, {"text": "Specifically, the robot must be able to: \u2022 Understand natural language instructions, which might be ambiguous inform and meaning.", "labels": [], "entities": []}, {"text": "\u2022 Ground symbols occurring in these instructions within the surrounding physical world.", "labels": [], "entities": []}, {"text": "The images are used in order to learn the meaning of language constituents and then ground them in the physical world.", "labels": [], "entities": []}, {"text": "\u2022 Conceptually differentiate between instances of those symbolic terms, based on features pertaining to their grounded instantiation, e.g. shapes and colours of the objects.", "labels": [], "entities": []}, {"text": "Being able to relate abstract symbols to observations with physical properties in the real world is known as the physical symbol grounding problem); which is recognised as being one of the main challenges for human-robot interaction and constitutes the focus of this paper.", "labels": [], "entities": []}, {"text": "There is increasing recognition that the meaning of natural language words derives from how they manifest themselves across multiple modalities.", "labels": [], "entities": []}, {"text": "Researchers have actively studied this problem from a multitude of perspectives.", "labels": [], "entities": []}, {"text": "This includes works that explore the ability of agents to interpret natural language instructions with respect to a previously annotated semantic map or fuse high-level natural language inputs with low-level sensory observations in order to produce a semantic map ().; and tackle learning symbol grounding in language commands combined with gesture input in a table-top scenario.", "labels": [], "entities": [{"text": "learning symbol grounding in language commands", "start_pos": 280, "end_pos": 326, "type": "TASK", "confidence": 0.761867438753446}]}, {"text": "However, all these approaches depend on having predefined specifications of different concepts in the environment: they either assume a pre-annotated semantic map with respect to which they ground the linguistic input or have an offline trained symbol classifier that decides whether a detected object can be labelled with a specific symbol; e.g. colour and shape in).", "labels": [], "entities": []}, {"text": "Thus in order to deploy such a system, one should have access to an already trained classifier for every anticipated symbol, prior to any user interaction.", "labels": [], "entities": []}, {"text": "Multi-modal learning algorithms based on deep neural networks are also popular for grounding natural language instructions to the shared physical environment).", "labels": [], "entities": []}, {"text": "But the majority of these algorithms depend crucially on large and pre-labelled datasets, and the challenge is in collecting these large-scale labelled datasets so that they not only capture the variability in language but also manage to represent the nuances (especially across multiple high-bandwidth modalities, such as vision and eye-tracking) of inter-personal variability in assignment of meaning (e.g., what one person calls mustard another might call yellow), which we claim is a key attribute of freeform linguistic instructions in human-robot interaction applications.", "labels": [], "entities": [{"text": "assignment of meaning (e.g., what one person calls mustard another might call yellow)", "start_pos": 381, "end_pos": 466, "type": "TASK", "confidence": 0.5959115028381348}]}, {"text": "If a previously unseen instruction/visual observation is presented to these systems, they might fail to ground or recognize them in the way that the user might have intended in that specific setting.", "labels": [], "entities": []}, {"text": "potentially bypasses the need to collect a big dataset by demonstrating that a model trained in simulation can be successfully deployed on a robot in the real world.", "labels": [], "entities": []}, {"text": "However, the problem is then shifted to generating task-specific training data in a simulator which approximates the real world well enough.", "labels": [], "entities": []}, {"text": "A proposed alternative to this off-line learning approach is to interactively teach an embodied agent about its surrounding world, assuming limited prior knowledge.", "labels": [], "entities": []}, {"text": "demonstrates a model for incrementally learning the visual representation of words, but relies on temporally aligned videos with corresponding annotated natural language inputs. and represent the online concept learning problem as a variation of the interactive \"I Spy\" game.", "labels": [], "entities": []}, {"text": "However, these approaches assume an initial learning/exploratory phase in the world and extracted features are used as training data for all concept models associated with an object.", "labels": [], "entities": []}, {"text": "introduce a method called GLIDE (see \u00a72.2 for details), which successfully teaches agents how to map abstract instructions, represented as a LISP-like program, into their physical equivalents in the world.", "labels": [], "entities": []}, {"text": "Our work builds on this method: it uses it to achieve natural language symbol grounding, as a by-product of user interaction in a task-oriented scenario.", "labels": [], "entities": [{"text": "natural language symbol grounding", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.6265852600336075}]}, {"text": "Our approach achieves the following: \u2022 It maps natural language instructions to a planned behaviour, such as in a robotic manipulation domain; in so doing it supports a communication medium that human users find natural.", "labels": [], "entities": []}, {"text": "\u2022 It learns symbol grounding by exploiting the concept of intersective modification (Morzycki, 2013) -i.e., an object can be labelled with more than one symbol.", "labels": [], "entities": []}, {"text": "The meaning of the symbols is learned with respect to the observed features of the instances of the object.", "labels": [], "entities": []}, {"text": "In our work the agent assumes some prior knowledge about the world in the form of lowlevel features that it can extract from objects in the visual input-e.g. intensities in the primary colour channels and areas of pixel patches of any specific colour.", "labels": [], "entities": []}, {"text": "On top of this, we learn classifiers for performing symbol grounding.", "labels": [], "entities": [{"text": "symbol grounding", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7420127093791962}]}, {"text": "Each symbol has a probabilistic model which is fit to a subset of the extracted (visual) features.", "labels": [], "entities": []}, {"text": "When anew instruction is received, the classifier for each symbol makes a decision regarding the object in the world (and their respective features) to which the symbol maybe grounded.", "labels": [], "entities": []}, {"text": "Crucially, the data from which these classifiers are learned is collected from demonstrations at 'run time' and not prior to the specific human-robot interaction.", "labels": [], "entities": []}, {"text": "Images of objects are extracted from the high-frequency eye tracking and video streams, while symbols that refer to these objects in the images are extracted from the parsed natural language instructionssee.", "labels": [], "entities": []}, {"text": "Through cross-modal instructions,: Overview of the full system pipeline.", "labels": [], "entities": []}, {"text": "For instance, while observing how to make a fruit salad in a kitchen, apart from learning the sequence of steps, the system would also gain an initial approximation of the visual appearance of different pieces of fruit and their associated natural language symbols.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present results from initial experiments based on the framework in.", "labels": [], "entities": []}, {"text": "We focus our explanation on steps 3 and 4 in that figure, as these are the pertinent and novel elements introduced here.", "labels": [], "entities": []}, {"text": "The input data for (c) is derived from the process already well described in ().", "labels": [], "entities": []}, {"text": "For our experiments we used a total of six symbols defining S: 3 for colour (red, blue and yellow); and 3 for shape (cell, block, cube).", "labels": [], "entities": []}, {"text": "We used four extracted features for F : R, G, B values and pixel area.", "labels": [], "entities": []}, {"text": "The objects used were construction blocks that can be stacked together and images of them were gathered in a tabletop robotic manipulation setup (see).", "labels": [], "entities": []}, {"text": "Based on the empirical statistics of the recognition process in (, our input dataset to the Symbol Meaning Learning algorithm consists of 75% correctly annotated and 25% mislabelled images.", "labels": [], "entities": [{"text": "Symbol Meaning Learning", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7785041729609171}]}, {"text": "The total training dataset comprised of approximately 2000 labelled image patches, each of which is labelled with two symbols-e.g. blue cell, red block, yellow cube, etc.", "labels": [], "entities": []}, {"text": "The additional test set was designed in two parts: one that would test colour recognition and one that would test shape recognition.", "labels": [], "entities": [{"text": "colour recognition", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.6888645887374878}, {"text": "shape recognition", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.9080024659633636}]}, {"text": "Overall, 48 objects were presented to the algorithm where the features for each object would fall into one of the following categories: \u2022 Previously seen features (left)) \u2022 Previously unseen features, close to the features of the training data ( (middle)) \u2022 Previously unseen features, not close to the features of the training data ( (right))  Inference over new images is performed by thresholding the probability density function (PDF) values from the model parameters for each symbol.", "labels": [], "entities": []}, {"text": "The idea is to test how well the algorithm can differentiate the learned concepts with slight variations from concepts it has not seen before: e.g.: Variations in the objects from the test set for colour (top half) and shape (bottom half) given that the algorithm was trained on 3 colours and 3 shapes, we would expect that it should recognize different hues of the 3 colours and objects with similar shapes to the original 3; however, it may not be able to recognize objects with completely different features.", "labels": [], "entities": []}, {"text": "Moreover, we further group different symbols into concept groups.", "labels": [], "entities": []}, {"text": "If any two symbols are described by the same features, it is safe to assume that those two symbols are mutually exclusive: that is, they cannot both describe an object simultaneously.", "labels": [], "entities": []}, {"text": "Thus we go over each concept group and if there are symbols yielding PDF values above a predefined threshold, we assign the new image the symbol from that group with the highest PDF.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Confusion matrix for colour symbols", "labels": [], "entities": []}]}