{"title": [{"text": "Biasing Attention-Based Recurrent Neural Networks Using External Alignment Information", "labels": [], "entities": [{"text": "Biasing Attention-Based Recurrent Neural Networks", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8431926131248474}]}], "abstractContent": [{"text": "This work explores extending attention-based neural models to include alignment information as input.", "labels": [], "entities": []}, {"text": "We modify the attention component to have dependence on the current source position.", "labels": [], "entities": []}, {"text": "The attention model is then used as a lexical model together with an additional alignment model to generate translation.", "labels": [], "entities": []}, {"text": "The attention model is trained using external alignment information, and it is applied in decoding by performing beam search over the lexical and alignment hypotheses.", "labels": [], "entities": []}, {"text": "The alignment model is used to score these alignment candidates.", "labels": [], "entities": []}, {"text": "We demonstrate that the attention layer is capable of using the alignment information to improve over the baseline attention model that uses no such alignments.", "labels": [], "entities": []}, {"text": "Our experiments are performed on two tasks: WMT 2016 English\u2192Romanian and WMT 2017 German\u2192English.", "labels": [], "entities": [{"text": "WMT 2016 English\u2192Romanian", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.8799733638763427}, {"text": "WMT 2017 German\u2192English", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.9220579624176025}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has emerged recently as a successful end-to-end statistical machine translation approach.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8347089191277822}, {"text": "statistical machine translation", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.6409826278686523}]}, {"text": "The best performing NMT systems use an attention mechanism that focuses the attention of the decoder on parts of the source sentence ().", "labels": [], "entities": []}, {"text": "The attention component is computed as an intermediate part of the model, and is trained jointly with the rest of the model.", "labels": [], "entities": []}, {"text": "The approach is appealing because (1) it is end-to-end, where the neural model is trained from scratch without assistance from other trained models, and (2) the attention component is trained jointly with the rest of the model, requiring no pre-computed alignments.", "labels": [], "entities": []}, {"text": "In this work, we raise the question whether the attention component is self-sufficient to attend to the source side, and if it can still benefit from explicit dependence on the alignment information.", "labels": [], "entities": []}, {"text": "To this end, we modify the attention model to bias the attention layer towards the alignment information, and evaluate the model in a generative framework consisting of two steps: alignment prediction followed by lexical translation.", "labels": [], "entities": [{"text": "alignment prediction", "start_pos": 180, "end_pos": 200, "type": "TASK", "confidence": 0.9300397634506226}]}, {"text": "Two decades ago, () applied hidden Markov models to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7963780462741852}]}, {"text": "The idea was based on introducing word alignments as hidden variables, while using the firstorder Markov assumption to simplify the dependencies of the alignment sequence.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.690355509519577}]}, {"text": "The approach decomposed the translation process using a lexical model and an alignment model.", "labels": [], "entities": []}, {"text": "These models were simple tables enumerating all possible translation and alignment combinations.", "labels": [], "entities": [{"text": "translation and alignment", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.7927104830741882}]}, {"text": "Nowadays, HMM is used with IBM models to generate word alignments, which are needed to train phrase-based systems. and apply the hidden Markov model decomposition using feedforward lexical and alignment neural network models.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7161136120557785}]}, {"text": "In this work, we are interested in using more expressive models.", "labels": [], "entities": []}, {"text": "Namely, we leverage attention models as lexical models and use them with bidirectional recurrent alignment models.", "labels": [], "entities": []}, {"text": "These recurrent models are able to encode unbounded source and target context in comparison to feedforward networks.", "labels": [], "entities": []}, {"text": "The attention-based translation model is conditioned on the full source sentence, but it has no explicit dependence on alignments as input.", "labels": [], "entities": [{"text": "attention-based translation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.612437829375267}]}, {"text": "We propose to bias the attention mechanism using alignment information, while still allowing the model to compute attention weights dynamically.", "labels": [], "entities": []}, {"text": "Conditioning the model on the alignment information as such makes it possible to combine with an alignment model in a generative story.", "labels": [], "entities": [{"text": "generative story", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.9054107964038849}]}, {"text": "We demonstrate that the attention model can benefit from such external alignment information on two WMT tasks: the 2016 English\u2192Romanian task and the 2017 German\u2192English task.", "labels": [], "entities": [{"text": "WMT tasks", "start_pos": 100, "end_pos": 109, "type": "TASK", "confidence": 0.8556195795536041}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Translation results on the WMT 2016 English\u2192Romanian task and the WMT 2017  German\u2192English task.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9297219514846802}, {"text": "WMT 2016 English\u2192Romanian task", "start_pos": 37, "end_pos": 67, "type": "DATASET", "confidence": 0.8208205699920654}, {"text": "WMT 2017  German\u2192English task", "start_pos": 76, "end_pos": 105, "type": "DATASET", "confidence": 0.8944106499354044}]}, {"text": " Table 3: The effect of using the alignment model in decoding and block out in training . The alignment  bias term used here is \u03b4 j,bi c. Rows 1 and 2 are the same as rows 3 and 4 in Tab. (2). Block out means  including the alignment bias term for 50% of the training batches.", "labels": [], "entities": []}, {"text": " Table 4: A comparison between the WMT  German\u2192English proposed system and the base- line attention system in terms of the alignment er- ror rate (AER). The attention baseline and the pro- posed system are the same ones shown in Tab. (2),  rows 3 and 4, respectively.", "labels": [], "entities": [{"text": "WMT  German\u2192English proposed system", "start_pos": 35, "end_pos": 70, "type": "DATASET", "confidence": 0.673704594373703}, {"text": "alignment er- ror rate (AER)", "start_pos": 123, "end_pos": 151, "type": "METRIC", "confidence": 0.9506793767213821}, {"text": "Tab. (2)", "start_pos": 229, "end_pos": 237, "type": "DATASET", "confidence": 0.9502979278564453}]}]}