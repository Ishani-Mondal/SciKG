{"title": [{"text": "Predicting Target Language CCG Supertags Improves Neural Machine Translation", "labels": [], "entities": [{"text": "Predicting Target Language CCG Supertags Improves Neural Machine Translation", "start_pos": 0, "end_pos": 76, "type": "TASK", "confidence": 0.8621104756991068}]}], "abstractContent": [{"text": "Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.766771117846171}]}, {"text": "Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled.", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6405153274536133}]}, {"text": "This work aims to answer two questions: 1) Does explicitly modeling target language syntax help NMT?", "labels": [], "entities": [{"text": "NMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9173641204833984}]}, {"text": "2) Is tight integration of words and syntax better than multitask training?", "labels": [], "entities": []}, {"text": "We introduce syntactic information in the form of CCG supertags in the decoder, by interleaving the target supertags with the word sequence.", "labels": [], "entities": []}, {"text": "Our results on WMT data show that explicitly modeling target-syntax improves machine translation quality for German\u2192English, a high-resource pair, and for Romanian\u2192English, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.8064843416213989}, {"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.695954293012619}, {"text": "prepositional phrase attachment", "start_pos": 240, "end_pos": 271, "type": "TASK", "confidence": 0.6121616760889689}]}, {"text": "Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.", "labels": [], "entities": []}, {"text": "By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German\u2192English and 1.2 BLEU for Romanian\u2192English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9988530874252319}, {"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.996932864189148}]}], "introductionContent": [{"text": "Sequence-to-sequence neural machine translation (NMT) models;) are state-of-the-art on a multitude of language-pairs (.", "labels": [], "entities": [{"text": "Sequence-to-sequence neural machine translation (NMT)", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.747214559997831}]}, {"text": "Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed captured by these models.", "labels": [], "entities": []}, {"text": "Ina detailed analysis, show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.7385417222976685}, {"text": "prepositional phrase (PP) attachment", "start_pos": 240, "end_pos": 276, "type": "TASK", "confidence": 0.6575837135314941}]}, {"text": "Another study by shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 179, "end_pos": 192, "type": "TASK", "confidence": 0.7859318852424622}]}, {"text": "Recent work which incorporates additional source-side linguistic information in NMT models ( show that even though neural models have strong learning capabilities, explicit features can still improve translation quality.", "labels": [], "entities": []}, {"text": "In this work, we examine the benefit of incorporating global syntactic information on the target-side.", "labels": [], "entities": []}, {"text": "We also address the question of how best to incorporate this information.", "labels": [], "entities": []}, {"text": "For language pairs where syntactic resources are available on both the source and target-side, we show that approaches to incorporate source syntax and target syntax are complementary.", "labels": [], "entities": []}, {"text": "We propose a method for tightly coupling words and syntax by interleaving the target syntactic representation with the word sequence.", "labels": [], "entities": []}, {"text": "We compare this to loosely coupling words and syntax using a multitask solution, where the shared parts of the model are trained to produce either a target sequence of words or supertags in a similar fashion to.", "labels": [], "entities": []}, {"text": "We use CCG syntactic categories, also known as supertags, to represent syntax explicitly.", "labels": [], "entities": []}, {"text": "Supertags provide global syntactic information locally at the lexical level.", "labels": [], "entities": []}, {"text": "They encode subcategorization information, capturing short and long range dependencies and attach-ments, and also tense and morphological aspects of the word in a given context.", "labels": [], "entities": []}, {"text": "This sentence contains two PP attachments and could lead to several disambiguation possibilities (\"in\" can attach to \"Netanyahu\" or \"receives\", and \"of\" can attach to \"capital\", \"Netanyahu\" or \"receives\" ).", "labels": [], "entities": []}, {"text": "These alternatives may lead to different translations in other languages.", "labels": [], "entities": []}, {"text": "However the supertag ((S[dcl]\\NP)/PP)/NP of \"receives\" indicates that the preposition \"in\" attaches to the verb, and the supertag (NP\\NP)/NP of \"of\" indicates that it attaches to \"capital\", thereby resolving the ambiguity.", "labels": [], "entities": []}, {"text": "Our research contributions are as follows: \u2022 We propose a novel approach to integrating target syntax at word level in the decoder, by interleaving CCG supertags in the target word sequence.", "labels": [], "entities": []}, {"text": "\u2022 We show that the target language syntax improves translation quality for German\u2192English and Romanian\u2192English as measured by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.993937611579895}]}, {"text": "Our results suggest that a tight coupling of target words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training.", "labels": [], "entities": [{"text": "translation", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.953180730342865}]}, {"text": "\u2022 We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality.", "labels": [], "entities": []}, {"text": "\u2022 We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths.", "labels": [], "entities": [{"text": "SNMT", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.962106466293335}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of sentences in the training, de- velopment and test sets.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of baseline systems in  this work and in Sennrich et al. (2016a). Case- sensitive BLEU scores reported over newstest2016  with mteval-13a.perl. 1 Normalized diacritics.", "labels": [], "entities": [{"text": "Case- sensitive", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.8465628623962402}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.5982226729393005}]}, {"text": " Table 3: Experiments with target-side syntax for German\u2192English and Romanian\u2192English. BLEU  scores reported for baseline NMT, syntax-aware NMT (SNMT) and multitasking. The SNMT system is  also combined with source dependencies. Statistical significance is indicated with * p < 0.05 and **  p < 0.01, when comparing against the NMT baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9993780851364136}, {"text": "Statistical significance", "start_pos": 229, "end_pos": 253, "type": "METRIC", "confidence": 0.8460312783718109}]}, {"text": " Table 5: Sentence counts for different linguistic  constructions.", "labels": [], "entities": []}, {"text": " Table 6: Sentence counts for different sentence  lengths.", "labels": [], "entities": []}]}