{"title": [{"text": "Using Question-Answering Techniques to Implement a Knowledge-Driven Argument Mining Approach", "labels": [], "entities": [{"text": "Implement a Knowledge-Driven Argument Mining Approach", "start_pos": 39, "end_pos": 92, "type": "TASK", "confidence": 0.6958472480376562}]}], "abstractContent": [{"text": "This short paper presents a first implementation of a knowledge-driven argument mining approach.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.6863293647766113}]}, {"text": "The major processing steps and language resources of the system are surveyed.", "labels": [], "entities": []}, {"text": "An indicative evaluation outlines challenges and improvement directions.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents a first implementation of a knowledge-driven argument mining approach based on the principles developed in . This knowledge based approach to argument mining was felt to be challenging because of the heavy load put on knowledge description and acquisition, inference pattern development and implementation complexity.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.686365008354187}, {"text": "argument mining", "start_pos": 162, "end_pos": 177, "type": "TASK", "confidence": 0.7956766188144684}, {"text": "knowledge description and acquisition", "start_pos": 238, "end_pos": 275, "type": "TASK", "confidence": 0.7225250899791718}, {"text": "inference pattern development", "start_pos": 277, "end_pos": 306, "type": "TASK", "confidence": 0.6172284483909607}]}, {"text": "The aim of this paper is to introduce an architecture for the implementation, to structure the different sources of data: lexical, knowledge base and inferences, and to explore how the data can be specified or acquired.", "labels": [], "entities": []}, {"text": "We feel that this approach allows to develop in the middle and long term an accurate argument mining system that can identify arguments in any type of text given a standpoint or a controversial issue and to explain what facets of the issue are attacked or supported, why, how and how much.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7780843079090118}]}, {"text": "It also allows, as shown in, the construction of a synthesis of arguments based on domain knowledge, which is convenient for users and domain experts.", "labels": [], "entities": []}, {"text": "An original rule-based approach to argument mining is introduced in this contribution.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.850972443819046}]}, {"text": "We feel that this analysis, due to the diversity of knowledge, is difficult to develop with statistical-based methods.", "labels": [], "entities": []}, {"text": "However, our approach is in a very early development stage: this makes comparisons with statistical systems premature and not of mush use.", "labels": [], "entities": []}, {"text": "The implementation principles and the development of the associated data and inferences raise major challenges in NLP and AI.", "labels": [], "entities": []}, {"text": "We propose here an initial experiment which nevertheless produces interesting results.", "labels": [], "entities": []}, {"text": "We show how the concepts proper to a controversial issue can be extracted and expanded for the purpose of argument mining.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.7966358661651611}]}, {"text": "Then, patterns that encode the structure of arguments are developed in association with an approach to measure their relatedness to the issue.", "labels": [], "entities": []}, {"text": "An linguistic analysis of the structure of standpoints and arguments is proposed.", "labels": [], "entities": []}, {"text": "This paper ends by an indicative evaluation that analyzes challenges, e.g. such as those developed in (), (, and identifies the necessary improvement directions.", "labels": [], "entities": []}, {"text": "Due to its limited size, this paper outlines the main features of the implementation, while references point to additional material.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider that the evaluation carried out at this stage gives indications on the feasibility and accuracy of the process and suggests a number of improvement directions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9990215301513672}]}, {"text": "The evaluation presented below is developed by components so that the difficulties of each of them can be identified.", "labels": [], "entities": []}, {"text": "It is too early, but necessary in a later stage, to compare the results of our approach with others on the basis of existing datasets such as those defined by e.g. (Stab and Gurevych 2014) or (Aharoni et ali. 2014).", "labels": [], "entities": []}, {"text": "A. Corpus characteristics: summarizes the manual annotation process, realized hereby ourselves on the 21 texts advocated in section 3A.", "labels": [], "entities": []}, {"text": "Annotation by several annotators is planned and necessary, but requires some in depth training  and competence in knowledge representation.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.6884169578552246}]}, {"text": "All the arguments found have been annotated, including redundant ones over different texts.", "labels": [], "entities": []}, {"text": "Redundant arguments (between 40% to 50% of the total because authors often copy-paste each other) have been eliminated from the analysis below, but kept for further tests.", "labels": [], "entities": []}, {"text": "indicates the total of different arguments per issue.", "labels": [], "entities": []}, {"text": "On average, 22 different arguments for or against a given issue have been found, this is quite large for this type of issue.", "labels": [], "entities": []}, {"text": "B. Knowledge and lexical representation evaluation: The head terms of issues to are: vaccination, Ebola, nuclear plants, car traffic.", "labels": [], "entities": [{"text": "Knowledge and lexical representation evaluation", "start_pos": 3, "end_pos": 50, "type": "TASK", "confidence": 0.7213062882423401}]}, {"text": "The last two terms are compound terms: they are treated as a a specialization of plant and traffic respectively, with their own Qualias, some of which being inherited from the generic terms plant and traffic.", "labels": [], "entities": [{"text": "Qualias", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.977888822555542}]}, {"text": "presents the number of Qualia structures that have been developed for this experiment and the total number of concepts included in the telic, agentive and constitutive roles, which can potentially serve to identify arguments (D. above).", "labels": [], "entities": []}, {"text": "To each of these concepts correspond one or more lexical entries.", "labels": [], "entities": []}, {"text": "It is clear that a principled and partly automatic development of Qualia structures is a cornerstone to this approach.", "labels": [], "entities": []}, {"text": "For this experiment, for each issue, it took about a half day to develop the Qualias.", "labels": [], "entities": [{"text": "Qualias", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.8079177141189575}]}, {"text": "presents the distribution of the concepts over the three levels of the concept network.", "labels": [], "entities": []}, {"text": "Level 2 has several terminal concepts, with no associated Qualia, therefore, level 3 has less concepts.", "labels": [], "entities": []}, {"text": "C. Argument kernel identification: This step is realized using TextCoop, which is well-suited for the relatively simple structures found in these texts.", "labels": [], "entities": [{"text": "Argument kernel identification", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.5673640072345734}, {"text": "TextCoop", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9097768068313599}]}, {"text": "In this experiment, there is no manual discourse structure analysis, since this is not the task that is investigated here.", "labels": [], "entities": [{"text": "discourse structure analysis", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7752289573351542}]}, {"text": "In this type of text, summarizes the accuracy of the analysis w.r.t. the manual analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9994407296180725}]}, {"text": "Correctly identified arguments are given in column 2.", "labels": [], "entities": []}, {"text": "Column 3 gives indications on the concept level used in the concept network.", "labels": [], "entities": []}, {"text": "An argument can be selected on the basis of several concepts.", "labels": [], "entities": []}, {"text": "Non-overlapping arguments may also use the same concept(s).", "labels": [], "entities": []}, {"text": "indicates the rate of incorrectly recognized arguments (noise) and of arguments not found w.r.t. to the manual annotation (silence).", "labels": [], "entities": []}, {"text": "The size of the corpus that is investigated is rather modest but for each issue we feel we have quite a good coverage in terms of argument diversity: adding new texts does not produce any new, critical, argument.", "labels": [], "entities": []}, {"text": "The main reasons for noise and silence are the following, which need to betaken into account to extend the system, and to deal with more abstract issues: -noise: (1) some sentences are selected because they are related to the issue, but they are rather comments, general rules or explanation, not arguments, in spite of their main proposition evaluative structure; (2) some sentences involve level 3 concepts in the network, and have been judged to be too weak or remote in the manual annotation.", "labels": [], "entities": []}, {"text": "-silence: (1) some sentences which have been manually annotated require additional inferences  such as those developed in (Saint-Dizier 2012) and cannot be reduced to a concept network traversal; (2) other sentences have arguments which are not related to the concept network (e.g. vaccine prevents bio-terrorism), these are of much interest but difficult to relate to the issue at stake.", "labels": [], "entities": []}, {"text": "-over-performing humans: in a few cases, the automatic analysis can over-perform human annotators.", "labels": [], "entities": []}, {"text": "For example, 7 persons died under the Ebola vaccine tests is manually annotated as an attack of issue (1).", "labels": [], "entities": [{"text": "Ebola vaccine tests", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.851910134156545}]}, {"text": "However, in our implementation, the concept 'test' is in the agentive role of the Qualia of vaccine (how the vaccine was created), it is pre-telic and cannot bean attack of the issue which considers the uses and functions (telic) of the vaccine.", "labels": [], "entities": []}, {"text": "The system correctly ignored this statement.", "labels": [], "entities": []}, {"text": "This can be modeled by an axiomatization of the semantics of the Qualia roles.", "labels": [], "entities": []}, {"text": "These limitations of our implementation raise additional knowledge representation and inference features which are of much scientific and practical interest for the evolution of this approach.", "labels": [], "entities": []}, {"text": "E. Polarity: Polarity analysis is based on the equations developed in section 3, E. above.", "labels": [], "entities": [{"text": "Polarity analysis", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7918611466884613}]}, {"text": "The system is rather simple at the moment, but seems to be relatively satisfactory, with 39 correctly assigned polarity over the 44 correctly recognized arguments (accuracy of 88%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9994356036186218}]}], "tableCaptions": [{"text": " Table 3: The concept network.", "labels": [], "entities": []}]}