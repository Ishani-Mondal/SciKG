{"title": [{"text": "A Universal Dependencies Treebank for Marathi", "labels": [], "entities": [{"text": "Universal Dependencies Treebank", "start_pos": 2, "end_pos": 33, "type": "DATASET", "confidence": 0.631272703409195}]}], "abstractContent": [{"text": "This paper describes the creation of a free and open-source dependency treebank for Marathi, the first open-source treebank for Marathi following the Universal Dependencies (UD) syntactic annotation scheme.", "labels": [], "entities": []}, {"text": "In the paper, we describe some of the syntactic and morphological phenomena in the language that required special analysis, and how they fit into the UD guidelines.", "labels": [], "entities": []}, {"text": "We also evaluate the parsing results for three popular dependency parsers on our treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9603572487831116}]}], "introductionContent": [{"text": "The Universal Dependencies (UD) project ) is a recent effort to attempt to arrive at 'universal' annotation standards for dependency treebanks.", "labels": [], "entities": []}, {"text": "These annotation standards also cover POS tags and morphology, in addition to the expected dependency relations.", "labels": [], "entities": []}, {"text": "In recent years, the UD project has been growing more popular; the CoNLL 2017 shared task on dependency parsing ( resulted in the development and release of a number of dependency parsing pipelines that parse raw text to UD annotated trees.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7440883815288544}, {"text": "dependency parsing", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7241325080394745}]}, {"text": "UD's treebanks cover a number of languages; however, there are, as with most language resources, several gaps in treebank availability for certain languages or families.", "labels": [], "entities": [{"text": "UD's treebanks", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8667747179667155}]}, {"text": "In this paper, we describe the creation of a treebank for Marathi, an Indic language spoken primarily in the state of Maharashtra in western India.", "labels": [], "entities": []}, {"text": "In Section 2 of our paper, we briefly describe the grammar and political status of Marathi.", "labels": [], "entities": []}, {"text": "Section 3 describes some prior work on Marathi NLP, including work relevant to our treebank.", "labels": [], "entities": [{"text": "Marathi NLP", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8513666391372681}]}, {"text": "Section 4 describes the creation and size of our corpus.", "labels": [], "entities": []}, {"text": "Section 5 describes some of the more interesting linguistic phenomena in Marathi and how they fit into UD guidelines.", "labels": [], "entities": []}, {"text": "Section 6 describes our evaluation methodology and our results.", "labels": [], "entities": []}, {"text": "We conclude with Section 7, where we discuss future avenues for expansion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The pipeline that we primarily use for tokenisation and tagging is the popular UDPipe (; it is a trainable pipeline consisting of a tagger, a tokeniser (MorphoDiTa) () and a parser (Parsito) (.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.9334361553192139}]}, {"text": "Having tagged and tokenised our text using UDPipe, we evaluate three parsers.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.8679289817810059}]}, {"text": "The first of these parsers is Parsito, included in UDPipe itself.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.8819120526313782}]}, {"text": "It (like many modern parsers) uses a neural network to learn transitions for parsing dependencies.", "labels": [], "entities": []}, {"text": "We evaluate UDPipe twice -once using the.", "labels": [], "entities": []}, {"text": "Similar to UDPipe, it uses neural networks for parsing: sentences are processed using bidirectional LSTMs.", "labels": [], "entities": []}, {"text": "Unlike UDPipe, however, it also offers an implementation that uses a graph-based parsing strategy.", "labels": [], "entities": []}, {"text": "Whilst BIST also allows us to use custom word embeddings, we did not do so for infrastructural reasons: using custom embeddings results in exponential model size blowup.", "labels": [], "entities": [{"text": "BIST", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.8596236109733582}]}, {"text": "We intend to rectify these issues and evaluate BIST with embeddings in the future.", "labels": [], "entities": [{"text": "BIST", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.612963855266571}]}, {"text": "Finally, our third parser is the much older MaltParser (.", "labels": [], "entities": []}, {"text": "Unlike the others, MaltParser does not use a neural network for learning transitions.", "labels": [], "entities": []}, {"text": "Given that our treebank is still fairly small, we were interested in comparing the performance of the two approaches: neural networks famously require substantial amounts of data, and despite neural parsers showing clearly better results averaged across all treebanks in competetive evaluations, we wanted to compare their performance on our treebank.", "labels": [], "entities": []}, {"text": "Whilst our primary evaluation is on end-to-end parsing, we also perform a secondary evaluation given gold-standard tokenisation and POS tags.", "labels": [], "entities": []}, {"text": "We evaluated both labelled (LAS) and unlabelled (UAS) attachment scores; we also evaluated the weighted LAS, which underweights the contribution of correctly labelling certain relations (like case and punct) to the final score.", "labels": [], "entities": [{"text": "UAS) attachment scores", "start_pos": 49, "end_pos": 71, "type": "METRIC", "confidence": 0.6069130897521973}, {"text": "LAS", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.6405562162399292}]}, {"text": "Evaluation was carried out using the same script that was officially used for the CoNLL 2017 shared task.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.8150059133768082}]}, {"text": "Each evaluation involved training 10 models for use in 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "BIST parser required some held-out data to be used as a dev set; we used 45 (fixed) sentences for this data, and ran 10-fold CV on the remainder.", "labels": [], "entities": [{"text": "BIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.969119668006897}]}, {"text": "We ran all parsers with the default parameters, except for BIST parser, where we raised the number of training epochs to 50.: Unlabelled, labelled and weighted labelled attachment scores for our parsers, evaluated on a raw text pipeline and on gold-standard tokenisation and POS tags.", "labels": [], "entities": [{"text": "BIST", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8564238548278809}]}, {"text": "refers to our tokeniser's results.", "labels": [], "entities": []}, {"text": "The poor performance of the tokeniser on multiword tokens stands out; the relatively high frequency of multiword tokens due to orthographically joined postpositions is likely one of the reasons. is the performance of two taggers: one on gold-standard tokenised data, and the other on data tokenised by UDPipe in the previous step.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 302, "end_pos": 308, "type": "DATASET", "confidence": 0.9287384152412415}]}, {"text": "Finally, we present our dependency parsing results in.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7977066040039062}]}], "tableCaptions": [{"text": " Table 1: Tokeniser results on raw text.", "labels": [], "entities": []}, {"text": " Table 2: Tagger F 1 scores evaluated with both gold standard and automatic tokenisation.", "labels": [], "entities": [{"text": "Tagger F 1", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.7555476824442545}, {"text": "gold standard", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.9455535411834717}]}, {"text": " Table 3: Unlabelled, labelled and weighted labelled attachment scores for our parsers, evaluated on a raw  text pipeline and on gold-standard tokenisation and POS tags.", "labels": [], "entities": []}]}