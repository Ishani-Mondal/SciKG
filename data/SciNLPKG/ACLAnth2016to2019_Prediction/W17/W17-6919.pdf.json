{"title": [{"text": "Communicating and Acting: Understanding Gesture in Simulation Semantics", "labels": [], "entities": [{"text": "Simulation Semantics", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.8182096183300018}]}], "abstractContent": [{"text": "In this paper, we introduce an architecture for multimodal communication between humans and computers engaged in a shared task.", "labels": [], "entities": []}, {"text": "We describe a representative dialogue between an artificial agent and a human that will be demonstrated live during the presentation.", "labels": [], "entities": []}, {"text": "This assumes a multimodal environment and semantics for facilitating communication and interaction with a computational agent.", "labels": [], "entities": []}, {"text": "To this end, we have created an embodied 3D simulation environment enabling both the generation and interpretation of multiple modalities, including: language, gesture, and the visualization of objects moving and agents performing actions.", "labels": [], "entities": []}, {"text": "Objects are encoded with rich semantic typing and action affordances, while actions themselves are encoded as multimodal expressions (programs), allowing for contextually salient inferences and decisions in the environment.", "labels": [], "entities": []}], "introductionContent": [{"text": "In order to facilitate collaborative communication between a human and a computational agent, we have been working to integrate a multimodal model of semantics (Multimodal Semantic Simulations, MSS) with a real-time visual recognition system for identifying human gestures.", "labels": [], "entities": [{"text": "Multimodal Semantic Simulations, MSS)", "start_pos": 161, "end_pos": 198, "type": "TASK", "confidence": 0.7464116414388021}]}, {"text": "The language VoxML, Visual Object Concept Modeling Language (, is used as the platform for multimodal semantic simulations in the context of human-computer communication.", "labels": [], "entities": [{"text": "VoxML", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8670761585235596}, {"text": "Visual Object Concept Modeling Language", "start_pos": 20, "end_pos": 59, "type": "TASK", "confidence": 0.615029376745224}]}, {"text": "Gestural input is recognized in real time by a convolutional neural net-based machine vision system networked to the simulation environment, which is configured for joint activity and communication between a human and a computational agent.", "labels": [], "entities": []}, {"text": "This involves the integration of inputs from speech, gesture, and action, as mediated through a dialogue manager (DM) that tracks discourse and situational context variables embodied in a shared situated simulation.", "labels": [], "entities": []}, {"text": "Hence, the dynamic of human-computer interaction changes from giving and receiving orders to a peer-to-peer conversation.", "labels": [], "entities": []}, {"text": "We explore this idea in the context of the blocks world.", "labels": [], "entities": []}, {"text": "In particular, we consider a scenario in which one person (the builder) has a table with blocks that another person (the signaler) can see.", "labels": [], "entities": []}, {"text": "We also assume the builder and signaler can see each other.", "labels": [], "entities": []}, {"text": "The signaler is then given a pattern of blocks, and their job is to get the builder to recreate the pattern.", "labels": [], "entities": []}, {"text": "While blocks world is obviously not a real-world application, it serves as a surrogate for any cooperative task where both partners share a workspace.", "labels": [], "entities": []}, {"text": "Our system design and gesture vocabulary are taken primarily from an elicitation study, similar to that introduced in but with differences in the way gestures are elicited.", "labels": [], "entities": []}, {"text": "The purpose of the study underlying our gesture vocabulary ( was to analyze the natural dyadic communication used by two people when engaging in solving a collaborative task.", "labels": [], "entities": []}, {"text": "We asked pairs of participants to collaboratively build different pre-determined structures using wooden blocks.", "labels": [], "entities": []}, {"text": "Participants were put in separate rooms with similar setups.", "labels": [], "entities": []}, {"text": "Each participant stood in front of a table facing a TV screen on the opposite end of the table.", "labels": [], "entities": []}, {"text": "Microsoft Kinect v2 sensors were: Prototype peer-to-peer interface also setup on the opposite end, facing the participant.", "labels": [], "entities": []}, {"text": "We developed software to stream live video (and audio) from the Kinect sensors between the two setups so that participants could communicate with each other as if they were facing each other at opposite ends of the same table.", "labels": [], "entities": []}, {"text": "The Kinect sensors were also used to record the experiment, providing us with RGB video, depth data, and motion capture skeletons.", "labels": [], "entities": []}, {"text": "One participant was given the role of builder and was provided with a set of 12 wooden cubes (with 4-inch sides).", "labels": [], "entities": []}, {"text": "The other participant was given the role of signaler and was given an image of an arrangement, or layout, of these blocks.", "labels": [], "entities": []}, {"text": "The signaler was assigned the task of communicating to and directing the builder to replicate the layout; the builder needed to respond to the signalers commands by placing and arranging the blocks on the table.", "labels": [], "entities": []}, {"text": "The table acted as a shared workspace, as blocks placed on the table could be seen by both participants (although from opposite perspectives).", "labels": [], "entities": []}, {"text": "Not all 12 blocks were used for every layout, and the signaler was not allowed to show the layout to the builder.", "labels": [], "entities": []}, {"text": "Since we wanted to observe natural communication inaction, participants were also not allowed to talk or strategize beforehand, and no instruction on how to speak/gesture was given from the experimenter.", "labels": [], "entities": []}, {"text": "A trial began when the experimenter presented anew block layout to the signaler and ended when the participants replicated the block layout.", "labels": [], "entities": []}, {"text": "Communication between the two varied across three conditions: (1) the signaler and builder could both see and hear each other; (2) the signaler and builder could see but not hear each other; (3) the signaler could seethe builder (and therefore the blocks on the table), but the builder can only hear the signaler.", "labels": [], "entities": []}, {"text": "In our working prototype human-computer system, the signaler is a person and the builder is an avatar, with a virtual table and virtual blocks.", "labels": [], "entities": []}, {"text": "The signaler can see a graphical projection of the virtual world, and communicates to the avatar through gestures.", "labels": [], "entities": []}, {"text": "The avatar communicates back through language (text or speech), gesture, and action in the form of moving blocks (cf.).", "labels": [], "entities": []}, {"text": "In particular, we took initial inspiration for the system design from the setup within the elicitation study where the participants could only see but not hear each other, removing the audio channel entirely and forcing them to rely on gestures to communicate, in order to assess the impact that gesture had on the communication.", "labels": [], "entities": []}, {"text": "Therefore the initial setup only allowed the signaler to communicate though gesture, and subsequent refinements have introduced speech and language input to the signaler's capability, increasing the level of communicative symmetry in the interaction.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}