{"title": [{"text": "Learning Joint Multilingual Sentence Representations with Neural Machine Translation", "labels": [], "entities": [{"text": "Multilingual Sentence Representations", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.6151605447133383}, {"text": "Neural Machine Translation", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.725507378578186}]}], "abstractContent": [{"text": "In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.7667683164278666}]}, {"text": "Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics.", "labels": [], "entities": []}, {"text": "We define anew cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences.", "labels": [], "entities": []}, {"text": "We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax.", "labels": [], "entities": []}, {"text": "These relations also hold when comparing sentences in different languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is today common practice to use distributed representations of words, often called word embeddings, in almost all NLP applications.", "labels": [], "entities": []}, {"text": "It has been shown that syntactic and semantic relations can be captured in this embedding space, see for instance ( . To process sequences of words, ie. sentences or small paragraphs, these word embeddings need to be \"combined\" into a representation of the whole sequence.", "labels": [], "entities": []}, {"text": "Common approaches include: simple techniques like bag-of-words or some type of pooling, eg. (, recursive neural networks, eg.), recurrent neural networks, in particular LSTMs, eg. (), convolutional neural networks, eg. or hierarchical approaches, eg. ( . In some NLP applications, both the input and output are sentences.", "labels": [], "entities": []}, {"text": "Avery popular approach to handle such tasks is the so-called \"encoderdecoder approach\", also named \"sequence-tosequence learning (seq2seq)\".", "labels": [], "entities": []}, {"text": "The main idea is to first encode the input sentence into an internal representation, and then to generate the output sentence from this representation.", "labels": [], "entities": []}, {"text": "Avery successful application of this paradigm is neural machine translation (NMT), see for instance).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.8064444164435068}]}, {"text": "Current best practice is to use recurrent neural networks for the encoder and decoder, but alternative architectures like convolutional networks have been also explored.", "labels": [], "entities": []}, {"text": "The performance of these vanilla seq2seq models substantially degrades with the sequence length since it is difficult to encode long sequences into a single, fixed-size representation.", "labels": [], "entities": []}, {"text": "A plausible solution is the so-called attention mechanism (: where the generation of each target word is conditioned on a weighted subset of source words, instead of the full sentence.", "labels": [], "entities": []}, {"text": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7966737151145935}]}, {"text": "In this work, we aim at learning multilingual sentence representations, i.e. which are independent of the language.", "labels": [], "entities": []}, {"text": "Since we have to compare these representations among each other, for the same or between multiple languages, we only consider representations of fixed size.", "labels": [], "entities": []}, {"text": "There are many motivations to learn such a multilingual sentence representation, in particular: \u2022 it is likely to capture the underlying semantics of the sentence (since the meaning is the only common characteristic of a sentence formulated in several languages); \u2022 it has the potential to transfer many sentence processing applications to other languages (classification, sentiment analysis, semantic similarity, etc), without the need for language specific training data; \u2022 it enables multilingual search; \u2022 such representation could be considered assort of a continuous space interlingua.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 373, "end_pos": 391, "type": "TASK", "confidence": 0.734775960445404}]}, {"text": "To train these multilingual sentence embeddings we are using the framework of NMT with multiple encoders and decoders.", "labels": [], "entities": []}, {"text": "We first describe our model in detail, relate it to existing research, and then present an experimental evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "An important question is how to evaluate multilingual joint sentence embeddings.", "labels": [], "entities": []}, {"text": "Let us first define some desired properties of such embeddings: \u2022 multilingual closeness: the representations of the same sentence for different languages should be as similar as possible; \u2022 semantic closeness: similar sentences should be also close in the embeddings space, ie. sentences conveying the same meaning, but not necessarily the syntactic structure and word choice; \u2022 preservation of content: sentence representations are usually used in the context of a task, eg. classification, multilingual NMT or semantic relatedness.", "labels": [], "entities": []}, {"text": "This requires that enough information is preserved in the representations to perform the task; \u2022 scalability to many languages: it is desirable that the metric can be extended to many languages without important computational cost or need for human labeling of data.", "labels": [], "entities": []}, {"text": "Two main approaches have been used in the literature to evaluate multilingual sentence embeddings: 1) cross-lingual document classification based on the Reuters corpus, first described in (; and 2) cross-lingual evaluation of semantic textual similarity (in short STS).", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.6133275230725607}, {"text": "Reuters corpus", "start_pos": 153, "end_pos": 167, "type": "DATASET", "confidence": 0.950372964143753}]}, {"text": "This task was first introduced in the 2016 edition of SemEval (.", "labels": [], "entities": []}, {"text": "Both tasks focus on the evaluation of joint sentence representations of two languages only.", "labels": [], "entities": []}, {"text": "In the Reuters task, a document classifier is trained on English sentence representations and then applied to texts in another language, and in the opposite direction respectively.", "labels": [], "entities": []}, {"text": "STS seeks to measure the degree of semantic equivalence between two sentences (or small paragraphs).", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9597655534744263}]}, {"text": "Semantic similarity is expressed by a score between 0 (the two sentences are completely dissimilar) and 5 (the two sentences are completely equivalent).", "labels": [], "entities": []}, {"text": "In 2016, across lingual task was introduced (Es/En) and extended to two more language pairs in 2017 (Ar/En and Tr/En).", "labels": [], "entities": []}, {"text": "In this work, we propose an additional evaluation framework for multilingual joint representations, based on similarity search.", "labels": [], "entities": []}, {"text": "Our metric can be automatically calculated without the need of new human-labeled data and scaled to many languages and large corpora.", "labels": [], "entities": []}, {"text": "end for 15: end for The details of our approach are given in algorithm 1.", "labels": [], "entities": []}, {"text": "The basic idea is to search the closest sentence in all S sentences, and count an error if it is not the reference translation.", "labels": [], "entities": []}, {"text": "This requires the calculation of S 2 distance metrics and makes only sense when there are no duplicate sentences in the corpus.", "labels": [], "entities": []}, {"text": "With increasing S it maybe also likely that the corpus contains several alternative valid translations which could be closer than the 2 http://www.statmt.org/europarl/ reference one.", "labels": [], "entities": [{"text": "S", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9977887868881226}]}, {"text": "This is difficult to handle automatically at large scale and counted as error by our algorithm.", "labels": [], "entities": []}, {"text": "Similarity search mainly evaluates the multilingual closeness property and can be easily scaled to many languages.", "labels": [], "entities": [{"text": "Similarity search", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9220675528049469}]}, {"text": "We will report results how the similarity error rate is influenced by the number of language pairs and the size of the corpus.", "labels": [], "entities": [{"text": "similarity error rate", "start_pos": 31, "end_pos": 52, "type": "METRIC", "confidence": 0.9319312373797098}]}, {"text": "We have compared three distance metrics: L2, inner product and cosine.", "labels": [], "entities": []}, {"text": "In general, cosine performed best.", "labels": [], "entities": []}, {"text": "Note that all metrics are equivalent if the vectors are normalized.", "labels": [], "entities": []}, {"text": "We have performed all our experiments with the freely available UN corpus.", "labels": [], "entities": [{"text": "UN corpus", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.8657455742359161}]}, {"text": "It contains about 12M sentences in six languages (En, Fr, Es, Ru, Ar and Zh).", "labels": [], "entities": [{"text": "Ar", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.947597086429596}]}, {"text": "We have used the version which is 6-way parallel (about 8.3M sentences).", "labels": [], "entities": []}, {"text": "This corpus comes with a predefined Dev and Test set (4000 sentences each).", "labels": [], "entities": []}, {"text": "We lowercase all texts, limit the length of the training data to 50 words and use byte-pair encoding (BPE) with a 20k vocabulary.", "labels": [], "entities": [{"text": "byte-pair encoding (BPE", "start_pos": 82, "end_pos": 105, "type": "METRIC", "confidence": 0.6227128803730011}]}, {"text": "BPE allows to limit the size of the decoder output vocabulary, it has only a small impact on the sentence length (\u2248 +20%) and it showed similar or even superior performance in NMT in comparison to many other techniques to limit the size of the output vocabulary (.", "labels": [], "entities": [{"text": "BPE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.44206148386001587}]}, {"text": "We have also found that BPE is very robust to spelling errors which is important when handling informal texts.", "labels": [], "entities": [{"text": "BPE", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5964112281799316}]}], "tableCaptions": [{"text": " Table 1: Error rates of similarity search on the UN Dev corpus. Languages are abbreviated with the  following letters: e=English, f=French, s=Spanish, r=Russian, a=Arabic, z=Chinese.", "labels": [], "entities": [{"text": "UN Dev corpus", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9731835722923279}]}, {"text": " Table 2: Error rates of similarity search on the UN Dev corpus for five language pairs (efsra). Compari- sion of LSTMs and BLSTMs of different size and depth.", "labels": [], "entities": [{"text": "UN Dev corpus", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9530959328015646}]}, {"text": " Table 3: Different M:1 strategies for three input  languages (system efs-a). The baseline with the  1:1 strategy is 1.03% (line with ID 1).", "labels": [], "entities": []}, {"text": " Table 4: Pair-wise error rates of similarity search  for 6 languages (UN Dev). Training was per- formed with a one layer BLSTM with 512 hid- dens, max-pooling and the \"efsraz-all\" strategy.", "labels": [], "entities": [{"text": "UN Dev)", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.8669882416725159}]}]}