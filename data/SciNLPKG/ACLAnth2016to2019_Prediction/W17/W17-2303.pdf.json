{"title": [{"text": "Insights into Analogy Completion from the Biomedical Domain", "labels": [], "entities": [{"text": "Analogy Completion", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8013249635696411}]}], "abstractContent": [{"text": "Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains.", "labels": [], "entities": [{"text": "Analogy completion", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9188355803489685}]}, {"text": "Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship , and that each pair is Informative with respect to the other.", "labels": [], "entities": []}, {"text": "We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs.", "labels": [], "entities": [{"text": "MAP", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.8788146376609802}, {"text": "MRR", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.8913317322731018}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9986649751663208}]}, {"text": "We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Analogical reasoning has long been a staple of computational semantics research, as it allows for evaluating how well implicit semantic relations between pairs of terms are represented in a semantic model.", "labels": [], "entities": [{"text": "Analogical reasoning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9025753438472748}]}, {"text": "In particular, the recent boom of research on learning vector space models (VSMs) for text has leveraged analogy completion as a standalone method for evaluating VSMs without using a full NLP system.", "labels": [], "entities": [{"text": "analogy completion", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7718125879764557}]}, {"text": "This is due largely to the observations of \"linguistic regularities\" as linear offsets in context-based semantic models ().", "labels": [], "entities": []}, {"text": "In the analogy completion task, a system is presented with an example term pair and a query, e.g., London:England::Paris: , and the task is to correctly fill in the blank.", "labels": [], "entities": [{"text": "analogy completion task", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.7908411224683126}]}, {"text": "Recent methods consider the vector difference between related terms as representative of the relationship between them, and use this to find the closest vocabulary term fora target analogy, e.g., England -London + Paris \u2248 France.", "labels": [], "entities": []}, {"text": "However, recent analyses reveal weaknesses of such offset-based methods, including that the use of cosine similarity often reduces to just reflecting nearest neighbor structure, and that there is significant variance in performance between different kinds of relations ().", "labels": [], "entities": []}, {"text": "We identify three key assumptions encoded in the standard offset-based methodology for analogy completion: that a given analogy has only one correct answer, that all relationships between the example pair and the query-target pair are the same, and that the example pair is sufficiently informative with respect to the query-target pair.", "labels": [], "entities": [{"text": "analogy completion", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8294423818588257}]}, {"text": "We demonstrate that these assumptions are violated in real-world data, including in existing analogy datasets.", "labels": [], "entities": []}, {"text": "We then propose several modifications to the standard methodology to relax these assumptions, including allowing for multiple correct answers, making use of multiple examples when available, and reporting mean average precision (MAP) and mean reciprocal rank (MRR) to give a more complete picture of the implicit ranking used in finding the best candidate for completing a given analogy.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 205, "end_pos": 233, "type": "METRIC", "confidence": 0.934300035238266}, {"text": "mean reciprocal rank (MRR)", "start_pos": 238, "end_pos": 264, "type": "METRIC", "confidence": 0.8940915465354919}]}, {"text": "Furthermore, we present the BioMedical Analogic Similarity Set (BMASS), a novel dataset for analogical reasoning in the biomedical domain.", "labels": [], "entities": [{"text": "analogical reasoning", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.8894538581371307}]}, {"text": "This new resource presents real-world examples of semantic relations of interest for biomedical natural language processing research, and we hope it will support further research into biomedical VSMs (.", "labels": [], "entities": []}], "datasetContent": [{"text": "We assess how well biomedical word embeddings can perform on our dataset, and explore modifications to the standard evaluation methodology to relax the assumptions described in Section 3.2.", "labels": [], "entities": []}, {"text": "We use the skip-gram embeddings trained by on the PubMed citation database, one set using a window size of 2 (PM-2) and another set with window size 30 (PM-30).", "labels": [], "entities": [{"text": "PubMed citation database", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.9567873080571493}]}, {"text": "All other word2vec hyperparameters were tuned by Chiu et al. on a combination of similarity and relatedness and named entity recognition tasks.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.6670466661453247}]}, {"text": "Additionally, we use the hyperparameters they identified (minimum frequency=5, vector dimension=200, negative samples=10, sample=1e-4, Examples of each relation, along with their mappings to UMLS REL/RELA values, are available online.", "labels": [], "entities": [{"text": "UMLS REL/RELA values", "start_pos": 191, "end_pos": 211, "type": "DATASET", "confidence": 0.7853496551513672}]}], "tableCaptions": [{"text": " Table 4. At the level of individual re- lations,", "labels": [], "entities": [{"text": "individual re- lations", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.5743406936526299}]}, {"text": " Table 3: MAP performance on the three BMASS  relations with \u2265100 unigram analogies. Uni is us- ing unigram embeddings on unigram data, Uni M  is using MWE embeddings on unigram data, and  MWE is performance with MWE embeddings  over the full MWE data.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7061517834663391}, {"text": "BMASS  relations", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.867131233215332}]}, {"text": " Table 4: Average performance over all relations in the dataset, for each set of embeddings. Results are  reported as \"Mean (Standard deviation)\" for each metric.", "labels": [], "entities": [{"text": "Mean (Standard deviation)\"", "start_pos": 119, "end_pos": 145, "type": "METRIC", "confidence": 0.8378227949142456}]}, {"text": " Table 5: Acc R , MAP, and MRR performance  variation between Single-Answer (SA), Multi- Answer (MA), and All-Info (AI) settings for  GloVe embeddings on form-of (L1) and has-free- acid-or-base-form (L6)", "labels": [], "entities": [{"text": "Acc R", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8855381011962891}, {"text": "MAP", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8884376883506775}, {"text": "MRR", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.960987389087677}]}]}