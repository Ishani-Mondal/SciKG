{"title": [], "abstractContent": [{"text": "We describe the University of Mary-land machine translation systems submitted to the WMT17 German-English Bandit Learning Task.", "labels": [], "entities": [{"text": "University of Mary-land machine translation", "start_pos": 16, "end_pos": 59, "type": "TASK", "confidence": 0.6524012327194214}, {"text": "WMT17 German-English Bandit Learning Task", "start_pos": 85, "end_pos": 126, "type": "DATASET", "confidence": 0.8078962087631225}]}, {"text": "The task is to adapt a translation system to anew domain, using only bandit feedback: the system receives a German sentence to translate, produces an English sentence, and only gets a scalar score as feedback.", "labels": [], "entities": []}, {"text": "Targeting these two challenges (adaptation and bandit learning), we built a standard neural machine translation system and extended it in two ways: (1) robust reinforcement learning techniques to learn effectively from the bandit feedback, and (2) domain adaptation using data selection from a large corpus of parallel data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 248, "end_pos": 265, "type": "TASK", "confidence": 0.7884121835231781}]}], "introductionContent": [{"text": "We describe the University of Maryland systems for bandit machine translation.", "labels": [], "entities": [{"text": "bandit machine translation", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.6976928313573202}]}, {"text": "For the shared translation task of the EMNLP 2017's second conference on machine translation (WMT17), we focused on the task of bandit machine translation.", "labels": [], "entities": [{"text": "shared translation task of the EMNLP 2017's second conference on machine translation (WMT17)", "start_pos": 8, "end_pos": 100, "type": "TASK", "confidence": 0.6894366852939129}, {"text": "bandit machine translation", "start_pos": 128, "end_pos": 154, "type": "TASK", "confidence": 0.6064814925193787}]}, {"text": "This shared task was setup, consistent with (), simultaneously as a bandit learning problem and a domain adaptation problem.", "labels": [], "entities": []}, {"text": "This raises the natural question: can we combine these potentially complementary information sources?", "labels": [], "entities": []}, {"text": "To investigate this question, we started from a standard neural machine translation (NMT) setup \u00a72 1 , and then we: 1.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.8456978698571523}]}, {"text": "applied domain adaptation techniques by data selection to the outof-domain data, with the goals of filtering out Our implementation is based on OpenNMT (, an open-source toolkit for neural harmful data and fine-tuning the training process to focus only on relevant sentences ( \u00a74).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7398100197315216}]}, {"text": "2. trained robust reinforcement learning algorithms that can effectively learn from bandit feedback ( \u00a73); this allows our model to \"test\" proposed generalizations and adapt from the provided feedback signals.", "labels": [], "entities": []}, {"text": "Tackling the problem of learning with bandit feedback is important because neural machine translation systems, like other natural language processing technology, currently learn almost exclusively from labeled data fora specific domain.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7691700657208761}]}, {"text": "While this approach is useful, it cannot scale to abroad variety of language and domains, as linguistic systems often cannot generalize well beyond their training data.", "labels": [], "entities": []}, {"text": "Machine translation systems need to be able to learn to improve their performance from naturalistic interaction with users in addition to labeled data.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7934093773365021}]}, {"text": "Bandit feedback offers systems the opportunity to \"test\" proposed generalizations and receive feedback on their performance; particularly interesting are contextual bandit systems, which make predictions based on a given input context ().", "labels": [], "entities": []}, {"text": "For example, a neural translation system trained on parliament proceedings often performs quite poorly at translating anything else.", "labels": [], "entities": []}, {"text": "However, a translation system that is deployed to facilitate conversations between users might receive either explicit feedback (e.g. thumbs up/down) on its translations, or even implicit feedback, for example, the conversation partner asking for clarifications.", "labels": [], "entities": []}, {"text": "There has recently been a flurry of work specifically addressing the bandit structured prediction problem (, of which machine translation is a special case.", "labels": [], "entities": [{"text": "bandit structured prediction problem", "start_pos": 69, "end_pos": 105, "type": "TASK", "confidence": 0.710825689136982}, {"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7861045897006989}]}, {"text": "Because this task is-at it's core-a domain adaptation problem (for which a bandit learning signal is available to \"help\"), we also explored the use of standard domain adaptation techniques.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7161101996898651}]}, {"text": "We make a strong assumption that a sizable amount of monolingual, source language data is available before bandit feedback begins.", "labels": [], "entities": []}, {"text": "We believe that in many realistic settings, one can at least get some amount of unlabeled data to begin with (we consider 40k sentences).", "labels": [], "entities": []}, {"text": "Using this monolingual data, we use data selection on a large corpus of parallel out-of-domain data to seed an initial translation model.", "labels": [], "entities": []}, {"text": "Overall, the results support the following conclusions ( \u00a7 5), based on the limited setting of one new domain and one language pair: 1.", "labels": [], "entities": []}, {"text": "data selection for domain adaptation alone improves translation quality by about 1.5 BLEU points.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7768970131874084}, {"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9512649178504944}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9991493225097656}]}, {"text": "2. on top of the domain adaptation, reinforcement learning (which requires exploration) leads to an initial degradation of about 3 BLEU points, which is recovered (on development data) after approximately 40k sentences of bandit feedback.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9983018636703491}]}, {"text": "3 One limitation of our current setup is that we used bandit feedback on development data to train a \"critic\" function for our reinforcement learning implementation, which, in the worst case, means that our results over-estimate performance on the first 120k examples (more details in \u00a75.3).", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the experiments we conducted in attempt to assess the challenges posed by bandit machine translation and our exploration of efficient algorithms to improve machine translation systems using bandit feedback.", "labels": [], "entities": [{"text": "bandit machine translation", "start_pos": 97, "end_pos": 123, "type": "TASK", "confidence": 0.6895548899968466}, {"text": "machine translation", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7073789834976196}]}, {"text": "As explained in previous sections, this task requires performing domain adaptation for machine translation through bandit feedback.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.747065931558609}, {"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7261467278003693}]}, {"text": "With this in mind, we experimented with two types of models: simple domain adaptation without using the feedbacks, and reinforcement learning models that leverage the feedbacks.", "labels": [], "entities": []}, {"text": "In the following sections, we explain how we train the regular NMT model, how we select training data for domain adaptation, and how we use reinforcement learning to leverage the bandit feedbacks.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7493704557418823}]}, {"text": "We trained our systems using the out-of-domain parallel data restricted by the shared task.", "labels": [], "entities": []}, {"text": "The entire out-of-domain dataset contains 4.5 millions parallel German-English sentences from Europarl, NewsCommentary, CommonCrawl and (*) with BPE we no longer need to prune the vocabulary, and the exact size depends on the training data.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.9866080284118652}, {"text": "NewsCommentary", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.9513612389564514}, {"text": "CommonCrawl", "start_pos": 120, "end_pos": 131, "type": "DATASET", "confidence": 0.9619684219360352}, {"text": "BPE", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.6125056147575378}]}, {"text": "Rapid data for the News Translation (constrained) task.", "labels": [], "entities": [{"text": "News Translation (constrained) task", "start_pos": 19, "end_pos": 54, "type": "TASK", "confidence": 0.7789489229520162}]}, {"text": "Our NMT model is based on OpenNMT's () PyTorch implementation of attention-based encoder-decoder model.", "labels": [], "entities": []}, {"text": "We extended their implementation and added our implementation of the A2C algorithm.", "labels": [], "entities": [{"text": "A2C", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.5470700263977051}]}, {"text": "Details of the model configuration and training hyperparameters are listed in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NMT model's training hyperparameters.", "labels": [], "entities": []}, {"text": " Table 2: average BLEU scores of domain adapta- tion systems on the development server with dif- ferent combinations of in-domain size (x-axis) and  the percentage of out-of-domain data selected (y- axis). (*) we show the BLEU score of using all the  out-of-domain data, do data selection performed  for this row.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9984517097473145}, {"text": "BLEU", "start_pos": 222, "end_pos": 226, "type": "METRIC", "confidence": 0.9987805485725403}]}, {"text": " Table 3: Average BLEU scores of domain adap- tation systems on the training server with dif- ferent combinations of in-domain size, out-of- domain percentage, beam size, and the corre- sponding BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9639437794685364}, {"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9399329423904419}]}]}