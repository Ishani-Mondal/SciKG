{"title": [{"text": "EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity", "labels": [], "entities": [{"text": "Inner attention sentence embedding", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.8186577707529068}]}], "abstractContent": [{"text": "In this paper we describe a deep learning system that has been designed and built for the WASSA 2017 Emotion Intensity Shared Task.", "labels": [], "entities": [{"text": "WASSA 2017 Emotion Intensity Shared Task", "start_pos": 90, "end_pos": 130, "type": "TASK", "confidence": 0.7180947264035543}]}, {"text": "We introduce a representation learning approach based on inner attention on top of an RNN.", "labels": [], "entities": []}, {"text": "Results show that our model offers good capabilities and is able to successfully identify emotion-bearing words to predict intensity without leveraging on lexicons, obtaining the 13 th place among 22 shared task competitors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages.", "labels": [], "entities": []}, {"text": "This large, continuous, and dynamically updated content is considered a valuable resource for researchers.", "labels": [], "entities": []}, {"text": "In particular, many of these messages contain emotional charge, conveying affectemotions, feelings and attitudes, which can be studied to understand the expression of emotion in text, as well as the social phenomena associated.", "labels": [], "entities": []}, {"text": "While studying emotion in text it is commonly useful to characterize the emotional charge of a passage based on its words.", "labels": [], "entities": []}, {"text": "Some words have affect as a core part of their meaning.", "labels": [], "entities": []}, {"text": "For example, dejected and wistful denote some amount of sadness, and are thus associated with sadness.", "labels": [], "entities": []}, {"text": "On the other hand, some words are associated with affect even though they do not denote affect.", "labels": [], "entities": []}, {"text": "For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount of sadness.", "labels": [], "entities": []}, {"text": "While analyzing the emotional content in text, mosts tasks are almost always framed as classification tasks, where the intention is to identify one emotion among many fora sentence or passage.", "labels": [], "entities": []}, {"text": "However, it is often useful for applications to know the degree to which an emotion is expressed in text.", "labels": [], "entities": []}, {"text": "To this end, the WASSA-2017 Shared Task on Emotion Intensity (Mohammad and Bravo-Marquez, 2017b) represents the first task where systems have to automatically determine the intensity of emotions in tweets.", "labels": [], "entities": [{"text": "WASSA-2017 Shared Task on Emotion Intensity", "start_pos": 17, "end_pos": 60, "type": "TASK", "confidence": 0.6257028629382452}]}, {"text": "Concretely, the objective is to given a tweet containing the emotion of joy, sadness, fear or anger, determine the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one.", "labels": [], "entities": []}, {"text": "The task is specially challenging since tweets contain informal language, spelling errors and text referring to external content.", "labels": [], "entities": []}, {"text": "Given the 140 character limit of tweets, it is also possible to find some phenomena such as the intensive usage of emoticons and of other special Twitter features, such as hashtags and usernames mentions -used to call or notify other users.", "labels": [], "entities": []}, {"text": "In this paper we describe our system designed for the WASSA-2017 Shared Task on Emotion Intensity, which we tackle based on the premise of representation learning without the usage of external information, such as lexicons.", "labels": [], "entities": [{"text": "WASSA-2017 Shared Task on Emotion Intensity", "start_pos": 54, "end_pos": 97, "type": "TASK", "confidence": 0.5904161632061005}]}, {"text": "In particular, we use a Bi-LSTM model with intra-sentence attention on top of word embeddings to generate a tweet representation that is suitable for emotion intensity.", "labels": [], "entities": [{"text": "emotion intensity", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.7113955616950989}]}, {"text": "Our results show that our proposed model offers interesting capabilities compared to approaches that do rely on external information sources.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test our model, we experiment using the training, validation and test datasets provided for the shared task), which include tweets for four emotions: joy, sadness, fear, and anger.", "labels": [], "entities": []}, {"text": "These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores (.: Data summary.", "labels": [], "entities": [{"text": "Best-Worst Scaling (BWS)", "start_pos": 27, "end_pos": 51, "type": "METRIC", "confidence": 0.8344377875328064}]}, {"text": "We experimented with GloVe) as pre-trained word embedding vectors, for sizes 25, 50 and 100.", "labels": [], "entities": []}, {"text": "These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M.", "labels": [], "entities": []}, {"text": "To pre-process the data, we used Twokenizer (), which basically provides a set of curated rules to split the tweets into tokens.", "labels": [], "entities": []}, {"text": "We also use Tweeboparser () to get the POS-tags for each tweet.", "labels": [], "entities": []}, {"text": "summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer.", "labels": [], "entities": [{"text": "Twokenizer", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.9253597259521484}]}, {"text": "We can seethe four corpora offer similar characteristics in terms of length, with across dataset maximum length of 41 tokens.", "labels": [], "entities": []}, {"text": "We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.8899758458137512}, {"text": "coverage", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9735032320022583}]}, {"text": "To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings.", "labels": [], "entities": []}, {"text": "We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions.", "labels": [], "entities": []}, {"text": "Word elongation is very common in tweets, and is usually associated to strong sentiment.", "labels": [], "entities": []}, {"text": "The following are the POS tag-derived rules we used to generate our binary features.", "labels": [], "entities": []}, {"text": "In this paper we also only report results for LSTMs, which outperformed regular RNNs as well as GRUs and a batch normalized version of the LSTM in on preliminary experiments.", "labels": [], "entities": []}, {"text": "The hidden size of the attentional component is set to match the size of the augmented hidden vectors on each case.", "labels": [], "entities": []}, {"text": "Given this setting, we explored different hyper-parameter configurations, including context window sizes of 1, 3 and 5 as well as RNN hidden state sizes of 100, 200 and 300.", "labels": [], "entities": []}, {"text": "We experimented with unidirectional and bidirectional versions of the RNNs.", "labels": [], "entities": []}, {"text": "To avoid over-fitting, we used dropout regularization, experimenting with keep probabilities of 0.5 and 0.8.", "labels": [], "entities": [{"text": "keep probabilities", "start_pos": 74, "end_pos": 92, "type": "METRIC", "confidence": 0.9753795564174652}]}, {"text": "We also added a weighed L2 regularization term to our loss function.", "labels": [], "entities": []}, {"text": "We experimented with different values for weight \u03bb, with a minimum value of 0.01 and a maximum of 0.2.", "labels": [], "entities": []}, {"text": "To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 94, "end_pos": 125, "type": "METRIC", "confidence": 0.9696748852729797}, {"text": "Spearman rank coefficient", "start_pos": 134, "end_pos": 159, "type": "METRIC", "confidence": 0.7303210000197092}]}, {"text": "For training, we used mini-batch stochastic gradient descent with a batch size of 16 and padded sequences to a maximum size of 50 tokens, given the nature of the data.", "labels": [], "entities": []}, {"text": "We used exponential decay of ratio 0.9 and early stopping on the validation when there was no improvement after 1000 steps.", "labels": [], "entities": [{"text": "validation", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.9790434837341309}]}, {"text": "Our code is available for download on GitHub 2 .  For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021: Impact of adding our binary features.", "labels": [], "entities": [{"text": "correlation", "start_pos": 190, "end_pos": 201, "type": "METRIC", "confidence": 0.9256061315536499}]}, {"text": "for embeddings of size 100.", "labels": [], "entities": []}, {"text": "However on ly the first of these values was significant, with a p-value of 3.86 \u00d7 10 \u22125 . Regarding the hidden size of the RNN, we could not find statistical difference across the tested sizes.", "labels": [], "entities": []}, {"text": "Dropout also had inconsistent effects, but was generally useful.", "labels": [], "entities": []}, {"text": "In the joy dataset, our experiments showed us that GloVe vectors of dimension 50 again outperformed others, in this case obtaining an average correlation gain of 0.052 (p = 5.6 \u00d7 10 \u22122 ) over embeddings of size 100, and of 0.062 (p = 3.1 \u00d7 10 \u22122 ) for size 25.", "labels": [], "entities": [{"text": "joy dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.7673734128475189}]}, {"text": "Regarding the hidden size of the RNN, we observed that 100 hidden units offered better performance in our experiments, with an average absolute gain of 0.052 (p = 6.5\u00d710 \u22122 ) over 50 hidden units.", "labels": [], "entities": []}, {"text": "Compared to the models with 200 hidden units, the performance difference was statistically not significant.", "labels": [], "entities": []}, {"text": "On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 (p = 7 \u00d7 10 \u22124 ) and 0.11 (p = 1.9 \u00d7 10 \u22123 ) for sizes 25 and 100, respectively.", "labels": [], "entities": []}, {"text": "When it comes to the size of the RNN hidden state, our experiments showed that using 100 hidden units offered the best results, with average absolute gains of 0.117 (p = 9 \u00d7 10 \u22124 ) and 0.108 (p = 0.002.4\u00d710 \u22123 ) over sizes 50 and 200.", "labels": [], "entities": []}, {"text": "Finally, on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points (p = 1.3 \u00d7 10 \u22123 ) oversize 25.", "labels": [], "entities": [{"text": "sadness datasets", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.7331314980983734}]}, {"text": "Results were statistically equivalent for size 100.", "labels": [], "entities": []}, {"text": "We also observed that using 50 or 100 hidden units for the RNN offered statistically equivalent results, while both of these offered better performance than when using a hidden size of 200.", "labels": [], "entities": [{"text": "RNN", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.8739592432975769}]}], "tableCaptions": [{"text": " Table 2: Summary of the best results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8817732930183411}]}, {"text": " Table 3: Impact of adding our binary features.", "labels": [], "entities": []}]}