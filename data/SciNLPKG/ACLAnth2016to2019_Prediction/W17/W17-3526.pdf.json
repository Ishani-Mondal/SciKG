{"title": [{"text": "Towards Automatic Generation of Product Reviews from Aspect-Sentiment Scores", "labels": [], "entities": [{"text": "Generation of Product Reviews", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.7660766392946243}]}], "abstractContent": [{"text": "Data-to-text generation is very essential and important in machine writing applications.", "labels": [], "entities": [{"text": "Data-to-text generation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7472310960292816}]}, {"text": "The recent deep learning models, like Recurrent Neural Networks (RNNs), have shown a bright future for relevant text generation tasks.", "labels": [], "entities": [{"text": "text generation tasks", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7816975812117258}]}, {"text": "However, rare work has been done for automatic generation of long reviews from user opinions.", "labels": [], "entities": [{"text": "automatic generation of long reviews from user opinions", "start_pos": 37, "end_pos": 92, "type": "TASK", "confidence": 0.7573688887059689}]}, {"text": "In this paper, we introduce a deep neural network model to generate long Chinese reviews from aspect-sentiment scores representing users' opinions.", "labels": [], "entities": []}, {"text": "We conduct our study within the framework of encoder-decoder networks, and we propose a hierarchical structure with aligned attention in the Long-Short Term Memory (LSTM) decoder.", "labels": [], "entities": []}, {"text": "Experiments show that our model outper-forms retrieval based baseline methods, and also beats the sequential generation models in qualitative evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text generation is a central task in the NLP field.", "labels": [], "entities": [{"text": "Text generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7904836237430573}]}, {"text": "The progress achieved in text generation will help a lot in building strong artificial intelligence (AI) that can comprehend and compose human languages.", "labels": [], "entities": [{"text": "text generation", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7949686050415039}]}, {"text": "Review generation is an interesting subtask of data-to-text generation.", "labels": [], "entities": [{"text": "Review generation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7119005024433136}, {"text": "data-to-text generation", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7887880802154541}]}, {"text": "With more and more online trades, it usually happens that customers are lazy to do brainstorming to write reviews, and sellers want to benefit from good reviews.", "labels": [], "entities": []}, {"text": "As we can see, review generation can be really useful and worthy of study.", "labels": [], "entities": [{"text": "review generation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8570326566696167}]}, {"text": "But recent researches on text generation mainly focus on generation of weather reports, financial news, sports news, and soon.", "labels": [], "entities": [{"text": "text generation", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8011433184146881}, {"text": "generation of weather reports", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.834240049123764}]}, {"text": "The task of review generation still needs to be further explored.", "labels": [], "entities": [{"text": "review generation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8977563083171844}]}, {"text": "Think about how we generate review texts: we usually have the sentiment polarities with respect to product aspects before we speak or write.", "labels": [], "entities": []}, {"text": "Inspired by this, we focus on study of review generation from structured data, which consist of aspect-sentiment scores.", "labels": [], "entities": [{"text": "review generation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6743314862251282}]}, {"text": "Traditional generation models are mainly based on rules.", "labels": [], "entities": []}, {"text": "It is time consuming to handcraft rules.", "labels": [], "entities": []}, {"text": "Thanks to the quick development of neural networks and deep learning, text generation has achieved a breakthrough in recent years in many domains, e.g., image-to-text (, video-to-text (, and textto-text (, etc.", "labels": [], "entities": [{"text": "text generation", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.8075485825538635}]}, {"text": "More and more works show that generation models with neural networks can generate meaningful and grammatical texts (.", "labels": [], "entities": []}, {"text": "However, recent studies of text generation mainly focus on generating short texts of sentence level.", "labels": [], "entities": [{"text": "text generation", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7773616909980774}]}, {"text": "There are still challenges for modern sequential generation models to handle long texts.", "labels": [], "entities": []}, {"text": "And yet there is very few work having been done in generating long reviews.", "labels": [], "entities": []}, {"text": "In this paper, we aim to address the challenging task of long review generation within the encoderdecoder neural network framework.", "labels": [], "entities": [{"text": "long review generation", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6253597040971121}]}, {"text": "Based on the encoder-decoder framework, we investigate different models to generate review texts.", "labels": [], "entities": []}, {"text": "Among these models, the encoders are typically Multi-Layer Perceptron (MLP) to embed the input aspect-sentiment scores.", "labels": [], "entities": []}, {"text": "The decoders are RNNs with LSTM units, but differ in architectures.", "labels": [], "entities": []}, {"text": "We proposed a hierarchi-cal generation model with anew attention mechanism, which shows better results compared to other models in both automatic and manual evaluations based on areal Chinese review dataset.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our work is the first attempt to generate long review texts from aspectsentiment scores with neural network models.", "labels": [], "entities": []}, {"text": "Experiments proved that it is feasible to general long product reviews with our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the popular BLEU () scores as evaluation metrics and BLEU has shown good consistent with human evaluation in many machine translation and text generation tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9946042895317078}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9981545805931091}, {"text": "machine translation and text generation", "start_pos": 122, "end_pos": 161, "type": "TASK", "confidence": 0.7254136800765991}]}, {"text": "High BLEU score means many n-grams in the hypothesis texts meets the gold-standard references.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.9683444499969482}]}, {"text": "Here, we report BLEU-2 to BLEU-4 scores, and the evaluation is conducted after Chinese word segmentation.", "labels": [], "entities": [{"text": "BLEU-2", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9976499676704407}, {"text": "BLEU-4", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9938586354255676}, {"text": "Chinese word segmentation", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.5904902617136637}]}, {"text": "The only parameters in BLEU is the weights W for n-gram precisions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9974291920661926}]}, {"text": "In this study, we set W as average weights (W i = 1 n for BLEU-n evaluation).", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9980419874191284}]}, {"text": "As for multiple answers to the same input, we put all of them into the reference set of the input.", "labels": [], "entities": []}, {"text": "The results are shown in baselines get low BLEU scores in BLEU-2, BLEU-3 and BLEU-4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9993941783905029}, {"text": "BLEU-2", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9965813755989075}, {"text": "BLEU-3", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9968243837356567}, {"text": "BLEU-4", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9902341365814209}]}, {"text": "Among these models, Cos and Match even get lower BLEU scores than the lower bound methods in some BLEU evaluations, which maybe attributed to the sparsity of the data in the training set.", "labels": [], "entities": [{"text": "Match", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9950492978096008}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9991349577903748}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9256334900856018}]}, {"text": "Pick is better than lower bound methods in all of the BLEU evaluations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9524468779563904}]}, {"text": "Compared to the retrieval based baselines, SRGMs get higher scores in BLEU-2, BLEU-3, and BLEU-4.", "labels": [], "entities": [{"text": "SRGMs", "start_pos": 43, "end_pos": 48, "type": "TASK", "confidence": 0.9045534729957581}, {"text": "BLEU-2", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9963300824165344}, {"text": "BLEU-3", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9942796230316162}, {"text": "BLEU-4", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9844761490821838}]}, {"text": "It is very promising that HRGMs get the highest BLEU scores in all evaluations, which demonstrates the effectiveness of the hierarchical structures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9993206262588501}]}, {"text": "Moreover, HRGM-a achieves better scores than HRGM-o, which verifies the helpfulness of our proposed new attention mechanism.", "labels": [], "entities": []}, {"text": "In all, the retrieval models and sequential generation models cannot handle long sequences well, but hierarchical models can handle long sequences.", "labels": [], "entities": []}, {"text": "The reviews generated by our models are of better quality according to BLEU evaluations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9639987349510193}]}, {"text": "We also perform human evaluation to further compare these models.", "labels": [], "entities": []}, {"text": "Human evaluation requires human judges to read all the results and give judgments with respect to different aspects of quality.", "labels": [], "entities": []}, {"text": "We randomly choose 50 different inputs in the test set.", "labels": [], "entities": []}, {"text": "For each input, we compare the best models in each class, specifically, Rand-s, Pick, SRGMs, HRGM-a, and the Gold (gold-standard) answer.", "labels": [], "entities": [{"text": "HRGM-a", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.7131980657577515}]}, {"text": "We employ three subjects (excluding the authors of this paper) who have good knowledge in the domain of car reviews to evaluate the outputs of the models.", "labels": [], "entities": []}, {"text": "The outputs are shuffled before shown to subjects.", "labels": [], "entities": []}, {"text": "Without any idea which output belongs to which model, the subjects are required to rate on a 5-pt Likert scale 7 about readability, accuracy, and usefulness.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9989186525344849}]}, {"text": "In our 5-pt Likert scale, 5-point means \"very satisfying\", while 1-point means \"very terrible\".", "labels": [], "entities": []}, {"text": "The ratings with respect to each aspect of quality are then averaged across the three subjects and the 50 inputs.", "labels": [], "entities": []}, {"text": "To be more specific, we define readability, accuracy, and usefulness as follows.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.999334990978241}]}, {"text": "Readability is the metric concerned with the fluency and coherence of the texts.", "labels": [], "entities": []}, {"text": "Accuracy indicates how well the review text matches the given aspects and sentiment ratings.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9955332279205322}]}, {"text": "Usefulness is more subjective, and subjects need to decide whether to accept it or not when the text is shown to them.", "labels": [], "entities": []}, {"text": "The readability, accuracy, even the length of the review text will have an effect on the usefulness metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9993821382522583}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We can see that inhuman evaluations, all the models get high scores in readability.", "labels": [], "entities": []}, {"text": "The readability score of our model HRGM-a is very close to the highest readability score achieved by Pick.", "labels": [], "entities": []}, {"text": "Rand-s gets the worst scores for accuracy and usefulness, while the rest models perform much better in these metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9994972944259644}]}, {"text": "Compared to the strong baselines Pick and SRGM-s, although our model is not the best in readability, it performs better inaccuracy and usefulness.", "labels": [], "entities": [{"text": "Pick", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9398337006568909}, {"text": "SRGM-s", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.5735293030738831}]}, {"text": "The results also demonstrate the efficacy of our proposed models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of BLEU evaluations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9264110922813416}]}, {"text": " Table 2: Human evaluation results of typical models. We set", "labels": [], "entities": []}]}