{"title": [{"text": "Using Coreference Links to Improve Spanish-to-English Machine Translation", "labels": [], "entities": [{"text": "Improve Spanish-to-English Machine Translation", "start_pos": 27, "end_pos": 73, "type": "TASK", "confidence": 0.8191265016794205}]}], "abstractContent": [{"text": "In this paper, we present a proof-of-concept of a coreference-aware decoder for document-level machine translation.", "labels": [], "entities": [{"text": "document-level machine translation", "start_pos": 80, "end_pos": 114, "type": "TASK", "confidence": 0.642752488454183}]}, {"text": "We consider that better translations should have coreference links that are closer to those in the source text, and implement this criterion in two ways.", "labels": [], "entities": []}, {"text": "First, we define a similarity measure between source and target coreference structures, by projecting the target ones onto the source ones, and then reusing existing monolingual co-reference metrics.", "labels": [], "entities": []}, {"text": "Based on this similarity measure, we re-rank the translation hypotheses of a baseline MT system for each sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.968538761138916}]}, {"text": "Alternatively, to address the lack of diversity of mentions among the MT hypotheses, we focus on mention pairs and integrate their coreference scores with MT ones, resulting in post-editing decisions.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9293306469917297}]}, {"text": "Experiments with Spanish-to-English MT on the AnCora-ES corpus show that our second approach yields a substantial increase in the accuracy of pronoun translation , while BLEU scores remain constant.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.7329052090644836}, {"text": "AnCora-ES corpus", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9650875627994537}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9994356036186218}, {"text": "pronoun translation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.6979463249444962}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9994723200798035}]}], "introductionContent": [{"text": "Considering entire texts for machine translation, rather than separate sentences, has the potential to improve the consistency of the translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7230886369943619}, {"text": "consistency", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.9835030436515808}]}, {"text": "In this paper, we focus on coreference links, which connect referring expressions that denote the same entity within or across sentences.", "labels": [], "entities": []}, {"text": "As perfect translations should provide the reader the same understanding of entities as the source texts, we propose to use the similarity of coreference links between a source text and its translation as a criterion to improve translation hypotheses.", "labels": [], "entities": []}, {"text": "This information should be beneficial to the translation of pronouns, which often depends on the properties of their antecedent, but should also ensure lexical consistency in the translation of coreferent nouns.", "labels": [], "entities": []}, {"text": "We provide here the first proof-of-concept showing that the coreference criterion can lead to measurable improvements in the translation of referring expressions, in the case of Spanish-toEnglish machine translation (MT).", "labels": [], "entities": [{"text": "Spanish-toEnglish machine translation (MT)", "start_pos": 178, "end_pos": 220, "type": "TASK", "confidence": 0.7619792719682058}]}, {"text": "To implement this criterion, we need to compute first the coreference links in the source and target texts.", "labels": [], "entities": []}, {"text": "Then, we propose and compare two approaches: either computing a global coreference score by comparing the links and using it to rerank the hypotheses of an MT system; or integrating mention-pair scores from a coreference resolution system with MT scores, and post-editing each mention to maximize the total score.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present an overview of related work on coreference and anaphora resolution and MT.", "labels": [], "entities": [{"text": "coreference and anaphora resolution", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.6456976383924484}, {"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9919849038124084}]}, {"text": "In Section 3, we explain how we compute source and target-side coreference links, respectively by taking advantage of gold standard coreference links on the Spanish AnCora-ES corpus, and using the Stanford Coreference Resolution system on the English MT output -for both coreference-aware MT methods that we present.", "labels": [], "entities": [{"text": "Spanish AnCora-ES corpus", "start_pos": 157, "end_pos": 181, "type": "DATASET", "confidence": 0.7117867668469747}, {"text": "Coreference Resolution", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.6826514899730682}, {"text": "English MT output", "start_pos": 243, "end_pos": 260, "type": "DATASET", "confidence": 0.7319357593854269}, {"text": "MT", "start_pos": 289, "end_pos": 291, "type": "TASK", "confidence": 0.686547577381134}]}, {"text": "In Section 4, we compare coreference links globally by projecting the referring expressions (mentions) from target to source texts, and measuring similarity with existing coreference resolution metrics (MUC, B3, CEAF).", "labels": [], "entities": []}, {"text": "As a sanity check, in Section 4.2, we show that better translations, in the sense of higher BLEU scores, exhibit higher coreference similarity scores as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9980798959732056}]}, {"text": "Global coreference similarity is then used in Section 4.3 as a constraint to rerank hypotheses of the Moses MT decoder.", "labels": [], "entities": [{"text": "Moses MT decoder", "start_pos": 102, "end_pos": 118, "type": "DATASET", "confidence": 0.8849708239237467}]}, {"text": "Alternatively, as the top MT hypotheses do not vary enough in terms of mentions, we propose in Section 5 a different method, which focuses only on the translation variants of the mentions, and postedits them using information from coreference chains in the source text.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9570318460464478}]}, {"text": "Finally, the results presented in Section 6 show that the second method increases the accuracy of pronoun translation from Spanish to English, while obtaining BLEU scores similar to those of the MT baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9994046688079834}, {"text": "pronoun translation from Spanish to English", "start_pos": 98, "end_pos": 141, "type": "TASK", "confidence": 0.8403464605410894}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9995085000991821}, {"text": "MT", "start_pos": 195, "end_pos": 197, "type": "TASK", "confidence": 0.7366546392440796}]}], "datasetContent": [{"text": "Coreference resolution is the task of grouping together the expressions that refer to the same entity in a text.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9247495532035828}]}, {"text": "This task includes two stages: mention identification, and coreference resolution.", "labels": [], "entities": [{"text": "mention identification", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.8052511215209961}, {"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.9692001640796661}]}, {"text": "The first stage is usually based on part-of-speech annotation and named-entity recognition.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7163796424865723}]}, {"text": "Candidate mentions are usually noun phrases, pronouns, and named entities ().", "labels": [], "entities": []}, {"text": "Coreference resolvers follow three main approaches: pairwise, re-ranking, and clustering.", "labels": [], "entities": [{"text": "Coreference resolvers", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.909157782793045}]}, {"text": "Pairwise resolvers perform a binary classification, predicting if two mentions refer to the same entity or not.", "labels": [], "entities": [{"text": "Pairwise resolvers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.566641241312027}]}, {"text": "This assumes strong independence of mentions and does not utilize features of the entire entity.", "labels": [], "entities": []}, {"text": "The second approach lists a set of candidate antecedents for each mention that are simultaneously considered to find the best match.", "labels": [], "entities": []}, {"text": "Interpolation between the best and worse candidate is considered.", "labels": [], "entities": []}, {"text": "Finally, the clustering approach considers the features of a complete cluster of mentions to decide whether a mention belongs or not to a cluster.", "labels": [], "entities": []}, {"text": "Coreference resolution is typically evaluated in comparison with a gold-standard annotation).", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.58762626349926}]}, {"text": "The main metrics used for evaluation are MUC (, which counts the minimum number of links between mentions to be inserted or deleted in order to map the evaluated document to the gold-standard.", "labels": [], "entities": [{"text": "MUC", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9940404295921326}]}, {"text": "The B 3 measure () computes precision and recall for all mentions of a document, while CEAF () computes them at the entity level.", "labels": [], "entities": [{"text": "B 3 measure", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9508185982704163}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9947351217269897}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9977229237556458}, {"text": "CEAF", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.8473724126815796}]}, {"text": "BLANC () makes use of the Rand Index, an algorithm for the evaluation of clustering.", "labels": [], "entities": [{"text": "BLANC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5988588929176331}]}, {"text": "These metrics are implemented in the scorer for CoNLL 2012) and the).", "labels": [], "entities": [{"text": "CoNLL 2012", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.948619544506073}]}, {"text": "The objective of our initial experiments is to measure how much coreference can improve the correct choices of translation of mentions, and impact of these choices on global translation quality.", "labels": [], "entities": [{"text": "translation of mentions", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.9028089841206869}]}, {"text": "We translated 10 sample documents from the test set to serve as reference translations for evaluation.", "labels": [], "entities": []}, {"text": "The evaluation of global MT quality is made with the well-known BLEU n-gram precision metric (), while the evaluation of mentions, being less standardized, is performed in several ways.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9339211583137512}, {"text": "BLEU n-gram precision metric", "start_pos": 64, "end_pos": 92, "type": "METRIC", "confidence": 0.9244396984577179}]}, {"text": "We reuse previous insights on pronoun translation and therefore score them with a: Comparison of baseline MT and our proposals for reranking or post-editing, for three metrics.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7491693794727325}, {"text": "MT", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.8841946721076965}]}, {"text": "In addition to the average scores and standard deviation over the ten test documents, we indicate the statistical significance level of the difference between each of our systems and the baseline ( * for 95.0%, * * for 99.0% and * * * for 99.9%).", "labels": [], "entities": [{"text": "statistical significance level", "start_pos": 102, "end_pos": 132, "type": "METRIC", "confidence": 0.8080743352572123}]}, {"text": "metric that automatically computes the accuracy of pronoun translation (APT) in terms of number of pronouns that are identical vs. different from a human reference translation (Miculicich Werlen and Popescu-Belis, 2016) . More originally, in order to provide a complete view of the performance, we compute the \"accuracy of noun translation\" (ANT), by reusing the same idea as in APT to count the number of exactly matched nouns between MT and the reference translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9986849427223206}, {"text": "pronoun translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.6333973109722137}, {"text": "accuracy", "start_pos": 311, "end_pos": 319, "type": "METRIC", "confidence": 0.9964091181755066}, {"text": "noun translation\" (ANT)", "start_pos": 323, "end_pos": 346, "type": "TASK", "confidence": 0.6334184557199478}]}, {"text": "We test the two proposed methods re-ranking and post-editing vs. the phrase-based statistical MT (PBSMT) baseline described in Section 4.3.", "labels": [], "entities": []}, {"text": "We also include a neural machine translation (NMT) baseline () as a reference for comparison.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.7933637301127116}]}, {"text": "We chose to build our systems over a PBSMT system for simplicity, because the word-alignment can be obtained directly from the system.", "labels": [], "entities": []}, {"text": "Additionally, we also present the results obtained with an automatic coreference resolver in the source side, namely the CorZu system, for the post-editing approach.", "labels": [], "entities": []}, {"text": "shows the results of the experiments.", "labels": [], "entities": []}, {"text": "We first calculate BLEU, APT, and ANT values at document-level, and show the values of the average and standard deviation for the three evaluated systems: baseline, and our two proposed approaches.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9988468885421753}, {"text": "APT", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.8739153742790222}, {"text": "ANT", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8832207322120667}, {"text": "standard", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9624013900756836}]}, {"text": "Additionally, we show the significance levels (t-test) of the results in comparison to the baseline.", "labels": [], "entities": []}, {"text": "The post-editing approach improves the pronoun translation quite significantly, without decreasing the overall quality of translation.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7231439501047134}]}, {"text": "This improvement is demonstrated by the rise of APT score, whereas BLUE score remains without significant change.", "labels": [], "entities": [{"text": "APT score", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9486669301986694}, {"text": "BLUE score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.985752671957016}]}, {"text": "However, the quality of the translation of nouns does not change significantly, as shown by the ANT.", "labels": [], "entities": [{"text": "ANT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.8195636868476868}]}, {"text": "The re-ranking approach shows a significant increase in the quality of pronoun translation.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7424215972423553}]}, {"text": "Nevertheless, the overall quality of translation decreases significantly, as well as the quality of noun translation.", "labels": [], "entities": [{"text": "noun translation", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.8531502485275269}]}, {"text": "These results can be explained by the limitations of this approach.", "labels": [], "entities": []}, {"text": "The optimization was done by taking into account the correlation of mentions, but the changes were made at sentence level, and the overall quality of translation at sentence level was not considered.", "labels": [], "entities": []}, {"text": "To address this problem, a combination of coreference similarity and translation probability for each sentence could be used in future.", "labels": [], "entities": []}, {"text": "shows the distribution of pronouns translated by the three evaluated systems (i.e. baseline, re-ranking, and post-editing) in comparison with the reference.", "labels": [], "entities": []}, {"text": "The number of pronouns equal to the reference increases for both proposed approaches, specially for the post-editing.", "labels": [], "entities": []}, {"text": "The pronouns that improve the most were the thirdperson personal and possessive ones.", "labels": [], "entities": []}, {"text": "Also, the translation of some of the null pronouns in the source was improved.", "labels": [], "entities": []}, {"text": "The association with other mentions of the same entity, and the representation of the entity coming from the source side was important for this improvement.: Manual evaluation of fourth randomly selected documents.", "labels": [], "entities": []}, {"text": "The evaluation was done over nouns and pronouns.", "labels": [], "entities": []}, {"text": "Finally, we perform manual evaluation by examining source mentions, as annotated over AnCora-ES, and evaluating their individual translations by the baseline MT along with the two approaches presented above (in Sections 4 vs. 5).", "labels": [], "entities": [{"text": "AnCora-ES", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.775006890296936}]}, {"text": "When presented to the evaluator, the three translations of each source sentence are provided in a random order, so that the evaluator does not know to which system they belong.", "labels": [], "entities": []}, {"text": "The evaluator assigned a score of '2' to a translation identical to the reference, '1' for translation that is different but still good or acceptable, and '0' to a wrong or unacceptable translation.", "labels": [], "entities": []}, {"text": "To minimize the time spent on manual evaluation at this stage, one evaluator rated four test documents.", "labels": [], "entities": []}, {"text": "shows the results of the manual evaluation, scored as explained above, which includes nouns and pronouns together.", "labels": [], "entities": []}, {"text": "In general, it supports the results of the automatic evaluation.", "labels": [], "entities": []}, {"text": "Here, the post-editing approach has 32 less mentions scored as \"wrong\" than the baseline, 7 of them were score as \"acceptable\", and the rest 25 as identical to the reference.", "labels": [], "entities": []}, {"text": "The re-ranking approach, despite the theoretical appeal of its definition, fails to improve noun and pronoun translation.", "labels": [], "entities": [{"text": "noun and pronoun translation", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.8036305606365204}]}, {"text": "shows examples of translations obtained with our approaches.", "labels": [], "entities": []}, {"text": "The translations of nouns are already good for the baseline, and the differences are in many cases due to the use of synonyms and acronyms.", "labels": [], "entities": []}, {"text": "Still, there are source nouns that suffer from sense ambiguity, which maybe improved by our method.", "labels": [], "entities": []}, {"text": "However, this particular test set is too small and does not contain enough instances of this type to evaluate their translations with certainty.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Coreference similarity scores (%) be- tween source and target texts for different trans- lations. The scores increase with the quality of  translations.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of baseline MT and our pro- posals for reranking or post-editing, for three met- rics. In addition to the average scores and stan- dard deviation over the ten test documents, we in- dicate the statistical significance level of the differ- ence between each of our systems and the baseline  (  *  for 95.0%,  *  *  for 99.0% and  *  *  *  for 99.9%).", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9269475936889648}, {"text": "stan- dard deviation", "start_pos": 146, "end_pos": 166, "type": "METRIC", "confidence": 0.9216313660144806}]}, {"text": " Table 4: Manual evaluation of fourth randomly se- lected documents. The evaluation was done over  nouns and pronouns.", "labels": [], "entities": []}]}