{"title": [{"text": "Learning Phrase Embeddings from Paraphrases with GRUs", "labels": [], "entities": [{"text": "Learning Phrase Embeddings", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.617179811000824}]}], "abstractContent": [{"text": "Learning phrase representations has been widely explored in many Natural Language Processing (NLP) tasks (e.g., Sentiment Analysis, Machine Translation) and has shown promising improvements.", "labels": [], "entities": [{"text": "Learning phrase representations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6578570008277893}, {"text": "Sentiment Analysis", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.8933072984218597}, {"text": "Machine Translation)", "start_pos": 132, "end_pos": 152, "type": "TASK", "confidence": 0.831784188747406}]}, {"text": "Previous studies either learn non-compositional phrase representations with general word embedding learning techniques or learn compositional phrase representations based on syntactic structures, which either require huge amounts of human annotations or cannot be easily generalized to all phrases.", "labels": [], "entities": []}, {"text": "In this work, we propose to take advantage of large-scaled paraphrase database and present a pair-wise gated recurrent units (pairwise-GRU) framework to generate composi-tional phrase representations.", "labels": [], "entities": []}, {"text": "Our framework can be re-used to generate representations for any phrases.", "labels": [], "entities": []}, {"text": "Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks.", "labels": [], "entities": [{"text": "phrase similarity tasks", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.8497751355171204}]}], "introductionContent": [{"text": "Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (, Semantic Parsing (;, Sentiment Analysis (, Question Answering () and machine translation () and have yielded stateof-the-art results.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.7797150313854218}, {"text": "Semantic Parsing", "start_pos": 163, "end_pos": 179, "type": "TASK", "confidence": 0.7681417763233185}, {"text": "Sentiment Analysis", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.8924791514873505}, {"text": "Question Answering", "start_pos": 206, "end_pos": 224, "type": "TASK", "confidence": 0.8167401254177094}, {"text": "machine translation", "start_pos": 232, "end_pos": 251, "type": "TASK", "confidence": 0.842644065618515}]}, {"text": "However, single word embeddings are not enough to express natural languages.", "labels": [], "entities": []}, {"text": "In many applications, we need embeddings for phrases.", "labels": [], "entities": []}, {"text": "For example, in Information Extraction, we need representations for multi-word entity mentions, and in Question Answering, we may need representations for even longer question and answer phrases.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7814605534076691}, {"text": "Question Answering", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7930116355419159}]}, {"text": "Generally, there are two types of models to learn phrase emmbeddings: noncompositional models and compositional models.", "labels": [], "entities": []}, {"text": "Noncompositional models treat phrases as single information units while ignoring their components and structures.", "labels": [], "entities": []}, {"text": "Embeddings of phrases can thus be learned with general word embedding learning techniques (, however, such methods are not scalable to all English phrases and suffer from data sparsity.", "labels": [], "entities": []}, {"text": "On the other hand, compositional models derives a phrase's embedding from the embeddings of its component words (.", "labels": [], "entities": []}, {"text": "Previous work have shown good results from compositional models which simply used predefined functions such as element-wise addition ( ).", "labels": [], "entities": []}, {"text": "However, such methods ignore word orders and cannot capture complex linguistic phenomena.", "labels": [], "entities": []}, {"text": "Other studies on compositional models learn complex composition functions from data.", "labels": [], "entities": []}, {"text": "For instance, the Recursive Neural Network () finds all linguistically plausible phrases in a sentence and recursively compose phrase embedding from subphrase embeddings with learned matrix/tensor transformations.", "labels": [], "entities": []}, {"text": "Since compositional models can derive embeddings for unseen phrases from word embeddings, they suffer less from data sparsity.", "labels": [], "entities": []}, {"text": "However, the difficulty of training such complex compositional models lies in the choice of training data.", "labels": [], "entities": []}, {"text": "Although compositional models can be trained unsupervisedly with auto encoders such as the Recursive Auto Encoder), such models ignore contexts and actual usages of phrases and thus cannot fully capture the semantics of phrases.", "labels": [], "entities": []}, {"text": "Some previous work train compositional models fora specific task, such as Sentiment Analysis) or syntactic parsing (.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.8984038233757019}, {"text": "syntactic parsing", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7512578964233398}]}, {"text": "But these methods require large amounts of human annotated data.", "labels": [], "entities": []}, {"text": "Moreover, the embeddings obtained will be biased to a specific task and thus will not be applicable for other tasks.", "labels": [], "entities": []}, {"text": "A more general source of training data which does not require human annotation is plain text through language modeling.", "labels": [], "entities": []}, {"text": "For example, trained compositional models on bigram noun phrases with the language modeling objective.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7078930884599686}]}, {"text": "However, using the language modeling objective to train compositional models to compose every phrase in plain text would be impractical for large corpus.", "labels": [], "entities": []}, {"text": "In this work, we are aiming to tackle these challenges and generate more general and highquality phrase embeddings.", "labels": [], "entities": []}, {"text": "While it's impossible to provide \"gold\" annotation for the semantics of a phrase, we propose to take advantage of the largescaled paraphrases, since the only criteria of determining two phrases are parallel is that they express the same meaning.", "labels": [], "entities": []}, {"text": "This property can be naturally used as a training objective.", "labels": [], "entities": []}, {"text": "Considering this, we propose a general framework to train phrase embeddings on paraphrases.", "labels": [], "entities": []}, {"text": "We designed a pairwise-GRU architecture, which consists of a pair of GRU encoders on two paraphrases.", "labels": [], "entities": []}, {"text": "Our framework has much better generalizability.", "labels": [], "entities": []}, {"text": "Although in this work, we only trained and tested our framework on short paraphrases, our model can be further applied to any longer phrases.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our framework on various phrase similarity tasks.", "labels": [], "entities": [{"text": "phrase similarity tasks", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.8431227803230286}]}, {"text": "Results show that our model can achieve state-of-the-art performance on capturing semantics of phrases.", "labels": [], "entities": []}], "datasetContent": [{"text": "We randomly split the paraphrase pairs chosen from PPDB (as described in Section 2.1) to 80%, 10% and 10% as training, development and test sets.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8979422450065613}]}, {"text": "To see how the size of training data affects training results, we experimented training with 1%, 10% and 100% of our training set.", "labels": [], "entities": []}, {"text": "We also experimented setting the number of contrast phrases k to 9, 29 and 99 for each training set size (which correspond to a 10/30/100 choose 1 task for the model).", "labels": [], "entities": []}, {"text": "Finally, we evaluated the models trained under each configuration on our test set, where we set k to 99 and computed the accuracy of the model choosing a phrase's paraphrase among contrast phrases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9987598657608032}]}, {"text": "More formally, fora test example {p 1 , p 2 , c 1 , c 2 , ..., ck }, the models were given the phrase p 1 and asked to choose its paraphrase p 2 from the set {p 2 , c 1 , c 2 , ..., ck }.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of this training procedure, we also included the performance of the commonly used average encoder (AVG) on our test set.", "labels": [], "entities": [{"text": "average encoder (AVG)", "start_pos": 115, "end_pos": 136, "type": "METRIC", "confidence": 0.8146059513092041}]}, {"text": "AVG simply takes the element-wise average of a phrase's component word embeddings as the phrase's embedding.", "labels": [], "entities": [{"text": "AVG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5496698617935181}]}, {"text": "As shown in, the commonly used AVG encoder achieved a score of 88%, which suggests that it is indeed a rather effective compositional model.", "labels": [], "entities": []}, {"text": "But after adequate training on PPDB, PGRU is able to significantly improve upon AVG.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.732937216758728}, {"text": "PGRU", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.6004250049591064}, {"text": "AVG", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9229727983474731}]}, {"text": "This shows that AVG is not complex enough to fully capture semantics of phrases compared to complex compositional models like the GRU.", "labels": [], "entities": [{"text": "GRU", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.8896123766899109}]}, {"text": "It also suggests that, during PPDB training, our model can learn useful information about the meaning of phrases which were not learned byword embedding models during word embedding training.", "labels": [], "entities": []}, {"text": "From the figure, we can also see consistent performance gain from adding more training data.", "labels": [], "entities": []}, {"text": "This again proves that a large paraphrase database is useful for training compositional mod- els.", "labels": [], "entities": []}, {"text": "Moreover, for each training set size, while we observe obvious performance gain from increasing k from 9 to 29, the gain from further increasing k to 99 is more moderate.", "labels": [], "entities": []}, {"text": "Considering the amount of additional computation required, we conclude that it is not worth the computation efforts to increase k even further.", "labels": [], "entities": []}, {"text": "Following, we evaluated our model on human annotated datasets including SemEval2013 Task 5(a) (SemEval2013) () and the noun-modifer problem in Turney2012 (Turney2012)).", "labels": [], "entities": [{"text": "Turney2012 (Turney2012))", "start_pos": 143, "end_pos": 167, "type": "DATASET", "confidence": 0.800294816493988}]}, {"text": "SemEval2013 is a task to classify a phrase pair as either semantically similar or dissimilar.", "labels": [], "entities": []}, {"text": "Turney2012(5) is a task to select the most semantically similar word to the given bigram phrase among 5 candidate words.", "labels": [], "entities": [{"text": "Turney2012", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8310900330543518}]}, {"text": "In order to test the model's sensitivity to word orders, extended from Turney2012(5), Turney2012(10) reverse the bigram and add it to the original bigram side.", "labels": [], "entities": [{"text": "Turney2012", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.9476413130760193}]}, {"text": "Thus the model needs to choose a bigram from these two bigrams and also choose the most semantically similar word from 5 candidates.", "labels": [], "entities": []}, {"text": "Examples for these tasks are shown in.", "labels": [], "entities": []}, {"text": "Both tasks include separate training and evaluation sets.", "labels": [], "entities": []}, {"text": "Note that although both tasks only contain unigram and bigram noun phrases, our approach of learning phrase embeddings can be applied to n-grams of any kind.", "labels": [], "entities": []}, {"text": "We tested the performances of the GRU trained on the provided training set for each task (GRU) as well as the GRU trained only on the PPDB data (GRU(PPDB)), as described in Section 2.", "labels": [], "entities": [{"text": "PPDB data (GRU(PPDB))", "start_pos": 134, "end_pos": 155, "type": "DATASET", "confidence": 0.8737258229936872}]}, {"text": "For task-unspecific training (GRU(PPDB)), we used the training set of each task as development set and applied early stopping.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performances of our models and baselines on SemEval2013, Turney2012(5) and Tur- ney2012(10). Models are split into task-specific ones and task-unspecific ones for comparison.", "labels": [], "entities": [{"text": "Turney2012", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.9346845746040344}]}]}