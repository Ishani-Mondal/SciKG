{"title": [{"text": "Artificial Error Generation with Machine Translation and Syntactic Patterns", "labels": [], "entities": [{"text": "Artificial Error Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.675602783759435}, {"text": "Machine Translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7076863050460815}]}], "abstractContent": [{"text": "Shortage of available training data is holding back progress in the area of automated error detection.", "labels": [], "entities": [{"text": "automated error detection", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6723121603329977}]}, {"text": "This paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources.", "labels": [], "entities": []}, {"text": "We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors.", "labels": [], "entities": [{"text": "error generation", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7149127572774887}, {"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7253501713275909}]}, {"text": "In addition, we explore a system for extracting textual patterns from an annotated corpus , which can then be used to insert errors into grammatically correct sentences.", "labels": [], "entities": []}, {"text": "Our experiments show that the inclusion of artificially generated errors significantly improves error detection accuracy on both FCE and CoNLL 2014 datasets.", "labels": [], "entities": [{"text": "error detection", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.6777453273534775}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9450563192367554}, {"text": "FCE", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.9307533502578735}, {"text": "CoNLL 2014 datasets", "start_pos": 137, "end_pos": 156, "type": "DATASET", "confidence": 0.8589783509572347}]}], "introductionContent": [{"text": "Writing errors can occur in many different formsfrom relatively simple punctuation and determiner errors, to mistakes including word tense and form, incorrect collocations and erroneous idioms.", "labels": [], "entities": []}, {"text": "Automatically identifying all of these errors is a challenging task, especially as the amount of available annotated data is very limited.", "labels": [], "entities": []}, {"text": "showed that while some error detection algorithms perform better than others, it is additional training data that has the biggest impact on improving performance.", "labels": [], "entities": [{"text": "error detection", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.6831441074609756}]}, {"text": "Being able to generate realistic artificial data would allow for any grammatically correct text to be transformed into annotated examples containing writing errors, producing large amounts of additional training examples.", "labels": [], "entities": []}, {"text": "Supervised error generation systems would also provide an efficient method for anonymising the source corpus -error statistics from a private corpus can be aggregated and applied to a different target text, obscuring sensitive information in the original examination scripts.", "labels": [], "entities": [{"text": "error generation", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7509357631206512}]}, {"text": "However, the task of creating incorrect data is somewhat more difficult than might initially appear -naive methods for error generation can create data that does not resemble natural errors, thereby making downstream systems learn misleading or uninformative patterns.", "labels": [], "entities": [{"text": "error generation", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.7483940422534943}]}, {"text": "Previous work on artificial error generation (AEG) has focused on specific error types, such as prepositions and determiners), or noun number errors).", "labels": [], "entities": [{"text": "artificial error generation (AEG)", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.8197525491317114}]}, {"text": "investigated the use of linguistic information when generating artificial data for error correction, but also restricting the approach to only five error types.", "labels": [], "entities": [{"text": "error correction", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.6981066763401031}]}, {"text": "There has been very limited research on generating artificial data for all types, which is important for general-purpose error detection systems.", "labels": [], "entities": [{"text": "general-purpose error detection", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.6083546082178751}]}, {"text": "For example, the error types investigated by  cover only 35.74% of all errors present in the CoNLL 2014 training dataset, providing no additional information for the majority of errors.", "labels": [], "entities": [{"text": "CoNLL 2014 training dataset", "start_pos": 93, "end_pos": 120, "type": "DATASET", "confidence": 0.9403237998485565}]}, {"text": "In this paper, we investigate two supervised approaches for generating all types of artificial errors.", "labels": [], "entities": []}, {"text": "We propose a framework for generating errors based on statistical machine translation (SMT), training a model to translate from correct into incorrect sentences.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.7744857519865036}]}, {"text": "In addition, we describe a method for learning error patterns from an annotated corpus and transplanting them into error-free text.", "labels": [], "entities": []}, {"text": "We evaluate the effect of introducing artificial data on two error detection benchmarks.", "labels": [], "entities": [{"text": "error detection", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.6859067231416702}]}, {"text": "Our results show that each method provides significant improvements over using only the available training set, and a combination of both gives an absolute improvement of 4.3% in F 0.5 , without requiring any additional annotated data.", "labels": [], "entities": [{"text": "F", "start_pos": 179, "end_pos": 180, "type": "METRIC", "confidence": 0.9958873391151428}]}, {"text": "Original We area well-mixed class with equal numbers of boys and girls, all about 20 years old.", "labels": [], "entities": []}, {"text": "FY14 We am a well-mixed class with equal numbers of boys and girls, all about 20 years old.", "labels": [], "entities": []}, {"text": "PAT We area well-mixed class with equal numbers of boys an girls, all about 20 year old.", "labels": [], "entities": [{"text": "PAT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5259274244308472}]}, {"text": "MT We area well-mixed class with equals numbers of boys and girls, all about 20 years old.: Example artificial errors generated by three systems: the error generation method by Felice and Yuan (2014) (FY14), our pattern-based method covering all error types (PAT), and the machine translation approach to artificial error generation (MT).", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7866655588150024}, {"text": "error generation", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.6494830995798111}, {"text": "FY14", "start_pos": 201, "end_pos": 205, "type": "DATASET", "confidence": 0.9527712464332581}, {"text": "machine translation", "start_pos": 273, "end_pos": 292, "type": "TASK", "confidence": 0.7065032124519348}, {"text": "artificial error generation (MT)", "start_pos": 305, "end_pos": 337, "type": "TASK", "confidence": 0.7756700019041697}]}], "datasetContent": [{"text": "We trained our error generation models on the public FCE training set () and used them to generate additional artificial training data.", "labels": [], "entities": [{"text": "FCE training set", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9467413425445557}]}, {"text": "Grammatically correct text is needed as the starting point for inserting artificial errors, and we used two different sources: 1) the corrected version of the same FCE training set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens).", "labels": [], "entities": [{"text": "FCE training set", "start_pos": 164, "end_pos": 180, "type": "DATASET", "confidence": 0.9193706313769022}, {"text": "English Vocabulary Profile", "start_pos": 271, "end_pos": 297, "type": "DATASET", "confidence": 0.8703022996584574}]}, {"text": "While there are other text corpora that could be used (e.g., Wikipedia and news articles), our development experiments showed that keeping the writing style and vocabulary close to the target domain gives better results compared to simply including more data.", "labels": [], "entities": []}, {"text": "We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) ().", "labels": [], "entities": [{"text": "FCE test data", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9122553666432699}, {"text": "CoNLL 2014 Shared Task dataset", "start_pos": 129, "end_pos": 159, "type": "DATASET", "confidence": 0.8958053588867188}]}, {"text": "Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for training an error detection system.", "labels": [], "entities": [{"text": "error detection", "start_pos": 192, "end_pos": 207, "type": "TASK", "confidence": 0.7170492708683014}]}, {"text": "contains example sentences from the error generation systems, highlighting each of the edits that are marked as errors.", "labels": [], "entities": []}, {"text": "The error detection results can be seen in.", "labels": [], "entities": [{"text": "error detection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7488984167575836}]}, {"text": "We use F 0.5 as the main evaluation measure, which was established as the preferred measure for error correction and detection by the CoNLL-14 shared task ().", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9770866632461548}, {"text": "error correction and detection", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.789185106754303}, {"text": "CoNLL-14 shared task", "start_pos": 134, "end_pos": 154, "type": "DATASET", "confidence": 0.8083919684092203}]}, {"text": "F 0.5 calculates a weighted harmonic mean of precision and recall, which assigns twice as much importance to precision -this is motivated by practical applications, where accurate predictions from an error detection system are more important compared to coverage.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9535327553749084}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9991737008094788}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9982140064239502}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9991338849067688}]}, {"text": "For comparison, we also report the performance of the error detection system by, trained using the same FCE dataset.", "labels": [], "entities": [{"text": "error detection", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.6479608714580536}, {"text": "FCE dataset", "start_pos": 104, "end_pos": 115, "type": "DATASET", "confidence": 0.9635536670684814}]}, {"text": "The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods.", "labels": [], "entities": [{"text": "error detection", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7849813103675842}]}, {"text": "When comparing the error generation system by Felice and Yuan (2014) (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance.", "labels": [], "entities": [{"text": "error generation", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6712343543767929}, {"text": "FY14", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.9654662609100342}, {"text": "machine translation (MT)", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.8075915217399597}]}, {"text": "While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.7373503744602203}]}, {"text": "Combining the patternbased approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.6617727875709534}, {"text": "Ann+PAT+MT)", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.6603531241416931}]}, {"text": "The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.6506365686655045}]}, {"text": "We used the Approximate Randomisation Test: Error detection performance when combining manually annotated and artificial training data.", "labels": [], "entities": [{"text": "Approximate Randomisation Test", "start_pos": 12, "end_pos": 42, "type": "METRIC", "confidence": 0.8915513753890991}, {"text": "Error detection", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7934452295303345}]}, {"text": "for each of the systems using artificial data was significant over using only manual annotation.", "labels": [], "entities": []}, {"text": "In addition, the final combination system is also significantly better compared to the  system, on all three datasets.", "labels": [], "entities": []}, {"text": "While Rei and Yannakoudakis (2016) also report separate experiments that achieve even higher performance, these models were trained on a considerably larger proprietary corpus.", "labels": [], "entities": []}, {"text": "In this paper we compare error detection frameworks trained on the same publicly available FCE dataset, thereby removing the confounding factor of dataset size and only focusing on the model architectures.", "labels": [], "entities": [{"text": "error detection", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.695372074842453}, {"text": "FCE dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9610515832901001}]}, {"text": "The error generation methods can generate alternative versions of the same input text -the pattern-based method randomly samples the error locations, and the SMT system can provide an n-best list of alternative translations.", "labels": [], "entities": [{"text": "error generation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7083517611026764}, {"text": "SMT", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.9720175862312317}]}, {"text": "Therefore, we also investigated the combination of multiple error-generated versions of the input files when training error detection models.", "labels": [], "entities": []}, {"text": "shows the F 0.5 score on the development set, as the training data is increased by using more translations from the n-best list of the SMT system.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9793257514635721}, {"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9802441596984863}]}, {"text": "These results reveal that allowing the model to see multiple alternative versions of the same file gives a distinct improvement -showing the model both correct and incorrect variations of the same sentences likely assists in learning a discriminative model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Error detection performance when combining manually annotated and artificial training data.", "labels": [], "entities": [{"text": "Error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8770323395729065}]}]}