{"title": [{"text": "Machine translation with North Saami as a pivot language", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6905466318130493}]}], "abstractContent": [{"text": "Translating from a majority language into several minority languages implies duplicating both translation and terminology work.", "labels": [], "entities": [{"text": "translation", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.960514485836029}]}, {"text": "Our assumption is that a manual translation into one of the languages, and machine translation from this one into the other ones, will both cut translation time and be beneficial for work on terminology.", "labels": [], "entities": []}, {"text": "We test the functionality of North Saami as a pivot language, with subsequent machine translation into South, Lule and Inari Saami.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present a workflow with manual translation from the majority languages Finnish, Norwegian (and Swedish) into North Saami and subsequent rule-based machine translation (hereafter MT) into the target languages (hereafter TL) South, Lule and Inari Saami.", "labels": [], "entities": []}, {"text": "Thus North Saami is source language (SL) for the MT system and pivot language for the overall evaluation 1 . The system is based upon grammatical analysis of sme transfer lexica, lexical-selection rules, and transfer rules for the syntactic differences between the languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.8885083198547363}]}, {"text": "We deemed the rule-based approach a good fit for closely related languages with complex morphology and very few parallel texts.", "labels": [], "entities": []}, {"text": "In the remainder of the paper we delineate the linguistic and theoretical background of the project (Section 2), give an overview of the project (Section 3), describe the evaluation method of the systems (Section 4) and discuss different aspects of the evaluation method (Section 5).", "labels": [], "entities": []}, {"text": "Finally, we point out the importance of such systems both for research and for society (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the output of the MT systems in three steps.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9604498147964478}]}, {"text": "First, we estimated the lexical coverage, then we analysed and evaluated the amount on editing on the the MT output text via the pivot language.", "labels": [], "entities": [{"text": "MT output text", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.6671781341234843}]}, {"text": "Finally, the evaluators were asked to compare post-editing to translation of a similar text from the majority language, yet without access to any MT output text.", "labels": [], "entities": []}, {"text": "For the quantitative evaluation, we selected one text in nob and one in fin that had already been manually translated into sme.", "labels": [], "entities": []}, {"text": "Since the coverage was measured in a separate test (see Section 4.1), we added the missing sme words into each of the systems.", "labels": [], "entities": [{"text": "coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9678289294242859}]}, {"text": "Using the MT systems, we translated the sme text with a nob original into sma and smj, and the sme text with a fin original into smn.", "labels": [], "entities": []}, {"text": "For each language pair, we had three evaluators, who were all professional translators.", "labels": [], "entities": []}, {"text": "Each evaluator received both the nob or fin original and the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.49600568413734436}]}, {"text": "The task was then to produce a good target language text, either by correcting the MT version or by translating the original.", "labels": [], "entities": []}, {"text": "As two evaluators did not post-edit they are treated separately in Section 4.4.", "labels": [], "entities": []}, {"text": "For each evaluator, we calculated Word Error Rate and Position-independent Word Error Rate (hereafter WER and PER) of the MT version as compared to the post-edited text.", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 34, "end_pos": 49, "type": "METRIC", "confidence": 0.8110436797142029}, {"text": "Position-independent Word Error Rate", "start_pos": 54, "end_pos": 90, "type": "METRIC", "confidence": 0.8376285135746002}, {"text": "WER", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9826114773750305}, {"text": "PER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.890039324760437}, {"text": "MT", "start_pos": 122, "end_pos": 124, "type": "TASK", "confidence": 0.8563084006309509}]}, {"text": "WER is defined as the number of words being corrected, inserted, or deleted in the post-edited text.", "labels": [], "entities": [{"text": "WER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.974774956703186}]}, {"text": "PER differs from WER in ignoring word-order changes.", "labels": [], "entities": [{"text": "PER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9167840480804443}, {"text": "WER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.7553275227546692}]}, {"text": "Thus, a WER of 10% means that every tenth word has been changed in one way or another in the post-edited text.", "labels": [], "entities": [{"text": "WER", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9994439482688904}]}, {"text": "Average WER and PER values for all evaluators for the different languages are shown in.", "labels": [], "entities": [{"text": "WER", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9923006296157837}, {"text": "PER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9959810972213745}]}, {"text": "The best values were found for smn, which was also the language with the smallest WER/PER difference.", "labels": [], "entities": [{"text": "WER/PER difference", "start_pos": 82, "end_pos": 100, "type": "METRIC", "confidence": 0.793510913848877}]}, {"text": "sma had the highest values (i.e. worse results).", "labels": [], "entities": []}, {"text": "sma is also the language with the largest WER/PER difference.", "labels": [], "entities": [{"text": "PER difference", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.8624173700809479}]}, {"text": "Given the word order differences between sma and the other Saami languages, these values were as expected.", "labels": [], "entities": []}, {"text": "In order to get a better picture of the challenges, we looked at five different categories for each language pair.", "labels": [], "entities": []}, {"text": "This gave the picture in sma stands outwith word order being the largest category, for the two others lexical selection is largest, whereas word generation is problematic sma smj smn  for smj.", "labels": [], "entities": [{"text": "word generation", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.7330031394958496}]}, {"text": "We comment on the different types below.", "labels": [], "entities": []}, {"text": "In addition to the text discussed in the previous section (Text B), the evaluators got another, equally-sized text (Text A) in the original language (nob/fin), without a machine-translated version.", "labels": [], "entities": []}, {"text": "The level of difficulty of Text A was estimated to be similar to that of Text B. In addition to postediting or translating Text B to the target language, the evaluators were asked to translate Text A. The second part of the evaluation consisted in comparing the two tasks: translation with and without the help of a pivot language.", "labels": [], "entities": [{"text": "translation", "start_pos": 273, "end_pos": 284, "type": "TASK", "confidence": 0.9640132188796997}]}, {"text": "This step was carried out via a questionnaire 7 containing three multiple choice questions (cf.): 1.", "labels": [], "entities": []}, {"text": "Compare the time you spent on the two texts, Text A (translating from scratch) and Text B (using the MT version).", "labels": [], "entities": [{"text": "MT version", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.8006792068481445}]}, {"text": "2. How did you use the MT version?", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.8256052732467651}]}, {"text": "3. Do you think that such an MT program will be useful for you as a translator?", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9770393967628479}]}, {"text": "In addition, there were two open questions: The evaluators were asked to comment upon the terms suggested by the MT system that cannot be found  in relevant term collections, and they were invited to comment freely upon their experience with using the MT program.", "labels": [], "entities": []}, {"text": "Both the sma and smj evaluators appreciated the new terms suggested by the MT system, although, in several instances, they would not have used the terms proposed.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.877336859703064}]}, {"text": "Except for one smn evaluator, who had no comments, all others had positive overall comments to the program.", "labels": [], "entities": []}, {"text": "It was of 'great help', it did the job of looking up all unknown words, and it was able to consistently give a good translation, where a human translator might get bored and fallback to just copying the nob syntax.", "labels": [], "entities": [{"text": "translation", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.9542257189750671}]}], "tableCaptions": [{"text": " Table 1: Transfer rules for each of the language pairs. Macro  rules modify morphological attributes, as a part of ordinary  rules.", "labels": [], "entities": []}, {"text": " Table 2: Coverage of text corpus (1.0 = 100%)", "labels": [], "entities": []}, {"text": " Table 3: WER -all languages", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9266075491905212}]}, {"text": " Table 4: Distribution of correction types", "labels": [], "entities": []}, {"text": " Table 6: Answers to multiple choice questions", "labels": [], "entities": [{"text": "Answers to multiple choice questions", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.8143664121627807}]}]}