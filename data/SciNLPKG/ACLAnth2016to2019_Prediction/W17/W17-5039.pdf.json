{"title": [{"text": "Systematically Adapting Machine Translation for Grammatical Error Correction", "labels": [], "entities": [{"text": "Systematically Adapting Machine Translation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7816890925168991}, {"text": "Grammatical Error Correction", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6932727595170339}]}], "abstractContent": [{"text": "In this work we adapt machine translation (MT) to grammatical error correction, identifying how components of the statistical MT pipeline can be modified for this task and analyzing how each modification impacts system performance.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8412562131881713}, {"text": "grammatical error correction", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6163536906242371}]}, {"text": "We evaluate the contribution of each of these components with standard evaluation metrics and automatically characterize the morphological and lexical transformations made in system output.", "labels": [], "entities": []}, {"text": "Our model rivals the current state of the art using a fraction of the training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "This work presents a systematic investigation for automatic grammatical error correction (GEC) inspired by machine translation (MT).", "labels": [], "entities": [{"text": "automatic grammatical error correction (GEC)", "start_pos": 50, "end_pos": 94, "type": "TASK", "confidence": 0.7083793197359357}, {"text": "machine translation (MT)", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.8463308215141296}]}, {"text": "The task of grammatical error correction can be viewed as a noisy channel model, and therefore a MT approach makes sense, and has been applied to the task since.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6024171312650045}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9670386910438538}]}, {"text": "Currently, the best GEC systems all use machine translation in some form, whether statistical MT (SMT) as a component of a larger pipeline or neural MT (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.730957567691803}]}, {"text": "These approaches make use of a great deal of resources, and in this work we propose a lighter-weight approach to GEC by methodically examining different aspects of the SMT pipeline, identifying and applying modifications tailored for GEC, introducing artificial data, and evaluating how each of these specializations contributes to the overall performance.", "labels": [], "entities": [{"text": "GEC", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.715893566608429}, {"text": "SMT pipeline", "start_pos": 168, "end_pos": 180, "type": "TASK", "confidence": 0.9186277091503143}, {"text": "GEC", "start_pos": 234, "end_pos": 237, "type": "DATASET", "confidence": 0.8509540557861328}]}, {"text": "Specifically, we demonstrate that \u2022 Artificially generated rules improve performance by nearly 10%.", "labels": [], "entities": []}, {"text": "\u2022 Custom features describing morphological and lexical changes provide a small performance gain.", "labels": [], "entities": []}, {"text": "\u2022 Tuning to a specialized GEC metric is slightly better than tuning to a traditional MT metric.", "labels": [], "entities": [{"text": "GEC metric", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.7797987461090088}]}, {"text": "\u2022 Larger training data leads to better performance, but there is no conclusive difference between training on a clean corpus with minimal corrections and a noisy corpus with potential sentence rewrites.", "labels": [], "entities": []}, {"text": "We have developed and will release a tool to automatically characterize the types of transformations made in a corrected text, which are used as features in our model.", "labels": [], "entities": []}, {"text": "The features identify general changes such as insertions, substitutions, and deletions, and the number of each of these operations by part of speech.", "labels": [], "entities": []}, {"text": "Substitutions are further classified by whether the substitution contains a different inflected form of the original word, such as change in verb tense or noun number; if substitution has the same part of speech as the original; and if it is a spelling correction.", "labels": [], "entities": []}, {"text": "We additionally use these features to analyze the outputs generated by different systems and characterize their performance with the types of transformations it makes and how they compare to manually written corrections in addition to automatic metric evaluation.", "labels": [], "entities": []}, {"text": "Our approach, Specialized Machine translation for Error Correction (SMEC), represents a single model that handles morphological changes, spelling corrections, and phrasal substitutions, and it rivals the performance of the state-of-the-art neural MT system, which uses twice the amount of training data, most of which is not publicly available.", "labels": [], "entities": [{"text": "Specialized Machine translation for Error Correction (SMEC)", "start_pos": 14, "end_pos": 73, "type": "TASK", "confidence": 0.8204890555805631}, {"text": "spelling corrections", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.7264130562543869}]}, {"text": "The analysis provided in this work will help improve future efforts in GEC, and can be used to inform approaches rooted in both neural and statistical MT.", "labels": [], "entities": [{"text": "GEC", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9315351247787476}, {"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.7477073073387146}]}], "datasetContent": [{"text": "GEC systems are automatically evaluated by comparing their output on sentences that have been manually annotated corpora.", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7767537236213684}]}, {"text": "The Max-Match metric (M 2 ) is the most widely used, and calculates the F 0.5 over phrasal edits.", "labels": [], "entities": [{"text": "Max-Match metric (M 2 )", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.9072292347749075}, {"text": "F 0.5", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9782678484916687}]}, {"text": "proposed anew metric, GLEU, which has stronger correlation with human judgments.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9958475232124329}]}, {"text": "GLEU is based on BLEU and therefore is well-suited for MT.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.804657518863678}, {"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.990678608417511}, {"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9896709322929382}]}, {"text": "It calculates the n-gram overlap, rewarding n-grams that systems correctly changed and penalizing n-grams that were incorrectly left unchanged.", "labels": [], "entities": []}, {"text": "Unlike M 2 , it does not require token-aligned input and therefore is able to evaluate sentential rewrites instead of minimal error spans.", "labels": [], "entities": []}, {"text": "Since both metrics are commonly used, we will report the scores of both metrics in our results.", "labels": [], "entities": []}, {"text": "A new test set for GEC was recently released, JFLEG (, Unlike the CoNLL 2014 test set, which is apart of the NUCLE corpus, JFLEG contains fluency-based edits instead of error-coded corrections.", "labels": [], "entities": [{"text": "GEC", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.5225790739059448}, {"text": "CoNLL 2014 test set", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.9602731764316559}, {"text": "NUCLE corpus", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.9664255976676941}]}, {"text": "Like the Lang-8 and AESW corpora, fluency edits allow full sentence rewrites and do not constrain corrections to be error coded, and humans perceive sentences corrected with fluency edits to be more grammatical than those corrected with error-coded edits alone (.", "labels": [], "entities": [{"text": "AESW corpora", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9032508432865143}]}, {"text": "Four leading systems were evaluated on JFLEG, and the best system by both automatic metric and human evaluation is the neural MT system of (henceforth referred to as YB16).", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8998256325721741}, {"text": "YB16", "start_pos": 166, "end_pos": 170, "type": "DATASET", "confidence": 0.913108766078949}]}, {"text": "For our experiments, we use the Joshua 6 toolkit ( . Tokenization is done with Joshua and token-level alignment with fast-align ().", "labels": [], "entities": []}, {"text": "All text is lowercased, and we use a simple algorithm to recase the output.", "labels": [], "entities": []}, {"text": "We extract a hierarchical phrase-based translation model with Thrax (Weese et al., 2011) and perform parameter tuning with pairwise ranked optimization in Joshua.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.619640439748764}]}, {"text": "Our training data is from the Lang-8 corpus (Tomoya et al., 2011), which contains 1 million parallel sentences, and grammar is extracted from the 563k sentence pairs that contain corrections.", "labels": [], "entities": [{"text": "Lang-8 corpus", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8286978006362915}]}, {"text": "Systems are tuned to the JFLEG tuning set (751 sentences) and evaluated on the JFLEG test set (747 sentences).", "labels": [], "entities": [{"text": "JFLEG tuning set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8684367736180624}, {"text": "JFLEG test set", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9627194205919901}]}, {"text": "We use an English Gigaword 5-gram language model.", "labels": [], "entities": []}, {"text": "We evaluate performance with two metrics, GLEU and M 2 , which have similar rankings and 1.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9984044432640076}, {"text": "M 2", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9459823668003082}]}, {"text": "Generate POS tags of the cased input sentence 2.", "labels": [], "entities": []}, {"text": "Label proper nouns in the input 3.", "labels": [], "entities": []}, {"text": "Align the cased input tokens with the output 4.", "labels": [], "entities": []}, {"text": "Capitalize the first alphanumeric character of the output sentence (if a letter).", "labels": [], "entities": []}, {"text": "5. For each pair of aligned tokens (l i , r j ), capitalize r j if l i is labeled a proper noun or r j is the token \"i\".", "labels": [], "entities": []}, {"text": "We additionally report metric scores for the human corrections, which we determine by evaluating each reference set against the other three and reporting the mean score.", "labels": [], "entities": [{"text": "human corrections", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.6613579988479614}]}, {"text": "All systems outperform both baselines, and the spelling baseline is stronger than the MT baseline.", "labels": [], "entities": [{"text": "MT baseline", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.8007915616035461}]}, {"text": "The spelling baseline also has the highest precision except for the best automatic system, YB16, demonstrating that spelling correction is an important component in this corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9994089603424072}, {"text": "YB16", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.7230302691459656}, {"text": "spelling correction", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.8418798446655273}]}, {"text": "There is a disparity in the GLEU and M 2 scores for the baseline: the baseline GLEU is about 5% lower than the other systems but the M 2 is 30% lower.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9927608966827393}, {"text": "GLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9723803997039795}]}, {"text": "This can be attributed to the lesser extent of changes made by the baseline system which results in low recall for M 2 but which is not penalized by GLEU, which is a precision-based metric.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9994631409645081}, {"text": "GLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9955909252166748}, {"text": "precision-based", "start_pos": 166, "end_pos": 181, "type": "METRIC", "confidence": 0.9908921122550964}]}, {"text": "The human corrections have the highest metric scores, and make changes to 77% of the sentences, which is in between the number of sentences changed by YB16 and SMEC, however the human corrections have a higher mean edit distance, because the annotators made more extensive changes when a sentence needed to be corrected than any of the models.", "labels": [], "entities": [{"text": "YB16", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.9122551679611206}, {"text": "SMEC", "start_pos": 160, "end_pos": 164, "type": "DATASET", "confidence": 0.4023374915122986}, {"text": "mean edit distance", "start_pos": 210, "end_pos": 228, "type": "METRIC", "confidence": 0.6750426391760508}]}, {"text": "Our fully customized model with all modifications, Specialized Machine translation for Er-ror Correction (SMEC +morph ), scores lower than YB16 according to GLEU but has the same M 2 score.", "labels": [], "entities": [{"text": "Er-ror Correction (SMEC +morph )", "start_pos": 87, "end_pos": 119, "type": "METRIC", "confidence": 0.8616422755377633}, {"text": "YB16", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.5385574102401733}, {"text": "GLEU", "start_pos": 157, "end_pos": 161, "type": "DATASET", "confidence": 0.6271488666534424}, {"text": "M 2 score", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9747002720832825}]}, {"text": "SMEC +morph has higher M 2 recall, and visual examination of the output supports this, showing many incorrect or unnecessary number of tense changes.", "labels": [], "entities": [{"text": "SMEC +morph", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.4979429046312968}, {"text": "M 2", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.8604730367660522}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.6607627868652344}]}, {"text": "Automatic analysis reveals that it makes significantly more inflection changes than the humans or YB16 (detected with the same method described in Section 3.2), from which we can conclude that the morphological rules errors are applied too liberally.", "labels": [], "entities": []}, {"text": "If we remove the generated morphological rules but keep the spelling rules (SMEC), performance improves by 0.4 GLEU points and decreases by 0.1 M 2 points-but, more importantly, this system has higher precision and lower recall, and makes more conservative morphological changes.", "labels": [], "entities": [{"text": "spelling rules (SMEC)", "start_pos": 60, "end_pos": 81, "type": "METRIC", "confidence": 0.6612161517143249}, {"text": "GLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9975284934043884}, {"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9985461235046387}, {"text": "recall", "start_pos": 221, "end_pos": 227, "type": "METRIC", "confidence": 0.9991870522499084}]}, {"text": "Therefore, we consider SMEC, the model without artificial morphological rules, to be our best system.", "labels": [], "entities": [{"text": "SMEC", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.896026074886322}]}, {"text": "The metrics only give us a high-level overview of the changes made in the output.", "labels": [], "entities": []}, {"text": "With errorcoded text, the performance by feature type can be examined with M 2 , but this is not possible with GLEU or the un-coded JFLEG corpus.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.7706763744354248}, {"text": "JFLEG corpus", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.9110729694366455}]}, {"text": "To investigate the types of changes systems make on a more granular level, we apply the feature extraction method described in Section 3.2 to quantify the morphological and lexical transformations.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7019899785518646}]}, {"text": "While we developed this method for scoring translation rules, it can work on any aligned text, and is similar to the forthcoming ERRANT toolkit, which is uses a rule-based framework for automatically categorizes grammatical edits.", "labels": [], "entities": [{"text": "scoring translation rules", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6715329587459564}]}, {"text": "We calculate the number of each of these transformations made by to the input by each system and the human references, determining significant differences with a paired t-test (p < 0.05).", "labels": [], "entities": []}, {"text": "contains the mean number of these transformations per sentence made by SMEC, YB16, and the human-corrected references, and shows the number of operations by part of speech.", "labels": [], "entities": [{"text": "SMEC", "start_pos": 71, "end_pos": 75, "type": "TASK", "confidence": 0.7659605145454407}, {"text": "YB16", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8755894303321838}]}, {"text": "Even though the GLEU and M 2 scores of the two systems are nearly identical, they are significantly different in all of the transformations in, with SMEC having a higher edit distance from the original, but YB16 making more insertions and substitutions.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9926649928092957}, {"text": "M 2 scores", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.8722995718320211}, {"text": "SMEC", "start_pos": 149, "end_pos": 153, "type": "TASK", "confidence": 0.4145723283290863}]}, {"text": "Overall, the human corrections have a significantly more inserted tokens than either system, while YB16 makes the most substitutions and fewer deletions than SMEC or the human corrections.", "labels": [], "entities": [{"text": "YB16", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.5058495998382568}, {"text": "SMEC", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.6667678952217102}]}, {"text": "The bottom plot displays the mean number of operations by part of speech (operations include deletion, insertion, and substitution).", "labels": [], "entities": []}, {"text": "Both systems and the human corrections display similar rates of substitutions across different parts of speech, however the human references have significantly more preposition and verb operations and there are significant differences between the determiner and noun operations made by YB16 compared to SMEC and the references.", "labels": [], "entities": [{"text": "YB16", "start_pos": 286, "end_pos": 290, "type": "DATASET", "confidence": 0.9576489329338074}, {"text": "SMEC", "start_pos": 303, "end_pos": 307, "type": "TASK", "confidence": 0.6931458711624146}]}, {"text": "This information can be further analyzed by part of speech and edit operation, and the same information is available for other word classes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An example rule from our grammar and  the non-zero feature values from Section 3.2.", "labels": [], "entities": []}, {"text": " Table 3: Results on the JFLEG test set. In addition to the GLEU and M 2 scores, we also report the  percent of sentences changed from the input and the mean Levensthein distance.", "labels": [], "entities": [{"text": "JFLEG test set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9028207063674927}, {"text": "GLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9969233870506287}, {"text": "M 2", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8869794011116028}]}]}