{"title": [{"text": "Video Highlights Detection and Summarization with Lag-Calibration based on Concept-Emotion Mapping of Crowd-sourced Time-Sync Comments", "labels": [], "entities": []}], "abstractContent": [{"text": "With the prevalence of video sharing, there are increasing demands for automatic video digestion such as highlight detection.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7366186827421188}]}, {"text": "Recently, platforms with crowdsourced time-sync video comments have emerged worldwide, providing a good opportunity for highlight detection.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7555759847164154}]}, {"text": "However, this task is non-trivial: (1) time-sync comments often lag behind their corresponding shot; (2) time-sync comments are semantically sparse and noisy; (3) to determine which shots are highlights is highly subjective.", "labels": [], "entities": []}, {"text": "The present paper aims to tackle these challenges by proposing a framework that (1) uses concept mapped lexical-chains for lag-calibration; (2) models video highlights based on comment intensity and combination of emotion and concept concentration of each shot; (3) summarize each detected highlight using improved SumBasic with emotion and concept mapping.", "labels": [], "entities": [{"text": "summarize each detected highlight", "start_pos": 266, "end_pos": 299, "type": "TASK", "confidence": 0.8240771591663361}]}, {"text": "Experiments on large real-world datasets show that our highlight detection method and sum-marization method both outperform other benchmarks with considerable margins.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.6957496553659439}]}], "introductionContent": [{"text": "Every day, people watch billions of hours of videos on YouTube, with half of the views on mobile devices . With the prevalence of video shar-1 https://www.youtube.com/yt/press/statistics.html ing, there is increasing demand for fast video digestion.", "labels": [], "entities": []}, {"text": "Imagine a scenario where a user wants to quickly grasp along video, without dragging the progress bar repeatedly to skip shots unappealing to the user.", "labels": [], "entities": []}, {"text": "With automatically-generated highlights, users could digest the entire video in minutes, before deciding whether to watch the full video later.", "labels": [], "entities": []}, {"text": "Moreover, automatic video highlight detection and summarization could benefit video indexing, video search and video recommendation.", "labels": [], "entities": [{"text": "video highlight detection", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.6895067393779755}, {"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9519608020782471}, {"text": "video indexing", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.6393168568611145}, {"text": "video search", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.6968303620815277}, {"text": "video recommendation", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.7104803770780563}]}, {"text": "However, finding highlights from a video is not a trivial task.", "labels": [], "entities": []}, {"text": "First, what is considered to be a \"highlight\" can be very subjective.", "labels": [], "entities": []}, {"text": "Second, a highlight may not always be captured by analyzing low-level features in image, audio and motions.", "labels": [], "entities": []}, {"text": "Lack of abstract semantic information has become a bottleneck of highlight detection in traditional video processing.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7703944444656372}]}, {"text": "Recently, crowdsourced time-sync video comments, or \"bullet-screen comments\" have emerged, where real-time generated comments will be flying over or besides the screen, synchronized with the video frame by frame.", "labels": [], "entities": []}, {"text": "It has gained popularity worldwide, such as niconico in Japan, Bilibili and Acfun in China, YouTube Live and Twitch Live in USA.", "labels": [], "entities": [{"text": "Bilibili", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.719663143157959}]}, {"text": "The popularity of the timesync comments has suggested new opportunities for video highlight detection based on natural language processing.", "labels": [], "entities": [{"text": "video highlight detection", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.7405972878138224}]}, {"text": "Nevertheless, it is still a challenge to detect and label highlights using time-sync comments.", "labels": [], "entities": []}, {"text": "First, there is almost inevitable lag for comments related to each shot.", "labels": [], "entities": []}, {"text": "As in, ongoing discussion about one shot may extend to next a few shots.", "labels": [], "entities": []}, {"text": "Highlight detection and labeling without lagcalibration may cause inaccurate results.", "labels": [], "entities": []}, {"text": "Second, time-sync comments are sparse semantically, both in number of comments per shot and number of tokens per comment.", "labels": [], "entities": []}, {"text": "Traditionally bag-of-words statistical model may work poorly on such data.", "labels": [], "entities": []}, {"text": "Third, there is much uncertainty in highlight detection in an unsupervised setting without any prior knowledge.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.9219407141208649}]}, {"text": "Characteristics of highlights must be explicitly defined, captured and modeled.", "labels": [], "entities": []}, {"text": "To our best knowledge, little work has concentrated on highlight detection and labeling based on time-sync comments in unsupervised way.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.754434198141098}]}, {"text": "The most relevant work proposed to detect highlights based on topic concentration of semantic vectors of bullet-comments, and label each highlight with pre-trained classifier based on pre-defined tags.", "labels": [], "entities": []}, {"text": "Nevertheless, we argue that emotion concentration is more important in highlight detection than general topic concentration.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8481595814228058}]}, {"text": "Another work proposed to extract highlights based on frame-by-frame similarity of emotion distribution.", "labels": [], "entities": []}, {"text": "However, neither work proposed to tackle the issue of lag-calibration, emotion-topic concentration balance and unsupervised highlight labeling simultaneously.", "labels": [], "entities": []}, {"text": "To solve these problems, the present study proposes the following: (1) word-to-concept and word-to-emotion mapping based on global wordembedding, from which lexical-chains are constructed for bullet-comments lag-calibration; (2) highlight detection based on emotional and conceptual concentration and intensity of lagcalibrated bullet-comments; (3) highlight summarization with modified Basic Sum algorithm that treats emotions and concepts as basic units in a bullet-comment.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 229, "end_pos": 248, "type": "TASK", "confidence": 0.7450324892997742}, {"text": "highlight summarization", "start_pos": 349, "end_pos": 372, "type": "TASK", "confidence": 0.7592722475528717}]}, {"text": "The main contribution of the present paper are as follows: (1) We propose an entirely unsupervised framework for video highlight-detection and summarization based on time-sync comments; (2) We develop a lag-calibration technique based on concept-mapped lexical chains; (3) We construct large datasets for bullet-comment wordembedding, bullet-comment emotion lexicon and ground-truth for highlight-detection and labeling evaluation based on bullet-comments.", "labels": [], "entities": [{"text": "summarization", "start_pos": 143, "end_pos": 156, "type": "TASK", "confidence": 0.9544153213500977}]}], "datasetContent": [{"text": "In this section, we conduct experiments on large real datasets for highlight detection and summarization.", "labels": [], "entities": [{"text": "highlight detection", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.786111056804657}, {"text": "summarization", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.9534836411476135}]}, {"text": "We will describe the data collection process, evaluation metrics, benchmarks and experiment results.", "labels": [], "entities": []}, {"text": "In this section, we introduce evaluation metrics for highlight-detection and summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.9909747242927551}]}, {"text": "For the evaluation of video highlight detection, we need to define what is a \"hit\" between a highlight candidate and reference.", "labels": [], "entities": [{"text": "video highlight detection", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6623568038145701}]}, {"text": "A rigid definition would be a perfect match of beginnings and ends between candidate and reference highlights.", "labels": [], "entities": []}, {"text": "However, this is too harsh for any models.", "labels": [], "entities": []}, {"text": "A more tolerant definition would be whether there is an overlap between a candidate and reference highlight.", "labels": [], "entities": []}, {"text": "However, this will still underestimate model performance since users' selection of beginning and end of a highlight can be quite arbitrary some times.", "labels": [], "entities": []}, {"text": "Instead, we propose a \"hit\" with relaxation \u00ed \u00b5\u00ed\u00bc\u0080 between a candidate \u210e and the reference \u00ed \u00b5\u00ed\u00b0\u00bb as follows: Where \u00ed \u00b5\u00ed\u00b1 1 , \u00ed \u00b5\u00ed\u00b1\u0092 1 is the start time and end time of highlight \u210e, and \u00ed \u00b5\u00ed\u00bc\u0080 is the relaxation length of reference set \u00ed \u00b5\u00ed\u00b0\u00bb.", "labels": [], "entities": []}, {"text": "Further, the precision, recall and F-1 measure can be defined as: In present study, we set the relaxation length to be 5 seconds.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9997143149375916}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9994382262229919}, {"text": "F-1 measure", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.984288215637207}]}, {"text": "Also, the length fora candidate highlight is set to be 15 seconds.", "labels": [], "entities": [{"text": "length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9748536944389343}]}, {"text": "We use ROUGE-1 and ROUGE-2 (C.-Y.) as recall of candidate summary for evaluation: We use BLEU-1 and BLEU-2 (Papineni, Roukos,) as precision.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.9707781076431274}, {"text": "ROUGE-2", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9565882086753845}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9972221851348877}, {"text": "BLEU-1", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9966895580291748}, {"text": "BLEU-2", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9907039999961853}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9984425902366638}]}, {"text": "We choose BLEU for two reasons.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9905303120613098}]}, {"text": "First, a na\u00efve precision metric will be biased for shorter comments, and BLEU can compensate this with the \u00ed \u00b5\u00ed\u00b0\u00b5\u00ed \u00b5\u00ed\u00b1\u0083 product factor: Where \u00ed \u00b5\u00ed\u00b0 \u00b6 is the candidate summary and \u00ed \u00b5\u00ed\u00b1 is the reference summary.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9917172789573669}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9877423048019409}, {"text": "\u00ed \u00b5\u00ed\u00b0\u00b5\u00ed \u00b5\u00ed\u00b1\u0083 product factor", "start_pos": 107, "end_pos": 134, "type": "METRIC", "confidence": 0.8430119343101978}]}, {"text": "Second, while reference summary contains no redundancy, candidate summary could falsely select multiple comments that are very similar and match to the same keywords in reference.", "labels": [], "entities": []}, {"text": "In such case, the precision is extremely overestimated.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9997480511665344}]}, {"text": "BLEU will only count the match one-by-one, namely the number of match of a word will be the minimum frequencies in candidate and reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9760066270828247}]}, {"text": "Finally, the F-1 measure can be defined as:  In this section, we report experimental results for highlight detection and highlight summarization.", "labels": [], "entities": [{"text": "F-1 measure", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9521466493606567}, {"text": "highlight detection", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7581669092178345}, {"text": "highlight summarization", "start_pos": 121, "end_pos": 144, "type": "TASK", "confidence": 0.6281282007694244}]}], "tableCaptions": [{"text": " Table 2. Lag-Calibration of Time-Sync Com- ments.", "labels": [], "entities": []}, {"text": " Table 3. Number of Initial and Expanded Emo- tion Words.", "labels": [], "entities": []}, {"text": " Table 5. Our method outperforms all other meth- ods, especially on ROUGE-1. LSA has lowest  BLEU, mainly because LSA favors long and mul- ti-word sentences statistically, however these sen- tences are not representative in time-sync com-", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.8073506951332092}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9994953870773315}]}, {"text": " Table 5. Comparison of Highlight Summariza- tion Methods (1-Gram).", "labels": [], "entities": []}, {"text": " Table 4. Comparison of Highlight Detection  Methods.", "labels": [], "entities": [{"text": "Highlight Detection", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.6194380670785904}]}, {"text": " Table 6. Comparison of Highlight Summari- zation Methods (2-Gram).", "labels": [], "entities": []}]}