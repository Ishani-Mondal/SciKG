{"title": [], "abstractContent": [{"text": "In this paper the UHH submission to the WMT17 Metrics Shared Task is presented, which is based on sequence and tree kernel functions applied to the reference and candidate translations.", "labels": [], "entities": [{"text": "UHH", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8379803895950317}, {"text": "WMT17 Metrics Shared Task", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.6848393380641937}]}, {"text": "In addition we also explore the effect of applying the kernel functions on the source sentence and a back-translation of the MT output, but also on the pair composed of the candidate translation and a pseudo-reference of the source segment.", "labels": [], "entities": []}, {"text": "The newly proposed metric was evaluated using the data from WMT16, with the results demonstrating a high correlation with human judgments.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.9502636194229126}]}], "introductionContent": [{"text": "The evaluation of Machine Translation (MT) represents a very important domain of research, as providing meaningful, automatic and accurate methods for determining the quality of machinetranslated output is a key component in the development cycle of a MT system.", "labels": [], "entities": [{"text": "evaluation of Machine Translation (MT)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7825871961457389}, {"text": "MT", "start_pos": 252, "end_pos": 254, "type": "TASK", "confidence": 0.9830117225646973}]}, {"text": "However, the task is inherently difficult due to the expressiveness of natural language, which often allows conveying a message in more than one equivalent ways.", "labels": [], "entities": []}, {"text": "The referencefree evaluation, also known as Quality Estimation, aims at providing automatic methods, for assessing the quality of candidate translations, which do not require reference translations.", "labels": [], "entities": []}, {"text": "In the case of a reference-based evaluation, the target segment is compared with the reference translation resulting in a score that measures the similarity between the two sentences.", "labels": [], "entities": []}, {"text": "Different approaches for computing the comparison have been implemented, with the most frequently used one being BLEU (), which measures the quality of the candidate translation by counting the number of n-grams it has in common with the reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9982289671897888}]}, {"text": "Nonetheless, multiple disadvantages of BLEU have already been pointed out, as in Callison-, where it is shown that an increase of the BLEU score does not necessary correlate with a better performing system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9945862293243408}, {"text": "Callison-", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.6400372385978699}, {"text": "BLEU score", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9855604469776154}]}, {"text": "This has motivated further research into additional MT evaluation methods that rely on more than lexical matching by additionally including the syntactic and semantic structure of the sentences (e.g., ).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9652426838874817}]}, {"text": "We propose anew method for the evaluation of MT output, based on tree and sequence kernel functions, applied on the pair of reference and candidate translations.", "labels": [], "entities": [{"text": "MT output", "start_pos": 45, "end_pos": 54, "type": "TASK", "confidence": 0.8810924589633942}]}, {"text": "In addition, we study the impact of applying the kernels on the tuple consisting of the source segment and a back-translation, together with the pair comprised of the candidate translation and a pseudo-reference.", "labels": [], "entities": []}, {"text": "A pseudoreference is the result of translating the source segment into the target language, while a backtranslation is obtained by translating the target segment into the source language.", "labels": [], "entities": []}, {"text": "The evaluation results show that the new metric strongly correlates with human judgments, outperforming the state-of-the-art methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of TSKM was performed using data pertaining to the News domain from the First Conference On Machine Translation (WMT16) . For the results obtained in the WMT17 Metrics Task, please refer to the official results paper.", "labels": [], "entities": [{"text": "News domain", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.9383732676506042}, {"text": "First Conference On Machine Translation (WMT16)", "start_pos": 87, "end_pos": 134, "type": "TASK", "confidence": 0.7597558423876762}, {"text": "WMT17 Metrics Task", "start_pos": 169, "end_pos": 187, "type": "DATASET", "confidence": 0.7684819102287292}]}, {"text": "The following language pairs were used in the evaluation: English-German, Czech-English, GermanEnglish, Finnish-English, Russian-English and Turkish-English.", "labels": [], "entities": []}, {"text": "The MT outputs evaluated correspond to systems submitted to the WMT16 News Translation Task (, having different types ranging from statistical phrase-based to neural or syntax-based ones.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9543392062187195}, {"text": "WMT16 News Translation Task", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.8268751204013824}]}, {"text": "The test sets consist of approximately 3000 tuples, incorporating the source segment together with the reference and candidate translations.", "labels": [], "entities": []}, {"text": "We evaluated TSKM in terms of Pearson correlation with human judgments.", "labels": [], "entities": [{"text": "TSKM", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.5585870146751404}, {"text": "Pearson correlation", "start_pos": 30, "end_pos": 49, "type": "METRIC", "confidence": 0.9307873547077179}]}, {"text": "During the manual evaluation phase of WTM16, human judgments were collected by ranking five candidate translations, with ties being allowed.", "labels": [], "entities": [{"text": "WTM16", "start_pos": 38, "end_pos": 43, "type": "TASK", "confidence": 0.49133461713790894}]}, {"text": "In order to compute a single TSKM score for an MT system, all the individual sentence scores were combined by averaging them.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9774962663650513}]}, {"text": "Different variants of TSKM were taken into account for evaluation.", "labels": [], "entities": [{"text": "TSKM", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.47213512659072876}]}, {"text": "To investigate how the lexical variation affects the performance of the metric, we also implemented versions of the metric where lemmas are used instead of the exact lexical items.: Evaluation results in terms of Pearson correlation for the en-de and de-en language pairs", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 213, "end_pos": 232, "type": "METRIC", "confidence": 0.9267513155937195}]}], "tableCaptions": [{"text": " Table 1: Evaluation results in terms of Pearson correlation for the different TSKM variants. The high- lighted TSKM variant indicates the submission to the WMT17 Metrics Task.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 41, "end_pos": 60, "type": "METRIC", "confidence": 0.9535941779613495}, {"text": "WMT17 Metrics Task", "start_pos": 157, "end_pos": 175, "type": "DATASET", "confidence": 0.7207092444101969}]}, {"text": " Table 2: Evaluation results in terms of Pearson  correlation for the en-de and de-en language pairs", "labels": [], "entities": [{"text": "Pearson  correlation", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.9449082314968109}]}, {"text": " Table 3: Evaluation results in terms of Pearson  correlation for SPTK and CSPTK.", "labels": [], "entities": [{"text": "Pearson  correlation", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.902087926864624}, {"text": "CSPTK", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.8355010747909546}]}]}