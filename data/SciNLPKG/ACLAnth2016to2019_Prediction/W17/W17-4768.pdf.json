{"title": [], "abstractContent": [{"text": "Existing metrics to evaluate the quality of Machine Translation hypotheses take different perspectives into account.", "labels": [], "entities": [{"text": "Machine Translation hypotheses", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.8696231047312418}]}, {"text": "DPM-Fcomb, a metric combining the merits of a range of metrics, achieved the best performance for evaluation of to-English language pairs in the previous two years of WMT Metrics Shared Tasks.", "labels": [], "entities": [{"text": "DPM-Fcomb", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8261649012565613}, {"text": "WMT Metrics Shared Tasks", "start_pos": 167, "end_pos": 191, "type": "TASK", "confidence": 0.8562407940626144}]}, {"text": "This year, we submit a novel combined metric , Blend, to WMT17 Metrics task.", "labels": [], "entities": [{"text": "Blend", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9974588751792908}, {"text": "WMT17 Metrics task", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.7179121971130371}]}, {"text": "Compared to DPMFcomb, Blend includes the following adaptations: i) We use DA human evaluation to guide the training process with avast reduction in required training data, while still achieving improved performance when evaluated on WMT16 to-English language pairs; ii) We carryout experiments to explore the contribution of metrics incorporated in Blend, in order to find a trade-off between performance and efficiency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic machine translation evaluation (AMTE) has received much attention in recent years, with the aim of providing quick and stable measurements of the performance of machine translation (MT) systems.", "labels": [], "entities": [{"text": "Automatic machine translation evaluation (AMTE)", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8206483721733093}, {"text": "machine translation (MT)", "start_pos": 171, "end_pos": 195, "type": "TASK", "confidence": 0.8535459280014038}]}, {"text": "Various metrics for AMTE have been proposed and most operate via computation of the similarity between the MT hypothesis and the reference translation.", "labels": [], "entities": [{"text": "AMTE", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.9482957124710083}, {"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.8959416747093201}]}, {"text": "However, different metrics focus on different perspectives in terms of measuring similarity.", "labels": [], "entities": []}, {"text": "For lexical based metrics, BLEU () and NIST) count n-gram co-occurrence,) and GTM () catch different kinds of matches, ROUGE () captures common subsequences, WER (), PER () and TER () compute the post-editing distance between the hypothesis and the reference translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.997478187084198}, {"text": "NIST", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.9063683748245239}, {"text": "ROUGE", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.9777191281318665}, {"text": "WER", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.8568881750106812}, {"text": "PER", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.9754855632781982}, {"text": "TER", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.9917901754379272}]}, {"text": "Syntactic based metrics mainly use shallow syntactic structures), dependency tree structures or constituent tree structures).", "labels": [], "entities": []}, {"text": "Semantic measures () and discourse similarity based metrics) have also been proposed.", "labels": [], "entities": []}, {"text": "Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons.", "labels": [], "entities": []}, {"text": "One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (.", "labels": [], "entities": [{"text": "DPMFcomb", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.62125164270401}]}, {"text": "In WMT15 and WMT16 Metrics tasks, DPMFcomb was the best metric on average for toEnglish language pairs.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8310438990592957}, {"text": "WMT16", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.7279120683670044}, {"text": "DPMFcomb", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9329615235328674}]}, {"text": "DPMFcomb incorporates lexical, syntactic and semantic based metrics, using ranking SVM 1 to train parameters of each metric score and achieves a high correlation with human evaluation.", "labels": [], "entities": [{"text": "DPMFcomb", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8149564862251282}]}, {"text": "Human evaluations in terms of relative ranking (RR) accumulated in WMT Metrics tasks are adopted to generate training data and to guide the training process.", "labels": [], "entities": [{"text": "relative ranking (RR)", "start_pos": 30, "end_pos": 51, "type": "METRIC", "confidence": 0.8629858255386352}, {"text": "WMT Metrics tasks", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8495746652285258}]}, {"text": "Human relative ranking is carried out by ranking the quality of 5 MT hypotheses of the same source segment from 1 to 5 via comparison with the reference translation.", "labels": [], "entities": [{"text": "MT hypotheses", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.8586446046829224}]}, {"text": "cs-en de-en fi-en ro-en ru-en tr-en en-ru WMT15 500 500 500 \u2212 500 \u2212 500 WMT16 560 560 560 560 560 560 560 Therefore, human RR only provides relative differences in quality of a given 5 hypotheses rather than the overall absolute quality of hypotheses.", "labels": [], "entities": []}, {"text": "Besides, the low inter-annotator agreement level in RR) has been a longlasting issue in MT human evaluation.", "labels": [], "entities": [{"text": "RR", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.7587050795555115}, {"text": "MT human evaluation", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.9224691788355509}]}, {"text": "The ability and the reliability of RR raise our concern whether the capability of the model trained with RR as the golden standard maybe limited.", "labels": [], "entities": [{"text": "reliability", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.991255521774292}, {"text": "RR", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.7989416718482971}]}, {"text": "Fortunately, anew emerged evaluation approach, direct assessment (DA) (, has been proven more reliable for evaluation of metrics and was recently adopted as the official human evaluation in WMT17.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 190, "end_pos": 195, "type": "DATASET", "confidence": 0.8609220385551453}]}, {"text": "DA produces absolute quality scores of hypotheses, by measuring to what extend the hypothesis adequately expresses the meaning of the reference translation, through a 1-100 continuous rating scale that facilitates reliable quality control of crowd-sourcing.", "labels": [], "entities": []}, {"text": "Large numbers of repeat human assessments per translation are standardized and then combined into a mean score as the final quality score of the MT hypothesis.", "labels": [], "entities": [{"text": "MT", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.9896541237831116}]}, {"text": "The recent development inhuman evaluation of MT motivates us to propose anew combined metric, named as Blend 2 , by adopting DA, as opposed to RR, to guide the training process indicating that a more reliable gold standard can lead to more reliable results even with less training data.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9692574143409729}, {"text": "Blend 2", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.9848670065402985}, {"text": "DA", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9947441816329956}, {"text": "RR", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.9777700304985046}]}, {"text": "Furthermore, we explore the contribution of metrics incorporated in Blend, aiming at finding a trade-off between performance and efficiency of Blend.", "labels": [], "entities": [{"text": "Blend", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.8575876951217651}, {"text": "Blend", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.9368991255760193}]}, {"text": "What follows is a brief review of DPMFcomb, before a description of Blend formulation is provided in Section 2, followed by experiments and results in Section 3, before the conclusions in section 4.", "labels": [], "entities": [{"text": "DPMFcomb", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.673581063747406}, {"text": "Blend", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9474550485610962}]}], "datasetContent": [{"text": "We carryout experiments to compare the performance of DPMFcomb and Blend.", "labels": [], "entities": [{"text": "Blend", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.8941993117332458}]}, {"text": "We also explore the contribution of incorporated metrics in Blend to find a trade-off between performance and efficiency.", "labels": [], "entities": [{"text": "Blend", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.9013699293136597}]}, {"text": "Blend can be effective to evaluate the quality of from-English MT hypotheses if incorporated metrics support from-English language pairs.", "labels": [], "entities": [{"text": "Blend", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.984865665435791}, {"text": "MT hypotheses", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.8891730904579163}]}, {"text": "We carryout experiments on WMT16 for en-ru language pair as shown in: Segment-level Pearson correlation for enru in WMT16. is trained on only 500 sentences and incorporates default lexical based metrics from Asiya toolkit for en-ru, including 20 metrics, but with 9 kinds of metrics only.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8999996781349182}, {"text": "Segment-level Pearson correlation", "start_pos": 70, "end_pos": 103, "type": "METRIC", "confidence": 0.7522722780704498}, {"text": "WMT16.", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.9629221558570862}]}, {"text": "Compared with Blend.default, Blend.default+2 incorporates two more metrics, CharacTer and BEER, but achieves great improvement with segment-level Pearson correlation from .613 to .675.", "labels": [], "entities": [{"text": "Blend.default", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.8529109954833984}, {"text": "CharacTer", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9474566578865051}, {"text": "BEER", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.99687260389328}, {"text": "Pearson correlation", "start_pos": 146, "end_pos": 165, "type": "METRIC", "confidence": 0.8490642309188843}]}, {"text": "The incorporated metric BEER is the best performing metric (.666) on WMT16 for en-ru, which is trained with large amounts of data.", "labels": [], "entities": [{"text": "BEER", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9814557433128357}, {"text": "WMT16", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9551612138748169}]}, {"text": "Beer contributes to Blend apparently, meanwhile Blend can further improve the performance of BEER, indicating the effectiveness of the combined metric Blend.", "labels": [], "entities": [{"text": "Blend", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9952666759490967}, {"text": "Blend", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.918195366859436}, {"text": "BEER", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.995378851890564}]}, {"text": "We submit Blend.default+2 to WMT17 Metrics task for en-ru.", "labels": [], "entities": [{"text": "WMT17 Metrics task", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.8208285768826803}]}], "tableCaptions": [{"text": " Table 2: System-level Pearson correlation of metric scores and DA human scores with 10K hybrid sys- tems for to-English language pairs on WMT16, where \"avg\" denotes the average Pearson correlation of  all language pairs.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 23, "end_pos": 42, "type": "METRIC", "confidence": 0.8367672562599182}, {"text": "DA human scores", "start_pos": 64, "end_pos": 79, "type": "METRIC", "confidence": 0.8234310348828634}, {"text": "WMT16", "start_pos": 139, "end_pos": 144, "type": "DATASET", "confidence": 0.9880995154380798}, {"text": "Pearson correlation", "start_pos": 178, "end_pos": 197, "type": "METRIC", "confidence": 0.9492197930812836}]}, {"text": " Table 3: Segment-level Pearson correlation of metric scores and DA human scores for to-English lan- guage pairs on WMT16, where \"avg\" denotes the average Pearson correlation of all language pairs.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.8172024488449097}, {"text": "DA human scores", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.8933339715003967}, {"text": "WMT16", "start_pos": 116, "end_pos": 121, "type": "DATASET", "confidence": 0.9814718961715698}, {"text": "Pearson correlation", "start_pos": 155, "end_pos": 174, "type": "METRIC", "confidence": 0.9548098742961884}]}, {"text": " Table 4: Segment-level Pearson correlation of Blend incorporating different level of linguistic metrics  for to-English language pairs on WMT16, where \"avg\" denotes the average Pearson correlation of all  language pairs.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.8408948183059692}, {"text": "Blend", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.54837965965271}, {"text": "WMT16", "start_pos": 139, "end_pos": 144, "type": "DATASET", "confidence": 0.9814819097518921}, {"text": "Pearson correlation", "start_pos": 178, "end_pos": 197, "type": "METRIC", "confidence": 0.9327335357666016}]}, {"text": " Table 5: Segment-level Pearson correlation of Blend.lex incorporating 4 other metrics for to-English  language pairs on WMT16, where \"avg\" denotes the average Pearson correlation of all language pairs.", "labels": [], "entities": [{"text": "Segment-level Pearson correlation", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.6980650226275126}, {"text": "Blend.lex", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.913257896900177}, {"text": "WMT16", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.9812900424003601}, {"text": "Pearson correlation", "start_pos": 160, "end_pos": 179, "type": "METRIC", "confidence": 0.949296236038208}]}]}