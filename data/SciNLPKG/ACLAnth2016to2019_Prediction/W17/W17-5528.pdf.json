{"title": [{"text": "Neural-based Natural Language Generation in Dialogue using RNN Encoder-Decoder with Semantic Aggregation", "labels": [], "entities": [{"text": "Neural-based Natural Language Generation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.695290707051754}]}], "abstractContent": [{"text": "Natural language generation (NLG) is an important component in spoken dialogue systems.", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7579683065414429}]}, {"text": "This paper presents a model called Encoder-Aggregator-Decoder which is an extension of an Recurrent Neural Network based Encoder-Decoder architecture.", "labels": [], "entities": []}, {"text": "The proposed Semantic Aggregator consists of two components: an Aligner and a Refiner.", "labels": [], "entities": []}, {"text": "The Aligner is a conventional attention calculated over the encoded input information, while the Refiner is another attention or gating mechanism stacked over the attentive Aligner in order to further select and aggregate the semantic elements.", "labels": [], "entities": []}, {"text": "The proposed model can be jointly trained both text planning and text realization to produce natural language utterances.", "labels": [], "entities": [{"text": "text planning", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7669539749622345}, {"text": "text realization", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.758293628692627}]}, {"text": "The model was extensively assessed on four different NLG domains, in which the results showed that the proposed generator consistently outperforms the previous methods on all the NLG domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) plays a critical role in a Spoken Dialogue System (SDS), and its task is to convert a meaning representation produced by the dialogue manager into natural language sentences.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7479306757450104}, {"text": "Spoken Dialogue System (SDS)", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6174632360537847}]}, {"text": "Conventional approaches to NLG follow a pipeline which typically breaks down the task into sentence planning and surface realization.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7001883536577225}, {"text": "surface realization", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.773112952709198}]}, {"text": "Sentence planning decides the order and structure of a sentence, followed by a surface realization which converts the sentence structure into final utterance.", "labels": [], "entities": [{"text": "Sentence planning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9185818731784821}]}, {"text": "Previous approaches to NLG still rely on extensive hand-tuning templates and rules that require expert knowledge of linguistic representation.", "labels": [], "entities": []}, {"text": "There are some common and widely used approaches to solve NLG problems, including rule-based, corpus-based n-gram generator, and a trainable generator).", "labels": [], "entities": []}, {"text": "Recurrent Neural Network (RNN)-based approaches have recently shown promising results in NLG tasks.", "labels": [], "entities": []}, {"text": "The RNN-based models have been used for NLG as a joint training model and an end-to-end training network.", "labels": [], "entities": []}, {"text": "A recurring problem in such systems requiring annotated corpora for specific dialogue acts * (DAs).", "labels": [], "entities": []}, {"text": "More recently, the attentionbased RNN Encoder-Decoder (AREncDec) approaches () have been explored to tackle the NLG problems.", "labels": [], "entities": []}, {"text": "The AREncDEc-based models have also shown improved results on various tasks, e.g., image captioning (, machine translation (.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.8015271425247192}, {"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8059106469154358}]}, {"text": "To ensure that the generated utterance represents the intended meaning of the given DA, the previous RNN-based models were conditioned on a 1-hot vector representation of the DA.", "labels": [], "entities": []}, {"text": "proposed a Long Short-Term Memorybased (HLSTM) model which introduced a heuristic gate to guarantee that the slot-value pairs were accurately captured during generation.", "labels": [], "entities": []}, {"text": "Subsequently, proposed an LSTMbased generator (SC-LSTM) which jointly learned the controlling signal and language model.", "labels": [], "entities": []}, {"text": "proposed an AREncDec based generator (ENCDEC) which applied attention mechanism on the slot-value pairs.: Order issue in natural language generation, in which an incorrect generated sentence has wrong ordered slots.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.7809876799583435}]}], "datasetContent": [{"text": "We conducted an extensive set of experiments to assess the effectiveness of our model using several metrics, datasets, and model architectures, in order to compare to prior methods.", "labels": [], "entities": []}, {"text": "We assessed the proposed models using four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television.", "labels": [], "entities": []}, {"text": "The Restaurant and Hotel were collected in () which contain around 5K utterances and 200 distinct DAs.", "labels": [], "entities": []}, {"text": "The Laptop and TV datasets have been released by.", "labels": [], "entities": [{"text": "TV datasets", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.8585958480834961}]}, {"text": "These datasets contain about 13K distinct DAs in the Laptop domain and 7K distinct DAs in the TV.", "labels": [], "entities": [{"text": "TV", "start_pos": 94, "end_pos": 96, "type": "DATASET", "confidence": 0.9432064294815063}]}, {"text": "Both Laptop and TV datasets have a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs.", "labels": [], "entities": [{"text": "TV datasets", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.8330978155136108}]}, {"text": "As a result, the NLG tasks for the Laptop and TV datasets become much harder.", "labels": [], "entities": [{"text": "Laptop and TV datasets", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.6883990690112114}]}, {"text": "The generators were implemented using the TensorFlow library ( and trained by partitioning each of the datasets into training, validation and testing set in the ratio 3:1:1.", "labels": [], "entities": []}, {"text": "The hidden layer size was set to be 80 for all cases, and the generators were trained with a 70% of dropout rate.", "labels": [], "entities": []}, {"text": "We perform 5 runs with different random initialization of the network and the training is terminated by using early stopping as described in Section 3.5.", "labels": [], "entities": []}, {"text": "We select a model that yields the highest BLEU score on the validation set as shown in Table 2.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9834326803684235}]}, {"text": "Since the trained models can differ depending on the initialization, we also report the results which were averaged over 5 randomly initialized networks.", "labels": [], "entities": []}, {"text": "Note that, except the results reported in, all the results shown were averaged over 5 randomly initialized networks.", "labels": [], "entities": []}, {"text": "The decoder procedure used beam search with abeam width of 10.", "labels": [], "entities": []}, {"text": "We set \u03bb to 1000 to severely discourage the reranker from selecting utterances which contain either redundant or missing slots.", "labels": [], "entities": []}, {"text": "For each DA, we over-generated 20 candidate utterances and selected the top 5 realizations after reranking.", "labels": [], "entities": []}, {"text": "Moreover, in order to better understand the effectiveness of our proposed methods, we (1) trained the models on the Laptop domain with a varied proportion of training data, starting from 10% to 100%, and (2) trained general models by merging all the data from four domains together and tested them in each individual domain) .  The generator performance was assessed by using two objective evaluation metrics: the BLEU score and the slot error rate ERR.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 414, "end_pos": 424, "type": "METRIC", "confidence": 0.9728406369686127}, {"text": "slot error rate ERR", "start_pos": 433, "end_pos": 452, "type": "METRIC", "confidence": 0.8988239169120789}]}], "tableCaptions": []}