{"title": [{"text": "Creating Common Ground through Multimodal Simulations", "labels": [], "entities": [{"text": "Multimodal Simulations", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.6684961169958115}]}], "abstractContent": [{"text": "The demand for more sophisticated human-computer interactions is rapidly increasing, as users become more accustomed to conversation-like interactions with their devices.", "labels": [], "entities": []}, {"text": "In this paper, we examine this changing landscape in the context of human-machine interaction in a shared workspace to achieve a common goal.", "labels": [], "entities": []}, {"text": "In our prototype system, people and avatars cooperate to build blocks world structures through the interaction of language, gesture, vision, and action.", "labels": [], "entities": []}, {"text": "This provides a platform to study computational issues involved in multimodal communication.", "labels": [], "entities": []}, {"text": "In order to establish elements of the common ground in discourse between speakers, we have created an embodied 3D simulation , enabling both the generation and interpretation of multiple modalities, including: language, gesture, and the visualization of objects moving and agents acting in their environment.", "labels": [], "entities": []}, {"text": "The simulation is built on the modeling language VoxML, that encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment.", "labels": [], "entities": [{"text": "VoxML", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9113655090332031}]}, {"text": "We illustrate this with a walk-through of multimodal communication in a shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we discuss a developing approach towards modeling peer-to-peer communication using multiple modalities, e.g., language, gesture, vision, and action.", "labels": [], "entities": []}, {"text": "This platform integrates a multimodal model of semantics (Multimodal Semantic Simulations, MSS) () with a realtime vision system for recognizing human gestures ( . This framework assumes both a richer formal model of events and their participants, as well as a modeling language for constructing 3D visualizations of objects and events denoted by linguistic expressions.", "labels": [], "entities": []}, {"text": "We position this approach in the context of the questions posed by the workshop organizers, and provide more detail into the architecture that integrates the multiple sources of knowledge within the shared context of communication.", "labels": [], "entities": []}, {"text": "To begin with, let us distinguish between experience and action: two individuals who share an experience, such as witnessing a natural event, hearing a clap of thunder, or feeling the earth tremor, are jointly \"co-perceiving an event\".", "labels": [], "entities": []}, {"text": "Hence, they are co-situated and co-perceptive.", "labels": [], "entities": []}, {"text": "If these two beings are communicating in order to carryout a shared task, such as building a structure, moving objects, or clearing a space, then they can be considered \"agents\", who are not only co-perceiving the present situation and subsequent situations as they change, but are also acting, together or individually, as a result of communicative interactions.", "labels": [], "entities": []}, {"text": "Hence, what is being shared in the latter is considerably richer and more complex in character.", "labels": [], "entities": []}, {"text": "Namely, there is agreement, acceptance, or recognition of a common goal between the agents, what can be called \"co-intent\".", "labels": [], "entities": []}, {"text": "These combined factors constitute the first aspects of is called \"common ground\": namely, co-situatedness, co-perception, and co-intent.", "labels": [], "entities": []}, {"text": "The theory of common ground has a rich and diverse literature concerning what is shared or presupposed inhuman communication.", "labels": [], "entities": []}, {"text": "When engaged in accomplishing a task jointly, agents share one additional anchoring strategy that greatly enhances the expressiveness of common ground: namely, the ability to \"co-attend\".", "labels": [], "entities": []}, {"text": "Because of the inherently directed nature of attention and co-attention, we will, rather, speak of a \"shared situated reference\" in the discussion that follows.", "labels": [], "entities": []}, {"text": "This ability will emerge as central to determining the denotations of participants in shared events.", "labels": [], "entities": []}, {"text": "Events as we experience them are distinct from the way we refer to them with language.", "labels": [], "entities": []}, {"text": "The mechanisms in language allow us to package, quantify, measure, and order our experiences, creating rich conceptual reifications and semantic differentiations.", "labels": [], "entities": []}, {"text": "The surface realization of this ability is mostly manifest through our linguistic utterances, but is also witnessed through gestures.", "labels": [], "entities": []}, {"text": "By examining the nature of the common ground assumed in communication, we can study the conceptual expressiveness of these systems.", "labels": [], "entities": []}, {"text": "We believe that simulation can play a crucial role in human-computer communication; it creates a shared epistemic model of the environment inhabited by a human and an artificial agent, and demonstrates the knowledge held by the agent publicly.", "labels": [], "entities": []}, {"text": "Demonstrating knowledge is needed to ensure a shared understanding with the humans involved in the activity, but why create a simulation model, if the goal is to interact with an avatar or robot?", "labels": [], "entities": []}, {"text": "If a robotic agent is able to receive linguistic information from a human commander or collaborator and interpret that relative to its current physical circumstances, it can create an epistemic representation of that same information.", "labels": [], "entities": []}, {"text": "However, without a modality to express that representation independently, the human is unable to verify or query what the robotic agent is perceiving or how that perception is being interpreted.", "labels": [], "entities": []}, {"text": "Ina simulation environment the human and robot share an epistemic space, and any modality of communication that can be expressed within that space (e.g., linguistic, visual, gestural) enriches the number of ways that a human and a robotic agent can communicate within object and situation-based tasks, such as those investigated by,,.", "labels": [], "entities": []}, {"text": "The simulation environment provided by our model includes the perceptual domain of objects, properties, and events.", "labels": [], "entities": []}, {"text": "In addition, propositional content in the model is accessible to the discourse, allowing access to beliefs, desires, and intentions (BDI), and for them to be distinguished by the agents to act and communicate appropriately.", "labels": [], "entities": [{"text": "BDI", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9087352752685547}]}, {"text": "This provides the non-linguistic visual and action modalities, which are augmented by the inherently non-linguistic gestural modality enacted within the visual context.", "labels": [], "entities": []}, {"text": "As gesture is intended for visual interpretation, it is directly interpretable in the co-visual context if and only if the denotation of its interpretation function is directly available in the simulation through visual inspection (;;).", "labels": [], "entities": []}, {"text": "Direction or vector information encoded in a gesture hooks into direct deixis and satisfiable actions by projecting the gesture onto a distinct area of the simulation environment.", "labels": [], "entities": []}, {"text": "For entity descriptions, speech allows fora randomly accessible and indexable descriptions, while gesture allows for physically and spatially grounded regions, objects, or configurations.", "labels": [], "entities": []}, {"text": "Speech can express nominal, qualitative, tangible, or nontangible attributes, while gesture is limited to orientation and direction, shape, size, relative distance, and manner of motion.", "labels": [], "entities": []}, {"text": "For communicative events, where speech allows for any mood (declarative, interrogative, imperative), gesture is usually imperative, except for speech acts.", "labels": [], "entities": []}, {"text": "Speech allows for an essentially unconstrained expression of propositional content, while gesture is limited to the propositional content of a speech act, and the content of a deixis or action description.", "labels": [], "entities": []}, {"text": "For explicit spatial grounding, gesture is computationally advantageous as spatially-encoded information in the gesture (e.g., direction, vector, etc.) can be directly grounded in the scene without having to process an additional linguistic layer of indirection.", "labels": [], "entities": []}, {"text": "In some cases, spatially-denoting manner is easier to model with gesture than language.", "labels": [], "entities": []}, {"text": "This is also the case with technology-driven multimodal display interfaces, but here we are concerned with a co-situated situation where the interlocutors, human and virtual, share an epistemic space.", "labels": [], "entities": []}, {"text": "We assume interlocutors have explicit and distinct modes of interpretation as accessed through perception.", "labels": [], "entities": []}, {"text": "Input from the environment (our embedding space) evidences various beliefs that we establish as knowledge.", "labels": [], "entities": []}, {"text": "Our model of synthetic vision provides distinct but mutually coherent views on the shared environment.", "labels": [], "entities": []}, {"text": "An example involving object occlusion is discussed in Section 3.", "labels": [], "entities": []}, {"text": "Any of the different modalities in our model can be used to disambiguate new information introduced into the context by the other participant(s) by enumerating possible options, pruning question/answer steps based on the number of options, and interaction between interlocutors until an unambiguous interpretation is achieved.", "labels": [], "entities": []}, {"text": "Examples of this are discussed in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}