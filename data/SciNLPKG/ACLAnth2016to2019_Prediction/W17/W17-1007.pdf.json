{"title": [{"text": "Machine Learning Approach to Evaluate MultiLingual Summaries", "labels": [], "entities": [{"text": "Evaluate MultiLingual Summaries", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.7964188853899637}]}], "abstractContent": [{"text": "The present paper introduces anew Multil-ing text summary evaluation method.", "labels": [], "entities": [{"text": "Multil-ing text summary evaluation", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.7335589081048965}]}, {"text": "This method relies on machine learning approach which operates by combining multiple features to build models that predict the human score (overall responsiveness) of anew summary.", "labels": [], "entities": []}, {"text": "We have tried several single and \"ensemble learning\" classiers to build the best model.", "labels": [], "entities": []}, {"text": "We have experimented our method in summary level evaluation where we evaluate the quality of each text summary separately.", "labels": [], "entities": [{"text": "summary level evaluation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6941906213760376}]}, {"text": "The correlation between built models and human score is better than the correlation between the baselines and the manual score.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nowadays, the evaluation of summarization systems is an important step in the development cycle of those systems.", "labels": [], "entities": []}, {"text": "In fact, it accelerates the cycle of development by giving an analysis of errors, making an optimization of systems and comparing each system with others.", "labels": [], "entities": []}, {"text": "The evaluation of text summary covers its content, its linguistic quality or both.", "labels": [], "entities": []}, {"text": "Whatever the type of evaluation (content and/or linguistic quality), the evaluation of system summary output is a difficult task given that inmost times there is not a single good summary.", "labels": [], "entities": []}, {"text": "In the extreme case, two summaries of the same documents set may have completely different words and/or sentences with different structures.", "labels": [], "entities": []}, {"text": "Several metrics have been evaluated the content, the linguistic quality and the overall responsiveness of MonoLing text summaries.", "labels": [], "entities": [{"text": "MonoLing text summaries", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.585881769657135}]}, {"text": "We can cite ROUGE (), BE (), AutoSummENG (, , etc.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9939418435096741}, {"text": "BE", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9990272521972656}, {"text": "AutoSummENG", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.6700165867805481}]}, {"text": "Some of those metircs can assess MultiLing text summaries such as ROUGE and AutoSummENG.", "labels": [], "entities": [{"text": "MultiLing text summaries", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.7288188735644022}, {"text": "ROUGE", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9300189018249512}]}, {"text": "But, those features can only evaluate the content of MultiLing text summaries.", "labels": [], "entities": [{"text": "MultiLing text summaries", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6082352002461752}]}, {"text": "To encourage research to develop automatic multilingual multi-documents summarization systems anew task, dubbed MultiLing Pilot (), has been introduced for the first time in TAC2011 conference.", "labels": [], "entities": [{"text": "TAC2011 conference", "start_pos": 174, "end_pos": 192, "type": "DATASET", "confidence": 0.9023975729942322}]}, {"text": "Later, the two workshops 2013 ACL MultiLing Pilot and) have been organised with the same purpose as MultiLing Pilot 2011.", "labels": [], "entities": [{"text": "ACL MultiLing Pilot", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.7568748394648234}, {"text": "MultiLing Pilot 2011", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7379041711489359}]}, {"text": "The participated summarization systems in the MultiLing task have been assessed using automatic content metrics such as ROUGE-1, ROUGE-2 and MeMoG and a manual metric named Overall Respensiveness which covers the content and the linguistic quality of a text summary.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 120, "end_pos": 127, "type": "METRIC", "confidence": 0.9003915786743164}, {"text": "ROUGE-2", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.8183514475822449}]}, {"text": "However, the manual evaluation of both the content and the linguistic quality of multilingual multi-documents summarization systems is an arduous and costly process.", "labels": [], "entities": []}, {"text": "In addition, the automatic evaluation of only the content of summary is not enough because a summary should also have a good linguistic quality.", "labels": [], "entities": []}, {"text": "For this reason, automatic metrics that evaluate the content and the linguistic quality of summaries from several languages should be developed.", "labels": [], "entities": []}, {"text": "In this context, we propose anew method based on a machine learning approach for evaluating the overall quality of automatic text summaries.", "labels": [], "entities": [{"text": "text summaries", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.6031716614961624}]}, {"text": "This method could predict the human score (Overall Reponsiveness) of English and Arabic text summaries by combining multiple content and linguistic quality features.", "labels": [], "entities": [{"text": "human score (Overall Reponsiveness)", "start_pos": 30, "end_pos": 65, "type": "METRIC", "confidence": 0.6665104677279791}, {"text": "English and Arabic text summaries", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.5863062500953674}]}, {"text": "The rest of the paper is organized in the following way: First in Section 2 we introduce the main metrics that have been proposed to evaluate text summaries; then in Section 3 we explain the methodology adopted in our work.", "labels": [], "entities": [{"text": "evaluate text summaries", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.5998967985312144}]}, {"text": "In Section 4 we present the different experiments and results for summary level evaluation.", "labels": [], "entities": []}, {"text": "Finally, Section 5 describes the main conclusions and possible future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimented our method in summary level evaluation.", "labels": [], "entities": []}, {"text": "At this level, we take, for each Summarizer system, each produced summary in a separate entry.", "labels": [], "entities": []}, {"text": "It is worth mentioning that this evaluation level is more difficult than system level evaluation (i.e. where the average quality of a summarizing system is measured) even for MonoLingual summary evaluation (,.", "labels": [], "entities": []}, {"text": "For each language, we have tested several single and \"ensemble learning\" classifiers integrated on Weka environment and based on regression method like GaussianProcesses, linearRegression, vote, Bagging, etc.", "labels": [], "entities": [{"text": "Weka environment", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.9586592614650726}, {"text": "Bagging", "start_pos": 195, "end_pos": 202, "type": "TASK", "confidence": 0.8835253119468689}]}, {"text": "We validate our models using cross-validation with 10 folds and using supplied test set.", "labels": [], "entities": []}, {"text": "For cross-validation method, we have calculated the features from \"MultiLing 2013\" corpus.", "labels": [], "entities": [{"text": "MultiLing 2013\" corpus", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.8681264519691467}]}, {"text": "While, for supplied test set method we have used \"MultiLing 2013\" corpus as training set and \"MultiLing Pilot TAC'2011\" corpus as testing set.", "labels": [], "entities": [{"text": "MultiLing 2013\" corpus", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.7613247260451317}, {"text": "MultiLing Pilot TAC'2011\" corpus", "start_pos": 94, "end_pos": 126, "type": "DATASET", "confidence": 0.5711501955986023}]}, {"text": "We have chosen to train our models on \"MultiLing 2013\" corpus because we have more summaries in this corpus (150 summaries for Arabic and 149 for English).", "labels": [], "entities": [{"text": "MultiLing 2013\" corpus", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.760094903409481}]}, {"text": "To evaluate the proposed method, we study the correlation of Pearson (Pearson, 1895), Spearman and between the manual scores (Overall Responsiveness) and the scores produced by the proposed method.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9534894824028015}, {"text": "Spearman", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9628843665122986}]}, {"text": "Furthermore, we report the \"Root Mean Squared Error\" (RMSE) measure generated by each model.", "labels": [], "entities": [{"text": "Root Mean Squared Error\" (RMSE) measure", "start_pos": 28, "end_pos": 67, "type": "METRIC", "confidence": 0.8600333664152358}]}, {"text": "This measure is based on the difference between the manual scores (Overall responsiveness) and the predicted scores.", "labels": [], "entities": []}, {"text": "We begin with the experiments performed with Arabic language.", "labels": [], "entities": []}, {"text": "The selected features for Arabic models are: autosummeng 443 , unsmoothed-JSD, unigramProb, multinomialProb, ROUGE-3 and number of NP phrases in the summary.", "labels": [], "entities": [{"text": "autosummeng", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9858522415161133}, {"text": "ROUGE-3", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9845821261405945}]}, {"text": "The Pearson, the Spearman and the Kendall Correlations and the root mean square error (RMSE) generated by each classifier for Arabic language are presented in. shows the performance of the selected features in building the predictive models using several single and ensemble learning classifiers.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8363583087921143}, {"text": "root mean square error (RMSE) generated", "start_pos": 63, "end_pos": 102, "type": "METRIC", "confidence": 0.8404433615505695}]}, {"text": "In the case of cross validation method, the results show that the model built from the \"ensemble learning\" classifier \"Bagging\" produced the best Kendall (0.239) and Spearman (0.335) correlations, \"AdditiveRegression\" produced the best Pearson (0.337) correlation while the \"GaussianProcesses\" have produced the lowest RMSE (0.696).", "labels": [], "entities": [{"text": "cross validation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7098405957221985}, {"text": "Kendall (0.239) and Spearman (0.335) correlations", "start_pos": 146, "end_pos": 195, "type": "METRIC", "confidence": 0.7203990310430527}, {"text": "Pearson (0.337) correlation", "start_pos": 236, "end_pos": 263, "type": "METRIC", "confidence": 0.9491363406181336}, {"text": "RMSE", "start_pos": 319, "end_pos": 323, "type": "METRIC", "confidence": 0.9963395595550537}]}, {"text": "In the case of supplied test set method, indicates that the best \"ensemble learning\" classifier is the \"Stacking\" which provides a model having a Kendall correlation of 0.171 and a Spearman correlation of (0.232) while the \"GaussianProcesses\" have produced the best Pearson (0.224) correlation and the lowest RMSE.", "labels": [], "entities": [{"text": "Kendall correlation", "start_pos": 146, "end_pos": 165, "type": "METRIC", "confidence": 0.8777554929256439}, {"text": "Spearman correlation", "start_pos": 181, "end_pos": 201, "type": "METRIC", "confidence": 0.8850240111351013}, {"text": "Pearson (0.224) correlation", "start_pos": 266, "end_pos": 293, "type": "METRIC", "confidence": 0.9443702816963195}, {"text": "RMSE", "start_pos": 309, "end_pos": 313, "type": "METRIC", "confidence": 0.977689802646637}]}, {"text": "Another notable observation is that the correlation using cross-validation is more important than using supplied test set.", "labels": [], "entities": []}, {"text": "Whereas, the RMSE using supplied test set is lower than using cross-validation.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.4756816625595093}]}, {"text": "This means that the error between the predictive values and the actual values is less important using supplied test set.", "labels": [], "entities": []}, {"text": "The decrease of correlation between the cross-validation method and the supplied test set method needs to be studied further in future works.", "labels": [], "entities": []}, {"text": "We pass now to the comparison between the performance of the best obtained model and the baseline metrics that were adopted by the MultiLing workshop such as R-2, MeMoG and also we add the best variant of each of the three other famous metrics AutoSummENG, NPoWER and SIMetrix.", "labels": [], "entities": [{"text": "NPoWER", "start_pos": 257, "end_pos": 263, "type": "DATASET", "confidence": 0.8270841836929321}]}, {"text": "details the different correlations and RMSEs of baseline metrics and our different experimentations.", "labels": [], "entities": [{"text": "RMSEs", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.7994939088821411}]}, {"text": "From, the model built from the combination of selected features has the best correlation and RMSE comparing to baselines.", "labels": [], "entities": [{"text": "correlation", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9960224628448486}, {"text": "RMSE", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9983763694763184}]}, {"text": "When observing the, we seethe gap between baseline metrics and the model build from selected features.", "labels": [], "entities": []}, {"text": "In addition, we notice the decrease of correlation on both methods of validation (crossvalidation, supplied test set), when we tried to remove one of the classes of features.", "labels": [], "entities": [{"text": "correlation", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9895172715187073}]}, {"text": "Moreover, we remark that removing SIMetrix metric from the selected features have a big effect on its correlation with Overall Responsiveness when using supplied test set as validation method.", "labels": [], "entities": []}, {"text": "Besides, we note that the correlation of the best model with Overall Responsiveness is low, while it is more important than the correlation of baselines.", "labels": [], "entities": []}, {"text": "This maybe due to the small set of the observations per Arabic language.", "labels": [], "entities": []}, {"text": "We need a larger set of observations to determine the best combination of features and to have better correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9790305495262146}]}, {"text": "Furthermore, perhaps, this is due to the complexity of the Arabic language structure which is an agglutinative language where agglutination) occurs when articles, prepositions and conjunctions are attached to the beginning of words and pronouns are attached to the end of words.", "labels": [], "entities": []}, {"text": "This phenomenon can greatly influence the operation of comparing the candidate summary with reference summaries.", "labels": [], "entities": []}, {"text": "Especially when a word appears in the candidate summary without agglutination while it appears in a reference summary in an agglutinative form and vice versa.", "labels": [], "entities": []}, {"text": "English Summary Evaluation We pass now to the different experiments performed with English language.", "labels": [], "entities": []}, {"text": "The selected features for English models are NPowER 123 , autosummeng 443 , the number of NP phrases in the text summary, the average number of PP per sentence in a text summary.", "labels": [], "entities": [{"text": "NPowER 123", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.725823163986206}, {"text": "autosummeng", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9811962842941284}]}, {"text": "The Pearson, the Spearman and the Kendall Correlations and the rootmean-square error (RMSE) generated by each classifier for English language are presented in. shows the performance of the selected features in building the predictive models using several single and ensemble learning classifiers for the English language.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8448280096054077}, {"text": "rootmean-square error (RMSE) generated", "start_pos": 63, "end_pos": 101, "type": "METRIC", "confidence": 0.8648175398508707}]}, {"text": "For cross validation method, the results show that the model built from the \"ensemble learning\" classifier \"Bagging\" produced the best Kendall (0.393), Spearman (0.537) and Pearson (0.529) correlations and the lowest RMSE (0.652).", "labels": [], "entities": [{"text": "Spearman (0.537) and Pearson (0.529) correlations", "start_pos": 152, "end_pos": 201, "type": "METRIC", "confidence": 0.8472446501255035}, {"text": "RMSE", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9968109726905823}]}, {"text": "For supplied test set validation method, indicates that the best \"ensemble learning\" classifier in terms of correlation and RMSE is also the \"Bagging\".", "labels": [], "entities": [{"text": "correlation", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.982192873954773}, {"text": "RMSE", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9851399064064026}, {"text": "Bagging", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.852898120880127}]}, {"text": "In fact, this \"ensemble learning\" has the best correlations (i.e. Kendall: 0.322) and the lowest RMSE (0.754).", "labels": [], "entities": [{"text": "Kendall: 0.322)", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.9426643699407578}, {"text": "RMSE", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9977864027023315}]}, {"text": "Again, we note that the correlation using cross-validation is more important than using supplied test set.", "labels": [], "entities": []}, {"text": "The decrease of correlation between the cross-validation method and the supplied test set method can be caused by the variation of the human evaluator and/or the change of evaluation guidelines from MultiLing 2011 to MultiLing 2013.", "labels": [], "entities": []}, {"text": "We now move to the comparison between the performance of the best obtained model and the baseline metrics that were adopted by the MultiLing workshop such as ROUGE-2 and MeMoG and also we add the best variant of each of the three other famous metrics AutoSummENG, NPoWER and SIMetrix.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 158, "end_pos": 165, "type": "METRIC", "confidence": 0.7955352067947388}, {"text": "MeMoG", "start_pos": 170, "end_pos": 175, "type": "DATASET", "confidence": 0.8850545287132263}, {"text": "NPoWER", "start_pos": 264, "end_pos": 270, "type": "DATASET", "confidence": 0.8175285458564758}]}, {"text": "details the different correlations and RMSEs of baseline metrics, other famous metrics and our best model.", "labels": [], "entities": [{"text": "RMSEs", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.7152756452560425}]}, {"text": "From, we seethe gap between baseline metrics and our experiments, with both validation methods.", "labels": [], "entities": []}, {"text": "We have retained the model built from the \"Bagging\" classifier with both validation methods.", "labels": [], "entities": [{"text": "Bagging\" classifier", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.5776860316594442}]}, {"text": "We observe also that the elimination of one of the used classes of features decreases the correlation of the best model (built from selected features) with Overall Responsiveness and increases the RMSE.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.7687670588493347}]}, {"text": "Furthermore, we note that the elimination of syntactic features class decreases enormously the correlation with the use of both methods of validation.", "labels": [], "entities": []}, {"text": "The surprising notification is that the elimination of AutoSummENG score increases the correlation instead of decreasing it.", "labels": [], "entities": [{"text": "AutoSummENG score", "start_pos": 55, "end_pos": 72, "type": "METRIC", "confidence": 0.6510485112667084}, {"text": "correlation", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.9932946562767029}]}, {"text": "Generally, we have noted the effect of syntactic features in the best model for both languages (Arabic, English).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Kendall's Tau Correlation Between Gradings (R2, MeMoG, AutoSummENG, NPowER and  OR) with p-value < 0.1 from MultiLing 2013 corpus", "labels": [], "entities": [{"text": "OR", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9736214876174927}, {"text": "MultiLing 2013 corpus", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.8412800431251526}]}, {"text": " Table 2: Pearson, Spearman and Kendall Correlations with Overall Responsiveness and RMSE (between  brackets) for Various Single and Ensemble learning Classifiers for Arabic language", "labels": [], "entities": [{"text": "RMSE", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9955804944038391}]}, {"text": " Table 3: Pearson, Spearman and Kendall Correlations with Overall Responsiveness Score and RMSE  (between brackets) for Arabic language", "labels": [], "entities": [{"text": "Overall Responsiveness Score", "start_pos": 58, "end_pos": 86, "type": "METRIC", "confidence": 0.7474940220514933}, {"text": "RMSE", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9949813485145569}]}, {"text": " Table 4: Pearson, Spearman and Kendall Correlations with Overall Responsiveness and RMSE (between  brackets) for Various Single and Ensemble learning Classifiers for English language", "labels": [], "entities": [{"text": "RMSE", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9944522976875305}]}, {"text": " Table 5: Pearson, Spearman and Kendall Correlations with Overall Responsiveness Score and RMSE  (between brackets) for Arabic language", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.6438663005828857}, {"text": "Overall Responsiveness Score", "start_pos": 58, "end_pos": 86, "type": "METRIC", "confidence": 0.7488922476768494}, {"text": "RMSE", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.994881272315979}]}]}