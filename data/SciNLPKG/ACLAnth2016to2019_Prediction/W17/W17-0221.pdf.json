{"title": [{"text": "Evaluation of language identification methods using 285 languages", "labels": [], "entities": [{"text": "language identification", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.7386302053928375}]}], "abstractContent": [{"text": "Language identification is the task of giving a language label to a text.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6797183901071548}]}, {"text": "It is an important preprocessing step in many automatic systems operating with written text.", "labels": [], "entities": []}, {"text": "In this paper, we present the evaluation of seven language identification methods that was done in tests between 285 languages with an out-of-domain test set.", "labels": [], "entities": [{"text": "language identification", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7306695729494095}]}, {"text": "The evaluated methods are, furthermore, described using unified notation.", "labels": [], "entities": []}, {"text": "We show that a method performing well with a small number of languages does not necessarily scale to a large number of languages.", "labels": [], "entities": []}, {"text": "The HeLI method performs best on test lengths of over 25 characters, obtaining an F 1-score of 99.5 already at 60 characters.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9918763339519501}]}], "introductionContent": [{"text": "Automatic language identification of text has been researched since the 1960s.", "labels": [], "entities": [{"text": "Automatic language identification of text", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7319852471351623}]}, {"text": "Language identification is an important preprocessing step in many automatic systems operating with written text.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7200817912817001}]}, {"text": "State of the art language identifiers obtain high rates in both recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.999409556388855}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9977783560752869}]}, {"text": "However, even the best language identifiers do not give perfect results when dealing with a large number of languages, out-of-domain texts, or short texts.", "labels": [], "entities": []}, {"text": "In this paper seven language identification methods are evaluated in tests incorporating all three of these hard contexts.", "labels": [], "entities": [{"text": "language identification", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7074073702096939}]}, {"text": "The evaluations were done as part of the Finno-Ugric Languages and The Internet project) funded by the Kone Foundation Language Programme).", "labels": [], "entities": [{"text": "Finno-Ugric Languages and The Internet project", "start_pos": 41, "end_pos": 87, "type": "DATASET", "confidence": 0.8209580282370249}, {"text": "Kone Foundation Language Programme", "start_pos": 103, "end_pos": 137, "type": "DATASET", "confidence": 0.8775974661111832}]}, {"text": "One of the major goals of the project is creating text corpora for the minority languages within the Uralic group.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the methods chosen for this evaluation.", "labels": [], "entities": []}, {"text": "In Section 3, we present the corpora used for training and testing the methods and in Section 4 we discuss and present the results of the evaluations of the methods using these corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation of the method of we created Laplace and Lidstone smoothed language models from our training corpora and programmed a language identifier, which used the sum of log probabilities (we did not use NLTK) to measure the distance between the models and the mystery text.", "labels": [], "entities": []}, {"text": "We tested n-grams from 1 to 6 with several different values of \u03bb . used byte n-grams, but as our corpus is completely UTF-8 encoded, we use n-grams of characters instead.", "labels": [], "entities": []}, {"text": "The best results) in our tests were achieved with 5-grams and a \u03bb of 0.00000001.", "labels": [], "entities": []}, {"text": "These findings are not exactly inline with those of.", "labels": [], "entities": []}, {"text": "The number of languages used in both language identifiers is comparable, but the amount of training data in our corpus varies considerably between languages when compared with the corpus used by, where each language had about the same amount of material.", "labels": [], "entities": []}, {"text": "The smallest test set they used was 2%, which corresponds to around 100 -200 characters, which is comparable to the longest test sequences used in this article.", "labels": [], "entities": []}, {"text": "We believe that these two dissimilarities in test setting could be the reason for the differing results, but we decided that investigating this further was not within the scope of this article.", "labels": [], "entities": []}, {"text": "In the evaluation of the method of Brown (2013), we used the \"mklangid\" program provided with the Brown's package to create new language models for the 285 languages of our test suite.", "labels": [], "entities": []}, {"text": "The best results with the \"whatlang\" were obtained using up to 10-byte n-grams, 40,000 ngrams in the models, and 160 million bytes of training data as well as stop-grams.", "labels": [], "entities": []}, {"text": "Stop-grams were calculated for languages with a similarity score of 0.4 or higher.", "labels": [], "entities": [{"text": "Stop-grams", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9084004759788513}]}, {"text": "The average recall obtained for 65 character samples was 98.9% with an F 1 -score of 99.0%.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.999447762966156}, {"text": "F 1 -score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9931358098983765}]}, {"text": "Brown's method clearly outperforms the results of the algorithm of, as can be seen in.", "labels": [], "entities": []}, {"text": "One thing to note is also the running time.", "labels": [], "entities": []}, {"text": "Running the tests using the algorithm of with 20,000 n-grams took over two days, as opposed to the less than an hour with Brown's \"Whatlang\" program.", "labels": [], "entities": [{"text": "Whatlang\" program", "start_pos": 131, "end_pos": 148, "type": "DATASET", "confidence": 0.8039049506187439}]}, {"text": "In order to evaluate the method used by, we utilized the VariKN toolkit to create language models from our training data with the same settings: absolute discounting smoothing with a character n-gram length of 5.", "labels": [], "entities": [{"text": "VariKN toolkit", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9139551818370819}, {"text": "absolute discounting smoothing", "start_pos": 145, "end_pos": 175, "type": "TASK", "confidence": 0.5717640419801077}]}, {"text": "When compared with the Browns identifier the results are clearly in favor of the VariKN toolkit for short test lengths and almost equal attest lengths of 70 characters, after which Brown's language identifier performs better.", "labels": [], "entities": [{"text": "VariKN toolkit", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.8709174692630768}]}, {"text": "For the evaluation of the HeLI method we used a slightly modified Python based implementation of the method.", "labels": [], "entities": []}, {"text": "In our implementation, we used relative frequencies as cut-offs c instead of just the frequencies.", "labels": [], "entities": []}, {"text": "In order to find the best possible parameters using the training corpora, we applied a simple form of the greedy algorithm using the last 10% of the training corpus for each language as a development set.", "labels": [], "entities": []}, {"text": "We started with the same n-gram length n max and the penalty value p, which were found to provide the best results in.", "labels": [], "entities": []}, {"text": "Then we proceeded using the greedy algorithm and found at least a local optimum with the values n max = 6, c = 0.0000005, and p = 7.", "labels": [], "entities": []}, {"text": "The HeLI method obtains high recall and precision clearly sooner than the methods of.", "labels": [], "entities": [{"text": "HeLI", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.803363025188446}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9992139339447021}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9956606030464172}]}, {"text": "The F 1 -score of 99.5 is achieved at 60 characters, while Brown's method achieved it at 90 characters and the method of at more than 100 characters, which can be seen in.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9902135878801346}]}, {"text": "The method of performs better than the HeLI method when the length of the mystery text is 20 characters or less.", "labels": [], "entities": []}, {"text": "The HeLI method was also tested without using the language models composed of words.", "labels": [], "entities": []}, {"text": "It was found that in addition to obtaining slightly: Some of the easiest languages to identify showing how many characters were needed for 100.0% recall by each method.: Some of the most difficult languages to identify showing how many characters were needed for 100.0% recall by each method.", "labels": [], "entities": [{"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9316442608833313}, {"text": "recall", "start_pos": 270, "end_pos": 276, "type": "METRIC", "confidence": 0.9233986139297485}]}, {"text": "lower F 1 -scores, the language identifier was also much slower when the words were not used.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.9687923640012741}]}, {"text": "We also tested using Lidstone smoothing instead of the penalty values.", "labels": [], "entities": []}, {"text": "The best results were acquired with the Lidstone value of 0.0001, almost reaching the same F 1 -scores as the language identifier with the penalty value p of 7.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9853138327598572}]}, {"text": "The largest differences in F 1 -scores were at the lower mid-range of test lengths, being 0.5 with 25-character samples from the development set.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9722641408443451}]}, {"text": "Some of the languages in the test set had such unique writing systems that their average recall was 100% already at 5 characters by many of the methods as can be seen in.", "labels": [], "entities": [{"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.997731626033783}]}, {"text": "Some of the most difficult languages can be seen in LG for LIGA, LL for LogLIGA, VK for VariKN, WL for Whatlang, CT for Cavnar and Trenkle, and KG for King and Dehdari.", "labels": [], "entities": [{"text": "King and Dehdari", "start_pos": 151, "end_pos": 167, "type": "DATASET", "confidence": 0.8731206854184469}]}], "tableCaptions": [{"text": " Table 1: Some of the easiest languages to iden- tify showing how many characters were needed for  100.0% recall by each method.", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9666070938110352}]}, {"text": " Table 2: Some of the most difficult languages  to identify showing how many characters were  needed for 100.0% recall by each method.", "labels": [], "entities": [{"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9700626730918884}]}]}