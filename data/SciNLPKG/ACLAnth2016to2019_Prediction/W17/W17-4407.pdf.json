{"title": [{"text": "Simple Queries as Distant Labels for Predicting Gender on Twitter", "labels": [], "entities": [{"text": "Predicting Gender", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.9477307796478271}]}], "abstractContent": [{"text": "The majority of research on extracting missing user attributes from social media profiles use costly hand-annotated labels for supervised learning.", "labels": [], "entities": [{"text": "extracting missing user attributes from social media profiles", "start_pos": 28, "end_pos": 89, "type": "TASK", "confidence": 0.8669532462954521}]}, {"text": "Distantly supervised methods exist, although these generally rely on knowledge gathered using external sources.", "labels": [], "entities": []}, {"text": "This paper demonstrates the effectiveness of gathering distant labels for self-reported gender on Twitter using simple queries.", "labels": [], "entities": []}, {"text": "We confirm the reliability of this query heuristic by comparing with manual annotation.", "labels": [], "entities": [{"text": "reliability", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9705374836921692}]}, {"text": "Moreover, using these labels for distant supervision, we demonstrate competitive model performance on the same data as models trained on manual annotations.", "labels": [], "entities": []}, {"text": "As such, we offer a cheap, extensible, and fast alternative that can be employed beyond the task of gender classification.", "labels": [], "entities": [{"text": "gender classification", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.7737291753292084}]}], "introductionContent": [{"text": "The popularity of social media that rely on rich self-representation of users (e.g. Facebook, LinkedIn) make them a valuable resource for conducting research based on demographic information.", "labels": [], "entities": []}, {"text": "However, the volume of personal information users provide on such platforms is generally restricted to their personal connections only, and therefore off-limits for scientific research.", "labels": [], "entities": []}, {"text": "Twitter, on the other hand, allows only a restricted amount of structured personal information by design.", "labels": [], "entities": []}, {"text": "As a result, their users tend to connect with people outside of their social circle more frequently, making many profiles and communication publicly accessible.", "labels": [], "entities": []}, {"text": "A wide variety of research has long picked upon the interesting characteristics of this microblogging service, which is well facilitated by the Twitter REST API.", "labels": [], "entities": []}, {"text": "The applied Natural Language Processing (NLP) domain of author profiling aims to infer unknown user attributes, and is therefore broadly used to compensate for the lack thereof on Twitter.", "labels": [], "entities": []}, {"text": "While previous research has already proven to be quite effective at this task using predictive models trained on manual annotations, the process of hand-labelling profiles is costly.", "labels": [], "entities": []}, {"text": "Even for the ostensibly straight-forward task of annotating gender, a large portion of Twitter users purposefully avoids providing simple indicators such as real names or profile photos including a face.", "labels": [], "entities": []}, {"text": "Consequently, this forces annotators to either dive deep into the user's timeline in search for linguistic cues, or to make decisions based on some personal interpretation, for which they have shown to often incorrectly apply stereotypical biases.", "labels": [], "entities": []}, {"text": "We show that running a small collection of adhoc queries for self-reports of gender once (\"I'm a male, female, man, woman\" etc.)", "labels": [], "entities": []}, {"text": "-provides distant labels for 6,610 profiles with high confidence in one week worth of data.", "labels": [], "entities": []}, {"text": "Employing these for distant supervision, we demonstrate them to bean accurate signal for gender classification, and form a reliable, cheap method that has competitive performance with models trained on costly humanlabelled profiles.", "labels": [], "entities": [{"text": "gender classification", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7437727451324463}]}, {"text": "Our contributions are as follows: \u2022 We demonstrate a simple, extensible method for gathering self-reports on Twitter, that competes with expensive manual annotation.", "labels": [], "entities": []}, {"text": "\u2022 We publish the IDs, manual annotations, as well as the distant labels for 6.6K Twitter profiles, spanning 16.8M tweets.", "labels": [], "entities": []}, {"text": "The data, labels, and our code to collect more data and reproduce the experiments is made available open-source at https://github.com/ cmry/simple-queries.", "labels": [], "entities": []}], "datasetContent": [{"text": "For document classification, fastText 5 (Joulin et al., 2016) was employed; a simple linear model with one hidden embedding layer that learns sentence representations using bag of words or ngram input, producing a probability distribution over the given classes using the softmax function.", "labels": [], "entities": [{"text": "document classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7652293741703033}]}, {"text": "It therefore follows the same architecture as the continuous bag of words model from, replacing the middle word with a label.", "labels": [], "entities": []}, {"text": "demonstrate the model performs well on both sentiment and tag prediction tasks, significantly speeding up training and test time compared to several recent models.", "labels": [], "entities": [{"text": "sentiment and tag prediction tasks", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.8890915632247924}]}, {"text": "Gender predictions were made using atypical set of n-gram features as input; token uni-grams and bi-grams, and character tri-grams.", "labels": [], "entities": []}, {"text": "We incorporate only those grams that occur more than three times during training.", "labels": [], "entities": []}, {"text": "As the corpora are quite small, we use embeddings with only 30 dimensions, a learning rate of 0.1, and a bucket size of 1M.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.9660733044147491}]}, {"text": "All models are trained for 10 epochs.", "labels": [], "entities": []}, {"text": "Given that fastText uses Hogwild: Individual accuracy scores and averages for majority baseline (Majority), the lexicon of, and the three models (trained on Volkova, Plank, and our dataset respectively) evaluated on the test set for each corpus.", "labels": [], "entities": [{"text": "fastText", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.9225752949714661}, {"text": "Hogwild", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.8331453204154968}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9614579081535339}]}, {"text": "Standard deviation is reported after repeating the same experiment 20 times.", "labels": [], "entities": [{"text": "Standard deviation", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9231066703796387}]}, {"text": "for parallelising Stochastic Gradient Descent, randomness in the vector representations cannot be controlled using a seed.", "labels": [], "entities": [{"text": "parallelising Stochastic Gradient Descent", "start_pos": 4, "end_pos": 45, "type": "TASK", "confidence": 0.7449313327670097}]}, {"text": "To estimate the standard deviation in the results, we ran each experiment 20 times.", "labels": [], "entities": [{"text": "standard", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9687271118164062}]}, {"text": "To evaluate how our distantly supervised model compares to using manual annotations, we trained all models in this same configuration for all three corpora.", "labels": [], "entities": []}, {"text": "Each model was then evaluated on the test set for each corpus.", "labels": [], "entities": []}, {"text": "shows accuracy scores for this 3x3 experimental design, as well as a majority baseline score (always predicting female), and an average over the three test sets for each model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9996097683906555}]}, {"text": "We closely reproduced the results from Volkova and Bachrach (2016); despite the difference in user and tweet samples, exact split order, and their use of more features including style and part-of-speech tags, our performance approaches their reported .84 accuracy score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 255, "end_pos": 263, "type": "METRIC", "confidence": 0.9904592037200928}]}, {"text": "do not provide classification results for gender on their data.", "labels": [], "entities": []}, {"text": "For comparison to state-of-the-art gender classification for English, the lexicon of is included in the results.", "labels": [], "entities": []}, {"text": "Their work also compares with, and reports a higher score (.90) for their random sample setup than reproduced in our batch evaluation (.80).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Several filter rules applied to the dis- tant labels (effectively removing those matching  the rules), their impact on both data reduction (N  hand-labelled) and agreement increase. Agree- ment is specified for: only applying these filters  (F), and in combination with the rules from", "labels": [], "entities": [{"text": "Agree- ment", "start_pos": 192, "end_pos": 203, "type": "METRIC", "confidence": 0.9440767765045166}]}, {"text": " Table 3: Various metrics of the Twitter corpora  annotated with gender used in this research. The  train and test sizes reflect the amount of batches of  200 tweets.", "labels": [], "entities": []}, {"text": " Table 4: Individual accuracy scores and averages for majority baseline (Majority), the lexicon of", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9309330582618713}]}]}