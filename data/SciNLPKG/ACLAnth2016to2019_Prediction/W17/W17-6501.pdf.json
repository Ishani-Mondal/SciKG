{"title": [{"text": "Capturing Dependency Syntax with \"Deep\" Sequential Models", "labels": [], "entities": [{"text": "Capturing Dependency Syntax", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8359046975771586}]}], "abstractContent": [{"text": "Neural network (\"deep learning\") models are taking over machine learning approaches for language by storm.", "labels": [], "entities": []}, {"text": "In particular, recurrent neural networks (RNNs), which are flexible non-markovian models of sequential data, were shown to be effective fora variety of language processing tasks.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, these seemingly purely sequential models are very capable at modeling syntactic phenomena , and using them result in very strong dependency parsers, fora variety of languages.", "labels": [], "entities": []}, {"text": "In this talk, I will briefly describe recurrent-networks, and present empirical evidence for their capabilities of learning the subject-verb agreement relation in naturally occuring text, from relatively indirect supervision.", "labels": [], "entities": [{"text": "learning the subject-verb agreement relation in naturally occuring text", "start_pos": 115, "end_pos": 186, "type": "TASK", "confidence": 0.7078948020935059}]}, {"text": "This part is based on my joint work with Tal Linzen and Emmanuel Dupoux.", "labels": [], "entities": []}, {"text": "I will then describe bi-directional recurrent networks-a simple extension of recurrent networks-and show how they can be used as the basis of state-of-the-art dependency parsers.", "labels": [], "entities": []}, {"text": "This is based on my work with Eliyahu Kipperwasser, but will also touch on work by other researchers in that space.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}