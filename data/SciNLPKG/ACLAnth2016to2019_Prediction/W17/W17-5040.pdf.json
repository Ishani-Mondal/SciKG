{"title": [{"text": "Fine-grained essay scoring of a complex writing task for native speakers", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.6909306198358536}]}], "abstractContent": [{"text": "Automatic essay scoring is nowadays successfully used even in high-stakes tests, but this is mainly limited to holistic scoring of learner essays.", "labels": [], "entities": [{"text": "Automatic essay scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5949454208215078}]}, {"text": "We present anew dataset of essays written by highly proficient German native speakers that is scored using a fine-grained rubric with the goal to provide detailed feedback.", "labels": [], "entities": []}, {"text": "Our experiments with two state-of-the-art scoring systems (a neural and a SVM-based one) show a large drop in performance compared to existing datasets.", "labels": [], "entities": []}, {"text": "This demonstrates the need for such datasets that allow to guide research on more elaborate essay scoring methods.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.7367911636829376}]}], "introductionContent": [{"text": "Automatic essay scoring is the task of automatically rating free-form writings.", "labels": [], "entities": [{"text": "Automatic essay scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5842483043670654}]}, {"text": "The scores assigned are often holistic and are based both on content and form.", "labels": [], "entities": []}, {"text": "Automatic essay scoring is nowadays successfully used to reduce human scoring workload), for example for the assessment of language proficiency.", "labels": [], "entities": [{"text": "Automatic essay scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5970534384250641}, {"text": "assessment of language proficiency", "start_pos": 109, "end_pos": 143, "type": "TASK", "confidence": 0.6767369955778122}]}, {"text": "Automatically assigned scores are considered reliable enough that they have replaced one out of two human annotators even in high-stakes language proficiency tests such as TOEFL for many years now ().", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 172, "end_pos": 177, "type": "DATASET", "confidence": 0.7965941429138184}]}, {"text": "Essay scoring approaches in recent years have mainly focused on a small number of publicly available datasets, especially the ASAP dataset from the Kaggle competition.", "labels": [], "entities": [{"text": "Essay scoring", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7847819328308105}, {"text": "ASAP dataset from the Kaggle competition", "start_pos": 126, "end_pos": 166, "type": "DATASET", "confidence": 0.9003670314947764}]}, {"text": "On this dataset, many approaches reach very competitive results, comparable to human scoring performance, so that the impression might arise that automatic essay scoring is a solved problem.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.6793826669454575}]}, {"text": "In this paper, we present experiments on anew dataset that we consider to be more challenging than currently available ones.", "labels": [], "entities": []}, {"text": "We score essays written by prospective teachers, before starting their university education in Germany.", "labels": [], "entities": []}, {"text": "These essays in German language are collected to assess whether these native-speaking students might need additional language training in order to become a teacher.", "labels": [], "entities": []}, {"text": "While other datasets either measure the full range of language proficiency from novice learners to (near-)natives, or measure the writings of high-school students, our dataset shows much less variety in language proficiency.", "labels": [], "entities": []}, {"text": "As almost all test-takers are native speakers and possess a general qualification for university entrance, differences between good and a not so good essays are much less pronounced.", "labels": [], "entities": []}, {"text": "When applying state-of-the-art essay scoring systems on this dataset, we find that a feature set working well on a standard dataset shows a considerably worse performance on our data.", "labels": [], "entities": []}, {"text": "This makes it very questionable whether automatic scoring techniques could currently be applied in a real-life scenario, thus confirming the need for deeper methods able to handle such datasets.", "labels": [], "entities": []}, {"text": "We first present an overview of related work, especially publicly available datasets and present our corpus in detail.", "labels": [], "entities": []}, {"text": "We then asses the scorability of the corpus by a series of experiments using a supervised machine learning system with a standard feature set.", "labels": [], "entities": []}, {"text": "We first confirm that our system reaches state-of-the-art performance by evaluating it on the ASAP corpus and scores in our corpus assessing the writing globally.", "labels": [], "entities": [{"text": "ASAP corpus", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.9571132361888885}]}, {"text": "Subsequently, we assess how well such a feature set is suited to model the different scoring variables annotated in our data and find that the global scores are modeled best.", "labels": [], "entities": []}, {"text": "Concentrating on these scores, we investigate the influence of various feature settings and different amounts of training data on the scor-357 ing performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "As described above, most datasets are either small or target a wide range of proficiency levels, so that relatively shallow features are sufficient to achieve quite good performance.", "labels": [], "entities": []}, {"text": "To overcome this problem, we have created anew dataset from essays written in German by prospective university students, mostly native speakers.", "labels": [], "entities": []}, {"text": "The essays are one part of a large-scale assessment project at the University of Duisburg-Essen, SkaLa.", "labels": [], "entities": []}, {"text": "All students who intend to enroll in a degree program for future teachers have to participate in a compulsory language assessment.", "labels": [], "entities": []}, {"text": "A major constituent of this assessment is an open writing task with two parts.", "labels": [], "entities": []}, {"text": "First, students are asked to summarize a newspaper article dealing with an education-related topic (which we call the source text), in our datasets, an article about the pros and cons of study fees.", "labels": [], "entities": []}, {"text": "This part of the response is the summary part of the essay.", "labels": [], "entities": []}, {"text": "Second, the students shall briefly discuss a particular statement from the prompt (the discussion part).", "labels": [], "entities": []}, {"text": "The time limit for this task is 120 minutes and the produced text is supposed to consist of at least 350 words.", "labels": [], "entities": []}, {"text": "The aim of the fine-grained evaluation is to identify the participants' strengths and weaknesses as precisely as possible.", "labels": [], "entities": []}, {"text": "After a manual evaluation of the essays, students receive detailed feedback about their performance in each of the manually scored variables and -if need be-are informed about relevant available training programs at the university designed to foster written language competencies.", "labels": [], "entities": []}, {"text": "In this way, 2,020 essays with an average of around 600 tokens per essay were collected and scored as described next.", "labels": [], "entities": []}, {"text": "Scoring Rubric While many essay-scoring corpora provide only a holistic score, this dataset has been scored using a fine-grained rubric, targeting different aspects of writing.", "labels": [], "entities": []}, {"text": "The raters were asked to evaluate the students' texts with regard to a total of 41 variables.", "labels": [], "entities": []}, {"text": "The writing skills ratings are based upon analytical descriptors (cf. Weigle and).", "labels": [], "entities": []}, {"text": "provides an overview of the annotated variables.", "labels": [], "entities": []}, {"text": "11 variables measured content-related aspects, i.e. whether a certain argument regarding the topic of the source text is mentioned in the essay, 3 formal, 5 structural and 10 measured linguistic aspects.", "labels": [], "entities": []}, {"text": "In addition, there are 6 dimension variables and one overall variable.", "labels": [], "entities": []}, {"text": "Before the annotators scored the texts according to the fine-grained rubric, they evaluated the texts in a subjective-holistic overall rating (G1 -written language competence).", "labels": [], "entities": [{"text": "G1 -written language competence)", "start_pos": 143, "end_pos": 175, "type": "METRIC", "confidence": 0.880408356587092}]}, {"text": "This evaluation was always carried out immediately after the first reading of the text, hence before the extensive analytical evaluation.", "labels": [], "entities": []}, {"text": "The rating scheme includes three types of variables: a) descriptors are variables for the evaluation of specific individual aspects of an essay (e.g. whether a certain argument from the source text is covered in the summary, whether the central thesis is correctly identified or whether grammar is proficiently used).", "labels": [], "entities": []}, {"text": "The descriptors are directly annotated.", "labels": [], "entities": []}, {"text": "b) Dimension ratings (G2-G7) are weighted aggregations of individual descriptors, i.e. they are not annotated but computed based on the descriptor annotations (e.g. G4-Discussion is an aggregation of the descriptors for the discussion part D1 to D4).", "labels": [], "entities": []}, {"text": "c) Finally, a superordinate rating (G8 -informed overall judgment) emerges from the weighted aggregation of the dimension ratings and therefore relates to the entire text in all of its aspects covered by the rating scheme.", "labels": [], "entities": [{"text": "G8 -informed overall judgment", "start_pos": 36, "end_pos": 65, "type": "METRIC", "confidence": 0.8644439935684204}]}, {"text": "(Annotators were allowed to change the aggregated G8 score, if they felt it did not adequately represent the essay.)", "labels": [], "entities": [{"text": "aggregated G8 score", "start_pos": 39, "end_pos": 58, "type": "METRIC", "confidence": 0.7502280672391256}]}, {"text": "The essays were annotated by one out of 6 annotators each.", "labels": [], "entities": []}, {"text": "The annotators received extensive training on a subset of randomly selected 120 essays.", "labels": [], "entities": []}, {"text": "After training, annotators reached an inter-annotator agreement between 52 and 100% ModAgree (cf. and) for the different descriptor variables.", "labels": [], "entities": []}, {"text": "Percentage ModAgree is computed by measuring per essay and variable what percentage of all ratings assigned by the different annotators for this essay agrees with the mode, i.e. the value assigned most often.", "labels": [], "entities": []}, {"text": "These values are then aggregated across all essays.", "labels": [], "entities": []}, {"text": "For the subjective-holistic G1 score, annotators reached 59% ModAgree, for the aggregated G8 score 60% ModAgree was reached . Note that higher agreement values for the descriptor variables are partially due to fewer categories available for annotation.", "labels": [], "entities": [{"text": "annotators", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9582669734954834}, {"text": "ModAgree", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.8846455216407776}]}, {"text": "In very few cases (up to four essays per variable), it was not possible for the annotator to encode a certain variable fora category (e.g. discussion variables could not be annotated if the discussion part was missing).", "labels": [], "entities": []}, {"text": "We do not represent those essays in the label distributions and exclude them from the training and test data when performing machine learning experiments for that variable.", "labels": [], "entities": []}, {"text": "We split the data randomly into 90% training data and reserve 10%, i.e. 202 essays, as held-out test set.", "labels": [], "entities": []}, {"text": "If not reported otherwise, our results are based on ten-fold cross-validation on the training section.", "labels": [], "entities": []}, {"text": "In accordance with previous work, we evaluate using quadratically weighted kappa (QWK).", "labels": [], "entities": []}, {"text": "For a more intuitive interpretation of the results, we also report accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9994773268699646}]}, {"text": "All data is preprocessed using a DKPro pipeline) consisting of segmentation, POS-tagging (both OpenNLP 2 ), lemmatization using the MateLemmatizer () and parsing using the StanfordParser (  This section presents our experimental results.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.962459146976471}, {"text": "StanfordParser", "start_pos": 172, "end_pos": 186, "type": "DATASET", "confidence": 0.9743704795837402}]}, {"text": "We first evaluate state-of-the-art systems on the two global variables G1 and G8 in our data and compare to the performance on ASAP.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.9353298544883728}]}, {"text": "We then investigate the performance on all variables to measure to what factors our model is sensitive.", "labels": [], "entities": []}, {"text": "In subsequent experiments we address the influence of using the essay's summary and discussion part separately, of individual feature groups and the size of training data on the scoring performance.", "labels": [], "entities": []}, {"text": "In our first experiment, we assess the overall performance of the scoring system on the two global variables G1 (holistic) and G8 (aggregated) under different feature settings.", "labels": [], "entities": []}, {"text": "We apply the neural system and for the SVM-based system we test several conditions: As n-grams are known to be strong features), we evaluate a baseline taking only token n-grams into consideration.", "labels": [], "entities": []}, {"text": "We use two versions of the feature, one where we consider the top 1,000 most frequent ngrams (n-gram 1,000) and one where we consider the top 10,000 n-grams (n-gram 10,000).", "labels": [], "entities": []}, {"text": "We next evaluate the full system with and without stacking of the three groups of n-gram features individually in order to avoid that these feature groups might overpower the other features.", "labels": [], "entities": []}, {"text": "In, we report the performance of the different setups for the variables G1 and G8.", "labels": [], "entities": []}, {"text": "We see that we always reach a higher performance when predicting the informed overall score G8 than the holistic G1.", "labels": [], "entities": [{"text": "informed overall score G8", "start_pos": 69, "end_pos": 94, "type": "METRIC", "confidence": 0.8310339748859406}]}, {"text": "Remember that G1 is assigned before scoring the other essay variables while G8 is a score based on the other variables.", "labels": [], "entities": [{"text": "G1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9703398942947388}]}, {"text": "It seems plausible that G8 is more consistent and easier to predict automatically, although we do not find that reflected in agreement scores between human annotators.", "labels": [], "entities": []}, {"text": "We further observe that the performances of: Scoring performance for G1 (intuitive holistic) and G8 (aggregated holistic).", "labels": [], "entities": []}, {"text": "For comparison, we also provide the performance of a comparable English model for the ASAP dataset (averaged overall 8 prompts).", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.94764643907547}]}, {"text": "both variables benefit from a larger number of ngrams.", "labels": [], "entities": []}, {"text": "However, additional features in the full model are only beneficial if we have lower numbers of n-grams.", "labels": [], "entities": []}, {"text": "These findings suggest that there is some redundancy between the n-grams and the remaining features.", "labels": [], "entities": []}, {"text": "We observe that using the out-of-the-box neural system is at least on par with the best supervised configuration.", "labels": [], "entities": []}, {"text": "While this shows the potential of neural approaches on the global variables, in the following experiments, we concentrate on the SVM system that can be more easily targeted towards the individual variables.", "labels": [], "entities": []}, {"text": "Verification Using ASAP Data For comparison, we also evaluate our feature set (with minor adaptations from German to English) on the ASAP corpus, a well-known dataset for essay scoring (see Section 2).", "labels": [], "entities": [{"text": "ASAP Data", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.7538612484931946}, {"text": "ASAP corpus", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.931376576423645}, {"text": "essay scoring", "start_pos": 171, "end_pos": 184, "type": "TASK", "confidence": 0.7328877449035645}]}, {"text": "As labeled test data is not available, we evaluate using 5-fold cross validation on the training data -, the two rightmost columns.", "labels": [], "entities": []}, {"text": "For the neural system, we report results by.", "labels": [], "entities": []}, {"text": "Both the neural system .76 QWK and the SVM .72 QWK are on par with the best open-source system participating in the ASAP shared task that reached .71 QWK.", "labels": [], "entities": [{"text": "SVM .72 QWK", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8949882984161377}, {"text": "ASAP shared task", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.6660193006197611}, {"text": ".71 QWK", "start_pos": 146, "end_pos": 153, "type": "DATASET", "confidence": 0.6500803132851919}]}, {"text": "These results show that the applied systems are state of the art on established datasets and are thus probably also state of the art on our new dataset.", "labels": [], "entities": []}, {"text": "However, the performance level is much lower, as the task is more challenging.", "labels": [], "entities": []}, {"text": "Next, we want to assess how well our essay scoring system is able to predict the different variables.", "labels": [], "entities": []}, {"text": "We repeat Experiment 1 in the best-performing feature setting using the full model with 10,000 n-grams for each scoring variable separately, i.e. we use always the same features to train different models.: Scoring performance measured in quadratically weighted kappa for G1 and G8 with features computed on the complete essay text (merged) and with features computed on the summary and discussion part separately (split).", "labels": [], "entities": []}, {"text": "It is clear that our one-fits-all approach can be improved by using feature sets tailored towards the individual variables.", "labels": [], "entities": []}, {"text": "With this experiment we rather want to investigate which variables are sensitive to our model which uses features used for predicting global scores.", "labels": [], "entities": []}, {"text": "This could help to answer the question which aspects of a global score an essay scoring system actually measures.", "labels": [], "entities": []}, {"text": "We see that the feature set predicts at a very moderate level for many of the variables.", "labels": [], "entities": []}, {"text": "For the very skewed variables F1 to F3, L6 and L8, performance is particularly bad.", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9803594946861267}]}, {"text": "We also see that variables from each of the four categories (content discussion, structure, and language) can be learnt to a very limited degree.", "labels": [], "entities": []}, {"text": "Interestingly, the model performs a bit better on the aggregated scores G2 to G7 and the two global variables G1 and G8 although still on a level that prohibits a practical use of the system.", "labels": [], "entities": []}, {"text": "In the following, we concentrate on G1 an G8.", "labels": [], "entities": []}, {"text": "In doing so, we can also compare to scores in other essay datasets that use only holistic scores.", "labels": [], "entities": []}, {"text": "The prompt in our task asks for essays with a specific structure: a summarization and a discussion part.", "labels": [], "entities": []}, {"text": "Some of the variables are measured on only one of those two parts (cf.).", "labels": [], "entities": []}, {"text": "Therefore it seems reasonable to measure not only if a feature occurs, but also in which part of the essay.", "labels": [], "entities": []}, {"text": "For example, the trigram in my opinion might bean indicator fora good essay if it occurs in the discussion, but not in the summary.", "labels": [], "entities": []}, {"text": "Therefore, we also determine n-gram features (token, pos, and skip) separately for both essay parts.", "labels": [], "entities": []}, {"text": "In the split condition in, we duplicate each n-gram feature and compute it individually on the summary and There are essays where only one part was present.", "labels": [], "entities": []}, {"text": "In such cases all features for the other part have been set to 0.", "labels": [], "entities": []}, {"text": "the discussion part, the merged condition repeats values from for the best-performing SVM, the full model with 10,000 n-grams.", "labels": [], "entities": []}, {"text": "We see that for G8 we profit from that split, while for G1 we do not.", "labels": [], "entities": []}, {"text": "We do not have a good intuition why this is the case, but suspect that the more informed G8 score takes this additional information better into account.", "labels": [], "entities": []}, {"text": "What we learn from this experiment is that it is helpful to take additional prompt-specific structure in the data into consideration.", "labels": [], "entities": []}, {"text": "Identifying further automatically detectable sub-parts of the essay and treating them separately is a promising step for future work.", "labels": [], "entities": []}, {"text": "We have seen that a major contribution to the performance for both ASAP and our dataset comes from token n-gram features and that we benefit from a higher number of n-grams.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 67, "end_pos": 71, "type": "TASK", "confidence": 0.48998019099235535}]}, {"text": "To further assess this influence, we take the number of available n-grams to their extremes and perform experiments using token n-grams and POS n-grams individually while varying the number of k topfrequent n-grams to extract from 10 to 10,000.", "labels": [], "entities": []}, {"text": "Note that it can happen for POS n-grams and for token n-grams on ASAP that k is bigger than the actual number of n-grams present in the data.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.8802578449249268}]}, {"text": "In that case, we take all available n-grams.", "labels": [], "entities": []}, {"text": "In, we see a huge difference between ASAP and our corpus: in ASAP, a steep performance increase can be observed already with low numbers of n-grams and the curve flattens out early.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.7170100808143616}]}, {"text": "In our corpus, we see a steady increase of  performance that is less pronounced in the beginning and in general on a much lower level.", "labels": [], "entities": []}, {"text": "One corpus variable explaining this effect is the average length of the essay.", "labels": [], "entities": []}, {"text": "ASAP essays are shorter (the average number of tokens per prompt varies between 100 and 600) while our essays have a general average around 600 tokens.", "labels": [], "entities": [{"text": "ASAP essays", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8739542067050934}]}, {"text": "Even if we add more n-gram features the performance gap never closes and shows the difficulty in our data.", "labels": [], "entities": []}, {"text": "We perform an ablation test to discern the contribution of individual feature groups.", "labels": [], "entities": []}, {"text": "shows the performance for the full model (using top 1,000 token, POS and skip n-grams, and stacking) and for the model with individual feature groups ablated.", "labels": [], "entities": []}, {"text": "We chose this model because our models with more n-gram features show very similar results for the full model in comparison to n-grams only and the current settings seems most suitable to highlight potential contributions of individual features.", "labels": [], "entities": []}, {"text": "We can see that the feature group with the highest effect are unsurprisingly token n-grams.", "labels": [], "entities": []}, {"text": "Most of the other features have only a minor effect.", "labels": [], "entities": []}, {"text": "However, we saw in the comparison between the full model and n-grams only, that the additional features have a beneficial effect in our setting.", "labels": [], "entities": []}, {"text": "We assume that our feature set is quite redundant, so that e.g. the occurrence of a connective can also be learnt from the respective unigram.", "labels": [], "entities": []}, {"text": "Ina practical setting it is important to know how many training instances have to be available to reach a certain performance and at which amount of training data the performance levels off.", "labels": [], "entities": []}, {"text": "This helps us to decide whether we can already fully as- sess the performance of our method on the given data or whether more training data would be helpful.", "labels": [], "entities": []}, {"text": "We therefore perform a learning curve experiment showing the correlation between the number of training data and the scoring performance.", "labels": [], "entities": []}, {"text": "In this experiment, we keep the test data constant and use the 10% held-out data for this purpose.", "labels": [], "entities": []}, {"text": "We use the split feature set with 10,000 n-gram features which showed the best performance on the cross-validation experiments.", "labels": [], "entities": []}, {"text": "We always double the number of training data, starting from 7 until we reach 1,800.", "labels": [], "entities": []}, {"text": "We sample each number of training instances randomly 100 times from the pool of unlabeled data and report average, worst and best performance across those 100 runs.", "labels": [], "entities": []}, {"text": "The resulting learning curves are shown in We can see that the performance varies tremendously between the best and worst runs for smaller amounts of training data.", "labels": [], "entities": []}, {"text": "This highlights that a careful selection of training data can help when only limited human annotation effort is available.", "labels": [], "entities": []}, {"text": "We also see that the curve starts to flatten out in the end for the best case, so that we will not profit much more from more training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scoring categories in our corpus. Note that a lower score corresponds to a better essay.", "labels": [], "entities": []}, {"text": " Table 2: Scoring performance for G1 (intuitive holistic) and G8 (aggregated holistic). For comparison,  we also provide the performance of a comparable English model for the ASAP dataset (averaged over  all 8 prompts).", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 175, "end_pos": 187, "type": "DATASET", "confidence": 0.9357804954051971}]}, {"text": " Table 3: Scoring performance measured in  quadratically weighted kappa for G1 and G8 with  features computed on the complete essay text  (merged) and with features computed on the sum- mary and discussion part separately (split).", "labels": [], "entities": []}, {"text": " Table 4: Ablation test (QWK) for the global G8  variable.", "labels": [], "entities": [{"text": "Ablation test (QWK)", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.953858470916748}]}]}