{"title": [{"text": "A Syllable-based Technique for Word Embeddings of Korean Words", "labels": [], "entities": [{"text": "Word Embeddings of Korean Words", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.5973444819450379}]}], "abstractContent": [{"text": "Word embedding has become a fundamental component to many NLP tasks such as named entity recognition and machine translation.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.6733380953470866}, {"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8054258227348328}]}, {"text": "However, popular models that learn such embeddings are unaware of the morphology of words, so it is not directly applicable to highly agglu-tinative languages such as Korean.", "labels": [], "entities": []}, {"text": "We propose a syllable-based learning model for Korean using a convolutional neural network, in which word representation is composed of trained syllable vectors.", "labels": [], "entities": []}, {"text": "Our model successfully produces morphologically meaningful representation of Korean words compared to the original Skip-gram embeddings.", "labels": [], "entities": []}, {"text": "The results also show that it is quite robust to the Out-of-Vocabulary problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "Continuous word representation has been a fundamental ingredient to many NLP tasks with the advent of simple and successful approaches such as Word2Vec () and GloVe (.", "labels": [], "entities": [{"text": "word representation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.724786564707756}]}, {"text": "Although it has been verified that they are effective in formulating semantic and syntactic relationship between words, there are some limitations.", "labels": [], "entities": [{"text": "formulating semantic and syntactic relationship between words", "start_pos": 57, "end_pos": 118, "type": "TASK", "confidence": 0.8496276906558445}]}, {"text": "First, they are only available to words in pre-defined vocabulary thus prone to the Out-of-Vocabulary(OOV) problem.", "labels": [], "entities": []}, {"text": "Second, they cannot utilize subword information at all because they regard word as a basic unit.", "labels": [], "entities": []}, {"text": "Those problems become more magnified when applying word-based methods to agglutinative languages such as Korean, Japanese, Turkish, and Finnish.", "labels": [], "entities": []}, {"text": "In this work, we propose anew model * Portions of this research were done while the author was a student at Seoul National University. that utilizes syllables as basic components of word representation to alleviate the problems, especially for Korean.", "labels": [], "entities": [{"text": "Seoul National University.", "start_pos": 108, "end_pos": 134, "type": "DATASET", "confidence": 0.8957754770914713}]}, {"text": "In our experiment, we confirm that our model constructs representation of words which contains a semantic and syntactic relationship between words.", "labels": [], "entities": []}, {"text": "We also show that our model can handle OOV problem and capture morphological information without dedicated analysis.", "labels": [], "entities": [{"text": "OOV", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8301440477371216}]}], "datasetContent": [{"text": "The Experiments are performed on a randomly sampled subset of Korean News corpus collected from 2012 to 2014, containing approximately 2.7M tokens, 11k vocabulary, and 1k syllables.", "labels": [], "entities": [{"text": "Korean News corpus collected", "start_pos": 62, "end_pos": 90, "type": "DATASET", "confidence": 0.9816863238811493}]}, {"text": "We compare our model to the original skip-gram model with negative sampling () as a baseline.", "labels": [], "entities": []}, {"text": "We use the WordSim353 dataset () for the word similarity and relatedness task.", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 11, "end_pos": 29, "type": "DATASET", "confidence": 0.9835523366928101}]}, {"text": "As WordSim353 dataset is an English data, we translated it into Korean.", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 3, "end_pos": 21, "type": "DATASET", "confidence": 0.98469477891922}]}, {"text": "The quality of the word vector representation is evaluated by computing Pearson correlation coefficient between human judgment scores and the cosine similarity between word vectors.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 72, "end_pos": 103, "type": "METRIC", "confidence": 0.9668179551760355}]}, {"text": "The graph in shows that our model outperforms the baseline on WS353-Similarity dataset.", "labels": [], "entities": [{"text": "WS353-Similarity dataset", "start_pos": 62, "end_pos": 86, "type": "DATASET", "confidence": 0.9729173183441162}]}, {"text": "We estimated it since a lot of similar words share the same syllable(s) in Korean.", "labels": [], "entities": []}, {"text": "On the other hand, on WS353-Relatedness, the performance is not as good in comparison with the similarity task.", "labels": [], "entities": [{"text": "WS353-Relatedness", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.8399043083190918}]}, {"text": "We presume that leveraging syllables on computing representations can be a noise among related words without common syllables.", "labels": [], "entities": []}, {"text": "Out-Of-Vocabulary Test Since our model uses syllable vectors when computing word representation, it is possible to achieve representation of OOV words by combining syllables.", "labels": [], "entities": []}, {"text": "To evaluate the representations of OOV words, we manually chose 4 newly coined words not appear in training data.", "labels": [], "entities": []}, {"text": "These words were derived from original words.", "labels": [], "entities": []}, {"text": "For example, ' '(God Google, gugeulsin) is derived from ' '(Google, gugeul) and ' '(Gal'Note, gaelnoteu) is a abbreviation form of ' '(Galaxy Note, gaelleogsinoteu).", "labels": [], "entities": []}, {"text": "Morphologically, two of them concatenate additional syllables to the original word, and the other Original word Newly coined word (God google, gugeulsin) (Profit, ideug) (Real profit, gaeideug) (Leave work, toegeun) (Time to leave work, toegeungag) (Galaxy Note, gaelleogsinoteu) (Gal'Note, gaelnoteu): 4 newly coined words in Korean which did not appear in training data.", "labels": [], "entities": []}, {"text": "Proposed model successfully recognized stem from the original word, and predicted it as the most similar word.", "labels": [], "entities": []}, {"text": "We examined the nearest neighbor of the representations of OOV words, and confirmed that each original word vector is placed in the nearest distance.", "labels": [], "entities": []}, {"text": "It is no wonder since almost every newly coined word keeps the syllables of original word with their positions fixed.", "labels": [], "entities": []}], "tableCaptions": []}