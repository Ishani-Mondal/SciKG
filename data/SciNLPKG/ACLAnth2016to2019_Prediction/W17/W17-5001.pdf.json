{"title": [{"text": "Question Difficulty -How to Estimate Without Norming, How to Use for Automated Grading", "labels": [], "entities": [{"text": "Norming", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.8079579472541809}]}], "abstractContent": [{"text": "Question difficulty estimates guide test creation , but are too costly for small-scale testing.", "labels": [], "entities": []}, {"text": "We empirically verify that Bloom's Taxonomy, a standard tool for difficulty estimation during question creation, reliably predicts question difficulty observed after testing in a short-answer corpus.", "labels": [], "entities": [{"text": "difficulty estimation during question creation", "start_pos": 65, "end_pos": 111, "type": "TASK", "confidence": 0.6404399573802948}]}, {"text": "We also find that difficulty can be approximated by the amount of variation in student answers, which can be computed before grading.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9908389449119568}]}, {"text": "We show that question difficulty and its approximations are useful for automated grading, allowing us to identify the optimal feature set for grading each question even in an unseen-question setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Testing is a core component of teaching, and many tasks in NLP for education are concerned with creating good questions and correctly grading the answers.", "labels": [], "entities": []}, {"text": "We look at how to estimate question difficulty from question wording as a link between the two tasks.", "labels": [], "entities": []}, {"text": "From a test creation point of view, knowing question difficulty levels is imperative: Too many easy questions, and the test will be unable to distinguish between the more able test-takers, who all achieve equally good results.", "labels": [], "entities": []}, {"text": "Too many hard questions, and only the most able test-takers will be clearly distinguishable from the (low-performing) rest.", "labels": [], "entities": []}, {"text": "In large-scale testing, question difficulty and other measures of question quality are established through prior norming, where the questions are answered by a pool of test-takers in a dry run before definitive use with a similar demographic.", "labels": [], "entities": []}, {"text": "Difficulty is then determined on the basis of the observed results using probabilistic test theory (PTT).", "labels": [], "entities": [{"text": "Difficulty", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9681089520454407}]}, {"text": "Norming is usually not available in automated question creation or in ad-hoc testing in small classrooms, while the need for correctly determining question difficulty of course remains.", "labels": [], "entities": [{"text": "Norming", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9753064513206482}, {"text": "question creation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7131787091493607}]}, {"text": "In this situation, teachers often use Bloom's Taxonomy), a classification of the knowledge dimensions and cognitive processes involved in the completion of a test task, to formulate questions of appropriate difficulty.", "labels": [], "entities": []}, {"text": "In the literature, the difficulty of multiple-choice questions has been successfully aligned with the cognitive process dimension of the Bloom hierarchy;, but see also).", "labels": [], "entities": []}, {"text": "In this paper, we empirically evaluate the predictive power of both Bloom dimensions for estimating the empirically observed difficulty of short-answer questions, which require the test-taker to freely formulate one to three sentence answers.", "labels": [], "entities": []}, {"text": "We find that the Taxonomy allows a useful approximation of question difficulty at the time of question creation.", "labels": [], "entities": [{"text": "question creation", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7319821566343307}]}, {"text": "We find clear empirical evidence that the instructional context, that is the teaching materials presented in instruction, has to betaken into account when determining difficulty using the Taxonomy.", "labels": [], "entities": []}, {"text": "Once test-taker answers are available, but before grading makes PTT analysis possible, another predictor for question difficulty becomes available: Answer variation, the average amount of variation within the student answers for each question, is computed based only on the answer strings.", "labels": [], "entities": [{"text": "PTT analysis", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.8025935590267181}, {"text": "Answer variation", "start_pos": 148, "end_pos": 164, "type": "METRIC", "confidence": 0.8660257756710052}]}, {"text": "We also look at question difficulty from the point of view of improving automated short-answer grading (SAG).", "labels": [], "entities": [{"text": "automated short-answer grading (SAG)", "start_pos": 72, "end_pos": 108, "type": "TASK", "confidence": 0.6250347892443339}]}, {"text": "To date, the focus of research has been on finding informative features, ranging from deep processing () through text-based similarity () to shallow, string-based approaches. has proposed to perform pre-grading model selection by tailoring feature sets to the characteristics of 1 different short-answer corpora.", "labels": [], "entities": [{"text": "pre-grading model selection", "start_pos": 199, "end_pos": 226, "type": "TASK", "confidence": 0.6211440563201904}]}, {"text": "We refine this idea and show that within the same corpus, questions with different difficulty levels also profit from different feature sets, and that the Bloom Taxonomy levels and student answer variation can be used as stand-ins for feature set prediction if difficulty estimates are not available.", "labels": [], "entities": [{"text": "feature set prediction", "start_pos": 235, "end_pos": 257, "type": "TASK", "confidence": 0.6634862820307413}]}, {"text": "These results point to anew avenue of research in SAG.", "labels": [], "entities": [{"text": "SAG", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.8257389664649963}]}, {"text": "The paper is structured as follows: We begin by providing some theoretical background on PTT and Bloom's Taxonomy in Section 2.", "labels": [], "entities": [{"text": "PTT and Bloom's Taxonomy", "start_pos": 89, "end_pos": 113, "type": "DATASET", "confidence": 0.691839462518692}]}, {"text": "Our first set of analyses tests the reliability of the Bloom's Taxonomy question difficulty predictions for our data set (Section 3).", "labels": [], "entities": [{"text": "reliability", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9686837196350098}, {"text": "Bloom's Taxonomy question difficulty", "start_pos": 55, "end_pos": 91, "type": "DATASET", "confidence": 0.8096435785293579}]}, {"text": "The second analysis in Section 4 focuses on the relationship between answer variation and question difficulty.", "labels": [], "entities": []}, {"text": "Our final set of experiments investigates the use of question difficulty for question-level model selection in short-answer grading (Section 5).", "labels": [], "entities": [{"text": "question-level model selection", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.6039381424585978}]}, {"text": "We end with a discussion and conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Difficulty and the Cognitive Process lev- els, re-assigned using instructional context: Linear  model coefficients. *: p < 0.05, ns: not significant.", "labels": [], "entities": []}, {"text": " Table 3: Difficulty and the Knowledge levels:  Linear model coefficients. **: p < 0.01, ns: not  significant.", "labels": [], "entities": [{"text": "Difficulty", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9442204833030701}]}, {"text": " Table 4: Cognitive Process text&question and Knowledge dimensions, Rasch difficulty averages (number  of questions).", "labels": [], "entities": [{"text": "Rasch difficulty averages", "start_pos": 68, "end_pos": 93, "type": "METRIC", "confidence": 0.8213985562324524}]}, {"text": " Table 6: Overview of the feature set for automated grading", "labels": [], "entities": []}, {"text": " Table 8: Model Selection: Accuracy of predicting the best-performing feature set. Left: Logistic model,  right: Heuristic", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9653676748275757}]}]}