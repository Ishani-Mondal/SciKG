{"title": [], "abstractContent": [{"text": "In this paper, we propose three different methods for automatic evaluation of the machine translation (MT) quality.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.8170449614524842}]}, {"text": "Two of the metrics are trainable on direct-assessment scores and two of them use dependency structures.", "labels": [], "entities": []}, {"text": "The trainable metric AutoDA, which uses deep-syntactic features , achieved better correlation with humans compared e.g. to the chrF3 metric.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the ongoing research of the machine translation (MT) systems in the past the need for accurate automatic evaluation of the translation quality became unquestionable.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.847010362148285}]}, {"text": "Even though the human judgment of the MT system outputs still holds as the most reliable form of evaluation, the high cost of human evaluation together with the amount of time required for such evaluation makes human judgment unsuitable for large scale experiments where we need to evaluate many different system configurations in a relatively short timespan.", "labels": [], "entities": [{"text": "MT system outputs", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.8636997938156128}]}, {"text": "An additional important limitation of human evaluation is that it cannot be exactly repeated.", "labels": [], "entities": []}, {"text": "This led to development of various methods for automatic MT evaluation in the past with the aim to eliminate the need for the expensive human assessment of the developed MT systems.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9790243208408356}, {"text": "MT", "start_pos": 170, "end_pos": 172, "type": "TASK", "confidence": 0.9758176803588867}]}, {"text": "In this paper we suggest three novel methods for automatic MT evaluation together with their direct comparison: 3.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.9735443890094757}]}, {"text": "NMTScorer: A neural sequence classifier which assigns correct/incorrect flags to the evaluated sentence segments.", "labels": [], "entities": [{"text": "NMTScorer", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9297397136688232}]}, {"text": "shows the main properties of the proposed methods.", "labels": [], "entities": []}, {"text": "Some of them were mainly developed for Czech as the target language and were later modified to be applied to other languages.", "labels": [], "entities": []}, {"text": "The differences in the data preprocessing and their impact on the resulting evaluator are also described in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "TreeAggreg is a simple sentence-level metric, remotely inspired by HUME.", "labels": [], "entities": []}, {"text": "Rather than being a full standalone metric, it can be regarded as: Pearson correlations of different sentence-level metrics on WMT16 HUMEseg dataset.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 67, "end_pos": 87, "type": "METRIC", "confidence": 0.9460688233375549}, {"text": "WMT16 HUMEseg dataset", "start_pos": 127, "end_pos": 148, "type": "DATASET", "confidence": 0.9232584039370219}]}, {"text": "Standard NIST and chrF metrics are compared with our individual features matching.", "labels": [], "entities": [{"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8847922086715698}]}, {"text": "AutoDA combines all the features together with the chrF3 score and the NIST score computed on content lemmas only.", "labels": [], "entities": [{"text": "AutoDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8062348365783691}, {"text": "NIST score", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.7711932063102722}]}, {"text": "Other NIST scores are not included in AutoDA, since they do not bring any improvement.", "labels": [], "entities": [{"text": "NIST", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.5420342087745667}, {"text": "AutoDA", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.875806987285614}]}, {"text": "a metric template, for in principle, any stringbased MT metric can be plugged into it; we used chrF3) in our work.", "labels": [], "entities": []}, {"text": "In TreeAggreg, we are trying to improve an existing string-based metric by applying it in a syntax-tree-based context.", "labels": [], "entities": []}, {"text": "This is motivated by our belief that dependency trees area good means of capturing sentence structure, which maybe relevant for MT evaluation metrics, as the MT output should presumably transfer the information present in the source sentence into a similar syntactic structure as the reference translation uses.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.9063815176486969}]}, {"text": "However, in string-based MT metrics, the syntactic structure of a sentence is typically ignored.", "labels": [], "entities": [{"text": "MT metrics", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9261191785335541}]}, {"text": "In our rather light-weight attempt to employ syntactic analysis in MT evaluation, we segment the sentences into phrases based on their dependency parse trees, and evaluate these phrases independently with the string-based MT metric.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9681975543498993}]}, {"text": "The resulting scores are then aggregated into a final sentence-level score using a simple weighted average.", "labels": [], "entities": []}, {"text": "Our source codes are available online.", "labels": [], "entities": []}, {"text": "We evaluated the model on the WMT16 HUMEseg dataset, but currently it performs poorly.", "labels": [], "entities": [{"text": "WMT16 HUMEseg dataset", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.915587862332662}]}, {"text": "It: Evaluation of NMT Scorer with Pearson correlation to human judgments.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.9193187355995178}]}, {"text": "should be possible to improve it significantly by optimizing the training process for the metrics task (for example by adding another layer that uses the final representations p sand pt to predict human scores and finetune the entire model on some manually evaluated datasets).", "labels": [], "entities": []}, {"text": "The Pearson correlation coefficients to human judgements are shown in Table 6.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8768018782138824}]}], "tableCaptions": [{"text": " Table 3: Selected Czech deep-syntactic features  and their correlation against WMT16 HUMEseg  dataset. Comparison with BLEU, chrF3, and our  trainable AutoDA (using chrF3 as well).", "labels": [], "entities": [{"text": "WMT16 HUMEseg  dataset", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.9021561741828918}, {"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9917358756065369}, {"text": "AutoDA", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.8903610706329346}]}, {"text": " Table 4: Pearson correlations of different sentence-level metrics on WMT16 HUMEseg dataset. Standard  NIST and chrF metrics are compared with our individual features matching. AutoDA combines all the  features together with the chrF3 score and the NIST score computed on content lemmas only. Other  NIST scores are not included in AutoDA, since they do not bring any improvement.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9305770397186279}, {"text": "WMT16 HUMEseg dataset", "start_pos": 70, "end_pos": 91, "type": "DATASET", "confidence": 0.9337299664815267}]}, {"text": " Table 5: Evaluation of TreeAggreg (our metric)  and chrF3 (baseline) with Pearson's correlation to  human judgments.", "labels": [], "entities": []}, {"text": " Table 6: Evaluation of NMT Scorer with Pearson  correlation to human judgments.", "labels": [], "entities": [{"text": "Pearson  correlation", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.9382025897502899}]}]}