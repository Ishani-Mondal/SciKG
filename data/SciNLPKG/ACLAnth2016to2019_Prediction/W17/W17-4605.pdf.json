{"title": [{"text": "Enriching ASR Lattices with POS Tags for Dependency Parsing", "labels": [], "entities": [{"text": "ASR Lattices", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.82500821352005}, {"text": "POS Tags", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8810316026210785}, {"text": "Dependency Parsing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6748157143592834}]}], "abstractContent": [{"text": "Parsing speech requires a richer representation than 1-best or n-best hypotheses, e.g. lattices.", "labels": [], "entities": [{"text": "Parsing speech", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.925640344619751}]}, {"text": "Moreover, previous work shows that part-of-speech (POS) tags area valuable resource for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 88, "end_pos": 95, "type": "TASK", "confidence": 0.9760708808898926}]}, {"text": "In this paper , we therefore explore a joint model-ing approach of automatic speech recognition (ASR) and POS tagging to enrich ASR word lattices.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.7885742882887522}, {"text": "POS tagging", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.7189463526010513}, {"text": "ASR word lattices", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.8730712731679281}]}, {"text": "To that end, we manipulate the ASR process from the pronouncing dictionary onward to use word-POS pairs instead of words.", "labels": [], "entities": [{"text": "ASR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9818275570869446}]}, {"text": "We evaluate ASR, POS tagging and dependency parsing (DP) performance demonstrating a successful lattice-based integration of ASR and POS tagging.", "labels": [], "entities": [{"text": "ASR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9346763491630554}, {"text": "POS tagging and dependency parsing (DP)", "start_pos": 17, "end_pos": 56, "type": "TASK", "confidence": 0.7795052155852318}, {"text": "ASR", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9115102887153625}, {"text": "POS tagging", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.6399632692337036}]}], "introductionContent": [{"text": "Parsing speech is an essential part) of spoken language understanding (SLU) and difficult because spontaneous speech and syntax clash).", "labels": [], "entities": [{"text": "Parsing speech", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8899514973163605}, {"text": "spoken language understanding (SLU)", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.8207667668660482}]}, {"text": "Pipeline approaches concatenating a speech recognizer, a POS tagger and a parser often rely on n-best hypotheses decoded from lattices.", "labels": [], "entities": []}, {"text": "While n-best hypotheses cover more of the hypothesis space than the 1-best hypothesis, they are redundant and incomplete.", "labels": [], "entities": []}, {"text": "Lattices on the other hand are efficiently representing all hypotheses under consideration and therefore allow recovery from more ASR errors.", "labels": [], "entities": [{"text": "ASR", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.9762577414512634}]}, {"text": "Recent work on recurrent neural network architectures with lattices as input ( promises the use of enriched lattices in SLU.", "labels": [], "entities": []}, {"text": "The main contribution of this work is establishing a joint ASR and POS tagging approach using the) toolkit.", "labels": [], "entities": [{"text": "ASR", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9276281595230103}, {"text": "POS tagging", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.6765962392091751}]}, {"text": "To that end, we enrich the ASR word lattices with POS labels for all possible hypotheses on the word level.", "labels": [], "entities": [{"text": "ASR word", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.7880549430847168}]}, {"text": "This enables subsequent natural language processing (NLP) machinery to use these syntactically richer lattices.", "labels": [], "entities": []}, {"text": "We present our proposed method in detail including Kaldi specifics and address problems that occur when data that requires both speech and text information is used.", "labels": [], "entities": []}, {"text": "Our results show a slight but consistent improvement of the joint model throughout the evaluations in ASR, POS tagging and DP performance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.7027600407600403}, {"text": "POS tagging", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.6803145706653595}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. The lmdev  section of the SWBD corpus serves as the LM's  development set and was \"reserved for future use\"", "labels": [], "entities": [{"text": "SWBD corpus", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.694831907749176}]}, {"text": " Table 1: Summary of SWBD data splits. The columns for  utterances, tokens, average tokens per utterance and vocabu- lary depend on the choice of the transcription. These are the  counts for our Treebank-3 transcription.", "labels": [], "entities": [{"text": "SWBD data splits", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.7550760507583618}]}, {"text": " Table 2: PPL and OOVs on lmdev.", "labels": [], "entities": []}, {"text": " Table 5: Parsing results for subsets of correct tokenizations.  Labeled attachment scores (LAS) and unlabeled attachment  scores (UAS) given as percentages. Best scores on the com- mon sets in boldface.", "labels": [], "entities": [{"text": "Labeled attachment scores (LAS)", "start_pos": 65, "end_pos": 96, "type": "METRIC", "confidence": 0.8611256976922353}, {"text": "unlabeled attachment  scores (UAS)", "start_pos": 101, "end_pos": 135, "type": "METRIC", "confidence": 0.7880560755729675}]}, {"text": " Table 6: Parsing results on full dev and eval sets. LAS, UAS,  LS and US are given as percentages. The dev set has 3994  utterances with 44760 tokens and the eval set has 3912 utter- ances with 43277 tokens. Best scores per set in boldface.", "labels": [], "entities": [{"text": "LAS", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9894282221794128}, {"text": "UAS", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.7004806399345398}, {"text": "US", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9334513545036316}]}, {"text": " Table 7: Utterance-based parsing evaluation. The numbers  are counts of utterances where the model in the first column  is better than the other. Column All gives the counts for when  it is better on all four measures.", "labels": [], "entities": [{"text": "Utterance-based parsing evaluation", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.8299413124720255}]}]}