{"title": [], "abstractContent": [{"text": "This paper describes the statistical machine translation system developed at RWTH Aachen University for the English\u2192German and German\u2192English translation tasks of the EMNLP 2017 Second Conference on Machine Translation (WMT 2017).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6174942056337992}, {"text": "English\u2192German and German\u2192English translation tasks of the EMNLP 2017 Second Conference on Machine Translation (WMT 2017)", "start_pos": 108, "end_pos": 229, "type": "TASK", "confidence": 0.756099826910279}]}, {"text": "We use ensembles of attention-based neural machine translation system for both directions.", "labels": [], "entities": [{"text": "attention-based neural machine translation", "start_pos": 20, "end_pos": 62, "type": "TASK", "confidence": 0.6470497772097588}]}, {"text": "We use the provided parallel and synthetic data to train the models.", "labels": [], "entities": []}, {"text": "In addition, we also create a phrasal system using joint translation and reordering models in decoding and neural models in rescoring.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe the statistical machine translation (SMT) systems developed by RWTH Aachen University for the German\u2192English and English\u2192German language pairs of the WMT 2017 evaluation campaign.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.7787552724281946}, {"text": "WMT 2017 evaluation campaign", "start_pos": 162, "end_pos": 190, "type": "DATASET", "confidence": 0.6070571541786194}]}, {"text": "After testing multiple systems and system combinations we submitted an ensemble of multiple NMT networks since it outperformed every tested system combination.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe our data preprocessing.", "labels": [], "entities": []}, {"text": "Section 3 depicts the generation of synthetic data.", "labels": [], "entities": []}, {"text": "Our translation software and baseline setups are explained in Section 4, including the attention-based recurrent neural network ensemble in Subsection 4.1 and phrasal joint translation and reordering (JTR) system in Subsection 4.2.", "labels": [], "entities": [{"text": "phrasal joint translation and reordering (JTR)", "start_pos": 159, "end_pos": 205, "type": "TASK", "confidence": 0.8014266416430473}]}, {"text": "Our experiments for each track are summarized in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have mainly focused on building a strong German\u2192English system and run most experiments on this task.", "labels": [], "entities": []}, {"text": "We used newstest2015 as the development set.", "labels": [], "entities": []}, {"text": "After switching the preprocessing as described in Section 2, we have added the word fertility, which improves the baseline system by about 0.8 BLEU on newstest2016 as shown in.", "labels": [], "entities": [{"text": "fertility", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.767287015914917}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9992800354957581}, {"text": "newstest2016", "start_pos": 151, "end_pos": 163, "type": "DATASET", "confidence": 0.9618719220161438}]}, {"text": "Adding the synthetic data as described in Section 3 gives again of 3.8 BLEU on newstest2016.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9995926022529602}, {"text": "newstest2016", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9680994749069214}]}, {"text": "Changing the number of layers in the decoder from one to two improves the performance by additional 0.8 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9978836178779602}]}, {"text": "Filtering the rapid data corpus by scoring all bilingual sentences with an NMT system trained on all parallel data and removing the sentences with the worst scores improves the system on newstest2016 by 0.4 BLEU, but yield only in a small improvement on newstest2015.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9994285702705383}]}, {"text": "Surprisingly, it even decreases the performance on newstest2017, as observed at a later point in time.", "labels": [], "entities": [{"text": "newstest2017", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9533036947250366}]}, {"text": "Switching from merging the 4 best networks in a training run to continuing the training with an annealing scheme for learning rate reduction for SGD, as described in (, has barely changed the performance on newstest2016.", "labels": [], "entities": [{"text": "newstest2016", "start_pos": 207, "end_pos": 219, "type": "DATASET", "confidence": 0.9634850025177002}]}, {"text": "Nevertheless, we have decided to keep on using it, since it slightly helped on newstest2015.", "labels": [], "entities": [{"text": "newstest2015", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9630056023597717}]}, {"text": "We have used this, without the word fertility, as abase setup to train multiple systems with slightly different settings for an ensemble.", "labels": [], "entities": []}, {"text": "In the first setting we use all LSTM states of the first decoder layer as input for the second decoder layer.", "labels": [], "entities": []}, {"text": "This actually hurts the performance.", "labels": [], "entities": []}, {"text": "Adding the word fertility or the alignment feedback as additional information does not have a large impact.", "labels": [], "entities": []}, {"text": "Note, that the word fertility helpes when it is added to the baseline system -we are not sure why the effect disappears.", "labels": [], "entities": []}, {"text": "Combining systems in one ensemble improves the system again by 1.1 BLEU on newstest2016.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9991880059242249}]}, {"text": "We also combined the NMT system with the strongest phrasal JTR system and a few other combinations as well, but none of them has been able to improve over the NMT ensemble.", "labels": [], "entities": []}, {"text": "We therefore used the NMT system as our final submission.", "labels": [], "entities": [{"text": "NMT system", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.9377823173999786}]}, {"text": "In the table, we can see that using three alignment-based models is comparable to using a single attention-based model.", "labels": [], "entities": []}, {"text": "Note, however, that these models have relatively small LSTM layers of 200 and 350 nodes per layer.", "labels": [], "entities": []}, {"text": "Meanwhile, the attention model uses 1000-node LSTM layers.", "labels": [], "entities": []}, {"text": "When added on top of the alignment-based mix, the attention model only improves the mix slightly.", "labels": [], "entities": []}, {"text": "For the English\u2192German system we have simply used the three best working NMT systems from the German\u2192English setup and combined them in an ensemble.", "labels": [], "entities": []}, {"text": "The word fertility and alignment feedback extensions also did not improve the performance, but the ensemble increased the overall performance by 1 BLEU on newstest2016.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9937699437141418}, {"text": "newstest2016", "start_pos": 155, "end_pos": 167, "type": "DATASET", "confidence": 0.9678632020950317}]}, {"text": "Due to computation time limitations, we did not succeed in building a phrasal JTR system on time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Compares the performance of the preprocessing (PP) optimized for phrase-based systems  (WMT15) or a very simple setup (simple), as described in Section 2 on a PBT and a Neural Machine  Translation (NMT) system.", "labels": [], "entities": [{"text": "Neural Machine  Translation (NMT)", "start_pos": 179, "end_pos": 212, "type": "TASK", "confidence": 0.8499252498149872}]}, {"text": " Table 2: Results of the individual systems for the German\u2192English task. The base system contains  synthetic data, 2-decoder layers, filtered rapid data, and was trained with annealing learning rate instead  of merging. Details are explained in Section 4.1.", "labels": [], "entities": []}, {"text": " Table 3: Results of the individual systems for the German\u2192English task. The system combination  contains the system in line 3, 6, and 7.", "labels": [], "entities": []}, {"text": " Table 4: Results of the individual systems for the English\u2192German task.", "labels": [], "entities": []}]}