{"title": [], "abstractContent": [{"text": "We present a pilot study on parsing non-native texts written by learners of Czech.", "labels": [], "entities": [{"text": "parsing non-native texts written by learners of Czech", "start_pos": 28, "end_pos": 81, "type": "TASK", "confidence": 0.8862549960613251}]}, {"text": "We performed experiments that have shown that at least high-level syntactic functions, like subject, predicate, and object, can be assigned based on a parser trained on standard native language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Texts written by non-native speakers pose a challenge for natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6596090694268545}]}, {"text": "In this paper, we focus on parsing texts written by learners of Czech.", "labels": [], "entities": [{"text": "parsing texts written by learners of Czech", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.8973273294312614}]}, {"text": "There is no syntactically annotated corpus of non-native Czech.", "labels": [], "entities": []}, {"text": "Therefore, we are exploring a question whether it is possible to use the parser trained a traditional newspaper corpus.", "labels": [], "entities": []}, {"text": "In our experiments we use three main components: the Prague Dependency Treebank, the CzeSL corpus, and the maximum-spanning tree parser.", "labels": [], "entities": [{"text": "Prague Dependency Treebank", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.9348151286443075}, {"text": "CzeSL corpus", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.8756139278411865}]}, {"text": "The Prague Dependency Treebank (PDT) 1 is a corpus of newspaper texts with rich linguistic annotation.", "labels": [], "entities": [{"text": "Prague Dependency Treebank (PDT) 1", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.9498459526470729}]}, {"text": "As an illustration, consider the sentence in (1) and the corresponding labeled dependency tree in: (1) R\u00e1no in-the-morning p\u016fjdu I-will-go se with sv\u00b4ymsv\u00b4ym my kamar\u00e1dem friend na houby. mushrooming.", "labels": [], "entities": []}, {"text": "'I will go mushrooming with my friend in the morning.'", "labels": [], "entities": []}, {"text": "The CzeSL corpus includes essays written by non-native speakers of Czech (.", "labels": [], "entities": [{"text": "CzeSL corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7036596685647964}]}, {"text": "Finally, the maximum-spanning tree (MST) parser is a non-projective dependency parser that 1 https://ufal.mff.cuni.cz/pdt3.0 Figure 1: A sample of PDT tree searches for maximum spanning trees over directed graphs).", "labels": [], "entities": []}, {"text": "Given these data and tool components we specify the following initial steps to address our research question: 1.", "labels": [], "entities": []}, {"text": "Create a testing corpus, by annotating CzeSL according to the PDT annotation guidelines 2.", "labels": [], "entities": [{"text": "PDT", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8511550426483154}]}, {"text": "Parse CzeSL by the MST parser trained on PDT or its subset and evaluate its performance", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with two different parsers: (i) a traditional parser trained on PDT (ii) a parser trained on a simpler subset of Czech.", "labels": [], "entities": []}, {"text": "In both cases, we used the MST parser.", "labels": [], "entities": [{"text": "MST", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7608415484428406}]}], "tableCaptions": [{"text": " Table 2: Composition of CzeSL according to  CEFR levels.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.9102136492729187}]}, {"text": " Table 3: We measure the performance of the MST  parser using the following performance measures:  Precision, Recall, and F1 measure.", "labels": [], "entities": [{"text": "MST  parser", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9314801692962646}, {"text": "Precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9983494281768799}, {"text": "Recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9644913077354431}, {"text": "F1 measure", "start_pos": 122, "end_pos": 132, "type": "METRIC", "confidence": 0.9885144233703613}]}]}