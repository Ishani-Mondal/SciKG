{"title": [{"text": "Linguistically Rich Vector Representations of Supertags for TAG Parsing", "labels": [], "entities": [{"text": "TAG Parsing", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.7949123978614807}]}], "abstractContent": [{"text": "In this paper, we explore several techniques for producing vector representations of TAG supertags that can be used as inputs to a neural network-based TAG parser.", "labels": [], "entities": []}, {"text": "In the simplest case, the supertag is encoded as a 1-hot vector that is projected to a dense vector.", "labels": [], "entities": []}, {"text": "Secondly, we use a tree-recursive neural network that is given as input the structure of the elementary tree.", "labels": [], "entities": []}, {"text": "Thirdly, we use hand-crafted feature vectors that describe the syntactic features of each supertag, and project these to a dense vector.", "labels": [], "entities": []}, {"text": "These three representations are learned during the training of a neu-ral network TAG parser with a layer that embeds supertags in a low-dimensional space.", "labels": [], "entities": []}, {"text": "Finally, we consider an embedding that is trained only on patterns of linear co-occurrence among supertags.", "labels": [], "entities": []}, {"text": "By testing the resulting vector representations on the task of completing syntactic analogies, we show that these vector representations capture syntactically relevant information.", "labels": [], "entities": []}, {"text": "While our linguistically-informed embed-dings outperform atomic embeddings on the syntactic analogy task, we find that the same embeddings lead to only a slight improvement on the task of TAG parsing, indicating that the neural parser is able to induce useful representations of supertags from the data alone.", "labels": [], "entities": [{"text": "TAG parsing", "start_pos": 188, "end_pos": 199, "type": "TASK", "confidence": 0.9149587154388428}]}], "introductionContent": [{"text": "Ina Tree Adjoining Grammar (TAG), the set of elementary trees can bethought of as the possible lexical grammatical category assignments, much like the part of speech tags in a Context * Equal Contribution.", "labels": [], "entities": []}, {"text": "Free Grammar (CFG).", "labels": [], "entities": [{"text": "Free Grammar (CFG)", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.6995587468147277}]}, {"text": "However, the number of elementary trees is considerably larger than the category set typically found in other formalisms.", "labels": [], "entities": []}, {"text": "In the TAG that is extracted from the Penn Treebank by, there are more than 4,700 distinct elementary trees, as compared to the 48 POS tags found in the Penn Treebank or even the 1,286 categories found in the Combinatory Categorial Grammar (CCG) bank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9953886270523071}, {"text": "Penn Treebank", "start_pos": 153, "end_pos": 166, "type": "DATASET", "confidence": 0.9949702024459839}]}, {"text": "While this is indeed a large number, the set of elementary trees in a linguistically adequate TAG is finely structured, with systematic relationships holding between elementary trees.", "labels": [], "entities": []}, {"text": "Past work on grammar development in TAG has explored a variety of methods for capturing the relationships among and within so-called tree families, where all members of a tree family have the same basic argument structure (or have the same value for some other syntactic dimension) but differ from each other based on transformations such as passivization or wh-movement.", "labels": [], "entities": []}, {"text": "Under the approach suggested by, the first step of TAG parsing, called supertagging, involves the assignment of elementary trees to lexical items.", "labels": [], "entities": [{"text": "TAG parsing", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9765051603317261}]}, {"text": "Given the high degree of supertag ambiguity and the fact that state-of-the-art TAG supertagging accuracy is only around 90% (using the bi-LISTM supertagger reported in) as compared to 95% for CCG supertagging (, it is useful indeed if the parser can be made sensitive to the relationships between elementary trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9908724427223206}]}, {"text": "Certain errors in the supertagger might then not prove fatal to a parser if a single supertag can be interpreted as related to other elementary trees, providing potentially useful derivational options.", "labels": [], "entities": []}, {"text": "Furthermore, if relations among supertags are encoded, problems of data sparsity during training might be overcome; nearly half of the supertags present in the PTB WSJ Sections 1-22 appear only once, but they maybe related to other supertags that occur more frequently.", "labels": [], "entities": [{"text": "PTB WSJ Sections 1-22", "start_pos": 160, "end_pos": 181, "type": "DATASET", "confidence": 0.9556334316730499}]}, {"text": "Previous work by has proposed away of exploiting relationships among supertags in a transition-based parser, by adding a series of hand-coded linguistic features that characterize properties of the elementary trees in the grammar.", "labels": [], "entities": []}, {"text": "Such features had a beneficial effect on parser performance when used in conjunction with lexical identity, supertag identity, and POS tag.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate how the use of a neural network TAG parsing model, proposed by, facilitates the representation of similarity among supertags.", "labels": [], "entities": [{"text": "TAG parsing", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.6775196343660355}]}, {"text": "The input to this parser is a sequence of 1-best supertags output by a bidirectional LSTM supertagger.", "labels": [], "entities": []}, {"text": "As the first step in computation, the parsing network maps each supertag into a vector via an embedding matrix.", "labels": [], "entities": []}, {"text": "Given a set of supertag vectors, we can study the similarity relations among them using methods similar to those that have been applied to lexical vectors by.", "labels": [], "entities": []}, {"text": "For example, we can consider analogies between elementary trees that correspond to an operation of detransitivization, by asking whether an elementary tree representing a clause headed by a transitive verb (t27) stands in the same relationship to an elementary tree headed by an intransitive verb (t81) that a subject relative clause elementary tree headed by a transitive verb (t99) stands in to a subject relative headed by an intransitive verb (t109).", "labels": [], "entities": []}, {"text": "By interpreting these elementary trees as vectors, we can express this analogy by t27 \u2212 t81 + t109 \u2248 t99.", "labels": [], "entities": []}, {"text": "As we will demonstrate below, this formalization allows us to study the degree to which a representational scheme successfully captures a wide range of linguistic relationships among elementary trees.", "labels": [], "entities": []}, {"text": "Our discussion will compare four alternatives for constructing supertag embeddings.", "labels": [], "entities": []}, {"text": "Three of these are trained in conjunction with parser, and differ only in the representation of the supertag input to the parser: atomic encodings of supertag identity, recursive encoding of the structure of the elementary tree, and the hand-coded linguistic features from.", "labels": [], "entities": []}, {"text": "The fourth derives embeddings via a GloVe-type model of distributional similarity).", "labels": [], "entities": []}, {"text": "Recent work by has proposed a method to embed TAG supertags in the context of natural langauge generation.", "labels": [], "entities": [{"text": "natural langauge generation", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.6127912004788717}]}, {"text": "They utilize structural information of elementary trees through convolutional neural networks.", "labels": [], "entities": []}, {"text": "Our recursive encoding is similar to their approach in that the embedding procedure respects tree structure of supertags.", "labels": [], "entities": []}, {"text": "It should be noted, however, that our induction process runs in the opposite direction from that in.", "labels": [], "entities": []}, {"text": "In their application to natural language generation, the objective is surface realization from (unlabeled) dependency trees, whereas the problem of interest in this paper is derivation of dependency trees from surface realization.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6812481085459391}]}, {"text": "In the next section, we briefly describe the parsing model that is the foundation of our experiments, along with our four methods for constructing supertag embeddings.", "labels": [], "entities": []}, {"text": "Section 3 lays out our experimental set-up and Section 4 explains how we perform evaluation for supertag similarity and for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 124, "end_pos": 131, "type": "TASK", "confidence": 0.9754746556282043}]}, {"text": "Section 5 reports the results and discusses their implications.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments we conducted employ the same experimental setups as.", "labels": [], "entities": []}, {"text": "Specifically, we follow the protocol from and, and use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by.", "labels": [], "entities": [{"text": "TAG-annotated", "start_pos": 75, "end_pos": 88, "type": "METRIC", "confidence": 0.8702172636985779}, {"text": "WSJ Penn Tree Bank", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.8575367629528046}]}, {"text": "We use Sections 01-22 as the training set, Section 00 as the development set, and Section 23 as the test set.", "labels": [], "entities": []}, {"text": "The training, development, and test sets comprise 39832, 1921, and 2415 sentences, respectively.", "labels": [], "entities": [{"text": "1921", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.900215744972229}]}, {"text": "We use a publicly available string representation of supertags to associate each supertag with an elementary tree.", "labels": [], "entities": []}, {"text": "We consider the label of anode in an elementary tree to consist of the part of speech tag as well as the deep argument position and the node type, if relevant.", "labels": [], "entities": []}, {"text": "For example, an NP node marked for substitution with a deep argument position of 0 will be labeled \"NP0s\" and will be considered distinct from, for example, an NP foot node (\"NPf\").", "labels": [], "entities": []}, {"text": "The relation between anode i and a child j (rel(i, j)) is then considered to be the label of i subscripted by the index of j in the ordered list of children of i.", "labels": [], "entities": []}, {"text": "We implement the recursive neural network in TensorFlow and TensorFlow Fold, a library for creating TensorFlow models that take dynamically sized, structure inputs, like trees (.", "labels": [], "entities": []}, {"text": "For the sake of comparison with the results in, we use an embedding size of 50 in all of our experiments and set all of the hyperparameters of the parser to be the same as the best performing ones.", "labels": [], "entities": []}, {"text": "Specifically, we use two fully-connected layers with 200 hidden units each, dropout rates of 0.2 for the input and 0.3 for the hidden layer, and 3 for the stack and buffer scope.", "labels": [], "entities": []}, {"text": "We train stochastically using the Adam optimization algorithm and minibatches of size 100.", "labels": [], "entities": []}, {"text": "We use a publicly available TensorFlow implementation of the GloVe model (available at https://github.com/GradySimon/ tensorflow-glove) training for 50 iterations.", "labels": [], "entities": []}, {"text": "The hyperparameters are the same as those reported in.", "labels": [], "entities": [{"text": "hyperparameters", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9903883337974548}]}, {"text": "We use a context size of 5, which we found performed better than larger context windows.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Analogy task results.", "labels": [], "entities": []}, {"text": " Table 2: Parsing results on the development and test sets. In each cell, shown is the mean of 5 trials with different initialization;  the standard deviation for these values ranges from 0.01 to 0.26.", "labels": [], "entities": []}]}