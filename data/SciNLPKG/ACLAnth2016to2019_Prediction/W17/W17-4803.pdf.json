{"title": [{"text": "Using a Graph-based Coherence Model in Document-Level Machine Translation", "labels": [], "entities": [{"text": "Document-Level Machine Translation", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.6109826266765594}]}], "abstractContent": [{"text": "Although coherence is an important aspect of any text generation system, it has received little attention in the context of machine translation (MT) so far.", "labels": [], "entities": [{"text": "text generation", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7200763821601868}, {"text": "machine translation (MT)", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.8474316596984863}]}, {"text": "We hypothesize that the quality of document-level translation can be improved if MT models take into account the semantic relations among sentences during translation.", "labels": [], "entities": [{"text": "document-level translation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.6900096088647842}]}, {"text": "We integrate the graph-based coherence model proposed by Mesgar and Strube (2016) with Docent 1 (Hardmeier et al., 2012; Hardmeier, 2014) a document-level machine translation system.", "labels": [], "entities": [{"text": "document-level machine translation", "start_pos": 140, "end_pos": 174, "type": "TASK", "confidence": 0.6020107368628184}]}, {"text": "The application of this graph-based coherence mod-eling approach is novel in the context of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.809784322977066}]}, {"text": "We evaluate the coherence model and its effects on the quality of the machine translation.", "labels": [], "entities": []}, {"text": "The result of our experiments shows that our coherence model slightly improves the quality of translation in terms of the average Meteor score.", "labels": [], "entities": [{"text": "Meteor score", "start_pos": 130, "end_pos": 142, "type": "DATASET", "confidence": 0.7554945647716522}]}], "introductionContent": [{"text": "Coherence represents semantic connectivity of texts with regard to grammatical and lexical relations between sentences.", "labels": [], "entities": []}, {"text": "It is an essential part of natural texts and important in establishing structure and meaning of documents as a whole.", "labels": [], "entities": []}, {"text": "It is crucial for any text generation system to generate coherent texts.", "labels": [], "entities": [{"text": "text generation", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7077135741710663}]}, {"text": "For instance in real machine translation systems, we desire to translate a document, which consists of several sentences, from a source language to a target language.", "labels": [], "entities": []}, {"text": "Current machine translation systems (as an instance of text generation systems) mostly focus on the 1 https://github.com/chardmeier/docent sentence-level translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.744687408208847}, {"text": "text generation", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.6956448704004288}, {"text": "sentence-level translation", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.7467665672302246}]}, {"text": "Indeed, the state-ofthe-art machine translation models perform well on sentence-level translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7343002557754517}, {"text": "sentence-level translation", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7225914895534515}]}, {"text": "However, it is insufficient to just sequentially and independently translate sentences of the source document and concatenate them as the translated version.", "labels": [], "entities": []}, {"text": "The translated sentences should be coherently connected to each other in the target document as well.", "labels": [], "entities": []}, {"text": "From a linguistic point of view also the discourse-wide context must betaken into account to have a high-quality translation.", "labels": [], "entities": []}, {"text": "The current paradigm of machine translation needs to be improved as it does not consider any discourse coherence phenomena that establish a text's connectedness.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7731963098049164}]}, {"text": "One of the active research topics in modeling coherence focuses on entity connections over sentences based on Centering Theory (.", "labels": [], "entities": []}, {"text": "Previous research on coherence modeling shows its application mainly in readability assessment (.", "labels": [], "entities": [{"text": "coherence modeling", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.72359299659729}, {"text": "readability assessment", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.810417503118515}]}, {"text": "Recently, showed that the graph-based coherence model can be utilized to generate more coherent summaries of scientific articles.", "labels": [], "entities": []}, {"text": "The main goal of this paper is to integrate coherence features with a statistical machine translation system to improve the quality of the output translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6060775617758433}]}, {"text": "To achieve this goal, we combine the graph-based coherence representation by Guinaudeau and and its extensions into the documentlevel machine translation decoder Docent (.", "labels": [], "entities": [{"text": "documentlevel machine translation decoder Docent", "start_pos": 120, "end_pos": 168, "type": "TASK", "confidence": 0.5601492881774902}]}, {"text": "Docent defines an initial translation of the source document and modifies the translation of sentences aiming to maximize an objective function.", "labels": [], "entities": []}, {"text": "This function measures the quality of the S1: But the noise didn't disappear.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the WMT 2015 (Bojar et al., 2015) dataset for training and development of the sentence-level translation and language models 4 , and the DiscoMT 2015 Shared Task (Hardmeier et al., 2015) dataset for mining subgraphs (coherence patterns) and as our test data.", "labels": [], "entities": [{"text": "WMT 2015 (Bojar et al., 2015) dataset", "start_pos": 11, "end_pos": 48, "type": "DATASET", "confidence": 0.782562530040741}, {"text": "sentence-level translation", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.6616625785827637}, {"text": "DiscoMT 2015 Shared Task (Hardmeier et al., 2015) dataset", "start_pos": 144, "end_pos": 201, "type": "DATASET", "confidence": 0.7457525332768759}]}, {"text": "We run experiments on the language pair French-English.", "labels": [], "entities": []}, {"text": "Coherence patterns are extracted from the 1551 DiscoMT training documents using GloVe word embeddings.", "labels": [], "entities": [{"text": "1551 DiscoMT training documents", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.7242132127285004}]}, {"text": "We extract all k-node subgraphs fork \u2208 {3, 4, 5} using GASTON 5 ().", "labels": [], "entities": [{"text": "GASTON 5", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.6605636179447174}]}, {"text": "We use Moses to translate sentences independently and initialize the translation state in Docent.", "labels": [], "entities": []}, {"text": "We train our systems using the Moses decoder (. After standard preprocessing of the data, we train a 3-gram language model using).", "labels": [], "entities": []}, {"text": "We use the MGIZA++ () word aligner and employ standard grow-diag-fast-and symmetrization.", "labels": [], "entities": [{"text": "MGIZA", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8423912525177002}]}, {"text": "Tuning is done on the development data via minimum error rate training.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9748904705047607}]}, {"text": "After training the language model and creating the phrase table with Moses, we use these to initialize our translation systems.", "labels": [], "entities": []}, {"text": "We use the lcurvedocent binary of Docent, which outputs Docent's learning curve, i.e., files for the intermediate decoding states.", "labels": [], "entities": []}, {"text": "This additionally allows us to investigate the learning curves with regard to how our coherence feature behaves overtime.", "labels": [], "entities": []}, {"text": "We prune the translation table by only retaining all phrase translations with a probability greater than 0.0001 during training.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9610718488693237}]}, {"text": "In our configuration file for Docent, we set to use the simulated annealing algorithm with a maximum number of 16,384 steps 6 and the following features: geometric distortion model, word penalty cost, OOVpenalty cost, phrase table, and the 3-gram language model.", "labels": [], "entities": [{"text": "OOVpenalty cost", "start_pos": 201, "end_pos": 216, "type": "METRIC", "confidence": 0.9614611566066742}]}, {"text": "We follow the standard machine translation procedure of evaluation, measuring BLEU () for every system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7106381356716156}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9986636638641357}]}, {"text": "BLEU is an ngram based co-occurrence metric that operates with modified n-gram precision scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9726488590240479}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.874826967716217}]}, {"text": "The document n-gram precision scores are averaged using the geometric mean of these scores with n-grams up to length N and positive weights summing to one.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.976588249206543}]}, {"text": "The result is multiplied by an exponential brevity penalty factor that penalizes a translation if it does not match the reference translations in length, word choice, and word order.", "labels": [], "entities": []}, {"text": "We also calculate Meteor () as it is a widely used evaluation metric as well.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.7863584160804749}]}, {"text": "In contrast to BLEU, Meteor is a word-based metric that takes recall into account as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9968087077140808}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9954081177711487}]}, {"text": "Meteor creates a word alignment between a pair of strings that is incrementally produced using a sequence of various wordmapping modules, including the exact module, the Porter stem module, and the WordNet synonymy module (.", "labels": [], "entities": []}, {"text": "Because Meteor has been shown to have a higher correlation with human judgements than BLEU (), it is a useful alternative evaluation metric for our purposes.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.7571114897727966}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9984513521194458}]}, {"text": "As it also considers stemmed words and information from WordNet to determine synonymous words between a candidate and a reference translation, the metric is interesting with regard to surface variation with the same semantic content and how this affects the evaluation of our coherence model (as its graph construction is semantically grounded).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9427162408828735}]}], "tableCaptions": [{"text": " Table 2: Statistics on the datasets used. train is  the news commentary v10 corpus, dev is the 2012  newstest development data, and test is the Dis- coMT 2015 test data. The number (#) of tokens  corresponds to the English (target) side.", "labels": [], "entities": [{"text": "news commentary v10 corpus", "start_pos": 57, "end_pos": 83, "type": "DATASET", "confidence": 0.8253104388713837}, {"text": "newstest development data", "start_pos": 102, "end_pos": 127, "type": "DATASET", "confidence": 0.772967537244161}, {"text": "Dis- coMT 2015 test data", "start_pos": 145, "end_pos": 169, "type": "DATASET", "confidence": 0.5924326380093893}]}, {"text": " Table 3: Results of the coherence model (CM) compared to the baseline (BL) on the DiscoMT test set  (highest values are marked in bold). The scores of the entity graph model using average outdegree as  coherence feature are identical to the baseline model. The differences are not statistically significant  (p = 0.05) using Student's t-test", "labels": [], "entities": [{"text": "BL", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.8664088845252991}, {"text": "DiscoMT test set", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.9652292331059774}, {"text": "Student's t-test", "start_pos": 326, "end_pos": 342, "type": "DATASET", "confidence": 0.8499255379041036}]}, {"text": " Table 4: Comparison of the number of accepted  change-phrase-translation operations.", "labels": [], "entities": []}]}