{"title": [{"text": "Beyond Word2Vec: Embedding Words and Phrases in Same Vector Space", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings are being used for several linguistic problems and NLP tasks.", "labels": [], "entities": []}, {"text": "Improvements in solutions to such problems are great because of the recent breakthroughs in vector representation of words and research in vector space models.", "labels": [], "entities": [{"text": "vector representation of words", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.8135513663291931}]}, {"text": "However , vector embeddings of phrases keeping semantics intact with words has been challenging.", "labels": [], "entities": []}, {"text": "We propose a novel methodology using Siamese deep neural networks to embed multi-word units and fine-tune the current state-of-the-art word embed-dings keeping both in the same vector space.", "labels": [], "entities": []}, {"text": "We show several semantic relations between words and phrases using the embeddings generated by our system and evaluate that the similarity of words and their corresponding paraphrases are maximized using the modified embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector embeddings in computational linguistics is a model that encodes words in a vector space.", "labels": [], "entities": []}, {"text": "These vector encodings are used in mathematical models and serve as abase for computation in NLP.", "labels": [], "entities": []}, {"text": "Development of word embedding technique started in 2000 when Bengio et al. built neural probabilistic language models to reduce the high dimensionality of word representations in contexts by learning a distributed representation for words (.", "labels": [], "entities": [{"text": "word embedding", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7635316550731659}]}, {"text": "After that, continuous research has been done in the field resulting in remarkable improvements in word vector representations as well as the methods of learning the embeddings ().", "labels": [], "entities": [{"text": "word vector representations", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6887519359588623}]}, {"text": "The primary reason for the increase in quality and performance of word vector embeddings is the huge growth of data and and development in computational capabilities as of today.", "labels": [], "entities": []}, {"text": "Natural language has both single word and multi-word units.", "labels": [], "entities": []}, {"text": "If we want vector semantics to be near perfect, we need to embed multi-word units with the same quality as we do with the single word units.", "labels": [], "entities": []}, {"text": "Improvements in phrase representation will eventually help the areas of question answering, search and translation.", "labels": [], "entities": [{"text": "phrase representation", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.8845685720443726}, {"text": "question answering", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.877539873123169}]}, {"text": "For a phrase that is similar to a certain word, the embedding of both the word and phrase should be similar and should lie in the same space.", "labels": [], "entities": []}, {"text": "Only then a manipulation on a word and its paraphrase embedding can prove them to be similar.", "labels": [], "entities": []}, {"text": "Currently, compositional models are used to build phrase embeddings with less emphasis on building the compositions using deep learning and more using specific composition functions.", "labels": [], "entities": []}, {"text": "Our major contribution in this work is employing deep neural architectures to transform constituent word embeddings of a multi-word units into its vector representation.", "labels": [], "entities": []}, {"text": "We build a Siamese deep neural network architecture (Siamese LSTM, to be precise) that accepts two inputs, one being a word while another a phrase.", "labels": [], "entities": []}, {"text": "The energy function in the Siamese network measures the similarity between these two input units.", "labels": [], "entities": []}, {"text": "In the course of training the network, baseline word embeddings (Section 5.2) are modified and phrase embeddings are generated.", "labels": [], "entities": []}, {"text": "We describe the model in detail in further sections.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use PPDB dataset ( of size XL which has 3,76,000 pairs of words and their corresponding paraphrases.", "labels": [], "entities": [{"text": "PPDB dataset", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9414038956165314}]}, {"text": "Since these are word-paraphrase pairs, we label the output of these pairs as 0.", "labels": [], "entities": []}, {"text": "We augment the data by the same number of negative pairs by choosing a phrase fora word which is not its paraphrase.", "labels": [], "entities": []}, {"text": "We label these pairs as 1.", "labels": [], "entities": []}, {"text": "Thus, we train our model on 7,52,000 data samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Nearest words for a given word showing  that semantic relations are preserved even after the  modification in the base embeddings.", "labels": [], "entities": []}, {"text": " Table 5: Experiments in this category; word is 1- gram and phrase is n-gram where n \u2265 2", "labels": [], "entities": []}, {"text": " Table 7: Performance on the SemEval2013 5(a)  Semantic Similarity Task", "labels": [], "entities": [{"text": "SemEval2013 5(a)  Semantic Similarity Task", "start_pos": 29, "end_pos": 71, "type": "TASK", "confidence": 0.6989749744534492}]}]}