{"title": [{"text": "Analysing Errors of Open Information Extraction Systems", "labels": [], "entities": [{"text": "Analysing Errors of Open Information Extraction", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7787398795286814}]}], "abstractContent": [{"text": "We report results on benchmarking Open Information Extraction (OIE) systems using RelVis, a toolkit for benchmark-ing Open Information Extraction systems.", "labels": [], "entities": [{"text": "benchmarking Open Information Extraction (OIE)", "start_pos": 21, "end_pos": 67, "type": "TASK", "confidence": 0.7940378955432347}, {"text": "benchmark-ing Open Information Extraction", "start_pos": 104, "end_pos": 145, "type": "TASK", "confidence": 0.7388451844453812}]}, {"text": "Our comprehensive benchmark contains three data sets from the news domain and one data set from Wikipedia with overall 4522 labeled sentences and 11243 binary or n-ary OIE relations.", "labels": [], "entities": []}, {"text": "In our analysis on these data sets we compared the performance of four popular OIE systems, ClausIE, OpenIE 4.2, Stanford OpenIE and PredPatt.", "labels": [], "entities": [{"text": "ClausIE", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9663988947868347}, {"text": "PredPatt", "start_pos": 133, "end_pos": 141, "type": "DATASET", "confidence": 0.9118116497993469}]}, {"text": "In addition, we evaluated the impact of five common error classes on a subset of 749 n-ary tuples.", "labels": [], "entities": []}, {"text": "From our deep analysis we unreveal important research directions fora next generation of OIE systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open Information Extraction (OIE) is an important intermediate step of the nlp stack for many text mining tasks, such as summarization, relation extraction, knowledge base construction and question answering.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7492052664359411}, {"text": "text mining", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.7371975779533386}, {"text": "summarization", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.9808957576751709}, {"text": "relation extraction", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.8449923098087311}, {"text": "knowledge base construction", "start_pos": 157, "end_pos": 184, "type": "TASK", "confidence": 0.659442126750946}, {"text": "question answering", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.8977262377738953}]}, {"text": "OIE systems are designed for extracting n-ary tuples from diverse and large amounts of text, without being restricted to a fixed schema or domain.", "labels": [], "entities": []}, {"text": "These tuples consist of one predicate and n arguments e.g: flew(Obama; from Berlin; to New York).", "labels": [], "entities": []}, {"text": "Users often desire to select a suitable OIE system for their specific application domain.", "labels": [], "entities": []}, {"text": "Making the right choice is not an easy task.", "labels": [], "entities": []}, {"text": "Unfortunately, there is surprisingly little work on evaluating and comparing results among different OIE systems.", "labels": [], "entities": []}, {"text": "Worse, most OIE methods utilize proprietary and unpublished data sets.", "labels": [], "entities": []}, {"text": "In most cases users can only rely on publications and need to download, compile and apply existing systems to their own data sets.", "labels": [], "entities": []}, {"text": "Contribution Ideally, one could compare different OIE systems with a unified benchmarking suite.", "labels": [], "entities": []}, {"text": "As a result, the user could identify \"sweet spots\" of each system but also weaknesses for common error classes.", "labels": [], "entities": []}, {"text": "The benchmarking suite should feature a diverse set of gold annotations with several thousands of annotated sentences.", "labels": [], "entities": []}, {"text": "By exploring results and errors, the user can learn how to design the next generation of OIE systems or how to combine several systems into an ensemble.", "labels": [], "entities": []}, {"text": "Our contributions are: (1) We report results of a quantitative analysis on four commonly used OIE systems: STANFORD OPENIE (SIE)), OPENIE 4.2 (OIE) 1 , CLAUSIE (CIE) or PRED-PAT (PP) (.", "labels": [], "entities": [{"text": "STANFORD OPENIE", "start_pos": 107, "end_pos": 122, "type": "METRIC", "confidence": 0.48803186416625977}]}, {"text": "Which employ rule based as well as machine learning based methods on linguistic structures like dependency parses.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7138673514127731}]}, {"text": "These were applied on 4522 sentences and 11243 n-ary gold standard tuples.", "labels": [], "entities": []}, {"text": "(2) We share in-depth insights on a qualitative error analysis of 749 nary tuples in 68 sentences from four gold standard data sets annotated by all four OIE systems.", "labels": [], "entities": [{"text": "OIE systems", "start_pos": 154, "end_pos": 165, "type": "DATASET", "confidence": 0.9091562032699585}]}, {"text": "(3) We provide an integrated benchmark for OIE systems consisting of three news data sets NYT-222, WEB-500 (, PENN-100 () and a large OIE benchmark from Newswire and Wikipedia) combined in our evaluation tool RelVis.", "labels": [], "entities": [{"text": "NYT-222", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.5440041422843933}, {"text": "WEB-500", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.7247552871704102}, {"text": "PENN-100", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.70250403881073}, {"text": "RelVis", "start_pos": 209, "end_pos": 215, "type": "DATASET", "confidence": 0.9387649893760681}]}, {"text": "Our benchmark tool will be provided to the community under an open source license.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows: First, Section 2 gives detailed insights on methods used for qualitative and quantitative evaluation.", "labels": [], "entities": []}, {"text": "Section 3 introduces our evaluation system Name Type Domain Sent.", "labels": [], "entities": [{"text": "Name Type Domain Sent", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7048803120851517}]}], "datasetContent": [{"text": "Ina quantitative evaluation we report precision, recall and F 2 scores on all four data sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9997333884239197}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9995905756950378}, {"text": "F 2 scores", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9857514301935831}]}, {"text": "reports overall results for four OIE systems on all four data sets, with the limitation that only a subset of OIE2016, containing 1768 sentences, was available to us.", "labels": [], "entities": [{"text": "OIE", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.807826578617096}, {"text": "OIE2016", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9600017070770264}]}, {"text": "We conduct our experiments with an exact (a) and relaxed (b) containment match strategy.", "labels": [], "entities": []}, {"text": "For the qualitative evaluation we execute four OIE systems on 17 sentences of each data set.", "labels": [], "entities": []}, {"text": "This resulted in 749 predicted extractions which we evaluate and classify into error categories by two human judges, as shown in.", "labels": [], "entities": []}, {"text": "Additionally, gives an overview of the general performance of all tools overall data sets.", "labels": [], "entities": []}, {"text": "We apply a strict containment match strategy in this evaluation.", "labels": [], "entities": []}, {"text": "Observing that multiple errors can happen to a single extraction, we assign in these cases more than one error category.", "labels": [], "entities": []}, {"text": "Note, for both experiments, we configure system CIE to binary extraction mode for binary data sets and otherwise in n-ary mode.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sets in RelVis", "labels": [], "entities": [{"text": "RelVis", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.8511744737625122}]}, {"text": " Table 2: Quantitative Evaluation. The (b) variant are results with relaxed containment match strategy  and (a) are those with the strict containment strategy.", "labels": [], "entities": [{"text": "Quantitative Evaluation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8537039756774902}]}]}