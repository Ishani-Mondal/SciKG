{"title": [{"text": "Unsupervised Aspect Term Extraction with B-LSTM & CRF using Automatically Labelled Datasets", "labels": [], "entities": [{"text": "Unsupervised Aspect Term Extraction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6204118356108665}]}], "abstractContent": [{"text": "Aspect Term Extraction (ATE) identifies opinionated aspect terms in texts and is one of the tasks in the SemEval Aspect Based Sentiment Analysis (ABSA) contest.", "labels": [], "entities": [{"text": "Aspect Term Extraction (ATE) identifies opinionated aspect terms in texts", "start_pos": 0, "end_pos": 73, "type": "TASK", "confidence": 0.8411330680052439}, {"text": "SemEval Aspect Based Sentiment Analysis (ABSA) contest", "start_pos": 105, "end_pos": 159, "type": "TASK", "confidence": 0.8908380336231656}]}, {"text": "The small amount of available datasets for supervised ATE and the costly human annotation for aspect term labelling give rise to the need for unsu-pervised ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9596231579780579}, {"text": "aspect term labelling", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.6477320392926534}]}, {"text": "In this paper, we introduce an architecture that achieves top-ranking performance for supervised ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9153021574020386}]}, {"text": "Moreover , it can be used efficiently as feature extractor and classifier for unsuper-vised ATE.", "labels": [], "entities": []}, {"text": "Our second contribution is a method to automatically construct datasets for ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.6504973769187927}]}, {"text": "We train a classifier on our automatically labelled datasets and evaluate it on the human annotated SemEval ABSA test sets.", "labels": [], "entities": [{"text": "SemEval ABSA test sets", "start_pos": 100, "end_pos": 122, "type": "DATASET", "confidence": 0.7767857536673546}]}, {"text": "Compared to a strong rule-based baseline, we obtain a dramatically higher F-score and attain precision values above 80%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.99928218126297}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9996449947357178}]}, {"text": "Our unsupervised method beats the supervised ABSA baseline from SemEval, while preserving high precision scores.", "labels": [], "entities": [{"text": "ABSA baseline", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.5302418619394302}, {"text": "precision scores", "start_pos": 95, "end_pos": 111, "type": "METRIC", "confidence": 0.9710006415843964}]}], "introductionContent": [{"text": "For many years now, companies are offering users the possibility of adding reviews in the form of sentences or small paragraphs.", "labels": [], "entities": []}, {"text": "Reviews can be beneficial for both customers and companies.", "labels": [], "entities": []}, {"text": "On the one hand, people can make better decisions by getting insights about available products and solutions.", "labels": [], "entities": []}, {"text": "One the other hand, companies are interested in understanding how and what customers think about their products, which helps in employing marketing solutions and correction strategies.", "labels": [], "entities": []}, {"text": "To this end, performing an automated analysis of the user opinions becomes a crucial issue.", "labels": [], "entities": []}, {"text": "Performing sentiment analysis to detect the overall polarity of a sentence or paragraph comes with two major disadvantages.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9382567405700684}]}, {"text": "First, sentiment analysis on sentence (or paragraph) level does not fulfill the purpose of getting more accurate and precise information.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.9709210693836212}]}, {"text": "The polarity refers to a broader context, instead of pinpointing specific targets.", "labels": [], "entities": []}, {"text": "Secondly, many sentences or paragraphs contain opposing polarities towards distinct targets, making it impossible to assign an accurate overall polarity.", "labels": [], "entities": []}, {"text": "The need for identifying aspect terms and their respective polarity gave rise to the Aspect Based Sentiment Analysis (ABSA), where the task is first to extract aspects or features of an entity (i.e. Aspect Term Extraction or ATE 1 ) from a given text, and second to determine the sentiment polarity (SP), if any, towards each aspect of that entity.", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis (ABSA)", "start_pos": 85, "end_pos": 123, "type": "TASK", "confidence": 0.7341291563851493}, {"text": "sentiment polarity (SP)", "start_pos": 280, "end_pos": 303, "type": "METRIC", "confidence": 0.6478953838348389}]}, {"text": "The importance of ABSA led to the creation of the ABSA task in the SemEval 2 contest in 2014).", "labels": [], "entities": [{"text": "SemEval 2 contest", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.5829545855522156}]}, {"text": "Supervised ATE using human annotated datasets leads to high performance for aspect term detection on unseen data, however it has two major drawbacks.", "labels": [], "entities": [{"text": "ATE", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9273625016212463}, {"text": "aspect term detection", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7117637197176615}]}, {"text": "First, the size of the labelled datasets is quite small, reducing the performance of the classifiers.", "labels": [], "entities": []}, {"text": "Second, human annotation is a very slow and costly procedure.", "labels": [], "entities": [{"text": "human annotation", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7848245203495026}]}, {"text": "The drawbacks of supervised ATE can be tackled using unsupervised ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.956334114074707}]}, {"text": "The size of the datasets can be significantly increased using targets from publicly available reviews (e.g. Amazon or Yelp).", "labels": [], "entities": [{"text": "Yelp", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.8389025926589966}]}, {"text": "Reviews are opinion texts and contain plenty of opinionated aspect terms, which makes them perfect candidates for constructing new datasets for ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.7260093688964844}]}, {"text": "With respect to the second drawback, an au-tomated data labelling process with high precision can replace the slow and error-prone human annotation procedure.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.987652063369751}]}, {"text": "We innovate by performing ATE starting from opinion texts (e.g. reviews).", "labels": [], "entities": [{"text": "ATE", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8039681911468506}]}, {"text": "This is a completely unsupervised task since there are no labels to explicitly pinpoint that certain tokens of the text are aspect terms.", "labels": [], "entities": []}, {"text": "Reviews may contain labels (e.g. number of stars in a 1-5 star rating system) which are related to their overall polarity.", "labels": [], "entities": []}, {"text": "However, such labels are not useful for ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9390645027160645}]}, {"text": "In this work, we present a classifier, which can be used for feature extraction and aspect term detection for both unsupervised and supervised ATE.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.772283524274826}, {"text": "aspect term detection", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.669210821390152}]}, {"text": "We validate its suitability for ATE by achieving top-ranking results for supervised ATE using the SemEval-2014 ABSA task datasets . Then, we use it for unsupervised ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9767401814460754}, {"text": "SemEval-2014 ABSA task datasets", "start_pos": 98, "end_pos": 129, "type": "DATASET", "confidence": 0.6846419423818588}, {"text": "ATE", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.9512404203414917}]}, {"text": "Moreover, we contribute by introducing anew, completely automated, unsupervised and domain independent method for annotating raw opinion texts.", "labels": [], "entities": []}, {"text": "Then, we use our classifier to perform unsupervised ATE by training it on the automatically labelled datasets obtained with our method.", "labels": [], "entities": [{"text": "ATE", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.6346895694732666}]}, {"text": "Against all expectations, our unsupervised method beats the supervised ABSA baseline from SemEval-2014, while achieving high precision scores.", "labels": [], "entities": [{"text": "ABSA baseline from SemEval-2014", "start_pos": 71, "end_pos": 102, "type": "DATASET", "confidence": 0.7772419601678848}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9982686042785645}]}, {"text": "The latter is very important for unsupervised techniques since we wish to extract nonnoisy aspect terms, i.e. minimize the number of false positives.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the related work for ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.7736959457397461}]}, {"text": "Our approach for supervised and unsupervised ATE is described in Sections 3 and 4 respectively.", "labels": [], "entities": [{"text": "ATE", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8801674842834473}]}, {"text": "Section 5 presents our experiments and results for both supervised and unsupervised ATE.", "labels": [], "entities": [{"text": "ATE", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.783268392086029}]}, {"text": "Finally, Section 6 focuses on our conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train a B-LSTM & CRF classifier to perform unsupervised ATE for both domains using the automatically labelled datasets constructed in Section 4.1.", "labels": [], "entities": [{"text": "B-LSTM", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9467500448226929}, {"text": "ATE", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9068016409873962}]}, {"text": "The classifier is evaluated on the human annotated test datasets of the SemEval-2014 ABSA contest.", "labels": [], "entities": [{"text": "SemEval-2014 ABSA contest", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.5634800791740417}]}, {"text": "We perform experiments for supervised and unsupervised ATE in the laptop and the restaurant domain and evaluate our classifier using the CoNLL 12 F-score.", "labels": [], "entities": [{"text": "CoNLL 12 F-score", "start_pos": 137, "end_pos": 153, "type": "METRIC", "confidence": 0.6558865904808044}]}, {"text": "Compared to other supervised learning methods, we reach the performance of SemEval-2014 ABSA winners in the restaurant domain.", "labels": [], "entities": [{"text": "SemEval-2014 ABSA", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.5187222510576248}]}, {"text": "For laptops, our supervised system exceeds the best F-score of the SemEval-2014 ABSA contest by approximately 3%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9982616305351257}, {"text": "SemEval-2014 ABSA contest", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6106278498967489}]}, {"text": "With respect to unsupervised ATE, our technique achieves (i) very high precision and (ii) an F-score that exceeds the supervised baseline of the SemEval ABSA.", "labels": [], "entities": [{"text": "ATE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.5497816205024719}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9993671774864197}, {"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9988600015640259}, {"text": "SemEval ABSA", "start_pos": 145, "end_pos": 157, "type": "TASK", "confidence": 0.7040504515171051}]}, {"text": "For supervised learning, we perform experiments using the human annotated training and test sets provided by the SemEval-2014 ABSA contest for We use a random 80-20% split on the original training set of SemEval-2014 ABSA contest in order to create anew train and validation set.", "labels": [], "entities": [{"text": "SemEval-2014 ABSA contest", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.48730459809303284}, {"text": "SemEval-2014 ABSA contest", "start_pos": 204, "end_pos": 229, "type": "DATASET", "confidence": 0.6337512334187826}]}, {"text": "We keep the test set for our final evaluation.", "labels": [], "entities": []}, {"text": "For most of the parameters, we use the default values of).", "labels": [], "entities": []}, {"text": "However, we use the adam optimizer with learning rate \u03b1 = 0.01 and a batch size of 64.", "labels": [], "entities": [{"text": "learning rate \u03b1", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.9304322600364685}]}, {"text": "Moreover, we use the pre-trained word embeddings of fastText.", "labels": [], "entities": []}, {"text": "We train the classifier using the reduced training set fora maximum number of 100 epochs.", "labels": [], "entities": []}, {"text": "After each epoch, we evaluate our model using the CoNLL F-score on the validation set.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.6126393675804138}, {"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.49735763669013977}]}, {"text": "Moreover, we use early stopping with a patience of 20 epochs.", "labels": [], "entities": []}, {"text": "This means that the experiment terminates earlier if the CoNLL F-score of the validation set does not improve for 20 consecutive epochs.", "labels": [], "entities": [{"text": "CoNLL F-score", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.6826634407043457}]}, {"text": "At the end of each experiment we choose the model of the epoch that gives the best performance on the validation set and make predictions on the test set.", "labels": [], "entities": []}, {"text": "We repeat the aforementioned procedure for 50 experiments and present the experimental results for both domains in The F-score of the SemEval-2014 ABSA winners is 74.55 and 84.01 for the laptop and the restaurant domain respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 119, "end_pos": 126, "type": "METRIC", "confidence": 0.9992647767066956}, {"text": "SemEval-2014 ABSA winners", "start_pos": 134, "end_pos": 159, "type": "DATASET", "confidence": 0.6796533167362213}]}, {"text": "The B-LSTM & CRF classifier achieves an F-score of 77.96 \u00b1 0.38 for laptops and an F-score of 84.12 \u00b1 0.2 for restaurants with a confidence interval of 95%.", "labels": [], "entities": [{"text": "B-LSTM", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9662392735481262}, {"text": "F-score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9993564486503601}, {"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9991132616996765}]}, {"text": "With our performance, we would have surely won in the laptop domain and probably also in the restaurant domain.", "labels": [], "entities": []}, {"text": "We also perform experiments for ATE with unsupervised learning.", "labels": [], "entities": [{"text": "ATE", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9774622321128845}]}, {"text": "For training, we use the automatically labelled datasets (hereafter denoted as ALD) obtained using the methodology described in Section 4.1 with q th = 0.7 and q th = 0.6 for the laptop and the restaurant domain respectively.", "labels": [], "entities": []}, {"text": "For testing, we use the human labelled datasets (hereafter denoted as HLD) of the SemEval-2014 ABSA task.", "labels": [], "entities": [{"text": "SemEval-2014 ABSA task", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7059415578842163}]}, {"text": "Our main goal is to evaluate our unsupervised technique on human annotated datasets.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, the largest available human annotated datasets for ATE are provided by the SemEval ABSA task and contain laptop and restaurant reviews.", "labels": [], "entities": [{"text": "ATE", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9194822907447815}, {"text": "SemEval ABSA task", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.632463534673055}]}, {"text": "Therefore, our analysis focuses only on these two domains.", "labels": [], "entities": []}, {"text": "We start by creating a rule-based baseline model to make predictions for the HLD simply by applying techniques presented in Section 4.1.", "labels": [], "entities": [{"text": "HLD", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.8947442173957825}]}, {"text": "This baseline (presented in the following section) does not rely on any machine learning techniques for the annotation procedure.", "labels": [], "entities": []}, {"text": "We aim at beating the rule-based baseline by using machine learning.", "labels": [], "entities": []}, {"text": "To this end, we use the ALD to train our classifier.", "labels": [], "entities": []}, {"text": "For unsupervised ATE, we run two types of experiments.", "labels": [], "entities": [{"text": "ATE", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.871155858039856}]}, {"text": "The first one uses the traditional IOB labelling format and is stricter.", "labels": [], "entities": [{"text": "IOB labelling", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.43778327107429504}]}, {"text": "The second one is more relaxed and uses only B and O labels (i.e. I labels are converted to B).", "labels": [], "entities": []}, {"text": "The intuition is that aspect terms can be identified by separating B and I labels from O.", "labels": [], "entities": []}, {"text": "Therefore, I and B labels are treated equally against O.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experiments for unsupervised ATE. We  compare B-LSTM & CRF classifier against the  rule-based baseline, an SVM classifier and the  baseline of the SemEval-2014 ABSA contest.", "labels": [], "entities": [{"text": "B-LSTM", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.962209939956665}, {"text": "SemEval-2014 ABSA contest", "start_pos": 157, "end_pos": 182, "type": "TASK", "confidence": 0.5976700782775879}]}]}