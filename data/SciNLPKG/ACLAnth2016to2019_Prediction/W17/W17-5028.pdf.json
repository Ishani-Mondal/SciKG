{"title": [{"text": "Fewer features perform well at Native Language Identification task", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.7168577512105306}]}], "abstractContent": [{"text": "This paper describes our results at the NLI shared task 2017.", "labels": [], "entities": [{"text": "NLI shared task 2017", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.6985829770565033}]}, {"text": "We participated in essays , speech, and fusion task that uses text, speech, and i-vectors for the task of identifying the native language of the given input.", "labels": [], "entities": []}, {"text": "In the essay track, a linear SVM system using word bigrams and character 7-grams performed the best.", "labels": [], "entities": []}, {"text": "In the speech track, an LDA classifier based only on i-vectors performed better than a combination system using text features from speech transcriptions and i-vectors.", "labels": [], "entities": []}, {"text": "In the fusion task, we experimented with systems that used combination of i-vectors with higher order n-grams features, combination of i-vectors with word unigrams, a mean probability ensemble, and a stacked ensemble system.", "labels": [], "entities": []}, {"text": "Our finding is that word unigrams in combination with i-vectors achieve higher score than systems trained with larger number of n-gram features.", "labels": [], "entities": [{"text": "score", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9776324033737183}]}, {"text": "Our best-performing systems achieved F1-scores of 87.16 %, 83.33 % and 91.75 % on the essay track, the speech track and the fusion track respectively.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9995456337928772}]}], "introductionContent": [{"text": "In this paper, we describe our (team tubafs) efforts in three different tasks during our participation in NLI shared task ).", "labels": [], "entities": []}, {"text": "All the three tasks aim at identifying native language using essays (essay track), speech transcriptions along with i-vectors (speech track) and fusion track that allows the participants to use all the three data sources to design and test a system for the purpose of NLI.", "labels": [], "entities": []}, {"text": "The first NLI task employed only essays written in English for the identification of native language.", "labels": [], "entities": [{"text": "identification of native language", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.8375639617443085}]}, {"text": "To date, all NLI shared tasks have been based on L2 English data, but NLI research has been extended to at least six other non-English languages.", "labels": [], "entities": []}, {"text": "In addition to using the written responses, a recent trend has been the use of speech transcriptions and audio features for dialect identification . The combination of transcriptions and acoustic features has also provided good results for dialect identification (.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.7662647366523743}, {"text": "dialect identification", "start_pos": 240, "end_pos": 262, "type": "TASK", "confidence": 0.7954765558242798}]}, {"text": "Following this trend, the 2016 Computational Paralinguistics Challenge () also included an NLI task based on the spoken response.", "labels": [], "entities": []}, {"text": "The NLI 2017 shared task attempts to combine these approaches by including a written response (essay) and a spoken response (speech transcriptions and i-vector acoustic features) for each subject.", "labels": [], "entities": [{"text": "NLI 2017 shared task", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8961883336305618}]}, {"text": "The task also allows for the fusion of all features.", "labels": [], "entities": []}, {"text": "Recent years have seen a large amount of work on employing text based features for the purpose of native language identification.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 98, "end_pos": 128, "type": "TASK", "confidence": 0.6436923742294312}]}, {"text": "The winning system (Jarvis et al., 2013) of NLI shared task 2013 featured a single model SVM system that used n-grams of lemmas, words, and part-ofspeech tags.", "labels": [], "entities": [{"text": "NLI shared task 2013", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.4874703139066696}]}, {"text": "The authors normalized each text to unit length and obtained an accuracy of 83.60 %.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9996620416641235}]}, {"text": "In another work, applied a union of character n-gram based string kernels and obtained an accuracy of 85.30 % on the dataset from NLI shared task 2013.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9994325041770935}, {"text": "NLI shared task 2013", "start_pos": 130, "end_pos": 150, "type": "DATASET", "confidence": 0.8468792140483856}]}, {"text": "Using the data from NLI shared task 2013, explored the use of phrase structure rules for the purpose of NLI.", "labels": [], "entities": [{"text": "NLI shared task 2013", "start_pos": 20, "end_pos": 40, "type": "DATASET", "confidence": 0.6106876730918884}]}, {"text": "The authors obtained an accuracy of 84.82 % which is similar to the results reported by previous authors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996628761291504}]}, {"text": "In another paper, employed an ensemble of SVM classifiers trained on character, word, part-of-speech n-grams, and syntactic dependencies and showed that the system achieves an accuracy of 81.82 % at NLI task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.999259889125824}]}, {"text": "Recently, explored ensemble related classifiers using word, character, lemma, and grammar based features and found that stacking the classifiers' ensemble achieves an accuracy of 87.10 %.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9991697072982788}]}, {"text": "In this paper, we used the single SVM model of C \u00b8 \u00a8 oltekin and Rama (2016) that combines character n-grams with word n-grams for the essay task.", "labels": [], "entities": []}, {"text": "We explored different ensemble models such as hard majority ensemble, mean majority ensemble, and stacked ensemble for the fusion task.", "labels": [], "entities": []}, {"text": "In the case of speech task, we found that a linear classifier trained on i-vectors (alone) achieves an accuracy greater than 80 % on the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9989036321640015}]}, {"text": "We also found that i-vectors combined with word unigrams from essays and speech transcriptions achieve an accuracy of 90.64 % on the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9996016621589661}]}, {"text": "The main result from our experiments is that i-vectors contribute towards improving the performance of NLI systems.", "labels": [], "entities": []}, {"text": "We also experimented with adding POS tags as features, and a number of neural network classifiers.", "labels": [], "entities": []}, {"text": "However, within our efforts, neither options improve the results.", "labels": [], "entities": []}, {"text": "As a result we only submitted results with the linear models noted above, and we only discuss these models in detail in this paper.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the different tasks and systems.", "labels": [], "entities": []}, {"text": "In section 3, we describe the results of our experiments.", "labels": [], "entities": []}, {"text": "We conclude our paper in section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results of LDA classifier on i-vectors  and the results on combined transcriptions and i- vectors.", "labels": [], "entities": [{"text": "LDA classifier", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.6787152588367462}]}, {"text": " Table 3: Results of different submissions for Fu- sion track.", "labels": [], "entities": [{"text": "Fu- sion track", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9004292190074921}]}]}