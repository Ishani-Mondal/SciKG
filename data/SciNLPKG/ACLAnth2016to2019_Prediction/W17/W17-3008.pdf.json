{"title": [{"text": "Abusive Language Detection on Arabic Social Media", "labels": [], "entities": [{"text": "Abusive Language Detection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8483934203783671}]}], "abstractContent": [{"text": "In this paper, we present our work on detecting abusive language on Arabic social media.", "labels": [], "entities": []}, {"text": "We extract a list of obscene words and hashtags using common patterns used in offensive and rude communications.", "labels": [], "entities": []}, {"text": "We also classify Twitter users according to whether they use any of these words or not in their tweets.", "labels": [], "entities": []}, {"text": "We expand the list of obscene words using this classification , and we report results on a newly created dataset of classified Arabic tweets (obscene, offensive, and clean).", "labels": [], "entities": []}, {"text": "We make this dataset freely available for research, in addition to the list of obscene words and hashtags.", "labels": [], "entities": []}, {"text": "We are also publicly releasing a large corpus of classified user comments that were deleted from a popular Arabic news site due to violations the site's rules and guidelines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media is a popular medium for discussion, expression of views, sharing of content, and promotion of ideas and products.", "labels": [], "entities": []}, {"text": "Like any other medium of communication, the content maybe clean or obscene/profane or and cordial/polite or offensive/rude.", "labels": [], "entities": []}, {"text": "Identification of profane and offensive exchanges on social media can be useful fora variety of applications.", "labels": [], "entities": [{"text": "Identification of profane and offensive exchanges on social media", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.7562440501319038}]}, {"text": "For example, users maybe interested in filtering out obscenities or indecent content from their social media stream or in filtering out such content for their children.", "labels": [], "entities": []}, {"text": "Further, detecting obscene or offensive language in asocial media exchange may indicate the discussion of contentious/controversial subjects/content or the presence of hate speech that maybe connected to or promoting hate crimes.", "labels": [], "entities": [{"text": "detecting obscene or offensive language in asocial media exchange", "start_pos": 9, "end_pos": 74, "type": "TASK", "confidence": 0.7258441978030734}]}, {"text": "Some sites such as Facebook: Google \"safe search\" setting allows users to filter out content based on a word list that users provide.", "labels": [], "entities": []}, {"text": "Similarly, as shown in, popular web search engines, such as Google and Bing, and media sharing sites, such as YouTube, have settings for \"safe search\" that filters out obscenities and pornographic contents.", "labels": [], "entities": []}, {"text": "On way to filter out such desirable content is to maintain a list of obscene words to filter content against.", "labels": [], "entities": []}, {"text": "However, the manual construction and maintenance of such lists is arduous.", "labels": [], "entities": []}, {"text": "This is due to the fact that list curators may not coverall words, particularly country/culture specific ones (written in local dialects or understood in certain cultures) and users may coin new words or alter the spelling of existing words (ex. by replacing letters with similarly looking characters, such as \"0\" instead of \"O\").", "labels": [], "entities": []}, {"text": "Jay and Janschewitz (2008) identified three categories of offensive speech, namely: Vulgar, which include explicit and rude sexual references, Pornographic, and Hateful, which includes offensive remarks concerning peoples race, religion, country, etc.", "labels": [], "entities": []}, {"text": "The goal of this work is to detect vulgar and pornographic obscene speech in Arabic social media without the need for manually curating word lists.", "labels": [], "entities": [{"text": "detect vulgar and pornographic obscene speech in Arabic social media", "start_pos": 28, "end_pos": 96, "type": "TASK", "confidence": 0.6717480063438416}]}, {"text": "The detection of offensive language that includes personal attacks, demeaning comments, or hateful language is left for future work.", "labels": [], "entities": []}, {"text": "Unlike previous work on obscenity and offensive language detection for different languages, such as English () and German (, very limited previous work for this task was done for Arabic (.", "labels": [], "entities": [{"text": "offensive language detection", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6805439194043478}]}, {"text": "Arabic poses interesting challenges primarily due to the lexical variations of different Arabic dialects.", "labels": [], "entities": []}, {"text": "Our approach is concerned with an automated approach to construct of an offensive word list.", "labels": [], "entities": []}, {"text": "The approach mines tweets to nominate new obscene words, which can be provided to judges who would either add them to the word list (if obscene) or not.", "labels": [], "entities": []}, {"text": "Our approach is based on the intuition that if we can identify users who often use obscene words from a seed word list of obscenities, then by contrasting these users against other users who never use words from the list, we can net additional obscenities.", "labels": [], "entities": []}, {"text": "We also introduce two new datasets for this task.", "labels": [], "entities": []}, {"text": "The first contains 1,100 manually labeled tweets, and the second contains 32K user comments that the moderators of a popular Arabic news site deemed inappropriate.", "labels": [], "entities": []}, {"text": "We are publicly releasing the datasets along with the lexicons we created.", "labels": [], "entities": []}], "datasetContent": [{"text": "To measure the effectiveness of our approach, we used intrinsic as well as extrinsic evaluation.", "labels": [], "entities": []}, {"text": "For intrinsic evaluation, we randomly selected 100 words (unigrams or bigrams) from the list of generated words with LOR equals to infinity.", "labels": [], "entities": [{"text": "LOR", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9944905042648315}]}, {"text": "We 53 marked the words as either obscene or not.", "labels": [], "entities": []}, {"text": "Of the 100 words, 59 were found to be obscene.", "labels": [], "entities": []}, {"text": "For extrinsic evaluation, we built a test set for the obscene and offensive language detection that contains 100 highly discussed tweets that each had at least 10 replies.", "labels": [], "entities": [{"text": "extrinsic evaluation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8687495291233063}, {"text": "obscene and offensive language detection", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.7885038256645203}]}, {"text": "Specifically, we collected the 100 tweets by identifying 10 controversial tweeps from the top tweeps in Egypt, according to SocialBakers.com.", "labels": [], "entities": []}, {"text": "For each of the tweeps, we randomly selected 10 tweets that have 10 or more comments/replies.", "labels": [], "entities": []}, {"text": "In all, we had 100 original tweets plus 1,000 comment/reply tweets -1,100 tweets all together.", "labels": [], "entities": []}, {"text": "For the tweets, we submitted each tweet along with its context (thread of replies) to CrowdFlower.com to be judged by 3 different annotators from Egypt.", "labels": [], "entities": []}, {"text": "The annotators could mark the tweets as: obscene, offensive (but not obscene), or clean.", "labels": [], "entities": []}, {"text": "shows three tweets and the their output judgments.", "labels": [], "entities": []}, {"text": "Of the judged tweets, the percentages of obscene, offensive (but not obscene), and clean tweets were 19.1%, 40.3%, and 40.6% respectively.", "labels": [], "entities": []}, {"text": "The average inter-annotator agreement was 85%.", "labels": [], "entities": []}, {"text": "In the context of this paper, we are only considering obscene tweets in our evaluation.", "labels": [], "entities": []}, {"text": "Offensive tweets are left for future work.", "labels": [], "entities": []}, {"text": "The 1,100 annotated tweets can be downloaded from http:// alt.qcri.org/ \u02dc hmubarak/offensive/ TweetClassification-Summary.xlsx.", "labels": [], "entities": []}, {"text": "Given the annotated test set and our list of obscene words, we automatically tagged each tweet in the test set as obscene if it contained a word in the list.", "labels": [], "entities": []}, {"text": "We experimented with several lists namely: the SeedWords list, the LOR list (word unigrams only), the LOR list (word bigrams only), combined LOR (unigrams only) + SeedWords lists, and combined LOR (bigrams only) + SeedWords lists.", "labels": [], "entities": []}, {"text": "shows the results (Precision, Recall, and F1) using the different lists.", "labels": [], "entities": [{"text": "Precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9975969195365906}, {"text": "Recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9940206408500671}, {"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9997373223304749}]}, {"text": "As can be seen in the results, using word unigrams is superior to using word bigrams.", "labels": [], "entities": []}, {"text": "The results suggests that the initial seed word list yields high precision with relatively low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9989916682243347}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9977110624313354}]}, {"text": "Combining SeedWords and LOR (unigram) lists yielded slightly improved recall, while maintaining the precision.", "labels": [], "entities": [{"text": "LOR", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9612502455711365}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9997479319572449}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9990178346633911}]}, {"text": "Using list-based methods to detect abusive language is proved to be good and robust).", "labels": [], "entities": []}, {"text": "However, this approach is limited by its reliance on lists.", "labels": [], "entities": []}, {"text": "This is shown also in our results in the form of high precision and low recall.) suggest using lexical and syntactical features along with automatically generated black lists.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9969354867935181}, {"text": "recall.", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9985383749008179}]}, {"text": "We plan to explore such features to account for the complexities and richness of Arabic and its dialects.", "labels": [], "entities": []}, {"text": "We also plan to look at morphological features to account for the rich morphology of Arabic.", "labels": [], "entities": []}, {"text": "Breaking Arabic words into constituent clitics can be useful in generating appropriate morphological features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Extrinsic evaluation results", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8908997774124146}]}]}