{"title": [{"text": "Shakespearizing Modern Language Using Copy-Enriched Sequence-to-Sequence Models", "labels": [], "entities": [{"text": "Shakespearizing Modern Language", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8779711127281189}]}], "abstractContent": [{"text": "Variations in writing styles are commonly used to adapt the content to a specific context , audience, or purpose.", "labels": [], "entities": []}, {"text": "However, applying stylistic variations is still largely a manual process, and there have been little efforts towards automating it.", "labels": [], "entities": []}, {"text": "In this paper we explore automated methods to transform text from modern English to Shake-spearean English using an end to end train-able neural model with pointers to enable copy action.", "labels": [], "entities": []}, {"text": "To tackle limited amount of parallel data, we pre-train embeddings of words by leveraging external dictionaries mapping Shakespearean words to modern English words as well as additional text.", "labels": [], "entities": []}, {"text": "Our methods are able to get a BLEU score of 31+, an improvement of \u2248 6 points over the strongest baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.982511967420578}]}, {"text": "We publicly release our code to foster further research in this area.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text is often morphed using a variety of lexical and grammatical transformations, adjusting the degree of formality, usage of catchy phrases, and other such stylistic changes to make it more appealing.", "labels": [], "entities": []}, {"text": "Moreover, different text styles appeal to different user segments (Saha Roy et al., 2015) (Kitis, 1997) (.", "labels": [], "entities": []}, {"text": "Thus there is a need to effectively adapt text to different styles.", "labels": [], "entities": []}, {"text": "However, manually transforming text to a desired style can be a tedious process.", "labels": [], "entities": []}, {"text": "However, there is a dearth of automated solutions for adapting text quickly to different styles.", "labels": [], "entities": []}, {"text": "We consider the problem of transforming text written in modern English text to Shakepearean style English.", "labels": [], "entities": [{"text": "Shakepearean", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9155687093734741}]}, {"text": "For the sake of brevity and clarity of exposition, we henceforth refer to the Shakespearean sentences/side as Original and the modern English paraphrases as Modern.", "labels": [], "entities": [{"text": "clarity", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.966113269329071}]}, {"text": "Unlike traditional domain or style transfer, our task is made more challenging by the fact that the two styles employ diachronically disparate registers of English -one style uses the contemporary language while the other uses Early Modern English 2 from the Elizabethan Era (1558-1603).", "labels": [], "entities": [{"text": "style transfer", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7344426363706589}]}, {"text": "Although Early Modern English is not classified as a different language (unlike Old English and Middle English), it does have novel words (acknown and belike), novel grammatical constructions (two second person forms -thou (informal) and you (formal)), semantically drifted senses (e.g fetches is a synonym of excuses) and non-standard orthography (.", "labels": [], "entities": []}, {"text": "Additionally, there is a domain difference since the Shakespearean play sentences are from a dramatic screenplay whereas the parallel modern English sentences are meant to be simplified explanation for high-school students.", "labels": [], "entities": []}, {"text": "Prior works in this field leverage a language model for the target style, achieving transformation either using phrase tables (, or by inserting relevant adjectives and adverbs.", "labels": [], "entities": []}, {"text": "Such works have limited scope in the type of transformations that can be achieved.", "labels": [], "entities": []}, {"text": "Moreover, statistical and rule MT based systems do not provide a direct mechanism to a) share word representation information between source and target sides b) incorporating constraints between words into word representations in end-to-end fashion.", "labels": [], "entities": [{"text": "rule MT", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.5203538239002228}]}, {"text": "Neural sequence-tosequence models, on the other hand, provide such flexibility.", "labels": [], "entities": []}, {"text": "Our main contributions are as follows: \u2022 We use a sentence level sequence to sequence neural model with a pointer network component to enable direct copying of words from input.", "labels": [], "entities": []}, {"text": "We demonstrate that this method performs much better than prior phrase transla-  \u2022 We leverage a dictionary providing mapping between Shakespearean words and modern English words to retrofit pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Incorporating this extra information enables our model to perform well in spite of small size of parallel data.", "labels": [], "entities": []}, {"text": "Rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first provide a brief analysis of our dataset in ( \u00a72).", "labels": [], "entities": []}, {"text": "We then elaborate on details of our methods in ( \u00a73, \u00a74, \u00a75, \u00a76).", "labels": [], "entities": []}, {"text": "We then discuss experimental setup and baselines in ( \u00a77).", "labels": [], "entities": []}, {"text": "Thereafter, we discuss the results and observations in ( \u00a78).", "labels": [], "entities": []}, {"text": "We conclude with discussions on related work ( \u00a79) and future directions ( \u00a710).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our dataset is a collection of line-by-line modern paraphrases for 16 of Shakespeare's 36 plays (Antony & Cleopatra, As You Like It, Comedy of Errors, Hamlet, Henry V etc) from the educational site Sparknotes 3 . This dataset was compiled by and is freely available on github.", "labels": [], "entities": [{"text": "Comedy of Errors, Hamlet, Henry V", "start_pos": 133, "end_pos": 166, "type": "TASK", "confidence": 0.5001594796776772}]}, {"text": "14 plays covering 18,395 sentences form the training data split.", "labels": [], "entities": [{"text": "training data split", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.7672957181930542}]}, {"text": "We kept 1218 sentences from the play Twelfth Night as validation data set.", "labels": [], "entities": []}, {"text": "The last play, Romeo and Juliet, comprising of 1462 sentences, forms the test set.", "labels": [], "entities": []}, {"text": "shows some parallel pairs from the test split of our data, along with the corresponding target outputs from some of our models.", "labels": [], "entities": []}, {"text": "Copy and SimpleS2S refer to our best performing attentional S2S models with and without a Copy component respectively.", "labels": [], "entities": []}, {"text": "Stat refers to the best statistical machine translation baseline using off-theshelf GIZA++ aligner and MOSES.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.5917070309321085}, {"text": "MOSES", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.580881655216217}]}, {"text": "We can see through many of the examples how direct copying from the source side helps the Copy generates better outputs than the SimpleS2S.", "labels": [], "entities": [{"text": "SimpleS2S", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.9064350724220276}]}, {"text": "The approaches are described in greater detail in ( \u00a73) and ( \u00a77).", "labels": [], "entities": []}, {"text": "shows some statistics from the training split of the dataset.", "labels": [], "entities": []}, {"text": "In general, the Original side has longer sentences and a larger vocabulary.", "labels": [], "entities": []}, {"text": "The slightly higher entropy of the Original side's frequency distribution indicates that the frequencies are more spread out over words.", "labels": [], "entities": []}, {"text": "Intuitively, the large number of shared word types indicates that sharing the representation between Original and Modern sides could provide some benefit.", "labels": [], "entities": []}, {"text": "In this section we describe the experimental setup and evaluation criteria used.", "labels": [], "entities": []}, {"text": "Our primary evaluation metric is BLEU () . We compute BLEU using the freely available and very widely used perl script 7 from the MOSES decoder.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.998497486114502}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9962164759635925}, {"text": "MOSES decoder", "start_pos": 130, "end_pos": 143, "type": "DATASET", "confidence": 0.955025851726532}]}, {"text": "We also report PINC (Chen and Dolan, 2011), which originates from paraphrase evaluation liter-ature and evaluates how much the target side paraphrases resemble the source side.", "labels": [], "entities": [{"text": "PINC", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.993986964225769}]}, {"text": "Given a source sentence sand a target side paraphrase c generated by the system, PINC(s,c) is defined as where N gram(x, n) denotes the set of n-grams of length n in sentence x, and N is the maximum length of ngram considered.", "labels": [], "entities": []}, {"text": "We set N = 4.", "labels": [], "entities": []}, {"text": "Higher the PINC, greater the novelty of paraphrases generated by the system.", "labels": [], "entities": [{"text": "PINC", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9938190579414368}, {"text": "novelty", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9847593903541565}]}, {"text": "Note, however, that PINC does not measure fluency of generated paraphrases.", "labels": [], "entities": []}], "tableCaptions": []}