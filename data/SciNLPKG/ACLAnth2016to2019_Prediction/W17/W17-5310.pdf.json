{"title": [{"text": "Refining Raw Sentence Representations for Textual Entailment Recognition via Attention", "labels": [], "entities": [{"text": "Refining Raw Sentence Representations", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9132608324289322}, {"text": "Textual Entailment Recognition", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.8321597377459208}]}], "abstractContent": [{"text": "In this paper we present the model used by the team Rivercorners for the 2017 RepE-val shared task.", "labels": [], "entities": [{"text": "RepE-val shared task", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.7840301195780436}]}, {"text": "First, our model separately encodes a pair of sentences into variable-length representations by using a bidirectional LSTM.", "labels": [], "entities": []}, {"text": "Later, it creates fixed-length raw representations by means of simple aggregation functions, which are then refined using an attention mechanism.", "labels": [], "entities": []}, {"text": "Finally it combines the refined representations of both sentences into a single vector to be used for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.9598854780197144}]}, {"text": "With this model we obtained test accuracies of 72.057% and 72.055% in the matched and mismatched evaluation tracks respectively, outperforming the LSTM baseline, and obtaining performances similar to a model that relies on shared information between sentences (ESIM).", "labels": [], "entities": []}, {"text": "When using an ensemble both accuracies increased to 72.247% and 72.827% respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.997650682926178}]}], "introductionContent": [{"text": "The task of Natural Language Inference (NLI) aims at characterizing the semantic concepts of entailment and contradiction, and is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning, as both entailment and contradiction are central concepts in natural language meaning.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.7974406480789185}, {"text": "information retrieval to semantic parsing", "start_pos": 162, "end_pos": 203, "type": "TASK", "confidence": 0.6372103691101074}, {"text": "commonsense reasoning", "start_pos": 207, "end_pos": 228, "type": "TASK", "confidence": 0.8365204334259033}]}, {"text": "The aforementioned task has been addressed with a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks.", "labels": [], "entities": []}, {"text": "With the advent of deep learning techniques, NLI has become an important testing ground for approaches that employ distributed word and phrase representations, which are typical of these models.", "labels": [], "entities": []}, {"text": "In this context, the Second Workshop on Evaluating Vector Space Representations for NLP (RepEval 2017) features a shared task meant to evaluate natural language understanding models based on sentence encoders by the means of NLI in the style of a three-class balanced classification problem over sentence pairs.", "labels": [], "entities": []}, {"text": "The shared task includes two evaluations, a standard in-domain (matched) evaluation in which the training and test data are drawn from the same sources, and a cross-domain (mismatched) evaluation in which the training and test data differ substantially.", "labels": [], "entities": []}, {"text": "This cross-domain evaluation is aimed at testing the ability of submitted systems to learn representations of sentence meaning that capture broadly useful features.", "labels": [], "entities": []}], "datasetContent": [{"text": "To make our results comparable to the baselines reported in the Kaggle platform we randomly sampled 15% of the SNLI corpus ( and added it to the MultiNLI corpus.", "labels": [], "entities": [{"text": "Kaggle platform", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9205836355686188}, {"text": "SNLI corpus", "start_pos": 111, "end_pos": 122, "type": "DATASET", "confidence": 0.8436014950275421}, {"text": "MultiNLI corpus", "start_pos": 145, "end_pos": 160, "type": "DATASET", "confidence": 0.9588906168937683}]}, {"text": "We used the pre-trained 300-dimensional GloVe vectors trained on 840B tokens ().", "labels": [], "entities": []}, {"text": "These embeddings were not fine-tuned during training and unknown word vectors were initialized by randomly sampling from the uniform distribution in (\u22120.05, 0.05).", "labels": [], "entities": []}, {"text": "Each character embedding was initialized as a 20-dimensional vector and the character-level LSTM output dimension was set to 50.", "labels": [], "entities": []}, {"text": "The wordlevel LSTM output dimension was set to 300, which means that after concatenating word-level and character-level representations the word vectors for each direction are 350-dimensional (i.e., hi \u2208 R 700 ).", "labels": [], "entities": []}, {"text": "For the Inner Attention Layer we defined the parameter W as a square matrix matching the dimension of the concatenated vector [ \u00af h; hi ] (i.e., W \u2208 R 1400\u00d71400 ), and v as a vector matching the same dimension (i.e., v \u2208 R 1400 ).", "labels": [], "entities": []}, {"text": "Both W and v were initialized by randomly sampling from the uniform distribution on the interval (\u22120.005, 0.005).", "labels": [], "entities": []}, {"text": "The final layer was created as a 3-layer MLP with 2000 hidden units each, and with ReLU activations.", "labels": [], "entities": []}, {"text": "Additionally, we used the Rmsprop optimizer with a learning rate of 0.001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 51, "end_pos": 64, "type": "METRIC", "confidence": 0.9644731283187866}]}, {"text": "We applied dropout of 0.25 only between the MLP layers of the Dense Layer.", "labels": [], "entities": [{"text": "dropout", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9722648859024048}]}, {"text": "Further, we found out that normalizing the capitalization of words by making all characters lowercase, and transforming numbers into a specific numeric token improved the model's performance while reducing the size of the embedding matrix.", "labels": [], "entities": []}, {"text": "We also ignored the sentence pairs with a premise longer than 200 words during training (for improved memory stability), and those without a valid label (\"-\") both during training and validation.", "labels": [], "entities": []}, {"text": "Since one of the most conceptually important parts of our model was the raw sentence representation created in the Pooling Layer, we used four different methods for generating it.", "labels": [], "entities": []}, {"text": "We also tried using other architectures that rely on some sort of \"inner\" attention such as the selfattentive model proposed by and the co-attentive model by, but our preliminary results were not promising so we did not invest in fine-tuning them.", "labels": [], "entities": []}, {"text": "All the experiments were repeated without using character-level embeddings (i.e., hi \u2208 R 600 ).", "labels": [], "entities": []}, {"text": "presents the results of using different pooling strategies for generating a raw sentence representation vector from the word vectors.", "labels": [], "entities": []}, {"text": "We can observe that that both the mean method, and picking the last hidden state for both directions performed slightly better than the two other strategies, however at 95% confidence we cannot assert that any of these methods is statistically different from one another.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Best matched validation accuracies (%)  obtained by each pooling method in presence and  absence of character embeddings.", "labels": [], "entities": []}, {"text": " Table 3: Validation accuracies (%) for our best  model broken down by genre. Both CBOW and  ESIM results are reported as in (Williams et al.,  2017).", "labels": [], "entities": [{"text": "Validation accuracies", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8137050867080688}, {"text": "CBOW", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.682812511920929}]}]}