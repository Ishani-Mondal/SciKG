{"title": [{"text": "Sequential Attention: A Context-Aware Alignment Function for Machine Reading", "labels": [], "entities": [{"text": "Sequential Attention", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8903985619544983}, {"text": "Machine Reading", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.8009031116962433}]}], "abstractContent": [{"text": "In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in away that takes into account not just how well that word matches a query, but how well surrounding words match.", "labels": [], "entities": []}, {"text": "We evaluate this approach on the task of reading comprehension (on the Who did What and CNN datasets) and show that it dramatically improves a strong baseline-the Stanford Reader-and is competitive with the state of the art.", "labels": [], "entities": [{"text": "Who did What and CNN datasets", "start_pos": 71, "end_pos": 100, "type": "DATASET", "confidence": 0.8247902393341064}, {"text": "Stanford Reader-and", "start_pos": 163, "end_pos": 182, "type": "DATASET", "confidence": 0.9166057407855988}]}], "introductionContent": [{"text": "Soft attention (), a differentiable method for selecting the inputs fora component of a model from a set of possibilities, has been crucial to the success of artificial neural network models for natural language understanding tasks like reading comprehension that take short passages as inputs.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 195, "end_pos": 231, "type": "TASK", "confidence": 0.7293485552072525}]}, {"text": "However, standard approaches to attention in NLP select words with only very indirect consideration of their context, limiting their effectiveness.", "labels": [], "entities": []}, {"text": "This paper presents a method to address this by adding explicit context sensitivity into the soft attention scoring function.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of this approach on the task of cloze-style reading comprehension.", "labels": [], "entities": []}, {"text": "A problem in the cloze style consists of a passage p, a question q and an answer a drawn from among the entities mentioned in the passage.", "labels": [], "entities": []}, {"text": "In particular, we use the CNN dataset), which introduced the task into widespread use in evaluating neural networks for language understanding, and the newer and more * These authors contributed equally to this work.", "labels": [], "entities": [{"text": "CNN dataset", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9026485979557037}, {"text": "language understanding", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.7741713523864746}]}, {"text": "RNNs first encode the question into a vector j and the document into a sequence of vectors H.", "labels": [], "entities": []}, {"text": "For each word index i in the document, a scoring vector \u03b3 i is then computed from j and hi using a function like the partial bilinear function shown here.", "labels": [], "entities": []}, {"text": "These vectors are then used as inputs to another RNN layer, the outputs of which (\u03b7 i ) are summed elementwise and used as attention scores (\u03b1 i ) in answer selection.", "labels": [], "entities": [{"text": "attention scores (\u03b1 i )", "start_pos": 123, "end_pos": 146, "type": "METRIC", "confidence": 0.8257193664709727}, {"text": "answer selection", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.8569027185440063}]}, {"text": "carefully quality-controlled Who did What dataset (.", "labels": [], "entities": []}, {"text": "In standard approaches to soft attention over passages, a scoring function is first applied to every word in the source text to evaluate how closely that word matches a query vector (here, a function of the question).", "labels": [], "entities": [{"text": "soft attention over passages", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.8770545274019241}]}, {"text": "The resulting scores are then normalized and used as the weights in a weighted sum which produces an output or context vector summarizing the most salient words of the input, which is then used in a downstream model (here, to select an answer).", "labels": [], "entities": []}, {"text": "In this work we propose a novel scoring function for soft attention that we call Sequential Attention (SA), shown in.", "labels": [], "entities": [{"text": "Sequential Attention (SA)", "start_pos": 81, "end_pos": 106, "type": "METRIC", "confidence": 0.6144794225692749}]}, {"text": "In an SA model, a mutiplicative interaction scoring function is used to produce a scoring vector for each word in the source text.", "labels": [], "entities": []}, {"text": "A newly-added bidirectional RNN then consumes those vectors and uses them to produce a context-aware scalar score for each word.", "labels": [], "entities": []}, {"text": "We evaluate this scoring function within the context of the Stanford Reader (, and show that it yields dramatic improvements in performance.", "labels": [], "entities": [{"text": "Stanford Reader", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9323630332946777}]}, {"text": "On both datasets, it is outperformed only by the Gated Attention Reader, which in some cases has access to features not explicitly seen by our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on two tasks, CNN and Who did What (WDW initialized from a U \u223c (\u22120.01, 0.01) while GRU weights were initialized from a N \u223c (0, 0.1).", "labels": [], "entities": [{"text": "CNN", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8602471351623535}, {"text": "Who did What", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.45055801669756573}]}, {"text": "Learning was carried outwith SGD with a learning rate of 0.1, batch size of 32, gradient clipping of norm 10 and dropout of 0.2 in all the vertical layers 4 (including the Sequential Attention layer).", "labels": [], "entities": []}, {"text": "Also, all the anonymized entities were relabeled according to the order of occurrence, as in the Stanford Reader.", "labels": [], "entities": [{"text": "Stanford Reader", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9188483357429504}]}, {"text": "We trained all models for 30 epochs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on WDW and CNN test sets", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989027976989746}, {"text": "WDW", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.9363276362419128}, {"text": "CNN test sets", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8615978360176086}]}, {"text": " Table 2: Accuracy on CNN test sets and number of  trainable parameters for various Stanford Reader  (SR) and Sequential Attention (SA) models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9979897737503052}, {"text": "CNN test sets", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.8718109329541525}, {"text": "Stanford Reader  (SR)", "start_pos": 84, "end_pos": 105, "type": "DATASET", "confidence": 0.8636156916618347}]}]}