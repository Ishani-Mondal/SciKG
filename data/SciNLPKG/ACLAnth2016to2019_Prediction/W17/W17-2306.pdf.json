{"title": [{"text": "Results of the fifth edition of the BioASQ Challenge", "labels": [], "entities": [{"text": "BioASQ Challenge", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.6999770998954773}]}], "abstractContent": [{"text": "The goal of the BioASQ challenge is to engage researchers into creating cutting-edge biomedical information systems.", "labels": [], "entities": []}, {"text": "Specifically, it aims at the promotion of systems and methodologies that are able to deal with a plethora of different tasks in the biomedical domain.", "labels": [], "entities": []}, {"text": "This is achieved through the organization of challenges.", "labels": [], "entities": []}, {"text": "The fifth challenge consisted of three tasks: semantic indexing, question answering and anew task on information extraction.", "labels": [], "entities": [{"text": "semantic indexing", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.834896057844162}, {"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.869428962469101}, {"text": "information extraction", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.825356662273407}]}, {"text": "In total, 29 teams with more than 95 systems participated in the challenge.", "labels": [], "entities": []}, {"text": "Overall, as in previous years, the best systems were able to outperform the strong baselines.", "labels": [], "entities": []}, {"text": "This suggests that state-of-the art systems are continuously improving , pushing the frontier of research.", "labels": [], "entities": []}], "introductionContent": [{"text": "The aim of this paper is twofold.", "labels": [], "entities": []}, {"text": "First, we aim to give an overview of the data issued during the BioASQ challenge in 2017.", "labels": [], "entities": [{"text": "BioASQ challenge in 2017", "start_pos": 64, "end_pos": 88, "type": "DATASET", "confidence": 0.8035620748996735}]}, {"text": "In addition, we aim to present the systems that participated in the challenge and evaluate their performance.", "labels": [], "entities": []}, {"text": "To achieve these goals, we begin by giving a brief overview of the tasks, which took place from February to May 2017, and the challenge's data.", "labels": [], "entities": []}, {"text": "Thereafter, we provide an overview of the systems that participated in the challenge.", "labels": [], "entities": []}, {"text": "Detailed descriptions of some of the systems are given in workshop proceedings.", "labels": [], "entities": []}, {"text": "The evaluation of the systems, which was carried out using state-of-the-art measures or manual assessment, is the last focal point of this paper, with remarks regarding the results of each task.", "labels": [], "entities": []}, {"text": "The conclusions sum up this year's challenge.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics on test datasets for Task 5a.", "labels": [], "entities": []}, {"text": " Table 2: Statistics on the training and test datasets  of Task 5b. All the numbers for the documents and  snippets refer to averages.", "labels": [], "entities": []}, {"text": " Table 3: Dataset overview for Task 5c.", "labels": [], "entities": []}, {"text": " Table 7: Average system ranks across the batches of the Task 5a. A hyphenation symbol (-) is used when- ever the system participated in fewer than 4 tests in the batch. Systems with fewer than 4 participations  in all batches are omitted.", "labels": [], "entities": []}, {"text": " Table 8: Results for snippet retrieval in batch 3 of phase A of Task 5b.", "labels": [], "entities": [{"text": "snippet retrieval", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8698005974292755}]}, {"text": " Table 9: Results for document retrieval in batch 3 of phase A of Task 5b.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8018573522567749}]}, {"text": " Table 10: Results for batch 4 for exact answers in phase B of Task 5b.", "labels": [], "entities": []}, {"text": " Table 11: Micro Recall (MR) results on the test set of Task 5c.", "labels": [], "entities": [{"text": "Micro Recall (MR)", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.8699601769447327}]}]}