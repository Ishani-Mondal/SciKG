{"title": [], "abstractContent": [{"text": "This work presents a novel approach to Automatic Post-Editing (APE) and Word-Level Quality Estimation (QE) using ensembles of specialized Neural Machine Translation (NMT) systems.", "labels": [], "entities": [{"text": "Word-Level Quality Estimation (QE)", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.5957376807928085}, {"text": "Neural Machine Translation (NMT)", "start_pos": 138, "end_pos": 170, "type": "TASK", "confidence": 0.8503412703673044}]}, {"text": "Word-level features that have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis.", "labels": [], "entities": [{"text": "QE", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.964420735836029}, {"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.690148338675499}]}, {"text": "We train a suite of NMT models that use different input representations, but share the same output space.", "labels": [], "entities": []}, {"text": "These models are then en-sembled together, and tuned for both the APE and the QE task.", "labels": [], "entities": [{"text": "APE", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.774842381477356}]}, {"text": "We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework.", "labels": [], "entities": [{"text": "APE", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.7100462913513184}]}, {"text": "Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation destined for human consumption often must pass through multiple editing stages.", "labels": [], "entities": []}, {"text": "In one common scenario, human translators correct machine translation (MT) output, correcting errors and omissions until a perfect translation has been produced.", "labels": [], "entities": [{"text": "translators correct machine translation (MT) output", "start_pos": 30, "end_pos": 81, "type": "TASK", "confidence": 0.7674853131175041}]}, {"text": "Several studies has shown that this process, referred to as \"post-editing\", is faster than translation from scratch (Specia, 2011), or interactive machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.7371869683265686}]}, {"text": "A relatively recent line of research has tried to build models which correct errors in MT automatically (.", "labels": [], "entities": [{"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.9880069494247437}]}, {"text": "Automatic Post-Editing (APE) typically views the system that produced the original translation as a black box, which cannot be modified or inspected.", "labels": [], "entities": [{"text": "Automatic Post-Editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5433521270751953}]}, {"text": "An APE system has access to the same data that a human translator would see: a source sentence and a translation hypothesis.", "labels": [], "entities": []}, {"text": "The job of the system is to output a corrected hypothesis, attempting to fix errors made by the original translation system.", "labels": [], "entities": []}, {"text": "This can be viewed as a sequence-tosequence task, and is also similar to multi-source machine translation (.", "labels": [], "entities": [{"text": "multi-source machine translation", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6119340062141418}]}, {"text": "However, APE intuitively tries to make the minimum number of edits required to transform the hypothesis into a satisfactory translation, because we would like our system to mimic human translators in attempting to minimize the time spent correcting each MT output.", "labels": [], "entities": [{"text": "APE", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.5818453431129456}]}, {"text": "This additional constraint on APE models differentiates the task from multisource MT.", "labels": [], "entities": [{"text": "multisource MT", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.5668878853321075}]}, {"text": "The Word Level QE task is ostensibly a simpler version of APE, where a system must only decide whether or not each word in an MT hypothesis belongs in the post-edited version -it is not necessary to propose a fix for errors.", "labels": [], "entities": [{"text": "Word Level QE task", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.5978023633360863}, {"text": "APE", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.7086952924728394}]}, {"text": "Most recent work has considered word-level QE to be a sequence labeling task, and employed the standard tools of structured prediction to solve it, i.e. structured predictors such as CRFs or structured SVMs, which take advantage of sparse representations and very large feature sets, as well as dependencies between labels in the output sequence ().", "labels": [], "entities": []}, {"text": "However, recently proposed anew method of word-level QE using APE, which simply uses an APE system to produce a \"pseudo-post-edit\" given a source sentence and an MT hypothesis.", "labels": [], "entities": [{"text": "APE", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.5800794959068298}, {"text": "MT", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.8880892992019653}]}, {"text": "Their approach, which we call APE-QE, is the basis of the work presented here.", "labels": [], "entities": [{"text": "APE-QE", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.5091885328292847}]}, {"text": "In APE-QE, the original MT hypothesis is then aligned with the pseudo-postedit from the APE system using word level edit-distance, and words which correspond to Insert or Delete operations are labeled as incorrect.", "labels": [], "entities": [{"text": "APE-QE", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.8384885191917419}, {"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9469226598739624}, {"text": "APE system", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.8948580622673035}]}, {"text": "Note that this also corresponds exactly to the way QE datasets are currently created, with the only difference being that human post-edits are typically used to create gold-standard data.", "labels": [], "entities": [{"text": "QE datasets", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.7610932290554047}]}, {"text": "A key similarity between the QE and APE tasks is that both use information from two sequences: (1) the original source input, and (2) an MT hypothesis., showed that APE systems with no knowledge about the QE task already provide a very strong baseline for QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 256, "end_pos": 258, "type": "TASK", "confidence": 0.9022613763809204}]}, {"text": "Because the essential training data for the APE and QE tasks is identical, consisting of parallel triples of (SRC, MT, P E), it is also natural to consider these tasks as two subtasks that make use of a single underlying model.", "labels": [], "entities": [{"text": "APE and QE tasks", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.5603198781609535}]}, {"text": "In this work, we explicitly design ensembles of NMT models for both word-level QE, and APE.", "labels": [], "entities": []}, {"text": "This approach builds upon the approach presented in, by incorporating features which have proven effective for Word Level QE as \"factors\" in the input to Neural Machine Translation (NMT) systems.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 154, "end_pos": 186, "type": "TASK", "confidence": 0.8533448179562887}]}, {"text": "We achieve state-of-the-art results in both Automatic Post-Editing and WordLevel Quality Estimation, matching the performance of much more complex QE systems, and significantly outperforming the current state-ofthe-art in APE.", "labels": [], "entities": [{"text": "WordLevel Quality Estimation", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6067866384983063}]}, {"text": "The main contributions of this work are: \u2022 Novel Input Representations for Neural APE models \u2022 New tuned ensembles for APE-QE \u2022 An open-source decoder supporting ensembles of models with different inputs 1 The following sections discuss our approach to creating hybrid models for APE-QE, which should be able to solve both tasks with minimal modification.", "labels": [], "entities": [{"text": "APE-QE", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.862559974193573}]}], "datasetContent": [{"text": "All of our models are trained using Nematus.", "labels": [], "entities": []}, {"text": "At inference time we use our own decoder, which supports weighted loglinear ensembles of Nematus models . Following Junczys-Dowmunt and Grundkiewicz (2016), we first train each model type on the large (4M) synthetic training data, then fine tune using the 500K dataset, concatenated with the task-internal training data upsampled 20x.", "labels": [], "entities": []}, {"text": "Finally, for SRC+MT and SRC+MT-factor we continued fine-tuning each model fora small number of iterations using the min-risk training implementation available in Nematus (.", "labels": [], "entities": [{"text": "SRC+MT", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.5011024177074432}, {"text": "Nematus", "start_pos": 162, "end_pos": 169, "type": "DATASET", "confidence": 0.9310664534568787}]}, {"text": "shows the best dev result after each stage of training.", "labels": [], "entities": []}, {"text": "For both APE and QE, we use only the taskspecific training data provided for the WMT 2017 APE task, including the extra synthetic training data . However, note that the SpaCy models used to extract features for the factored models are trained with external data -we only use the offthe-shelf models provided by the SpaCy developers.", "labels": [], "entities": [{"text": "APE", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.6881623268127441}, {"text": "WMT 2017 APE task", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.7976454496383667}]}, {"text": "To convert the output sequence from an APE system into OK, BAD labels for QE, we use the APE hypothesis as a \"pseudo-reference\", which is then aligned with the original MT hypothesis using TER).", "labels": [], "entities": [{"text": "BAD", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9814266562461853}, {"text": "TER", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9840403199195862}]}, {"text": "shows the results of our experiments using the WMT 16 development and test sets.", "labels": [], "entities": [{"text": "WMT 16 development and test sets", "start_pos": 47, "end_pos": 79, "type": "DATASET", "confidence": 0.9071188469727834}]}, {"text": "For each system, we measure performance on BLEU and TER, which are the metrics used in APE task, and also on F1-Mult, which is the primary metric used for the Word Level QE task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9993306398391724}, {"text": "TER", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9964726567268372}, {"text": "APE task", "start_pos": 87, "end_pos": 95, "type": "TASK", "confidence": 0.5923361778259277}, {"text": "F1-Mult", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9922651052474976}, {"text": "Word Level QE task", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.5531457290053368}]}, {"text": "Overall tagging accuracy is included as a secondary metric for QE.", "labels": [], "entities": [{"text": "tagging", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9526452422142029}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9647424221038818}, {"text": "QE", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.8719286322593689}]}], "tableCaptions": [{"text": " Table 1: Results for all models and ensembles on WMT 16 development and test datasets", "labels": [], "entities": [{"text": "WMT 16 development and test datasets", "start_pos": 50, "end_pos": 86, "type": "DATASET", "confidence": 0.7605472604433695}]}, {"text": " Table 2: Final weights for each model type after  10 iterations of MERT for tuning objectives TER  and F1-Mult.", "labels": [], "entities": [{"text": "MERT", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.947952151298523}, {"text": "TER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9913894534111023}, {"text": "F1-Mult", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9606863856315613}]}, {"text": " Table 3: Examples of the input for the five model types used in the APE and QE ensembles. The pipe  symbol '|' separates each factor. '-' followed by whitespace indicates segmentation according to the  subword encoding.", "labels": [], "entities": [{"text": "APE and QE ensembles", "start_pos": 69, "end_pos": 89, "type": "DATASET", "confidence": 0.6461734101176262}]}, {"text": " Table 4: Best BLEU score on dev set after each  of the training stages. General is training with 4M  instances, Fine-tune is training with 500K + up- sampled in-domain data, Min-Risk uses the same  dataset as Fine-tune, but uses a minimum-risk loss  with BLEU score as the target metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.998604953289032}, {"text": "BLEU score", "start_pos": 256, "end_pos": 266, "type": "METRIC", "confidence": 0.9728852212429047}]}]}