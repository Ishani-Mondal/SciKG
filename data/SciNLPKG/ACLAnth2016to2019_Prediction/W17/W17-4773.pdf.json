{"title": [{"text": "Multi-source Neural Automatic Post-Editing: FBK's participation in the WMT 2017 APE shared task", "labels": [], "entities": [{"text": "FBK", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9291157722473145}, {"text": "WMT 2017 APE shared task", "start_pos": 71, "end_pos": 95, "type": "DATASET", "confidence": 0.7046846747398376}]}], "abstractContent": [{"text": "Previous phrase-based approaches to Automatic Post-editing (APE) have shown that the dependency of MT errors from the source sentence can be exploited by jointly learning from source and target information.", "labels": [], "entities": [{"text": "Automatic Post-editing (APE)", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.6506008803844452}, {"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9724711179733276}]}, {"text": "By integrating this notion in a neu-ral approach to the problem, we present the multi-source neural machine translation (NMT) system submitted by FBK to the WMT 2017 APE shared task.", "labels": [], "entities": [{"text": "multi-source neural machine translation (NMT)", "start_pos": 80, "end_pos": 125, "type": "TASK", "confidence": 0.7458637952804565}, {"text": "FBK", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.954677164554596}, {"text": "WMT 2017 APE shared task", "start_pos": 157, "end_pos": 181, "type": "TASK", "confidence": 0.5552879989147186}]}, {"text": "Our system implements multi-source NMT in a weighted ensemble of 8 models.", "labels": [], "entities": []}, {"text": "The n-best hypotheses produced by this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis , as well as other statistical models (n-gram language model and operation sequence model).", "labels": [], "entities": []}, {"text": "This solution resulted in the best system submission for this round of the APE shared task for both en-de and de-en language directions.", "labels": [], "entities": [{"text": "APE shared task", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.5757405559221903}]}, {"text": "For the former language direction, our primary submission improves over the MT baseline up to-4.9 TER and +7.6 BLEU points.", "labels": [], "entities": [{"text": "MT baseline", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.7651741206645966}, {"text": "TER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9992175102233887}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9968672394752502}]}, {"text": "For the latter, where the higher quality of the original MT output reduces the room for improvement, the gains are lower but still significant (-0.25 TER and +0.3 BLEU).", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9284579157829285}, {"text": "TER", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9978571534156799}, {"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9960212111473083}]}], "introductionContent": [{"text": "Automatic post-editing (APE) aims to correct systematic machine translation (MT) errors, thereby reducing translators workload and eventually increasing translation productivity.", "labels": [], "entities": [{"text": "Automatic post-editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6073297023773193}, {"text": "correct systematic machine translation (MT)", "start_pos": 37, "end_pos": 80, "type": "TASK", "confidence": 0.7613038463251931}]}, {"text": "The task, well motivated in ( and, becomes necessary when working in a \"black-box\" condition where the MT engine used to translate is not directly accessible for retraining or for more radical internal modifications.", "labels": [], "entities": []}, {"text": "As pointed out in (, from the application point of view an APE system can help to: i) improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; ii) provide professional translators with improved MT output quality to reduce (human) postediting effort and iii) adapt the output of a generalpurpose MT system to the lexicon/style requested in a specific application domain.", "labels": [], "entities": [{"text": "MT output", "start_pos": 94, "end_pos": 103, "type": "TASK", "confidence": 0.8891472518444061}, {"text": "MT output", "start_pos": 292, "end_pos": 301, "type": "TASK", "confidence": 0.8882802128791809}]}, {"text": "Different APE paradigms based on statistical methods () have been proposed in the past showing the effectiveness of APE systems.", "labels": [], "entities": [{"text": "APE", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.954931914806366}, {"text": "APE", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9181075096130371}]}, {"text": "In the previous round of the APE shared task (WMT16), neural), hybrid, and phrase-based () solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016.", "labels": [], "entities": [{"text": "APE shared task (WMT16)", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6430317411820093}, {"text": "MT output", "start_pos": 140, "end_pos": 149, "type": "TASK", "confidence": 0.9186750948429108}]}, {"text": "Some of the previous approaches, both phrase-based) and neural (Libovick\u00b4ybovick\u00b4y et al., 2016) also suggested the importance of jointly learning both from the source sentences and from the corresponding translations in order to take advantage of the strict dependency between translation errors and the original source sentences.", "labels": [], "entities": []}, {"text": "Learning from these lessons, this year the FBK participation in the APE task is based on a multisource neural sequence-to-sequence architecture.", "labels": [], "entities": [{"text": "APE task", "start_pos": 68, "end_pos": 76, "type": "TASK", "confidence": 0.8793324530124664}]}, {"text": "We extend the existing NMT implementation in the Nematus toolkit) to facilitate multi-source training and decoding.", "labels": [], "entities": [{"text": "Nematus toolkit", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.8927371203899384}]}, {"text": "This year we participated in both translation directions (en-de and de-en) with similar system architectures consisting of an ensemble of 8 neural models followed by a re-ranker.", "labels": [], "entities": []}, {"text": "On both tasks, our primary submissions achieved the best results, with significant improvements over the baseline (-4.9 TER and +7.6 BLEU for en-de and -0.25 TER and +0.3 BLEU for de-en).", "labels": [], "entities": [{"text": "TER", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9961814880371094}, {"text": "BLEU", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9849439263343811}, {"text": "TER", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.9724926948547363}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9929853081703186}]}], "datasetContent": [{"text": "In this section we summarize how our systems have been trained, tuned and combined to produce the FBK submissions to the WMT 2017 APE shared task.", "labels": [], "entities": [{"text": "FBK", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.7997403740882874}, {"text": "WMT 2017 APE shared task", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.6969304978847504}]}, {"text": "We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (), which is based on modified n-gram precision.", "labels": [], "entities": [{"text": "TER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9976186156272888}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9990766048431396}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9129143357276917}]}, {"text": "In addition to the standard evaluation metrics, we also measure the precision of our APE system using sentence level TER score as defined in (Chatterjee et al., 2015a):", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9993847608566284}, {"text": "TER score", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9029074609279633}]}], "tableCaptions": [{"text": " Table 1: Performance of the APE systems on dev.  2016 (en-de) (\" \u2020\" indicates statistically significant  differences wrt. MT Baseline with p<0.05).", "labels": [], "entities": [{"text": "APE systems on dev.  2016", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.7229368140300115}, {"text": "MT Baseline", "start_pos": 123, "end_pos": 134, "type": "DATASET", "confidence": 0.8564085364341736}]}, {"text": " Table 2: Performance of the APE systems on dev.  2017 (de-en) (\" \u2020\" indicates statistically significant  differences wrt. MT Baseline with p<0.05).", "labels": [], "entities": [{"text": "APE systems on dev.  2017", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.7222598294417063}, {"text": "MT Baseline", "start_pos": 123, "end_pos": 134, "type": "DATASET", "confidence": 0.8540300726890564}]}, {"text": " Table 3.  We observe that this system achieves significant  improvement over the MT baseline (-5.4 TER and  8.7 BLEU points) also on the 2016 test set.", "labels": [], "entities": [{"text": "MT baseline", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.6685574948787689}, {"text": "TER", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.989655613899231}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9949276447296143}]}, {"text": " Table 3: Performance of the APE systems on  the 2016 test set (en-de) (\" \u2020\" indicates statisti- cally significant differences wrt. MT Baseline  with p<0.05).", "labels": [], "entities": [{"text": "APE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6905894875526428}, {"text": "2016 test set", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8045603533585867}, {"text": "MT Baseline", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.6256318390369415}]}, {"text": " Table 4: Official results on 2017 test set.", "labels": [], "entities": [{"text": "2017 test set", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8241522510846456}]}]}