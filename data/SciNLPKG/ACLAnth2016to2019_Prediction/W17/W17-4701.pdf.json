{"title": [{"text": "Sense-Aware Statistical Machine Translation using Adaptive Context-Dependent Clustering", "labels": [], "entities": [{"text": "Sense-Aware Statistical Machine Translation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5312753394246101}]}], "abstractContent": [{"text": "Statistical machine translation (SMT) systems use local cues from n-gram translation and language models to select the translation of each source word.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7900137603282928}]}, {"text": "Such systems do not explicitly perform word sense disambiguation (WSD), although this would enable them to select translations depending on the hypothesized sense of each word.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7823065767685572}]}, {"text": "Previous attempts to constrain word translations based on the results of generic WSD systems have suffered from their limited accuracy.", "labels": [], "entities": [{"text": "word translations", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.6961987912654877}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9951047897338867}]}, {"text": "We demonstrate that WSD systems can be adapted to help SMT, thanks to three key achievements: (1) we consider a larger context for WSD than SMT can afford to consider; (2) we adapt the number of senses per word to the ones observed in the training data using clustering-based WSD with K-means; and (3) we initialize sense-clustering with definitions or examples extracted from WordNet.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9962102174758911}, {"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9405457377433777}, {"text": "WordNet", "start_pos": 377, "end_pos": 384, "type": "DATASET", "confidence": 0.9702959060668945}]}, {"text": "Our WSD system is competitive, and in combination with a factored SMT system improves noun and verb translation from English to Chinese, Dutch, French, German, and Spanish.", "labels": [], "entities": [{"text": "WSD", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6904603838920593}, {"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9474972486495972}, {"text": "noun and verb translation", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.6539452821016312}]}], "introductionContent": [{"text": "Selecting the correct translation of polysemous words remains an important challenge for machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.8120646953582764}]}, {"text": "While some translation options maybe interchangeable, substantially different senses of source words must generally be rendered by different words in the target language.", "labels": [], "entities": []}, {"text": "In this case, an MT system should identify -implicitly or explicitly -the correct sense conveyed by each occurrence in order to select the appropriate translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9900968074798584}]}, {"text": "Source: And I do really like this shot, because it shows all the detritus that's sort of embedded in the sole of the sneakers.", "labels": [], "entities": []}, {"text": "Baseline SMT: Und ich mag dieses Bild . .", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.6862488389015198}]}, {"text": "Online NMT: Und ich mag diesen Schuss wirklich, . .", "labels": [], "entities": [{"text": "NMT", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.776694655418396}]}, {"text": "Sense-aware MT: Und ich mag diese Aufnahme wirklich, . .", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.8298878073692322}]}, {"text": "Reference translation: Ich mag diese Aufnahme wirklich, . .", "labels": [], "entities": []}, {"text": ".: Example of sense-aware translation that is closer to a reference translation than a baseline statistical MT system or an online neural one.", "labels": [], "entities": [{"text": "sense-aware translation", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.6790053099393845}]}, {"text": "Current statistical or neural MT systems perform word sense disambiguation (WSD) implicitly, for instance through the n-gram frequency information stored in the translation and language models.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.804586743315061}]}, {"text": "However, the context taken into account by an MT system when performing implicit WSD is limited.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9547204375267029}, {"text": "WSD", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9007155299186707}]}, {"text": "For instance, in the case of phrasebased SMT, it is the order of the language model (often between 3 and 5) and the length of n-grams in the phrase table (seldom above 5).", "labels": [], "entities": [{"text": "phrasebased SMT", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.586838036775589}]}, {"text": "In attentionbased neural MT systems, the context extends to the entire sentence, but is not specifically trained to be used for WSD.", "labels": [], "entities": [{"text": "attentionbased neural MT", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.43661438425381977}, {"text": "WSD", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9657094478607178}]}, {"text": "For instance, shows an English sentence translated into German by a baseline statistical MT, an online neural MT, and the sense-aware MT system proposed in this paper.", "labels": [], "entities": []}, {"text": "The word shot is respectively translated as Schuss (gun shot), Bild (drawing) and Aufnahme (picture) by the online NMT, the baseline system, and our sense-aware system.", "labels": [], "entities": [{"text": "NMT", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9395752549171448}]}, {"text": "The latter selects a correct sense, which is identical to the reference translation, while the first two are incorrect (especially the online NMT).: Adaptive WSD for MT: vectors from WordNet definitions (or examples) are clustered with context vectors of each occurrence (here of 'rock'), resulting in sense labels used as factors for MT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.9263213872909546}, {"text": "MT", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.9633838534355164}, {"text": "MT", "start_pos": 335, "end_pos": 337, "type": "TASK", "confidence": 0.9869735240936279}]}, {"text": "In this paper, we introduce a sense-aware statistical MT system that performs explicit WSD, and uses for this task a larger context than is accessible to state-of-the-art SMT.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.8804130554199219}, {"text": "WSD", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8910453915596008}, {"text": "SMT", "start_pos": 171, "end_pos": 174, "type": "TASK", "confidence": 0.9770656824111938}]}, {"text": "Our WSD system performs context-dependent clustering of word occurrences and is initialized with knowledge from WordNet, in the form of vector representations of definitions or examples for each sense.", "labels": [], "entities": [{"text": "context-dependent clustering of word occurrences", "start_pos": 24, "end_pos": 72, "type": "TASK", "confidence": 0.7576102018356323}, {"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9472081661224365}]}, {"text": "The labels of the resulting clusters are used as abstract source-side sense labels within a factored phrasebased SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.8324383497238159}]}, {"text": "The stages of our method are presented in, and will be explained in detail in Section 3.", "labels": [], "entities": []}, {"text": "Our results, presented in Section 5, show first that our WSD system is competitive on the SemEval 2010 WSD task, but especially that it helps SMT to increase its BLEU scores and to improve the translation of polysemous nouns and verbs, when translating from English into Chinese, German, French, Spanish or Dutch, in comparison to an SMT baseline that is not aware of word senses.", "labels": [], "entities": [{"text": "WSD", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8341038227081299}, {"text": "SemEval 2010 WSD task", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.7242688834667206}, {"text": "SMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9877331256866455}, {"text": "BLEU scores", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.9807193279266357}, {"text": "translation of polysemous nouns and verbs", "start_pos": 193, "end_pos": 234, "type": "TASK", "confidence": 0.7667540510495504}]}], "datasetContent": [{"text": "We evaluate our sense-aware SMT on the UN Corpus 7) and on the Europarl Corpus 8 ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9232915639877319}, {"text": "UN Corpus 7", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9840529163678488}, {"text": "Europarl Corpus 8", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.9925818045934042}]}, {"text": "We select 0.5 million parallel sentences for each language pair from Europarl, as shown in.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.981547474861145}]}, {"text": "We also use the smaller WIT3 Corpus 9 (), a collection of transcripts of TED talks, to evaluate the impact of costly model choices, namely the type of the resource (definition vs. examples), the length of the context window, and the k-means method (adaptive vs. original).", "labels": [], "entities": [{"text": "WIT3 Corpus 9", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9563744862874349}]}, {"text": "Before assigning sense labels, we first tokenize all the texts and identify the parts of speech (POS) using the Stanford POS tagger . Then, we filter out the stopwords and the nouns which are proper names according to the Stanford Name Entity Recognizer . Furthermore, we convert the plural forms of nouns to their singular form and the verb forms to infinitive using the stemmer and lemmatizer from NLTK 11 , which is essential because WordNet has description entries only for singular nouns and infinitive form of verbs.", "labels": [], "entities": [{"text": "NLTK 11", "start_pos": 400, "end_pos": 407, "type": "DATASET", "confidence": 0.8748553395271301}, {"text": "WordNet", "start_pos": 437, "end_pos": 444, "type": "DATASET", "confidence": 0.9442159533500671}]}, {"text": "The pre-processed text is used for assigning sense labels to each occurrence of a noun or verb which has more than one sense in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.901707649230957}]}, {"text": "For translation, we train and tune baseline and factored phrase-based models with Moses ( ).", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9844210147857666}]}, {"text": "We also carried out pilot experiments with neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.8052573899428049}]}, {"text": "However, due to the large datasets NMT requires for training, its performance was below SMT on the datasets above, and sense labels did not improve it.", "labels": [], "entities": [{"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.8158754706382751}]}, {"text": "We thus focus on SMT in what follows, and leave WSD for NMT for future studies.", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9959174990653992}, {"text": "WSD", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.4619515538215637}, {"text": "NMT", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.7572641372680664}]}, {"text": "We select the optimal model configuration based on the MT performance, measured with the traditional BLEU score (), on the WIT3 corpus for EN/ZH and EN/DE.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9625293612480164}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9990702271461487}, {"text": "WIT3 corpus", "start_pos": 123, "end_pos": 134, "type": "DATASET", "confidence": 0.9783566892147064}]}, {"text": "Unless otherwise stated, we use the following settings in the k-means algorithm, starting from the implementation provided in Scikit-learn (Pedregosa et al., 2011): \u2022 we use the definition of each sense for initializing the centroids in the adaptive k-means methods (and compare this later with using the examples); \u2022 we set kt equal tom t , i.e. the number of senses of an ambiguous word type W t ; \u2022 the window size for the context surrounding each occurrence is set to c = 8.", "labels": [], "entities": []}, {"text": "For the evaluation of intrinsic WSD performance, we use the V -metric, the F 1 -metric, and their average, as used for instance at SemEval 2010 ( . To measure the impact of WSD on MT, besides BLEU, we also measure the actual impact on the nouns and verbs that appear in WordNet with several senses, by comparing how many of them are translated as in the reference translation, by our system vs. the baseline.", "labels": [], "entities": [{"text": "WSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9478975534439087}, {"text": "F 1 -metric", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9714768975973129}, {"text": "MT", "start_pos": 180, "end_pos": 182, "type": "TASK", "confidence": 0.8844061493873596}, {"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9958869814872742}]}, {"text": "For a certain set of tokens in the source data, we note as N improved the number of tokens which are translated by our system as in the reference translation, but whose baseline translation differs from it.", "labels": [], "entities": []}, {"text": "Conversely, we note as N degraded the number of tokens which are translated by the: Statistics of the corpora used for machine translation: '\u223c' indicates a similar size, though not identical texts, because the English source texts for the different language pairs from Europarl are different.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.72074094414711}, {"text": "Europarl", "start_pos": 269, "end_pos": 277, "type": "DATASET", "confidence": 0.9710738658905029}]}, {"text": "Hence, the number of words found in WordNet differ as well.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9616492986679077}]}, {"text": "baseline system as in the reference, but differently by our system.", "labels": [], "entities": []}, {"text": "We will use the normalized coefficient \u03c1 = (N improved \u2212 N degraded )/T , where T is the total number of tokens, as a metric focusing explicitly on the words submitted to WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 171, "end_pos": 174, "type": "TASK", "confidence": 0.45138779282569885}]}], "tableCaptions": [{"text": " Table 1: Statistics of the corpora used for machine translation: '\u223c' indicates a similar size, though  not identical texts, because the English source texts for the different language pairs from Europarl are  different. Hence, the number of words found in WordNet differ as well.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.766636461019516}, {"text": "Europarl", "start_pos": 196, "end_pos": 204, "type": "DATASET", "confidence": 0.9799841046333313}, {"text": "WordNet", "start_pos": 257, "end_pos": 264, "type": "DATASET", "confidence": 0.9591331481933594}]}, {"text": " Table 2: Performance of our WSD+MT factored  system for two language pairs from WIT3, with  two initialization conditions for the k-means clus- ters, i.e. definitions or examples for each sense.", "labels": [], "entities": [{"text": "WSD+MT factored", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.5059117525815964}, {"text": "WIT3", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9405482411384583}]}, {"text": " Table 3. To offer a fair compar- ison, we set the number of clusters, in the case of  random initializations, respectively to the number  of synsets with definitions or examples, for each  word type. Clearly, our adaptive, informed initial- izations of clusters are beneficial to MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 281, "end_pos": 283, "type": "TASK", "confidence": 0.9884557723999023}]}, {"text": " Table 3: Performance of our WSD+MT factored  system for EN-ZH from WIT3, comparing the two  initialization conditions for the k-means clusters,  i.e. definitions or examples for each sense, with  random initializations.", "labels": [], "entities": [{"text": "WSD+MT factored", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.5635508745908737}, {"text": "WIT3", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.6044273972511292}]}, {"text": " Table 4: WSD results from the SemEval 2010 shared task in terms of V -score, F 1 score and their  average. Our adaptive k-means using definitions (last but one line) outperforms all the other systems on  the average of V and F 1 , when considering both nouns and verbs, or nouns only.", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6575648188591003}, {"text": "SemEval 2010 shared task", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.736243262887001}, {"text": "V -score", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9364396135012308}, {"text": "F 1 score", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9896010359128317}]}, {"text": " Table 5: BLEU scores of our WSD+MT factored system, with both noun and verb senses, along with  baseline MT and oracle WSD+MT, on five language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999377965927124}, {"text": "WSD+MT", "start_pos": 29, "end_pos": 35, "type": "TASK", "confidence": 0.7299164732297262}]}, {"text": " Table 6: BLEU scores of our WSD+MT factored system, trained separately on disambiguated nouns vs.  verbs, and tested separately or jointly, along with baseline MT and oracle WSD+MT, on five language  pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995259046554565}, {"text": "WSD+MT factored", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7698475420475006}]}, {"text": " Table 7: Detailed confusion matrix of our factored MT system and the baseline MT system with respect  to the reference on the EN/DE pair from Europarl corpus and the EN/ZH from UN corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9331587553024292}, {"text": "Europarl corpus", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.8948584496974945}, {"text": "UN corpus", "start_pos": 178, "end_pos": 187, "type": "DATASET", "confidence": 0.7911113500595093}]}]}