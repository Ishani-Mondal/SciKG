{"title": [{"text": "Reference Scope Identification for Citances Using Convolutional Neural Network", "labels": [], "entities": [{"text": "Scope Identification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6117443293333054}]}], "abstractContent": [{"text": "In the task of summarization of a scientific paper, a lot of information stands to be gained about a reference paper, from the papers that cite it.", "labels": [], "entities": [{"text": "summarization of a scientific paper", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.8688152551651}]}, {"text": "Automatically generating the reference scope (the span of cited text) in a reference paper, corresponding to citances (sentences in the citing papers that cite it) has great significance in preparing a structured summary of the reference paper.", "labels": [], "entities": []}, {"text": "We treat this task as a binary classification problem, by extracting feature vectors from pairs of citances and reference sentences.", "labels": [], "entities": []}, {"text": "These features are lexical, corpus-based, surface and knowledge-based.", "labels": [], "entities": []}, {"text": "We extend the current feature set employed for reference-citance pair identification in the current state-of-the-art system.", "labels": [], "entities": [{"text": "reference-citance pair identification", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.6252890030543009}]}, {"text": "Using these features, we present a novel classification approach for this task, that employs a deep Convolu-tional Neural Network along with two boosting ensemble algorithms.", "labels": [], "entities": []}, {"text": "We outperform the existing state-of-the-art for distinguishing between cited spans and non-cited spans of text in the reference paper.", "labels": [], "entities": []}], "introductionContent": [{"text": "Citation sentences or 'citances' that cite a reference paper (RP) can give valuable information about the larger context in which the RP is written, key ideas behind the RP and a concise synopsis of it.", "labels": [], "entities": []}, {"text": "All of this is important fora task like scientific paper summarization, which not only requires the content of a paper but also meta-information about it.", "labels": [], "entities": [{"text": "scientific paper summarization", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6042919854323069}]}, {"text": "This kind of information would otherwise have to be obtained from sources such as literature reviews and surveys about the paper, which in turn is time-consuming and laborintensive.", "labels": [], "entities": []}, {"text": "This goal has also been outlined in a recent shared task on scientific paper summarization, the 3 rd Computational Linguistics Scientific Document Summarization Shared Task 1 . The first step towards building a system that can obtain information about an RP from a citing paper (CP) that cites it, is to find spans of text in the RP that are cited by a particular citance in the CP.", "labels": [], "entities": [{"text": "scientific paper summarization", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.6194090843200684}, {"text": "3 rd Computational Linguistics Scientific Document Summarization Shared Task", "start_pos": 96, "end_pos": 172, "type": "TASK", "confidence": 0.668969472249349}]}, {"text": "In the context of the above-mentioned shared task, this first step is referred to as Task 1A.", "labels": [], "entities": []}, {"text": "Task 1A, thus offers a good foundation for the goal mentioned above, by identifying the relevant reference sentences fora citance.", "labels": [], "entities": []}, {"text": "We present a novel approach to Task 1A.", "labels": [], "entities": [{"text": "Task 1A", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.7177274227142334}]}, {"text": "While we build on previous work by, our major contributions can be described as: \u2022 We model anew feature set to represent a citance-reference sentence pair along with building a classification system that uses a binary classification technique for classifying a <CP sentence, RP sentence> pair according to whether the CP sentence cites the RP sentence or not.", "labels": [], "entities": []}, {"text": "\u2022 We show performance gains over the results of(which is the current state-of-the-art) by achieving better F1-scores, using a feature set that has lesser number of features than that used in the above work.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.998540997505188}]}, {"text": "\u2022 We explore various measures for evaluating similarity between texts while build-ing this feature set.", "labels": [], "entities": []}, {"text": "Feature representations extracted (as described later), are used to train three binary classifiers -an Adaptive Boosting Classifier (ABC), a Gradient Boosting Classifier (GBC) and a CNN classifier.", "labels": [], "entities": [{"text": "CNN", "start_pos": 182, "end_pos": 185, "type": "DATASET", "confidence": 0.866467297077179}]}, {"text": "The datasets provided for this year's as well as last year's shared task have been used.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the development corpus, the training corpora and the test corpus provided for the CL-SciSumm Shared Tasks 2016 4 and 2017 . As reported in, each corpus comprises 10 reference articles, their citing papers and annotation files for each reference article.", "labels": [], "entities": [{"text": "CL-SciSumm Shared Tasks 2016 4", "start_pos": 89, "end_pos": 119, "type": "DATASET", "confidence": 0.8636280655860901}]}, {"text": "The citation annotations specify citances, their associated reference text and the discourse facet that it represents.", "labels": [], "entities": []}, {"text": "Precision, Recall and F1-Score are used as evaluation metrics.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9949793219566345}, {"text": "Recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9972614049911499}, {"text": "F1-Score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9990636706352234}]}, {"text": "The average score on all topics in the test corpus is reported.", "labels": [], "entities": []}, {"text": "We run experiments on two separate training sets.", "labels": [], "entities": []}, {"text": "In the first run, we use data only from the 2016 shared task, and not from the 2017 shared task.", "labels": [], "entities": []}, {"text": "This is because we need a common ground for comparison with the existing state-of-the-art (), which used this dataset.", "labels": [], "entities": []}, {"text": "We first train our data on the training set, and tune the CNN's hyperparameters on the development set.", "labels": [], "entities": [{"text": "CNN", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9534180760383606}]}, {"text": "We then augment the training data and the development data to train the final models.", "labels": [], "entities": []}, {"text": "We test our model on the test provided as part of this dataset.", "labels": [], "entities": []}, {"text": "shows the performance of the CNN model on this test set, and compares it with the existing state-ofart and another well-performing model.", "labels": [], "entities": []}, {"text": "We have reported only the CNN's performance in this table as (as will be seen in the results of the second run), this is a better performing model than ABC and GBC, in our experimental setup.", "labels": [], "entities": [{"text": "GBC", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.8171448707580566}]}, {"text": "In the second run, we make use of the datasets from both 2016 and 2017.", "labels": [], "entities": []}, {"text": "Both the training datasets are augmented to form the initial training set.", "labels": [], "entities": []}, {"text": "After tuning the CNN's hyperparameters on the development set (which is the same for both, the initial training and development sets are augmented to form the final training set.", "labels": [], "entities": []}, {"text": "Grid search algorithm, as given by, over 10-fold cross validation is used to find the best model parameters for ABC and GBC listed in Table 1.", "labels": [], "entities": [{"text": "GBC", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.8255615234375}]}, {"text": "Since the gold-standard annotations for the 2017 test set were not yet available at the time of conducting our experiments, we use only the test set of 2016.", "labels": [], "entities": []}, {"text": "We report performance of ABC, GBC as well as the CNN classifier on this test set.", "labels": [], "entities": [{"text": "ABC", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.911060631275177}, {"text": "GBC", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.7956699728965759}, {"text": "CNN", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9339854717254639}]}], "tableCaptions": [{"text": " Table 3: Results obtained by different  models", "labels": [], "entities": []}]}