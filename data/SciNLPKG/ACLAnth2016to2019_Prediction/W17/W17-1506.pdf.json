{"title": [{"text": "Multi-source annotation projection of coreference chains: assessing strategies and testing opportunities", "labels": [], "entities": [{"text": "Multi-source annotation projection of coreference chains", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.6469296465317408}]}], "abstractContent": [{"text": "In this paper, we examine the possibility of using annotation projection from multiple sources for automatically obtaining coreference annotations in the target language.", "labels": [], "entities": []}, {"text": "We implement a multi-source annotation projection algorithm and apply it on an English-German-Russian parallel corpus in order to transfer coreference chains from two sources to the target side.", "labels": [], "entities": []}, {"text": "Operating in two settings-a low-resource and a more linguistically-informed one-we show that automatic coreference transfer could benefit from combining information from multiple languages, and assess the quality of both the extraction and the linking of target coreference mentions.", "labels": [], "entities": [{"text": "coreference transfer", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.8968305587768555}]}], "introductionContent": [{"text": "While monolingual coreference resolution systems are being constantly improved, multilingual coreference resolution has received much less attention in the NLP community.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8749692738056183}, {"text": "coreference resolution", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.7497407495975494}]}, {"text": "Most of the coreference systems can only work on English data and are not ready to be adapted to other languages.", "labels": [], "entities": []}, {"text": "Developing a coreference resolution system fora new language from scratch is challenging due to its technical complexity and the variability of coreference phenomena in different languages, and it depends on high-quality language technologies (such as mention extraction, syntactic parsing, named entity recognition) as well as gold standard data, which are not available fora wide range of languages.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.9068867266178131}, {"text": "mention extraction", "start_pos": 252, "end_pos": 270, "type": "TASK", "confidence": 0.7195663452148438}, {"text": "syntactic parsing", "start_pos": 272, "end_pos": 289, "type": "TASK", "confidence": 0.7201792597770691}, {"text": "named entity recognition", "start_pos": 291, "end_pos": 315, "type": "TASK", "confidence": 0.6117293238639832}]}, {"text": "However, this can be alleviated by using crosslingual projection which allows for transferring existing methods or resources across languages.", "labels": [], "entities": [{"text": "crosslingual projection", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7035488784313202}]}, {"text": "There have been some influential work on annotation projection for different NLP tasks which performed quite well cross-lingually, e.g. for semantic role labelling ( or syntactic parsing (.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7191362231969833}, {"text": "semantic role labelling", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.6829768518606821}, {"text": "syntactic parsing", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.7581937909126282}]}, {"text": "At the same time, several recent studies on annotation projection for coreference have proven it to be a more difficult task than POS tagging or syntactic parsing, which is hard to be tackled by projection algorithms.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7262844294309616}, {"text": "coreference", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.885339617729187}, {"text": "POS tagging", "start_pos": 130, "end_pos": 141, "type": "TASK", "confidence": 0.828999400138855}, {"text": "syntactic parsing", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.7380770146846771}]}, {"text": "These works are limited to the existing multilingual resources (mostly newswire, mostly CoNLL 2012 () and, surprisingly, are not even able to beat a threshold of 40.0 F1 for coreference resolvers trained on projections only.", "labels": [], "entities": [{"text": "CoNLL 2012", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.9621495604515076}, {"text": "F1", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.9508041143417358}, {"text": "coreference resolvers", "start_pos": 174, "end_pos": 195, "type": "TASK", "confidence": 0.8432352542877197}]}, {"text": "The best-performing system based on projection achieves 38.82 for English-Spanish and 37.23 for English-Portuguese F1-score, while state-of-the-art monolingual coreference systems are already able to achieve 64.21 Fscore for English (.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9773331880569458}, {"text": "Fscore", "start_pos": 214, "end_pos": 220, "type": "METRIC", "confidence": 0.9901314973831177}]}, {"text": "While being quite powerful for other tasks, annotation projection is less successful for coreference resolution.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7532934844493866}, {"text": "coreference resolution", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.9739179611206055}]}, {"text": "Therefore, our question is, how can the quality of annotation projection be improved for the task of coreference resolution?", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7354568541049957}, {"text": "coreference resolution", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.9699429571628571}]}, {"text": "In our opinion, projection from multiple source languages can be a long-term solution, assuming that we have access to two or more reliable coreference resolvers on the source sides.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.8625705540180206}]}, {"text": "Our idea is that multi-source annotation projection for coreference resolution would grant a bigger pool of potential mentions to choose from, which can be beneficial for overcoming language divergences.", "labels": [], "entities": [{"text": "multi-source annotation projection", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.6399048467477163}, {"text": "coreference resolution", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.9659896194934845}]}, {"text": "Therefore, the main goals of this study are: (a) to explore different strategies of multi-source projection of coreference chains on a small experimental corpus, and (b) to evaluate the projection errors and assess the prospects of this approach for multilingual coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 263, "end_pos": 285, "type": "TASK", "confidence": 0.8071406185626984}]}, {"text": "This paper is structured as follows: The related work is discussed in Section 2, and the dataset is presented in Section 3.", "labels": [], "entities": []}, {"text": "The methodology adapted 41 for our experiments is explained in Section 4.", "labels": [], "entities": []}, {"text": "We then analyse the projection errors and evaluate the target annotations (Section 5).", "labels": [], "entities": []}, {"text": "Finally, Section 6 summarises the outcomes of this study, and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Combining information coming from two or more languages is a more challenging task as compared to single-source projection where one just transfers all the information from one language to the other.", "labels": [], "entities": []}, {"text": "For coreference, this task is non-trivial (as opposed to, for instance, multi-source projection of POS information where an intuitive majority voting strategy could be chosen), since we cannot operate on the token level and even not on the mention level: We cannot implement a strategy to choose e.g. the most frequent label fora token or a sequence of tokens (coreferent/non-coreferent), since they belong to mention clusters which are not aligned on the source sides.", "labels": [], "entities": []}, {"text": "In other words, if mention x a belongs to chain A in the first source language and mention y b belongs to chain B in the second source language, and they are projected onto the same mention z ab on the target side, we do not know whether both target chains A and B projected from A and B respectively and both containing the mention in question are equal or not, as we cannot rely on chain IDs which are not common across languages.", "labels": [], "entities": []}, {"text": "Therefore, we have to operate on the chain level and first compare projected coreference chains.", "labels": [], "entities": []}, {"text": "We treat coreference chains as clusters, measure the similarity between them and use this information to choose between them or combine them together in the projection.", "labels": [], "entities": []}, {"text": "Projecting coreference chains (=clusters of mentions) from more than one language, we can have the following cases: While cases (a) and (b) are quite straightforward, case (c) is more difficult since we have to determine whether to treat these chains as being equal or not.", "labels": [], "entities": []}, {"text": "Following the work of (Rasooli and Collins, 2015), we rely upon two strategies -concatenation and voting -to process coreference chains coming from two sources.", "labels": [], "entities": []}, {"text": "Since we only have two sources, instead of voting we implement intersection.", "labels": [], "entities": []}, {"text": "In the case of coreference, we can enrich annotations from one language with the annotations from the other one or create a completely new set out of two projection sets.", "labels": [], "entities": []}, {"text": "In particular, we experiment with several naive methods and evaluate their quality, and then combine them with each other in order to find the optimal strategy.", "labels": [], "entities": []}, {"text": "We implement the following methods: (1) Concatenation: Data is obtained from each of the languages separately and then concatenated.", "labels": [], "entities": []}, {"text": "We use the following formula to estimate the overlap between two coreference chains: where A and B are the number of mentions for coreference chains in question.", "labels": [], "entities": []}, {"text": "We experiment with different values of overlap and choose the best one for each of the methods 5 . For u-int, we perform intersection of mentions for all the chains with mention overlap over 0.05.", "labels": [], "entities": []}, {"text": "For u-con, we select chains with 0.5 overlap value for German and 0.7 for Russian.", "labels": [], "entities": [{"text": "overlap", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9548201560974121}]}, {"text": "If the overlap is less than these values, we treat these chains as disjoint.", "labels": [], "entities": []}, {"text": "Each of the methods is applied in the following settings: 1.", "labels": [], "entities": []}, {"text": "Setting 1: no additional linguistic information available.", "labels": [], "entities": []}, {"text": "In this setting, we use only word alignments to transfer information from one language to the other.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7192989587783813}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics for English, German and Russian", "labels": [], "entities": []}, {"text": " Table 2: Results for German", "labels": [], "entities": []}, {"text": " Table 4: Distribution of all projected markables by type for u-int and u-con methods", "labels": [], "entities": []}, {"text": " Table 5: Projection accuracy for u-int and u-con methods", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9759327173233032}]}]}