{"title": [{"text": "A Biomedical Question Answering System in BioASQ 2017", "labels": [], "entities": [{"text": "Biomedical Question Answering", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.735447883605957}]}], "abstractContent": [{"text": "Question answering, the identification of short accurate answers to users questions, is a longstanding challenge widely studied over the last decades in the open-domain.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9183704257011414}, {"text": "identification of short accurate answers to users questions", "start_pos": 24, "end_pos": 83, "type": "TASK", "confidence": 0.8558100089430809}]}, {"text": "However, it still requires further efforts in the biomedical domain.", "labels": [], "entities": []}, {"text": "In this paper, we describe our participation in phase B of task 5b in the 2017 BioASQ challenge using our biomedical question answering system.", "labels": [], "entities": [{"text": "biomedical question answering", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.639079878727595}]}, {"text": "Our system, dealing with four types of questions (i.e., yes/no, factoid, list, and summary), is based on (1) a dictionary-based approach for generating the exact answers of yes/no questions , (2) UMLS metathesaurus and term frequency metric for extracting the exact answers of factoid and list questions, and (3) the BM25 model and UMLS concepts for retrieving the ideal answers (i.e., paragraph-sized summaries).", "labels": [], "entities": [{"text": "BM25", "start_pos": 317, "end_pos": 321, "type": "DATASET", "confidence": 0.8101403713226318}]}, {"text": "Preliminary results show that our system achieves good and competitive results in both exact and ideal answers extraction tasks as compared with the participating systems.", "labels": [], "entities": [{"text": "answers extraction tasks", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.782682071129481}]}], "introductionContent": [{"text": "Finding accurate answers to biomedical questions written in natural language from the biomedical literature is the key to creating high-quality systematic reviews that support the practice of evidence-based medicine ( and improve the quality of patient care).", "labels": [], "entities": []}, {"text": "However, with the large and increasing volume of textual data in the biomedical domain makes it difficult to absorb all relevant information).", "labels": [], "entities": []}, {"text": "Since time and quality are of the essence in finding answers to biomedical questions, developing and improving question answering systems are desirable.", "labels": [], "entities": [{"text": "question answering", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7826234996318817}]}, {"text": "Question answering (QA) systems aim at directly producing and providing short precise answers to users questions by automatically analyzing thousands of articles using information extraction and natural language processing methods.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8967396974563598}]}, {"text": "Although different types of QA systems have different architectures, most of them, especially in the biomedical domain, follow a framework in which (1) question classification and query formulation, (2) document retrieval, (3) passage retrieval, and (4) answer extraction components play a vital role.", "labels": [], "entities": [{"text": "question classification", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.7880015671253204}, {"text": "query formulation", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.7269488573074341}, {"text": "document retrieval", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.7304437905550003}, {"text": "passage retrieval", "start_pos": 227, "end_pos": 244, "type": "TASK", "confidence": 0.8364890217781067}, {"text": "answer extraction", "start_pos": 254, "end_pos": 271, "type": "TASK", "confidence": 0.8405067324638367}]}, {"text": "Question answering in the open-domain is a longstanding challenge widely studied over the last decades).", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9306782782077789}]}, {"text": "However, it still remains areal challenge in the biomedical domain.", "labels": [], "entities": []}, {"text": "As has been extensively documented in the recent research literature, open-domain QA is concerned with questions which were not restricted to any domain, while in restricted-domain QA such as the biomedical one, the domain of application provides a context for the QA process.", "labels": [], "entities": [{"text": "QA process", "start_pos": 265, "end_pos": 275, "type": "TASK", "confidence": 0.9018330276012421}]}, {"text": "Additionally, report the following characteristics for QA in the biomedical domain: (1) large-sized textual corpora, (2) highly complex domain-specific terminology, and (3) domain specific format and typology of questions.", "labels": [], "entities": [{"text": "QA", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9547083377838135}]}, {"text": "Since the launch of the biomedical QA track at the BioASQ 1 challenge (, various approaches in biomedical QA have been presented.", "labels": [], "entities": [{"text": "BioASQ 1 challenge", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.7306333780288696}]}, {"text": "The BioASQ challenge, within 2017 edition, comprised three tasks: (1) task 5a on large-scale online biomedical semantic indexing, (2) task 5b on biomedical semantic QA, and (3) Figure 1: Overall architecture of the proposed biomedical question-answering system task 5c on funding information extraction from biomedical literature.", "labels": [], "entities": [{"text": "biomedical semantic indexing", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.6423761546611786}, {"text": "funding information extraction from biomedical literature", "start_pos": 272, "end_pos": 329, "type": "TASK", "confidence": 0.7746802866458893}]}, {"text": "Task 5b consists of two phases: In phase A, BioASQ released questions in English from benchmark datasets.", "labels": [], "entities": [{"text": "BioASQ released questions in English from benchmark datasets", "start_pos": 44, "end_pos": 104, "type": "DATASET", "confidence": 0.5035443976521492}]}, {"text": "There were four types of questions: yes/no, factoid, list and summary questions ().", "labels": [], "entities": []}, {"text": "Participants had to respond with relevant concepts, relevant documents, relevant snippets retrieved from the relevant documents, and relevant RDF triples.", "labels": [], "entities": []}, {"text": "In phase B, the released questions contained the golden answers for the required elements (documents and snippets) of the first phase.", "labels": [], "entities": []}, {"text": "The participants had to answer with exact answers (e.g., biomedical entity, number, list of biomedical entities, yes, no, etc.) as well as with ideal answers (i.e., paragraph-sized summaries) (.", "labels": [], "entities": []}, {"text": "In this paper, we describe our participation in the phase B (i.e., exact and ideal answers) of task5b in the 2017 BioASQ challenge.", "labels": [], "entities": []}, {"text": "In our biomedical QA system, we have used (1) a dictionary-based approach to generate the exact answers to yes/no questions, (2) the unified medical language system (UMLS) metathesaurus and term frequency metrics for extracting the exact answers of factoid and list questions, and (3) the BM25 model and UMLS concepts for retrieving the ideal answers.", "labels": [], "entities": [{"text": "BM25", "start_pos": 289, "end_pos": 293, "type": "DATASET", "confidence": 0.7684957385063171}, {"text": "UMLS", "start_pos": 304, "end_pos": 308, "type": "DATASET", "confidence": 0.8244330286979675}]}, {"text": "illustrates the generic architecture of our biomedical QA system.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces related work and discussion about the main biomedical QA approaches with a particular focus on BioASQ participants.", "labels": [], "entities": []}, {"text": "Section 3 describes the answer extraction methods used in our biomedical QA system.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8612940311431885}]}, {"text": "Section 4 presents the preliminary results we obtained in the 2017 BioASQ challenge.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.5022633224725723}]}, {"text": "Finally, the conclusion and future work are made in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the preliminary results we obtained in BioASQ 2017.", "labels": [], "entities": [{"text": "BioASQ 2017", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.8567294776439667}]}, {"text": "We first introduce the evaluation metrics, then give the experimental results, and finally discuss the results.", "labels": [], "entities": []}, {"text": "The evaluation metrics used for the exact answers in phase B of task 5b are accuracy, strict accuracy and lenient accuracy, mean reciprocal rank (MRR), mean precision, mean recall, and mean F-measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9993465542793274}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.6618951559066772}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9280887246131897}, {"text": "mean reciprocal rank (MRR)", "start_pos": 124, "end_pos": 150, "type": "METRIC", "confidence": 0.9154607057571411}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.7746787667274475}, {"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9715448617935181}, {"text": "F-measure", "start_pos": 190, "end_pos": 199, "type": "METRIC", "confidence": 0.7982755899429321}]}, {"text": "Accuracy, MRR and F-measure are the official measures used for evaluating the exact answers of yes/no, factoid and list questions, respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9955061674118042}, {"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9954924583435059}, {"text": "F-measure", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.998868465423584}]}, {"text": "ROUGE-2 and ROUGE-SU4, on the other hand, are the main measures for an automatic evaluation of ideal answers.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9005890488624573}, {"text": "ROUGE-SU4", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.8935249447822571}]}, {"text": "Details of these evaluations metrics appear in ().", "labels": [], "entities": []}, {"text": "highlights the preliminary results of our system in phase B (i.e., exact and ideal answers) of BioASQ task 5b.", "labels": [], "entities": [{"text": "BioASQ task 5b", "start_pos": 95, "end_pos": 109, "type": "DATASET", "confidence": 0.5931023955345154}]}, {"text": "More details on the results can be found in the BioASQ website 4 .", "labels": [], "entities": [{"text": "BioASQ website 4", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.9431168039639791}]}], "tableCaptions": [{"text": " Table 1: The primarily results of our system in phase B of BioASQ task 5b. P, R, and F indicate  precision, recall, and F-measure, respectively. The values inside parameters indicate our current rank  and the total number of submissions for the batch.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9994896650314331}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9979974627494812}, {"text": "F-measure", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9920386075973511}]}]}