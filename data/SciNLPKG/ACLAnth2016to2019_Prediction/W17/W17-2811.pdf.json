{"title": [{"text": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction", "labels": [], "entities": []}], "abstractContent": [{"text": "Recognition of social signals, from human facial expressions or prosody of speech, is a popular research topic in human-robot interaction studies.", "labels": [], "entities": [{"text": "Recognition of social signals", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9052300602197647}]}, {"text": "There is also along line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics.", "labels": [], "entities": []}, {"text": "However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot.", "labels": [], "entities": []}, {"text": "In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users' impressions of the robot after a conversation.", "labels": [], "entities": []}, {"text": "We find that happiness in the user's recognised facial expression strongly correlates with likeabil-ity of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent.", "labels": [], "entities": []}, {"text": "In addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and inter-pretability.", "labels": [], "entities": []}, {"text": "As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learning-based adaptive human-robot dialogue systems.", "labels": [], "entities": [{"text": "Reinforcement Learning-based adaptive human-robot dialogue", "start_pos": 92, "end_pos": 150, "type": "TASK", "confidence": 0.8520904064178467}]}, {"text": "Figure 1: Left: a live view of experimental setup showing a participant interacting with Pepper.", "labels": [], "entities": []}, {"text": "Right: a diagram of experimental setup showing the participant (green) and the robot (white) positioned face to face.", "labels": [], "entities": []}, {"text": "The scene was recorded by cameras (triangles C) from the robot's perspective focusing on the face of the participant and from the side, showing the whole scene.", "labels": [], "entities": []}, {"text": "The experimenter (red) was seated behind a divider.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social signals, such as emotional expressions, play an important role in human-human interaction, thus they are increasingly recognised as an important factor to be considered both in human-robot interaction research ( and in the area of spoken dialogue systems (.", "labels": [], "entities": []}, {"text": "Recognition of human social signals has become a popular topic in Human-Robot Interaction (HRI) in recent years.", "labels": [], "entities": [{"text": "Recognition of human social signals", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8972841262817383}, {"text": "Human-Robot Interaction (HRI)", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.8090546607971192}]}, {"text": "Social signals are recognized well from human facial expressions or prosodic features of speech, and have become the most popular methods for recognising human affective signals in human-robot interaction.", "labels": [], "entities": []}, {"text": "In human-robot interaction, recognized human emotions are mostly used for mimicking human behaviour and enhancing the empathy towards a robot both in children) and in adult users.", "labels": [], "entities": []}, {"text": "In the area of spoken dialogue systems, signals recognised from linguistic cues and prosody have been used to detect problematic dialogues and to assess dialogue quality as a whole (.", "labels": [], "entities": []}, {"text": "This type of dialogue-related signals has also been used to automatically detect miscommunication (, or to predict the user satisfaction (.", "labels": [], "entities": []}, {"text": "However, there is very little research combining the areas of detecting multi-modal signals during spoken HRI and evaluation of human-robot conversation, and using them to create an adaptive social dialogue.", "labels": [], "entities": []}, {"text": "In this paper, we make a first step towards building a multi-modally-rich, conversational, and human-like robotic agent, potentially able to react to the changes inhuman behaviour during face-toface dialogue and able to adjust the dialogue strategy in order to improve an interlocutor's impression.", "labels": [], "entities": []}, {"text": "We present a setup that targets the development of a dialogue system to explore verbal and non-verbal conversational cues in a face-to-face situated dialogue with asocial robot.", "labels": [], "entities": []}, {"text": "We show that different emotional facial expressions of a human interlocutor, in combination with prosodic characteristics of human speech and features of humanrobot dialogue, correlate strongly with users' perceptions of a robot after a conversation.", "labels": [], "entities": []}, {"text": "Based on these features, we developed a model capable of predicting potential human ratings of a robot and discuss its implications for future work in developing adaptive human-robot dialogue systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "The human-robot dialogue system was evaluated via a user study in which human subjects interacted with a Pepper robot 1 acting autonomously using the system described in ( ).", "labels": [], "entities": []}, {"text": "The dialogue system used, combines task-based with chat-based dialogue features, deciding the most appropriate action on each consequent turns, using a pre-trained Reinforcement Learning (RL) policy.", "labels": [], "entities": []}, {"text": "The robot decides among a pool of possible actions at \u2208 A where A = [PerformTask, Greet, Goodbye, Chat, GiveDirections, Wait, RequestTask, RequestShop].", "labels": [], "entities": []}, {"text": "If a task is recognised in the user utterance (e.g. \"where can I find discounts\"), a response is synthesized using database lookup and predefined utterances (like the example shown in).", "labels": [], "entities": []}, {"text": "If no task was recognised, then the user request is being forwarded to a Chatbot, written in AIML and based on the chatbot Rosie 2 , where a chat-style response is formulated based on AIML template/ pattern matching.", "labels": [], "entities": []}, {"text": "All interactions were in English.", "labels": [], "entities": []}, {"text": "The physical setup of the experiment can be seen in.", "labels": [], "entities": []}, {"text": "The task and the setup chosen in the study were considered as first steps towards understanding how a humanoid social robot should behave in the context of a shopping mall while also providing useful information to the mall's visitors.", "labels": [], "entities": []}, {"text": "To this end, participants were asked to imagine that they were entering a shopping mall they had never been to before where the robot was installed in the entry area interacting with visitors one at a time.", "labels": [], "entities": []}, {"text": "Participants were asked to complete as many as possible of the following five tasks: \u2022 Get information from the robot on whereto get a coffee.", "labels": [], "entities": []}, {"text": "\u2022 Get information from the robot on whereto buy clothes.", "labels": [], "entities": []}, {"text": "\u2022 Get the directions to the clothing shop of their choice.", "labels": [], "entities": []}, {"text": "\u2022 Find out if there are any current sales or discounts in the shopping mall and try to get a voucher from the robot.", "labels": [], "entities": []}, {"text": "\u2022 Make a selfie with the robot.", "labels": [], "entities": []}, {"text": "Instructions were given to use natural language spontaneously while interacting with the robot.", "labels": [], "entities": []}, {"text": "41 people (13 females, 28 males) participated in our study, ranging in age from 18 to 38 (M=24.46, SD=4.72).", "labels": [], "entities": [{"text": "M", "start_pos": 90, "end_pos": 91, "type": "METRIC", "confidence": 0.9909668564796448}, {"text": "SD", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.8864518404006958}]}, {"text": "The majority of them were students (93% students and 7% staff) that had no or little previous experience with robots (56% with little or no experience, 39% with some experience, and 5% with a lot of experience).", "labels": [], "entities": []}, {"text": "Participants were initially given a briefing script describing the goal of the task and providing hints on how to better communicate with the robot, e.g. \"wait for your turn to speak\" and \"please keep in mind that the robot only listens to you while its eyes are blinking blue\" . We reassured our participants that we were testing the robot, not them, and controlled environment-introduced biases by avoiding non-task-related distractions during the experiment.", "labels": [], "entities": []}, {"text": "During experimental sessions, participants stood in front of the robot and the experimenter was hidden in another corner of the room but available in case the participant would need any help (see.", "labels": [], "entities": []}, {"text": "At the end of the experiment participants were debriefed and received a \u00a310 gift voucher.", "labels": [], "entities": []}, {"text": "The duration of each session did not exceed thirty minutes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptive statistics of emotional,  prosodic, non-linguistic and linguistic dialogue  features for human and robot actors. Here, bold  indicates a higher value, ** denotes p < 0.01, *  denotes p < 0.05", "labels": [], "entities": []}, {"text": " Table 2: Results of linguistic dialogue analysis. *  denotes p < 0.05.", "labels": [], "entities": [{"text": "linguistic dialogue analysis", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.8297449151674906}]}, {"text": " Table 6: Performance of prediction, calculated using root-mean-square error (RMSE). The results are  averaged over all the ratings that belong to the group and outperform the baseline. Bold denotes the  smallest average error and means the best predicted result of the model.", "labels": [], "entities": [{"text": "root-mean-square error (RMSE)", "start_pos": 54, "end_pos": 83, "type": "METRIC", "confidence": 0.8408532857894897}]}]}