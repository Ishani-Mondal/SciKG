{"title": [{"text": "Assessing the performance of Olelo, a real-time biomedical question answering application", "labels": [], "entities": [{"text": "Olelo", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.7303011417388916}, {"text": "biomedical question answering", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.6331187585989634}]}], "abstractContent": [{"text": "Question answering (QA) can support physicians and biomedical researchers to find answers to their questions in the scientific literature.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9384280323982239}]}, {"text": "Such systems process large collections of documents in real time and include many natural language processing (NLP) procedures.", "labels": [], "entities": []}, {"text": "We recently developed Olelo, a QA system for biomedicine which includes various NLP components, such as question processing, document and passage retrieval, answer processing and multi-document summarization.", "labels": [], "entities": [{"text": "Olelo", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.7265559434890747}, {"text": "question processing", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8114221096038818}, {"text": "document and passage retrieval", "start_pos": 125, "end_pos": 155, "type": "TASK", "confidence": 0.593118853867054}, {"text": "answer processing", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.8940927684307098}, {"text": "multi-document summarization", "start_pos": 179, "end_pos": 207, "type": "TASK", "confidence": 0.652326375246048}]}, {"text": "In this work, we present an evaluation of our system on the the fifth BioASQ challenge.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.670893520116806}]}, {"text": "We participated with the current state of the application and with an extension based on semantic role labeling that we are currently investigating.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.6113055149714152}]}, {"text": "In addition to the BioASQ evaluation, we compared our system to other on-line biomedical QA systems in terms of the response time and the quality of the answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) is the task of automatically answering questions posed by users.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9270524859428406}]}, {"text": "As opposed to information retrieval (IR), input is in the form of natural language, e.g., English, instead of keywords, and answers are provided as short answers, instead of presenting a list of relevant documents.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.8712476968765259}]}, {"text": "Therefore, QA systems need to rely on various natural language processing (NLP) components, such as question understanding, namedentity recognition (NER), document and passage retrieval, answer extraction and multi-document summarization, among others.", "labels": [], "entities": [{"text": "question understanding", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.8317546248435974}, {"text": "namedentity recognition (NER)", "start_pos": 124, "end_pos": 153, "type": "TASK", "confidence": 0.8095581591129303}, {"text": "document and passage retrieval", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.5890343710780144}, {"text": "answer extraction", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.8645933866500854}, {"text": "multi-document summarization", "start_pos": 209, "end_pos": 237, "type": "TASK", "confidence": 0.6516257524490356}]}, {"text": "QA systems have been developed for many domains, including biomedicine (.", "labels": [], "entities": []}, {"text": "Given the large collection of biomedical documents, e.g., in PubMed, researchers and physicians need to obtain answers for their various questions in a timely manner.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.9641302227973938}]}, {"text": "Much research has been published in the past for biomedical QA, but focus was previously mainly on clinical documents.", "labels": [], "entities": [{"text": "biomedical QA", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.6943172216415405}]}, {"text": "QA for biomedicine has recently gained importance owing to the BioASQ challenges (), for which the organizers created comprehensive datasets of questions, answers and intermediate results.", "labels": [], "entities": []}, {"text": "The BioASQ challenge considers four types of questions: (i) yes/no, (ii) factoid, (iii) list and (iv) summary.", "labels": [], "entities": []}, {"text": "For yes/no questions, a system should return either of the two answers, factoid and list questions expect one or more short answers, e.g., a gene name, while a short paragraph should be generated as answer for summary questions.", "labels": [], "entities": []}, {"text": "Despite the accessibility of these datasets to support development and evaluation of QA systems for biomedicine, few QA applications are currently available on-line.", "labels": [], "entities": []}, {"text": "We recently developed Olelo 1 , a QA system for biomedicine (.", "labels": [], "entities": []}, {"text": "It relies on a local index of the Medline documents, includes domain terminologies and implements algorithms specifically designed for biomedical QA.", "labels": [], "entities": [{"text": "Medline documents", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9841530025005341}, {"text": "biomedical QA", "start_pos": 135, "end_pos": 148, "type": "TASK", "confidence": 0.6653596460819244}]}, {"text": "Previous versions of our system were evaluated in the last three editions of the BioASQ challenges (.", "labels": [], "entities": [{"text": "BioASQ challenges", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.5822788178920746}]}, {"text": "In this work, we perform a comprehensive evaluation of our application, both automatically, during participation in the fifth edition of the BioASQ challenge, as well as manually, by checking our answers against the gold standard ones from BioASQ benchmarks.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.6345726549625397}, {"text": "BioASQ benchmarks", "start_pos": 240, "end_pos": 257, "type": "DATASET", "confidence": 0.9133590757846832}]}], "datasetContent": [{"text": "In this section we present results for both Olelo and the SRL approach.", "labels": [], "entities": [{"text": "Olelo", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.9084236025810242}, {"text": "SRL", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.6360949873924255}]}, {"text": "These are the official results that were made available and based on the official metrics that are described in the guidelines 2 . presents the results for phase A based on mean average precision (MAP).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 173, "end_pos": 201, "type": "METRIC", "confidence": 0.9382988711198171}]}, {"text": "For this phase, we provide results only for document and passage retrieval.", "labels": [], "entities": [{"text": "document and passage retrieval", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.522719606757164}]}, {"text": "We simply provide the top 10 documents and passages as returned by Olelo for each question, following the maximum number of documents and snippets which is specified in the BioASQ's guidelines.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 173, "end_pos": 179, "type": "DATASET", "confidence": 0.7926542162895203}]}, {"text": "presents results of Olelo and the SRL approach for the exact answers of Phase B. We only provide results for yes/no questions using the SRL approach as this question type is not supported by Olelo.", "labels": [], "entities": [{"text": "Olelo", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.8303487300872803}, {"text": "SRL", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.6561892628669739}]}, {"text": "For the first batch, we had two submissions for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8601143956184387}]}, {"text": "SRL2 considers the detection of enumerations for list questions and fixes some minor bugs regarding the retrieval of ideal answers.", "labels": [], "entities": [{"text": "SRL2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8841120600700378}]}, {"text": "The results in batch 1, SRL2 shows a significant improvement for list questions.", "labels": [], "entities": [{"text": "SRL2", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.4686698913574219}]}, {"text": "Given that SRL2 was an improvement of SRL, we did not submit the latter from the second batch on.", "labels": [], "entities": [{"text": "SRL2", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.7717597484588623}, {"text": "SRL", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.7725991010665894}]}, {"text": "For yes/no questions we did not measure any achievements in comparison to the approach of just saying \"yes\" to any question.", "labels": [], "entities": []}, {"text": "The training data from recent years was very \"yes\"-biased and subsequently was our system.", "labels": [], "entities": []}, {"text": "The results imply that this must have changed for the 4th and 5th batch.", "labels": [], "entities": []}, {"text": "The results for factoid questions based on SRL were constantly lower than the Olelo system, but they both reached a similar magnitude, which indicates a potential fora combination of both.", "labels": [], "entities": [{"text": "Olelo system", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.810317188501358}]}, {"text": "For list questions, the SRL approach achieved much higher F-Measure scores than Olelo.", "labels": [], "entities": [{"text": "SRL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8370396494865417}, {"text": "F-Measure", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9980859756469727}]}, {"text": "However, it should be noted that the Olelo QA system was performing its own passage retrieval and was not simply relying on the gold standard snippets provided by the challenge.", "labels": [], "entities": [{"text": "Olelo QA system", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.8850254416465759}, {"text": "passage retrieval", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8898966312408447}]}, {"text": "Finally, presents our results for Olelo and the SRL approach for the ideal answers, i.e., custom summaries.", "labels": [], "entities": [{"text": "Olelo", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8845948576927185}, {"text": "SRL", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.5629817843437195}]}, {"text": "These summaries should be provided for all questions, independent of their type.", "labels": [], "entities": []}, {"text": "The difference between the Olelo and the Olelo-GS submissions is that the later relies on the gold standard (GS) snippets, instead of the ones retrieved by the system.", "labels": [], "entities": [{"text": "Olelo", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8738656044006348}]}, {"text": "As expected, the Olelo-GS submissions usually obtained a higher score than the Olelo ones, but difference was lower than our expectations.", "labels": [], "entities": [{"text": "Olelo-GS", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8289582133293152}, {"text": "difference", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9945553541183472}]}, {"text": "The SRL-based approaches obtained much lower scores than Olelo runs.", "labels": [], "entities": []}, {"text": "All Rouge metrics for the SRL approach were below 10%, which can be explained by the fact that it was basically just an answer snippet selection approach.", "labels": [], "entities": [{"text": "SRL", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9733836650848389}]}, {"text": "As of the date of the BioASQ submissions, our experiments on SRL were still in a preliminary phase.", "labels": [], "entities": [{"text": "BioASQ submissions", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.6873967945575714}, {"text": "SRL", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9781750440597534}]}, {"text": "For the specific case of list questions, we could already show how a biomedical QA system could benefit from SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9501994848251343}]}, {"text": "However, in general, we got the impression that SRL should not be used to design a QA system from scratch (as we tried in our experiments) but to improve our existing approaches.", "labels": [], "entities": [{"text": "SRL", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.7119103074073792}]}, {"text": "A major problem of our SRL approach was its coverage: if no matching labels fora question were found, we need an alternative approach to it.", "labels": [], "entities": [{"text": "SRL", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9853982925415039}]}, {"text": "Otherwise, the recall will be too low, as experienced in our experiments.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9983327984809875}]}, {"text": "For list questions, considering enumerations as a baseline approach was very helpful.", "labels": [], "entities": []}, {"text": "For yes/no questions, more sophisticated detection strategies based on negation might be applied to find out when the answer is \"no\" with higher precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9829369187355042}]}, {"text": "There might be further potential when analyzing occurrences of double negation or other sophisticated contextual information.", "labels": [], "entities": [{"text": "double negation", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.6624249517917633}]}, {"text": "A less \"yes\"-biased training dataset in the BioASQ challenge could also produce further insights.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.7511411011219025}]}, {"text": "At least having training data with more \"no\"-samples might be desirable and allow more sophisticated approaches like machine learning.", "labels": [], "entities": []}, {"text": "As stated before, the answer snippet selection strategy for the summarization task was not meant to be very promising.", "labels": [], "entities": [{"text": "answer snippet selection", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.7079825202624003}, {"text": "summarization task", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9297576248645782}]}, {"text": "Nevertheless, the strategy could be combined with the current approach in the olelo system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for mean average precision  (MAP) for Olelo in BioASQ task 5b phase A,  i.e., for document retrieval and passage retrieval.  Range of top results in all batches are presented in  the last row.", "labels": [], "entities": [{"text": "mean average precision  (MAP)", "start_pos": 22, "end_pos": 51, "type": "METRIC", "confidence": 0.9400437275568644}, {"text": "document retrieval", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7719249725341797}, {"text": "passage retrieval", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.8455401957035065}]}, {"text": " Table 2: Results for Olelo and the SRL approach in the BioASQ task 5b phases B (exact answers).  Results for yes/no questions are in terms of accuracy, MRR for factoid questions and f-measure for list  questions. Range of top results in all batches are presented in the last row.", "labels": [], "entities": [{"text": "Olelo", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8623090386390686}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9989389777183533}, {"text": "MRR", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9989457726478577}]}, {"text": " Table 3: Results for ideal answers (summaries) in  terms of Rouge metrics for Olelo and the SRL ap- proach. Range of top results in all batches are pre- sented in the last row.", "labels": [], "entities": [{"text": "Olelo", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.926094651222229}, {"text": "SRL ap- proach", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.658446654677391}]}, {"text": " Table 5: Results in terms of number of correct an- swers and response time for the on-line QA appli- cations.", "labels": [], "entities": []}]}