{"title": [], "abstractContent": [{"text": "Standard approaches in entity identification hard-code boundary detection and type prediction into labels and perform Viterbi.", "labels": [], "entities": [{"text": "entity identification", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8594944179058075}, {"text": "type prediction", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.8141641914844513}]}, {"text": "This has two disadvantages: 1.", "labels": [], "entities": []}, {"text": "the runtime complexity grows quadratically in the number of types, and 2.", "labels": [], "entities": []}, {"text": "there is no natural segment-level representation.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural architecture that addresses these disadvantages.", "labels": [], "entities": []}, {"text": "We frame the problem as multitasking, separating boundary detection and type prediction but optimizing them jointly.", "labels": [], "entities": [{"text": "boundary detection", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7497707009315491}, {"text": "type prediction", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.8377994000911713}]}, {"text": "Despite its simplicity, this architecture performs competitively with fully structured models such as BiLSTM-CRFs while scaling linearly in the number of types.", "labels": [], "entities": []}, {"text": "Furthermore , by construction, the model induces type-disambiguating embeddings of predicted mentions.", "labels": [], "entities": []}], "introductionContent": [{"text": "A popular convention in segmentation tasks such as named-entity recognition (NER) and chunking is the so-called \"BIO\"-label scheme.", "labels": [], "entities": [{"text": "segmentation tasks", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9021894931793213}, {"text": "named-entity recognition (NER)", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.8212672948837281}]}, {"text": "It hard-codes boundary detection and type prediction into labels using the indicators \"B\" (Beginning), \"I\" (Inside), and \"O\" (Outside).", "labels": [], "entities": [{"text": "boundary detection", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.6895124465227127}, {"text": "type prediction", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8931169509887695}]}, {"text": "For instance, the sentence Where is John Smith is tagged as Where/O is/O John/B-PER Smith/I-PER.", "labels": [], "entities": []}, {"text": "In this way, we can treat the problem as sequence labeling and apply standard structured models such as CRFs.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.65227010846138}]}, {"text": "But this approach has certain disadvantages.", "labels": [], "entities": []}, {"text": "First, the runtime complexity grows quadratically in the number of types (assuming exact decoding with first-order label dependency).", "labels": [], "entities": []}, {"text": "We emphasize that the asymptotic runtime remains quadratic even if we heuristically prune previous labels based on the BIO scheme.", "labels": [], "entities": []}, {"text": "This is not an issue when the number of types is small but quickly becomes problematic as the number grows.", "labels": [], "entities": []}, {"text": "Second, there is no segment-level prediction: every prediction happens at the word-level.", "labels": [], "entities": []}, {"text": "As a consequence, models do not induce representations corresponding to multi-word mentions, which can be useful for downstream tasks such as named-entity disambiguation (NED).", "labels": [], "entities": [{"text": "named-entity disambiguation (NED)", "start_pos": 142, "end_pos": 175, "type": "TASK", "confidence": 0.8396033883094788}]}, {"text": "In this paper, we propose a neural architecture that addresses these disadvantages.", "labels": [], "entities": []}, {"text": "Given a sentence, the model uses bidirectional LSTMs (BiLSTMs) to induce features and separately predicts: 1.", "labels": [], "entities": []}, {"text": "Boundaries of mentions in the sentence.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test F1 scores on CoNLL 2003 and  OntoNotes newswire portion.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9640597701072693}, {"text": "CoNLL 2003", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9765624701976776}, {"text": "OntoNotes newswire portion", "start_pos": 44, "end_pos": 70, "type": "DATASET", "confidence": 0.9540936152140299}]}, {"text": " Table 2: Test F1 scores on CoNLL 2003.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9633320569992065}, {"text": "CoNLL 2003", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9138668775558472}]}]}