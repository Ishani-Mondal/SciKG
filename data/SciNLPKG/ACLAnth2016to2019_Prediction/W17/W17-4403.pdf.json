{"title": [{"text": "Churn Identification in Microblogs using Convolutional Neural Networks with Structured Logical Knowledge", "labels": [], "entities": [{"text": "Churn Identification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.880430668592453}]}], "abstractContent": [{"text": "For brands, gaining new customer is more expensive than keeping an existing one.", "labels": [], "entities": []}, {"text": "Therefore, the ability to keep customers in a brand is becoming more challenging these days.", "labels": [], "entities": []}, {"text": "Churn happens when a customer leaves a brand to another competitor.", "labels": [], "entities": []}, {"text": "Most of the previous work considers the problem of churn prediction using CDRs.", "labels": [], "entities": [{"text": "churn prediction", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.9185207188129425}]}, {"text": "In this paper, we use micro-posts to classify customers into churny or non-churny.", "labels": [], "entities": []}, {"text": "We explore the power of CNNs since they achieved state-of-the-art in various computer vision and NLP applications.", "labels": [], "entities": []}, {"text": "However, the robustness of end-to-end models has some limitations such as the availability of a large amount of labeled data and uninterpretability of these models.", "labels": [], "entities": []}, {"text": "We investigate the use of CNNs augmented with structured logic rules to overcome or reduce this issue.", "labels": [], "entities": []}, {"text": "We developed our system called Churn teacher by using an iterative distillation method that transfers the knowledge, extracted using just the combination of three logic rules, directly into the weight of Deep Neural Networks (DNNs).", "labels": [], "entities": []}, {"text": "Furthermore, we used weight normalization to speedup training our convolutional neural networks.", "labels": [], "entities": []}, {"text": "Experimental results showed that with just these three rules, we were able to get state-of-the-art on publicly available Twitter dataset about three Telecom brands.", "labels": [], "entities": []}], "introductionContent": [{"text": "Customer churn maybe defined as the process of losing a customer that recently switches from a brand to another competitor.", "labels": [], "entities": []}, {"text": "The churn problem can be tackled from different angles: most of the previous work used Call Detail Records (CDRs) to identify churners from non-churners (.", "labels": [], "entities": []}, {"text": "More recently, with more data became available on the web, brands can use customers opinionated comments via social networks, forums and especially Twitter to detect churny from non-churny customers.", "labels": [], "entities": []}, {"text": "We used the churn dataset developed by.", "labels": [], "entities": []}, {"text": "This dataset was collected from Twitter for three telecommunication brands: Verizon, TMobile, and AT&T.", "labels": [], "entities": []}, {"text": "In recent years, deep learning models have achieved great success in various domains and difficult problems such as computer vision () and speech recognition ( ).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.8239889442920685}]}, {"text": "In natural language processing, much of the work with deep learning models has involved language modeling (, sentiment analysis (, and more recently, neural machine translation ().", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6477780938148499}, {"text": "language modeling", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7179998010396957}, {"text": "sentiment analysis", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.8792418837547302}, {"text": "neural machine translation", "start_pos": 150, "end_pos": 176, "type": "TASK", "confidence": 0.7128468950589498}]}, {"text": "Furthermore, these models can use backpropagation algorithm for training).", "labels": [], "entities": []}, {"text": "Regardless of the success of deep neural networks, these models still have a gap compared to human learning process.", "labels": [], "entities": []}, {"text": "While the success came from the high expressiveness, it leads them to predict results in uninterpretable ways, which could have a negative side effects on the whole learning process.", "labels": [], "entities": []}, {"text": "In addition, these models require a huge amount of labeled data, which is considered as time consuming for the community since it requires human experts inmost applications (natural language and computer vision).", "labels": [], "entities": []}, {"text": "Recent works tackled this issue by trying to bridge this gap in different applications: in supervised learning such as machine translation () and unsupervised learning ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.793395608663559}]}, {"text": "In the Natural Language Processing (NLP) community, there has been much work to augment the training process with additional and useful features which proved its success in various NLP applications such as Named Entity Recognition (NER) and Sentiment Analysis.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 206, "end_pos": 236, "type": "TASK", "confidence": 0.7875917851924896}, {"text": "Sentiment Analysis", "start_pos": 241, "end_pos": 259, "type": "TASK", "confidence": 0.9539735317230225}]}, {"text": "The majority of these works used pretrained word embeddings obtained from unlabeled data to initialize their word vectors, which allow them to improve the performance.", "labels": [], "entities": []}, {"text": "Another solution came from integrating logical rules extracted from the data directly into the weights of neural networks.", "labels": [], "entities": []}, {"text": "() explored a distillation framework that transfers structured knowledge coded as logic rules into the weights of neural networks.", "labels": [], "entities": []}, {"text": "() developed a neural network from a given rule to do reasoning.", "labels": [], "entities": []}, {"text": "In this paper, we combine the two ideas: firstly, we used unsupervised word representations to initialize our word vectors.", "labels": [], "entities": []}, {"text": "We explored three different pretraiend word embeddings and compared the results with a randomly sampled one.", "labels": [], "entities": []}, {"text": "Secondly, we used three main logic rules, which were proven to be useful and crucial.", "labels": [], "entities": []}, {"text": "The \"but\" rule was explored by) in sentiment analysis and we add two new rules: \"switch to\" and \"switch from\".", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9680928885936737}]}, {"text": "( showed that the last two rules have a remarkable influence into the churn classification problem.", "labels": [], "entities": [{"text": "churn classification", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.9346626400947571}]}, {"text": "Moreover, in order to accelerate training our model on churn training dataset, we conduct an investigation of using weight normalization, which is anew recently developed method to accelerate training deep neural networks.", "labels": [], "entities": []}, {"text": "Experiments on Twitter dataset built from a large number of tweets about three Telecommunication brands show that we were able to obtain state-of-the-art results for churn classification in microblogs.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.770535409450531}, {"text": "churn classification", "start_pos": 166, "end_pos": 186, "type": "TASK", "confidence": 0.8653620183467865}]}, {"text": "Our system, called Churn teacher, is constructed by using a structured logical knowledge expressed into three logic rules transferred into the weights of convolutional neural networks.", "labels": [], "entities": []}, {"text": "We outperform the previous models based on hand engineering features or also using recurrent neural networks combined with minimal features.", "labels": [], "entities": []}, {"text": "Our system is philosophical close to (, which showed that combining deep neural networks with logic rules performed well on two NLP tasks: NER and Sentiment Analysis.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.8086780905723572}]}, {"text": "The rest of this paper is structured as follows: in section 2, we discuss the related work done in churn prediction application.", "labels": [], "entities": [{"text": "churn prediction application", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.8878186742464701}]}, {"text": "In section 3, we present our churn prediction approach which is based on structured logical knowledge transferred into the weights of Convolutional Neural Networks (CNNs).", "labels": [], "entities": [{"text": "churn prediction", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.8301132619380951}]}, {"text": "In section 4, we discuss the impact of pretrained word embeddings on the churn classification.", "labels": [], "entities": [{"text": "churn classification", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.8810770511627197}]}, {"text": "The experimental results are presented in section 5.", "labels": [], "entities": []}, {"text": "Finally, we present the conclusion with the future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation of our model, we use the dataset provided by.", "labels": [], "entities": []}, {"text": "The authors collected the data from twitter for three telecom brands: Verizon, T-Mobile, and AT&T.", "labels": [], "entities": []}, {"text": "Table 2 presents the details about the entries of this dataset.", "labels": [], "entities": []}, {"text": "We divide the experimental process into two stages: the first stage concerns running the experiments using the convolutional neural network without and with different logic rules in order to select the best achieved results.", "labels": [], "entities": []}, {"text": "In the second stage, we compare our best settings with the previous state-of-art system in churn prediction in microblogs.", "labels": [], "entities": [{"text": "churn prediction", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.8132206201553345}]}, {"text": "shows the churn classification results.", "labels": [], "entities": [{"text": "churn classification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9455036520957947}]}, {"text": "The first row represents the baseline where we use only the convolutional neural network.", "labels": [], "entities": []}, {"text": "In the second row, we initialize our word vectors using: Statistics about the three rules (but, switch to and switch from) in the training and test set.", "labels": [], "entities": []}, {"text": "pretrained word vectors using GloVe model since it gives us the best results among the other pretrained word vectors.", "labels": [], "entities": []}, {"text": "We get an improvement around 2.5% in F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9985331296920776}]}, {"text": "We refer to this model by \"CNN-pre\".", "labels": [], "entities": [{"text": "CNN-pre", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9426104426383972}]}, {"text": "This results is consistent with the fact that these pretrained word vectors are universal feature extractors that shown an important results in different NLP applications such as sentiment analysis, named entity recognition and Partof-speech tagging.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.9574068486690521}, {"text": "named entity recognition", "start_pos": 199, "end_pos": 223, "type": "TASK", "confidence": 0.6305238405863444}, {"text": "Partof-speech tagging", "start_pos": 228, "end_pos": 249, "type": "TASK", "confidence": 0.8987136483192444}]}, {"text": "By transferring the knowledge extracted using the \"but\" logic rule into the weights of the convolutional neural network, we were able to improve the F1-score over the CNN-pre model by 1.28 points in F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9990645051002502}, {"text": "F1-score", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9899397492408752}]}, {"text": "For the \"switch from\" logic rule, we get a slight improvement over the CNN-pre model by 0.25 points in F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9981480836868286}]}, {"text": "The biggest improvement among the three logic rules was obtained by the \"switch to\" rule where we were able to improve the performance over the CNN-pre model by 1.93 points in F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9965890645980835}]}, {"text": "The last row in concerns the results that we obtained by using all the three logic rules where logically we achieved best results and outperformed the CNN-pre model by 3.18 points in F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9968371391296387}]}, {"text": "While we do not have a complete explanation why we got better results with \"switch to\" rule, we believe that it is caused by the fact that there 321 sentences in the training data containing this rule which represents around 8% sentences contains the word \"switch to\".", "labels": [], "entities": []}, {"text": "Moreover, it will be clear that customer will leave a specific brand to another new brand.", "labels": [], "entities": []}, {"text": "For the \"switch from\" rule, we get slight improvement over the CNN-pre model because few sentences containing this rule (around 2% sentences contains the word \"switch from\").", "labels": [], "entities": []}, {"text": "shows the statistics about the presence of the three rules in the training data.", "labels": [], "entities": []}, {"text": "For \"but\" rule, we also get an important improvement over the CNN-pre model which confirms the results obtained by) using the same rule in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9117774069309235}]}, {"text": "We note that around 9% senModels F1 score Unigram + Nb ( 73.42 78.30 Churn teacher 83.85: Comparison of our system with two previous systems.", "labels": [], "entities": [{"text": "F1 score Unigram + Nb ( 73.42 78.30 Churn teacher 83.85", "start_pos": 33, "end_pos": 88, "type": "METRIC", "confidence": 0.7550723986192183}]}, {"text": "tences contains the word \"but\".", "labels": [], "entities": []}, {"text": "In the last row, we combine all the three rules and we were able to obtain the best performance.", "labels": [], "entities": []}, {"text": "We refer to this model as \"Churn teacher\".", "labels": [], "entities": []}, {"text": "This is consistent with the argument by ( where they argued that more rules will allow the system to improve its performance over the base convolutional neural network.", "labels": [], "entities": []}, {"text": "We test our model using this dataset and compare the obtained results with two other systems.", "labels": [], "entities": []}, {"text": "The state-of-the-art results were produced by where they achieved 78.30% in F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9638573527336121}]}, {"text": "They used a combination of Bag of Words features and Recurrent Neural Networks.", "labels": [], "entities": []}, {"text": "The second system referenced here as \"Unigram + Nb\" developed by) used different N-grams (n = 1, 2, 3) and their combination on both of the word and character levels.", "labels": [], "entities": []}, {"text": "By adding three rules to the convolutional neural networks, we outperformed the \"Unigram + Nb\" system by a large margin (10.43 points in F1-score).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9988922476768494}]}, {"text": "Furthermore, our model also outperformed the system developed by) by a good margin (5.55 points in F1-score).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9983077049255371}]}, {"text": "shows a brief presentation of the experimental results and the comparison with the two other systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results with different choices of pre- trained word embeddings with a comparison with  randomly initialized ones on churn predicition in  microblogs", "labels": [], "entities": []}, {"text": " Table 2: Statistics and details about the churn mi- croblog dataset.", "labels": [], "entities": [{"text": "churn mi- croblog dataset", "start_pos": 43, "end_pos": 68, "type": "DATASET", "confidence": 0.5386546611785888}]}, {"text": " Table 4: Statistics about the three rules (but,  switch to and switch from) in the training and test  set.", "labels": [], "entities": []}]}