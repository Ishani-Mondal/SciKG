{"title": [{"text": "WASSA-2017 Shared Task on Emotion Intensity", "labels": [], "entities": [{"text": "WASSA-2017", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.8803377151489258}]}], "abstractContent": [{"text": "We present the first shared task on detecting the intensity of emotion felt by the speaker of a tweet.", "labels": [], "entities": [{"text": "detecting the intensity of emotion felt by the speaker of a tweet", "start_pos": 36, "end_pos": 101, "type": "TASK", "confidence": 0.7566253940264384}]}, {"text": "We create the first datasets of tweets annotated for anger, fear, joy, and sadness intensities using a technique called best-worst scaling (BWS).", "labels": [], "entities": []}, {"text": "We show that the annotations lead to reliable fine-grained intensity scores (rankings of tweets by intensity).", "labels": [], "entities": [{"text": "fine-grained intensity scores", "start_pos": 46, "end_pos": 75, "type": "METRIC", "confidence": 0.6718549033006033}]}, {"text": "The data was partitioned into training, development , and test sets for the competition.", "labels": [], "entities": []}, {"text": "Twenty-two teams participated in the shared task, with the best system obtaining a Pearson correlation of 0.747 with the gold intensity scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 83, "end_pos": 102, "type": "METRIC", "confidence": 0.9659081697463989}]}, {"text": "We summarize the machine learning setups, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful for the task.", "labels": [], "entities": []}, {"text": "The emotion intensity dataset and the shared task are helping improve our understanding of how we convey more or less intense emotions through language.", "labels": [], "entities": []}], "introductionContent": [{"text": "We use language to communicate not only the emotion we are feeling but also the intensity of the emotion.", "labels": [], "entities": []}, {"text": "For example, our utterances can convey that we are very angry, slightly sad, absolutely elated, etc.", "labels": [], "entities": []}, {"text": "Here, intensity refers to the degree or amount of an emotion such as anger or sadness.", "labels": [], "entities": [{"text": "intensity", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.954105794429779}]}, {"text": "Automatically determining the intensity of emotion felt by the speaker has applications in commerce, public health, intelligence gathering, and social welfare.", "labels": [], "entities": [{"text": "intelligence gathering", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.8429721593856812}]}, {"text": "Twitter has a large and diverse user base which entails rich textual content, including nonstandard language such as emoticons, emojis, creatively spelled words (happee), and hashtagged words (#luvumom).", "labels": [], "entities": []}, {"text": "Tweets are often used to convey one's emotion, opinion, and stance).", "labels": [], "entities": []}, {"text": "Thus, automatically detecting emotion intensities in tweets is especially beneficial in applications such as tracking brand and product perception, tracking support for issues and policies, tracking public health and well-being, and disaster/crisis management.", "labels": [], "entities": [{"text": "automatically detecting emotion intensities in tweets", "start_pos": 6, "end_pos": 59, "type": "TASK", "confidence": 0.7641592820485433}, {"text": "tracking brand and product perception", "start_pos": 109, "end_pos": 146, "type": "TASK", "confidence": 0.781072735786438}, {"text": "disaster/crisis management", "start_pos": 233, "end_pos": 259, "type": "TASK", "confidence": 0.6578714177012444}]}, {"text": "Here, for the first time, we present a shared task on automatically detecting intensity of emotion felt by the speaker of a tweet: WASSA-2017 Shared Task on Emotion Intensity.", "labels": [], "entities": [{"text": "Emotion Intensity", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7474611699581146}]}, {"text": "Specifically, given a tweet and an emotion X, the goal is to determine the intensity or degree of emotion X felt by the speaker-a real-valued score between 0 and 1.", "labels": [], "entities": []}, {"text": "A score of 1 means that the speaker feels the highest amount of emotion X.", "labels": [], "entities": []}, {"text": "A score of 0 means that the speaker feels the lowest amount of emotion X.", "labels": [], "entities": []}, {"text": "We first ask human annotators to infer this intensity of emotion from a tweet.", "labels": [], "entities": []}, {"text": "Later, automatic algorithms are tested to determine the extent to which they can replicate human annotations.", "labels": [], "entities": []}, {"text": "Note that often a tweet does not explicitly state that the speaker is experiencing a particular emotion, but the intensity of emotion felt by the speaker can be inferred nonetheless.", "labels": [], "entities": []}, {"text": "Sometimes a tweet is sarcastic or it conveys the emotions of a different entity, yet the annotators (and automatic algorithms) are to infer, based on the tweet, the extent to which the speaker is likely feeling a particular emotion.", "labels": [], "entities": []}, {"text": "In order to provide labeled training, development, and test sets for this shared task, we needed to annotate instances for degree of affect.", "labels": [], "entities": []}, {"text": "This is a substantially more difficult undertaking than annotating only for the broad affect class: respondents are presented with greater cognitive load and it is particularly hard to ensure consistency (both across responses by different annotators and within the responses produced by an individual annotator).", "labels": [], "entities": [{"text": "consistency", "start_pos": 192, "end_pos": 203, "type": "METRIC", "confidence": 0.9885241389274597}]}, {"text": "Thus, we used a technique called BestWorst Scaling (BWS), also sometimes referred to as Maximum Difference Scaling (MaxDiff).", "labels": [], "entities": [{"text": "BestWorst Scaling (BWS)", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.5847153782844543}]}, {"text": "It is an annotation scheme that addresses the limitations of traditional rating scales.", "labels": [], "entities": []}, {"text": "We used BWS to create the Tweet Emotion Intensity Dataset, which currently includes four sets of tweets annotated for intensity of anger, fear, joy, and sadness, respectively).", "labels": [], "entities": [{"text": "BWS", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.8568112254142761}, {"text": "Tweet Emotion Intensity Dataset", "start_pos": 26, "end_pos": 57, "type": "DATASET", "confidence": 0.6203084290027618}]}, {"text": "These are the first datasets of their kind.", "labels": [], "entities": []}, {"text": "The competition is organized on a CodaLab website, where participants can upload their submissions, and the leaderboard reports the results.", "labels": [], "entities": []}, {"text": "Twenty-two teams participated in the 2017 iteration of the competition.", "labels": [], "entities": []}, {"text": "The best performing system, Prayas, obtained a Pearson correlation of 0.747 with the gold annotations.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 47, "end_pos": 66, "type": "METRIC", "confidence": 0.9237923622131348}]}, {"text": "Seven teams obtained scores higher than the score obtained by a competitive SVM-based benchmark system (0.66), which we had released at the start of the competition.", "labels": [], "entities": []}, {"text": "Low-dimensional (dense) distributed representations of words (word embeddings) and sentences (sentence vectors), along with presence of affect-associated words (derived from affect lexicons) were the most commonly used features.", "labels": [], "entities": []}, {"text": "Neural network were the most commonly used machine learning architecture.", "labels": [], "entities": []}, {"text": "They were used for learning tweet representations as well as for fitting regression functions.", "labels": [], "entities": [{"text": "learning tweet representations", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7809658646583557}]}, {"text": "Support vector machines (SVMs) were the second most popular regression algorithm.", "labels": [], "entities": []}, {"text": "Keras and TensorFlow were some of the most widely used libraries.", "labels": [], "entities": [{"text": "Keras", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8851690292358398}]}, {"text": "The top performing systems used ensembles of models trained on dense distributed representations of the tweets as well as features drawn from affect lexicons.", "labels": [], "entities": []}, {"text": "They also made use of a substantially larger number of affect lexicons than systems that did not perform as well.", "labels": [], "entities": []}, {"text": "The emotion intensity dataset and the corresponding shared task are helping improve our understanding of how we convey more or less intense emotions through language.", "labels": [], "entities": []}, {"text": "The task also adds a dimensional nature to model of basic emotions, which has traditionally been viewed as categorical (joy or no joy, fear or no fear, etc.).", "labels": [], "entities": []}, {"text": "On going work with annotations on the same data for valence , arousal, and dominance aims to better understand the relationships between the circumplex model of emotions) and the categorical model of emotions.", "labels": [], "entities": []}, {"text": "Even though the 2017 WASSA shared task has concluded, the CodaLab competition website is kept open.", "labels": [], "entities": [{"text": "WASSA shared task", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.826027512550354}, {"text": "CodaLab competition website", "start_pos": 58, "end_pos": 85, "type": "DATASET", "confidence": 0.8076266447703043}]}, {"text": "Thus new and improved systems can continually be tested.", "labels": [], "entities": []}, {"text": "The best results obtained by any system on the 2017 test set can be found on the CodaLab leaderboard.", "labels": [], "entities": [{"text": "2017 test set", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7810294230779012}, {"text": "CodaLab leaderboard", "start_pos": 81, "end_pos": 100, "type": "DATASET", "confidence": 0.9723627865314484}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We begin with related work and a brief background on best-worst scaling (Section 2).", "labels": [], "entities": []}, {"text": "In Section 3, we describe how we collected and annotated the tweets for emotion intensity.", "labels": [], "entities": [{"text": "emotion intensity", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.6960804611444473}]}, {"text": "We also present experiments to determine the quality of the annotations.", "labels": [], "entities": []}, {"text": "Section 4 presents details of the shared task setup.", "labels": [], "entities": []}, {"text": "In Section 5, we present a competitive SVM-based baseline that uses a number of common text classification features.", "labels": [], "entities": []}, {"text": "We describe ablation experiments to determine the impact of different feature types on regression performance.", "labels": [], "entities": []}, {"text": "In Section 6, we present the results obtained by the participating systems and summarize their machine learning setups.", "labels": [], "entities": []}, {"text": "Finally, we present conclusions and future directions.", "labels": [], "entities": []}, {"text": "All of the data, annotation questionnaires, evaluation scripts, regression code, and interactive visualizations of the data are made freely available on the shared task website.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each emotion, systems were evaluated by calculating the Pearson Correlation Coefficient of the system predictions with the gold ratings.", "labels": [], "entities": [{"text": "Pearson Correlation Coefficient", "start_pos": 60, "end_pos": 91, "type": "METRIC", "confidence": 0.957221786181132}]}, {"text": "Pearson coefficient, which measures linear correlations between two variables, produces scores from -1 (perfectly inversely correlated) to 1 (perfectly correlated).", "labels": [], "entities": [{"text": "Pearson coefficient", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9172064065933228}]}, {"text": "A score of 0 indicates no correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9720359444618225}]}, {"text": "The correlation scores across all four emotions was averaged to determine the bottom-line competition metric by which the submissions were ranked.", "labels": [], "entities": []}, {"text": "In addition to the bottom-line competition metric described above, the following additional metrics were also provided: \u2022 Spearman Rank Coefficient of the submission with the gold scores of the test data.", "labels": [], "entities": [{"text": "Spearman Rank Coefficient", "start_pos": 122, "end_pos": 147, "type": "METRIC", "confidence": 0.960110584894816}]}, {"text": "Motivation: Spearman Rank Coefficient considers only how similar the two sets of ranking are.", "labels": [], "entities": []}, {"text": "The differences in scores between adjacently ranked instance pairs is ignored.", "labels": [], "entities": []}, {"text": "On the one hand this has been argued to alleviate some biases in Pearson, but on the other hand it can ignore relevant information.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.706677258014679}]}, {"text": "\u2022 Correlation scores (Pearson and Spearman) over a subset of the testset formed by taking instances with gold intensity scores \u2265 0.5.", "labels": [], "entities": [{"text": "Correlation scores", "start_pos": 2, "end_pos": 20, "type": "METRIC", "confidence": 0.9576092064380646}]}, {"text": "Motivation: In some applications, only those instances that are moderately or strongly emotional are relevant.", "labels": [], "entities": []}, {"text": "Here it maybe much more important fora system to correctly determine emotion intensities of instances in the higher range of the scale as compared to correctly determine emotion intensities in the lower range of the scale.", "labels": [], "entities": []}, {"text": "Results with Spearman rank coefficient were largely inline with those obtained using Pearson coefficient, and so in the rest of the paper we report only the latter.", "labels": [], "entities": [{"text": "Spearman rank coefficient", "start_pos": 13, "end_pos": 38, "type": "METRIC", "confidence": 0.7580577333768209}]}, {"text": "However, the CodaLab leaderboard and the official results posted on the task website show both metrics.", "labels": [], "entities": [{"text": "CodaLab leaderboard", "start_pos": 13, "end_pos": 32, "type": "DATASET", "confidence": 0.9086581468582153}]}, {"text": "The official evaluation script (which calculates correlations using both metrics and also acts as a format checker) was made available along with the training and development data well in advance.", "labels": [], "entities": []}, {"text": "Participants were able to use it to monitor progress of their system by crossvalidation on the training set or testing on the development set.", "labels": [], "entities": []}, {"text": "The script was also uploaded on the CodaLab competition website so that the system evaluates submissions automatically and updates the leaderboard.", "labels": [], "entities": [{"text": "CodaLab competition website", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.8075446287790934}]}, {"text": "We developed the baseline system by learning models from each of the Tweet Emotion Intensity Dataset training sets and applying them to the corresponding development sets.", "labels": [], "entities": [{"text": "Tweet Emotion Intensity Dataset training sets", "start_pos": 69, "end_pos": 114, "type": "DATASET", "confidence": 0.7121080756187439}]}, {"text": "Once the system parameters were frozen, the system learned new models from the combined training and development corpora.", "labels": [], "entities": []}, {"text": "This model was applied to the test sets.", "labels": [], "entities": []}, {"text": "shows the results obtained on the test sets using various features, individually and in combination.", "labels": [], "entities": []}, {"text": "The last column 'avg.'", "labels": [], "entities": []}, {"text": "shows the macro-average of the correlations for all of the emotions.", "labels": [], "entities": []}, {"text": "Using just character or just word n-grams leads to results around 0.48, suggesting that they are reasonably good indicators of emotion intensity by themselves.", "labels": [], "entities": []}, {"text": "(Guessing the intensity scores at random between 0 and 1 is expected to get correlations close to 0.)", "labels": [], "entities": []}, {"text": "Word embeddings produces statistically significant improvement over the ngrams (avg. r = 0.55).", "labels": [], "entities": [{"text": "avg. r", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9789834320545197}]}, {"text": "Among the lexicons, NRC-Hash-Emo is the most predictive single lexicon.", "labels": [], "entities": [{"text": "NRC-Hash-Emo", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.8689335584640503}]}, {"text": "Lexicons that include Twitter-specific entries, lexicons that include intensity scores, and lexicons that label emotions and not just sentiment, tend to be more predictive on this task-dataset combination.", "labels": [], "entities": []}, {"text": "NRC-Aff-Int has real-valued fine-grained wordemotion association scores for all the words in NRC-EmoLex that were marked as being associated with anger, fear, joy, and sadness.", "labels": [], "entities": [{"text": "NRC-Aff-Int", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9707021713256836}, {"text": "NRC-EmoLex", "start_pos": 93, "end_pos": 103, "type": "DATASET", "confidence": 0.9395043849945068}]}, {"text": "Improvement in scores obtained using NRC-Aff-Int over the scores obtained using NRC-EmoLex also show that using fine intensity scores of word-emotion association are beneficial for tweet-level emotion intensity detection.", "labels": [], "entities": [{"text": "NRC-Aff-Int", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9040334820747375}, {"text": "NRC-EmoLex", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.9433935284614563}, {"text": "tweet-level emotion intensity detection", "start_pos": 181, "end_pos": 220, "type": "TASK", "confidence": 0.857798844575882}]}, {"text": "The correlations for anger, fear, and joy are similar (around 0.65), but the correlation for sadness is markedly higher (0.71).", "labels": [], "entities": []}, {"text": "We can observe from that this boost in performance for sadness is to some extent due to word embeddings, but is more so due to lexicon features, especially those from SentiStrength.", "labels": [], "entities": []}, {"text": "SentiStrength focuses solely on positive and negative classes, but provides numeric scores for each.", "labels": [], "entities": []}, {"text": "To assess performance in the moderate-to-high range of the intensity scale, we calculated correla- The increase from 0.63 to 0.66 is statistically significant.: Pearson correlations (r) of emotion intensity predictions with gold scores.", "labels": [], "entities": [{"text": "Pearson correlations (r)", "start_pos": 161, "end_pos": 185, "type": "METRIC", "confidence": 0.9159243941307068}]}, {"text": "Best results for each column are shown in bold: highest score by a feature set, highest score using a single lexicon, and highest score using feature set combinations.", "labels": [], "entities": []}, {"text": "tion scores over a subset of the test data formed by taking only those instances with gold emotion intensity scores \u2265 0.5.", "labels": [], "entities": [{"text": "tion", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9561518430709839}]}, {"text": "The last row in shows the results.", "labels": [], "entities": []}, {"text": "We observe that the correlation scores are in general lower herein the 0.5 to 1 range of intensity scores than in the experiments over the full intensity range.", "labels": [], "entities": [{"text": "correlation", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9743497967720032}]}, {"text": "This is simply because this is a harder task as now the systems do not benefit by making coarse distinctions over whether a tweet is in the lower range or in the higher range.", "labels": [], "entities": []}, {"text": "We created an interactive visualization to allow ease of exploration of the Tweet Emotion Intensity Dataset.", "labels": [], "entities": [{"text": "Tweet Emotion Intensity Dataset", "start_pos": 76, "end_pos": 107, "type": "DATASET", "confidence": 0.5856435894966125}]}, {"text": "This visualization was made public after the the official evaluation period had concludedso participants in the shared task did not have access to it when building their system.", "labels": [], "entities": []}, {"text": "It is worth noting that if one intends to evaluate their emotion intensity detection system on the Tweet Emotion Intensity Dataset, then as a matter of commonlyfollowed best practices, they should not use the visualization to explore the test data in the system development phase (until all the system parameters are frozen).", "labels": [], "entities": [{"text": "emotion intensity detection", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.6837232112884521}, {"text": "Tweet Emotion Intensity Dataset", "start_pos": 99, "end_pos": 130, "type": "DATASET", "confidence": 0.6777468919754028}]}, {"text": "The visualization has three main components: 1 of instances in each of the emotion partitions (train, dev, test).", "labels": [], "entities": []}, {"text": "Hovering over a row shows the corresponding number of instances.", "labels": [], "entities": []}, {"text": "Clicking on an emotion filters out data from all other emotions, in all visualization components.", "labels": [], "entities": []}, {"text": "Similarly, one can click on just the train, dev, or test partitions to view information just for that data.", "labels": [], "entities": []}, {"text": "Clicking again deselects the item.", "labels": [], "entities": []}, {"text": "2. A histogram of emotion intensity scores.", "labels": [], "entities": []}, {"text": "A slider that one can use to view only those tweets within a certain score range.", "labels": [], "entities": []}, {"text": "3. The list of tweets, emotion label, and emotion intensity scores.", "labels": [], "entities": []}, {"text": "Notably, the three components are interconnected, such that clicking on an item in one component will filter information in all other components to show only the relevant details.", "labels": [], "entities": []}, {"text": "For example, clicking on 'joy' in 'a' will cause 'b' to show the histogram for only the joy tweets, and 'c' to show only the 'joy' tweets.", "labels": [], "entities": []}, {"text": "Similarly one can click on the test/dev/train set, a particular band of emotion intensity scores, or a particular tweet.", "labels": [], "entities": []}, {"text": "Clicking again deselects the item.", "labels": [], "entities": []}, {"text": "One can use filters in combination.", "labels": [], "entities": []}, {"text": "For e.g., clicking on fear, test data, and setting the slider for the 0.5 to 1 range, shows information for only those fear-testdata instances with scores \u2265 0.5.", "labels": [], "entities": [{"text": "slider", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.946811854839325}]}], "tableCaptions": [{"text": " Table 2: Split-half reliabilities (as measured by  Pearson correlation and Spearman rank correla- tion) for the anger, fear, joy, and sadness tweets  in the Tweet Emotion Intensity Dataset.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 52, "end_pos": 71, "type": "METRIC", "confidence": 0.9492530822753906}, {"text": "Spearman rank correla- tion", "start_pos": 76, "end_pos": 103, "type": "METRIC", "confidence": 0.8145676195621491}, {"text": "Tweet Emotion Intensity Dataset", "start_pos": 158, "end_pos": 189, "type": "DATASET", "confidence": 0.6096709296107292}]}, {"text": " Table 3: The number of instances in the Tweet  Emotion Intensity dataset.", "labels": [], "entities": [{"text": "Tweet  Emotion Intensity dataset", "start_pos": 41, "end_pos": 73, "type": "DATASET", "confidence": 0.6907647028565407}]}, {"text": " Table 4: Affect lexicons used in our experiments.", "labels": [], "entities": []}, {"text": " Table 7: Pearson correlations (r) and ranks (in brackets) obtained by the systems on a subset of the test  set where gold scores \u2265 0.5", "labels": [], "entities": [{"text": "Pearson correlations (r)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.9081690192222596}]}, {"text": " Table 8: Feature types (resources) used by the participating systems. Teams are indicated by their rank.", "labels": [], "entities": []}, {"text": " Table 9: Regression methods used by the participating systems. Teams are indicated by their rank.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8801231980323792}]}, {"text": " Table 10: Tools and libraries used by the participating systems. Teams are indicated by their rank.", "labels": [], "entities": []}]}