{"title": [{"text": "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology Based Representations", "labels": [], "entities": [{"text": "Document Classification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.806971549987793}]}], "abstractContent": [{"text": "We investigate the pertinence of methods from algebraic topology for text data analysis.", "labels": [], "entities": [{"text": "text data analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.713858167330424}]}, {"text": "These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space.", "labels": [], "entities": []}, {"text": "In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7493413090705872}, {"text": "sentiment classification", "start_pos": 150, "end_pos": 174, "type": "TASK", "confidence": 0.9022290706634521}]}, {"text": "We find that the embed-dings do not benefit text analysis.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.7951372563838959}]}, {"text": "In fact, performance is worse than simple techniques like tf-idf, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a embedding model mapping words ton dimensional vectors, every document can be represented as a finite subset of Rn . Comparing documents then amounts to comparing such subsets.", "labels": [], "entities": []}, {"text": "While previous work shows that the Earth Mover's Distance ( or distance between the weighted average of word vectors () provides information that is useful for classification tasks, we wish to go a step further and investigate whether useful information can also be found in the 'shape' of a document in word embedding space.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 160, "end_pos": 180, "type": "TASK", "confidence": 0.9038883745670319}]}, {"text": "Persistent homology is a tool from algebraic topology used to compute topological signatures (called persistence diagrams) on compact metric * *The indicated authors contributed equally to this spaces.", "labels": [], "entities": []}, {"text": "These have the property of being stable with respect to the Gromov-Haussdorff distance (.", "labels": [], "entities": []}, {"text": "In other words, compact metric spaces that are close, up to an isometry, will have similar embeddings.", "labels": [], "entities": []}, {"text": "In this work, we examine the utility of such embeddings in text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.8560635646184286}]}, {"text": "To the best of our knowledge, no previous work has been performed on using topological representations for traditional NLP tasks, nor has any comparison been made with state-ofthe-art approaches.", "labels": [], "entities": []}, {"text": "We begin by considering a document as the set of its word vectors, generated with a pretrained word embedding model.", "labels": [], "entities": []}, {"text": "These form the metric space on which we build persistence diagrams, using Euclidean distance as the distance measure.", "labels": [], "entities": []}, {"text": "The diagrams area representation of the document's geometry in the metric space.", "labels": [], "entities": []}, {"text": "We then perform clustering on the Twenty Newsgroups dataset with the features extracted from the persistence diagram.", "labels": [], "entities": [{"text": "Twenty Newsgroups dataset", "start_pos": 34, "end_pos": 59, "type": "DATASET", "confidence": 0.8503027757008871}]}, {"text": "We also evaluate the method on sentiment classification tasks, using the Cornell Sentence Polarity (CSP)) and IMDb movie review datasets.", "labels": [], "entities": [{"text": "sentiment classification tasks", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.9600218534469604}, {"text": "Cornell Sentence Polarity (CSP))", "start_pos": 73, "end_pos": 105, "type": "DATASET", "confidence": 0.7696643769741058}, {"text": "IMDb movie review datasets", "start_pos": 110, "end_pos": 136, "type": "DATASET", "confidence": 0.8547850549221039}]}, {"text": "As suggested by, we posit that the information about the intrinsic geometry of documents, found in the persistence diagrams, might yield information that our classifier can leverage, either on its own or in combination with other representations.", "labels": [], "entities": []}, {"text": "The primary objective of our work is to empirically evaluate these representations in the case of sentiment and topic classification, and assess their usefulness for real-world tasks.", "labels": [], "entities": [{"text": "sentiment and topic classification", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.736292690038681}]}], "datasetContent": [{"text": "The pipeline for our experiments is shown in.", "labels": [], "entities": []}, {"text": "In order to build a persistence diagram, we convert each document to the set of its word vectors.", "labels": [], "entities": []}, {"text": "We then use Dionysus), a C++ library for computing persistence diagrams, and form the signatures described in 2.3.", "labels": [], "entities": []}, {"text": "We will subsequently refer to these diagrams as Persistent Homology (PH) embeddings.", "labels": [], "entities": []}, {"text": "Once we have the embeddings for each document, they can be used as input to standard clustering or classification algorithms.", "labels": [], "entities": []}, {"text": "As a baseline document representation, we use the average of the word vectors for that document (subsequently called AW2V embeddings).", "labels": [], "entities": []}, {"text": "For clustering, we experiment with K-means and Gaussian Mixture Models (GMM) on a subset 4 of the Twenty Newsgroups dataset.", "labels": [], "entities": [{"text": "Twenty Newsgroups dataset", "start_pos": 98, "end_pos": 123, "type": "DATASET", "confidence": 0.793971578280131}]}, {"text": "The subset was selected to ensure that most documents are from related topics, making clustering nontrivial, and the documents are of reasonable length to compute the representation.", "labels": [], "entities": []}, {"text": "For classification, we perform both sentencelevel and document-level binary sentiment classification using logistic regression on the CSP and IMDb corpora respectively.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9630686044692993}, {"text": "document-level binary sentiment classification", "start_pos": 54, "end_pos": 100, "type": "TASK", "confidence": 0.6953718066215515}, {"text": "IMDb corpora", "start_pos": 142, "end_pos": 154, "type": "DATASET", "confidence": 0.8771787583827972}]}], "tableCaptions": [{"text": " Table 1: Performance on the CSP dataset", "labels": [], "entities": [{"text": "CSP dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.8560108244419098}]}, {"text": " Table 2: Performance on the IMDb dataset", "labels": [], "entities": [{"text": "IMDb dataset", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.938981294631958}]}]}