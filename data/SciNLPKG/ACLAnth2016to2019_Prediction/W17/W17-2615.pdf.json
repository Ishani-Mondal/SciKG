{"title": [], "abstractContent": [{"text": "Recently Le & Mikolov described two log-linear models, called Paragraph Vector , that can be used to learn state-of-the-art distributed representations of documents.", "labels": [], "entities": []}, {"text": "Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval.", "labels": [], "entities": []}, {"text": "We show that binary paragraph vectors outper-form autoencoder-based binary codes, despite using fewer bits.", "labels": [], "entities": []}, {"text": "We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9986440539360046}]}, {"text": "Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domain-specific documents.", "labels": [], "entities": []}, {"text": "Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations.", "labels": [], "entities": []}, {"text": "This model can be used to rapidly retrieve a shortlist of highly relevant documents from a large document collection.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the significant challenges in contemporary information processing is the sheer volume of available data., for example, claim that the amount of digital data in the world doubles every two years.", "labels": [], "entities": [{"text": "information processing", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7856767475605011}]}, {"text": "This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets.", "labels": [], "entities": []}, {"text": "One class of such algorithms, represented by, e.g., Locality Sensitive Hashing, relies on hashing data into short, locality-preserving binary codes (.", "labels": [], "entities": [{"text": "Locality Sensitive Hashing", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.731570303440094}]}, {"text": "The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items.", "labels": [], "entities": []}, {"text": "Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data.", "labels": [], "entities": []}, {"text": "Nevertheless, some methods target specific kind of input data, like text or image.", "labels": [], "entities": []}, {"text": "In this work we focus on learning binary codes for text documents.", "labels": [], "entities": []}, {"text": "An important work in this direction has been presented by.", "labels": [], "entities": []}, {"text": "Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation.", "labels": [], "entities": []}, {"text": "Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.6638166010379791}]}, {"text": "Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TF-IDF-based retrieval.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9990317821502686}, {"text": "TF-IDF-based retrieval", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.7715614438056946}]}, {"text": "Learning binary representation from BOW, however, has its disadvantages.", "labels": [], "entities": [{"text": "BOW", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.5770311951637268}]}, {"text": "First, word-count representation, and in turn the learned codes, are not in itself stronger than TF-IDF.", "labels": [], "entities": [{"text": "word-count representation", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.7538082897663116}]}, {"text": "Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions.", "labels": [], "entities": [{"text": "BOW", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9801157712936401}]}, {"text": "Learning fully-connected autoencoders for such highdimensional vectors is impractical.", "labels": [], "entities": []}, {"text": "Salakhutdinov & Hinton restricted the BOW vocabulary in their experiments to 2000 most frequent words.", "labels": [], "entities": []}, {"text": "Binary codes have also been applied to crossmodal retrieval where text is one of the modalities.", "labels": [], "entities": [{"text": "crossmodal retrieval", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8564936816692352}]}, {"text": "Specifically, incorporated tag information that often accompany text documents, while employed siamese neural networks to learn single binary representation for text and image data.", "labels": [], "entities": []}, {"text": "Recently several works explored simple neural models for unsupervised learning of distributed representations of words, sentences and documents.", "labels": [], "entities": [{"text": "unsupervised learning of distributed representations of words, sentences and documents", "start_pos": 57, "end_pos": 143, "type": "TASK", "confidence": 0.6759430739012632}]}, {"text": "proposed loglinear models that learn distributed representations of words by predicting a central word from its context (CBOW model) or by predicting context words given the central word (Skip-gram model).", "labels": [], "entities": []}, {"text": "The CBOW model was then extended by to learn distributed representations of documents.", "labels": [], "entities": []}, {"text": "Specifically, they proposed Paragraph Vector Distributed Memory (PV-DM) model, in which the central word is predicted given the context words and the document vector.", "labels": [], "entities": [{"text": "Paragraph Vector Distributed Memory (PV-DM)", "start_pos": 28, "end_pos": 71, "type": "TASK", "confidence": 0.7252395067896161}]}, {"text": "During training, PV-DM learns the word embeddings and the parameters of the softmax that models the conditional probability distribution for the central words.", "labels": [], "entities": []}, {"text": "During inference, word embeddings and softmax weights are fixed, but the gradients are backpropagated to the inferred document vector.", "labels": [], "entities": []}, {"text": "In addition to PV-DM, Le & Mikolov studied also a simpler model, namely Paragraph Vector Distributed Bag of Words (PV-DBOW).", "labels": [], "entities": [{"text": "Paragraph Vector Distributed Bag of Words", "start_pos": 72, "end_pos": 113, "type": "TASK", "confidence": 0.6358421047528585}]}, {"text": "This model predicts words in the document given only the document vector.", "labels": [], "entities": []}, {"text": "It therefore disregards context surrounding the predicted word and does not learn word embeddings.", "labels": [], "entities": []}, {"text": "Le & Mikolov demonstrated that paragraph vectors outperform BOW and bag-of-bigrams in information retrieval task, while using only few hundreds of dimensions.", "labels": [], "entities": [{"text": "BOW", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9011411666870117}, {"text": "information retrieval task", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.8360305825869242}]}, {"text": "These models are also amendable to learning and inference overlarge vocabularies.", "labels": [], "entities": []}, {"text": "Original CBOW network used hierarchical softmax to model the probability distribution for the central word.", "labels": [], "entities": [{"text": "CBOW network", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.8971339166164398}]}, {"text": "One can also use noise-contrastive estimation ( or importance sampling ( to approximate the gradients with respect to the softmax logits.", "labels": [], "entities": []}, {"text": "An alternative approach to learning representation of pieces of text has been recently described by.", "labels": [], "entities": [{"text": "learning representation of pieces of text", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.803820883234342}]}, {"text": "Networks proposed therein, inspired by the Skip-gram model, learn to predict surrounding sentences given the center sentence.", "labels": [], "entities": []}, {"text": "To this end, the center sentence is encoded by an encoder network and the surrounding sentences are predicted by a decoder network conditioned on the center sentence code.", "labels": [], "entities": []}, {"text": "Once trained, these models can encode sentences without resorting to backpropagation inference.", "labels": [], "entities": []}, {"text": "However, they learn representations at the sentence level but not at the document level.", "labels": [], "entities": []}, {"text": "In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents.", "labels": [], "entities": []}, {"text": "One inspiration for binary paragraph vectors comes from a recent work by on learning binary codes for images.", "labels": [], "entities": []}, {"text": "Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in away that encourages binary activations.", "labels": [], "entities": []}, {"text": "We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes.", "labels": [], "entities": []}, {"text": "We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora.", "labels": [], "entities": []}, {"text": "Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking.", "labels": [], "entities": [{"text": "document filtering", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7583995461463928}]}, {"text": "While employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups 1 , a cleansed version (also called v2) of Reuters Corpus Volume 1 2 (RCV1) and English Wikipedia . As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text.", "labels": [], "entities": [{"text": "Reuters Corpus Volume 1 2 (RCV1)", "start_pos": 158, "end_pos": 190, "type": "DATASET", "confidence": 0.9382927566766739}, {"text": "English Wikipedia", "start_pos": 195, "end_pos": 212, "type": "DATASET", "confidence": 0.9010827243328094}]}, {"text": "However, we removed stop words as well as words shorter than two characters and longer than 15 characters.", "labels": [], "entities": []}, {"text": "Results reported by ( indicate that performance of PV-DBOW can be improved by including n-grams in the model.", "labels": [], "entities": []}, {"text": "We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams.", "labels": [], "entities": []}, {"text": "Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents.", "labels": [], "entities": []}, {"text": "This amounts to a vocabulary with slightly over one million elements.", "labels": [], "entities": []}, {"text": "For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements.", "labels": [], "entities": [{"text": "RCV1 dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9846293926239014}]}, {"text": "In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.", "labels": [], "entities": []}, {"text": "The 20 Newsgroups dataset comes with reference train/test sets.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7218161821365356}]}, {"text": "In case of RCV1 we used half of the documents for training and the other half for evaluation.", "labels": [], "entities": [{"text": "RCV1", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.9294667840003967}]}, {"text": "In case of English Wikipedia we held out for testing randomly selected 10% of the documents.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.8673606216907501}]}, {"text": "We perform document retrieval by selecting queries from the test set and ordering other test documents according to the similarity of the inferred codes.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7065593153238297}]}, {"text": "We use Hamming distance for binary codes and cosine similarity for real-valued representations.", "labels": [], "entities": []}, {"text": "Results are averaged over queries.", "labels": [], "entities": []}, {"text": "We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10)).", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.9938005805015564}, {"text": "mean average precision (MAP)", "start_pos": 123, "end_pos": 151, "type": "METRIC", "confidence": 0.9447649220625559}, {"text": "normalized discounted cumulative gain", "start_pos": 160, "end_pos": 197, "type": "METRIC", "confidence": 0.6978680491447449}]}, {"text": "The results depend, of course, on the chosen document relevancy measure.", "labels": [], "entities": []}, {"text": "Relevancy measure for the 20 Newsgroups dataset is straightforward: a retrieved document is relevant to the query if they both belong to the same newsgroup.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.6837263802687327}]}, {"text": "In RCV1 each document belongs to a hierarchy of topics, making the definition of relevancy less obvious.", "labels": [], "entities": []}, {"text": "In this case we adopted the relevancy measure used by.", "labels": [], "entities": []}, {"text": "That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document.", "labels": [], "entities": [{"text": "relevancy", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9574971795082092}]}, {"text": "Overall, our selection of test datasets and relevancy measures for 20 Newsgroups and RCV1 follows Salakhutdinov and Hinton (2009), enabling comparison with semantic hashing codes.", "labels": [], "entities": [{"text": "RCV1", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.5857758522033691}]}, {"text": "To assess the relevancy of articles in English Wikipedia we can employ categories assigned to them.", "labels": [], "entities": []}, {"text": "However, unlike in RCV1, Wikipedia categories can have multiple parent categories and cyclic dependencies.", "labels": [], "entities": []}, {"text": "Therefore, for this dataset we adopted a simplified relevancy measure: two articles are relevant if they share at least one category.", "labels": [], "entities": []}, {"text": "We also removed from the test set categories with less than 20 documents as well as documents that were left with no categories.", "labels": [], "entities": []}, {"text": "Overall, the relevancy is measured over more than 11, 800 categories, making English Wikipedia harder than the other two benchmarks.", "labels": [], "entities": []}, {"text": "We use AdaGrad (Duchi et al., 2011) for training and inference in all experiments reported in this work.", "labels": [], "entities": []}, {"text": "During training we employ dropout () in the embedding layer.", "labels": [], "entities": []}, {"text": "To facilitate models with large vocabularies, we approximate the gradients with respect to the softmax logits using the method described by.", "labels": [], "entities": []}, {"text": "Binary PV-DM networks use the same number of dimensions for document codes and word embeddings.", "labels": [], "entities": []}, {"text": "Performance of 128-and 32-bit binary paragraph vector codes is reported in and in.", "labels": [], "entities": []}, {"text": "For comparison we also report performance of real-valued paragraph vectors.", "labels": [], "entities": []}, {"text": "Note that the binary codes perform very well, despite their far lower capacity: on 20 Newsgroups and RCV1 the 128-bit Binary PV-DBOW trained with bigrams approaches the performance of the real-valued paragraph vectors, while on English Wikipedia its performance is slightly lower.", "labels": [], "entities": [{"text": "20 Newsgroups and RCV1", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.7048008143901825}]}, {"text": "Furthermore, Binary PV-DBOW with bigrams outperforms semantic hashing codes: comparison of precision-recall curves from with shows that 128-bit codes learned with this model outperform 128-bit semantic hashing codes on 20 Newsgroups and RCV1.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.9802834391593933}, {"text": "20 Newsgroups", "start_pos": 219, "end_pos": 232, "type": "DATASET", "confidence": 0.7118963599205017}, {"text": "RCV1", "start_pos": 237, "end_pos": 241, "type": "DATASET", "confidence": 0.4924156665802002}]}, {"text": "Moreover, the 32-bit codes from this model outperform 128-bit semantic hashing codes on the RCV1 dataset, and  on the 20 Newsgroups dataset give similar precision up to approximately 3% recall and better precision for higher recall levels.", "labels": [], "entities": [{"text": "RCV1 dataset", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.9887343347072601}, {"text": "20 Newsgroups dataset", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.8924672802289327}, {"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9962360262870789}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9990156888961792}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9992924928665161}, {"text": "recall", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.9974266886711121}]}, {"text": "Note that the difference in this case lies not only in retrieval precision: the short 32-bit Binary PV-DBOW codes are more efficient for indexing than long 128-bit semantic hashing codes.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.8901122808456421}]}, {"text": "We also compared binary paragraph vectors against codes constructed by first inferring short, real-valued paragraph vectors and then using a separate hashing algorithm for binarization.", "labels": [], "entities": []}, {"text": "When the dimensionality of the paragraph vectors is equal to the size of binary codes, the number of network parameters in this approach is similar to that of Binary PV models.", "labels": [], "entities": []}, {"text": "We experimented with two standard hashing algorithms, namely random hyperplane projection) and iterative quantization).", "labels": [], "entities": [{"text": "random hyperplane projection", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6436397035916647}]}, {"text": "Paragraph vectors in these experiments were inferred using PV-DBOW with bigrams.", "labels": [], "entities": []}, {"text": "Results reported in show no benefit from using a separate algorithm for binarization.", "labels": [], "entities": []}, {"text": "On the 20 Newsgroups and RCV1 datasets Binary PV-DBOW yielded higher MAP than the two baseline approaches.", "labels": [], "entities": [{"text": "RCV1 datasets Binary PV-DBOW", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.8390467017889023}, {"text": "MAP", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9797437191009521}]}, {"text": "On English Wikipedia iterative quantization achieved MAP equal to Binary PV-DBOW, while random hyperplane projection yielded lower MAP.", "labels": [], "entities": [{"text": "MAP", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9973515272140503}, {"text": "MAP", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9878461956977844}]}, {"text": "Some gain in precision of top hits can be observed for iterative quantization, as indicated by NDCG@10.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9987736344337463}, {"text": "NDCG", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.6542725563049316}]}, {"text": "However, precision of top hits can also be improved by querying with Real-Binary PV-DBOW model (Section 3.2).", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9994218349456787}]}, {"text": "It is also worth noting that end-to-end inference in Binary PV models is more convenient than inferring real-valued vectors and then using another algorithm for hashing.", "labels": [], "entities": []}, {"text": "argue that PV-DBOW outperforms PV-DM on a sentiment classification task, and demonstrate that the performance of PV-DBOW can be improved by including bigrams in the vocabulary.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.929039736588796}]}, {"text": "We observed similar results with Binary PV models.", "labels": [], "entities": []}, {"text": "That is, including bigrams in the vocabulary usually improved retrieval precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9893209934234619}]}, {"text": "Also, codes learned with Binary PV-DBOW provided higher retrieval precision than Binary PV-DM codes.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.8654974102973938}]}, {"text": "Furthermore, to choose the context size for the Binary PV-DM models, we evaluated several networks on validation sets taken out of the training data.", "labels": [], "entities": []}, {"text": "The best results were obtained with a minimal one-word, one-sided context window.", "labels": [], "entities": []}, {"text": "This is the distributed memory architecture most similar to the Binary PV-DBOW model.: Information retrieval results for 32-bit binary codes constructed by first inferring 32d real-valued paragraph vectors and then employing a separate hashing algorithm for binarization.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.8318040072917938}]}, {"text": "Paragraph vectors were inferred using PV-DBOW with bigrams.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Information retrieval results. The best results with binary models are highlighted.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8524363338947296}]}, {"text": " Table 2: Information retrieval results for 32-bit binary codes constructed by first inferring 32d real-valued  paragraph vectors and then employing a separate hashing algorithm for binarization. Paragraph vectors  were inferred using PV-DBOW with bigrams.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.883065938949585}]}, {"text": " Table 3: Information retrieval results for the Bi- nary PV-DBOW model trained on an unrelated  text corpus. Results are reported for 128-bit codes.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8098902702331543}]}, {"text": " Table 4: Information retrieval results for the Real- Binary PV-DBOW model. Real-valued represen- tations have 300 dimensions. (A) Binary codes are  used for selecting documents within a given Ham- ming distance to the query and real-valued rep- resentations are used for ranking. (B) For com- parison, variant A was repeated with binary codes  inferred using plain Binary PV-DBOW and real- valued representation inferred using original PV- DBOW model.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8334962129592896}]}]}