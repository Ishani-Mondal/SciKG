{"title": [], "abstractContent": [{"text": "This paper presents the results of the WMT17 Metrics Shared Task.", "labels": [], "entities": [{"text": "WMT17 Metrics Shared Task", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6197262108325958}]}, {"text": "We asked participants of this task to score the outputs of the MT systems involved in the WMT17 news translation task and Neu-ral MT training task.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9315287470817566}, {"text": "WMT17 news translation task", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.8374534696340561}, {"text": "Neu-ral MT training task", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.504469096660614}]}, {"text": "We collected scores of 14 metrics from 8 research groups.", "labels": [], "entities": []}, {"text": "In addition to that, we computed scores of 7 standard metrics (BLEU, SentBLEU, NIST, WER, PER, TER and CDER) as baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9987298846244812}, {"text": "NIST", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.4956520199775696}, {"text": "WER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9439495801925659}, {"text": "PER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.8898788094520569}, {"text": "TER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9236746430397034}]}, {"text": "The collected scores were evaluated in terms of system-level correlation (how well each metric's scores correlate with WMT17 official manual ranking of systems) and in terms of segment level correlation (how often a metric agrees with humans in judging the quality of a particular sentence).", "labels": [], "entities": [{"text": "WMT17 official manual ranking", "start_pos": 119, "end_pos": 148, "type": "DATASET", "confidence": 0.857989490032196}, {"text": "segment level correlation", "start_pos": 177, "end_pos": 202, "type": "METRIC", "confidence": 0.664261668920517}]}, {"text": "This year, we build upon two types of manual judgements: direct assessment (DA) and HUME manual semantic judgements .", "labels": [], "entities": [{"text": "HUME manual semantic judgements", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.6113197654485703}]}], "introductionContent": [{"text": "Evaluating the quality of machine translation (MT) is critical for developers of MT systems to monitor progress as well as for MT users to select among available MT engines for their language pair of interest.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8303864002227783}]}, {"text": "Manual evaluation is however costly and difficult to reproduce.", "labels": [], "entities": []}, {"text": "Automatic MT evaluation can resolve these issues, if it matches manual evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9800088703632355}]}, {"text": "The Metrics Shared Task 1 of WMT annually evaluates the performance of automatic machine translation metrics in their ability to provide a substitute for human assessment of translation quality.", "labels": [], "entities": [{"text": "machine translation metrics", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.7849199672540029}]}, {"text": "In contrast to MT quality estimation, the metrics task provides participating metrics with reference translations with which MT outputs are compared.", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.8854854504267374}, {"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9617546796798706}]}, {"text": "The metrics task itself then needs manual judgements of translation quality in order to check the extent to which the automatic metrics can approximate the judgement.", "labels": [], "entities": []}, {"text": "For situations where the reference translation is not available, please consult the results of Quality Estimation Task ().", "labels": [], "entities": []}, {"text": "We keep the two main types of metric evaluation unchanged from the previous years.", "labels": [], "entities": [{"text": "metric evaluation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6689641177654266}]}, {"text": "In system-level evaluation, each metric provides a quality score for the whole translated test set (usually a set of documents, in fact).", "labels": [], "entities": []}, {"text": "In segment-level evaluation, a score has to be assigned to every individual sentence.", "labels": [], "entities": []}, {"text": "The underlying texts and MT systems come from two other WMT tasks, namely News Translation Task (, denoted as Findings 2017 in the following) and Neural MT training task, and from the EU project HimL, aiming at translation of healthrelated documents.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9903355836868286}, {"text": "News Translation Task", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7805286248524984}, {"text": "Neural MT training task", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.7153436541557312}, {"text": "HimL", "start_pos": 195, "end_pos": 199, "type": "DATASET", "confidence": 0.820015549659729}, {"text": "translation of healthrelated documents", "start_pos": 211, "end_pos": 249, "type": "TASK", "confidence": 0.8097101598978043}]}, {"text": "The texts were drawn mainly from the news domain and, to a limited extent, from the medical domain and involve translations to/from Chinese (zh), Czech (cs), Finnish (fi), German (de), Latvian (lv), Russian (ru), and Turkish (tr), each paired with English, and additionally English into Romanian and Polish, making a total of 16 language pairs.", "labels": [], "entities": []}, {"text": "Two sources of golden truth of translation quality judgement are used this year: \u2022 In Direct Assessment (DA) (, humans assess the quality of a given MT output translation by comparison with a reference translation (but not the source).", "labels": [], "entities": [{"text": "MT output translation", "start_pos": 149, "end_pos": 170, "type": "TASK", "confidence": 0.8441566824913025}]}, {"text": "DA is the new standard used in WMT news translation task evaluation, requiring only monolingual evaluators.", "labels": [], "entities": [{"text": "DA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5440026521682739}, {"text": "WMT news translation task evaluation", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.941382360458374}]}, {"text": "The added benefit for the metrics task is that the manual and automatic evaluations are now a little closer: both humans and metrics compare the MT output with the reference.", "labels": [], "entities": [{"text": "MT", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.9423750638961792}]}, {"text": "\u2022 The HUME score () is a segment-level score aggregated over manual judgements of translation quality of semantic units of the source sentence.", "labels": [], "entities": [{"text": "HUME score", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9487697780132294}]}, {"text": "In contrast to previous years, the official method of evaluation changes, moving from \"relative ranking\" (RR, evaluating up to five system outputs on an annotation screen relative to each other) to DA and employing the Pearson correlation r inmost cases.", "labels": [], "entities": [{"text": "DA", "start_pos": 198, "end_pos": 200, "type": "METRIC", "confidence": 0.9940273761749268}, {"text": "Pearson correlation r", "start_pos": 219, "end_pos": 240, "type": "METRIC", "confidence": 0.9738683104515076}]}, {"text": "Due to difficulties in obtaining sufficient number of judgements for segment-level evaluation of some language pairs, we re-interpret DA judgements for these language pairs as relative comparisons and use Kendall's \u03c4 as a substitute, see below for details and references.", "labels": [], "entities": []}, {"text": "Section 2 describes our datasets, i.e. the sets of underlying sentences, system outputs, human judgements of translation quality and also participating metrics.", "labels": [], "entities": []}, {"text": "Sections 3.1 and 3.2 then provide the results of system and segment-level metric evaluation, respectively.", "labels": [], "entities": []}, {"text": "We discuss the results in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "As in the previous year, hybrid super-sampling proved very effective and allowed to obtain conclusive results of system-level evaluation even for language pairs whereas few as 4 MT systems participated.", "labels": [], "entities": []}, {"text": "We should however note that this style of aggregated evaluation may not be a substitute for truly document-level evaluation.", "labels": [], "entities": []}, {"text": "Hybrid systems are constructed by randomly mixing sentence and they therefore may possibly break cross-sentence links in MT outputs (if such links are at all preserved by current MT systems).", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.8396781384944916}]}, {"text": "account, but this would have to be empirically validated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of judgements for the five out-of- English language pairs employing DA converted", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9711527824401855}]}, {"text": " Table 4: Absolute Pearson correlation of to-English system-level metrics with DA human assessment;  correlations of metrics not significantly outperformed by any other for that language pair are highlighted  in bold; ensemble metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8025404413541158}]}, {"text": " Table 5: Absolute Pearson correlation of out-of-English system-level metrics with DA human assess- ment; correlations of metrics not significantly outperformed by any other for that language pair are  highlighted in bold; ensemble metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8025236229101816}, {"text": "DA human assess- ment", "start_pos": 83, "end_pos": 104, "type": "METRIC", "confidence": 0.7324362754821777}]}, {"text": " Table 6: Absolute Pearson correlation of to-English system-level metrics with DA human assessment for  10K hybrid super-sampled systems; ensemble metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.802342931429545}]}, {"text": " Table 7: Absolute Pearson correlation of out-of-English system-level metrics with DA human assessment  for 10K hybrid super-sampled systems; ensemble metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.802272895971934}]}, {"text": " Table 8: Absolute Pearson correlation of system- level metrics with HUME human assessment; en- semble metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8019574681917826}, {"text": "HUME human assessment", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.5211590528488159}]}, {"text": " Table 9: Absolute Pearson correlation of system- level metrics with HUME human assessment; en- semble metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7973328431447347}, {"text": "HUME human assessment", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.5160686870416006}]}, {"text": " Table 10: Absolute Pearson correlation of system- level metrics with HUME human assessment  for 10K hybrid super-sampled systems; ensemble  metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 11, "end_pos": 39, "type": "METRIC", "confidence": 0.7998197674751282}]}, {"text": " Table 13: Absolute Pearson correlation of  segment-level metric scores with HUME scores  for himltest2017a; ensemble metrics are high- lighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 11, "end_pos": 39, "type": "METRIC", "confidence": 0.8280717730522156}, {"text": "HUME", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9905195236206055}, {"text": "himltest2017a", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.6503288745880127}]}]}