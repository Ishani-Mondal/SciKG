{"title": [{"text": "Knowledge Base Completion: Baselines Strike Back", "labels": [], "entities": [{"text": "Baselines Strike Back", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.4678339660167694}]}], "abstractContent": [{"text": "Many papers have been published on the knowledge base completion task in the past few years.", "labels": [], "entities": [{"text": "knowledge base completion task", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.729191929101944}]}, {"text": "Most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as FB15k and WN18.", "labels": [], "entities": [{"text": "relation learning", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.9803430140018463}, {"text": "FB15k", "start_pos": 114, "end_pos": 119, "type": "DATASET", "confidence": 0.9833793044090271}, {"text": "WN18", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.8895111680030823}]}, {"text": "This paper shows that the accuracy of almost all models published on the FB15k can be out-performed by an appropriately tuned base-line-our reimplementation of the Dist-Mult model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993088245391846}, {"text": "FB15k", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.9846893548965454}]}, {"text": "Our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyper-parameter tuning or different training objectives.", "labels": [], "entities": []}, {"text": "This should prompt future research to reconsider how the performance of models is evaluated and reported.", "labels": [], "entities": []}], "introductionContent": [{"text": "Projects such as Wikidata 1 or earlier Freebase () have successfully accumulated a formidable amount of knowledge in the form of entity1 -relation -entity2 triplets.", "labels": [], "entities": []}, {"text": "Given this vast body of knowledge, it would be extremely useful to teach machines to reason over such knowledge bases.", "labels": [], "entities": []}, {"text": "One possible way to test such reasoning is knowledge base completion (KBC).", "labels": [], "entities": [{"text": "knowledge base completion (KBC)", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.6260658750931422}]}, {"text": "The goal of the KBC task is to fill in the missing piece of information into an incomplete triple.", "labels": [], "entities": [{"text": "KBC task", "start_pos": 16, "end_pos": 24, "type": "TASK", "confidence": 0.7477436363697052}]}, {"text": "For instance, given a query Donald Trump, president of, ? one should predict that the target entity is USA.", "labels": [], "entities": [{"text": "USA", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.9377682209014893}]}, {"text": "More formally, given a set of entities E and a set of binary relations Rover these entities, a knowledge base (sometimes also referred to as a knowl-edge graph) can be specified by a set of triplets h, r, t where h, t \u2208 E are head and tail entities respectively and r \u2208 R is a relation between them.", "labels": [], "entities": []}, {"text": "In entity KBC the task is to predict either the tail entity given a query h, r, ?, or to predict the head entity given ?, r, t.", "labels": [], "entities": []}, {"text": "Not only can this task be useful to test the generic ability of a system to reason over a knowledge base, but it can also find use in expanding existing incomplete knowledge bases by deducing new entries from existing ones.", "labels": [], "entities": []}, {"text": "An extensive amount of work has been published on this task (for a review see (, fora plain list of citations see).", "labels": [], "entities": []}, {"text": "Among those DistMult () is one of the simplest.", "labels": [], "entities": []}, {"text": "Still this paper shows that even a simple model with proper hyperparameters and training objective evaluated using the standard metric of Hits@10 can outperform 27 out of 29 models which were evaluated on two standard KBC datasets,.", "labels": [], "entities": [{"text": "KBC datasets", "start_pos": 218, "end_pos": 230, "type": "DATASET", "confidence": 0.9680180847644806}]}, {"text": "This suggests that there maybe a huge space for improvement in hyper-parameter tuning even for the more complex models, which maybe in many ways better suited for relational learning, e.g. can capture directed relations.", "labels": [], "entities": [{"text": "hyper-parameter tuning", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.6771434992551804}]}], "datasetContent": [{"text": "In our experiments we use two standard datasets WN18 derived from WordNet) and FB15k derived from the Freebase knowledge graph.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9077723622322083}, {"text": "FB15k", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.7933756113052368}, {"text": "Freebase knowledge graph", "start_pos": 102, "end_pos": 126, "type": "DATASET", "confidence": 0.977724572022756}]}, {"text": "For evaluation, we use the filtered evaluation protocol proposed by.", "labels": [], "entities": []}, {"text": "During training and validation we transform each triplet h, r, t into two examples: tail query h, r, ? and head query ?, r, t.", "labels": [], "entities": []}, {"text": "We train the model by minimizing negative log-likelihood (NLL) of the ground truth triplet h, r, t against randomly sampled pool of M negative triplets h, r, t , t \u2208 E \\ {t} (this applies for tail queries, head queries are handled analogically).", "labels": [], "entities": [{"text": "negative log-likelihood (NLL)", "start_pos": 33, "end_pos": 62, "type": "METRIC", "confidence": 0.7102790594100952}]}, {"text": "In the filtered protocol we rank the validation or test set triplet against all corrupted (supposedly untrue) triplets -those that do not appear in the train, valid and test dataset (excluding the test set triplet in question itself).", "labels": [], "entities": []}, {"text": "Formally, fora query h, r, ? where the correct answer is t, we compute the rank of h, r, tin a candidate set C h,r = {{h, r, t : \u2200t \u2208 E} \\ (T rain \u222a V alid \u222a T est) \u222a {{h, r, t}, where T rain, V alid and T est are sets of true triplets.", "labels": [], "entities": []}, {"text": "Head queries ?, r, tare handled analogically.", "labels": [], "entities": []}, {"text": "Note that softmax normalization is suitable under the filtered protocol since exactly one correct triplet is guaranteed to be among the candidates.", "labels": [], "entities": [{"text": "softmax normalization", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7348851263523102}]}, {"text": "In our preliminary experiments on FB15k, we varied the batch size b, embedding dimensionality N , number of negative samples in training M , L2 regularization parameter and learning rate lr.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.954496443271637}, {"text": "learning rate lr", "start_pos": 173, "end_pos": 189, "type": "METRIC", "confidence": 0.8898529609044393}]}, {"text": "Based on these experiments we fixed lr=0.001, L2=0.0 and we decided to focus on influence of batch size, embedding dimension and number of negative samples.", "labels": [], "entities": []}, {"text": "For final experiments we trained several models from hyperparameter range: N \u2208 {128, 256, 512, 1024}, b \u2208 {16,.", "labels": [], "entities": []}, {"text": "We train the final models using Adam ( optimizer (lr = 0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999, = 10 \u22128 , decay = 0.0).", "labels": [], "entities": []}, {"text": "We also performed limited experiments with Adagrad, Adadelta and plain SGD.", "labels": [], "entities": [{"text": "Adagrad", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9256672859191895}, {"text": "Adadelta", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8561874032020569}]}, {"text": "Adagrad usually required substantially more iterations than ADAM to achieve the same performance.", "labels": [], "entities": []}, {"text": "We failed to obtain competitive performance with Adadelta and plain SGD.", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.886619508266449}]}, {"text": "On FB15k and WN18 validation datasets the best hyper-parameter combinations were N = 512, b = 2048, M = 2000 and N = 256, b = 1024, M = 1000, respectively.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9689950942993164}, {"text": "WN18 validation datasets", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.8857070207595825}]}, {"text": "Note that we tried substantially more hyperparameter combinations on FB15k than on WN18.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9857293367385864}, {"text": "WN18", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.9771102666854858}]}, {"text": "Unlike most previous works we do not normalize neither entity nor relation embeddings.", "labels": [], "entities": []}, {"text": "To prevent over-fitting, we stop training once Hits@10 stop improving on the validation set.", "labels": [], "entities": []}, {"text": "On the FB15k dataset our Keras (Chollet, 2015) based implementation with TensorFlow () backend needed about 4 hours to converge when run on a single GeForce GTX 1080 GPU.", "labels": [], "entities": [{"text": "FB15k dataset", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9914594292640686}, {"text": "Keras (Chollet, 2015)", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.8385128776232401}]}, {"text": "Besides single models, we also evaluated performance of a simple ensemble that averages predictions of multiple models.", "labels": [], "entities": []}, {"text": "This technique consistently improves performance of machine learning models in many domains and it slightly improved results also in this case.", "labels": [], "entities": []}, {"text": "The results of our experiments together with previous results from the literature are shown in.", "labels": [], "entities": []}, {"text": "DistMult with proper hyperparameters twice achieves the second best score and once the third best score in three out of four commonly reported benchmarks (mean rank (MR) and Hits@10 on WN18 and FB15k).", "labels": [], "entities": [{"text": "mean rank (MR)", "start_pos": 155, "end_pos": 169, "type": "METRIC", "confidence": 0.9293833613395691}, {"text": "Hits@10", "start_pos": 174, "end_pos": 181, "type": "METRIC", "confidence": 0.9474802414576212}, {"text": "WN18", "start_pos": 185, "end_pos": 189, "type": "DATASET", "confidence": 0.9757887721061707}, {"text": "FB15k", "start_pos": 194, "end_pos": 199, "type": "DATASET", "confidence": 0.8283275365829468}]}, {"text": "On FB15k only the IRN model shows better Hits@10 and the ProjE ( has better MR.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9644933938980103}, {"text": "ProjE", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9029040336608887}, {"text": "MR", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.999151349067688}]}, {"text": "Our implementation has the best reported mean reciprocal rank (MRR) on FB15k, however this metric is not reported that often.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 41, "end_pos": 67, "type": "METRIC", "confidence": 0.8246528506278992}, {"text": "FB15k", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.7924589514732361}]}, {"text": "MRR is a metric of ranking quality that is less sensitive to outliers than MR.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.47964438796043396}, {"text": "MR", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9947163462638855}]}, {"text": "On WN18 dataset again the IRN model together with R-GCN+ shows better Hits@10.", "labels": [], "entities": [{"text": "WN18 dataset", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.9874040186405182}, {"text": "IRN", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8944408893585205}]}, {"text": "However, in MR and MRR DistMult performs poorly.", "labels": [], "entities": [{"text": "MR", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.846784234046936}, {"text": "MRR DistMult", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.535045400261879}]}, {"text": "Even though DistMult's inability to model asymmetric relations still allows it to achieve competitive results in Hits@10 the other metrics clearly show its limitations.", "labels": [], "entities": []}, {"text": "These results highlight qualitative differences between FB15k and WN18 datasets.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9144232273101807}, {"text": "WN18 datasets", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9234118759632111}]}, {"text": "Interestingly on FB15k recently published models (including our baseline) that use only rand h or t as their input outperform models that utilize richer features such as text or knowledge base path information.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.9701036214828491}]}, {"text": "This shows a possible gap for future improvement.", "labels": [], "entities": []}, {"text": "shows accuracy (Hits@1) of several models that reported this metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.99960857629776}, {"text": "Hits@1)", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9010007083415985}]}, {"text": "On WN18 our implementation performs worse than HolE and ComplEx models (that are equivalent as shown by).", "labels": [], "entities": [{"text": "WN18", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.9605891704559326}, {"text": "HolE", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.6827734708786011}]}, {"text": "On FB15k our implementation outperforms all other models.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.965898334980011}]}], "tableCaptions": [{"text": " Table 1: Accuracy (Hits@1) results sorted by per- formance on FB15k. Results marked by  \u2020 ,  \u2021 and  are from (Nickel et al., 2016), (Trouillon et al.,  2017) and (", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994581341743469}, {"text": "FB15k", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9687809944152832}]}]}