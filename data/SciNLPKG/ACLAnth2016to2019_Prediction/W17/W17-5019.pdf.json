{"title": [{"text": "GEC into the future: Where are we going and how do we get there?", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9377493858337402}]}], "abstractContent": [{"text": "The field of grammatical error correction (GEC) has made tremendous bounds in the last ten years, but new questions and obstacles are revealing themselves.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.7953962882359823}]}, {"text": "In this position paper, we discuss the issues that need to be addressed and provide recommendations for the field to continue to make progress, and propose anew shared task.", "labels": [], "entities": []}, {"text": "We invite suggestions and critiques from the audience to make the new shared task a community-driven venture.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the field of grammatical error correction (GEC), the Helping Our Own shared tasks in) and 2012 (, and then the CoNLL shared tasks of) and 2014 () marked a sea change.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.7959280957778295}]}, {"text": "For the first time there were public datasets, most notably the NUS Corpus of Learner English (NUCLE;, and evaluation metrics, of which the most commonly used to date is M 2 (.", "labels": [], "entities": [{"text": "NUS Corpus of Learner English (NUCLE;", "start_pos": 64, "end_pos": 101, "type": "DATASET", "confidence": 0.9523566886782646}]}, {"text": "This has allowed researchers from other fields, such as machine translation, to enter GEC more easily.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7993586659431458}, {"text": "GEC", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.6466053128242493}]}, {"text": "It has also enabled new developments, with many papers published on metrics, new algorithms (most recently neural methods), and occasionally new datasets.", "labels": [], "entities": []}, {"text": "Even with the accelerated progress in GEC, problems yet remain in the field.", "labels": [], "entities": [{"text": "GEC", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.6229823231697083}]}, {"text": "The use of specific datasets maybe GEC's worst enemy, as system and even evaluation metric development rely too heavily on the NUCLE test set.", "labels": [], "entities": [{"text": "GEC", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8618571162223816}, {"text": "NUCLE test set", "start_pos": 127, "end_pos": 141, "type": "DATASET", "confidence": 0.9597320954004923}]}, {"text": "While probably one of the most important contributions to the field's development to date, the lack of publicly available alternatives has caused some overoptimization.", "labels": [], "entities": []}, {"text": "Other issues have also gone undiscussed.", "labels": [], "entities": []}, {"text": "For example, nearly all work that has been published in the NLP community has focused on standalone systems, and very few investigate their impact on downstream users, except, e.g.,;.", "labels": [], "entities": []}, {"text": "In this short paper, we take stock of the current state of GEC ( \u00a72) and its limitations ( \u00a73), and outline where we believe the field should be five years from now ( \u00a74).", "labels": [], "entities": [{"text": "GEC", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.5194576978683472}]}, {"text": "We finish with a recommendation fora new community-driven shared task that will help the field progress even further ( \u00a75).", "labels": [], "entities": []}, {"text": "We look forward to discussing this proposal with the community and to refine a shared task for 2018.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are several error-annotated corpora, and for the purposes of this paper, we only focus on the most recent public datasets.", "labels": [], "entities": []}, {"text": "The size and characteristics of each corpus is summarized in Table 1.", "labels": [], "entities": []}, {"text": "The most frequently used corpus for GEC is NUCLE, which was the official dataset of the 2013 and 2014 shared tasks.", "labels": [], "entities": [{"text": "GEC", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8662712574005127}, {"text": "NUCLE", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.9508031606674194}]}, {"text": "It is a collection of essays written by students at the National University of Singapore ().", "labels": [], "entities": []}, {"text": "The test set and system results from the most recent shared task were released to the community (, and have been the focus of recent work on automatic metrics (see \u00a72.2).", "labels": [], "entities": []}, {"text": "Additionally, this test set has been augmented with eight additional annotations from and eight from English (FCE) portion is publicly available).", "labels": [], "entities": []}, {"text": "The FCE is approximately the same size as NUCLE and was used for the 2012 shared task.", "labels": [], "entities": [{"text": "FCE", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.6631779670715332}, {"text": "NUCLE", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.9262779355049133}]}, {"text": "However it has not been used to the same extent as NUCLE, presumably because it lacks multiple annotations and the 2012 shared task system outputs were not released.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9263023734092712}]}, {"text": "All of the corpora described above have been annotated with spans of text containing an error and assigned an error code.", "labels": [], "entities": []}, {"text": "Unlike these, the Lang-8 Learner Corpora Corpus of Learner English () is a parallel set of original and corrected sentences from lang-8.com, an online community of language learners who post text that is corrected by other users.", "labels": [], "entities": [{"text": "Lang-8 Learner Corpora Corpus of Learner English", "start_pos": 18, "end_pos": 66, "type": "DATASET", "confidence": 0.9180424979754856}]}, {"text": "It is also the largest public GEC corpora, with more than 2 million English sentences.", "labels": [], "entities": []}, {"text": "1 Another large corpus currently available was released for the first Automatic Evaluation of Scientific Writing shared task (AESW;.", "labels": [], "entities": [{"text": "Automatic Evaluation of Scientific Writing shared task", "start_pos": 70, "end_pos": 124, "type": "TASK", "confidence": 0.5816272497177124}]}, {"text": "Unlike the other corpora, it contains scientific writing by native and non-native English speakers, corrected by professional editors.", "labels": [], "entities": []}, {"text": "Because the writers are highly proficient, there is a lower diversity of errors than the other corpora.", "labels": [], "entities": [{"text": "diversity of errors", "start_pos": 60, "end_pos": 79, "type": "METRIC", "confidence": 0.8203405340512594}]}, {"text": "More than half of the errors are related to punctuation, which compose less than 7% of NUCLE errors.", "labels": [], "entities": []}, {"text": "Finally, the JHU FLuency-Extended GUG corpus (JFLEG) is a small dataset for tuning and evaluating GEC systems.", "labels": [], "entities": [{"text": "JHU FLuency-Extended GUG corpus (JFLEG)", "start_pos": 13, "end_pos": 52, "type": "DATASET", "confidence": 0.8220359470163073}]}, {"text": "1.5k sentences are taken from the GUG corpus), which labels sentences with an ordinal grammaticality score.", "labels": [], "entities": [{"text": "GUG corpus", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.959848552942276}]}, {"text": "In JFLEG, each sentence is corrected four times for grammaticality and fluency (Sakaguchi et al., 2016).", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.7409123778343201}]}, {"text": "We envision evaluation metrics which check that corrections are not only grammatically valid, but also check that the corrections are native-sounding and preserve the original meaning or intent of the writer.", "labels": [], "entities": []}, {"text": "Future metrics should be easy to compute and be interpretable.", "labels": [], "entities": []}, {"text": "For instance, a range between -1 and 1 maybe preferred (like IM uses), since it is possible a suggested set of corrections could produce a sentence which is worse than the original.", "labels": [], "entities": []}, {"text": "If multiple references are used, metrics should assign credit to corrections which match different references in different places, assuming the outcome is overall coherent.", "labels": [], "entities": []}, {"text": "In addition, most (if not all) evaluation schemes to date have focused on the sentence as the minimal unit.", "labels": [], "entities": []}, {"text": "It would be good to take the entire document into account and allow for more global rewrites, such as consistent tense.", "labels": [], "entities": []}, {"text": "Ultimately, a metric should say whether or not a system has attained the same level of performance as a human judge.", "labels": [], "entities": []}, {"text": "One way of doing this is through a GEC Turing Test, where system outputs are blindly judged alongside human corrections of the same sentences.", "labels": [], "entities": [{"text": "GEC Turing Test", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.7723968625068665}]}, {"text": "If human adjudicators think the system outputs are indistinguishable in quality from the human corrections (for example, given a set of criteria such as being good corrections, meaning preserving and native-sounding) then that is a very strong signal that GEC has attained human-level performance.", "labels": [], "entities": [{"text": "meaning preserving", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.6912414282560349}, {"text": "GEC", "start_pos": 256, "end_pos": 259, "type": "TASK", "confidence": 0.6081522703170776}]}, {"text": "To illustrate the shortcomings of current metrics, contains a JFLEG sentence corrected by current leading systems (AMU16) and the automatic metric scores.", "labels": [], "entities": [{"text": "AMU16", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.9362620115280151}]}, {"text": "Notice that the CAMB16 sentence, which changes tooth pastes \u2192 tooth problems, is ranked the highest system output by GLEU and the second highest by IM and M 2 . All metrics score it higher than the unchanged Source sentence.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.7636207342147827}, {"text": "IM", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.5167548656463623}]}, {"text": "Another issues evidenced in the table is that IM and M 2 score the imperfect correction (CAMB14) as better than Reference; and according to M 2 , the Dummy output is better than Source.", "labels": [], "entities": [{"text": "imperfect correction (CAMB14)", "start_pos": 67, "end_pos": 96, "type": "METRIC", "confidence": 0.8620625138282776}]}, {"text": "We believe that the GEC field should take notes from the Workshop on Machine Translation (WMT) (.", "labels": [], "entities": [{"text": "GEC", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.7010104060173035}, {"text": "Machine Translation (WMT)", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.8175339639186859}]}, {"text": "There the participants in the evaluation shared tasks are also responsible for contributing system ranking judgments.", "labels": [], "entities": []}, {"text": "This makes the whole effort more community-driven and takes the pressure off one group from having to supply all annotations.", "labels": [], "entities": []}, {"text": "As we saw in the previous section, the majority of the commonly used datasets are limited to students, specifically college-level ESL writers.", "labels": [], "entities": []}, {"text": "To date, the overwhelmingly majority of publications benchmark on NUCLE, save fora few exceptions such as and which means that research efforts are becoming over-optimized for one set.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8132801055908203}]}, {"text": "This lack of diversity means that it is not clear how systems perform on other genres under different training conditions.", "labels": [], "entities": []}, {"text": "We should look to the parsing community as a warning sign.", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9656320810317993}]}, {"text": "For well over a decade, the field was heavily focused on improving parsing accuracy on the Penn Treebank), but robustness was greatly improved with the advent of Ontonotes () and the Google Web Treebank (Petrov and: Metric scores of three artificially contrived systems (Game), input source sentences (Src), and top 3 system outputs (Sys) on CoNLL14 data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.964346170425415}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9193261861801147}, {"text": "Penn Treebank", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9970705807209015}, {"text": "Google Web Treebank", "start_pos": 183, "end_pos": 202, "type": "DATASET", "confidence": 0.8985047936439514}, {"text": "CoNLL14 data", "start_pos": 342, "end_pos": 354, "type": "DATASET", "confidence": 0.9705122411251068}]}, {"text": "The bottom two rows show whether each metric scores the systems better than Game or worse than Source.", "labels": [], "entities": [{"text": "Game", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.724890947341919}]}, {"text": "Humans judge all systems be better than over Source..", "labels": [], "entities": []}, {"text": "Another issue is training data size.", "labels": [], "entities": []}, {"text": "The sister field of machine translation (MT) usually has datasets in the orders of millions or even tens of millions of sentence pairs.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.8483724236488343}]}, {"text": "The largest GEC datasets barely approach that figure, with 2.5 million sentences at a maximum, a number which includes sentences that were not corrected.", "labels": [], "entities": [{"text": "GEC datasets", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.8644199669361115}]}, {"text": "summarizes the strengths and weaknesses of the most commonly used GEC corpora across different properties ranging from size to diversity in native language (L1).", "labels": [], "entities": []}, {"text": "The most notable weakness across corpora is the lack of multiple reference corrections.", "labels": [], "entities": []}, {"text": "NUCLE contains two corrections per sentence and JFLEG 4.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9467350244522095}, {"text": "corrections", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9621067643165588}, {"text": "JFLEG 4", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.8685028851032257}]}, {"text": "M 2 and GLEU scores increase with more references but at a diminishing rate.", "labels": [], "entities": [{"text": "M 2", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6337641775608063}, {"text": "GLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9851250648498535}]}, {"text": "Further investigation is warranted to determine what an ideal number of references is, given the trade off between cost and reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.974850058555603}]}, {"text": "Some corpora contain little diversity in proficiency, topic, and/or native language of the writers (namely NUCLE and AESW), however AESW is the only corpus to contain sentences by native English speakers.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.9387046098709106}, {"text": "AESW", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.5467554330825806}]}, {"text": "The 2014 CoNLL shared task has enabled, for the first time, the development of evaluation metrics.", "labels": [], "entities": []}, {"text": "These metrics are evaluated by comparing their ranking of the shared task systems with the ranking done by human annotators.", "labels": [], "entities": []}, {"text": "showed that GLEU could rank systems closer to a human ranking than M 2 and IM, and a higher correlation could be found when combining GLEU with a reference-less fluency metric (.", "labels": [], "entities": []}, {"text": "However, it is important to take these results with a grain of salt-all benchmarking of the metrics was done with the CoNLL 2014 systems and data, and it remains to be seen if this ranking would hold on other, larger datasets.", "labels": [], "entities": [{"text": "CoNLL 2014 systems and data", "start_pos": 118, "end_pos": 145, "type": "DATASET", "confidence": 0.9541242599487305}]}, {"text": "Another issue with the metrics is the number of references available for comparison.", "labels": [], "entities": []}, {"text": "As in machine translation, the more references (humangenerated gold-standard corrections) one has, the better one can evaluate a system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.747078150510788}]}, {"text": "The CoNLL 2014 test set has 18 references annotated, but one can find examples where a system produces a correction which is not reflected in the references.", "labels": [], "entities": [{"text": "CoNLL 2014 test set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9750959724187851}]}, {"text": "This gets more complicated when human raters feel it is necessary to rewrite a sentence.", "labels": [], "entities": []}, {"text": "A third issue is that no metric directly measures meaning preservation.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.9261819124221802}]}, {"text": "This means that a system could produce a more fluent version of the original but accidentally change one word, and that could change the meaning of the whole sentence.", "labels": [], "entities": []}, {"text": "For example, if a system accidentally corrected documentary to document in \"The documentary gave a nice summary of global warming.\"", "labels": [], "entities": []}, {"text": "By current metrics, that error would have the same penalty as a minor spelling mistake.", "labels": [], "entities": []}, {"text": "Finally, the most commonly used GEC metric, M 2 , has a serious weakness, which has been noted in earlier papers).", "labels": [], "entities": []}, {"text": "The phrasal alignments under-penalize a sequence of incorrect tokens, and to illustrate how troubling this is, we tested a series of dummy systems, where each system produces the same sentence regardless of input (the sentences produced by each system area, a a, and a a a).", "labels": [], "entities": []}, {"text": "shows their scores on the CoNLL 2014 test set evaluated on the official NUCLE references (without alternatives), compared to the top 3 systems in the shared task, CAMB14), CUUI14 (Rozovskaya et al., 2014), and AMU14 (JunczysDowmunt and).", "labels": [], "entities": [{"text": "CoNLL 2014 test set", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.9405533075332642}, {"text": "NUCLE references", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.951788455247879}, {"text": "CUUI14", "start_pos": 172, "end_pos": 178, "type": "DATASET", "confidence": 0.8538165092468262}, {"text": "AMU14", "start_pos": 210, "end_pos": 215, "type": "DATASET", "confidence": 0.8419589400291443}, {"text": "JunczysDowmunt", "start_pos": 217, "end_pos": 231, "type": "DATASET", "confidence": 0.7774471640586853}]}, {"text": "The reader will notice that GLEU and IM score these sentences at or near zero, however according to M 2 , the dummy system that only returns the string \"a a\" scores higher than 7/13 systems participating in the 2014 Shared Task.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.6328743696212769}]}, {"text": "The IM score is also problematic in that the gamed sentences have the same score as the source.", "labels": [], "entities": [{"text": "IM", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.7338347434997559}]}], "tableCaptions": [{"text": " Table 1: GEC corpora available for free (for research purposes) and desired properties, identified in  \u00a73.1. and indicate  whether the corpus exhibits each property. Fluency edits for the NUCLE test set were added by Sakaguchi et al. (2016).", "labels": [], "entities": [{"text": "NUCLE test set", "start_pos": 189, "end_pos": 203, "type": "DATASET", "confidence": 0.9528569181760153}]}, {"text": " Table 2: Metric scores of three artificially contrived systems  (Game), input source sentences (Src), and top 3 system out- puts (Sys) on CoNLL14 data. The bottom two rows show  whether each metric scores the systems better than Game or  worse than Source. Humans judge all systems be better than  over Source.", "labels": [], "entities": [{"text": "CoNLL14 data", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.963725358247757}]}]}