{"title": [], "abstractContent": [{"text": "Sequence-to-sequence models with attention have been successful fora variety of NLP problems, but their speed does not scale well for tasks with long source sequences such as document summariza-tion.", "labels": [], "entities": []}, {"text": "We propose a novel coarse-to-fine attention model that hierarchically reads a document, using coarse attention to select top-level chunks of text and fine attention to read the words of the chosen chunks.", "labels": [], "entities": []}, {"text": "While the computation for training standard attention models scales linearly with source sequence length, our method scales with the number of top-level chunks and can handle much longer sequences.", "labels": [], "entities": []}, {"text": "Empirically , we find that while coarse-to-fine attention models lag behind state-of-the-art baselines, our method achieves the desired behavior of sparsely attending to subsets of the document for generation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The sequence-to-sequence architecture of , also known as the encoder-decoder architecture, is now the gold standard for many NLP tasks, including machine translation (, question answering (), dialogue (, caption generation (, and in particular summarization (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7913425266742706}, {"text": "question answering", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7996228635311127}, {"text": "caption generation", "start_pos": 204, "end_pos": 222, "type": "TASK", "confidence": 0.8941076695919037}, {"text": "summarization", "start_pos": 244, "end_pos": 257, "type": "TASK", "confidence": 0.9806177020072937}]}, {"text": "A popular variant of sequence-to-sequence models are attention models (.", "labels": [], "entities": []}, {"text": "By keeping an encoded representation of each part of the input, we \"attend\" to the relevant part each time we produce an output from the decoder.", "labels": [], "entities": []}, {"text": "In practice, this means computing attention weights for all encoder hidden states, then taking the weighted average as our new context vector.", "labels": [], "entities": []}, {"text": "While successful, existing sequence-tosequence methods are computationally limited by the length of source and target sequences.", "labels": [], "entities": []}, {"text": "For a problem such as document summarization, a source sequence of length N (where N could potentially be very large) requires O(N ) model computations to encode.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7399176955223083}]}, {"text": "However, it makes sense intuitively that not every word of the source will be necessary for generating a summary, and so we would like to reduce the amount of computation performed on the source.", "labels": [], "entities": []}, {"text": "Therefore, in order to scale attention models for this problem, we aim to prune down the length of the source sequence in an intelligent way.", "labels": [], "entities": []}, {"text": "Instead of naively attending to all the words of the source at once, our solution is to use a two-layer hierarchical attention.", "labels": [], "entities": []}, {"text": "For document summarization, this means dividing the document into chunks of text, sparsely attending to one or a few chunks at a time using hard attention, then applying the usual full attention over those chunks -we call this method coarse-to-fine attention.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.5817049443721771}]}, {"text": "Through experiments, we find that while coarse-to-fine attention does not perform as well as standard attention, it does show the desired behavior of sparsely reading the source sequence.", "labels": [], "entities": []}, {"text": "We structure the rest of the paper as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce related work on summarization and neural attention.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9933642148971558}]}, {"text": "In Section 3, we review the encoder-decoder framework, and in Section 4 introduce our models.", "labels": [], "entities": []}, {"text": "In Section 5, we describe our experimental setup, and in Section 6 show results.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report metrics for perplexity and ROUGE balanced F-scores) on the test set.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9990366697311401}, {"text": "F-scores", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9120733141899109}]}, {"text": "With multiple gold summaries in the CNN/Dailymail highlights, we take the max ROUGE score over the gold summaries fora predicted summary, as our models are trained to produce a single sentence.", "labels": [], "entities": [{"text": "CNN/Dailymail highlights", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.9524336457252502}, {"text": "max", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9838634729385376}, {"text": "ROUGE score", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.9138989150524139}]}, {"text": "The final metric is then the average overall test data points.", "labels": [], "entities": []}, {"text": "Note that because we are training the model to output a single highlight, our numbers are not comparable with or. shows summarization results.", "labels": [], "entities": []}, {"text": "We see that our soft attention models comfortably beat the baselines, while hard attention lags behind.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summarization results for CNN/Dailymail (first highlight as target) on perplexity (PPL) and ROUGE metrics.", "labels": [], "entities": [{"text": "CNN/Dailymail", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.885674774646759}, {"text": "ROUGE", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9040102958679199}]}, {"text": " Table 2: Entropy over coarse attention, averaged over all at-", "labels": [], "entities": [{"text": "Entropy", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9838442206382751}]}]}