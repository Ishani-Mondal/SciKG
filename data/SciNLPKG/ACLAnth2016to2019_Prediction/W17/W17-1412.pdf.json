{"title": [{"text": "The First Cross-Lingual Challenge on Recognition, Normalization and Matching of Named Entities in Slavic Languages", "labels": [], "entities": [{"text": "Recognition, Normalization and Matching of Named Entities in Slavic Languages", "start_pos": 37, "end_pos": 114, "type": "TASK", "confidence": 0.7852960554036227}]}], "abstractContent": [{"text": "This paper describes the outcomes of the First Multilingual Named Entity Challenge in Slavic Languages.", "labels": [], "entities": []}, {"text": "The Challenge targets recognizing mentions of named entities in web documents, their normal-ization/lemmatization, and cross-lingual matching.", "labels": [], "entities": [{"text": "recognizing mentions of named entities in web documents", "start_pos": 22, "end_pos": 77, "type": "TASK", "confidence": 0.8099141418933868}, {"text": "cross-lingual matching", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.724882185459137}]}, {"text": "The Challenge was organized in the context of the 6th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL-2017 conference.", "labels": [], "entities": [{"text": "Balto-Slavic Natural Language Processing Workshop", "start_pos": 54, "end_pos": 103, "type": "TASK", "confidence": 0.5993495762348175}]}, {"text": "Eleven teams registered for the evaluation, two of which submitted results on schedule , due to the complexity of the tasks and short time available for elaborating a solution.", "labels": [], "entities": []}, {"text": "The reported evaluation figures reflect the relatively higher level of complexity of named entity tasks in the context of Slavic languages.", "labels": [], "entities": []}, {"text": "Since the Challenge extends beyond the date of the publication of this paper, updates to the results of the participating systems can be found on the official web page of the Challenge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to the rich inflection, derivation, free word order, and other morphological and syntactic phenomena exhibited by Slavic languages, analysis of named entities (NEs) in these languages poses a challenging task.", "labels": [], "entities": [{"text": "analysis of named entities (NEs)", "start_pos": 136, "end_pos": 168, "type": "TASK", "confidence": 0.8613041979925973}]}, {"text": "Fostering research and development on detection and lemmatization of NEsand the closely related problem of entity linkingis of paramount importance for enabling effective multilingual and cross-lingual information access in these languages.", "labels": [], "entities": [{"text": "entity linkingis", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.6867409944534302}]}, {"text": "This paper describes the outcomes of the first shared task on multilingual named entity recognition (NER) that aims at recognizing mentions of named entities in web documents in Slavic languages, their normalization/lemmatization, and cross-lingual matching.", "labels": [], "entities": [{"text": "multilingual named entity recognition (NER)", "start_pos": 62, "end_pos": 105, "type": "TASK", "confidence": 0.7761148640087673}, {"text": "recognizing mentions of named entities in web documents in Slavic languages", "start_pos": 119, "end_pos": 194, "type": "TASK", "confidence": 0.8159469853747975}, {"text": "cross-lingual matching", "start_pos": 235, "end_pos": 257, "type": "TASK", "confidence": 0.7742427587509155}]}, {"text": "The task initially covers seven languages and four types of NEs: person, location, organization, and miscellaneous, where the last category covers all other types of named entities, e.g., event or product.", "labels": [], "entities": []}, {"text": "The input text collection consists of documents in seven Slavic languages collected from the web, each collection revolving around a certain \"focus\" entity.", "labels": [], "entities": []}, {"text": "The main rationale of such a setup is to foster development of \"all-rounder\" NER and cross-lingual entity matching solutions that are not tailored to specific, narrow domains.", "labels": [], "entities": [{"text": "cross-lingual entity matching", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.5895397265752157}]}, {"text": "The shared task was organized in the context of the 6th Balto-Slavic Natural Language Processing Workshop co-located with the EACL 2017 conference.", "labels": [], "entities": [{"text": "Balto-Slavic Natural Language Processing Workshop co-located with the EACL 2017 conference", "start_pos": 56, "end_pos": 146, "type": "TASK", "confidence": 0.6071562387726523}]}, {"text": "Similar shared tasks have been organized previously.", "labels": [], "entities": []}, {"text": "The first non-English monolingual NER evaluations-covering Chinese, Japanese, Spanish, and Arabic-were carried out in the context of the Message Understanding Conferences (MUCs) and the ACE Programme ().", "labels": [], "entities": [{"text": "NER evaluations-covering Chinese", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.8977656960487366}, {"text": "Message Understanding Conferences (MUCs)", "start_pos": 137, "end_pos": 177, "type": "TASK", "confidence": 0.6910384396711985}, {"text": "ACE Programme", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.9056257009506226}]}, {"text": "The first shared task focusing on multilingual named entity recognition, which covered some European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences.", "labels": [], "entities": [{"text": "multilingual named entity recognition", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.5705045014619827}]}, {"text": "The NE types covered in these campaigns were similar to the NE types covered in our Challenge.", "labels": [], "entities": []}, {"text": "Also related to our task is the Entity Discovery and Linking (EDL) track) of the NIST Text Analysis Conferences (TAC).", "labels": [], "entities": [{"text": "Entity Discovery and Linking (EDL)", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.8461792639323643}, {"text": "NIST Text Analysis Conferences (TAC)", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.7337573170661926}]}, {"text": "EDL aimed to extract entity mentions from a collection of textual documents in multiple languages, and to partition the entities into cross-document equivalence classes, by either linking mentions to a knowledge base or directly 76 clustering them.", "labels": [], "entities": []}, {"text": "An important difference between EDL and our task is that we do not link entities to a knowledge base.", "labels": [], "entities": []}, {"text": "Related to cross-lingual NE recognition is NE transliteration, i.e., linking NEs across languages that use different scripts.", "labels": [], "entities": [{"text": "cross-lingual NE recognition", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.6339832146962484}]}, {"text": "A series of NE Transliteration Shared Tasks were organized as apart of NEWS-Named Entity Workshops-(, focusing mostly on Indian and Asian languages.", "labels": [], "entities": []}, {"text": "In 2010, the NEWS Workshop included a shared task on Transliteration Mining (, i.e., mining of names from parallel corpora.", "labels": [], "entities": [{"text": "NEWS Workshop", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.8530819714069366}, {"text": "Transliteration Mining", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.8997506201267242}]}, {"text": "This task included corpora in English, Chinese, Tamil, Russian, and Arabic.", "labels": [], "entities": []}, {"text": "Prior work targeting NEs specifically for Slavic languages includes tools for NE recognition for Croatian (), a tool tailored for NE recognition in Croatian tweets (), a manually annotated NE corpus for Croatian, tools for NE recognition in Slovene (), a Czech corpus of 11,000 manually annotated NEs (, NER tools for Czech (, tools and resources for fine-grained annotation of NEs in the National Corpus of Polish () and a recent shared task on NE Recognition in Russian (.", "labels": [], "entities": [{"text": "NE recognition", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.8861936330795288}, {"text": "NE recognition in Croatian tweets", "start_pos": 130, "end_pos": 163, "type": "TASK", "confidence": 0.8621302962303161}, {"text": "NE recognition", "start_pos": 223, "end_pos": 237, "type": "TASK", "confidence": 0.8639269471168518}, {"text": "National Corpus of Polish", "start_pos": 389, "end_pos": 414, "type": "DATASET", "confidence": 0.8684862405061722}, {"text": "NE Recognition", "start_pos": 446, "end_pos": 460, "type": "TASK", "confidence": 0.7424923181533813}]}, {"text": "To the best of our knowledge, the shared task described in this paper is the first attempt at multilingual name recognition, normalization, and cross-lingual entity matching that covers a large number of Slavic languages.", "labels": [], "entities": [{"text": "multilingual name recognition", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.6035285194714864}, {"text": "cross-lingual entity matching", "start_pos": 144, "end_pos": 173, "type": "TASK", "confidence": 0.6074661413828532}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the task; Section 3 describes the annotation of the dataset.", "labels": [], "entities": []}, {"text": "The evaluation methodology is introduced in Section 4.", "labels": [], "entities": []}, {"text": "Participant systems are described in Section 5 and the results obtained by these systems are presented in Section 6.", "labels": [], "entities": []}, {"text": "Finally, lessons learnt and conclusions are discussed in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The registered participants were provided two trial datasets: (1) a dataset related to Beata Szyd\u0142o, the current prime minister of Poland, and (2) a dataset related to ISIS, the so-called \"Islamic State of Iraq and Syria\" terrorist group.", "labels": [], "entities": []}, {"text": "These datasets consisted of 187 and 186 documents, respectively, with equal distribution of documents across the seven languages of interest.", "labels": [], "entities": []}, {"text": "Two datasets were prepared for evaluation, each consisting of documents extracted from the web and related to a given entity.", "labels": [], "entities": []}, {"text": "One dataset contains documents related to Donald Trump, the recently elected President of United States (henceforth referred to as TRUMP), and the second dataset con- The test datasets were created as follows.", "labels": [], "entities": [{"text": "TRUMP", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.855949342250824}]}, {"text": "For each \"focus\" entity, we posed a separate search query to Google, in each of the seven target languages.", "labels": [], "entities": []}, {"text": "The query returned links to documents only in the language of interest.", "labels": [], "entities": []}, {"text": "We extracted the first 100 links 2 returned by the search engine, removed duplicate links, downloaded the corresponding HTML pages-mainly news articles or fragments thereof-and converted them into plain text, using a hybrid HTML parser.", "labels": [], "entities": []}, {"text": "This process was done semi-automatically using the tool described in.", "labels": [], "entities": []}, {"text": "In particular, some of the meta-data fields-i.e., creation date, title, URL-were automatically computed using this tool.", "labels": [], "entities": []}, {"text": "HTML parsing resulted in texts that included not only the core text of a web page, but also some additional pieces of text, e.g., a list of labels from a menu, user comments, etc., which may not constitute well-formed utterances in the target language.", "labels": [], "entities": [{"text": "HTML parsing", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8860082030296326}]}, {"text": "This occurred in a small fraction of texts processed.", "labels": [], "entities": []}, {"text": "Some of these texts were included in the test dataset in order to maintain the flavour of \"real-data.\"", "labels": [], "entities": []}, {"text": "However, obvious HTML parser failure (e.g., extraction of JavaScript code, extraction of empty texts, etc.) were removed from the data sets.", "labels": [], "entities": []}, {"text": "Some of the downloaded documents were additionally polished by removing erroneously extracted boilerplate content.", "labels": [], "entities": []}, {"text": "The resulting set of partially \"cleaned\" documents were used to select circa 20-25 documents for each language and topic, for the preparation of the final test datasets.", "labels": [], "entities": []}, {"text": "Annotations for Croatian, Czech, Polish, Russian, and Slovene were made by native speakers; annotations for Slovak were made by native speakers of Czech, capable of understanding Slovak.", "labels": [], "entities": []}, {"text": "Annotations for Ukrainian were made partly by native speakers and partly by near-native speakers of Ukrainian.", "labels": [], "entities": []}, {"text": "Cross-lingual alignment of the entity identifiers was performed by two annotators.", "labels": [], "entities": []}, {"text": "provides more quantitative details about the annotated datasets.", "labels": [], "entities": []}, {"text": "gives the breakdown of entity classes.", "labels": [], "entities": []}, {"text": "It is noteworthy that a high proportion of the annotated mentions have abase form that differs from the form appearing in text.", "labels": [], "entities": []}, {"text": "For instance, for the TRUMP dataset this figure is between 37.5% (Slovak) and 57.5% (Croatian).: Quantitative data about the test datasets.", "labels": [], "entities": [{"text": "TRUMP dataset", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.7272323966026306}]}, {"text": "#docs and #ment refer to the number of documents and NE mention annotations, respectively.", "labels": [], "entities": []}, {"text": "provides examples of genitive forms of the name \"European Commission\" that occurred in the ECOMMISSION corpus frequently.", "labels": [], "entities": [{"text": "ECOMMISSION corpus", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.9387233853340149}]}, {"text": "While normalization of the inflected forms in could be achieved by lemmatization of each of the constituents of the noun phrase separately and then concatenating the corresponding base forms together, many entity mentions in the test dataset are complex noun phrases, whose lemmatization requires detection of inner syntactic structure.", "labels": [], "entities": []}, {"text": "For instance, the inflected form of the Polish proper name Europejskiego Funduszu Rozwoju Regionalnego (European GEN Fund GEN Development GEN Regional GEN ) consists of two basic genitive noun phrases, of which only the first one (\"European Fund\") needs to be normalized, whereas the second (\"Regional Development\") should remain unchanged.", "labels": [], "entities": [{"text": "Polish proper name Europejskiego Funduszu Rozwoju Regionalnego (European GEN Fund GEN Development GEN Regional GEN )", "start_pos": 40, "end_pos": 156, "type": "DATASET", "confidence": 0.6967484074480393}]}, {"text": "The corresponding base form is \"Europejski Fundusz Rozwoju Regionalnego\".", "labels": [], "entities": [{"text": "Europejski Fundusz Rozwoju Regionalnego\"", "start_pos": 32, "end_pos": 72, "type": "DATASET", "confidence": 0.9311875462532043}]}, {"text": "Since in some Slavic languages adjectives may precede or follow a noun in a noun phrase (like in the example above), detection of inner syntactic structure of complex proper names is not trivial, and thus complicates the process of automated lemmatization.", "labels": [], "entities": [{"text": "detection of inner syntactic structure of complex proper names", "start_pos": 117, "end_pos": 179, "type": "TASK", "confidence": 0.7334467834896512}]}, {"text": "Complex person name declension paradigms () add another level of complexity.", "labels": [], "entities": []}, {"text": "It is worth mentioning that, for the sake of compliance with the NER guidelines in Section 2, documents that included hard-to-decide entity mention annotations were excluded from the test datasets for the present.", "labels": [], "entities": [{"text": "NER", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.5668613910675049}]}, {"text": "A casein point is a document in Croatian that contained the phrase \"Zagreba\u010dka, Sisa\u010dko-Moslava\u010dka i Karlova\u010dka \u017eupanija\"-a contracted version of three named entities (\"Zagreba\u010dka \u017eupanija\",: Inflected (genitive) forms of the name \"European Commission\" found in test data.", "labels": [], "entities": []}, {"text": "\"Sisa\u010dko-Moslava\u010dka \u017eupanija\", and \"Karlova\u010dka \u017eupanija\") expressed using ahead noun with three coordinate modifiers.", "labels": [], "entities": []}, {"text": "The NER task (exact case-insensitive matching) and Name Normalization task (also called \"lemmatization\") were evaluated in terms of precision, recall, and F1-scores.", "labels": [], "entities": [{"text": "NER task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.6680698692798615}, {"text": "Name Normalization", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6708472520112991}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.99950110912323}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9992002844810486}, {"text": "F1-scores", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9980554580688477}]}, {"text": "In relaxed evaluation mode we additionally distinguish between exact and partial matching, i.e., in the case of the latter an entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one partial match of a named mention of this entity.", "labels": [], "entities": []}, {"text": "In the evaluation we consider various levels of granularity, i.e., the performance for: (a) all NE types and all languages, (b) each particular NE 80  type and all languages, (c) all NE types for each language, and (d) each particular NE type per language.", "labels": [], "entities": []}, {"text": "In the name normalization sub-task, only correctly recognized entity mentions in the system response and only those that were normalized (on both the annotation and system's sides) are taken into account.", "labels": [], "entities": [{"text": "name normalization", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7138663828372955}]}, {"text": "Formally, let correct N denote the number of all correctly recognized entity mentions for which the system returned a correct base form.", "labels": [], "entities": []}, {"text": "Let key N denote the number of all normalized entity mentions in the gold-standard answer key and response N denote the number of all normalized entity mentions in the system's response.", "labels": [], "entities": []}, {"text": "We define precision and recall for the name normalization task as: In evaluating the document-level, singlelanguage and cross-lingual entity matching task we have adapted the Link-Based Entity-Aware metric (LEA)) which considers how important the entity is and how well it is resolved.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9991152882575989}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9991835951805115}, {"text": "name normalization task", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8444371422131857}, {"text": "cross-lingual entity matching task", "start_pos": 120, "end_pos": 154, "type": "TASK", "confidence": 0.7033989802002907}]}, {"text": "LEA is defined as follows.", "labels": [], "entities": [{"text": "LEA", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6857231855392456}]}, {"text": "Let K = {k 1 , k 2 , . .", "labels": [], "entities": []}, {"text": ", k |K| } and R = {r 1 , r 2 , . .", "labels": [], "entities": []}, {"text": ", r |R| } denote the key entity set and the response entity set, respectively, i.e., k i \u2208 K (r i \u2208 R) stand for set of mentions of the same entity in the key entity set (response entity set).", "labels": [], "entities": []}, {"text": "LEA recall and precision are then defined as follows: where imp and res denote the measure of importance and the resolution score for an entity, respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7290179133415222}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9991694688796997}, {"text": "resolution score", "start_pos": 113, "end_pos": 129, "type": "METRIC", "confidence": 0.9668105244636536}]}, {"text": "In our setting, we define imp(e) = log 2 |e| for an entity e (in K or R), |e| is the number of mentions of e-i.e., the more mentions an entity has the more important it is.", "labels": [], "entities": []}, {"text": "To avoid biasing the importance of the more frequent entities log is used.", "labels": [], "entities": []}, {"text": "The resolution score of key entity k i is computed as the fraction of correctly resolved coreference links of k i : where link (e) = (|e| \u00d7 (|e| \u2212 1))/2 is the number of unique co-reference links in e.", "labels": [], "entities": [{"text": "resolution score", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9702803194522858}]}, {"text": "For each k i , LEA checks all response entities to check whether they are partial matches fork i . Analogously, the resolution score of response entity r i is computed as the fraction of co-reference links in r i that are extracted correctly: Using LEA brings several benefits.", "labels": [], "entities": [{"text": "resolution score", "start_pos": 116, "end_pos": 132, "type": "METRIC", "confidence": 0.9710528552532196}]}, {"text": "For example, LEA considers resolved co-reference relations instead of resolved mentions and has more discriminative power than other metrics used for evaluation of co-reference resolution.", "labels": [], "entities": [{"text": "LEA", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.6543692350387573}, {"text": "co-reference resolution", "start_pos": 164, "end_pos": 187, "type": "TASK", "confidence": 0.7286948412656784}]}, {"text": "It is important to note at this stage that the evaluation was carried out in \"case-insensitive\" mode: all named mentions in system response and test corpora were lowercased.", "labels": [], "entities": []}, {"text": "The results of the runs submitted for Phase I are presented in.", "labels": [], "entities": []}, {"text": "The figures provided for the recognition are micro-averaged F1-scores.", "labels": [], "entities": [{"text": "recognition", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9514363408088684}, {"text": "F1-scores", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9610549807548523}]}, {"text": "For normalization, we report F1-scores, using the Recall N and Precision N definitions from Section 4, computed for entity mentions for which the annotation or system response contains a different base form compared to the surface form.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9969657063484192}, {"text": "Recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9573232531547546}, {"text": "Precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.8883156180381775}]}, {"text": "This evaluation includes only correctly recognized entity mentions to suppress the influence of entity recognition performance.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.6899588257074356}]}, {"text": "Lastly, for entity matching, the micro-averaged F1-scores are provided, computed using LEA precision and recall values (see Section 4).", "labels": [], "entities": [{"text": "entity matching", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.819800466299057}, {"text": "F1-scores", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.8288397789001465}, {"text": "LEA precision", "start_pos": 87, "end_pos": 100, "type": "METRIC", "confidence": 0.7974476516246796}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.99446702003479}]}, {"text": "System pw performed substantially better on Polish than system jhu.", "labels": [], "entities": []}, {"text": "Considering the entity types, performance was overall better for LOC and PER, and substantially lower for ORG and MISC, which is not unexpected.", "labels": [], "entities": [{"text": "ORG", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.7077540755271912}, {"text": "MISC", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.5286685824394226}]}, {"text": "Considering the tested languages and scenarios, system jhu achieved best performance on TRUMP in Croatian, its poorest performance was on ECOMMISSION in Ukrainian.", "labels": [], "entities": [{"text": "TRUMP", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.6982027292251587}]}, {"text": "System pw performed better on the TRUMP scenario than on ECOMMISSION.", "labels": [], "entities": [{"text": "ECOMMISSION", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.885306715965271}]}, {"text": "Overall, the TRUMP scenario appears to be easier, due to the mix of named entities that predominate in the texts.", "labels": [], "entities": [{"text": "TRUMP", "start_pos": 13, "end_pos": 18, "type": "TASK", "confidence": 0.9740769863128662}]}, {"text": "The ECOMMIS-SION documents discuss organizations with complex geo-political inter-relationships and affiliations.", "labels": [], "entities": [{"text": "ECOMMIS-SION documents", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9267256557941437}]}, {"text": "Furthermore, cross-lingual co-reference seems to be a difficult task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Quantitative data about the test datasets.  #docs and #ment refer to the number of documents  and NE mention annotations, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Breakdown of the annotations according  to the entity type.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9574840664863586}]}, {"text": " Table 5: Breakdown of the recognition perfor- mance according to the entity type for TRUMP  dataset.", "labels": [], "entities": [{"text": "TRUMP  dataset", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.7458666265010834}]}, {"text": " Table 6: Breakdown of the recognition perfor- mance according to the entity type for ECOMMIS- SION dataset.", "labels": [], "entities": [{"text": "ECOMMIS- SION dataset", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.769072413444519}]}]}