{"title": [{"text": "Improving the generation of personalised descriptions", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9695286154747009}]}], "abstractContent": [{"text": "Referring expression generation (REG) models that use speaker-dependent information require a considerable amount of training data produced by every individual speaker, or may otherwise perform poorly.", "labels": [], "entities": [{"text": "Referring expression generation (REG)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.778541753689448}]}, {"text": "In this work we propose a simple personalised method for this task, in which speakers are grouped into profiles according to their referential behaviour.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation shows that the use of speaker's profiles generally outperforms the personalised method found in previous work.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural language generation systems, referring expression generation (REG) is the microplanning task responsible for generating references of discourse entities).", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7065497438112894}, {"text": "referring expression generation (REG)", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.784730926156044}]}, {"text": "Choice of referential form, i.e., deciding whether a reference should be a proper name ('Ayrton Senna'), a pronoun ('He') or a description ('The racing driver'), is the first decision to be made in this task.", "labels": [], "entities": []}, {"text": "Albeit notable studies on pronominalisation) and proper name generation, research on REG has largely focused on the generation of descriptions or, more specifically, on content selection.", "labels": [], "entities": [{"text": "proper name generation", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7401865522066752}, {"text": "REG", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9581258893013}, {"text": "content selection", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.6925909072160721}]}, {"text": "For instance, in the previous example, Ayrton Senna's occupation is the content selected to describe him.", "labels": [], "entities": []}, {"text": "This work focuses on this kind of content selection task, hereby called REG for brevity.", "labels": [], "entities": [{"text": "content selection task", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.7592231432596842}]}, {"text": "Existing work in computational REG and related fields have identified a wide range of factors that may drive content selection.", "labels": [], "entities": [{"text": "content selection", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7302756309509277}]}, {"text": "To a considerable extent, however, content selection is known to be influenced by human variation.", "labels": [], "entities": [{"text": "content selection", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8256921172142029}]}, {"text": "In other words, under identical circumstances (i.e., in the same referential context), different speakers will often produce different descriptions, and a single entity maybe described by different speakers as 'the racing driver', 'the McLaren pilot', etc.", "labels": [], "entities": []}, {"text": "Existing REG algorithms as in Bohnet and usually pay regard to human variation by computing personalised features from a training set of descriptions produced by each speaker.", "labels": [], "entities": []}, {"text": "This highly personalised training method may of course be considered an ideal account of human variation but, in practice, will only be effective if every speaker in the domain is represented by a sufficiently large number of training instances.", "labels": [], "entities": []}, {"text": "As means to improve REG results when the amount of training data is limited, in this work we propose a simple training method for speakerdependent REG in which training referring expressions are grouped into profiles according to the speaker's referential behaviour.", "labels": [], "entities": [{"text": "REG", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9811181426048279}]}, {"text": "The method relies on the observation that speakers tend to be consistent in their choices of referential overspecification, and it is shown to outperform the use of personalised information.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Six REG datasets: TUNA-Furniture and TUNA-People () (in both cases, only descriptions to single objects were considered), GRE3D3 (), GRE3D7), Stars () and Stars2 ().", "labels": [], "entities": [{"text": "REG datasets", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.7140714377164841}, {"text": "GRE3D3", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.6233994960784912}, {"text": "GRE3D7", "start_pos": 138, "end_pos": 144, "type": "DATASET", "confidence": 0.7379709482192993}]}, {"text": "Models As in, all classifiers were built using Support Vector Machines (SVMs) with a Gaussian Kernel.", "labels": [], "entities": []}, {"text": "For the relation prediction, we use an \"one-against-one\" multi-class method.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.9264843761920929}]}, {"text": "All models were evaluated using crossvalidation with a balanced number of referring expressions per participant within each fold.", "labels": [], "entities": []}, {"text": "For TUNA and Stars, descriptions were divided into six folds each.", "labels": [], "entities": [{"text": "TUNA and Stars", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.6412229537963867}]}, {"text": "For GRE3D3/7 and Stars2, descriptions were divided into ten folds each.", "labels": [], "entities": [{"text": "GRE3D3/7", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7686732212702433}]}, {"text": "Grid-search was used to obtain an optimal model setting by testing values for the SVM C parameter (1, 10, 100 and 1000) and the Gaussian kernel \u03b3 (1, 0.1, 0.01, and 0.001) in a validation set before the test step.", "labels": [], "entities": []}, {"text": "Baseline We make use of a baseline method called Speaker.", "labels": [], "entities": []}, {"text": "In this method, classifiers are trained on the set of referring expressions produced by each individual speaker.", "labels": [], "entities": []}, {"text": "Metrics We measured Dice coefficients to assess the similarity between each description generated by the model and the corpus description.", "labels": [], "entities": []}, {"text": "We also computed the overall REG Accuracy  by counting the number of exact matches between each description pair.", "labels": [], "entities": [{"text": "REG", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.6206890344619751}, {"text": "Accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8163257837295532}]}, {"text": "Regarding the results in individual domains, we notice that Profile outperforms Speaker in terms of Dice scores in the case of TUNA-People, GRE3D3, GRE3D7 and Stars2.", "labels": [], "entities": [{"text": "Dice", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9591833353042603}, {"text": "GRE3D3", "start_pos": 140, "end_pos": 146, "type": "DATASET", "confidence": 0.7455117702484131}, {"text": "GRE3D7", "start_pos": 148, "end_pos": 154, "type": "DATASET", "confidence": 0.840255618095398}]}, {"text": "A pairwise comparison shows that these differences are significant at p<.01.", "labels": [], "entities": []}, {"text": "In the case of TUNA-Furniture and Stars the difference was not significant.", "labels": [], "entities": [{"text": "TUNA-Furniture and Stars", "start_pos": 15, "end_pos": 39, "type": "DATASET", "confidence": 0.6758884787559509}]}, {"text": "Profile also outperforms Speaker in terms of Accuracy in TUNA-People, GRE3D3, GRE3D7 and Stars2, with pairwise comparisons significant at p<0.01.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9995170831680298}, {"text": "TUNA-People", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.8116308450698853}, {"text": "GRE3D3", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.7591158151626587}, {"text": "GRE3D7", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.8332483768463135}]}, {"text": "In the case of TUNA-Furniture, the difference was not significant, and in the case of Stars a significant effect in the opposite direction was observed (\u03c7 2 =9.38, p<0.01).", "labels": [], "entities": [{"text": "TUNA-Furniture", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.8593940734863281}]}], "tableCaptions": [{"text": " Table 2: Reference type classification for each corpus", "labels": [], "entities": []}]}