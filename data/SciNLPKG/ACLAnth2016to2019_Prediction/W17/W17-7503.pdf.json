{"title": [{"text": "Three-phase training to address data sparsity in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.8295159339904785}]}], "abstractContent": [{"text": "Data sparsity is a key problem in contemporary neural machine translation (NMT) techniques, especially for resource-scarce language pairs.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.8707476158936819}]}, {"text": "NMT models when coupled with large, high quality parallel corpora provide promising results and are an emerging alternative to phrase-based Statistical Machine Translation (SMT) systems.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 140, "end_pos": 177, "type": "TASK", "confidence": 0.7648986975351969}]}, {"text": "A solution to overcome data sparsity can facilitate leveraging of NMT models across language pairs, thereby providing high quality translations despite the lack of large parallel corpora.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate a three-phase integrated approach which combines weakly supervised and semi-supervised learning with NMT techniques to build a robust model using a limited amount of parallel data.", "labels": [], "entities": []}, {"text": "We conduct experiments for five language pairs (thereby generating ten systems) and our results show a substantial increase in translation quality over a baseline NMT model trained only on parallel data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) is an emerging technique which utilizes deep neural networks,), () to generate end-to-end translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8034264395634333}]}, {"text": "NMT has shown promising results for various language pairs and has been consistently performing better than Phrase based SMT, the state-of-the-art MT paradigm until a few years back.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8653013110160828}, {"text": "Phrase based SMT", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.5237427254517873}, {"text": "MT paradigm", "start_pos": 147, "end_pos": 158, "type": "TASK", "confidence": 0.9089489877223969}]}, {"text": "A major benefit in NMT which makes it so popular is its ability to use deep neural networks and learn linguistic information from the parallel data itself without being fed any learning features.", "labels": [], "entities": []}, {"text": "This makes it a conceptually simple method which provides significantly better translations than other MT paradigms like rule-based MT and statistical MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.978602409362793}, {"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.797315239906311}, {"text": "statistical MT", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.5310393124818802}]}, {"text": "Furthermore, it eliminates the need for complex feature engineering by providing end-to-end translation.", "labels": [], "entities": []}, {"text": "The newly proposed attention mechanism is a valuable addition to NMT contributing to significant gain in performance.", "labels": [], "entities": []}, {"text": "NMT systems have achieved competitive accuracy scores under large-data training conditions for language pairs such as En \u2192 Fr (English -French) and En \u2192 De (English -German).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9957298636436462}]}, {"text": "However, on the other hand, NMT models are unable to extract sufficient linguistic information in terms of morphology, word order, syntactic structure and semantics in low resource scenario.", "labels": [], "entities": []}, {"text": "This makes translation among morphologically rich languages especially challenging.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9843853116035461}]}, {"text": "Also, due to the unavailability of large parallel corpora, the vocabulary size tends to below, due to which any word which is not included in the vocabulary is mapped to a special token representing an unknown word.", "labels": [], "entities": []}, {"text": "This causes a large number of's in the target sentence, which results in a drastic drop in the translation quality.", "labels": [], "entities": []}, {"text": "This behaviour makes vanilla NMT a poor choice for low resource language pairs, especially if they are morphologically rich.", "labels": [], "entities": []}, {"text": "In this paper, we propose an integrated approach for reducing the impact of data sparsity in NMT, which leverages a large monolingual corpus of the source language, which is easier to obtain in comparison to parallel corpus.", "labels": [], "entities": []}, {"text": "We employ a small parallel corpus in addition to the monolingual 13 corpus, and through a combination of weaklysupervised and semi-supervised learning, we build an efficient model which delivers promising results.", "labels": [], "entities": []}, {"text": "Our approach along with the intuition driving it is described in detail in Section 4.", "labels": [], "entities": []}, {"text": "Our model obtains an improvement of five to eight points in BLEU score over an attention based encoder-decoder model trained over a parallel corpus.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9766518771648407}]}, {"text": "The results obtained on test sets from different domains are also promising, which suggests that the proposed model is able to perform domain adaptation successfully due to the presence of a rich vocabulary learnt from hree-phase training.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.7508166134357452}]}, {"text": "The main contributions of our work are : \u2022 We propose an integrated approach which combines weakly-supervised learning and semi-supervised learning to reduce the impact of data sparsity on NMT, by utilizing a large monolingual corpus of the source language in addition to a small parallel corpus.", "labels": [], "entities": []}, {"text": "\u2022 We tweak the NMT architecture to generate optimum performance and conduct experiments on different Indian language pairs using the proposed approach.", "labels": [], "entities": []}, {"text": "We demonstrate that we are able to build a robust NMT model which produces quality translation and delivers promising results, significantly better than a baseline NMT model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employ a small parallel corpus and large monolingual corpora for training.", "labels": [], "entities": []}, {"text": "For the former, we use the multilingual Indian Language Corpora Initiative (ILCI) corpus 1 , which contains 50,000 sentences from the health and tourism domains aligned across eleven Indian languages.", "labels": [], "entities": [{"text": "Indian Language Corpora Initiative (ILCI) corpus 1", "start_pos": 40, "end_pos": 90, "type": "DATASET", "confidence": 0.6127642028861575}]}, {"text": "We employed manual preprocessing to eliminate misalignments -the resultant dataset has a size of 47,382 sentences.", "labels": [], "entities": []}, {"text": "These are split randomly into training set, validation set and test set containing 44,000, 1382 and 2000 sentences respectively.", "labels": [], "entities": []}, {"text": "The statistics for the ILCI corpus are given in.", "labels": [], "entities": [{"text": "ILCI corpus", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.9526820480823517}]}, {"text": "We use the EMILLE monolingual corpora () for five languages and the UrMonoCorp () for Coarse Learning detailed in Section 4. 2 . These statistics are given in.", "labels": [], "entities": [{"text": "UrMonoCorp", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.7744863629341125}]}, {"text": "In addition to these, we extract samples from the EMILLE () parallel corpus for the Housing and Legal domains.", "labels": [], "entities": [{"text": "EMILLE () parallel corpus", "start_pos": 50, "end_pos": 75, "type": "DATASET", "confidence": 0.7369532883167267}]}, {"text": "These datasets are used as test sets to show coverage of our NMT model.", "labels": [], "entities": []}, {"text": "We train a baseline NMT model using the small parallel corpus (ILCI) described in Section 3.2.", "labels": [], "entities": []}, {"text": "We call this model NM T Base . In order to compare our results with the state-ofthe-art, we train a phrase based SMT model using the same corpus.", "labels": [], "entities": [{"text": "NM T Base", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.7641561428705851}, {"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9435715675354004}]}, {"text": "The SMT model is trained using Moses () for phrase extraction and lexicalized reordering as described in () . We call this model SM T SA . In this section, we describe a three-phase integrated approach which leverages a large monolingual corpus of the source language and an existing MT tool to improve translation accuracy as well as domain coverage.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9831199645996094}, {"text": "phrase extraction", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8215606212615967}, {"text": "accuracy", "start_pos": 315, "end_pos": 323, "type": "METRIC", "confidence": 0.7765724658966064}]}, {"text": "shows the block diagram of this approach.", "labels": [], "entities": []}, {"text": "The entire process is divided into three stages : Coarse learning, Finetuning and Self-training.", "labels": [], "entities": [{"text": "Coarse learning", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7895108461380005}]}, {"text": "We begin by Coarse Learning, which can bethought of as providing the neural model with some information about grammatical constructs of the target language.", "labels": [], "entities": []}, {"text": "The second phase employs Fine-tuning to enrich the linguistic knowledge of the model with the help of a hand-annotated gold parallel corpus.", "labels": [], "entities": []}, {"text": "This is then followed by self-training, where the fine-tuned model is employed to generate a synthetic corpus again, on which we perform Coarse Learning for the next training iteration.", "labels": [], "entities": []}, {"text": "Thus, this is a cyclical process, which is stopped when further increase inaccuracy is observed to be negligible.", "labels": [], "entities": []}, {"text": "The following sections explain the three phases in detail :", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics -ILCI", "labels": [], "entities": [{"text": "ILCI", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.6737059354782104}]}, {"text": " Table 1. We use the EMILLE monolingual  corpora (", "labels": [], "entities": [{"text": "EMILLE", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.915074348449707}]}, {"text": " Table 2: Monolingual Corpora statistics - EMILLE and *UrMonoCorp", "labels": [], "entities": [{"text": "EMILLE", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.4580988585948944}, {"text": "UrMonoCorp", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.7549080848693848}]}, {"text": " Table 3: Parallel Corpus Statistics - EMILLE. H: Housing, L: Legal", "labels": [], "entities": [{"text": "Parallel Corpus Statistics - EMILLE", "start_pos": 10, "end_pos": 45, "type": "DATASET", "confidence": 0.5944591581821441}]}, {"text": " Table 4: Detailed parameters for training the NMT models.  LR : Learning Rate, DS : Start Decay at", "labels": [], "entities": [{"text": "LR", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9497745037078857}, {"text": "Learning Rate", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.8957299888134003}, {"text": "DS", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.960119366645813}, {"text": "Start", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.8832114338874817}]}, {"text": " Table 5: Performance Comparison during various phases over ILCI test set in terms of BLEU  scores", "labels": [], "entities": [{"text": "ILCI test set", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8979158202807108}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9993025064468384}]}, {"text": " Table 6: Robustness comparison of models over different domains (in terms of BLEU scores)", "labels": [], "entities": [{"text": "Robustness comparison", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9432594776153564}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9991229176521301}]}]}