{"title": [], "abstractContent": [{"text": "In this paper, we describe our submissions to the WMT17 Multimodal Translation Task.", "labels": [], "entities": [{"text": "WMT17 Multimodal Translation Task", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7358759939670563}]}, {"text": "For Task 1 (multimodal translation), our best scoring system is a purely textual neural translation of the source image caption to the target language.", "labels": [], "entities": [{"text": "multimodal translation)", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.830914040406545}, {"text": "textual neural translation of the source image caption", "start_pos": 73, "end_pos": 127, "type": "TASK", "confidence": 0.6216869503259659}]}, {"text": "The main feature of the system is the use of additional data that was acquired by selecting similar sentences from parallel corpora and by data synthesis with back-translation.", "labels": [], "entities": []}, {"text": "For Task 2 (cross-lingual image captioning), our best submitted system generates an English caption which is then translated by the best system used in Task 1.", "labels": [], "entities": [{"text": "cross-lingual image captioning)", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.7465156614780426}]}, {"text": "We also present negative results, which are based on ideas that we believe have potential of making improvements, but did not prove to be useful in our particular setup.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent advances in deep learning allowed inferring distributed vector representations of both textual and visual data.", "labels": [], "entities": []}, {"text": "In models combining text and vision modalities, this representation can be used as a shared datatype.", "labels": [], "entities": []}, {"text": "Unlike the classical natural language processing tasks where everything happens within one language or across languages, multimodality tackles how the language entities relate to the extra-lingual reality.", "labels": [], "entities": []}, {"text": "One of these tasks is multimodal translation whose goal is using cross-lingual information in automatic image captioning.", "labels": [], "entities": [{"text": "multimodal translation", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7242851853370667}, {"text": "automatic image captioning", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.6194773813088735}]}, {"text": "In this system-description paper, we describe our submission to the WMT17 Multimodal Translation Task.", "labels": [], "entities": [{"text": "WMT17 Multimodal Translation Task", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.7085618823766708}]}, {"text": "In particular, we discuss the effect of mining additional training data and usability of advanced attention strategies.", "labels": [], "entities": []}, {"text": "We report our results on both the 2016 and 2017 test sets and discuss efficiency of tested approaches.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the tasks we handle in this paper and the datasets that were provided to the task.", "labels": [], "entities": []}, {"text": "Section 3 summarizes the state-of-the-art methods applied to the task.", "labels": [], "entities": []}, {"text": "In Section 4, we describe our models and the results we have achieved.", "labels": [], "entities": []}, {"text": "Section 5 presents the negative results and Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The challenge of the WMT Multimodal Translation Task is to exploit cross-lingual information in automatic image caption generation.", "labels": [], "entities": [{"text": "WMT Multimodal Translation Task", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.880545437335968}, {"text": "automatic image caption generation", "start_pos": 96, "end_pos": 130, "type": "TASK", "confidence": 0.6914334520697594}]}, {"text": "The stateof-the-art models in both machine translation and automatic image caption generation use similar architectures for generating the target sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.782486617565155}, {"text": "image caption generation", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.8117452164491018}]}, {"text": "The simplicity with which we can combine the learned representations of various inputs in a single deep learning model inevitably leads to a question whether combining the modalities can lead to some interesting results.", "labels": [], "entities": []}, {"text": "In the shared task, this is explored in two subtasks with different roles of visual and textual modalities.", "labels": [], "entities": []}, {"text": "In the multimodal translation task (Task 1), the input of the model is an image and its caption in English.", "labels": [], "entities": [{"text": "multimodal translation task", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.8190180063247681}]}, {"text": "The system then should output a German or French translation of the caption.", "labels": [], "entities": []}, {"text": "The system output is evaluated using the METEOR (Denkowski and Lavie, 2011) and BLEU () scores computed against a single reference sentence.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9790539741516113}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9994614720344543}]}, {"text": "The question this task tries to answer is whether and how is it possible to use visual information to disambiguate the translation.", "labels": [], "entities": []}, {"text": "In the cross-lingual captioning task (Task 2), the input to the model at test-time is the image alone.", "labels": [], "entities": [{"text": "cross-lingual captioning task", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.8022434314092001}]}, {"text": "However, additionally to the image, the model is supplied with the English (source) caption during training.", "labels": [], "entities": []}, {"text": "The evaluation method differs from Task 1 in using five reference captions instead of a single one.", "labels": [], "entities": []}, {"text": "In Task 2, German is the only target language.", "labels": [], "entities": []}, {"text": "The motivation of Task 2 is to explore ways of easily creating an image captioning system in anew language once we have an existing system for another language, assuming that the information transfer is less complex across languages than between visual and textual modalities.", "labels": [], "entities": [{"text": "image captioning system", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.8254823684692383}]}, {"text": "All models are based on the encoder-decoder architecture with attention mechanism () as implemented in Neural Monkey (Helcl and Libovick\u00b4yLibovick\u00b4y, 2017).", "labels": [], "entities": [{"text": "Neural Monkey", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.7557380199432373}]}, {"text": "The decoder uses conditional GRUs (Firat and Cho, 2016) with 500 hidden units and word embeddings with dimension of 300.", "labels": [], "entities": []}, {"text": "The target sentences are decoded using beam search with beam size 10, and with exponentially weighted length penalty () with \u03b1 parameter empirically estimated as 1.5 for German and 1.0 for French.", "labels": [], "entities": [{"text": "exponentially weighted length penalty", "start_pos": 79, "end_pos": 116, "type": "METRIC", "confidence": 0.6526439115405083}]}, {"text": "Because of the low OOV rate (see), we used vocabularies of maximum 30,000 tokens and we did not use sub-word units.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9687495529651642}]}, {"text": "The textual encoder is a bidirectional GRU network with 500 units in each direction and word embeddings with dimension of 300.", "labels": [], "entities": []}, {"text": "We use the last convolutional layer VGG-16 network) of dimensionality 14 \u00d7 14 \u00d7 512 for image processing.", "labels": [], "entities": [{"text": "VGG-16 network", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9206211864948273}, {"text": "image processing", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.8537161946296692}]}, {"text": "The model is optimized using the Adam optimizer () with learning rate 10 \u22124 with early stopping based on validation BLEU score.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 81, "end_pos": 95, "type": "METRIC", "confidence": 0.8646779358386993}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9759436845779419}]}], "tableCaptions": [{"text": " Table 1: Multi30k statistics on training and valida- tion data -total number of tokens, average number  of tokens per sentence, and the sizes of the shortest  and the longest sentence.", "labels": [], "entities": []}, {"text": " Table 3: Results of Task 1 in BLEU / METEOR points. 'C' denotes constrained configuration, 'U'  unconstrained, '2016' is the 2016 test set, 'Flickr' and 'MSCOCO' denote the 2017 test sets. The two  unconstrained textual models differ in using the additional textual data, which was not used for the  training of the multimodal systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9961251616477966}, {"text": "METEOR", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.6677154302597046}, {"text": "Flickr", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9770157933235168}]}]}