{"title": [{"text": "Comparison of Short-Text Sentiment Analysis Methods for Croatian", "labels": [], "entities": [{"text": "Short-Text Sentiment Analysis", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.6391841073830923}]}], "abstractContent": [{"text": "We focus on the task of supervised sentiment classification of short and informal texts in Croatian, using two simple yet effective methods: word embeddings and string kernels.", "labels": [], "entities": [{"text": "sentiment classification of short and informal texts in Croatian", "start_pos": 35, "end_pos": 99, "type": "TASK", "confidence": 0.881709893544515}]}, {"text": "We investigate whether word embeddings offer any advantage over corpus-and preprocessing-free string kernels , and how these compare to bag-of-words baselines.", "labels": [], "entities": []}, {"text": "We conduct a comparison on three different datasets, using different preprocessing methods and kernel functions.", "labels": [], "entities": []}, {"text": "Results show that, on two out of three datasets, word embeddings outper-form string kernels, which in turn outper-form word and n-gram bag-of-words base-lines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis) -a task of predicting whether the text expresses a positive, negative, or neutral opinion in general or with respect to an entity -has attracted considerable attention over the last two decades.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9282938838005066}, {"text": "predicting whether the text expresses a positive, negative, or neutral opinion in general or with respect to an entity", "start_pos": 31, "end_pos": 149, "type": "Description", "confidence": 0.7572260527383714}]}, {"text": "Some of the more popular applications include political popularity (O') and stock price prediction.", "labels": [], "entities": [{"text": "political popularity (O')", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.5268328249454498}, {"text": "stock price prediction", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.6387957334518433}]}, {"text": "Social media texts, including user reviews () and microblogs (), are particularly amenable to sentiment analysis, with applications in social studies and marketing analyses (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.9537538886070251}]}, {"text": "At the same time, social media poses a great challenge for sentiment analysis, as such texts are often short, informal, and noisy (, and make heavy use of figurative language ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.9743798673152924}]}, {"text": "Sentiment analysis is most often framed as a supervised classification task.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9636270403862}]}, {"text": "Many approaches resort to rich, domain-specific features (, including surfaceform, lexicon-based, and syntactic features.", "labels": [], "entities": []}, {"text": "On the other hand, there has been a growing trend in using feature-light methods, including neural word embeddings) and kernel-based methods ().", "labels": [], "entities": []}, {"text": "In particular, two methods that standout in terms of both their simplicity and effectiveness are word embeddings () and string kernels (.", "labels": [], "entities": []}, {"text": "In this paper we focus on sentiment classification of short text in Croatian, a morphologically complex South Slavic language.", "labels": [], "entities": [{"text": "sentiment classification of short text", "start_pos": 26, "end_pos": 64, "type": "TASK", "confidence": 0.8765337467193604}]}, {"text": "We compare two simple yet effective methods -word embeddings and string kernels -which are often used in text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.8401269912719727}]}, {"text": "While both methods are easy to setup, they differ in terms of preprocessing required: word embeddings require a sizable, possibly lemmatized corpus, whereas string kernels require no preprocessing at all.", "labels": [], "entities": []}, {"text": "This motivates the main question of our research: do word embeddings offer any advantage over corpus-and preprocessing-free string kernels, and how do these methods compare to simpler bag-of-words methods?", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this question has not explicitly been addressed before, especially fora morphologically complex language like Croatian.", "labels": [], "entities": []}, {"text": "We present findings from the comparison on three different short-text datasets in Croatian, manually labeled for sentiment polarity, using different levels of morphological preprocessing.", "labels": [], "entities": [{"text": "sentiment polarity", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8329124450683594}]}, {"text": "To spur further research, we make one dataset publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted our comparison on three short-text datasets in Croatian.", "labels": [], "entities": []}, {"text": "The datasets differ in domain, genre, size, and the number of classes.", "labels": [], "entities": []}, {"text": "Game reviews (GR).", "labels": [], "entities": [{"text": "Game reviews (GR)", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.3989356279373169}]}, {"text": "This dataset originally consisted of longer reviews of computer games, in which annotators have labeled 1858 text spans that express positive or negative sentiment.", "labels": [], "entities": []}, {"text": "We used the text spans for our analysis.", "labels": [], "entities": []}, {"text": "The spans were labeled by three annotators, and the final annotation was determined by the majority vote on a per-token basis.", "labels": [], "entities": []}, {"text": "The spans need not contain full sentences nor need to be limited to a single sentence.", "labels": [], "entities": []}, {"text": "Domain-specific tweets (TD).", "labels": [], "entities": [{"text": "Domain-specific tweets (TD)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6259408056735992}]}, {"text": "This dataset contains tweets related to the television singing competition \"The Voice of Croatia\".", "labels": [], "entities": []}, {"text": "The dataset contains 2967 tweets labeled as positive, neutral, or negative by three annotators.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement in terms of Fleiss' kappa is 0.721.", "labels": [], "entities": [{"text": "Fleiss' kappa", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8179000914096832}]}, {"text": "The final label for each tweet was determined by the majority vote.", "labels": [], "entities": []}, {"text": "General-topic tweets (TG).", "labels": [], "entities": [{"text": "General-topic tweets (TG)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6213011085987091}]}, {"text": "This is a collection of 7999 general-topic tweets, labeled as positive, neutral, or negative by a single annotator.", "labels": [], "entities": []}, {"text": "The two Twitter datasets, TD and TG, mostly contain informal and often ungrammatical text, whereas the GR dataset is mostly edited, grammatical text.", "labels": [], "entities": [{"text": "Twitter datasets", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.7587683796882629}, {"text": "GR dataset", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.921825110912323}]}, {"text": "Furthermore, as can be seen from Table 1, Twitter datasets are fairly unbalanced across the three classes, whereas GR is more balanced across the two classes.", "labels": [], "entities": [{"text": "Twitter datasets", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.7546873986721039}, {"text": "GR", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.695479154586792}]}, {"text": "The GR dataset exhibits the greatest lexical variance, as evidenced by the high type-token ratio.", "labels": [], "entities": [{"text": "GR dataset", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7914760410785675}]}, {"text": "On the other hand, as indicated by the average number of words per text segment/tweet, the texts in TG are longer than the text in the other two datasets.", "labels": [], "entities": []}, {"text": "We evaluated all models using nested k-folded evaluation with hyperparameter grid search (C and \u03b3 for RBF, \u03bb and n for SSK, n for SK, \u03b1 for the cosine kernel).", "labels": [], "entities": []}, {"text": "We used 10 folds in the outer and 5 folds in inner (model selection) loop.", "labels": [], "entities": []}, {"text": "Following the established practice in evaluating sentiment classifiers (, we evaluated using the average of the F1-scores for the positive and the negative classes.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8062608242034912}, {"text": "F1-scores", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9943867921829224}]}, {"text": "We used a t-test (p<0.05, with Bonferroni correction for multiple comparisons where applicable) for testing the significance of differences between the F1-scores.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9844919443130493}]}, {"text": "shows the F1-scores on the three datasets for the baseline, word embeddings, and string kernel models, using different feature sets and kernel configurations.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9975280165672302}]}, {"text": "For BoW baselines, the best results are obtained using stemming on all three datasets, i.e., lemmatization does not outperform stemming on neither of the three datasets.", "labels": [], "entities": [{"text": "BoW baselines", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9256201386451721}]}, {"text": "For word embeddings, non-linear kernels, cosine kernel in particular, outperform the linear kernel.", "labels": [], "entities": []}, {"text": "Lemmatization improves the performance only slightly on the GR dataset, and does not improve or even hurts the performance on the other two datasets.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9743157029151917}, {"text": "GR dataset", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.9390216767787933}]}, {"text": "Finally, for string kernels, we obtain the best results with the spectrum kernel on GR and TD datasets, and subsequence kernel on the TG dataset.", "labels": [], "entities": [{"text": "GR and TD datasets", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.7423665523529053}, {"text": "TG dataset", "start_pos": 134, "end_pos": 144, "type": "DATASET", "confidence": 0.9243431389331818}]}, {"text": "Comparing the best results for the three models, we observe that both word embeddings and string kernels outperform the BoW baseline on the GR and TG datasets (statistically significant difference).", "labels": [], "entities": [{"text": "BoW baseline", "start_pos": 120, "end_pos": 132, "type": "DATASET", "confidence": 0.8242716789245605}, {"text": "GR and TG datasets", "start_pos": 140, "end_pos": 158, "type": "DATASET", "confidence": 0.6653566807508469}]}, {"text": "Overall, word embeddings yield the best performance on these two datasets, while string kernels give the best performance on the TD dataset, though the difference is not statistically significant.", "labels": [], "entities": [{"text": "TD dataset", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.8797521591186523}]}, {"text": "Comparing across the datasets, we notice that the performance on TD and TG datasets is worse than on the GR dataset.", "labels": [], "entities": [{"text": "TG datasets", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.6845635622739792}, {"text": "GR dataset", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.9174677431583405}]}, {"text": "This can be traced back to the informality of TD and TG texts, and also the fact that these datasets have three sentiment classes, whereas the GR dataset has only two.", "labels": [], "entities": [{"text": "GR dataset", "start_pos": 143, "end_pos": 153, "type": "DATASET", "confidence": 0.8883391618728638}]}, {"text": "The performance on the TG set is probably further impeded by the fact that it covers a variety of topics, and has been annotated by a single annotator.", "labels": [], "entities": [{"text": "TG set", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9486266076564789}]}, {"text": "We can make three main observations based on the results obtained.", "labels": [], "entities": []}, {"text": "The first is that a word embedding model with a cosine kernel and with either words or lemmas as features significantly outperforms both the baseline and the string kernel model on two out of three datasets.", "labels": [], "entities": []}, {"text": "This suggest that a word embedding model should be the model of choice for short-text sentiment analysis in Croatian.", "labels": [], "entities": [{"text": "short-text sentiment analysis", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.770840992530187}]}, {"text": "The second observation is that lemmatization was mostly not useful in our case: for BoW baseline, stems and n-grams offer better or comparable performance, while for word embeddings lemmatization improved performance on only one out of three datasets.", "labels": [], "entities": [{"text": "BoW baseline", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.935894250869751}]}, {"text": "While this could probably be traced back to the noisiness of the informal text (at least for TD and TG datasets), it suggests that lemmatization does not really payoff for this task, especially considering its complexity relative to stemming.", "labels": [], "entities": [{"text": "TG datasets", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.7205932289361954}]}, {"text": "Finally, we observe that, although string kernels did not significantly outperform the best baseline models, they do significantly outperform the BoW with words as features on two out of three datasets.", "labels": [], "entities": [{"text": "BoW", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.9361782073974609}]}, {"text": "Thus, in cases when both a stemmer and word embeddings are not available, string kernels maybe the model of choice.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BoW baseline feature vector dimensions", "labels": [], "entities": [{"text": "BoW baseline feature vector", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.9253271967172623}]}, {"text": " Table 3: F1-scores for the BoW, word embeddings,  and string kernel models on the game reviews (GR),  domain-specific (TD), and general-topic (TG) twit- ter datasets. The best-performing configuration  for each model is indicated in bold. Statistically  significant differences are marked with  *  .", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988498687744141}, {"text": "BoW", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8960159420967102}]}]}