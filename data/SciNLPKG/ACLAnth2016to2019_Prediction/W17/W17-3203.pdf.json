{"title": [{"text": "Stronger Baselines for Trustable Results in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7732657194137573}]}], "abstractContent": [{"text": "Interest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7169182101885477}]}, {"text": "New research regularly introduces architectural and algorith-mic improvements that lead to significant gains over \"vanilla\" NMT implementations.", "labels": [], "entities": []}, {"text": "However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-the-art production and shared-task systems.", "labels": [], "entities": []}, {"text": "As a result, it is often difficult to determine whether improvements from research will carryover to systems deployed for real-world use.", "labels": [], "entities": []}, {"text": "In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems.", "labels": [], "entities": []}, {"text": "Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9986125230789185}]}, {"text": "We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen.", "labels": [], "entities": []}, {"text": "This indicates that choosing a strong baseline is crucial for reporting reliable experimental results.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the relatively short time since its introduction, neural machine translation has risen to prominence in both academia and industry.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.6467646658420563}]}, {"text": "Neural models have consistently shown top performance in shared evaluation tasks and are becoming the technology of choice for commercial MT service providers (.", "labels": [], "entities": [{"text": "MT service", "start_pos": 138, "end_pos": 148, "type": "TASK", "confidence": 0.92162024974823}]}, {"text": "New work from the research community regularly introduces model extensions and algorithms that show significant gains over baseline NMT.", "labels": [], "entities": []}, {"text": "However, the continuous improvement of real-world translation systems has led to a substantial performance gap between the first published neural translation models and the current state of the art.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7473838925361633}]}, {"text": "When promising new techniques are only evaluated on very basic NMT systems, it can be difficult to determine how much (if any) improvement will carryover to stronger systems; is new work actually solving new problems or simply re-solving problems that have already been addressed elsewhere?", "labels": [], "entities": []}, {"text": "In this work, we recommend three specific techniques for strengthening NMT systems and empirically demonstrate how their use improves reliability of experimental results.", "labels": [], "entities": [{"text": "reliability", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.971743643283844}]}, {"text": "We analyze in depth how these techniques change the behavior of NMT systems by addressing key weaknesses and discuss how these findings can be used to understand the effect of other types of system extensions.", "labels": [], "entities": []}, {"text": "Our recommended techniques include: (1) a training approach using Adam with multiple restarts and learning rate annealing, (2) sub-word translation via byte pair encoding, and (3) decoding with ensembles of independently trained models.", "labels": [], "entities": [{"text": "sub-word translation", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.7793786823749542}]}, {"text": "We begin the paper content by introducing atypical NMT baseline system as our experimental starting point ( \u00a72.1).", "labels": [], "entities": []}, {"text": "We then present and examine the effects of each recommended technique: Adam with multiple restarts and step size annealing ( \u00a73), byte pair encoding ( \u00a74), and independent model ensembling ( \u00a75).", "labels": [], "entities": []}, {"text": "We show that combining these techniques can lead to a substantial improvement of over 5 BLEU ( \u00a76) and that results for several previously published techniques can dramati-cally differ (up to being reversed) when evaluated on stronger systems ( \u00a76.2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9993035793304443}]}, {"text": "We then conclude by summarizing our findings ( \u00a77).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our starting point for experimentation is a standard baseline neural machine translation system implemented using the Lamtram and DyNet 2 toolkits.", "labels": [], "entities": [{"text": "Lamtram", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.9765971302986145}]}, {"text": "This system uses the attentional encoder-decoder architecture described by, building on work by.", "labels": [], "entities": []}, {"text": "The translation model uses a bi-directional encoder with a single LSTM layer of size 1024, multilayer perceptron attention with a layer size of 1024, and word representations of size 512.", "labels": [], "entities": []}, {"text": "Translation models are trained until perplexity convergence on held-out data using the Adam algorithm with a maximum step size of 0.0002 (.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9575750231742859}]}, {"text": "Maximum training sentence length is set to 100 words.", "labels": [], "entities": []}, {"text": "Model vocabulary is limited to the top 50K source words and 50K target words by frequency, with all others mapped to an unk token.", "labels": [], "entities": []}, {"text": "A post-processing step replaces any unk tokens in system output by attempting a dictionary lookup 3 of the corresponding source word (highest attention score) and backing off to copying the source word directly (.", "labels": [], "entities": []}, {"text": "Experiments in each section evaluate this system against incremental extensions such as improved model vocabulary or training algorithm.", "labels": [], "entities": []}, {"text": "Evaluation is conducted by average BLEU score over multiple independent training runs ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9742665886878967}]}, {"text": "In this section, we evaluate and discuss the effects that choice of baseline can have on experimental conclusions regarding neural MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.9144636392593384}]}, {"text": "First, we build systems that include Adam with rate annealing, byte pair encoding, and independent model ensembling and compare them to the vanilla baselines described in \u00a72.1.", "labels": [], "entities": []}, {"text": "As shown in  niques shown to improve NMT performance and compare their effects as baseline systems are iteratively strengthened.", "labels": [], "entities": [{"text": "NMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9515857696533203}]}, {"text": "Focusing on English-French and Czech-English, we evaluate the following techniques with and without the proposed improvements, reporting results in: Dropout: Apply the improved dropout technique for sequence models described by to LSTM layers with a rate of 0.2.", "labels": [], "entities": []}, {"text": "We find this version to significantly outperform standard dropout.", "labels": [], "entities": []}, {"text": "Lexicon bias: Incorporate scores from a pretrained lexicon (fast align model learned on the same data) directly as additional weights when selecting output words (.", "labels": [], "entities": []}, {"text": "Target word lexicon scores are computed as weighted sums over source words based on attention scores.", "labels": [], "entities": []}, {"text": "Pre-translation: Translate source sentences with a traditional phrase-based system trained on the same data.", "labels": [], "entities": []}, {"text": "Input for the neural system is the original source sentence concatenated with the PBMT output ( . Input words are prefixed with either s or t to denote source or target language.", "labels": [], "entities": []}, {"text": "We improve performance with a novel extension where word alignments are used to weave together source and PBMT output so that each original word is immediately followed by its suggested translation from the phrase-based system.", "labels": [], "entities": []}, {"text": "As pre-translation doubles source vocabulary size and input length, we only apply it to sub-word systems to keep complexity reasonable.", "labels": [], "entities": []}, {"text": "Data bootstrapping: Expand training data by extracting phrase pairs (sub-sentence translation examples) and including them as additional training instances).", "labels": [], "entities": []}, {"text": "We apply a novel extension where we train a phrase-based system and use it to re-translate the training data, providing a near-optimal phrase segmentation as a byproduct.", "labels": [], "entities": [{"text": "phrase segmentation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.7403860688209534}]}, {"text": "We use these phrases in place of the heuristically chosen phrases in the original work, improving coverage and leading to more fine-grained translation examples.", "labels": [], "entities": [{"text": "coverage", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9754152297973633}]}, {"text": "The immediately noticeable trend from is that while all techniques improve basic systems, only a single technique, data bootstrapping, improves the fully strengthened system for both data sets (and barely so).", "labels": [], "entities": []}, {"text": "This can be attributed to a mix of redundancy and incompatibility between the improvements we've discussed in previous sections and the techniques evaluated here.", "labels": [], "entities": []}, {"text": "Lexicon bias and pre-translation both incorporate scores from pre-trained models that are shown to improve handling of rare words.", "labels": [], "entities": [{"text": "Lexicon bias", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8247580230236053}]}, {"text": "When NMT models are sub-optimally trained, they can benefit from the suggestions of a better-trained model.", "labels": [], "entities": []}, {"text": "When full-word NMT models struggle to learn translations for infrequent words, they can learn to simply trust the lexical or phrase-based model.", "labels": [], "entities": []}, {"text": "However, when annealing Adam and BPE alleviate these underlying problems, the neural model's accuracy can match or exceed that of the pretrained model, making external scores either completely redundant or (in the worst case) harmful bias that must be overcome to produce correct translations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9992061257362366}]}, {"text": "While pre-translation fares better than lexicon bias, it suffers a reversal in one scenario and a significant degradation in the other when moving from a single model to an ensemble.", "labels": [], "entities": []}, {"text": "Even when bias from an external model improves translation, it does so at the cost of diversity by pushing the neural model's preferences toward those of the pre-trained model.", "labels": [], "entities": []}, {"text": "These results further validate claims of the importance of diversity in model ensembles.", "labels": [], "entities": []}, {"text": "Applying dropout significantly improves all configurations of the Czech-English system and some configurations of the English-French system, leveling off with the strongest.", "labels": [], "entities": []}, {"text": "This trend follows previous work showing that dropout combats overfitting of small data, though the point of inflection is worth noting.", "labels": [], "entities": []}, {"text": "Even though the English-French data is still relatively small (220K sentences), BPE leads to a smaller vocabulary of more general translation units, effectively reducing sparsity, while annealing Adam can avoid getting stuck in poor local optima.", "labels": [], "entities": [{"text": "BPE", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9257394075393677}]}, {"text": "These techniques already lead to better generalization without the need for dropout.", "labels": [], "entities": [{"text": "generalization", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.9663266539573669}]}, {"text": "Finally, we can observe a few key properties of data bootstrapping, the best performing technique on fully strengthened systems.", "labels": [], "entities": []}, {"text": "Unlike lexicon bias and pre-translation, it modifies only the training data, allowing \"purely neural\" models to be learned from random initialization points.", "labels": [], "entities": []}, {"text": "This preserves model diversity, allowing ensembles to benefit as well as single models.", "labels": [], "entities": []}, {"text": "Further, data bootstrapping is complementary to annealing Adam and BPE; better optimization and a more general vocabulary can make better use of the new training instances.", "labels": [], "entities": [{"text": "BPE", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.7624275088310242}]}, {"text": "While evaluation on simple vanilla NMT systems would indicate that all of the techniques in this section lead to significant improvement for both data sets, only evaluation on systems using annealing Adam, byte pair encoding, and independent model ensembling reveals both the reversals of results on state-of-the-art systems and nuanced interactions between techniques that we have reported.", "labels": [], "entities": []}, {"text": "Based on these results, we highly recommend evaluating new techniques on systems that are at least this strong and representative of those deployed for real-world use.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU scores for training NMT models  with full word and byte pair encoded vocabularies.  Full word models limit vocabulary size to 50K.  All models are trained with annealing Adam and  scores are averaged over 3 optimizer runs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994269609451294}]}, {"text": " Table 3: Test set BLEU scores for \"vanilla\" NMT  (full words and standard Adam), and our recom- mended systems (byte pair encoding and anneal- ing Adam, with and without ensembling). Scores  for single models are averaged over 3 independent  optimizer runs while scores for ensembles are the  result of combining 3 runs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9990167617797852}]}]}