{"title": [{"text": "Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words.", "labels": [], "entities": [{"text": "Distributional word representation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.691888431708018}]}, {"text": "While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning.", "labels": [], "entities": []}, {"text": "In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants.", "labels": [], "entities": []}, {"text": "We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors.", "labels": [], "entities": [{"text": "word-word similarity prediction", "start_pos": 156, "end_pos": 187, "type": "TASK", "confidence": 0.6014649967352549}]}, {"text": "Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional approaches to meaning representation have enabled a substantial amount of progress in natural language processing over the past years.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.8744350671768188}]}, {"text": "They center around a classic insight from at least as early as;: You shall know a word by the company it keeps.", "labels": [], "entities": []}, {"text": "Popular distributional analysis methods which exploit this intuition such as word2vec () and GloVe ( have been critical to the success of many recent All project code available at github.com/lucy3/ grounding-embeddings.", "labels": [], "entities": []}, {"text": "large-scale natural language processing applications (e.g..", "labels": [], "entities": []}, {"text": "These methods operationalize distributional meaning via tasks where words are optimized to predict words which cooccur with them in text corpora.", "labels": [], "entities": []}, {"text": "These methods yield compact word representations -vectors in some high-dimensional space -which are optimized to solve these prediction tasks.", "labels": [], "entities": []}, {"text": "These vector representations form the foundation of practically all modern deep learning models applied within natural language processing.", "labels": [], "entities": []}, {"text": "Despite the success of distributional representations in standard natural language processing tasks, a small but growing consensus within the artificial intelligence community suggests that these methods cannot be sufficient to induce adequate representations of words and concepts (.", "labels": [], "entities": []}, {"text": "These sorts of claims, which often draw on experimental evidence from cognitive science (see e.g., are used to backup arguments for multimodal learning (at the weakest) or complete embodiment (at the strongest).", "labels": [], "entities": []}, {"text": "claim the following: . .", "labels": [], "entities": []}, {"text": "the best way for acquiring humanlevel semantics is to have machines learn through (physical) experience: if we want to teach a system the true meaning of \"bumping into a wall,\" we simply have to bump it into walls repeatedly.", "labels": [], "entities": [{"text": "acquiring humanlevel semantics", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.6987883845965067}]}, {"text": "Discussions like the one above have an intuitive pull: certainly \"bump\" is best understood through a sense of touch, just as \"loud\" is best understood through a sense of sound.", "labels": [], "entities": []}, {"text": "It seems inefficientor perhaps just wrong -to learn these sorts of concepts from distributional evidence.", "labels": [], "entities": []}, {"text": "Despite the intuitive pull, there is not much evidence from a computational perspective that grounded or multimodal learning actually earns us anything in terms of general meaning representation.", "labels": [], "entities": [{"text": "general meaning representation", "start_pos": 164, "end_pos": 194, "type": "TASK", "confidence": 0.654284139474233}]}, {"text": "Will our robots and chat-bots be worse off for not having physically bumped into walls before they hold discussions on wall-collisions?", "labels": [], "entities": []}, {"text": "Will our representation of the concept loud somehow be faulty unless we explicitly associate it with certain decibel levels experienced in the real world?", "labels": [], "entities": []}, {"text": "Before we proceed to embed our learning agents in multimodal games and robot-shells, it is important that we have some concrete idea of how grounding actually affects meaning.", "labels": [], "entities": []}, {"text": "This paper presents a thorough analysis of the contents of distributional word representations with respect to this question.", "labels": [], "entities": []}, {"text": "Our results suggest that several common distributional word representations may indeed be deficient in the sort of grounded meaning necessary for languageenabled agents deployed in the real world.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Semantic norm datasets used in this pa- per. The final two columns show the mean con- cepts per feature / features per concept.", "labels": [], "entities": []}, {"text": " Table 3: Examples of features in each category with feature fit scores based on using GloVe-CC to predict  norms from CSLB.", "labels": [], "entities": []}, {"text": " Table 4: Selected domains from the clustering  analysis on GloVe-CC, with median feature fit  scores over concepts.", "labels": [], "entities": [{"text": "GloVe-CC", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8987847566604614}]}]}