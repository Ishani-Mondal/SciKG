{"title": [{"text": "A Multilingual Entity Linker Using PageRank and Semantic Graphs", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes HERD, a multilingual named entity recognizer and linker.", "labels": [], "entities": []}, {"text": "HERD is based on the links in Wikipedia to resolve mappings between the entities and their different names, and Wikidata as a language-agnostic reference of entity identifiers.", "labels": [], "entities": [{"text": "HERD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.4506785571575165}]}, {"text": "HERD extracts the mentions from text using a string matching engine and links them to entities with a combination of rules, PageRank, and feature vectors based on the Wikipedia categories.", "labels": [], "entities": []}, {"text": "We evaluated HERD with the evaluation protocol of ERD'14 (Carmel et al., 2014) and we reached the competitive F1-score of 0.746 on the development set.", "labels": [], "entities": [{"text": "HERD", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.7711767554283142}, {"text": "ERD'14", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.7719660997390747}, {"text": "F1-score", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9996557235717773}]}, {"text": "HERD is designed to be multilingual and has versions in English, French, and Swedish.", "labels": [], "entities": [{"text": "HERD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.628253161907196}]}], "introductionContent": [{"text": "Named entity recognition (NER) refers to the process of finding mentions of persons, locations, and organizations in text, while entity linking (or disambiguation) associates these mentions with unique identifiers.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7810057500998179}, {"text": "entity linking (or disambiguation", "start_pos": 129, "end_pos": 162, "type": "TASK", "confidence": 0.7426808595657348}]}, {"text": "shows an example of entity linking with the mention Michael Jackson, an ambiguous name that may refer to thousands of people and where 21 are famous enough to have a Wikipedia page.", "labels": [], "entities": []}, {"text": "In, the search engine selected the most popular entity (top) and used the cue word footballer (bottom) to link the phrase Michael Jackson footballer to the English defender born in 1973.", "labels": [], "entities": []}, {"text": "Entity recognition and linking has become a crucial component to many language processing applications: Search engines, question answering, or dialogue agents.", "labels": [], "entities": [{"text": "Entity recognition and linking", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7580478191375732}, {"text": "question answering", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.8832391202449799}]}, {"text": "This importance is reflected by a growing number of available systems; see TAC-KBP2015 (, for instance, with 10 participating teams.", "labels": [], "entities": [{"text": "TAC-KBP2015", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.7867202162742615}]}, {"text": "Although many applications include entity linkers, the diversity of the input texts, which can include tweets, search queries, news wires, or encyclopedic articles, makes their evaluation problematic.", "labels": [], "entities": []}, {"text": "While some evaluations consider entity linking in isolation and mark the mentions in the input, end-to-end pipelines, where the input consists of raw text, need to combine entity recognition and linking.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.7330876886844635}]}, {"text": "The ERD'14 challenge) is an example of the latter.", "labels": [], "entities": [{"text": "ERD'14 challenge", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.6717157363891602}]}], "datasetContent": [{"text": "We evaluated the system with two different data sets for English: ERD-51 and AIDA/YAGO and we used the evaluation metrics of ERD.", "labels": [], "entities": [{"text": "AIDA/YAGO", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.6846176783243815}, {"text": "ERD", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.7915341854095459}]}, {"text": "We did not have access to evaluation sets for the other languages.", "labels": [], "entities": []}, {"text": "ERD-51 is the development set of.", "labels": [], "entities": [{"text": "ERD-51", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.868411660194397}]}, {"text": "It consists of 51 documents that have been scraped from a variety of sources with 1,169 human-annotated mentions.", "labels": [], "entities": []}, {"text": "Each annotation has a start, an end, and a Freebase identifier).", "labels": [], "entities": []}, {"text": "In the competition, a set of entities, slightly over 2 million, was given, and thus the problem of defining what a named entity actually is was avoided.", "labels": [], "entities": []}, {"text": "We filtered our knowledge base to only contain mentions of entities from the given set.", "labels": [], "entities": []}, {"text": "AIDA/YAGO is derived from the CoNLL-2003 shared task.", "labels": [], "entities": [{"text": "AIDA/YAGO", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.3647279441356659}, {"text": "CoNLL-2003 shared task", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.8490654627482096}]}, {"text": "It is a collection of 1393 news articles with 34,587 human-annotated names.", "labels": [], "entities": []}, {"text": "27,632 of those names have been disambiguated with a link to the corresponding Wikipedia site using a dump from 2010-08-17.", "labels": [], "entities": []}, {"text": "show the results evolving over different versions of the system.", "labels": [], "entities": []}, {"text": "The execution time is normalized to the time spent to process 5,000 characters.", "labels": [], "entities": []}, {"text": "It should not be considered an absolute value, but can be used to compare the different algorithms. and, respectively second and third of ERD'14, reported results of 0.735 and 0.72 on the test set.", "labels": [], "entities": [{"text": "ERD'14", "start_pos": 138, "end_pos": 144, "type": "DATASET", "confidence": 0.5202236175537109}]}, {"text": "Our results are not exactly comparable as we used the development set.", "labels": [], "entities": []}, {"text": "Nonetheless we believe our system competitive.", "labels": [], "entities": []}, {"text": "The highest scoring participant) with a score of 0.76 was one of the organizers and was not part of the competition.", "labels": [], "entities": []}, {"text": "The reason for the difference in recognition score between the ERD-51 and the AIDA/YAGO dataset lies in the text genres.", "labels": [], "entities": [{"text": "AIDA/YAGO dataset", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.8370355516672134}]}, {"text": "AIDA/YAGO is collected from well written news wires, mainly written by professionals with proper punctuation and capitalization, while ERD-51 is a collection of more poorly written texts collected from a wider variety of sources, spanning from blogs to eBay sales sites.", "labels": [], "entities": [{"text": "AIDA/YAGO", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.6014125148455302}]}, {"text": "For the score of the linked systems, the results are the opposite.", "labels": [], "entities": []}, {"text": "ERD-51 has been annotated and linked by humans from a predefined set of 2 million entities, while the linking in AIDA/YAGO has been done from an older dump of Wikipedia.", "labels": [], "entities": [{"text": "ERD-51", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8278839588165283}, {"text": "AIDA/YAGO", "start_pos": 113, "end_pos": 122, "type": "DATASET", "confidence": 0.6332518259684244}]}, {"text": "The dump contains around 3 million different sites, unlike the dump we used that has around 5 million entities.", "labels": [], "entities": []}, {"text": "Both the mismatch in the entities that are possible to use and the larger size of the knowledge base lead to a larger gap between recognition and linking.", "labels": [], "entities": []}, {"text": "With a latency of around 200 ms per 5,000 characters, the system should be able to tag the entire of the English Wikipedia under a day with a heavy duty computer.", "labels": [], "entities": [{"text": "latency", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.9739221930503845}]}, {"text": "We estimate that the average length of a Wikipedia article is around 2,000 characters, that there are 5 million articles, 24 available cores and 100% run time.", "labels": [], "entities": []}, {"text": "We implemented the entity linker as an interactive demonstration.", "labels": [], "entities": []}, {"text": "The user can paste a text and visualize the results in the form of a text annotated with entity labels.", "labels": [], "entities": []}, {"text": "Once the user hovers over the label, the ranked candidates are displayed with a link to the Wikidata page for that entity.", "labels": [], "entities": []}, {"text": "shows an output of the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An example of the mention-entity counts for the entity Q18094, most commonly called Hon- olulu", "labels": [], "entities": [{"text": "Hon- olulu", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8618790109952291}]}, {"text": " Table 2: Top 5 categories for Sweden, Q34", "labels": [], "entities": [{"text": "Q34", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.7417502999305725}]}, {"text": " Table 3: Results on the ERD51 dataset", "labels": [], "entities": [{"text": "ERD51 dataset", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9319439232349396}]}, {"text": " Table 4: Results on the AIDA/YAGO dataset", "labels": [], "entities": [{"text": "AIDA/YAGO dataset", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.6889971345663071}]}]}