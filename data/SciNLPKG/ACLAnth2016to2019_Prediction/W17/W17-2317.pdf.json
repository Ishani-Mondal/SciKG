{"title": [{"text": "Unsupervised Context-Sensitive Spelling Correction of Clinical Free-Text with Word and Character N-Gram Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an unsupervised context-sensitive spelling correction method for clinical free-text that uses word and character n-gram embeddings.", "labels": [], "entities": [{"text": "context-sensitive spelling correction", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.6184843083222707}]}, {"text": "Our method generates misspelling replacement candidates and ranks them according to their semantic fit, by calculating a weighted cosine similarity between the vectorized representation of a candidate and the misspelling context.", "labels": [], "entities": []}, {"text": "We greatly outperform two baseline off-the-shelf spelling correction tools on a manually annotated MIMIC-III test set, and counter the frequency bias of an optimized noisy channel model, showing that neural embeddings can be successfully exploited to include context-awareness in a spelling correction model.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8787169456481934}, {"text": "MIMIC-III test set", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.9123669862747192}, {"text": "spelling correction", "start_pos": 282, "end_pos": 301, "type": "TASK", "confidence": 0.8964383006095886}]}, {"text": "Our source code, including a script to extract the annotated test data, can be found at https://github.com/ pieterfivez/bionlp2017.", "labels": [], "entities": []}], "introductionContent": [{"text": "The genre of clinical free-text is notoriously noisy.", "labels": [], "entities": []}, {"text": "Corpora contain observed spelling error rates which range from 0.1% () and 0.4% () to 4% and 7% (, and even 10% (.", "labels": [], "entities": [{"text": "spelling error rates", "start_pos": 25, "end_pos": 45, "type": "METRIC", "confidence": 0.923983653386434}]}, {"text": "This high spelling error rate, combined with the variable lexical characteristics of clinical text, can render traditional spell checkers ineffective.", "labels": [], "entities": [{"text": "spelling error rate", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7847815950711569}]}, {"text": "Recently, have achieved nearly 80% correction accuracy on a test set of clinical notes with their noisy channel model.", "labels": [], "entities": [{"text": "correction", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9680386781692505}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8504191637039185}]}, {"text": "However, their model does not leverage any contextual information, while the context of a misspelling can provide important clues for the spelling correction process, for instance to counter the frequency bias of a context-insensitive corpus frequency-based system.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.9116621613502502}]}, {"text": "Flor (2012) also pointed out that ignoring contextual clues harms performance where a specific misspelling maps to different corrections in different contexts, e.g. iron deficiency due to enemia \u2192 anemia vs. fluid injected with enemia \u2192 enema.", "labels": [], "entities": []}, {"text": "A noisy channel model like the one by Lai et al. will choose the same item for both corrections.", "labels": [], "entities": []}, {"text": "Our proposed method exploits contextual clues by using neural embeddings to rank misspelling replacement candidates according to their semantic fit in the misspelling context.", "labels": [], "entities": []}, {"text": "Neural embeddings have recently proven useful fora variety of related tasks, such as unsupervised normalization and reducing the candidate search space for spelling correction.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.9136415123939514}]}, {"text": "We hypothesize that, by using neural embeddings, our method can counter the frequency bias of a noisy channel model.", "labels": [], "entities": []}, {"text": "We test our system on manually annotated misspellings from the MIMIC-III () clinical notes.", "labels": [], "entities": [{"text": "MIMIC-III () clinical notes", "start_pos": 63, "end_pos": 90, "type": "DATASET", "confidence": 0.8910003751516342}]}, {"text": "In this paper, we focus on already detected non-word misspellings, i.e. where the misspellings are not real words, following Lai et al.", "labels": [], "entities": []}], "datasetContent": [{"text": "HunSpell a total of 243.", "labels": [], "entities": [{"text": "HunSpell", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8907349705696106}]}, {"text": "While the difference is very pronounced in the much larger tuning corpora, the artificial nature of those corpora does not lead to strong evidence.", "labels": [], "entities": []}, {"text": "Moreover, considering the similarity of the general performance of both models on the test set, more test data is needed to make a strong empirical claim about this specific aspect of our model.", "labels": [], "entities": []}, {"text": "While we have optimized the prior probabilities of Lai et al.'s ranking model, the posterior probabilities are still estimated with Lai et al.'s rudimentary spell score, which is a weighted combination of Damerau-Levenshtein and Double Metaphone edit distance.", "labels": [], "entities": [{"text": "Double Metaphone edit distance", "start_pos": 229, "end_pos": 259, "type": "METRIC", "confidence": 0.7504431307315826}]}, {"text": "While this error model leads to a noisy channel model which is robust in performance, as shown by our test results, an empirical error model derived from a large confusion matrix can for example help correct the instance described in, by capturing that the wordfinal transformation t \u2192 g is more probable than the word-initial transformation g \u2192 p.", "labels": [], "entities": []}, {"text": "As of now, however, such a resource is not available for the clinical domain.", "labels": [], "entities": []}, {"text": "The errors that our model makes concern, predictably, misspellings for which the contextual clues are too unspecific or misguiding.", "labels": [], "entities": []}, {"text": "These cases remain challenging for the concept of our method.", "labels": [], "entities": []}, {"text": "While our tuning experiments have explicitly tried to maximize the scope and efficiency of our model, there is still room for improvement, especially for OOV corrections, even as we handle them more effectively than context-insensitive frequency-based methods.", "labels": [], "entities": [{"text": "OOV corrections", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.7797633707523346}]}], "tableCaptions": [{"text": " Table 1: Correction accuracies for our 3 tuning  setups.", "labels": [], "entities": []}, {"text": " Table 2: The correction accuracies for our test set, evaluated for two different scenarios. Off-the-shelf :  gives the accuracies of all off-the-shelf tools. With completed lexicon: gives the accuracies of our  implemented models for the scenario where correct replacements missing from the lexicon are included  in the lexicon before the experiment.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9745137095451355}]}]}