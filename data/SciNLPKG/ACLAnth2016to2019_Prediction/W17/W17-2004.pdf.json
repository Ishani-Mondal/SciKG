{"title": [{"text": "Human Evaluation of Multi-modal Neural Machine Translation: a Case Study on E-commerce Listing Titles", "labels": [], "entities": [{"text": "Multi-modal Neural Machine Translation", "start_pos": 20, "end_pos": 58, "type": "TASK", "confidence": 0.6098898574709892}]}], "abstractContent": [{"text": "In this paper, we study how humans perceive the use of images as an additional knowledge source to machine-translate user-generated product listings in an e-commerce company.", "labels": [], "entities": []}, {"text": "We conduct a human evaluation where we assess how a multi-modal neural machine translation (NMT) model compares to two text-only approaches: a conventional state-of-the-art attention-based NMT and a phrase-based statistical machine translation (PBSMT) model.", "labels": [], "entities": [{"text": "multi-modal neural machine translation (NMT)", "start_pos": 52, "end_pos": 96, "type": "TASK", "confidence": 0.8213511960847037}, {"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 199, "end_pos": 251, "type": "TASK", "confidence": 0.7113264799118042}]}, {"text": "We evaluate translations obtained with different systems and also discuss the data set of user-generated product listings, which in our case comprises both product listings and associated images.", "labels": [], "entities": []}, {"text": "We found that humans preferred translations obtained with a PBSMT system to both text-only and multi-modal NMT over 56% of the time.", "labels": [], "entities": []}, {"text": "Nonetheless, human evaluators ranked translations from a multi-modal NMT model as better than those of a text-only NMT over 88% of the time, which suggests that images do help NMT in this use-case.", "labels": [], "entities": []}], "introductionContent": [{"text": "In e-commerce, leveraging Machine Translation (MT) to make products accessible regardless of the customer's native language or country of origin is a very persuasive use-case.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8654085874557496}]}, {"text": "In this work, we study how humans perceive the machine translation of usergenerated auction listings' titles as listed on the eBay main site . Among the challenges for MT are the specialized language and grammar for listing titles, as well as a high percentage of user-generated content for non-business sellers, who are often not native speakers themselves.", "labels": [], "entities": [{"text": "machine translation of usergenerated auction listings' titles", "start_pos": 47, "end_pos": 108, "type": "TASK", "confidence": 0.7704268097877502}, {"text": "eBay main site", "start_pos": 126, "end_pos": 140, "type": "DATASET", "confidence": 0.9163331588109335}, {"text": "MT", "start_pos": 168, "end_pos": 170, "type": "TASK", "confidence": 0.9843310713768005}]}, {"text": "This is reflected on the data by means of extremely high trigram perplexities of product listings, which is in 4 digit numbers even for language models (LMs) trained on in-domain data, as we discuss in \u00a73.", "labels": [], "entities": []}, {"text": "This is not only a challenge for LMs but also for automatic evaluation metrics such as the n-gram precisionbased BLEU metric ().", "labels": [], "entities": [{"text": "precisionbased BLEU metric", "start_pos": 98, "end_pos": 124, "type": "METRIC", "confidence": 0.8179992437362671}]}, {"text": "The majority of listings are accompanied by a product image, often (but not always) a user-generated shot.", "labels": [], "entities": []}, {"text": "Moreover, images are known to bring useful complementary information to MT).", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9703735709190369}]}, {"text": "Therefore, in order to explore whether product images can benefit the machine translation of auction titles, we evaluate a multi-modal neural MT (NMT) system to eBay's production system, specifically a phrase-based statistical MT (PBSMT) one.", "labels": [], "entities": [{"text": "machine translation of auction titles", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.8341539144515991}]}, {"text": "We additionally train a text-only attention-based NMT baseline, so as to be able to measure eventual gains from the additional multi-modal data independently of the MT architecture.", "labels": [], "entities": []}, {"text": "According to a quantitative evaluation using a combination of four automatic MT evaluation metrics, a PBSMT system outperforms both text-only and multimodal NMT models in the translation of product listings, contrary to recent findings (.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.9191294610500336}, {"text": "translation of product listings", "start_pos": 175, "end_pos": 206, "type": "TASK", "confidence": 0.9002796709537506}]}, {"text": "We hypothesise that these automatic metrics were not created for the purpose of measuring the impact an image brings to an MT model, so we conduct a human evaluation of translations generated by three different systems: a PBSMT, a text-only attentionbased NMT and a multi-modal NMT system.", "labels": [], "entities": [{"text": "MT model", "start_pos": 123, "end_pos": 131, "type": "TASK", "confidence": 0.9057638645172119}]}, {"text": "With that human evaluation we wish to see whether those findings corroborate the automatic scores or instead support results included in recent papers in the literature.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In \u00a72 we briefly describe the text-only and multi-modal MT models we evaluate in this work and in \u00a73 the data sets we used, together with a discussion of interesting findings.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9610682129859924}]}, {"text": "In \u00a74 we discuss how we structure our evaluation and in \u00a75 we analyse and discuss our results.", "labels": [], "entities": []}, {"text": "In \u00a76 we discuss important related work and finally in \u00a77 we draw conclusions and suggest avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the eBay24k, the additional back-translated eBay80k and the) data sets to train all our models.", "labels": [], "entities": [{"text": "eBay24k", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9643070101737976}, {"text": "eBay80k", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.939197301864624}]}, {"text": "In our experiments, we wish to contrast the human assessments of the adequacy of translations obtained with two text-only baselines, PBSMT and NMT t , and one multi-modal model NMT m , with scores computed with four automatic MT metrics: BLEU4 (), ME-TEOR (), TER), and chrF3.", "labels": [], "entities": [{"text": "MT", "start_pos": 226, "end_pos": 228, "type": "TASK", "confidence": 0.9405918121337891}, {"text": "BLEU4", "start_pos": 238, "end_pos": 243, "type": "METRIC", "confidence": 0.995789110660553}, {"text": "ME-TEOR", "start_pos": 248, "end_pos": 255, "type": "METRIC", "confidence": 0.9103970527648926}, {"text": "TER", "start_pos": 260, "end_pos": 263, "type": "METRIC", "confidence": 0.9896069169044495}]}, {"text": "We report statistical significance with approximate randomisation for the first three metrics using the MultEval tool).", "labels": [], "entities": [{"text": "MultEval", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.698333740234375}]}, {"text": "For our qualitative human evaluation, we ask bilingual native German speakers: 1.", "labels": [], "entities": []}, {"text": "to assess the multi-modal adequacy of translations (number of participants N = 18), described in \u00a74.1; 2.", "labels": [], "entities": []}, {"text": "to rank translations generated by different models from best to worst (number of participants N = 18), described in \u00a74.2.", "labels": [], "entities": []}, {"text": "On average, our evaluators' consisted of 72% women and 28% men.", "labels": [], "entities": []}, {"text": "They were recruited from employees at eBay Inc., Aachen, Germany, as well as the student and staff body of Dublin City University, Dublin, Ireland.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of product listings accompanied by  product images from the eBay test set.", "labels": [], "entities": [{"text": "eBay test set", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9802331328392029}]}, {"text": " Table 2: Difficulty to understand product listings with  and without images and adequacy of listings and im- ages. N is the number of raters (Calixto et al., 2017b).", "labels": [], "entities": []}, {"text": " Table 3: Adequacy of translations and four automatic metrics on eBay's test set: BLEU, METEOR, TER and chrF3.  For the first three metrics, results are significantly better than those of NMT t (  \u2020 ) or NMT m (  \u2021 ) with p < 0.01.", "labels": [], "entities": [{"text": "eBay's test set", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9206220358610153}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.998953104019165}, {"text": "METEOR", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9930684566497803}, {"text": "TER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.990465521812439}]}]}