{"title": [{"text": "CLUZH at VarDial GDI 2017: Testing a Variety of Machine Learning Tools for the Classification of Swiss German Dialects", "labels": [], "entities": [{"text": "VarDial GDI 2017", "start_pos": 9, "end_pos": 25, "type": "DATASET", "confidence": 0.8664653499921163}, {"text": "Classification of Swiss German Dialects", "start_pos": 79, "end_pos": 118, "type": "TASK", "confidence": 0.7459534883499146}]}], "abstractContent": [{"text": "Our submissions for the GDI 2017 Shared Task are the results from three different types of classifiers: Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, Conditional Random Fields (CRF), and Support Vector Machine (SVM).", "labels": [], "entities": [{"text": "GDI 2017 Shared Task", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.66946180164814}]}, {"text": "Our CRF-based run achieves a weighted F1 score of 65% (third rank) being beaten by the best system by 0.9%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9869714975357056}]}, {"text": "Measured by classification accuracy, our ensemble run (Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, CRF, SVM) reaches 67% (second rank) being 1% lower than the best system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8857311010360718}, {"text": "Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, CRF, SVM)", "start_pos": 55, "end_pos": 84, "type": "METRIC", "confidence": 0.46266375888477673}]}, {"text": "We also describe our experiments with Recurrent Neural Network (RNN) architectures.", "labels": [], "entities": []}, {"text": "Since they performed worse than our non-neural approaches we did not include them in the submission.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of our participation in the newly introduced German Dialect Identification (GDI) Shared Task of the VarDial Workshop 2017 () was to quickly test how far we could get on this classification problem using standard machine learning techniques (as only closed runs were allowed for this task).", "labels": [], "entities": [{"text": "German Dialect Identification (GDI) Shared Task", "start_pos": 54, "end_pos": 101, "type": "TASK", "confidence": 0.7471197620034218}, {"text": "VarDial Workshop 2017", "start_pos": 109, "end_pos": 130, "type": "DATASET", "confidence": 0.6069497863451639}]}, {"text": "The task is to predict the correct Swiss German dialect for manually transcribed utterances (.", "labels": [], "entities": [{"text": "Swiss German dialect", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.8142706553141276}]}, {"text": "The Dieth transcription-developed in the 1930s in Switzerland-is not a scholarly phonetic transcription system.", "labels": [], "entities": [{"text": "Dieth transcription-developed", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.8454535603523254}]}, {"text": "It is designed to be applicable by laymen to all Swiss German dialects and uses the Standard German alphabet and a few optional diacritics.", "labels": [], "entities": []}, {"text": "In this task, the number of possible Swiss German dialects is limited to four main varieties: the dialects spoken in the cantons of Basel (BS), Bern (BE), Lucerne (LU), and Zurich (ZH).", "labels": [], "entities": []}], "datasetContent": [{"text": "We have invested a considerable amount of effort in RNN models.", "labels": [], "entities": []}, {"text": "We implement particularly simple Long Short-Term Memory (LSTM) networks): with and without an initial character embedding layer, with a recurrent layer, and a softmax output layer.", "labels": [], "entities": []}, {"text": "Like in the other runs, we experiment with single character and character group replacements.", "labels": [], "entities": []}, {"text": "We fix the size of the character embedding layer to two thirds the input size, which therefore varies from model to model as a result of character replacements (twenty five or twenty nine units).", "labels": [], "entities": []}, {"text": "The size of the LSTM layer is fixed to ninety hidden units.", "labels": [], "entities": []}, {"text": "The softmax layer takes as input the values of the LSTM hidden units at the final character.", "labels": [], "entities": []}, {"text": "All the models are rather small, with the leanest models having 41,760 parameters and the largest having 48,600 parameters.", "labels": [], "entities": []}, {"text": "Adding a character embedding layer results in a 9% reduction in model parameters, on average.", "labels": [], "entities": []}, {"text": "The reduction in the number of character types shrinks the model by another 4%, and the replacement of common di-and trigraphs shortens input sequences and further speeds up training.", "labels": [], "entities": []}, {"text": "We discarded the idea of using bidirectional LSTMs (: They are slower to train (the number of model parameters roughly doubles), which has been the main bottleneck for us since we have intended to experiment with multiple model set-ups.", "labels": [], "entities": []}, {"text": "One important theme in our neural network experiments has been data augmentation.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7498707175254822}]}, {"text": "Having examined the predictions of the baseline classifier, we observed that the longer the utterance the more likely it is to be classified correctly.", "labels": [], "entities": []}, {"text": "We hypothesized that a simple trick of slicing long utterances into multiple shorter chunks and substituting those chunks for the original utterances in the training data would improve performance.", "labels": [], "entities": []}, {"text": "Like in the other runs, we drop short utterances com-: Official results for the GDI task.", "labels": [], "entities": [{"text": "GDI task", "start_pos": 80, "end_pos": 88, "type": "TASK", "confidence": 0.6909285485744476}]}, {"text": "The baseline predicts the majority class.", "labels": [], "entities": []}, {"text": "For all classes, F1 (micro) is the same as accuracy.", "labels": [], "entities": [{"text": "F1 (micro)", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.7673265263438225}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9992967844009399}]}, {"text": "pletely (in this case, one-word and two-word utterances).", "labels": [], "entities": []}, {"text": "As a result of this data augmentation, the training data for the internal system evaluation have grown by almost a quarter (from 11,726 to 15,340 utterances).", "labels": [], "entities": []}, {"text": "All neural-network implementation is done using high-level structures of the keras neural networks library.", "labels": [], "entities": [{"text": "keras neural networks library", "start_pos": 77, "end_pos": 106, "type": "DATASET", "confidence": 0.7876541614532471}]}, {"text": "For training the models, we use the Root Mean Square Propagation (RMSProp) algorithm, a variant of Stochastic Gradient Descent, with default hyper-parameters suggested by the library.", "labels": [], "entities": [{"text": "Root Mean Square Propagation (RMSProp) algorithm", "start_pos": 36, "end_pos": 84, "type": "METRIC", "confidence": 0.7026032991707325}]}, {"text": "We use Dropout () for regularization.", "labels": [], "entities": []}, {"text": "We train for at least 100 epochs and at most 300 epochs.", "labels": [], "entities": []}, {"text": "shows the official results of our submitted runs.", "labels": [], "entities": []}, {"text": "Run 3 has the best accuracy among our runs, but is slightly worse on the macro-averaged F1 score and the weighted F1 score (see for further information on the evaluation metrics).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9992682337760925}, {"text": "F1 score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9717840850353241}, {"text": "F1 score", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9714238047599792}]}, {"text": "The performance in absolute numbers is much lower than expected from cross-validation.", "labels": [], "entities": []}, {"text": "shows average validation scores of the systems featured in our submissions.", "labels": [], "entities": []}, {"text": "We retrain the systems with the same hyper-parameter settings as in the submissions.", "labels": [], "entities": []}, {"text": "The ensemble performs best followed closely by the baseline system of Run 1.", "labels": [], "entities": []}, {"text": "To compare the systems with the bestperforming LSTM from the post-submission experiments, we set aside a stratified sample of one tenth the size of the training data as an internal evaluation set.", "labels": [], "entities": []}, {"text": "Again, we retrain the models on the remaining data with the same hyper-parameter settings.", "labels": [], "entities": []}, {"text": "Since these hyper-parameter values have been found to produce the best performance on the entire training data, internal evaluation set results are potentially biased upward for all the systems but the LSTM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of classes in the training and test sets of the GDI task. Row \"Training 4+\" shows  the effect of removing sentences with less than 4 tokens on the training set composition.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of numbers of tokens per utterance in the training and test sets of the GDI task.", "labels": [], "entities": [{"text": "GDI task", "start_pos": 95, "end_pos": 103, "type": "TASK", "confidence": 0.8334383368492126}]}, {"text": " Table 4: Confusion matrices and result breakdown for our official GDI runs. Rows are true labels,  columns are predicted labels.", "labels": [], "entities": []}, {"text": " Table 5: Official results for the GDI task. The  baseline predicts the majority class. For all  classes, F1 (micro) is the same as accuracy.", "labels": [], "entities": [{"text": "GDI task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.846523255109787}, {"text": "F1 (micro)", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.8421283662319183}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9996298551559448}]}, {"text": " Table 6: System comparison: Results for ten-fold stratified cross-validation and performance on an  internal evaluation set. Cross validation results: We report mean scores across the folds and indicate  standard deviations in parentheses. The SVM is a model from the ensemble of Run 3.", "labels": [], "entities": []}, {"text": " Table 7: Comparison of RNN sequence classifiers.", "labels": [], "entities": [{"text": "RNN sequence classifiers", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.5898724297682444}]}]}