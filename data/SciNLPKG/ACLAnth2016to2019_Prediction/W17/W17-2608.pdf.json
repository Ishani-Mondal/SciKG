{"title": [{"text": "Modeling Large-Scale Structured Relationships with Shared Memory for Knowledge Base Completion", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent studies on knowledge base completion , the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embed-dings from multi-step relations.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6457599500815073}]}, {"text": "However, due to the size of knowledge bases, learning multi-step relations directly on top of observed triplets could be costly.", "labels": [], "entities": []}, {"text": "Hence, a manually designed procedure is often used when training the models.", "labels": [], "entities": []}, {"text": "In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform multi-step inference implicitly through a controller and shared memory.", "labels": [], "entities": []}, {"text": "Without a human-designed inference procedure, IRNs use training data to learn to perform multi-step inference in an embedding neural space through the shared memory and controller.", "labels": [], "entities": []}, {"text": "While the inference procedure does not explicitly operate on top of observed triplets, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "labels": [], "entities": [{"text": "FB15k benchmark", "start_pos": 157, "end_pos": 172, "type": "DATASET", "confidence": 0.9484255313873291}]}], "introductionContent": [{"text": "Knowledge bases such as WordNet,, or Yago () contain many realworld facts expressed as triples, e.g., (Bill Gates, FOUNDEROF, Microsoft).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9716526865959167}]}, {"text": "These knowledge bases are useful for many downstream applications such as question answering) and information extraction ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.9251691102981567}, {"text": "information extraction", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8686407208442688}]}, {"text": "However, despite the formidable size of knowledge bases, many important facts are still missing.", "labels": [], "entities": []}, {"text": "For example, showed that 21% of the 100K most frequent * These authors contributed equally.", "labels": [], "entities": []}, {"text": "PERSON entities have no recorded nationality in a recent version of Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.955740749835968}]}, {"text": "We seek to infer unknown entities based on the observed entities and relations.", "labels": [], "entities": []}, {"text": "Thus, the knowledge base completion (KBC) task has emerged an important open research problem).", "labels": [], "entities": [{"text": "knowledge base completion (KBC) task", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7758027400289264}]}, {"text": "Neural-network based methods have been very popular for solving the KBC task.", "labels": [], "entities": [{"text": "KBC task", "start_pos": 68, "end_pos": 76, "type": "TASK", "confidence": 0.5608416199684143}]}, {"text": "Following, one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations attest time.", "labels": [], "entities": [{"text": "KBC", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.8665016889572144}]}, {"text": "However, several recent papers demonstrate limitations of prior approaches relying upon vector-space models alone (.", "labels": [], "entities": []}, {"text": "By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately.", "labels": [], "entities": []}, {"text": "For example, assume that we want to fill in the missing relation for the triple (Obama, NATIONALITY, ?), a multi-step search procedure might be needed to discover the evidence in the observed triples such as (Obama, BORNIN, Hawaii) and (Hawaii, PARTOF, U.S.A).", "labels": [], "entities": [{"text": "NATIONALITY", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.763454020023346}, {"text": "BORNIN", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9356407523155212}]}, {"text": "To address this issue,;;;; propose different approaches of injecting structured information based on the human-designed inference procedure (e.g., random walk) that directly operates on the observed triplets.", "labels": [], "entities": []}, {"text": "Unfortunately, due to the size of knowledge bases, these newly proposed approaches suffer from some limitations, as most paths are not informative for inferring missing relations, and it is prohibitive to consider all possible paths during the training time.", "labels": [], "entities": []}, {"text": "In this paper, we propose Implicit ReasoNets (IRNs) that take a different approach from prior work on KBC by addressing the challenges of performing multi-step inference through the design of controller and shared memory.", "labels": [], "entities": []}, {"text": "We design a shared memory component to store KB information implicitly.", "labels": [], "entities": []}, {"text": "That is, the model needs to determine what information it should store.", "labels": [], "entities": []}, {"text": "Moreover, instead of explicitly manipulating the observed triples based on the human-designed inference procedure, the proposed model learns the multi-step inference procedure implicitly, i.e., without human intervention.", "labels": [], "entities": []}, {"text": "Specifically, our model makes the prediction several times while forming different intermediate representations along the way.", "labels": [], "entities": []}, {"text": "The controller determines how many steps the model should proceed given an input.", "labels": [], "entities": []}, {"text": "At each step, anew representation is formed by taking the current representation and a context vector generated by accessing the shared memory.", "labels": [], "entities": []}, {"text": "The detailed process is introduced in Section 3.3 and an overview of the model is shown in.", "labels": [], "entities": []}, {"text": "The main contributions of our paper are as follows: \u2022 We propose Implicit ReasoNets (IRNs), which use a shared memory guided by a controller to model multi-step structured relationships implicitly.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate IRNs and demonstrate that our proposed model achieves the state-of-the-art results on the popular FB15k benchmark, surpassing prior approaches by more than 5.7%.", "labels": [], "entities": [{"text": "FB15k benchmark", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.9314556419849396}]}, {"text": "\u2022 Our analysis shows that the multi-step inference is crucial to the performance of our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the performance of our model on the benchmark FB15k and WN18 datasets for KBC ().", "labels": [], "entities": [{"text": "FB15k", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.8144950270652771}, {"text": "WN18 datasets", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.8896509110927582}, {"text": "KBC", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.8107350468635559}]}, {"text": "These datasets contain multi-relations between head and tail entities.", "labels": [], "entities": []}, {"text": "Given ahead entity and a relation, the model produces a ranked list of the entities according to the score of the entity being the tail entity of this triple.", "labels": [], "entities": []}, {"text": "To evaluate the ranking, we report mean rank (MR), the mean of rank of the correct entity across the test examples, and hits@10, the proportion of correct entities ranked in the top-10 predictions.", "labels": [], "entities": [{"text": "mean rank (MR)", "start_pos": 35, "end_pos": 49, "type": "METRIC", "confidence": 0.9643560886383057}]}, {"text": "Lower MR or higher hits@10 indicates a better prediction performance.", "labels": [], "entities": [{"text": "MR", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9993307590484619}]}, {"text": "We follow the evaluation protocol in to report filtered results, where negative examples N are removed from the dataset.", "labels": [], "entities": []}, {"text": "In this case, we avoid some negative examples being valid and ranked above the target triplet.", "labels": [], "entities": []}, {"text": "We use the same hyper-parameters of our model for both FB15k and WN18 datasets.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9645056128501892}, {"text": "WN18 datasets", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.8732883334159851}]}, {"text": "Entity embeddings (which are not shared between input and output modules) and relation embedding are both 100-dimensions.", "labels": [], "entities": []}, {"text": "We use the encoder module and decoder module to encode input entities and relations, and output entities, respectively.", "labels": [], "entities": []}, {"text": "There are 64 memory vectors with 200 dimensions each, initialized by random vectors with unit L 2 -norm.", "labels": [], "entities": []}, {"text": "We use single-layer GRU with 200 cells as the search controller.", "labels": [], "entities": []}, {"text": "We set the maximum inference step T max of the IRN to 5.", "labels": [], "entities": [{"text": "inference step T max", "start_pos": 19, "end_pos": 39, "type": "METRIC", "confidence": 0.7790668159723282}, {"text": "IRN", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.4474242627620697}]}, {"text": "We randomly initialize all model parameters, and use SGD as the training algorithm with mini-batch size of 64.", "labels": [], "entities": []}, {"text": "We set the learning rate to a constant number, 0.01.", "labels": [], "entities": []}, {"text": "To prevent the model from learning a trivial solution by increasing entity embeddings norms, we follow to enforce the L 2 -norm of the entity embeddings as 1.", "labels": [], "entities": []}, {"text": "We use hits@10 as the validation metric for the IRN.", "labels": [], "entities": [{"text": "IRN", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.703927755355835}]}, {"text": "Following the work, we add reverse relations into the training triplet set to increase the training data.", "labels": [], "entities": []}, {"text": "Following, we divide the results of previous work into two groups.", "labels": [], "entities": []}, {"text": "The first group contains the models that directly optimize a scoring function for the triples in a knowledge base without using extra information.", "labels": [], "entities": []}, {"text": "The second group of models make uses of additional information from multi-step relations.", "labels": [], "entities": []}, {"text": "For example, RTransE (Garc\u00eda-Dur\u00e1n et al., 2015) and PTransE.", "labels": [], "entities": [{"text": "RTransE", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.5309937596321106}, {"text": "PTransE", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.7091488242149353}]}, {"text": "We find the performance of IRNs increases significantly if the number of inference step increases.", "labels": [], "entities": []}, {"text": "Note that an IRN with T max = 1 is the case that an IRN without the shared memory.", "labels": [], "entities": [{"text": "T max", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9712443649768829}]}, {"text": "Interestingly, given T max = 5, IRNs are not sensitive to memory sizes.", "labels": [], "entities": [{"text": "T max", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9836238622665405}]}, {"text": "In particular, larger memory always improves the MR score, but the best hit@10 is obtained by |M | = 64 memory vectors.", "labels": [], "entities": [{"text": "MR score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9886505305767059}]}, {"text": "A possible reason is that the best memory size is determined by the complexity of the tasks.", "labels": [], "entities": []}, {"text": "We evaluate hits@10 results on FB15k with respect to the relation categories.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.6554297804832458}]}, {"text": "Following the evaluation in, we categorize the relations according to the cardinalities of their associated head and tail entities in four types: 1-1, * Nguyen et al.", "labels": [], "entities": []}, {"text": "(2016) reported two results on WN18, where the first one is obtained by choosing to optimize hits@10 on the validation set, and second one is obtained by choosing to optimize MR on the validation set.", "labels": [], "entities": [{"text": "WN18", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8466792702674866}, {"text": "MR", "start_pos": 175, "end_pos": 177, "type": "METRIC", "confidence": 0.9869229197502136}]}, {"text": "We list both of them in 1-Many, Many-1, and Many-Many.", "labels": [], "entities": []}, {"text": "A given relation is 1-1 if ahead entity can appear with at most one tail entity, 1-Many if ahead entity can appear with many tail entities, Many-1 if multiple heads can appear with the same tail entity, and Many-Many if multiple head entities can appear with multiple tail entities.", "labels": [], "entities": []}, {"text": "The detailed results are shown in.", "labels": [], "entities": []}, {"text": "The IRN significantly improves the hits@10 results in the Many-1 category on predicting the head entity (18.8%), the 1-Many category on predicting the tail entity (16.5%), and the Many-Many category (over 8% in average).", "labels": [], "entities": [{"text": "IRN", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5001382827758789}]}, {"text": "In order to show the inference procedure determined by IRNs, we map the representation st back to human-interpretable entity and relation names in the KB.", "labels": [], "entities": []}, {"text": "In, we show a randomly sampled example with its top-3 closest triplets (h, R, ?) in terms of L 2 -distance, and top-3 answer predictions along with the termination probability at each step.", "labels": [], "entities": []}, {"text": "Throughout our observation, the inference procedure is quite different from the traditional inference chain that people designed in the symbolic space (.", "labels": [], "entities": []}, {"text": "The potential reason is that IRNs operate in the neural space.", "labels": [], "entities": []}, {"text": "Instead of connecting triplets that share exactly the same entity as in the symbolic space, IRNs update the representations and connects other triplets in the semantic space instead.", "labels": [], "entities": []}, {"text": "As we can observe in the examples of, the model reformulates the representation st at each step and gradually increases the ranking score of the correct tail entity with higher termination probability during the inference process.", "labels": [], "entities": []}, {"text": "In the last step of, the closest tuple (Phoenix Suns, /BASKETBALL_ROSTER_POSITION/POSITION) is actually within the training set with a tail entity Forward-center, which is the same as the tar-: Interpret the state st in each step via finding the closest (entity, relation) tuple, and the corresponding the top-3 predictions and termination probability.", "labels": [], "entities": [{"text": "BASKETBALL_ROSTER", "start_pos": 55, "end_pos": 72, "type": "METRIC", "confidence": 0.8367272019386292}]}, {"text": "\"Rank\" stands for the rank of the target entity and \"Term.", "labels": [], "entities": [{"text": "Term", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9398287534713745}]}, {"text": "Prob.\" stands for termination probability.", "labels": [], "entities": [{"text": "termination probability", "start_pos": 18, "end_pos": 41, "type": "METRIC", "confidence": 0.692902147769928}]}, {"text": "To understand what the model has learned in the shared memory in the KBC tasks, in, we visualize the shared memory in an IRN trained from FB15k.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.9787060618400574}]}, {"text": "We compute the average attention scores of each relation type on each memory cell.", "labels": [], "entities": []}, {"text": "In the table, we show the top 8 relations, ranked by the average attention scores, of some memory cells.", "labels": [], "entities": []}, {"text": "These memory cells are activated by certain semantic patterns within the knowledge graph.", "labels": [], "entities": []}, {"text": "It suggests that the shared memory can efficiently capture the relationships implicitly.", "labels": [], "entities": []}, {"text": "We can still see a few noisy relations in each clustered memory cell, e.g., \"bridge-player-teammates/teammate\" relation in the \"film\" memory cell, and \"olympic-medalhonor/medalist\" in the \"disease\" memory cell.", "labels": [], "entities": []}, {"text": "We provide some more IRN prediction examples at each step from FB15k as shown in Appendix A. In addition to the KBC tasks, we construct a synthetic task, shortest path synthesis, to evaluate the inference capability over a shared memory as shown in the Appendix B.", "labels": [], "entities": [{"text": "IRN prediction", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.8233108520507812}, {"text": "FB15k", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.956494927406311}, {"text": "shortest path synthesis", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.6086624165376028}]}], "tableCaptions": [{"text": " Table 1: The knowledge base completion (link prediction) results on WN18 and FB15k.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6644532829523087}, {"text": "WN18", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9672784209251404}, {"text": "FB15k", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.9179551005363464}]}, {"text": " Table 2: The performance of IRNs with different  memory sizes and inference steps on FB15k, where  |M | and T max represent the number of memory  vectors and the maximum inference step, respec- tively.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.9675012230873108}]}, {"text": " Table 3: Hits@10 (%) in the relation category on FB15k. (M stands for Many)", "labels": [], "entities": [{"text": "FB15k", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9750860929489136}]}, {"text": " Table 4: Interpret the state s t in each step via finding the closest (entity, relation) tuple, and the corre- sponding the top-3 predictions and termination probability. \"Rank\" stands for the rank of the target entity  and \"Term. Prob.\" stands for termination probability.", "labels": [], "entities": [{"text": "Term. Prob.\"", "start_pos": 226, "end_pos": 238, "type": "METRIC", "confidence": 0.9187955856323242}]}]}