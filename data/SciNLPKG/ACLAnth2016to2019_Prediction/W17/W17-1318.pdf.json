{"title": [{"text": "Not All Segments are Created Equal: Syntactically Motivated Sentiment Analysis in Lexical Space", "labels": [], "entities": [{"text": "Syntactically Motivated Sentiment Analysis", "start_pos": 36, "end_pos": 78, "type": "TASK", "confidence": 0.6140080690383911}]}], "abstractContent": [{"text": "Although there is by now a considerable amount of research on subjectivity and sentiment analysis on morphologically-rich languages, it is still unclear how lexical information can best be modeled in these languages.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7996799349784851}]}, {"text": "To bridge this gap, we build effective models exploiting exclusively gold and machine-segmented lexical input and successfully employ syntactically motivated feature selection to improve classification.", "labels": [], "entities": []}, {"text": "Our best models achieve significantly above the base-lines, with 67.93% and 69.37% accuracies for subjectivity and sentiment classification respectively.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.9407463669776917}]}], "introductionContent": [{"text": "The task of subjectivity detection refers to identifying aspects of language that are objective (i.e., I have a meeting at 2:00pm.) vs. those that express opinions, feelings, evaluations, and speculations and hence are subjective.", "labels": [], "entities": [{"text": "subjectivity detection", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7214057445526123}]}, {"text": "Subjective language is further classified based on its sentiment into positive (e.g., The new machines are revolutionary!), negative (e.g., The Syria war is terrifying!), neutral (e.g., The new models maybe released next week.), or, sometimes, mixed (e.g., I really like this phone, but it is way too expensive!).", "labels": [], "entities": []}, {"text": "The field of subjectivity and sentiment analysis (SSA) is a very vibrant one and there has been a flurry of research on especially the English language ().", "labels": [], "entities": [{"text": "subjectivity and sentiment analysis (SSA)", "start_pos": 13, "end_pos": 54, "type": "TASK", "confidence": 0.7928780232157026}]}, {"text": "By now, there is also a fair amount of work on morphologically rich languages (MRL) () like Arabic (.", "labels": [], "entities": []}, {"text": "SSA work on MRLs, however, is still in an early stage as MRLs raise a range of questions on their own.", "labels": [], "entities": [{"text": "SSA", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9483004808425903}, {"text": "MRLs", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.8568404912948608}]}, {"text": "In the current work, we focus on answering the question: \"How it is that Arabic can be modeled within lexical space?\"", "labels": [], "entities": []}, {"text": "More specifically, we investigate the utility of teasing apart lexical input based on grammatical criteria and empirically weigh the contribution of features therein toward SSA.", "labels": [], "entities": [{"text": "SSA", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9503483772277832}]}, {"text": "The current work is a followup on submitted work) where we measure both gold and machine-predicted tree-bank style segmentation () on the two tasks of subjectivity and sentiment.", "labels": [], "entities": []}, {"text": "Breaking down surface forms into their component segments is known as segmentation.", "labels": [], "entities": []}, {"text": "Segmentation is possible when morphological boundaries within a word are identified.", "labels": [], "entities": []}, {"text": "In the Penn Arabic Treebank (ATB) (), a segment can be a stem, an inflectional affix, or a clitic.", "labels": [], "entities": [{"text": "Penn Arabic Treebank (ATB)", "start_pos": 7, "end_pos": 33, "type": "DATASET", "confidence": 0.9697613716125488}]}, {"text": "For example, the surface word wbHsnAthm (Eng. 'and by their virtues') is segmented as w+b+Hsn+At+hm with the prefixal clitics (w and b, Eng.", "labels": [], "entities": []}, {"text": "'and' and 'by'), the stem Hsn, the inflection morpheme At, and the suffixal pronominal morpheme hm.", "labels": [], "entities": []}, {"text": "In (), we have shown how reducing a word to its component segments is a desirable measure for SSA since it reduces the number of observed forms and hence alleviates sparsity: The system does not see as many forms attest time that have not been seen at training time.", "labels": [], "entities": [{"text": "SSA", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9891231656074524}]}, {"text": "Providing all lexical segmented input to a classifier, however, mayor may not bean ideal procedure.", "labels": [], "entities": []}, {"text": "In English, usually words like 'a,' 'the,' and 'from' are treated as stop words and hence removed before classification.", "labels": [], "entities": []}, {"text": "These tokens are viewed as functional words that do not usually contribute to classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.9598015546798706}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.8609805107116699}]}, {"text": "Are there ways to breakdown the lexical space based on relevant, if not comparable, grammatical grounds?", "labels": [], "entities": []}, {"text": "That is the question we seek to answer in the current work.", "labels": [], "entities": []}, {"text": "Overall, we make the following contributions: (1) We present anew human-labeled ATB dataset for SSA; (2) We introduce anew syntactically motivated feature selection method for SSA on Arabic that can arguably also help classification on other languages of rich morphology; and (3) We present detailed linguistically-motivated (error) analyses of the behavior of the lexical models, against the background of Arabic morphological complexity.", "labels": [], "entities": [{"text": "SSA", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9570676684379578}]}, {"text": "The rest of this paper is organized as follows: In Section 2, we describe our datasets and methods.", "labels": [], "entities": []}, {"text": "In Section 3, we present our results.", "labels": [], "entities": []}, {"text": "In Section 4, we provide a literature review, and in Section 5 we conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: We gold-label a subset from each of the first three parts of the ATB (i.e., the first 70 documents from ATB1V4.1, the first 50 documents from ATB2V3.1, and the first 58 documents from ATB3V3.2) at the sentence level with tags from the set {OBJ, subjective-positive (S-POS), subjective-negative (S-NEG), subjective-mixed (S-MIXED)}.", "labels": [], "entities": [{"text": "ATB", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.9394916296005249}, {"text": "ATB1V4.1", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.9641528725624084}, {"text": "ATB2V3.1", "start_pos": 148, "end_pos": 156, "type": "DATASET", "confidence": 0.9475166201591492}]}, {"text": "The data belong to the newswire genre and were manually labeled by the Linguistic Data Consortium (LDC) for part-of-speech (POS), morphology, gloss, and syntactic treebank annotation.", "labels": [], "entities": []}, {"text": "A single annotator, with a Ph.D. in linguistics and a native Arabic fluency, labeled the data after being provided written guidelines and several sessions of training and discussions with the authors.", "labels": [], "entities": []}, {"text": "We followed the guidelines in the literature.", "labels": [], "entities": []}, {"text": "To ensure quality, 5% of the data (n=250 sentences) was double labeled by a second annotator.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement reached 83% without adjudication, and hence the first annotator's decisions were judged sufficient.", "labels": [], "entities": []}, {"text": "shows class distribution in our data.", "labels": [], "entities": []}, {"text": "Procedure: We divide each of the three treebank parts into 80% training, 10% development, and 10% test.", "labels": [], "entities": [{"text": "development", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9561694264411926}, {"text": "test", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9545941948890686}]}, {"text": "The training parts from each Treebank are then added up to build TRAIN, the development parts are added up to build DEV, and the test parts are combined to build TEST.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.8933339715003967}, {"text": "DEV", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.840630829334259}, {"text": "TEST", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.5919837951660156}]}, {"text": "For our experiments, results are reported both on DEV and TEST.", "labels": [], "entities": [{"text": "DEV", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.9595703482627869}, {"text": "TEST", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.7129647135734558}]}, {"text": "Importantly, only the DEV set is used for  OBJ S-P S-N S-M ALL ATB1V4: Data statistics.", "labels": [], "entities": [{"text": "OBJ S-P S-N S-M ALL ATB1V4", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.6903767387072245}]}, {"text": "S-P= subjective positive and S-N= subjective negative.", "labels": [], "entities": []}, {"text": "tuning classifier performance and error analyses.", "labels": [], "entities": []}, {"text": "TEST is used as a fully blind set.", "labels": [], "entities": [{"text": "TEST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5410604476928711}]}, {"text": "We follow a twostage classification process where the first stage is to tease apart the OBJ and SUBJ classes, and the second stage is to distinguish the S-POS and the S-NEG classes.", "labels": [], "entities": [{"text": "OBJ", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.7892722487449646}]}, {"text": "For this work, we do not handle the MIXED class, since it is minimal in our data.", "labels": [], "entities": [{"text": "MIXED class", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.48110975325107574}]}, {"text": "Settings: We use two settings based on text preprocessing: Gold and machine-predicted.", "labels": [], "entities": []}, {"text": "For the gold setting, human-annotated segmentation and morphosyntactic disambiguation as labeled by LDC are exploited.", "labels": [], "entities": []}, {"text": "For the machine-predicted setting, we use the ASMA tool (, which renders state of the art segmentation and morphosyntactic tagging for MSA.", "labels": [], "entities": []}, {"text": "For all the subjectivity and sentiment experiments, we use SVMs with a linear kernel.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics. S-P= subjective positive  and S-N= subjective negative.", "labels": [], "entities": []}, {"text": " Table 3: Subjectivity classification with syntactically motivated feature selection. Th prefixes gold-and  asma-refer to Treebank-acquired and ASMA-acquired segments (i.e., -segs), content segments (i.e.,  -cont), and select content segments (i.e., -cont-M * ), respectively.", "labels": [], "entities": [{"text": "Subjectivity classification", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.850699245929718}, {"text": "Th prefixes gold-and  asma-refer", "start_pos": 86, "end_pos": 118, "type": "METRIC", "confidence": 0.908313125371933}]}, {"text": " Table 2: POS tags for content segments", "labels": [], "entities": []}, {"text": " Table 4: Type statistics and OOV percentages for  gold and ASMA-predicted content segments", "labels": [], "entities": [{"text": "OOV percentages", "start_pos": 30, "end_pos": 45, "type": "METRIC", "confidence": 0.9845684766769409}]}, {"text": " Table 5: Sentiment classification with syntactically motivated feature selection.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9721408486366272}]}]}