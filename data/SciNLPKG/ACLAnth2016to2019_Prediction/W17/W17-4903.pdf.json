{"title": [{"text": "Discovering Stylistic Variations in Distributional Vector Space Models via Lexical Paraphrases", "labels": [], "entities": []}], "abstractContent": [{"text": "Detecting and analyzing stylistic variation in language is relevant to diverse Natural Language Processing applications.", "labels": [], "entities": [{"text": "Detecting", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9605363607406616}]}, {"text": "In this work, we investigate whether salient dimensions of style variations are embedded in standard distributional vector spaces of word meaning.", "labels": [], "entities": []}, {"text": "We hypothesize that distances between embeddings of lexical paraphrases can help isolate style from meaning variations and help identify latent style dimensions.", "labels": [], "entities": []}, {"text": "We conduct a qualitative analysis of latent style dimensions, and show the effectiveness of identified style subspaces on a lexical formality prediction task.", "labels": [], "entities": [{"text": "lexical formality prediction task", "start_pos": 124, "end_pos": 157, "type": "TASK", "confidence": 0.7290475592017174}]}], "introductionContent": [{"text": "Automatically analyzing and generating natural language requires capturing not only what is said, but also how it is said.", "labels": [], "entities": []}, {"text": "Consider the sentences \"he shot himself\" and \"he committed suicide\".", "labels": [], "entities": []}, {"text": "The first one is less formal than the second one, and carries information beyond its literal meaning, such as the situation in which it might be used.", "labels": [], "entities": []}, {"text": "Another example is \"stamp show\" vs. \"philatelic exhibition\", English learners with limited vocabulary can use the former term since it is simpler.", "labels": [], "entities": []}, {"text": "As Natural Language Processing systems are deployed in a variety of settings, detecting and analyzing stylistic variations is becoming increasingly important, and is relevant to applications ranging from dialogue systems to predicting power differences in social interactions (Danescu-Niculescu-.", "labels": [], "entities": [{"text": "predicting power differences in social interactions", "start_pos": 224, "end_pos": 275, "type": "TASK", "confidence": 0.8880000313123068}]}, {"text": "In this work we aim to determine to what extent such stylistic variations are embedded in the topology of distributional vector space models.", "labels": [], "entities": []}, {"text": "We focus on dense word embeddings, which provide a compact summary of word usage on the basis of the distributional hypothesis, and have been showed to capture semantic similarity and other lexical semantic relations ().", "labels": [], "entities": []}, {"text": "We hypothesize that differences between embeddings of words that share the same meaning are indicative of style differences.", "labels": [], "entities": []}, {"text": "In order to test this hypothesis, we introduce a method based on Principal Component Analysis to identify salient dimensions of variations betwen word embeddings of lexical paraphrases.", "labels": [], "entities": []}, {"text": "Applying our method to word embeddings learned from two large corpora representing distinct genres, we conduct a qualitative analysis of the principal components discovered.", "labels": [], "entities": []}, {"text": "It suggests that the principal components indeed discover variations that are relevant to style.", "labels": [], "entities": []}, {"text": "Second, we evaluate the style dimensions more directly, using them to distinguish more formal from less formal words.", "labels": [], "entities": []}, {"text": "Formality is considered a key dimension of style variation, and it encompasses a range of finer-grained dimensions, including politeness, serious-trivial, etc).", "labels": [], "entities": []}, {"text": "The formality prediction task lets us evaluate empirically the impact of different factors in identifying style-relevant dimensions, including dimensionality of the subspace and the nature of the prediction method.", "labels": [], "entities": [{"text": "formality prediction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8096472918987274}]}, {"text": "We also conduct an error analysis revealing the limitation of predicting formality based on vector space models.", "labels": [], "entities": [{"text": "predicting formality", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.9100940227508545}]}], "datasetContent": [{"text": "We evaluate the usefulness of the latent dimensions discovered in Section 4 on a lexical formality prediction task.", "labels": [], "entities": [{"text": "lexical formality prediction task", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.7359063625335693}]}, {"text": "If the dimensions discovered are relevant to style, they should help predict formality with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9935019612312317}]}, {"text": "Task Following, we used a list of 399 synonym pairs from a writing manual -Choose the Right Word (CTRW) -to evaluate the formality model.", "labels": [], "entities": []}, {"text": "Given a pair of words, such as \"hurry\" vs. \"expedite\", the task is to predict which is the more formal of the two.", "labels": [], "entities": [{"text": "hurry", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.974486768245697}]}, {"text": "Ranking method The predictions were made by linear SVM classifiers (similar to the method proposed by).", "labels": [], "entities": []}, {"text": "They were trained on 105 formal seed words and 138 informal seed words used by.", "labels": [], "entities": []}, {"text": "Each word was represented by a feature vector in word2vec spaces or their subspaces.", "labels": [], "entities": []}, {"text": "When ranking two words, we actually compared their distances to the separating hyperplane, i.e. w \u00b7 e \u2212 \u03c1, where w, e and \u03c1 are weight, embedding and bias.", "labels": [], "entities": []}, {"text": "Embedding spaces We first trained word2vec (W2V) models of blogs corpus with different space sizes.", "labels": [], "entities": []}, {"text": "We then fixed the space size of word2vec models to 300 since it provides large enough original vector space and is a routinely used setting.", "labels": [], "entities": []}, {"text": "All subspaces would be extracted from these 300-dimensional original spaces.", "labels": [], "entities": []}, {"text": "Style subspaces Next, we identified style subspaces (i.e. top PCs) using the PCA method introduced in Section 3.", "labels": [], "entities": []}, {"text": "We examined every possible subspace size in the range of and denoted this method as PCA-PPDB.", "labels": [], "entities": [{"text": "PCA-PPDB", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.884632408618927}]}, {"text": "For comparison, we also trained PCA subspaces using the seed words (PCA-seeds).", "labels": [], "entities": []}, {"text": "Since seed words are not paraphrases, the PCA model was simply applied on word vectors.", "labels": [], "entities": []}, {"text": "This method is based on the assumption that representative formal/informal words principally vary along the direction of formality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top (mis-)predicted CTRW word pairs, where s i is the SVM (formality) score for word w i . w 2  is supposed to be more formal than w 1 .  \u2020 This word is more frequent than the other in a pair according  to the blogs corpus. ( \u2021/  \u2021  \u2020/  \u2021  \u2021 means at least 10/100/1000 times more.)", "labels": [], "entities": []}]}