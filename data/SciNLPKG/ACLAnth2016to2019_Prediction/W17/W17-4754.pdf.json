{"title": [], "abstractContent": [{"text": "We present in this paper the participation of the University of Hamburg in the Biomedical Translation Task of the Second Conference on Machine Translation (WMT 2017).", "labels": [], "entities": [{"text": "Biomedical Translation Task of the Second Conference on Machine Translation (WMT 2017)", "start_pos": 79, "end_pos": 165, "type": "TASK", "confidence": 0.8262681450162616}]}, {"text": "Our contribution lies in adopting anew direction for performing data selection for Machine Translation via Paragraph Vector and a Feed Forward Neural Network Classifier.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8283720016479492}]}, {"text": "Continuous distributed vector representations of the sentences are used as features for the binary classifier.", "labels": [], "entities": []}, {"text": "Most approaches in data selection rely on scoring and ranking general domain sentences with respect to their similarity to the in-domain and setting a range of thresholds for selecting a percentage of them for training various MT systems.", "labels": [], "entities": [{"text": "data selection", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.6971300840377808}, {"text": "MT", "start_pos": 227, "end_pos": 229, "type": "TASK", "confidence": 0.987575352191925}]}, {"text": "The novelty of our method consists in developing an automatic threshold detection paradigm for data selection which provides an efficient and simple way for selecting the most similar sentences to the in-domain.", "labels": [], "entities": [{"text": "data selection", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.6667831987142563}]}, {"text": "Encouraging results are obtained using this approach for seven language pairs and four data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data selection for Machine Translation (MT) represents a standard domain adaptation technique with the aim of tackling the problem of selecting from various general domain data the sentences that are most similar to sentences from the in-domain.", "labels": [], "entities": [{"text": "Data selection for Machine Translation (MT)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7687818109989166}, {"text": "domain adaptation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7718687355518341}]}, {"text": "Irrespective of having available vast amounts or small amounts of in-domain data, one of the advantages of data selection consists in providing more in-domain data selected from large amounts of general domain data.", "labels": [], "entities": []}, {"text": "Two difficult tasks arise when performing data selection: what method to use for scoring the sentences from the general domain according to their similarity to the in-domain and how many of the scored sentences to keep for later use in training MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 245, "end_pos": 247, "type": "TASK", "confidence": 0.9367652535438538}]}, {"text": "Standard state-of-the-art methods resolve the first difficulty by means of information retrieval, perplexity or edit distance methods.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7271229028701782}]}, {"text": "However, the second difficulty remains a challenge.", "labels": [], "entities": []}, {"text": "There are no standard start-threshold and incrementthreshold defined in the community., for example, uses the top N = {35k, 70k, 150k} sentence pairs from the scored general domain data, while increasingly select N \u2208 {100, 200, 500, 1000, 2000, 3000, 5000, 10000} instances for each test sentence for training and select subsets of 10%, 20%, 30% and 40% of the data.", "labels": [], "entities": []}, {"text": "We present a time and resource efficient method of performing data selection using Paragraph Vector () for representing the sentences and a Feed Forward Neural Network Classifier for determining which general domain sentences should be considered similar to the indomain.", "labels": [], "entities": []}, {"text": "The paragraph vectors and the binary classifiers are trained using standard parameters and have a great advantage of dropping the need to experiment with different sentence selection thresholds.", "labels": [], "entities": []}, {"text": "Therefore, we call our method automatic threshold detection for data selection (ATD).", "labels": [], "entities": [{"text": "automatic threshold detection for data selection (ATD)", "start_pos": 30, "end_pos": 84, "type": "TASK", "confidence": 0.6778306298785739}]}, {"text": "The method has been applied in the Biomedical translation task of the Second Conference on Machine Translation (WMT) 2017.", "labels": [], "entities": [{"text": "Biomedical translation task of the Second Conference on Machine Translation (WMT)", "start_pos": 35, "end_pos": 116, "type": "TASK", "confidence": 0.8610336046952468}]}, {"text": "The in-domain corpora were made available by the competition and the general domain corpora we have chosen to select data from are the Wikipedia corpora) and the Commoncrawl corpora . Experiments were performed on the language pairs EnglishFrench, English-Spanish, English-Portuguese and English-German (both directions for all language pairs except for English-German as the competition did not require German-English translations).", "labels": [], "entities": [{"text": "Commoncrawl corpora", "start_pos": 162, "end_pos": 181, "type": "DATASET", "confidence": 0.9864946901798248}]}, {"text": "Good results have been obtained for all language pairs.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: related work is presented in Section 2, then the data, tools and data selection method are described in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 contains the experimental results and the last section presents conclusions and suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the corpora and tools used, as well as the automatic threshold detection method we propose.", "labels": [], "entities": [{"text": "threshold detection", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7223835289478302}]}, {"text": "We report in this section the BLEU () scores obtained by our submissions, as well as the classifiers accuracy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9991590976715088}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9944355487823486}]}, {"text": "For each language pair and for each test set provided by the Biomedical task, we submitted three runs as follows: \u2022 the selected sentences with the classifier trained on the source language data (run 1) \u2022 the selected sentences with the classifier trained on the target language data (run 2) \u2022 the union (without duplicates) of the selected sentences proposed by the two classifiers (run 3) Intrinsic evaluation of the proposed data selection technique was performed by computing the classifier accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 495, "end_pos": 503, "type": "METRIC", "confidence": 0.965531051158905}]}, {"text": "Following the recommendations from, we employ the stratified cross-validation method with ten folds.", "labels": [], "entities": []}, {"text": "The accuracy values were computed using scikit-learn: Classifier accuracy (%): mean and standard deviation This year four datasets were used in the evaluation: Scielo, EDP, Cochrane and NHS belonging to scientific publications or health information texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9986660480499268}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.6494176983833313}, {"text": "EDP", "start_pos": 168, "end_pos": 171, "type": "DATASET", "confidence": 0.8570095300674438}, {"text": "NHS", "start_pos": 186, "end_pos": 189, "type": "DATASET", "confidence": 0.7921378016471863}]}, {"text": "The format of the datasets differed as Scielo and the EDP datasets follow the BioC format and Cochrane and NHS follow the format of the UFAL Corpus (sgm).", "labels": [], "entities": [{"text": "EDP datasets", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.977575957775116}, {"text": "BioC format", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8352678716182709}, {"text": "NHS", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.5188731551170349}, {"text": "UFAL Corpus (sgm)", "start_pos": 136, "end_pos": 153, "type": "DATASET", "confidence": 0.9461094498634338}]}, {"text": "The results of our submissions are presented with respect to different datasets.", "labels": [], "entities": []}, {"text": "depicts all the BLEU scores of our submissions.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9702523350715637}]}, {"text": "For the Scielo dataset, our team was the only one that submitted runs.", "labels": [], "entities": [{"text": "Scielo dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8472039103507996}]}, {"text": "The organisers provided baselines for all language pairs and our best run improves with almost 9 BLEU points over the baseline for EN-PT and EN-ES, and almost 7 BLEU point over the baseline for PT-EN and ES-EN.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9987527132034302}, {"text": "BLEU", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9984772801399231}]}, {"text": "There were small differences between the results of the three runs which suggests that either method could be used for gaining positive results.", "labels": [], "entities": []}, {"text": "For the EDP dataset (FR-EN and EN-FR) there were eight submissions and our best run for EN-FR had again of around 10 BLEU points over the baseline, as for FR-EN again of around 6 BLEU points.", "labels": [], "entities": [{"text": "EDP dataset", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.9577221274375916}, {"text": "FR-EN", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.7505525350570679}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9969807267189026}, {"text": "FR-EN", "start_pos": 155, "end_pos": 160, "type": "DATASET", "confidence": 0.7731245756149292}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9945935606956482}]}, {"text": "Considering our runs, there is 1 BLEU point difference between run 2 and run 3 for FR-EN and 0.5 difference between run 3 and run 2 for EN-FR.", "labels": [], "entities": [{"text": "BLEU point difference", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.9720266064008077}, {"text": "FR-EN", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.48569831252098083}]}, {"text": "This indicates that the union method provides the best results.", "labels": [], "entities": []}, {"text": "On the Cochrane and NHS datasets our team was the only one that submitted for EN-ES obtaining high BLEU scores (48.99, 48.45 and 48.70 for Cochrane and 40.97, 41.20 and 41.22 for NHS).", "labels": [], "entities": [{"text": "Cochrane and NHS datasets", "start_pos": 7, "end_pos": 32, "type": "DATASET", "confidence": 0.8386874198913574}, {"text": "EN-ES", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.6833070516586304}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9996219873428345}, {"text": "NHS", "start_pos": 179, "end_pos": 182, "type": "DATASET", "confidence": 0.8523377776145935}]}, {"text": "The differences between the runs are again very small.", "labels": [], "entities": []}, {"text": "For EN-FR there were two teams participating.", "labels": [], "entities": [{"text": "EN-FR", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8065184950828552}]}, {"text": "In our runs the union method gave better results for both datasets.", "labels": [], "entities": []}, {"text": "For EN-DE there were six teams participating and the differences between our runs are again small.", "labels": [], "entities": [{"text": "EN-DE", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9037303924560547}]}, {"text": "In the general ranking among all participating teams, our team ranked first for EN-FR for the Cochrane and NHS datasets, second on FR-EN and third on EN-FR for the EDP datasets, last place on EN-DE for the Cochrane and NHS datasets, and was the only team submitting for Scielo (PT-EN, EN-PT, ES-EN, EN-ES) as well as for Cochrane and NHS (EN-ES).", "labels": [], "entities": [{"text": "NHS datasets", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.8696855008602142}, {"text": "FR-EN", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.9201762080192566}, {"text": "EDP datasets", "start_pos": 164, "end_pos": 176, "type": "DATASET", "confidence": 0.9549009799957275}, {"text": "NHS datasets", "start_pos": 219, "end_pos": 231, "type": "DATASET", "confidence": 0.9121058881282806}, {"text": "Cochrane", "start_pos": 321, "end_pos": 329, "type": "DATASET", "confidence": 0.9448133111000061}, {"text": "NHS", "start_pos": 334, "end_pos": 337, "type": "DATASET", "confidence": 0.5273111462593079}]}, {"text": "Lavie points out that BLEU scores above 30 reflect understandable translations, while scores over 50 are considered good and fluent translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9976250529289246}]}, {"text": "Within 36 submitted runs by our team, 24 runs have BLEU scores between \u224832 and \u224849 (for six language pairs).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9997121691703796}]}, {"text": "Therefore, we conclude that the method presented obtains generally good translation results on a variety of language pairs.", "labels": [], "entities": []}, {"text": "Another important result consists in the fact that small amounts of general domain data were selected using ATD ranging from 3.1% up to 9.35%.", "labels": [], "entities": [{"text": "ATD", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.861419141292572}]}, {"text": "This represents a promising direction for applying this method on much larger general domain corpora where selecting small amounts of data matters even more.", "labels": [], "entities": []}, {"text": "The union of the selected sentences with the classifiers trained on the source and target languages ranges from 5.6% up to 12.1%.", "labels": [], "entities": []}, {"text": "The following table presents the amount of general data selected using ATD for the three runs along with the percentage of general domain data that it represents:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora used for ATD", "labels": [], "entities": [{"text": "ATD", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.914191484451294}]}, {"text": " Table 2: Classifier accuracy (%): mean and stan- dard deviation", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9735805988311768}, {"text": "mean", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9720049500465393}, {"text": "stan- dard deviation", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.6775477081537247}]}, {"text": " Table 3: Size of the test sets", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9678277373313904}]}, {"text": " Table 5: WMT results in terms of BLEU", "labels": [], "entities": [{"text": "WMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7154969573020935}, {"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9989884495735168}]}]}