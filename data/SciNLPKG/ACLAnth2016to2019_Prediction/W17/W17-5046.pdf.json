{"title": [{"text": "Native Language Identification using Phonetic Algorithms", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6015285948912302}]}], "abstractContent": [{"text": "In this paper, we discuss the results of the IUCL system in the NLI Shared Task 2017.", "labels": [], "entities": [{"text": "NLI Shared Task 2017", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.76927749812603}]}, {"text": "For our system, we explore a variety of phonetic algorithms to generate features for Native Language Identification.", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.638498455286026}]}, {"text": "These features are contrasted with one of the most successful type of features in NLI, character n-grams.", "labels": [], "entities": []}, {"text": "We find that although phonetic features do not perform as well as character n-grams alone, they do increase overall F1 score when used together with character n-grams.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.982323944568634}]}], "introductionContent": [{"text": "Native Language Identification (NLI) is the task of automatically predicting the native language (L1) of a speaker given an unlabeled artifact such as a writing sample or speech transcript in a second language (L2).", "labels": [], "entities": [{"text": "Native Language Identification (NLI) is the task of automatically predicting the native language (L1) of a speaker given an unlabeled artifact such as a writing sample or speech transcript in a second language (L2)", "start_pos": 0, "end_pos": 214, "type": "Description", "confidence": 0.7453720375895501}]}, {"text": "Ina typical encounter with a nonnative speaker, humans have a variety of contextual clues such as race, approximate age, style of dress, and accent to assist us in making a prediction about the person's native language.", "labels": [], "entities": []}, {"text": "However, when predicting L1 relying on features that can be extracted from text alone, we must proceed without the assistance of these visual and acoustic signals.", "labels": [], "entities": []}, {"text": "Acoustic cues can bean important source of information since speakers often transfer characteristics of their L1 onto L2.", "labels": [], "entities": []}, {"text": "For example, a Japanese L1 speaker may transfer the rigid CV syllable structure onto English and epenthesize vowels into consonant clusters, which may also be reflected in writing.", "labels": [], "entities": []}, {"text": "Thus, having phonetic information may prove useful in an NLI classification task.", "labels": [], "entities": [{"text": "NLI classification task", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.9014042019844055}]}, {"text": "However, we need to make sure that the features we add can be acquired from text and do not contribute to data sparsity.", "labels": [], "entities": []}, {"text": "For the IUCL system in the Native Language Identification Shared Task, we began with the premise that acoustic features lost in text are important for language identification and they can be reconstructed using pseudo-auditory features derived from phonetic algorithms that were developed for robust matching in text search.", "labels": [], "entities": [{"text": "Native Language Identification Shared Task", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.7599458813667297}, {"text": "language identification", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.7180847823619843}]}, {"text": "Additionally, we explore a dictionary lookup that provides a phonetic representation of the words in text.", "labels": [], "entities": []}, {"text": "English orthography is rich, complex, and at times idiosyncratic.", "labels": [], "entities": []}, {"text": "Using phonetic algorithms, we can reduce some of this complexity by producing a phonetic representation of a word through a series of transformations that map characters and character sequences with similar pronunciation to a single representation such as mapping both <ph> and <f> \u2192 <f>.", "labels": [], "entities": []}, {"text": "To our knowledge, phonetic algorithms have not been explored to generate features for NLI.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all experiments, we use the C-Support Vector Classifier implementation in scikit-learn with a linear kernel.", "labels": [], "entities": []}, {"text": "For feature values, rather than using a frequency count matrix for features in the document, the TF-IDF score for the term is used instead.", "labels": [], "entities": [{"text": "TF-IDF score", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.9726387560367584}]}, {"text": "TF is the term frequency, i.e., the number of occurrences of a term in a document.", "labels": [], "entities": [{"text": "TF", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8505074977874756}, {"text": "term frequency", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.6057721674442291}]}, {"text": "IDF is the inverse document frequency, which is calculated by dividing the total number of documents in a corpus by the number of documents containing term t (if t is not equal 0).", "labels": [], "entities": [{"text": "IDF", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9853156805038452}, {"text": "inverse document frequency", "start_pos": 11, "end_pos": 37, "type": "METRIC", "confidence": 0.6535260876019796}]}, {"text": "The application of TF-IDF weighting has been used to good effect in previous research.", "labels": [], "entities": [{"text": "TF-IDF weighting", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6148546040058136}]}, {"text": "The benefit of using TF-IDF weighting in this task is that it dampens the effect of terms that are well dispersed throughout the corpus while emphasizing terms that occur less frequently and only within a smaller set of documents.", "labels": [], "entities": []}, {"text": "For NLI, TF-IDF weighting is useful for capturing and amplifying the effects of vocabulary choices that are L1-specific.", "labels": [], "entities": [{"text": "TF-IDF weighting", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7014448642730713}]}, {"text": "When using binary features, for example, only the presence or absence of a feature is recorded.", "labels": [], "entities": []}, {"text": "This effectively weights rare features, such as low frequency words and spelling errors, the same as common features, such as function words.", "labels": [], "entities": []}, {"text": "In contrast, TF-IDF gives a measure of the informativeness of a word since a word that appears in many documents will have a lower IDF than one that rarely occurs.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.7033292651176453}, {"text": "IDF", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9930434226989746}]}, {"text": "Therefore, terms that   receive a high TF-IDF score will occur with high frequency in a small number of documents.", "labels": [], "entities": [{"text": "TF-IDF score", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.9528364539146423}]}, {"text": "All experiments were conducted using character n-grams of length 2-9.", "labels": [], "entities": []}, {"text": "Additionally, we restricted the minimum document frequency to 5 documents and the maximum to 5% of documents in the training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Essay track results. Systems marked by  \u2020  were submitted as part of the official NLI Shared  Task. The remaining systems were submitted out- side of the official testing phase.", "labels": [], "entities": []}]}