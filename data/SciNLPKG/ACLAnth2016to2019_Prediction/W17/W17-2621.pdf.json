{"title": [{"text": "Gradual Learning of Matrix-Space Models of Language for Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.943625271320343}]}], "abstractContent": [{"text": "Learning word representations to capture the semantics and compositionality of language has received much research interest in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.6522131562232971}]}, {"text": "Beyond the popular vector space models, matrix representations for words have been proposed, since then, matrix multiplication can serve as natural composition operation.", "labels": [], "entities": []}, {"text": "In this work, we investigate the problem of learning matrix representations of words.", "labels": [], "entities": []}, {"text": "We present a learning approach for compositional matrix-space models for the task of sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.947812408208847}]}, {"text": "We show that our approach, which learns the matrices gradually in two steps, outperforms other approaches and a gradient-descent baseline in terms of quality and computational cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, a lot of NLP research has been devoted to word representations with the goal to capture language semantics, compositionality, and other linguistic aspects.", "labels": [], "entities": [{"text": "word representations", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7012546360492706}]}, {"text": "A prominent class of approaches to produce word representations are Vector Space Models (VSMs) of language.", "labels": [], "entities": []}, {"text": "In VSMs, a vector representation is created for each word in the text, mostly based on distributional information.", "labels": [], "entities": []}, {"text": "One of the recent prominent methods to extract vector representations of words is Word2vec, introduced by.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.9404103755950928}]}, {"text": "These models measure both syntactic and semantic aspects of words and also seem to exhibit good compositionality properties.", "labels": [], "entities": []}, {"text": "The principle of compositionality states that the meaning of a complex expression can be obtained from combining the meaning of its constituents.", "labels": [], "entities": []}, {"text": "In the Word2vec case and many other VSM approaches, some vector space operations (such as * Supported by vector addition) are used as composition operation.", "labels": [], "entities": []}, {"text": "One of the downsides of using vector addition (or other commutative operations like the component-wise product) as the compositionality operation is that word order information is inevitably lost.", "labels": [], "entities": []}, {"text": "Alternative word-order-sensitive compositionality models for word representations have been introduced, such as Compositional Matrix-Space Models (CMSMs).", "labels": [], "entities": []}, {"text": "In such models, matrices instead of vectors are used as word representations and compositionality is realized via matrix multiplication.", "labels": [], "entities": []}, {"text": "It has been proven that CMSMs are capable of simulating a wide range of VSM-based compositionality operations.", "labels": [], "entities": [{"text": "VSM-based compositionality", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.8601753115653992}]}, {"text": "The question, however, how to learn suitable word-to-matrix mappings has remained largely unexplored with few exceptions ().", "labels": [], "entities": []}, {"text": "The task is exacerbated by the fact that this amounts to a non-convex optimization problem, where a good initialization is crucial for the success of gradient descent techniques.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of learning CMSM in the domain of sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9430086612701416}]}, {"text": "As has been observed before, the sentiment of a phrase is very much influenced by the presence and position of negators and modifiers, thus word order seems to be particularly relevant for establishing an accurate sentiment score.", "labels": [], "entities": []}, {"text": "We propose to apply a two-step learning method where the output of the first step serves as initialization for the second step.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our method on the task of finegrained sentiment analysis and compare it to a previous work on learning CMSM for sentiment analysis ().", "labels": [], "entities": [{"text": "finegrained sentiment analysis", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6549073755741119}, {"text": "sentiment analysis", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.9277521073818207}]}, {"text": "Moreover, the performance of our representation learning in sentiment composition is evaluated on sentiment composition in opposing polarity phrases.", "labels": [], "entities": [{"text": "sentiment composition", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.8951819837093353}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides the related works.", "labels": [], "entities": []}, {"text": "A detailed description of the approach is presented in Section 3, followed by experiments and discussion in Section 4, and the conclusion in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach on two different datasets which provide short phrases annotated with sentiment values.", "labels": [], "entities": []}, {"text": "In both cases, we perform a ten-fold cross-validation.", "labels": [], "entities": []}, {"text": "Phrases from the MPQA Corpus Experimental Setting: For the first evaluation of the proposed approach, we use the MPQA corpus . This corpus contains newswire documents annotated with phrase-level polarity and intensity.", "labels": [], "entities": [{"text": "MPQA Corpus Experimental Setting", "start_pos": 17, "end_pos": 49, "type": "DATASET", "confidence": 0.95982426404953}, {"text": "MPQA corpus", "start_pos": 113, "end_pos": 124, "type": "DATASET", "confidence": 0.9784778356552124}]}, {"text": "We extracted the annotated phrases from the corpus documents, obtaining 9501 phrases.", "labels": [], "entities": []}, {"text": "We removed phrases with low intensity similar to Yessenalina and Cardie (2011).", "labels": [], "entities": [{"text": "Yessenalina and Cardie (2011)", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.8091069062550863}]}, {"text": "The levels of polari-ties and intensities and their translation into numerical values are as per.", "labels": [], "entities": []}, {"text": "For the evaluation, we apply a ten-fold cross-validation process on the training data as follows: eight folds are used as training set, one fold as validation set and one fold as test set.", "labels": [], "entities": []}, {"text": "The initial number of iterations in the first learning and second learning steps are set to T = 200 each, but we stop iterating when we obtain the minimum ranking loss, e = 1 n i |\u02c6\u03c9|\u02c6\u03c9 i \u2212 \u03c9 i |, on the validation set.", "labels": [], "entities": []}, {"text": "Finally, we record the ranking loss of the obtained model for the test set.", "labels": [], "entities": []}, {"text": "The learning rate \u03b7 and regularization parameter \u03bb in gradient descent are set to 0.01, by experiment.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8778014779090881}]}, {"text": "The dimension of matrices is set tom = 3 in order to be able to compare our results to the approach described by, called Matrix-space OLogReg+BowInit.", "labels": [], "entities": []}, {"text": "We call our approach Gradual Gradient descentbased Matrix-Space Models (Grad-GMSM).", "labels": [], "entities": []}, {"text": "All implementations have been done in Python 2.7.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ranking loss of compared methods.", "labels": [], "entities": [{"text": "Ranking", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9367843866348267}]}, {"text": " Table 3: Frequent phrases with average sentiment  scores", "labels": [], "entities": []}, {"text": " Table 4: Performance comparison for different  initializations in MPQA", "labels": [], "entities": [{"text": "MPQA", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.7329709529876709}]}, {"text": " Table 6: Performance comparison for different di- mensions in SCL-OPP", "labels": [], "entities": [{"text": "SCL-OPP", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.7407247424125671}]}]}