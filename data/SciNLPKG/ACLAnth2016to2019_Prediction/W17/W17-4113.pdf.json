{"title": [{"text": "Syllable-level Neural Language Model for Agglutinative Language", "labels": [], "entities": []}], "abstractContent": [{"text": "Language models for agglutinative languages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes.", "labels": [], "entities": []}, {"text": "We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property.", "labels": [], "entities": []}, {"text": "Our model outper-forms character-level embedding in per-plexity by 16.87 with 9.50M parameters.", "labels": [], "entities": []}, {"text": "Proposed method achieves state of the art performance over existing input prediction methods in terms of Key Stroke Saving and has been commercialized.", "labels": [], "entities": [{"text": "Key Stroke Saving", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7455563346544901}]}], "introductionContent": [{"text": "Recurrent neural networks (RNNs) exhibit dynamic temporal behavior which makes them ideal architectures to model sequential data.", "labels": [], "entities": []}, {"text": "In recent times, RNNs have shown state of the art performance on tasks of language modeling (RNN-LM), beating the statistical modeling techniques by a huge margin (.", "labels": [], "entities": []}, {"text": "RNN-LMs model the probability distribution over the words in vocabulary conditioned on a given input context.", "labels": [], "entities": []}, {"text": "The sizes of these networks are primarily dependent on their vocabulary size.", "labels": [], "entities": []}, {"text": "Since agglutinative languages, such as Korean, Japanese, and Turkish, have a huge number of words in the vocabulary, it is considerably hard to train word-level RNN-LM.", "labels": [], "entities": []}, {"text": "Korean is agglutinative in its morphology; words mainly contain different morphemes to determine the meaning of the word hence increasing the vocabulary size for language model training.", "labels": [], "entities": []}, {"text": "A given word in Korean * Equal contribution could have similar meaning with more than 10 variations in the suffix as shown in.", "labels": [], "entities": []}, {"text": "Various language modeling methods that rely on character or morpheme like segmentation of words have been developed () explored the idea of joint training for character and word embedding.", "labels": [], "entities": []}, {"text": "Morpheme based segmentation has been explored in both Large Vocabulary Continuous Speech Recognition (LVCSR) tasks for Egyptian Arabic ( and German newspaper corpus (.", "labels": [], "entities": [{"text": "Morpheme based segmentation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6392763058344523}, {"text": "Large Vocabulary Continuous Speech Recognition (LVCSR)", "start_pos": 54, "end_pos": 108, "type": "TASK", "confidence": 0.6917003579437733}, {"text": "German newspaper corpus", "start_pos": 141, "end_pos": 164, "type": "DATASET", "confidence": 0.7961217363675436}]}, {"text": "( used subword units to perform machine translation for rare words.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7084222584962845}]}, {"text": "Morpheme distribution has a relatively smaller frequency tail as compared to the word distribution from vocabulary, hence avoids over-fitting for tail units.", "labels": [], "entities": []}, {"text": "However, even with morpheme segmentation the percentage of out-of-vocabulary (OOV) words is significantly high in Korean.", "labels": [], "entities": [{"text": "morpheme segmentation", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7564140260219574}]}, {"text": "Character embedding in Korean is unfeasible as the context of the word is not sufficiently captured by the long sequence which composes the word.", "labels": [], "entities": [{"text": "Character embedding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7712583541870117}]}, {"text": "We select as features syllable-level embedding which has shorter sequence length and morpheme-level embedding to capture the semantics of the word.", "labels": [], "entities": []}, {"text": "We deploy our model for input word prediction on mobile devices.", "labels": [], "entities": [{"text": "input word prediction", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.6404925982157389}]}, {"text": "To achieve desirable performance we are required to create a model that has as small as possible memory and CPU footprint without compromising its performance.", "labels": [], "entities": []}, {"text": "We use differentiated softmax) for the output layer.", "labels": [], "entities": []}, {"text": "This method uses more parameters for the words that are frequent and less for the ones that occur rarely.", "labels": [], "entities": []}, {"text": "We achieve better performance than existing approaches in terms of Key Stroke Savings (KSS)) and our approach has been commercialized.", "labels": [], "entities": [{"text": "Key Stroke Savings (KSS))", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7300682812929153}]}], "datasetContent": [{"text": "Proposed method shows the best performance compared to other solutions in terms of Key Stroke Savings (KSS) as shown in.", "labels": [], "entities": [{"text": "Key Stroke Savings (KSS)", "start_pos": 83, "end_pos": 107, "type": "METRIC", "confidence": 0.7138367543617884}]}, {"text": "KSS is a percentage of key strokes not pressed compared to a vanilla keyboard which does not have any prediction or completion capabilities.", "labels": [], "entities": [{"text": "KSS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9416667819023132}, {"text": "completion", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.8743221759796143}]}, {"text": "Every user typed characters using the predictions of the language model counts as keystroke saving.", "labels": [], "entities": []}, {"text": "The dataset 1 used to evaluate KSS was manually curated to mimic user keyboard usage patterns.", "labels": [], "entities": []}, {"text": "The results in for other commercialized solutions are manually evaluated due to lack of access to their language model.", "labels": [], "entities": []}, {"text": "We use three evaluators from inspection group to cross-validate the results and remove human errors.", "labels": [], "entities": []}, {"text": "Each evaluator performed the test independently for all the other solutions to reach a consensus.", "labels": [], "entities": []}, {"text": "We try to minimize user personalization in predictions by creating anew user profile while evaluating KSS.", "labels": [], "entities": [{"text": "KSS", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.7471179962158203}]}, {"text": "The proposed method shows 37.62% in terms of KSS and outperforms compared solutions.", "labels": [], "entities": [{"text": "KSS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9930421113967896}]}, {"text": "We have achieved more than 13% improvement over the best score among existing solutions which is 33.20% in KSS.", "labels": [], "entities": [{"text": "KSS", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.8183359503746033}]}, {"text": "If the user inputs a word with our solution, we require on an average 62.38% of the word prefix to recommend the intended word, while other solutions need 66.80% of the same.", "labels": [], "entities": []}, {"text": "shows an example of word prediction across different solutions.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8230230212211609}]}, {"text": "In this example, the predictions from other solutions are same irrespective: Example of comparison with other commercialized solutions.", "labels": [], "entities": []}, {"text": "Predicted words for the Context A (rain heavily) and Context B (too much rice).", "labels": [], "entities": []}, {"text": "Other solutions make same prediction regardless of the context (only consider the last two words of context). of the context, while the proposed method treats them differently with appropriate predictions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of different embedding meth- ods. Param. : Total model paramerters, Vocab: In- put vocabulary size, Syl : Syllable, Morph: Mor- pheme.", "labels": [], "entities": []}, {"text": " Table 3: Performance comparison of proposed  method and other commercialized keyboard solu- tions by various developers.", "labels": [], "entities": []}]}