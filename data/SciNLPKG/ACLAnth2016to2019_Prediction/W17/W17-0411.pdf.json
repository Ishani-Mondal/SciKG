{"title": [], "abstractContent": [{"text": "Multilingual parser evaluation has fora longtime been hampered by the lack of cross-linguistically consistent annotation.", "labels": [], "entities": [{"text": "Multilingual parser evaluation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7963259021441141}]}, {"text": "While initiatives like Universal Dependencies have greatly improved the situation , they have also raised questions about the adequacy of existing parser evaluation metrics when applied across typologically different languages.", "labels": [], "entities": []}, {"text": "This paper argues that the usual attachment score metrics used to evaluate dependency parsers are biased in favor of analytic languages, where grammatical structure tends to be encoded in free morphemes (function words) rather than inbound morphemes (inflection).", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7215431779623032}]}, {"text": "We therefore propose an alternative evaluation metric that excludes functional relations from the attachment score.", "labels": [], "entities": []}, {"text": "We explore the effect of this change in experiments using a subset of treebanks from release v2.0 of Universal Dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "The last decade has seen a steadily growing interest in multilingual parsing research, inspired by such events as the CoNLL shared tasks on multilingual dependency parsing () and the SPMRL shared tasks on parsing morphologically rich languages ().", "labels": [], "entities": [{"text": "multilingual parsing research", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.7163234353065491}, {"text": "CoNLL shared tasks on multilingual dependency parsing", "start_pos": 118, "end_pos": 171, "type": "TASK", "confidence": 0.614277469260352}, {"text": "SPMRL shared tasks on parsing morphologically rich languages", "start_pos": 183, "end_pos": 243, "type": "TASK", "confidence": 0.7427094653248787}]}, {"text": "This has led to a number of conjectures about the suitability of different parsing models for languages with different structural characteristics, but it has been surprisingly hard to study the interplay of parsing technology and language typology in a systematic way.", "labels": [], "entities": []}, {"text": "To some extent, this is due to datarelated factors such as text genre and training set size, which are hard to control for, but even more important has been the fact that syntactic annotation is not standardized across languages.", "labels": [], "entities": []}, {"text": "This has made it almost impossible to isolate the influence of typological variables, such as word order or morphosyntactic alignment, from the effect of more or less arbitrary choices in linguistic representations.", "labels": [], "entities": []}, {"text": "The absence of cross-linguistically consistent annotation has also been a constant source of noise in the evaluation of cross-lingual learning of syntax (.", "labels": [], "entities": []}, {"text": "Fortunately, there is now also a growing interest in developing cross-linguistically consistent syntactic annotation, which has led to a number of initiatives and proposals).", "labels": [], "entities": []}, {"text": "Many of these initiatives have now converged into Universal Dependencies (UD), an open community effort that aims to develop crosslinguistically consistent treebank annotation for many languages and that has so far released 70 treebanks representing 50 languages).", "labels": [], "entities": []}, {"text": "The basic idea behind the UD scheme is to maximize parallelism across languages by focusing on dependency relations between content words, which are more likely to be similar across languages, and to use crosslinguistically valid categories for morphological and syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 263, "end_pos": 281, "type": "TASK", "confidence": 0.7159113883972168}]}, {"text": "The UD scheme is illustrated in for two translationally equivalent sentences in English and Finnish.", "labels": [], "entities": []}, {"text": "For readability, we display only a subset of the full annotation, in particular suppressing all morphological features except case.", "labels": [], "entities": []}, {"text": "The example shows that English and Finnish have rather different structural characteristics.", "labels": [], "entities": []}, {"text": "What is expressed by eight words in English is expressed by four words in Finnish, and whereas word order and function words like from are crucial in English for understanding who does what to whom, the same information is encoded in Finnish mainly by nominal case inflection (nominative for the subject, accusative for the object, and  tive for the locative modifier).", "labels": [], "entities": []}, {"text": "Moreover, Finnish has no explicit encoding of the information expressed by the definite article the in English.", "labels": [], "entities": []}, {"text": "Nevertheless, the main grammatical relations are exactly parallel in the two sentences, with the main verb chased/jahtasi having three direct nominal dependents, which can be categorized in both languages as (nominal) subject (nsubj), object (obj), and oblique modifier (obl).", "labels": [], "entities": []}, {"text": "This illustrates how UD maximizes parallelism by giving priority to dependency relations between content words.", "labels": [], "entities": [{"text": "UD maximizes parallelism", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8491969505945841}]}, {"text": "It is tempting to assume that cross-linguistically consistent annotation automatically guarantees cross-linguistically valid parser evaluation.", "labels": [], "entities": []}, {"text": "Unfortunately, this is not the case, because our old established evaluation metrics may not be adequate for the new harmonized representations.", "labels": [], "entities": []}, {"text": "The most commonly used metric in dependency parsing is the (labeled or unlabeled) attachment score, which measures the percentage of words that have been assigned the correct head (with or without taking the dependency label into account).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.88126340508461}]}, {"text": "Suppose now that a parser makes a single mistake on each of the sentences in, say, by attaching the locative modifier to the object instead of to the verb.", "labels": [], "entities": []}, {"text": "It seems intuitively correct to say that the parser has done an equally good job in both cases.", "labels": [], "entities": []}, {"text": "However, for simple arithmetical reasons, the English parser will be credited with an attachment score of 87.5%, while the Finnish parser only gets 75%.", "labels": [], "entities": [{"text": "attachment score", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.989655077457428}]}, {"text": "In other words, the impact of a single error is doubled in Finnish because of the smaller denominator.", "labels": [], "entities": []}, {"text": "Using the attachment score for crosslinguistic comparisons can therefore be quite misleading even if the annotation has been harmonized across languages.", "labels": [], "entities": []}, {"text": "What should we do about this?", "labels": [], "entities": []}, {"text": "A drastic proposal would be to give up intrinsic evaluation altogether, on the grounds that it will always be biased one way or the other, and instead put all our hope on extrinsic evaluation.", "labels": [], "entities": []}, {"text": "In doing so, however, we would run the risk of just moving the problem elsewhere.", "labels": [], "entities": []}, {"text": "For example, if we decide to evaluate parsers through their impact on machine translation quality, how do we guarantee that the latter evaluation is comparable across languages?", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7466019690036774}]}, {"text": "Furthermore, intrinsic evaluation metrics will always be useful for internal testing purposes, so we might as well do our best to develop new metrics that are better suited for cross-linguistic comparisons.", "labels": [], "entities": []}, {"text": "This is the purpose of this paper.", "labels": [], "entities": []}, {"text": "More precisely, we want to find an alternative evaluation metric for parsing with UD representations, a metric that puts more emphasis on dependency relations between content words in order to maximize comparability across languages, following the same principle as in the design of the annotation itself.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9672005772590637}]}, {"text": "We will begin by dividing the syntactic relations used in UD representations into a number of different groups and study their impact on evaluation scores.", "labels": [], "entities": [{"text": "UD representations", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8741486668586731}]}, {"text": "We will then propose anew metric called CLAS, for Content-Word Labeled Attachment Score, and analyze in more depth how different languages are affected by excluding different functional relations from the evaluation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Evaluation scores for 42 UD treebanks (development sets). LAS = Labeled Attachment Score  (overall and subsets). LAS Diff = Difference in LAS when excluding a subset of relations. Language  families/branches with at least 2 members: Slavonic, Germanic, Romance, Finno-Ugric , Baltic, Greek,  Indian, Semitic.", "labels": [], "entities": [{"text": "LAS", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9815789461135864}, {"text": "Labeled Attachment Score", "start_pos": 74, "end_pos": 98, "type": "METRIC", "confidence": 0.7449528972307841}, {"text": "LAS Diff", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9282244741916656}]}]}