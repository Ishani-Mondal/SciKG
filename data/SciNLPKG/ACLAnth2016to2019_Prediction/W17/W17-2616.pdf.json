{"title": [{"text": "Representing Compositionality based on Multiple Timescales Gated Recurrent Neural Networks with Adaptive Temporal Hierarchy for Character-Level Language Models", "labels": [], "entities": [{"text": "Representing Compositionality", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8553989827632904}]}], "abstractContent": [{"text": "A novel character-level neural language model is proposed in this paper.", "labels": [], "entities": []}, {"text": "The proposed model incorporates a biologically inspired temporal hierarchy in the architecture for representing multiple compositions of language in order to handle longer sequences for the character-level language model.", "labels": [], "entities": []}, {"text": "The temporal hierarchy is introduced in the language model by utilizing a Gated Recurrent Neural Network with multiple timescales.", "labels": [], "entities": []}, {"text": "The proposed model incorporates a timescale adaptation mechanism for enhancing the performance of the language model.", "labels": [], "entities": []}, {"text": "We evaluate our proposed model using the popular Penn Tree-bank and Text8 corpora.", "labels": [], "entities": [{"text": "Penn Tree-bank", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.9933627545833588}, {"text": "Text8 corpora", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.8820753395557404}]}, {"text": "The experiments show that the use of multiple timescales in a Neural Language Model (NLM) enables improved performance despite having fewer parameters and with no additional computation requirements.", "labels": [], "entities": []}, {"text": "Our experiments also demonstrate the ability of the adaptive temporal hierarchies to represent multiple compositonality without the help of complex hierarchical architec-tures and shows that better representation of the longer sequences lead to enhanced performance of the probabilistic language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language Modeling is a fundamental task central to Natural Language Processing (NLP) and language understanding.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.772330641746521}, {"text": "Natural Language Processing (NLP)", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.6907407740751902}, {"text": "language understanding", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7376125752925873}]}, {"text": "A character-level language model (CLM) can be interpreted as a probability estimation method for the next character given a sequence of characters as input.", "labels": [], "entities": []}, {"text": "From the perspective of sequence generation, predicting one character at a time has higher importance since it allows the network to invent novel words and strings.", "labels": [], "entities": [{"text": "sequence generation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7274368703365326}, {"text": "predicting one character", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.8853577772776285}]}, {"text": "CLMs are commonly used for modeling new words and there have been successful techniques that use generative language models (LMs) based on characters or phonemes.", "labels": [], "entities": []}, {"text": "Recurrent neural networks have been applied to CLMs.", "labels": [], "entities": []}, {"text": "Recently introduced a LM with explicit hierarchical architecture to work at character levels and word levels.", "labels": [], "entities": []}, {"text": "introduced recurrent batch normalization into CLMs which significantly improved the performance.", "labels": [], "entities": []}, {"text": "However, since the population statistics are estimated separately for each time step, the model is computationally intensive particularly fora CLM where the number of steps are more than conventional word level LMs.", "labels": [], "entities": []}, {"text": "Similarly, introduced regularization in CLMs using a norm-stabilizer and reported an increased training time for higher levels of regularization.", "labels": [], "entities": []}, {"text": "In spite of the recent successes, CLMs still have inferior performance compared to its equivalent word-level models ( ) since these LMs need to consider longer history of tokens to properly predict the next one.", "labels": [], "entities": []}, {"text": "In order to improve the performance of the CLMs, there is a need for better representation of the additional levels of compositionality and the richer discourse structure found in CLMs.", "labels": [], "entities": []}, {"text": "used multiple timescale RNNs to learn the linguistic hierarchy for speech related tasks and demonstrated that, during listening to connected speech, cortical activity of different timescales concurrently tracked the time course of abstract linguistic compositionality at different hierarchical levels, such as words, phrases and sentences.", "labels": [], "entities": []}, {"text": "In this work, we propose a character-level recurrent neural network (RNN) LM that employs an adaptive multiple timescales approach to incorporate temporal hierarchies in the architecture to enhance the representation of multiple compositionalities.", "labels": [], "entities": [{"text": "character-level recurrent neural network (RNN) LM", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.5793559215962887}]}, {"text": "Our proposed model includes a novel timescale update mechanism which enhances the adaptation of the temporal hierarchy during the learning process.", "labels": [], "entities": []}, {"text": "We build the temporal hierarchical structure using fast and slow context units to imitate different timescales.", "labels": [], "entities": []}, {"text": "This temporal hierarchy concept is implemented based on the multiple timescales gated recurrent unit (MTGRU) () that incorporates multiple timescales at different layers of the RNN.", "labels": [], "entities": []}, {"text": "Our model, inspired by the concept of temporal hierarchy found in the human brain, demonstrates the ability to capture multiple compositionalities similar to the findings of.", "labels": [], "entities": []}, {"text": "This better representation learning capability enhances the ability of our model to handle longer sequences for the CLM.", "labels": [], "entities": []}, {"text": "The resulting LM is a much simpler model that does not incorporate explicit hierarchical structures or normalization techniques.", "labels": [], "entities": []}, {"text": "We show that our CLM with the biologically inspired temporal hierarchy is able to achieve performance comparable to the existing state-of-theart CLMs evaluated over the Penn Treebank (PTB) and Text8 corpora.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 169, "end_pos": 188, "type": "DATASET", "confidence": 0.9702102303504944}, {"text": "Text8 corpora", "start_pos": 193, "end_pos": 206, "type": "DATASET", "confidence": 0.7928027510643005}]}], "datasetContent": [{"text": "We evaluate our CLM on the Penn Treebank (PTB) corpus.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) corpus", "start_pos": 27, "end_pos": 53, "type": "DATASET", "confidence": 0.9782185653845469}]}, {"text": "We use orthogonal initialization for all the weight matrices and use stochastic gradient descent with gradient clipping at 1.0 and step rule determined by Adam ().", "labels": [], "entities": []}, {"text": "We report the hyperparameter values that were explored in our experiments in.", "labels": [], "entities": []}, {"text": "The timescale for the fast layer is initialized to 1 in all the experiments as \u03c4 = 1 defines the default or the input timescale.", "labels": [], "entities": []}, {"text": "We also conduct an additional experiment for the comparison of computational efficiency of our model with normalization based techniques.", "labels": [], "entities": []}, {"text": "The details of all the experiments are described in the sections below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Bits-Per-Character on PTB test and size  of the models. MTGRU-CLM and MTGRU- CLM-Adaptive correspond to our CLMs with a  constant timescale and an adaptive timescales re- spectively.  *  These are estimated model sizes as  the actual number of parameters is not available in  the literature.", "labels": [], "entities": [{"text": "MTGRU-CLM", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.7816128730773926}]}, {"text": " Table 3: Bits-Per-Character on Text8 test and size  of the Models. MTGRU-CLM and MTGRU- CLM-Adaptive correspond to our CLMs with a  constant timescale and an adaptive timescales re- spectively.  *  These are estimated parameter sizes  as the actual value is not available in the literature.", "labels": [], "entities": [{"text": "MTGRU-CLM", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.802592396736145}]}, {"text": " Table 5: Sequential MNIST classification results  with training duration of 100K steps.", "labels": [], "entities": [{"text": "MNIST classification", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8035088777542114}]}]}