{"title": [{"text": "Neural Machine Translation with Extended Context", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7460605104764303}]}], "abstractContent": [{"text": "We investigate the use of extended context in attention-based neural machine translation.", "labels": [], "entities": [{"text": "attention-based neural machine translation", "start_pos": 46, "end_pos": 88, "type": "TASK", "confidence": 0.6083387732505798}]}, {"text": "We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units.", "labels": [], "entities": []}, {"text": "We study the use of extended source language context as well as bilingual context extensions.", "labels": [], "entities": []}, {"text": "The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality.", "labels": [], "entities": []}, {"text": "In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7460891902446747}]}, {"text": "Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult.", "labels": [], "entities": [{"text": "MT aware", "start_pos": 30, "end_pos": 38, "type": "TASK", "confidence": 0.8247604370117188}]}, {"text": "Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena).", "labels": [], "entities": []}, {"text": "The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders).", "labels": [], "entities": []}, {"text": "Modest improvements could, for example, be shown for the translation of pronouns) and the generation of appropriate discourse connectives (.", "labels": [], "entities": [{"text": "translation of pronouns", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.8909834225972494}]}, {"text": "Textual coherence is also often tackled in terms of translation consistency for domainspecific terminology based on the one-translationper-discourse principle.", "labels": [], "entities": []}, {"text": "Overall, none of the ideas lead to significant improvements of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.9679319262504578}]}, {"text": "Besides, the development of task-and problem-specific models that work independently from the general translation task is not very satisfactory.", "labels": [], "entities": []}, {"text": "However, the recent success of neural machine translation opens new possibilities for tackling discourserelated phenomena in a more generic way.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.767055610815684}]}, {"text": "In this paper, we present a pilot study that looks at simple ideas for extending the context in the framework of standard attention-based encoder-decoder models.", "labels": [], "entities": []}, {"text": "The purpose of the paper is to identify the capabilities of NMT to discover cross-sentential dependencies without explicit annotation or guidance.", "labels": [], "entities": []}, {"text": "In contrast to related work that modifies the neural MT model by an additional context encoder an a separate attention mechanism (, we keep the standard setup and just modify the input and output segments.", "labels": [], "entities": []}, {"text": "We run a series of experiments with different context windows and discuss the effect of additional information on translation and attention.", "labels": [], "entities": [{"text": "translation", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.9793503880500793}]}], "datasetContent": [{"text": "We train attention-based models using the Helsinki NMT system with similar parameters but different training data to seethe effect of contextual information.", "labels": [], "entities": [{"text": "Helsinki NMT system", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.9444953799247742}]}, {"text": "Our baseline system involves a standard setup where the training examples come from the aligned parallel subtitle corpus (1 source translation unit and 1 target translation unit).", "labels": [], "entities": []}, {"text": "This will be the reference in our evaluations and discussions.", "labels": [], "entities": []}, {"text": "In all cases, we translate the test set of 5,000 sentences with an ensemble model consisting of the final four savepoint models after running roughly the same number of training itera-tions with similar amounts of training instances seen by the model.", "labels": [], "entities": []}, {"text": "Savepoint averaging slightly alleviates the problem that each model will differ due to the stochastic nature of the training procedures, making a direct comparison of the outcomes difficult especially if the observed differences are small.", "labels": [], "entities": [{"text": "Savepoint averaging", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6749781370162964}]}, {"text": "Automatic evaluation metrics are problematic, in particular for assessing discourse-related phenomena.", "labels": [], "entities": []}, {"text": "However, it is important to verify that the context-models are on-par with the baseline.", "labels": [], "entities": []}, {"text": "shows the BLEU scores and also the alternative character-level chrF3 measure for all systems (2+1 in its two variants with and without prefix markup).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984203577041626}]}, {"text": "The 2+2 model is evaluated on the last segment in the generated output and ignores all other parts before.", "labels": [], "entities": []}, {"text": "The table shows that all models are quite similar to each other, with a slightly higher BLEU score for the 2+1 system with sentence breaks.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9785082936286926}]}, {"text": "The chrF3 score is also slightly higher for both, the 2+1 and 2+2 systems with sentence breaks, due to a higher recall.", "labels": [], "entities": [{"text": "chrF3 score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.8382071852684021}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9985995888710022}]}, {"text": "The differences are small but the results already show that the system is capable of handling larger units without harming the performance and additional improvements are possible.", "labels": [], "entities": []}, {"text": "Let us know look at some details to study the effects of contextual information on translation output.", "labels": [], "entities": [{"text": "translation output", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.9014483392238617}]}, {"text": "The example of raises the question whether the extended model is able to reliably and systematically disambiguate pronominal translations.", "labels": [], "entities": []}, {"text": "In order to answer this question, we extracted all occurrences of the ambiguous pronoun sie/Sie from our test set (1143 occurrences in 1018 sentences, i.e. in every fifth sentence of the test set) and manually evaluated about half of them (565 occurrences in 516 sentences), comparing the output of the baseline system with the one of the 2+2 system.", "labels": [], "entities": []}, {"text": "We distinguish four categories on the basis of the reference translation: polite imperative Sie, other occurrences of the polite pronoun Sie, feminine singular sie and plural sie.", "labels": [], "entities": []}, {"text": "The table shows that polite forms are most frequent in the corpus and also rather easy to translate thanks to capitalisation.", "labels": [], "entities": []}, {"text": "In the case of imperatives, they simply are deleted (e.g., Kommen Sie!", "labels": [], "entities": []}, {"text": "becomes Come!), whereas in other contexts they are consistently translated to you.", "labels": [], "entities": []}, {"text": "The remaining errors are mainly due to entire segments that are left untranslated, or to erroneous lowercasing of sentence-initial positions during preprocessing.", "labels": [], "entities": []}, {"text": "Distinguishing singular from plural readings is harder: a non-polite form sie can be translated as she or it in its singular reading (depending on the grammatical gender of the antecedent), or as they or them in its plural reading (depending on case).", "labels": [], "entities": []}, {"text": "The figures show that the extended model is better at correctly predicting they (and them), but that correctly predicting she or it is equally hard with or without context.", "labels": [], "entities": []}, {"text": "It remains to be investigated if this also holds for the 2+1 model and variants thereof.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluation: BLEU and chrF3  (including precision and recall).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.998444139957428}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9994885921478271}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9975554943084717}]}, {"text": " Table 2: Word types with the highest external at- tention and the rank of some cross-lingually am- biguous pronouns in the list sorted by the propor- tion (prop.) of external attention. \u2205 pos. gives the  average token position of the target word.", "labels": [], "entities": []}, {"text": " Table 3: Word types with the highest average of  external attention peaks.", "labels": [], "entities": []}, {"text": " Table 4: BLEU and chrF3 on extended context  segments (sliding window). Individual segments  are simply concatenated in the baseline system  where necessary.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.990998387336731}]}, {"text": " Table 5: Word types with the highest cross- segmental attention (excluding attention on sen- tence break symbols)).", "labels": [], "entities": []}, {"text": " Table 6: Word types with the highest average of  cross-segmental attention peaks.", "labels": [], "entities": []}, {"text": " Table 7: Word types with the highest proportion  of cross-segmental attention peaks, with absolute  frequencies of cross-segmental attention peak and  overall absolute word frequencies.", "labels": [], "entities": []}, {"text": " Table 8: Percentages of correct translations of the  pronoun sie/Sie.", "labels": [], "entities": [{"text": "Percentages", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9613680243492126}]}]}