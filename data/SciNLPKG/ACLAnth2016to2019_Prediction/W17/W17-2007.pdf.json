{"title": [], "abstractContent": [{"text": "Finding a product in the fashion world can be a daunting task.", "labels": [], "entities": []}, {"text": "Everyday, e-commerce sites are updating with thousands of images and their associated metadata (tex-tual information), deepening the problem.", "labels": [], "entities": []}, {"text": "In this paper, we leverage both the images and textual metadata and propose a joint multi-modal embedding that maps both the text and images into a common latent space.", "labels": [], "entities": []}, {"text": "Distances in the latent space correspond to similarity between products, allowing us to effectively perform retrieval in this latent space.", "labels": [], "entities": []}, {"text": "We compare against existing approaches and show significant improvements in retrieval tasks on a large-scale e-commerce dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "The level of traffic of modern e-commerce is growing fast.", "labels": [], "entities": []}, {"text": "U.S. retail e-commerce, for instance, was expected to grow 16.6% on 2016 Christmas holidays (after a 15.3% increase in 2014).", "labels": [], "entities": []}, {"text": "In order to adapt to these trend, sellers must provide a good experience with easy to find and well classified products.", "labels": [], "entities": []}, {"text": "In this work, we consider the problem of multi-modal retrieval, in which a user searches for either text or images given a text or image query.", "labels": [], "entities": []}, {"text": "Existing approaches for retrieval focus image-only and require hard to obtain datasets for training.", "labels": [], "entities": []}, {"text": "Instead, we opt to leverage easily obtained metadata for training our model, and learning a mapping from text and images to a common latent space, in which distances correspond to similarity.", "labels": [], "entities": []}, {"text": "We evaluate our approach in the retrieval and classification tasks and it outperforms KCCA () and Bag-of-word features on a large e-commerce dataset.: Example of a text and nearest images from the test set.", "labels": [], "entities": []}, {"text": "Our embedding produces low distances between texts and images referring to similar objects.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of our method compared to  KCCA and our method using Bag of Words for text  representation.", "labels": [], "entities": [{"text": "KCCA", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.937563955783844}, {"text": "text  representation", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7869684398174286}]}]}