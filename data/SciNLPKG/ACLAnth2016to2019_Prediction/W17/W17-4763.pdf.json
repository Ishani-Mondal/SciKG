{"title": [{"text": "Predictor-Estimator using Multilevel Task Learning with Stack Propagation for Neural Quality Estimation", "labels": [], "entities": [{"text": "Neural Quality Estimation", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.6021322806676229}]}], "abstractContent": [{"text": "In this paper, we present a two-stage neu-ral quality estimation model that uses mul-tilevel task learning for translation quality estimation (QE) at the sentence, word, and phrase levels.", "labels": [], "entities": [{"text": "translation quality estimation (QE)", "start_pos": 111, "end_pos": 146, "type": "TASK", "confidence": 0.7639532883961996}]}, {"text": "Our approach is based on an end-to-end stacked neural model named Predictor-Estimator, which has two stages consisting of a neural word prediction model and neural QE model.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.6897917091846466}]}, {"text": "To efficiently train the two-stage model, a stack propagation method is applied, thereby enabling us to jointly learn the word prediction model and QE model in a single learning mode.", "labels": [], "entities": [{"text": "stack propagation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7327906787395477}, {"text": "word prediction", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.7430954575538635}]}, {"text": "In addition, we deploy multilevel task learning with stack propagation, where the training examples available for all QE subtasks (i.e., sen-tence/word/phrase levels) are used to train a Predictor-Estimator fora specific sub-task.", "labels": [], "entities": [{"text": "stack propagation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7132522165775299}]}, {"text": "All of our submissions to the QE task of WMT17 are ensembles that combine a set of neural models trained under different settings of varying dimensionali-ties and shuffling training examples, eventually achieving the best performances for all subtasks at the sentence, word, and phrase levels.", "labels": [], "entities": [{"text": "QE task of WMT17", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.659646600484848}]}], "introductionContent": [{"text": "In this paper, we describe the two-stage end-toend neural models submitted to the Shared Task on Sentence/Word/Phrase-Level Quality Estimation (QE task) at the 2017 Conference on Machine Translation (WMT17).", "labels": [], "entities": [{"text": "Shared Task on Sentence/Word/Phrase-Level Quality Estimation (QE task) at the 2017 Conference on Machine Translation (WMT17)", "start_pos": 82, "end_pos": 206, "type": "TASK", "confidence": 0.7416101545095444}]}, {"text": "The task aims at estimating quality scores/categories for an unseen translation without a reference translation at various granularities (i.e., sentence/word/phrase levels) ().", "labels": [], "entities": []}, {"text": "Our neural network-based models for sentence/word/phrase-level QE are based on Predictor-Estimator architecture, which is a two-stage end-to-end neural QE model.", "labels": [], "entities": [{"text": "sentence/word/phrase-level QE", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.5628130485614141}]}, {"text": "In this submission to WMT 2017, our Predictor-Estimator model is further advanced by extensively applying a stack propagation method) in order to efficiently train the two-stage model.", "labels": [], "entities": [{"text": "WMT 2017", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8659465610980988}]}, {"text": "The Predictor-Estimator architecture) is the two-stage neural QE model) consisting of two types of stacked neural models: 1) a neural word prediction model (i.e., word predictor) trained from additional large-scale parallel corpora and 2) a neural QE model (i.e., quality estimator) trained from quality-annotated noisy parallel corpora called QE data.", "labels": [], "entities": [{"text": "word predictor", "start_pos": 163, "end_pos": 177, "type": "TASK", "confidence": 0.6594336181879044}]}, {"text": "The Predictor-Estimator architecture uses word prediction as a pre-task for QE.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7729104459285736}]}, {"text": "showed that word prediction is helpful for improving the QE performance.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7711895108222961}, {"text": "QE", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.7933033108711243}]}, {"text": "In the first stage, the word predictor, which is based on a bidirectional and bilingual recurrent neural network (RNN) language model -the modification of the attention-based RNN encoder-decoder () -predicts a target word conditioned with unbounded source and target contexts.", "labels": [], "entities": [{"text": "word predictor", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.6719573438167572}]}, {"text": "QE feature vectors (QEFVs) are the approximated knowledge transferred from word prediction to QE.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.7269248068332672}]}, {"text": "In the second stage, QEFVs are used as inputs to the quality estimator for estimating sentence/word/phrase-level translation quality.", "labels": [], "entities": [{"text": "estimating sentence/word/phrase-level translation quality", "start_pos": 75, "end_pos": 132, "type": "TASK", "confidence": 0.7924108095467091}]}, {"text": "Stack propagation () is a learning method for efficient joint learning that enables backpropagation down the stacked models.", "labels": [], "entities": [{"text": "Stack propagation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.823470413684845}]}, {"text": "where continuous hidden layer activations of the POS tagger network are used as an input to the parser network.", "labels": [], "entities": []}, {"text": "We applied the Predictor-Estimator architecture to the sentence/word/phrase-level QE task of WMT17.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.9218279123306274}]}, {"text": "In the original Predictor-Estimator architecture proposed by, the word predictor and quality estimator are trained individually.", "labels": [], "entities": []}, {"text": "As a result, the backpropagation in training the quality estimator does not go down for the word predictor network.", "labels": [], "entities": [{"text": "word predictor", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.6480400264263153}]}, {"text": "Because there exists a continuous and differentiable link between the stacked word predictor and quality estimator, we used stack propagation to jointly learn twostage models in the Predictor-Estimator.", "labels": [], "entities": [{"text": "stack propagation", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7298964560031891}]}, {"text": "Furthermore, we deployed multilevel task learning with stack propagation, where a task-specific PredictorEstimator is trained by using not only the taskspecific training examples but also all other training examples of QE subtasks.", "labels": [], "entities": [{"text": "stack propagation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.734648585319519}]}, {"text": "Finally, all of our submissions for the QE task of WMT17 were ensembles that combine a set of neural models trained under different settings of varying dimensionalities and shuffled training examples.", "labels": [], "entities": [{"text": "QE task of WMT17", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.6693075895309448}]}], "datasetContent": [{"text": "We evaluated our models for the WMT17 QE task of sentence/word/phrase-level EnglishGerman and German-English.", "labels": [], "entities": [{"text": "WMT17 QE task", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.5173085530598959}]}, {"text": "To train our twostage models, we used QE data for the WMT17 QE task (Specia and Logacheva, 2017) and par-1 QE data consist of source sentences, target translations (not references), and their target quality annotations for sentence/word/phrase levels.", "labels": [], "entities": [{"text": "WMT17 QE task", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.6959487398465475}]}, {"text": "An original phrase-level Predictor-Estimator and original word-level Predictor-Estimator have different architectures in that the input of the former is phrase-level QEFV, which is the average of its constituent word-level QEFVs.", "labels": [], "entities": []}, {"text": "However, in multilevel task learning with stack propagation for phrase-level QE, we use a word-level PredictorEstimator architecture.", "labels": [], "entities": [{"text": "stack propagation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.727351188659668}]}, {"text": "In the word-level Predictor-Estimator for phrase-level QE, if any word in the phrase boundary is tagged as 'BAD,' the output of the phrase level has a 'BAD' tag, which exactly corresponds with the purpose of the phrase-level QE.", "labels": [], "entities": [{"text": "BAD", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9735428690910339}, {"text": "BAD", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.960384726524353}]}, {"text": "allel corpora including the Europarl corpus, common crawl corpus, news commentary, rapid corpus of EU press releases for the WMT17 translation task 3 , and src-pe (source sentences-their target post-editions) pairs for the WMT17 QE task.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9879101514816284}, {"text": "WMT17 translation task 3", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.5829775631427765}, {"text": "WMT17 QE task", "start_pos": 223, "end_pos": 236, "type": "DATASET", "confidence": 0.8289397160212199}]}, {"text": "All Predictor-Estimator models were initialized with a word predictor and quality estimator that were pre-trained individually.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the single Predictor-Estimator models on the WMT17 En-De dev set.", "labels": [], "entities": [{"text": "WMT17 En-De dev set", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.9373408555984497}]}, {"text": " Table 2: Results of the single Predictor-Estimator models on the WMT17 En-De test set.", "labels": [], "entities": [{"text": "WMT17 En-De test set", "start_pos": 66, "end_pos": 86, "type": "DATASET", "confidence": 0.9412333220243454}]}, {"text": " Table 3: Results of the single Predictor-Estimator models on the WMT17 De-En test set.", "labels": [], "entities": [{"text": "WMT17 De-En test set", "start_pos": 66, "end_pos": 86, "type": "DATASET", "confidence": 0.9480109959840775}]}, {"text": " Table 4: Results of ensembles of multi-instance Predictor-Estimator models on the WMT17 En-De test  set.", "labels": [], "entities": [{"text": "WMT17 En-De test  set", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.9339525252580643}]}, {"text": " Table 5: Results of ensembles of multi-instance Predictor-Estimator models on WMT17 De-En test set.", "labels": [], "entities": [{"text": "WMT17 De-En test set", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.9434668719768524}]}]}