{"title": [{"text": "Assessing SRL Frameworks with Automatic Training Data Expansion", "labels": [], "entities": [{"text": "Assessing SRL", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8058308064937592}]}], "abstractContent": [{"text": "We present the first experiment-based study that explicitly contrasts the three major semantic role labeling frameworks.", "labels": [], "entities": []}, {"text": "As a prerequisite, we create a dataset labeled with parallel FrameNet-, PropBank-, and VerbNet-style labels for German.", "labels": [], "entities": []}, {"text": "We train a state-of-the-art SRL tool for German for the different annotation styles and provide a comparative analysis across frameworks.", "labels": [], "entities": [{"text": "SRL", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.948127031326294}]}, {"text": "We further explore the behavior of the frameworks with automatic training data generation.", "labels": [], "entities": []}, {"text": "VerbNet provides larger semantic expressivity than PropBank, and we find that its generalization capacity approaches PropBank in SRL training, but it benefits less from training data expansion than the sparse-data affected FrameNet.", "labels": [], "entities": [{"text": "SRL training", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.9199119806289673}, {"text": "FrameNet", "start_pos": 223, "end_pos": 231, "type": "DATASET", "confidence": 0.9010716676712036}]}], "introductionContent": [{"text": "We present the first study that explicitly contrasts the three popular theoretical frameworks for semantic role labeling (SRL) -FrameNet, PropBank, and VerbNet 1 in a comparative experimental setup, i.e., using the same training and test sets annotated with predicate and role labels from the different frameworks and applying the same conditions and criteria for training and testing.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.7948326170444489}, {"text": "PropBank", "start_pos": 138, "end_pos": 146, "type": "DATASET", "confidence": 0.836571455001831}]}, {"text": "Previous work comparing these frameworks either provides theoretical investigations, for instance for the pair PropBank-FrameNet (), or presents experimental investigations for the pair PropBank-VerbNet (.", "labels": [], "entities": [{"text": "PropBank-FrameNet", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.9043011665344238}]}, {"text": "Theoretical analyses contrast the richness of the semantic model of FrameNet with efficient annotation of PropBank labels and their suitability for system training.", "labels": [], "entities": []}, {"text": "Verb-Net is considered to range between them on both scales: it fulfills the need for semantically meaningful role labels; also, since the role labels are shared across predicate senses, it is expected to generalize better to unseen predicates than FrameNet, which suffers from data sparsity due to a fine-grained sense-specific role inventory.", "labels": [], "entities": []}, {"text": "Yet, unlike PropBank and FrameNet, VerbNet has been neglected in recent work on SRL, partially due to the lack of training and evaluation data, whereas PropBank and FrameNet were popularized in shared tasks.", "labels": [], "entities": [{"text": "SRL", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9472538232803345}]}, {"text": "As a result, the three frameworks have not been compared under equal experimental conditions.", "labels": [], "entities": []}, {"text": "This motivates our contrastive analysis of all three frameworks for German.", "labels": [], "entities": []}, {"text": "We harness existing datasets for German () to create SR3de (Semantic Role Triple Dataset for German), the first benchmark dataset labeled with FrameNet, VerbNet and PropBank roles in parallel.", "labels": [], "entities": []}, {"text": "Our motivation for working on German is thatas for many languages besides English -sufficient amounts of training data are not available.", "labels": [], "entities": []}, {"text": "This clearly applies to our German dataset, which contains about 3,000 annotated predicates.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9483247101306915}]}, {"text": "In such a scenario, methods to extend training data automatically or making efficient use of generalization across predicates (i.e., being able to apply role labels to unseen predicates) are particularly desirable.", "labels": [], "entities": []}, {"text": "We assume that SRL frameworks that generalize better across predicates gain more from automatic training data generation, and lend themselves better to cross-predicate SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9843944907188416}, {"text": "SRL", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.6726459264755249}]}, {"text": "System performance also needs to be correlated with the semantic expressiveness of frameworks: with the ever-growing expectations in semantic NLP applications, SRL frameworks also need to be judged with regard to their contribution to advanced applications where expressiveness may play a role, such as question answering or summarization.", "labels": [], "entities": [{"text": "SRL frameworks", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.8976413309574127}, {"text": "question answering", "start_pos": 303, "end_pos": 321, "type": "TASK", "confidence": 0.8877499103546143}, {"text": "summarization", "start_pos": 325, "end_pos": 338, "type": "TASK", "confidence": 0.783642590045929}]}, {"text": "Our work explores the generalization properties of three SRL frameworks in a contrastive setup, assessing SRL performance when training and evaluating on a dataset with parallel annotations for each framework in a uniform SRL system architecture.", "labels": [], "entities": [{"text": "SRL frameworks", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.8956923484802246}, {"text": "SRL", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9849167466163635}]}, {"text": "We also explore to what extent the frameworks benefit from training data generation via annotation projection.", "labels": [], "entities": []}, {"text": "Since all three frameworks have been applied to several languages, 2 we expect our findings for German to generalize to other languages as well.", "labels": [], "entities": []}, {"text": "Our contributions are (i) novel resources: parallel German datasets for the three frameworks, including automatically acquired training data; and (ii) empirical comparison of the labeling performance and generalization capabilities of the three frameworks, which we discuss in view of their respective semantic expressiveness.", "labels": [], "entities": []}], "datasetContent": [{"text": "SR3de: a German parallel SRL dataset The VerbNet-style dataset by covers a subset of the PropBank-style CoNLL 2009 annotations, which are based on the German FrameNet-style SALSA corpus.", "labels": [], "entities": [{"text": "SR3de", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9035037159919739}, {"text": "German parallel SRL dataset", "start_pos": 9, "end_pos": 36, "type": "DATASET", "confidence": 0.5831486284732819}, {"text": "VerbNet-style dataset", "start_pos": 41, "end_pos": 62, "type": "DATASET", "confidence": 0.9163871109485626}, {"text": "PropBank-style CoNLL 2009 annotations", "start_pos": 89, "end_pos": 126, "type": "DATASET", "confidence": 0.8935551047325134}, {"text": "FrameNet-style SALSA corpus", "start_pos": 158, "end_pos": 185, "type": "DATASET", "confidence": 0.7464482386906942}]}, {"text": "This allowed us to create SR3de, the first corpus with parallel sense and role labels from SALSA, PropBank, and GermaNet/VerbNet, which we henceforth abbreviate as FN, PB, and VN respectively.", "labels": [], "entities": []}, {"text": "displays an example with parallel annotations.", "labels": [], "entities": []}, {"text": "Data statistics in shows that with almost 3,000 predicate instances, the corpus is fairly small.", "labels": [], "entities": []}, {"text": "The distribution of role types across frameworks highlights their respective role granularity, ranging from 10 for PB to 30 for VN and 278 for FN.", "labels": [], "entities": [{"text": "FN", "start_pos": 143, "end_pos": 145, "type": "DATASET", "confidence": 0.867203414440155}]}, {"text": "The corpus offers 2,196 training predicates and covers the CoNLL 2009 development and test sets; thus it is a suitable base for comparing the three frameworks.", "labels": [], "entities": [{"text": "CoNLL 2009 development and test sets", "start_pos": 59, "end_pos": 95, "type": "DATASET", "confidence": 0.9260582625865936}]}, {"text": "We use SR3de for the contrastive analysis of the different SRL frameworks below.", "labels": [], "entities": [{"text": "SR3de", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.8645837306976318}]}, {"text": "Training data expansion To overcome the data scarcity of our corpus, we use monolingual annotation projection (F\u00fcrstenau and Lapata, 2012) to generate additional training data.", "labels": [], "entities": []}, {"text": "Given a set of labeled seed sentences and a set of unlabeled ex-: Data statistics for SR3de.", "labels": [], "entities": [{"text": "SR3de", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.8138710260391235}]}, {"text": "pansion sentences, we select suitable expansions based on the predicate lemma and align dependency graphs of seeds and expansions based on lexical similarity of the graph nodes and syntactic similarity of the edges.", "labels": [], "entities": []}, {"text": "The alignment is then used to map predicate and role labels from the seed sentences to the expansion sentences.", "labels": [], "entities": []}, {"text": "For each seed instance, the k best-scoring expansions are selected.", "labels": [], "entities": []}, {"text": "Given a seed set of size n and the maximal number of expansions per seed k, we getup ton \u00b7 k additional training instances.", "labels": [], "entities": []}, {"text": "Lexical and syntactic similarity are balanced using the weight parameter \u03b1.", "labels": [], "entities": []}, {"text": "Our adjusted re-implementation uses the matetools dependency parser (Bohnet, 2010) and word2vec embeddings () trained on deWAC () for word similarity calculation.", "labels": [], "entities": [{"text": "word similarity calculation", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.7417018016179403}]}, {"text": "We tune the parameter \u03b1 via intrinsic evaluation on the SR3de dev set.", "labels": [], "entities": [{"text": "SR3de dev set", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9424750606218973}]}, {"text": "We project the seed set SR3de-train directly to SR3de-dev and compare the labels from the k=1 best seeds fora dev sentence to the gold label, measuring F1 for all projections.", "labels": [], "entities": [{"text": "SR3de-dev", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9383591413497925}, {"text": "F1", "start_pos": 152, "end_pos": 154, "type": "METRIC", "confidence": 0.9975038170814514}]}, {"text": "Then we use the best-scoring \u03b1 value for each framework to project annotations from the SR3de training set to deWAC for predicate lemmas occurring at least 10 times.", "labels": [], "entities": [{"text": "SR3de training set", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.969384491443634}]}, {"text": "We vary the number of expansions k, selecting k from {1, 3, 5, 10, 20}.", "labels": [], "entities": []}, {"text": "Using larger k values is justified because a) projecting to a huge corpus is likely to generate many high-quality expansions, and b) we expect a higher variance in the generated data when also selecting lower-scoring expansions.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation on the dev set provides an estimate of the projection quality: we observe F1 score of 0.73 for PB and VN, and of 0.53 for FN.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9907616674900055}, {"text": "FN", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.8248693943023682}]}, {"text": "The lower scores for FN are due to data sparsity in the intrinsic setting and are expected to improve when projecting on a large corpus.", "labels": [], "entities": [{"text": "FN", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.6472101807594299}]}, {"text": "Experiment setup We perform extrinsic evaluation on SR3de with parallel annotations for the three frameworks, using the same SRL system for each framework, to a) compare the labeling perfor-mance of the learned models, and b) explore their behavior in response to expanded training data.", "labels": [], "entities": [{"text": "SR3de", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8116282224655151}]}, {"text": "We employ the following settings (cf.: #BL: Baseline We train on SR3de train, which is small, but comparable across frameworks.", "labels": [], "entities": [{"text": "BL", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9925767779350281}]}, {"text": "#FB: Full baseline We train on the full CoNLLtraining sections for PropBank and SALSA, to compare to state-of-the-art results and contrast the low-resource #BL to full resources.", "labels": [], "entities": [{"text": "FB", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9910271763801575}, {"text": "PropBank", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9203340411186218}, {"text": "BL", "start_pos": 157, "end_pos": 159, "type": "METRIC", "confidence": 0.9488073587417603}]}, {"text": "3 #EX: Expanded We train on data expanded via annotation projection.", "labels": [], "entities": [{"text": "EX", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.8337515592575073}]}, {"text": "We train mateplus using the reranker option and the default featureset for German 4 excluding word embedding features.", "labels": [], "entities": [{"text": "German 4", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9461497366428375}]}, {"text": "We explore the following role labeling tasks: predicate sense prediction (pd in mateplus), argument identification (ai) and role labeling (ac) for predicted predicate sense (pd+ai+ac) and oracle predicate sense (ai+ac).", "labels": [], "entities": [{"text": "role labeling", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.6960709244012833}, {"text": "predicate sense prediction", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.8733416795730591}, {"text": "argument identification", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7137170135974884}, {"text": "role labeling", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.7273391336202621}]}, {"text": "We report F1 scores for all three role labeling tasks.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.999474823474884}, {"text": "role labeling tasks", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.852576474348704}]}, {"text": "We assure equivalent treatment of all three SRL frameworks in mateplus and train the systems only on the given training data without any frameworkspecific information.", "labels": [], "entities": [{"text": "SRL", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9004859328269958}]}, {"text": "Specifically, we do not exploit constraints on predicate senses for PB in mateplus (i.e., selecting sense.1 as default sense), nor constraints for licensed roles (or role sets) fora given sense (i.e., encoding the FN lexicon).", "labels": [], "entities": []}, {"text": "Thus, mateplus learns predicate senses and role sets only from training instances.", "labels": [], "entities": []}, {"text": "Experiment results for the different SRL frameworks are summarized in.", "labels": [], "entities": [{"text": "SRL", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9611047506332397}]}, {"text": "Below, we discuss the results for the different settings.", "labels": [], "entities": []}, {"text": "#BL: for role labeling with oracle senses (ai+ac), PB performs best, VN is around 5 percentage points (pp.) lower, and FN again 5 pp. lower.", "labels": [], "entities": [{"text": "BL", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9954813718795776}, {"text": "role labeling", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.8096550703048706}, {"text": "VN", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9634319543838501}, {"text": "FN", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9923572540283203}]}, {"text": "With predicate sense prediction (pd+ai+ac), performance only slightly decreases for VN and PB, while FN suffers strongly: F1 is 17 pp. lower than for VN, despite the fact that its predicate labeling F1 is similar to PB and higher than VN.", "labels": [], "entities": [{"text": "predicate sense prediction", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.7297409971555074}, {"text": "FN", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9810541868209839}, {"text": "F1", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.995751142501831}]}, {"text": "This indicates that generalization across senses works much better for VN and PB roles.", "labels": [], "entities": []}, {"text": "By contrast, FN, with its sense-dependent role labels, is lacking generalization capacity, and thus suffers from data sparsity.", "labels": [], "entities": []}, {"text": "Both #FB training sets contain \u2248 17,000 predicate instances.", "labels": [], "entities": [{"text": "FB training sets", "start_pos": 6, "end_pos": 22, "type": "DATASET", "confidence": 0.7347580293814341}]}, {"text": "There is no additional labeled training data for VN.", "labels": [], "entities": [{"text": "VN", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.5653050541877747}]}, {"text": "https://github.com/microth/mateplus/ tree/master/featuresets/ger 5 Given only small differences in mateplus performance when using word embeddings, we report results without them.", "labels": [], "entities": []}, {"text": "Significance is computed using approximation randomization, i.e., SIGF: F1 scores for predicate sense and role labeling on the SR3de test set; pd: predicate sense labeling; pd+ai+ac: sense and role labeling (cf. official CoNLL scores); ai+ac: role labeling with oracle predicate sense.", "labels": [], "entities": [{"text": "SIGF", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.6521443724632263}, {"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.987203061580658}, {"text": "predicate sense and role labeling", "start_pos": 86, "end_pos": 119, "type": "TASK", "confidence": 0.6106344699859619}, {"text": "SR3de test set", "start_pos": 127, "end_pos": 141, "type": "DATASET", "confidence": 0.9842812418937683}, {"text": "predicate sense labeling", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.6286856134732565}]}, {"text": "We report statistical significance of role labeling F1 with expanded data #EX to the respective #BL ( * : p < 0.05; ** : p < 0.01).", "labels": [], "entities": [{"text": "role labeling", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.750594973564148}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.8495619893074036}, {"text": "EX", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9640256762504578}, {"text": "BL", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9897627830505371}]}], "tableCaptions": [{"text": " Table 1: Data statistics for SR3de", "labels": [], "entities": [{"text": "SR3de", "start_pos": 30, "end_pos": 35, "type": "TASK", "confidence": 0.6257126331329346}]}, {"text": " Table 2: F1 scores for predicate sense and role la- beling on the SR3de test set; pd: predicate sense  labeling; pd+ai+ac: sense and role labeling (cf.  official CoNLL scores); ai+ac: role labeling with  oracle predicate sense. We report statistical signifi- cance of role labeling F1 with expanded data #EX  to the respective #BL ( * : p < 0.05; ** : p < 0.01).", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9988971948623657}, {"text": "SR3de test set", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.9807731111844381}, {"text": "predicate sense  labeling", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.6565443277359009}, {"text": "role labeling", "start_pos": 269, "end_pos": 282, "type": "TASK", "confidence": 0.6996701508760452}, {"text": "BL", "start_pos": 329, "end_pos": 331, "type": "METRIC", "confidence": 0.9791914820671082}]}]}