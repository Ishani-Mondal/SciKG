{"title": [{"text": "Evaluation of Automatically Generated Pronoun Reference Questions", "labels": [], "entities": [{"text": "Automatically Generated Pronoun Reference", "start_pos": 14, "end_pos": 55, "type": "TASK", "confidence": 0.5520078614354134}]}], "abstractContent": [{"text": "This study provides a detailed analysis of evaluation of English pronoun reference questions which are created automatically by machine.", "labels": [], "entities": []}, {"text": "Pronoun reference questions are multiple choice questions that ask test takers to choose an antecedent of a target pronoun in a reading passage from four options.", "labels": [], "entities": []}, {"text": "The evaluation was performed from two perspectives: the perspective of English teachers and that of English learners.", "labels": [], "entities": []}, {"text": "Item analysis suggests that machine-generated questions achieve comparable quality with human-made questions.", "labels": [], "entities": []}, {"text": "Correlation analysis revealed a strong correlation between the scores of machine-generated questions and that of human-made questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Asking questions has been widely used as a method to assess the effectiveness of teaching and learning activities.", "labels": [], "entities": []}, {"text": "By asking questions, teachers can get feedback whether students understand about the teaching materials.", "labels": [], "entities": []}, {"text": "In this context, creating questions becomes an important task in teaching and learning activities.", "labels": [], "entities": []}, {"text": "Questions are usually made by human experts, which demands manual efforts; thus it is time-consuming and expensive.", "labels": [], "entities": []}, {"text": "Automatic question generation is a solution to solve this problem.", "labels": [], "entities": [{"text": "question generation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7317583411931992}]}, {"text": "Several past studies worked on various kinds of automatic question generation.", "labels": [], "entities": [{"text": "question generation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7396400570869446}]}, {"text": "worked on the automatic question generation for the purpose of reading comprehension assessment and practice.", "labels": [], "entities": [{"text": "automatic question generation", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.6351142823696136}, {"text": "reading comprehension assessment", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.7415777643521627}]}, {"text": "worked on the automatic generation of trigger questions (directive and facilitative) for supporting writing activities.", "labels": [], "entities": []}, {"text": "worked on the automatic generation of all possible questions given a topic of interest.", "labels": [], "entities": [{"text": "automatic generation of all possible questions", "start_pos": 14, "end_pos": 60, "type": "TASK", "confidence": 0.7890268017848333}]}, {"text": "worked on the automatic generation of questions about an image.", "labels": [], "entities": [{"text": "automatic generation of questions about an image", "start_pos": 14, "end_pos": 62, "type": "TASK", "confidence": 0.8402648568153381}]}, {"text": "Research on automatic question generation has been active, yet there are few studies which elaborate the detailed evaluation process and in-depth analysis of the machine-generated questions.", "labels": [], "entities": [{"text": "automatic question generation", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6385912895202637}]}, {"text": "QG-STEC 2010 is the first shared task about question generation that comprises two subtasks: question generation from paragraphs and question generation from sentences (.", "labels": [], "entities": [{"text": "QG-STEC 2010", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9284763932228088}, {"text": "question generation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8041750192642212}, {"text": "question generation from paragraphs", "start_pos": 93, "end_pos": 128, "type": "TASK", "confidence": 0.7990318536758423}, {"text": "question generation from sentences", "start_pos": 133, "end_pos": 167, "type": "TASK", "confidence": 0.8198555633425713}]}, {"text": "Human judges were utilised to evaluate question quality by considering five criteria: syntactic correctness and fluency, question type, relevance, ambiguity, and variety.", "labels": [], "entities": [{"text": "variety", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9589037299156189}]}, {"text": "evaluated their trigger question generation system for academic writing support by comparing machine-generated trigger questions to human-made trigger questions based on five aspects: clarity, correctness, relevance, usefulness for learning concepts, and usefulness to improve the literature review documents.", "labels": [], "entities": [{"text": "trigger question generation", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6417558888594309}, {"text": "clarity", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.9945803284645081}]}, {"text": "Twentythree students were instructed to write essays and then to assess the trigger questions if these questions could improve their essays.", "labels": [], "entities": []}, {"text": "Because the machine-generated trigger questions were created based on the collected student essays, their analysis showed that they were effective only for the collected student essays while the human-made trigger questions were effective for general essays as well as the collected essays.", "labels": [], "entities": []}, {"text": "Zhang and VanLehn (2016) employed students to rate machine-generated questions and humanmade questions based on relevance, fluency, ambiguity, pedagogy and depth.", "labels": [], "entities": []}, {"text": "evaluated their question generation system by judging the questions on three metrics: grammatical correctness, answer existence and inference steps.", "labels": [], "entities": [{"text": "question generation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7431197166442871}]}, {"text": "On John Black Tuley's land, on Meshach Creek, 6 miles northeast of Tompkinsville, two human skeletons were found in a small opening, which has since been known as the Bone Cave.", "labels": [], "entities": [{"text": "Meshach Creek", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9175620377063751}]}, {"text": "It is a room not over 10 feet across at any part, in a limestone conglomerate, and maybe of quite recent origin.", "labels": [], "entities": []}, {"text": "Being inconvenient of access, it is not in a position for residence purposes.", "labels": [], "entities": []}, {"text": "The skeletons were probably those of Indian hunters.", "labels": [], "entities": []}, {"text": "They were less than 2 feet below the surface.", "labels": [], "entities": []}, {"text": "The material in which the little cave is formed will crumble easily in cold weather, being rather wet from the soil water soaking through the hill above it.", "labels": [], "entities": []}, {"text": "The word \"they\" in the passage refers to (A) skeletons (B) feet (C) purposes (D) hunters utilised English teachers and students to evaluate their question generation system.", "labels": [], "entities": [{"text": "question generation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7400501370429993}]}, {"text": "English teachers were asked to distinguish machine-generated questions from humanmade questions apart.", "labels": [], "entities": []}, {"text": "The English teachers also judged the questions on their usability in areal test and their difficulties using five scale rating.", "labels": [], "entities": []}, {"text": "They also received suggestions to improve the questions from the English teachers.", "labels": [], "entities": []}, {"text": "Furthermore, students were asked to answer the machine-generated questions and human-made questions; their answers were analysed using item analysis and the analysis based on Neural Test Theory.", "labels": [], "entities": []}, {"text": "To sum up, the evaluation of automatic question generation systems in the past research was performed by utilising human judges and students.", "labels": [], "entities": [{"text": "question generation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7107378989458084}]}, {"text": "In this study, we provide detailed evaluation experiments and analysis of automatically generated pronoun reference questions.", "labels": [], "entities": []}, {"text": "Pronoun reference questions consist of four components, i.e. a reading passage, a target pronoun, a correct answer, and three distractors as illustrated in.", "labels": [], "entities": []}, {"text": "We focus on pronoun reference questions because they measure the test taker's ability to resolve pronoun in reading passages.", "labels": [], "entities": []}, {"text": "We argue that resolving pronoun is an important skill for reading comprehension.", "labels": [], "entities": []}, {"text": "The evaluation target of this study is the English pronoun reference questions generated by our system (.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no other system for generating pronoun reference questions.", "labels": [], "entities": []}, {"text": "The system generates questions from human-written texts by performing a sentence splitting technique on nonrestrictive relative clauses.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7375108450651169}]}, {"text": "The details of the question generation system are explained in Section 2.", "labels": [], "entities": [{"text": "question generation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8315876126289368}]}, {"text": "We evaluate the questions from two different perspectives following.", "labels": [], "entities": []}, {"text": "The first perspective is from English teachers.", "labels": [], "entities": []}, {"text": "We argue that English teachers have the ability to differentiate the good questions from the bad ones because creating questions is one of the teacher's responsibilities in the classroom; thus asking English teachers to judge the quality of machine-generated questions is reasonable.", "labels": [], "entities": []}, {"text": "The second perspective is from English learners.", "labels": [], "entities": []}, {"text": "Good questions can discriminate high proficiency learners from low proficiency learners.", "labels": [], "entities": []}, {"text": "English learners were instructed to answer the questions and their responses were used for analysing the characteristics of the questions.", "labels": [], "entities": []}, {"text": "In what follows, we explain the automatic question generation system to be evaluated (Section 2), followed by the elaboration of the evaluation from the English teacher perspective (Section 3) and the English learner perspective (Section 4).", "labels": [], "entities": [{"text": "question generation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7144158780574799}]}, {"text": "We conclude the evaluation results and point out the possible future research direction (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "We prepared three sets of questions each of which contains ten machine-generated questions (MGQs) and ten human-made questions (HMQs), in total 20 questions.", "labels": [], "entities": []}, {"text": "These 30 HMQs were randomly selected from TOEFL preparation books while these 30 MGQs were randomly selected from the set of MGQs which were judged acceptable on the majority principle in the evaluation by the English teachers as described in Section 3.", "labels": [], "entities": [{"text": "TOEFL preparation books", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.9116235772768656}]}, {"text": "The question sets were created so that the difference of the average of question difficulty across the question sets was minimised.", "labels": [], "entities": []}, {"text": "The balance of question difficulty among three groups, and between MGQs and HMQs is important because we calculate the student-wise score correlation between scores from MGQs and HMQs as explained later in 4.2.", "labels": [], "entities": []}, {"text": "To balance question difficulty among the question sets, we utilised the reading passage difficulty.", "labels": [], "entities": []}, {"text": "A question is considered difficult if its readDr.1 M.", "labels": [], "entities": [{"text": "readDr.1 M", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.719508558511734}]}, {"text": "Aurel9 Stein9, principal2 of1 the1 Oriental7 College1 at1 Lahore9, has1 now1 ready1 for1 publication4 the1 first1 volume2 of1 his1 critical3 edition4 of1 the1 Rajatarangini9, or1 Chronicles8 of1 the1 Kings1 of1 Kashmir9, upon1 which1 he1 has1 been1 engaged3 for1 some1 years1.", "labels": [], "entities": [{"text": "Oriental7 College1 at1 Lahore9", "start_pos": 35, "end_pos": 65, "type": "DATASET", "confidence": 0.7632041871547699}, {"text": "Rajatarangini9", "start_pos": 159, "end_pos": 173, "type": "DATASET", "confidence": 0.9166012406349182}]}, {"text": "This1 work1 is1 of1 special1 interest1 as1 being1 almost1 the1 sole4 example1 of1 historical2 literature2 in1 Sanskrit9.", "labels": [], "entities": []}, {"text": "It1 was1 written2 by1 the1 poet2 Kalhana9 in1 the1 middle1 of1 the1 twelfth1 century1.", "labels": [], "entities": []}, {"text": "ing passage is difficult and vice versa.", "labels": [], "entities": []}, {"text": "The reading passage difficulty is calculated based on the word difficulty in the passages.", "labels": [], "entities": []}, {"text": "We employed JACET8000 (), a list of 8,000 English words divided into eight levels of word difficulty based on their word frequency.", "labels": [], "entities": []}, {"text": "Level 1 is the most frequent (i.e. the easiest) while level 8 is least frequent (i.e. the most difficult).", "labels": [], "entities": []}, {"text": "Words that do not appear in the list are considered even less frequent than level 8; thus they are considered to be level 9.", "labels": [], "entities": []}, {"text": "To obtain the reading passage difficulty, we assigned a JACET8000 word difficulty level to every word in the reading passage as illustrated in and calculated the average of the difficulty levels.", "labels": [], "entities": [{"text": "JACET8000 word difficulty level", "start_pos": 56, "end_pos": 87, "type": "METRIC", "confidence": 0.6397797986865044}]}, {"text": "The average of reading passage difficulty for each question set is presented in.", "labels": [], "entities": []}, {"text": "Many metrics to measure text readability have been proposed in the past, such as Flesch-Kincaid grade level (, Flesch-Kincaid reading ease ( and Dale-Chall readability formula.", "labels": [], "entities": [{"text": "Flesch-Kincaid grade level", "start_pos": 81, "end_pos": 107, "type": "METRIC", "confidence": 0.9138854146003723}, {"text": "Flesch-Kincaid reading ease", "start_pos": 111, "end_pos": 138, "type": "METRIC", "confidence": 0.8427923321723938}]}, {"text": "The first two calculate text difficulty with respect to the number of sentences, words and syllables in the text.", "labels": [], "entities": []}, {"text": "The third one takes into account the difficulty of each word as well.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9663968682289124}]}, {"text": "the mean values of these metrics for each question set and generation mode, i.e. machine-generated vs. human-made.", "labels": [], "entities": []}, {"text": "Overall, the difficulty of reading passages in every question set is well balanced against every metric.", "labels": [], "entities": []}, {"text": "Eighty-one Japanese university students (57 first year and 24 second year students) were recruited and divided into three groups, 27 students for each group, considering their TOEIC scores; we did our best to minimise the difference of the score distribution and the mean of the scores across these three groups.", "labels": [], "entities": [{"text": "TOEIC", "start_pos": 176, "end_pos": 181, "type": "METRIC", "confidence": 0.8158424496650696}]}, {"text": "Each student group was assigned a different question set and instructed to finish the assigned question set within 30 minutes.", "labels": [], "entities": []}, {"text": "The evaluation from the English learner perspective was conducted to evaluate the behaviour of machine-generated questions in measuring test taker's proficiency.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1.  The evaluators leave the problematic components  columns empty for acceptable quality questions.  The evaluators may optionally give comments on  problematic components or suggestions to im- prove the question quality.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of pairwise disagreement", "labels": [], "entities": []}, {"text": " Table 3: Distribution of rating", "labels": [], "entities": []}, {"text": " Table 4: Majority quality scores of 60 questions", "labels": [], "entities": [{"text": "Majority quality scores", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.7509980400403341}]}, {"text": " Table 5: Average quality scores of 60 questions", "labels": [], "entities": [{"text": "Average quality scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.9133984446525574}]}, {"text": " Table 6: Evaluator's comments with frequency", "labels": [], "entities": [{"text": "Evaluator", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9413624405860901}]}, {"text": " Table 7: Mean of reading passage difficulty", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9937177896499634}]}, {"text": " Table 9: Item difficulty of MGQs and HMQs", "labels": [], "entities": [{"text": "Item difficulty", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.89250448346138}, {"text": "HMQs", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.46791577339172363}]}, {"text": " Table 10: Peason correlation coefficients between  test taker's scores", "labels": [], "entities": []}]}