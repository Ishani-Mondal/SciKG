{"title": [{"text": "Character-based Neural Embeddings for Tweet Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks.", "labels": [], "entities": [{"text": "tweet clustering", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.8627402484416962}]}, {"text": "The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content.", "labels": [], "entities": []}, {"text": "Our evaluation results and code are available on-line 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Our use case scenario, as part of the InVID project 2 , originates from the needs of professional journalists responsible for reporting breaking news in a timely manner.", "labels": [], "entities": [{"text": "InVID project 2", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.8768919308980306}]}, {"text": "News often appear on social media exclusively or right before they appear in the traditional news media.", "labels": [], "entities": []}, {"text": "Social media is also responsible for the rapid propagation of inaccurate or incomplete information (rumors).", "labels": [], "entities": [{"text": "rapid propagation of inaccurate or incomplete information (rumors)", "start_pos": 41, "end_pos": 107, "type": "TASK", "confidence": 0.6765258938074112}]}, {"text": "Therefore, it is important to provide efficient tools to enable journalists rapidly detect breaking news in social media streams (.", "labels": [], "entities": []}, {"text": "The SNOW 2014 Data Challenge provided the task of extracting newsworthy topics from Twitter.", "labels": [], "entities": [{"text": "SNOW 2014 Data Challenge", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7695328965783119}, {"text": "extracting newsworthy topics from Twitter", "start_pos": 50, "end_pos": 91, "type": "TASK", "confidence": 0.8061214089393616}]}, {"text": "The results of the challenge confirmed that the task is ambitious: The best result was 0.4 F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9948470592498779}]}, {"text": "Breaking-news detection involves 3 subtasks: selection, clustering, and ranking of tweets.", "labels": [], "entities": [{"text": "Breaking-news detection", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8041655421257019}]}, {"text": "In this paper, we address the task of tweet clustering as one of the pivotal subtasks required to enable effective breaking news detection from Twitter.", "labels": [], "entities": [{"text": "tweet clustering", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.84952312707901}, {"text": "breaking news detection from Twitter", "start_pos": 115, "end_pos": 151, "type": "TASK", "confidence": 0.8250089526176453}]}, {"text": "Traditional approaches to clustering textual documents involve construction of a documentterm matrix, which represents each document as a bag-of-words.", "labels": [], "entities": []}, {"text": "These approaches also require language-specific sentence and word tokenization.", "labels": [], "entities": [{"text": "language-specific sentence and word tokenization", "start_pos": 30, "end_pos": 78, "type": "TASK", "confidence": 0.5784966588020325}]}, {"text": "Word-based approaches fall short when applied to social media data, e.g., Twitter, where a lot of infrequent or misspelled words occur within very short documents.", "labels": [], "entities": []}, {"text": "Hence, the document representation matrix becomes increasingly sparse.", "labels": [], "entities": []}, {"text": "One way to overcome sparseness in a tweetterm matrix is to consider only the terms that appear frequently across the collection and drop all the infrequent terms.", "labels": [], "entities": []}, {"text": "This procedure effectively removes a considerable amount of information content.", "labels": [], "entities": []}, {"text": "As a result, all tweets that do not contain any of the frequent terms receive a null-vector representation.", "labels": [], "entities": []}, {"text": "These tweets are further ignored by the model and cannot influence clustering outcomes in the subsequent time intervals, where the frequency distribution may change, which hinders the detection of emerging topics.", "labels": [], "entities": []}, {"text": "Artificial neural networks (ANNs) allow to generate dense vector representation (embeddings), which can be efficiently generated on the word-as well as character levels (dos.", "labels": [], "entities": []}, {"text": "The main advantage of the character-based approaches is their language-independence, since they do not require any language-specific parsing.", "labels": [], "entities": []}, {"text": "The major contribution of our work is the evaluation of the character-based neural embeddings on the tweet clustering task.", "labels": [], "entities": [{"text": "tweet clustering task", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.8610554337501526}]}, {"text": "We show how to employ character-based tweet embeddings for the task of tweet clustering and demonstrate in the experimental evaluation that the proposed approach significantly outperforms the current state-of-theart in tweet clustering for breaking news detection.", "labels": [], "entities": [{"text": "tweet clustering", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.8145334422588348}, {"text": "tweet clustering", "start_pos": 219, "end_pos": 235, "type": "TASK", "confidence": 0.7234293520450592}, {"text": "breaking news detection", "start_pos": 240, "end_pos": 263, "type": "TASK", "confidence": 0.7173429727554321}]}, {"text": "The remaining of this paper is structured as follows: Section 2 provides an overview of the related work; we describe the setup of an extensive evaluation in Section 3; report and discuss the results in Sections 4 and 5, respectively; conclu-sion (Section 6) summarizes our findings and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the SNOW 2014 test dataset () in our evaluation.", "labels": [], "entities": [{"text": "SNOW 2014 test dataset", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.9392391294240952}]}, {"text": "It contains the IDs of about 1 million tweets produced within 24 hours.", "labels": [], "entities": []}, {"text": "We retrieved 845,626 tweets from the Twitter API, since other tweets had already been deleted from the platform.", "labels": [], "entities": []}, {"text": "The preprocessing procedure: remove RT prefixes, urls and user mentions, bring all characters to lowercase and separate punctuation with spaces (the later is necessary only for the word-level baseline).", "labels": [], "entities": [{"text": "RT prefixes", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.822105348110199}]}, {"text": "The dataset is further separated into 5 subsets corresponding to the 1-hour time intervals (18:00, 22:00, 23:15, 01:00 and 01:30) that are annotated with the list of breaking news topics.", "labels": [], "entities": []}, {"text": "In total, we have 48,399 tweets for clustering evaluation; the majority of them (42,758 tweets) are in English.", "labels": [], "entities": [{"text": "clustering evaluation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.905005544424057}]}, {"text": "The dataset comes with the list of the breaking news topics.", "labels": [], "entities": []}, {"text": "These topics were manually selected by the independent evaluators from the topic pool collected from all challenge participants (external topics).", "labels": [], "entities": []}, {"text": "The list of topics contains 70 breaking news headlines extracted from tweets (e.g., \"The new, full Godzilla trailer has roared online\").", "labels": [], "entities": []}, {"text": "Each topic is annotated with a few (at most 4) tweet IDs, which is not sufficient for an adequate evaluation of a tweet clustering algorithm.", "labels": [], "entities": [{"text": "tweet clustering", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.6958535611629486}]}, {"text": "We enrich the topic annotations by collecting larger tweet clusters using fuzzy string matching 3 for each of the topic labels.", "labels": [], "entities": []}, {"text": "Fuzzy string matching uses the Levenstein (edit) distance between the two input strings as the measure of similarity.", "labels": [], "entities": [{"text": "Fuzzy string matching", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8638651569684347}, {"text": "Levenstein (edit) distance", "start_pos": 31, "end_pos": 57, "type": "METRIC", "confidence": 0.890351140499115}]}, {"text": "Levenstein distance corresponds to the minimum number of character edits (insertions, deletions, or substitutions) required to transform one string into the other.", "labels": [], "entities": [{"text": "Levenstein distance", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8698798418045044}]}, {"text": "We choose only the tweets for which the similarity ratio with the topic string is greater than 0.9 threshold.", "labels": [], "entities": [{"text": "similarity ratio", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.9728515446186066}]}, {"text": "A sample tweet cluster produced with the fuzzy string matching for the topic \"Justin Trudeau apologizes for Ukraine joke\": \u2022 Justin Trudeau apologizes for Ukraine joke: Justin Trudeau said he's spoken the head...", "labels": [], "entities": []}, {"text": "\u2022 Justin Trudeau apologizes for Ukraine comments http://t.co/7ImWTRONXt \u2022 Justin Trudeau apologizes for Ukraine hockey joke #cdnpoli In total, we matched 2,585 tweets to 132 clusters using this approach.", "labels": [], "entities": []}, {"text": "The resulting tweet clusters represent the ground-truth topics within different time intervals.", "labels": [], "entities": []}, {"text": "The cluster size varies from 1 to 361 tweets with an average of 20 tweets per cluster (median: 6.5).", "labels": [], "entities": []}, {"text": "This simple procedure allows us to automatically generate high-quality partial labeling.", "labels": [], "entities": []}, {"text": "We further use this topic assignment as the groundtruth class labels to automatically evaluate different flat clustering partitions.", "labels": [], "entities": []}, {"text": "We evaluate the clustering results using the standard metrics for extrinsic clustering evaluation: homogeneity, completeness, V-Measure, Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) (.", "labels": [], "entities": [{"text": "V-Measure", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9250924587249756}, {"text": "Adjusted Rand Index (ARI)", "start_pos": 137, "end_pos": 162, "type": "METRIC", "confidence": 0.8358159363269806}, {"text": "Adjusted Mutual Information (AMI)", "start_pos": 167, "end_pos": 200, "type": "METRIC", "confidence": 0.6864161541064581}]}, {"text": "All metrics return a score on the range [0; 1] for the pair of sets that contain ground truth and cluster labels as input.", "labels": [], "entities": []}, {"text": "The higher the score the more similar the two clusterings are.", "labels": [], "entities": []}, {"text": "The Homogeneity score represents the measure for purity of the produced clusters.", "labels": [], "entities": [{"text": "Homogeneity score", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.7075567245483398}]}, {"text": "It penalizes clustering, where members of different classes get clustered together.", "labels": [], "entities": [{"text": "clustering", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.6280112862586975}]}, {"text": "Thus, the best homogeneity scores are always at the bottom of the dendrogram, i.e., at the level of the leaves, where each document belongs to its own cluster.", "labels": [], "entities": []}, {"text": "Completeness, on the contrary, favors larger clusters and reduces the score if the members of the same class are split into different clusters.", "labels": [], "entities": [{"text": "Completeness", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9366494417190552}]}, {"text": "Therefore, the top of the dendrogram, where all the documents reside in a single cluster always achieves the maximum completeness score.", "labels": [], "entities": []}, {"text": "V-Measure is designed to balance out the two extremes of homogeneity and completeness.", "labels": [], "entities": []}, {"text": "It is the harmonic mean of the two and corresponds to the Normalized Mutual Information (NMI) score.", "labels": [], "entities": [{"text": "Mutual Information (NMI) score", "start_pos": 69, "end_pos": 99, "type": "METRIC", "confidence": 0.7366716613372167}]}, {"text": "AMI score is an extension of NMI adjusted for chance.", "labels": [], "entities": [{"text": "AMI score", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8979881703853607}, {"text": "NMI", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.7399801015853882}]}, {"text": "The more clusters are considered the more chance the labelings correlate.", "labels": [], "entities": []}, {"text": "AMI allows us to compare the clustering performance across different time intervals since it normalizes the score by the number of labeled clusters in each interval.", "labels": [], "entities": [{"text": "AMI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.889362096786499}]}, {"text": "Finally, ARI is an alternative way to assess the agreement between two clusterings.", "labels": [], "entities": [{"text": "ARI", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9223219156265259}]}, {"text": "It counts all pairs clustered together or separated in different clusters.", "labels": [], "entities": []}, {"text": "ARI also accounts for the chance of an overlap in a random label assignment.", "labels": [], "entities": [{"text": "ARI", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7307530045509338}]}, {"text": "Our partial labeling covers a small subset of the data and by design provides the clusters with the high degree of string overlap with the annotated topics.", "labels": [], "entities": []}, {"text": "Therefore, we extend the clustering evaluation to the rest of the dataset to evaluate whether the models can uncover less straight-forward semantic similarities in tweets.", "labels": [], "entities": []}, {"text": "We select the results for manual evaluation motivated by the cluster label (headline) selection task.", "labels": [], "entities": []}, {"text": "The next step in the breaking news detection pipeline after the clustering task is headline selection (cluster labeling task).", "labels": [], "entities": [{"text": "breaking news detection", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.6406880021095276}, {"text": "headline selection", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7620391845703125}]}, {"text": "The most common approach to label a cluster of tweets is to select a single tweet as a representative member for the whole cluster ().", "labels": [], "entities": []}, {"text": "We decided to test this assumption and manually check how many clusters loose their semantics when represented with a single tweet.", "labels": [], "entities": []}, {"text": "Headline selection motivates the coherence assessment of the produced clusters since the clusters discarded at this stage will never make it to the final results.", "labels": [], "entities": []}, {"text": "To explore coherence of the produced clusters we pick several tweets in each cluster and check whether they are semantically similar.", "labels": [], "entities": []}, {"text": "The tweet selected as a headline (cluster label) can be the first published tweet as in First Story Detection (FSD) task, also used in Ifrim et al.", "labels": [], "entities": [{"text": "First Story Detection (FSD) task", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.7474218692098346}]}, {"text": "Alternative approaches include selection of the most recent tweet published on the topic, or the tweet that is semantically most similar to all other tweets in the cluster, i.e., the tweet closest to the centroid of the cluster (medoid-tweets).", "labels": [], "entities": []}, {"text": "Therefore, we sample 5 tweets from each cluster: the first published tweet, the most recent tweet and three medoid-tweets.", "labels": [], "entities": []}, {"text": "We setup a manual evaluation task as follows: 1.", "labels": [], "entities": []}, {"text": "Take the top 20 largest clusters sorted by the number of tweets that belong to the cluster.", "labels": [], "entities": []}, {"text": "2. For each cluster: (a) Take the first and the last published tweet (tweets are previously sorted by the publication date).", "labels": [], "entities": []}, {"text": "(b) Take three medoid-tweets, i.e., the tweets that appear closest to the centroid of the cluster.", "labels": [], "entities": []}, {"text": "(c) Add the 5 tweets to the set associated with the cluster (removing exact duplicate tweets) 3.", "labels": [], "entities": []}, {"text": "For all clusters, where the set of selected tweets contains at least two unique tweets: 4 human evaluators independently assess the coherence of each cluster.", "labels": [], "entities": []}, {"text": "According to the evaluation setup each model produced 20 top-clusters for each of the 5 intervals, i.e., 20 \u00d7 5 = 100 clusters per model.", "labels": [], "entities": []}, {"text": "We manually evaluate only the clusters that contain more than 1 distinct representative tweet (Clusters>1).", "labels": [], "entities": []}, {"text": "All other clusters, i.e., the ones for which all 5 selected tweets are identical (Clusters=1), are considered correct by default.", "labels": [], "entities": [{"text": "Clusters", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9336562156677246}]}, {"text": "Results for all 5 intervals were evaluated together in a single pool and the models were anonymized to avoid biases.", "labels": [], "entities": []}, {"text": "Each evaluator independently assigned a single score to each cluster: \u2022 Correct -all tweets report the same news; \u2022 Partial -some tweets are not related; \u2022 Incorrect -all tweets are not related.", "labels": [], "entities": []}, {"text": "Partial and Incorrect labels reflect different types of clustering errors.", "labels": [], "entities": []}, {"text": "Partial error is less severe indicating that the tweets of the cluster are semantically similar, but they report different news (events) and should be split into several clusters.", "labels": [], "entities": [{"text": "error", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.6884285807609558}]}, {"text": "Incorrect clusters indicate a random collection of tweets with no semantic similarities.", "labels": [], "entities": []}, {"text": "summarizes the results of our evaluation using the ground-truth partial labeling.", "labels": [], "entities": []}, {"text": "The scores highlighted with the bold font indicate the best result among the two competing approaches for the same subset of tweets corresponding to the respective time interval.", "labels": [], "entities": []}, {"text": "Tweet2Vec exhibits better clustering performance comparing to the baseline according to the majority of the evaluation metrics in all the intervals.", "labels": [], "entities": []}, {"text": "In all cases Tweet2Vec model wins in terms of Homogeneity score and TweetTerm wins in Completeness.", "labels": [], "entities": [{"text": "Homogeneity score", "start_pos": 46, "end_pos": 63, "type": "METRIC", "confidence": 0.9136921465396881}, {"text": "Completeness", "start_pos": 86, "end_pos": 98, "type": "METRIC", "confidence": 0.9877477884292603}]}, {"text": "This result shows that Tweet2Vec is better at separating tweets that are not similar enough than the baseline model.", "labels": [], "entities": []}, {"text": "Tweet2Vec fails only once to perfectly separate the ground-truth clusters (18:00 interval).", "labels": [], "entities": [{"text": "Tweet2Vec", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8472068905830383}]}, {"text": "This result shows that Tweet2Vec is able to replicate the results of the fuzzy string matching algorithm that was used to generate the ground-truth labeling.", "labels": [], "entities": []}, {"text": "Results of the manual cluster evaluation by four independent evaluators are summarized in.", "labels": [], "entities": []}, {"text": "Bold font indicates the maximum scores achieved across the competing representation approaches.", "labels": [], "entities": []}, {"text": "show sample clusters produced by both models alongside their average score.", "labels": [], "entities": []}, {"text": "TweetTerm assigns a 0-vector representation to tweets that do not contain any of the frequent terms.", "labels": [], "entities": [{"text": "TweetTerm", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8338524103164673}]}, {"text": "Hence, all these tweets end up in a single \"garbage\" cluster.", "labels": [], "entities": []}, {"text": "Therefore, we discount the number of the expected \"garbage\" clusters (1 cluster per interval = 5 clusters) from the score count for TweetTerm.", "labels": [], "entities": []}, {"text": "Tweet2Vec model produces the largest number of perfectly homogeneous clusters for which all 5 selected tweets are identical (see column Clusters=1).", "labels": [], "entities": []}, {"text": "The percentage of correct results among the manually evaluated clusters is higher for the TweetTerm model, but the number of errors (Incorrect) is higher as well.", "labels": [], "entities": [{"text": "Incorrect)", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.9638736248016357}]}, {"text": "Tweet2Vec produced the highest total % of correct clusters due to the larger proportion of detected clusters that contain identical tweets (Clusters=1).", "labels": [], "entities": []}, {"text": "Tweet2Vec also produced the least number of incorrect clusters: at most 2 incorrect clusters per 100 clusters (Precision: 0.98).", "labels": [], "entities": [{"text": "Tweet2Vec", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9206739068031311}, {"text": "Precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9959626793861389}]}, {"text": "The results of Tweet2Vec on the multilingual dataset are lower than on the English-language tweets.", "labels": [], "entities": []}, {"text": "However, we do not have alternative results to compare since the baseline approach is not language-independent and requires additional functionality (word-level tokenizers) to handle tweets in other languages, e.g., Arabic or Chinese.", "labels": [], "entities": []}, {"text": "We provide this evaluation results to demonstrate that Tweet2Vec overcomes this limitation and is able to cluster tweets in different languages.", "labels": [], "entities": []}, {"text": "In particular, we obtained correct clusters of Russian and Arabic tweets.", "labels": [], "entities": []}, {"text": "We observed that leaving the urls does not significantly affect clustering performance, i.e., the model tolerates noise.", "labels": [], "entities": []}, {"text": "However, replacement of the urls and user mentions with placeholders as in generates syntactic patterns in text, such as @user @user @user, which causes semantically unrelated tweets appear within the same cluster.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of clustering evaluation on the English-language dataset", "labels": [], "entities": [{"text": "clustering", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.9630564451217651}, {"text": "English-language dataset", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.726300984621048}]}, {"text": " Table 2: Results of manual cluster evaluation. Note: the last row shows results on a different dataset and  can not be directly compared with the other models.", "labels": [], "entities": []}]}