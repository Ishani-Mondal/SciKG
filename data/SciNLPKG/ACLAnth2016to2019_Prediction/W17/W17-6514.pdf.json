{"title": [{"text": "Assessing the Annotation Consistency of the Universal Dependencies Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "A fundamental issue in annotation efforts is to ensure that the same phenomena within and across corpora are annotated consistently.", "labels": [], "entities": []}, {"text": "To date, there has not been a clear and obvious way to ensure annotation consistency of dependency corpora.", "labels": [], "entities": []}, {"text": "Here, we revisit the method of Boyd et al.", "labels": [], "entities": []}, {"text": "(2008) to flag inconsistencies in dependency corpora, and evaluate it on three languages with varying degrees of morphology (English, French, and Finnish UD v2).", "labels": [], "entities": []}, {"text": "We show that the method is very efficient in finding errors in the annotations.", "labels": [], "entities": []}, {"text": "We also build an annotation tool, which we will make available, that helps to streamline the manual annotation required by the method.", "labels": [], "entities": []}], "introductionContent": [{"text": "In every annotation effort, it is necessary to make sure that the annotation guidelines are followed, and crucially that similar phenomena do receive a consistent analysis within and across corpora.", "labels": [], "entities": []}, {"text": "Given the recent success of the Universal Dependencies (UD) project 1 which aims at building cross-linguistically consistent treebanks for many languages and the rapid creation of 74 corpora for 51 languages supposedly following the UD scheme, investigating the quality of the dependency annotations and improving their consistency is, more than ever, of crucial importance.", "labels": [], "entities": [{"text": "consistency", "start_pos": 320, "end_pos": 331, "type": "METRIC", "confidence": 0.9619864225387573}]}, {"text": "While there has been a fair amount of work to automatically detect part-of-speech inconsistent annotations (i.a.,,), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De who focus on multi-word expressions in the UD v1 corpora ().", "labels": [], "entities": []}, {"text": "There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (;); PML TreeQuery); ICARUS ().", "labels": [], "entities": [{"text": "SETS", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.839970052242279}, {"text": "PML TreeQuery", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.8041603565216064}, {"text": "ICARUS", "start_pos": 145, "end_pos": 151, "type": "DATASET", "confidence": 0.6833842992782593}]}, {"text": "Statistical methods, such as the one of, are supplemented with hand-written rules.", "labels": [], "entities": []}, {"text": "While approaches based on heuristic patterns work extremely well to look forgiven constructions (e.g., clefts) or check that specific guidelines are taken into account (e.g., auxiliary dependencies should not form a chain in UD), such approaches are limited to finding what has been defined a priori.", "labels": [], "entities": []}, {"text": "In this paper, we adapt the method proposed by to flag potential dependency annotation inconsistencies, and evaluate it on three of the UD v2 corpora (English, French and Finnish).", "labels": [], "entities": [{"text": "UD v2 corpora", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.7846634785334269}]}, {"text": "The original Boyd et al. method finds pairs of words in identical context that vary in their dependency relation.", "labels": [], "entities": []}, {"text": "We show that this method works fairly well in finding annotation errors, within a given corpus.", "labels": [], "entities": []}, {"text": "We further hypothesize that using lemmas instead of word forms would improve recall in finding annotation errors, without a detrimental effect on precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.998666524887085}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9963808655738831}]}, {"text": "We show that our intuition is valid for languages that are not too morphologically-rich, like English and French, but not for Finnish.", "labels": [], "entities": []}, {"text": "We also examine whether we can extend the method by leveraging the availability of large corpora which are automatically dependencyannotated to identify more inconsistencies than when restricting ourselves only to the given manually annotated corpus.", "labels": [], "entities": []}, {"text": "We find that when based on automatic rather than manual annotation, the precision drops but not excessively so, but the gain in recall is rather moderate.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9997761845588684}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.99957674741745}]}, {"text": "To help streamline this manual validation process, we develop a visualization and annotation tool for the task, available to the UD community, with data for all UD treebanks.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 161, "end_pos": 173, "type": "DATASET", "confidence": 0.8666692972183228}]}, {"text": "Rather than a standalone tool such as ICARUS (), we provide an accessible browser-based interface.", "labels": [], "entities": [{"text": "ICARUS", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.7753475904464722}]}], "datasetContent": [{"text": "The method retrieves 266 pairs of lemmas displaying inconsistencies for English, 474 for French and 117 for Finnish, using the \"non-fringe\" heuristic (i.e., the pairs need to share context: same lemma to the left and same lemma to the right of the lemmas in the dependency pair).", "labels": [], "entities": []}, {"text": "Each pair varies in the number of inconsistent trees they are associated with.", "labels": [], "entities": []}, {"text": "But most pairs contain two trees, as can be seen in which shows the counts of pairs (y-axis) for the different numbers of trees they contain (x-axis).", "labels": [], "entities": []}, {"text": "For each language, to evaluate how many of the inconsistencies flagged are indeed annotation errors, we randomly sampled 100 of the pairs retrieved and annotated all the trees associated with these pairs, nevertheless limiting to 10 trees per dependency type.", "labels": [], "entities": []}, {"text": "In the \"non-fringe\" column, we computed how many of the 100 pairs do contain erroneous trees.", "labels": [], "entities": []}, {"text": "Thus these results indicate how precise the method is. propose an additional, more stringent heuristic of \"dependency context\".", "labels": [], "entities": []}, {"text": "This heuristic requires the word/lemma pairs to not only share the left/right context, but also the incoming relation type.", "labels": [], "entities": []}, {"text": "As we did not implement this heuristic when selecting the trees for annotation, we are able to evaluate its precision as well as its recall relative to the pairs retrieved when using only the \"non-fringe\" heuristic.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9995629191398621}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9992194175720215}]}, {"text": "Using the 100 pairs annotated in each language as a gold-standard, we calculated the precision and recall of the \"dependency context\" heuristic by examining which pairs are left when adding the further requirement of shared incoming relation to the governor.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9995861649513245}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9987645149230957}]}], "tableCaptions": [{"text": " Table 1: Size of the UD v2 English, French and  Finnish corpora.", "labels": [], "entities": []}, {"text": " Table 2: Results of the Boyd et al. method on 100 pairs in each corpus for the \"non-fringe\" and \"de- pendency context\" heuristics when using lemmas as well as for the \"non-fringe\" heuristic when using  wordforms. Recall is always reported relative to the \"non-fringe\" lemma-based method.", "labels": [], "entities": [{"text": "Recall", "start_pos": 214, "end_pos": 220, "type": "METRIC", "confidence": 0.9985969662666321}]}, {"text": " Table 3: The number of trees assessed as erro- neous (incorrect relation type or incorrect struc- ture), and the number of trees verified to be cor- rect.", "labels": [], "entities": []}, {"text": " Table 4: Results using parsebank data and lem- mas: the number of trees that were manually as- sessed as annotation errors, the precision of the  method, and the percentage of the erroneous trees  which would be also found based on the treebank  itself.", "labels": [], "entities": [{"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9994264841079712}]}]}