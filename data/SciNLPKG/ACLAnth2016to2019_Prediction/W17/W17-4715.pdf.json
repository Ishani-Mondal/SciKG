{"title": [{"text": "Copied Monolingual Data Improves Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Monolingual Data Improves Low-Resource Neural Machine Translation", "start_pos": 7, "end_pos": 72, "type": "TASK", "confidence": 0.8051977796213967}]}], "abstractContent": [{"text": "We train a neural machine translation (NMT) system to both translate source-language text and copy target-language text, thereby exploiting monolingual corpora in the target language.", "labels": [], "entities": []}, {"text": "Specifically, we create a bitext from the monolingual text in the target language so that each source sentence is identical to the target sentence.", "labels": [], "entities": []}, {"text": "This copied data is then mixed with the parallel corpus and the NMT system is trained like normal, with no metadata to distinguish the two input languages.", "labels": [], "entities": []}, {"text": "Our proposed method proves to bean effective way of incorporating monolingual data into low-resource NMT.", "labels": [], "entities": []}, {"text": "On Turkish\u2194English and Romanian\u2194English translation tasks, we see gains of up to 1.2 BLEU over a strong baseline with back-translation.", "labels": [], "entities": [{"text": "Romanian\u2194English translation", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6528762429952621}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9992278814315796}]}, {"text": "Further analysis shows that the linguistic phenomena behind these gains are different from and largely orthogonal to back-translation, with our copied corpus method improving accuracy on named entities and other words that should remain identical between the source and target languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9983019828796387}]}], "introductionContent": [{"text": "Neural machine translation (NMT) systems require a large amount of training data to make generalizations, both on the source side (in order to interpret the text well enough to translate it) and on the target side (in order to produce fluent translations).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8299031356970469}]}, {"text": "This data typically comes in the form of parallel corpora, in which each sentence in the source language is matched to a translation in the target language.", "labels": [], "entities": []}, {"text": "Recent work) has investigated incorporating monolingual training data (particularly on the target side) into NMT.", "labels": [], "entities": []}, {"text": "This effectively converts machine translation into a semi-supervised problem that takes advantage of both labeled (parallel) and unlabeled (monolingual) data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7506921887397766}]}, {"text": "Adding monolingual data to NMT is important because sufficient parallel data is unavailable for all but a few language pairs and domains.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a straightforward method for adding target-side monolingual training data to an NMT system without changing its architecture or training algorithm.", "labels": [], "entities": []}, {"text": "This method converts a monolingual corpus in the target language into a parallel corpus by copying it, so that each source sentence is identical to its corresponding target sentence.", "labels": [], "entities": []}, {"text": "This copied corpus is then mixed with the original parallel data and used to train the NMT system, with no distinction made between the parallel and the copied data.", "labels": [], "entities": []}, {"text": "We focus on language pairs with small amounts of parallel data where monolingual data has the most impact.", "labels": [], "entities": []}, {"text": "On the relatively lowresource language pairs of English\u2194Turkish and English\u2194Romanian, we find that our copying technique is effective both alone and combined with back-translation.", "labels": [], "entities": [{"text": "copying", "start_pos": 103, "end_pos": 110, "type": "TASK", "confidence": 0.9747819304466248}]}, {"text": "This is the case even when no additional monolingual data is used (i.e. when the copied corpus and the back-translated corpus are identical on the target side).", "labels": [], "entities": []}, {"text": "This implies that back-translation does not make full use of monolingual data in low-resource settings, which makes sense because it relies on low-resource (and therefore low-quality) translation in the reverse direction.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe a number of additional experiments on EN\u2192TR in order to investigate the effects of different experimental setups and aspects of the data.", "labels": [], "entities": []}, {"text": "Note that the BLEU scores in this section are not directly comparable with those in, since a different subset of the monolingual data is used for some of these experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9991737008094788}]}, {"text": "All BLEU scores reported in this section are on newstest2016 unless otherwise noted.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9980990290641785}, {"text": "newstest2016", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9897710084915161}]}], "tableCaptions": [{"text": " Table 1: Number of parallel and monolingual  training sentences for each language pair.", "labels": [], "entities": []}, {"text": " Table 2: Translation performance in BLEU with and without copied monolingual data. Statistically  significant differences are marked with  \u2020 (p < 0.01) and  \u2021 (p < 0.05).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9825006127357483}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.5411536693572998}, {"text": "\u2021", "start_pos": 158, "end_pos": 159, "type": "METRIC", "confidence": 0.981031060218811}]}, {"text": " Table 3: Language model perplexities for the outputs of each NMT system.", "labels": [], "entities": []}, {"text": " Table 4: Pass-through accuracy for the outputs of each NMT system.", "labels": [], "entities": [{"text": "Pass-through", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.979921817779541}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9274784922599792}]}, {"text": " Table 5: Comparison of translations generated by baseline and + copied systems.", "labels": [], "entities": []}, {"text": " Table 6: EN\u2192TR translation performance when  using the back-translated corpus twice vs. the  back-translated and copied corpora.", "labels": [], "entities": [{"text": "EN\u2192TR translation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.49950648844242096}]}, {"text": " Table 7: EN\u2192TR translation performance when  using the same or different data for copied and  back-translated corpora.", "labels": [], "entities": [{"text": "EN\u2192TR translation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5273559466004372}]}, {"text": " Table 8:  EN\u2192TR translation performance  without back-translated data. We include sys- tems trained with parallel and back-translated data  (without copied data) for comparison.", "labels": [], "entities": [{"text": "EN\u2192TR translation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.4543466791510582}]}, {"text": " Table 9: EN\u2192TR translation performance with  EN monolingual data.", "labels": [], "entities": [{"text": "EN\u2192TR translation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5342277064919472}]}, {"text": " Table 10: EN\u2192TR translation performance with  different amounts of monolingual data.", "labels": [], "entities": [{"text": "EN\u2192TR translation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.4518595337867737}]}]}