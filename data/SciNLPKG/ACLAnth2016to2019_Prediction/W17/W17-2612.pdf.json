{"title": [{"text": "Multi-task Domain Adaptation for Sequence Tagging", "labels": [], "entities": [{"text": "Multi-task Domain Adaptation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6256452600161234}, {"text": "Sequence Tagging", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.9057414531707764}]}], "abstractContent": [{"text": "Many domain adaptation approaches rely on learning cross domain shared representations to transfer the knowledge learned in one domain to other domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7223815470933914}]}, {"text": "Traditional domain adaptation only considers adapting for one task.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8142550587654114}]}, {"text": "In this paper, we explore multi-task representation learning under the domain adaptation scenario.", "labels": [], "entities": [{"text": "multi-task representation learning", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.7619331081708273}]}, {"text": "We propose a neural network framework that supports domain adaptation for multiple tasks simultaneously, and learns shared representations that better generalize for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7180567681789398}, {"text": "domain adaptation", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.716352254152298}]}, {"text": "We apply the proposed framework to domain adaptation for sequence tagging problems considering two tasks: Chinese word segmenta-tion and named entity recognition.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7721775770187378}, {"text": "sequence tagging", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.6996371746063232}, {"text": "named entity recognition", "start_pos": 137, "end_pos": 161, "type": "TASK", "confidence": 0.6163545250892639}]}, {"text": "Experiments show that multi-task domain adaptation works better than disjoint domain adaptation for each task, and achieves the state-of-the-art results for both tasks in the social media domain.", "labels": [], "entities": [{"text": "multi-task domain adaptation", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6365432739257812}]}], "introductionContent": [{"text": "Many natural language processing tasks have abundant annotations informal domain (news articles) but suffer a significant performance drop when applied to anew domain, where only a small number of annotated examples are available.", "labels": [], "entities": []}, {"text": "The idea behind domain adaptation is to leverage annotations from high-resource (source) domains to improve predictions in low-resource (target) domains by training a predictor fora single task across different domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7513225376605988}]}, {"text": "Domain adaptation work tends to focus on changes in data distributions, e.g. different words are used in each domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7803606986999512}]}, {"text": "Domain adaptation methods include unsupervised) and supervised variants, depending on whether there exists no or some training data in the target domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7458420991897583}]}, {"text": "This paper considers the case of supervised domain adaptation, where we have a limited amount of target domain training data, but much more training data in a source domain.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.673982838789622}]}, {"text": "Work on domain adaptation mostly follows two approaches: parameter tying (i.e. linking similar features during learning), and learning cross domain representations.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7511853277683258}]}, {"text": "Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7820522487163544}]}, {"text": "This is similar in spirit to multi-task learning (MTL)) which jointly learns models for several tasks, for example.", "labels": [], "entities": [{"text": "multi-task learning (MTL))", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7156724214553833}]}, {"text": "learning a single data representation common to each task (.", "labels": [], "entities": []}, {"text": "Given the similarity between domain adaptation and MTL, it is natural to ask: can domain adaptation benefit from jointly learning across several tasks?", "labels": [], "entities": []}, {"text": "This paper investigates how MTL can induce better representations for domain adaptation.", "labels": [], "entities": [{"text": "MTL", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9657678008079529}, {"text": "domain adaptation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7188588380813599}]}, {"text": "First, learning multiple tasks provides more training data for learning.", "labels": [], "entities": []}, {"text": "Second, MTL provides a better inductive learning bias so that the learned representations better generalize.", "labels": [], "entities": [{"text": "MTL", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.5586739182472229}]}, {"text": "Third, considering several tasks in domain adaptation opens up the opportunities to adapt from a different domain and a different task, a mismatch setting which has not previously been explored.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7442979216575623}]}, {"text": "We present a representation learning framework based on MTL that incorporates parameter tying strategies common in domain adaptation.", "labels": [], "entities": []}, {"text": "Our framework is based on a bidirectional long short-term memory network with a conditional random fields (BiLSTM-CRFs)) for sequence tagging.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7028213739395142}]}, {"text": "We consider sequence tagging problem since they are common in NLP applications and have been demonstrated to benefit from learning representations (.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.8369624316692352}]}, {"text": "This paper makes the following contributions: \u2022 A neural MTL domain adaptation framework that considers several tasks simultaneously when doing domain adaptation.", "labels": [], "entities": [{"text": "MTL domain adaptation", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8779921730359396}, {"text": "domain adaptation", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.7582285702228546}]}, {"text": "\u2022 A new domain/task mismatch setting: where you have two datasets from two different, but related domains and tasks.", "labels": [], "entities": []}, {"text": "\u2022 State-of-the-art results on Chinese word segmentation and named entity recognition in social media data.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.5958394805590311}, {"text": "named entity recognition", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.674206018447876}]}], "datasetContent": [{"text": "We test the effectiveness of the multi-task domain adaptation framework on two sequence tagging problems: Chinese word segmentation (CWS) and named entity recognition (NER).", "labels": [], "entities": [{"text": "multi-task domain adaptation", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.6254215141137441}, {"text": "sequence tagging", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.7485893964767456}, {"text": "Chinese word segmentation (CWS)", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.7154937734206518}, {"text": "named entity recognition (NER)", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.7740679333607355}]}, {"text": "We consider two domains: news and social media, with news the source domain and social media the target domain..", "labels": [], "entities": []}, {"text": "Both SighanCWS and SighanNER contain several portions 2 ; we use those for simplified Chinese (PKU and MSR respectively).", "labels": [], "entities": [{"text": "SighanCWS", "start_pos": 5, "end_pos": 14, "type": "DATASET", "confidence": 0.8768635392189026}]}, {"text": "The datasets do not have development data, so we holdout the last 10% of training data for development.", "labels": [], "entities": []}, {"text": "Sighan-NER contains three entity types (person, organization and location), while WeiboNER is annotated with four entity types (person, organization, location and geo-political entity), including named and nominal mentions.", "labels": [], "entities": [{"text": "WeiboNER", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.9334812760353088}]}, {"text": "To match the two tag sets, we only use named mentions in WeiboNER and merge geo-political entities and locations.", "labels": [], "entities": [{"text": "WeiboNER", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9748815298080444}]}, {"text": "The 2000 annotated instances in WeiboSeg were meant only for evaluation, so we split the data ourselves using an 8:1:1 split for training, development, and test.", "labels": [], "entities": [{"text": "WeiboSeg", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9540373682975769}]}, {"text": "Hyper-parameters are tuned on the development data and we report the precision, recall, and F1 score on the test portion.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9997950196266174}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.997961163520813}, {"text": "F1 score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9857785403728485}]}, {"text": "Detailed data statistics is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Model variations grouped by number of training datesets.", "labels": [], "entities": []}]}