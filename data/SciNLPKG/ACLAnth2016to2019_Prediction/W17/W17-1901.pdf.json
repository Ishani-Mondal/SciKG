{"title": [{"text": "Compositional Semantics using Feature-Based Models from WordNet", "labels": [], "entities": [{"text": "WordNet", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.8286453485488892}]}], "abstractContent": [{"text": "This article describes a method to build semantic representations of composite expressions in a compositional way by using WordNet relations to represent the meaning of words.", "labels": [], "entities": []}, {"text": "The meaning of a target word is modelled as a vector in which its semantically related words are assigned weights according to both the type of the relationship and the distance to the target word.", "labels": [], "entities": []}, {"text": "Word vectors are composi-tionally combined by syntactic dependencies.", "labels": [], "entities": []}, {"text": "Each syntactic dependency triggers two complementary compositional functions: the named head function and dependent function.", "labels": [], "entities": []}, {"text": "The experiments show that the proposed compositional method performs as the state-of-the-art for subject-verb expressions, and clearly outperforms the best system for transitive subject-verb-object constructions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The principle of compositionality states that the meaning of a complex expression is a function of the meaning of its constituent parts and of the mode of their combination.", "labels": [], "entities": []}, {"text": "In the recent years, different distributional semantic models endowed with a compositional component have been proposed.", "labels": [], "entities": []}, {"text": "Most of them define words as high-dimensional vectors where dimensions represent co-occurring context words.", "labels": [], "entities": []}, {"text": "This distributional semantic representation makes it possible to combine vectors using simple arithmetic operations such as addition and multiplication, or more advanced compositional methods such as learning functional words as tensors and composing constituents through inner product operations.", "labels": [], "entities": []}, {"text": "Notwithstanding, these models are usually qualified as black box systems because they are usually not interpretable by humans.", "labels": [], "entities": []}, {"text": "Currently, the field of interpretable computational models is gaining relevance and, therefore, the development of more explainable and understandable models in compositional semantics is also an open challenge.", "labels": [], "entities": []}, {"text": "On the other hand, distributional semantic models, given the size of the vectors, needs significant resources and they are dependent on particular corpus, which can generate some biases in their application to different languages.", "labels": [], "entities": []}, {"text": "Thus, in this paper, we will pay attention to compositional approaches which employ other kind of word semantic models, such as those based on the WordNet relationships; i.e., synsets, hypernyms, hyponyms, etc.", "labels": [], "entities": []}, {"text": "Only in) we can find a proposal for word vector representation using hand-crafted linguistic resources), although a compositional frame is not explicitly adopted.", "labels": [], "entities": [{"text": "word vector representation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.7235318422317505}]}, {"text": "Therefore, to the best of our knowledge, this is the first work using WordNet to build compositional semantic interpretations.", "labels": [], "entities": [{"text": "compositional semantic interpretations", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.5974468191464742}]}, {"text": "Thus, in this article, we propose a method to compositionally build the semantic representation of composite expressions using a feature-based approach): constituent elements are induced by WordNet relationships.", "labels": [], "entities": []}, {"text": "However, this proposal raises a serious problem: the semantic representation of two syntactically related words (e.g. the verb run and the noun computer in \"the computer runs\") encodes incompatible information and there is no direct way of combining the features used to represent the meaning of the two words.", "labels": [], "entities": []}, {"text": "On the one hand, the verb 1 run is related by synonymy, hypernym, hyponym and entailment to other verbs and, on the other, the noun computer is put in relation with other nouns by synonymy, hypernym, hyponym, and soon.", "labels": [], "entities": []}, {"text": "In order to solve this drawback, on the basis of previous work on dependency-based distributional compositionality, we distinguish between direct denotation and selectional preferences within a dependency relation.", "labels": [], "entities": []}, {"text": "More precisely, when two words are syntactically related, for instance computer and the verb run by the subject relation, we build two contextualized senses: the contextualized sense of computer given the requirements of run and the contextualized sense of run given computer.", "labels": [], "entities": []}, {"text": "The sense of computer is built by combining the semantic features of the noun (its direct denotation) with the selectional preferences imposed by the verb.", "labels": [], "entities": []}, {"text": "The features of the noun are built from the set of words linked to computer in WordNet, while the selectional preferences of run in the subject position are obtained by combining the features of all the nouns that can be the nominal subject of the verb (i.e. the features of runners).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9649660587310791}]}, {"text": "Then, the two sets of features are combined and the resulting new set represents the specific sense of the noun computer as nominal subject of run.", "labels": [], "entities": []}, {"text": "The sense of the verb given the noun is builtin a analogous way: the semantic features of the verb are combined with the (inverse) selectional preferences imposed by the noun, resulting in anew compositional representation of the verb run when it is combined with computer at the subject position.", "labels": [], "entities": []}, {"text": "The two new compositional feature sets represent the contextualized senses of the two related words.", "labels": [], "entities": []}, {"text": "During the contextualization process, ambiguous or polysemous words maybe disambiguated in order to obtain the right representation.", "labels": [], "entities": []}, {"text": "For dealing with any sequence with N (lexical) words (e.g., \"the coach runs the team\"), the semantic process can be applied in two different ways: from left-to-right and from right-to-left.", "labels": [], "entities": []}, {"text": "In the first case, it is applied N \u22121 times dependencyby-dependency in order to obtain N contextualized senses, one per lexical word.", "labels": [], "entities": []}, {"text": "Thus, firstly, the subject dependency builds two contextualized senses: that of run given the noun coach and that of the noun given the verb.", "labels": [], "entities": []}, {"text": "Then, the direct object dependency is applied on the already contextualized sense of the verb in order to contextualize it again given team at the direct object position.", "labels": [], "entities": []}, {"text": "This dependency also yields the contextualized sense of the object given the verb and its nominal subject (coach+run).", "labels": [], "entities": []}, {"text": "At the end of the interpretation process, we obtain three fully contextualized senses.", "labels": [], "entities": []}, {"text": "In the second case, from right-to-left, the semantic process process is applied in a similar way, being contextualized (and disambiguated) using the restrictions imposed by the verb and its nominal object (run+team).", "labels": [], "entities": []}, {"text": "As in the first case, three slightly different word senses are also obtained.", "labels": [], "entities": []}, {"text": "Lastly, word sense disambiguation is out of the aim of this paper.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.8076561093330383}]}, {"text": "Here, we only use WordNet for extracting semantic information from words, but not to identify word senses.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9501640796661377}]}, {"text": "The article is organized as follow: In the next section (2), different approaches on ontological feature-based representations and compositional semantics are introduced and discussed.", "labels": [], "entities": []}, {"text": "Then, sections 3 and 4 respectively describe our featurebased semantic representation and compositional strategy.", "labels": [], "entities": []}, {"text": "In Section 5, some experiments are performed to evaluate the quality of the word models and compositional word vectors.", "labels": [], "entities": []}, {"text": "Finally, relevant conclusions are reported in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have performed several similarity-based experiments using the semantic word model defined in Section 3 and the compositional algorithm described in 4.", "labels": [], "entities": []}, {"text": "First, in Subsection 5.1, we evaluate just word similarity without composition.", "labels": [], "entities": []}, {"text": "Then, in Subsection 5.2, we evaluate the simple compositional approach by making use of a dataset with similar noun-verb pairs (NV constructions).", "labels": [], "entities": []}, {"text": "Finally, the recursive application of compositional functions is evaluated in Subsection 5.3, by making use of a dataset with similar noun-verb-noun pairs.", "labels": [], "entities": []}, {"text": "In all experiments, we made use of datasets suited for the task at hand, and compare our results with those obtained by the best systems for the corresponding dataset.", "labels": [], "entities": []}, {"text": "Moreover, in order to build the selectional preferences of the syntactically related words, we used the British National Corpus (BNC).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 104, "end_pos": 133, "type": "DATASET", "confidence": 0.9676417509714762}]}, {"text": "Syntactic analysis on BNC was performed with the dependency parser DepPattern (, previously PoS tagged with Tree-Tagger (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy of three systems on the WBST  test (synonym detection on nouns, adjectives, and  verbs)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9927144646644592}, {"text": "WBST  test", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.8751049041748047}, {"text": "synonym detection", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.9144673049449921}]}, {"text": " Table 3: Spearman correlation for intransitive ex- pressions using the benchmark by Mitchell and  Lapata (2008)", "labels": [], "entities": [{"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.5128626227378845}]}, {"text": " Table 4: Spearman correlation for transitive ex- pressions using the benchmark by Grefenstette  and Sadrzadeh (2011)", "labels": [], "entities": []}]}