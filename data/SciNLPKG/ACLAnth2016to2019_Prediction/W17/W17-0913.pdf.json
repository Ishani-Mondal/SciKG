{"title": [{"text": "Story Cloze Ending Selection Baselines and Data Examination", "labels": [], "entities": [{"text": "Story Cloze Ending Selection Baselines", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6639143705368042}]}], "abstractContent": [{"text": "This paper describes two supervised base-line systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a).", "labels": [], "entities": []}, {"text": "We first build a classifier using features based on word embeddings and semantic similarity computation.", "labels": [], "entities": []}, {"text": "We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings.", "labels": [], "entities": []}, {"text": "Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models.", "labels": [], "entities": []}, {"text": "Our best model achieves an accuracy of 72.42, ranking 3rd in the official evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9997789263725281}]}], "introductionContent": [{"text": "Understanding commonsense stories is an easy task for humans but represents a challenge for machines.", "labels": [], "entities": [{"text": "Understanding commonsense stories", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.900292177995046}]}, {"text": "A recent commonsense story understanding task is the 'Story Cloze Test), where a human or an AI system has to read a given four-sentence story and select the proper ending out of two proposed endings.", "labels": [], "entities": [{"text": "commonsense story understanding", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.5987861752510071}]}, {"text": "While the majority class baseline performance on the given test set yields an accuracy of 51.3%, human performance achieves 100%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9995040893554688}]}, {"text": "This makes the task a good challenge for an AI system.", "labels": [], "entities": []}, {"text": "The Story Cloze Test task is proposed as a Shared Task for LSDSem 2017 . 17 teams registered for the Shared Task and 10 teams submitted their results 2 . Our contribution is that we set anew baseline for the task, showing that a simple linear model based on distributed representations and semantic similarity features achieves state-of-the-art results.", "labels": [], "entities": [{"text": "Story Cloze Test task", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.8148909360170364}]}, {"text": "We also evaluate the ability of different embedding models to represent common knowledge required for this task.", "labels": [], "entities": []}, {"text": "We present an LSTM-based classifier with different representation encodings that tries to model the relation between the story and alternative endings and argue about its inability to do so.", "labels": [], "entities": []}], "datasetContent": [{"text": "In we compare our best systems to existing baselines, Shared Task participant systems and human performance.", "labels": [], "entities": []}, {"text": "Our features baseline system is our best feature-based system using embeddings and word2vec trained on Dev and tuned with cross-validation.", "labels": [], "entities": []}, {"text": "Our neural system employs raw LSTM encodings as described in Section 4.1(i) and it is trained on the Dev-Dev dataset which consists of 90% of the Dev dataset selected randomly and tuned on the rest of Dev.", "labels": [], "entities": [{"text": "Dev-Dev dataset", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.8608967959880829}]}, {"text": "The best result in the task is achieved by (msap) who employ stylistic features combined with RNN representations.", "labels": [], "entities": []}, {"text": "We have no information about cogcomp and ukp.", "labels": [], "entities": []}, {"text": "The Story Cloze Test is a story understanding problem.", "labels": [], "entities": [{"text": "story understanding", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.767461895942688}]}, {"text": "However, the given stories are very short and they require background knowledge about relations between the given entities, entity types and events defining the story and their endings, as well as relations between these events.", "labels": [], "entities": []}, {"text": "We first train our feature-based model with alternative embedding representations in order to select the best source of knowledge for further experiments.", "labels": [], "entities": []}, {"text": "We experiment with different word embedding models pre-trained on a large number of tokens including word2vec 4 (), GloVe () and ConceptNet Numberbatch.", "labels": [], "entities": [{"text": "ConceptNet Numberbatch", "start_pos": 129, "end_pos": 151, "type": "DATASET", "confidence": 0.7967578172683716}]}, {"text": "Results on training the feature-based model with different word embeddings are shown in.", "labels": [], "entities": []}, {"text": "The results indicate how well the vector representation models perform in terms of encoding commonsense stories.", "labels": [], "entities": [{"text": "encoding commonsense stories", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.8176107803980509}]}, {"text": "We present the performance of the embedding models depending on the defined features.", "labels": [], "entities": []}, {"text": "We perform feature ablation experiments to determine the features which contribute most to the overall score for the different models.", "labels": [], "entities": []}, {"text": "Using All features defined in Section 3.1, the word2vec vectors, trained on Google News 100B corpus perform best followed by ConcepNet enriched embeddings and Glove trained on Common Crawl 840B.", "labels": [], "entities": [{"text": "Google News 100B corpus", "start_pos": 76, "end_pos": 99, "type": "DATASET", "confidence": 0.9420374035835266}, {"text": "Glove", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.9533606767654419}, {"text": "Common Crawl 840B", "start_pos": 176, "end_pos": 193, "type": "DATASET", "confidence": 0.936948835849762}]}, {"text": "The word2vec model suffers most when similarity features are excluded.", "labels": [], "entities": []}, {"text": "We note that the ConceptNet embeddings do not decrease performance when similarity features are excluded, unlike all other models.", "labels": [], "entities": []}, {"text": "We also see that the POS similarities are more important than the MaxSim and the Sim (cosine betwen all words in Story and Ending) as they yield worse results, for almost all models, when excluded from All features.", "labels": [], "entities": []}, {"text": "In column WE E1, E2 we report results on features based only on Ending1 and Ending 2.", "labels": [], "entities": []}, {"text": "We note that the overall results are still very good.", "labels": [], "entities": []}, {"text": "From this we can derive that the difference of Good vs. Bad endings is not only defined in the story context but it is also characterized by the information present in these sentences in isolation.", "labels": [], "entities": []}, {"text": "This could be due to a reporting bias (Gordon and Van Durme, 2013) employed by the crowdworkers in the corpus construction process.", "labels": [], "entities": [{"text": "corpus construction process", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.7578885356585184}]}, {"text": "The last column Sims only shows results with features based only on similarity features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of our models to shared  task participants' results and other baselines.  Word2Vec sim, Skip-thoughts sim and DSSM are  described in (Mostafazadeh et al., 2016b).", "labels": [], "entities": []}, {"text": " Table 3: Experiments using linear classifier with features based on word embeddings. Trained on Dev  (tuned with cross-validation) and evaluated on Test.", "labels": [], "entities": []}, {"text": " Table 4: Comparison between LSTM representa- tion strategies.", "labels": [], "entities": []}]}