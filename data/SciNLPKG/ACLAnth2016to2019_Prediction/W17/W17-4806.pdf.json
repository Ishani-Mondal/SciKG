{"title": [{"text": "Neural Machine Translation for Cross-Lingual Pronoun Prediction", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7181790073712667}, {"text": "Cross-Lingual Pronoun Prediction", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.6164126495520273}]}], "abstractContent": [{"text": "In this paper we present our systems for the DiscoMT 2017 cross-lingual pronoun prediction shared task.", "labels": [], "entities": [{"text": "DiscoMT 2017 cross-lingual pronoun prediction shared task", "start_pos": 45, "end_pos": 102, "type": "TASK", "confidence": 0.7655507240976606}]}, {"text": "For all four language pairs, we trained a standard attention-based neural machine translation system as well as three variants that incorporate information from the preceding source sentence.", "labels": [], "entities": [{"text": "attention-based neural machine translation", "start_pos": 51, "end_pos": 93, "type": "TASK", "confidence": 0.661935031414032}]}, {"text": "We show that our systems , which are not specifically designed for pronoun prediction and maybe used to generate complete sentence translations, generally achieve competitive results on this task.", "labels": [], "entities": [{"text": "pronoun prediction", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7514321506023407}]}], "introductionContent": [{"text": "Given a source document and its corresponding partial translation, the goal of the DiscoMT 2017 cross-lingual pronoun prediction shared task) is to correctly replace the missing pronouns, choosing among a small set of candidates.", "labels": [], "entities": [{"text": "DiscoMT 2017 cross-lingual pronoun prediction shared task", "start_pos": 83, "end_pos": 140, "type": "TASK", "confidence": 0.6911573708057404}]}, {"text": "In this paper, we propose and evaluate models on four sub-tasks: En-Fr, En-De, DeEn and Es-En.", "labels": [], "entities": []}, {"text": "We consider the use of attention-based neural machine translation systems ( ) for pronoun prediction and investigate the potential for incorporating discourse-level structure by integrating the preceding source sentence into the models.", "labels": [], "entities": [{"text": "attention-based neural machine translation", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.6737932413816452}, {"text": "pronoun prediction", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.796896755695343}]}, {"text": "More specifically, instead of modeling the conditional distribution p(Y |X) over translations given a source sentence, we explore different networks that model p(Y |X, X \u22121 ), where X \u22121 is the previous source sentence.", "labels": [], "entities": []}, {"text": "The proposed larger-context neural machine translation systems are inspired by recent work on largercontext language modeling ( 2 Baseline: Attention-based Neural Machine Translation An attention-based translation system ( ) is composed of three parts: encoder, decoder, and attention model.", "labels": [], "entities": [{"text": "larger-context neural machine translation", "start_pos": 13, "end_pos": 54, "type": "TASK", "confidence": 0.6287244856357574}, {"text": "Attention-based Neural Machine Translation", "start_pos": 140, "end_pos": 182, "type": "TASK", "confidence": 0.7011363059282303}]}, {"text": "The source sentence X = (x 1 , x 2 , . .", "labels": [], "entities": []}, {"text": ", x Tx ) is encoded into a set of annotation vectors {h 1 , h 2 , . .", "labels": [], "entities": []}, {"text": "To do so, we use a bidirectional recurrent network) with a gated recurrent unit (GRU,.", "labels": [], "entities": []}, {"text": "c i is a weighted sum of the annotations.", "labels": [], "entities": []}, {"text": "where e ij is the attention model score, which represents how well the output at time i aligns with the input around time j.", "labels": [], "entities": []}], "datasetContent": [{"text": "To train our models, which are fully differentiable, we use the Adadelta optimizer.", "labels": [], "entities": []}, {"text": "Word embeddings have dimensionality 620, decoder and source encoder RNNs have 1000-dimensional hidden representations, and the context encoder RNN hidden states are of size 620.", "labels": [], "entities": []}, {"text": "As the source and context annotations are the concatenation of the forward and backward encoder hidden states, their dimensionality are 2000 and 1240 respectively.", "labels": [], "entities": []}, {"text": "The models are regularized with 50% Dropout () applied to all RNN inputs and on the decoder hidden layer preceding the softmax.", "labels": [], "entities": []}, {"text": "Pronouns are predicted using a modified beam search where the beam is expanded only at the \"REPLACE\" placeholders, and is otherwise constrained to the reference.", "labels": [], "entities": [{"text": "REPLACE", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9839734435081482}]}, {"text": "The beam size is set to the number of pronoun classes, so that our approach is equivalent to exhaustive search for sentences with a single placeholder.", "labels": [], "entities": []}, {"text": "Models for which beam search lead to the highest validation macro-average recall were selected and submitted for the shared task.", "labels": [], "entities": [{"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9768994450569153}]}, {"text": "The baselines were also sent as contrastive submissions. and 2 respectively present validation and test results across all language pairs for the models described in sections 2 and 3.", "labels": [], "entities": []}, {"text": "Amongst the four models we evaluated on the test sets, a different one performs best for each language pair.", "labels": [], "entities": []}, {"text": "Nevertheless, the DGCM model is the most consistent, always ranking second or first amongst our systems.", "labels": [], "entities": [{"text": "DGCM", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.7123950719833374}]}, {"text": "Moreover, it beats the baseline on all tasks except Es-En, which it trails by a marginal 0.2%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Validation macro-average recall (in %) for cross-lingual pronoun prediction.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9100291728973389}, {"text": "cross-lingual pronoun prediction", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7887712121009827}]}, {"text": " Table 2: Test macro-average recall (in %) for cross-lingual pronoun prediction. The \"Best\" column  displays the highest score across all primary and contrastive submissions to the", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9743782877922058}, {"text": "cross-lingual pronoun prediction", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.7809743285179138}]}]}