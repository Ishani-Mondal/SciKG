{"title": [{"text": "Leveraging Newswire Treebanks for Parsing Conversational Data with Argument Scrambling", "labels": [], "entities": [{"text": "Leveraging Newswire Treebanks", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.594870924949646}, {"text": "Parsing Conversational Data with Argument Scrambling", "start_pos": 34, "end_pos": 86, "type": "TASK", "confidence": 0.7104159692923228}]}], "abstractContent": [{"text": "We investigate the problem of parsing conversational data of morphologically-rich languages such as Hindi where argument scrambling occurs frequently.", "labels": [], "entities": [{"text": "parsing conversational", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8723013401031494}]}, {"text": "We evaluate a state-of-the-art non-linear transition-based parsing system on anew dataset containing 506 dependency trees for sentences from Bollywood (Hindi) movie scripts and Twitter posts of Hindi mono-lingual speakers.", "labels": [], "entities": []}, {"text": "We show that a dependency parser trained on a newswire tree-bank is strongly biased towards the canon-ical structures and degrades when applied to conversational data.", "labels": [], "entities": []}, {"text": "Inspired by Trans-formational Generative Grammar (Chom-sky, 1965), we mitigate the sampling bias by generating all theoretically possible alternative word orders of a clause from the existing (kernel) structures in the treebank.", "labels": [], "entities": [{"text": "Trans-formational Generative Grammar (Chom-sky, 1965)", "start_pos": 12, "end_pos": 65, "type": "TASK", "confidence": 0.7986278831958771}]}, {"text": "Training our parser on canonical and transformed structures improves performance on conversational data by around 9% LAS over the baseline newswire parser.", "labels": [], "entities": [{"text": "LAS", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9986734390258789}]}], "introductionContent": [{"text": "In linguistics, every language in assumed to have a basic constituent order in a clause.", "labels": [], "entities": []}, {"text": "In some languages, constituent order is fixed to define the grammatical structure of a clause and the grammatical relations therein, while in others, that convey grammatical information through inflection or word morphology, constituent order assumes discourse function and defines the information structure of a sentence.", "labels": [], "entities": []}, {"text": "Despite word order freedom, most of the morphologicallyrich languages exhibit a preferred word order informal registers such as newswire.", "labels": [], "entities": []}, {"text": "Word order alternations are more commonplace in informal language use such as day-to-day conversations and social media content.", "labels": [], "entities": []}, {"text": "For statistical parsing, word order alternations (argument scrambling) area major bottleneck.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8308832049369812}, {"text": "word order alternations", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6897140443325043}]}, {"text": "Given appropriate pragmatic conditions a transitive sentence in a morphologicallyrich language allows n factorial (n!) permutations, where n is the number of verb arguments and/or adjuncts.", "labels": [], "entities": []}, {"text": "Argument scrambling often leads to structural discontinuities.", "labels": [], "entities": [{"text": "Argument scrambling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7961022555828094}]}, {"text": "Moreover, these scramblings worsen the data sparsity problem since data-driven parsers are usually trained on a limited size treebank where most of the valid structures may never show up.", "labels": [], "entities": []}, {"text": "More importantly, most of the available treebanks are built on newswire text which is more formal.", "labels": [], "entities": []}, {"text": "The chances of any deviation from the canonical word order are smaller, thereby creating sampling bias.", "labels": [], "entities": []}, {"text": "A common solution to address the sampling bias is to alter the distribution of classes in the training data by using sampling techniques.", "labels": [], "entities": []}, {"text": "However, simple sampling techniques such as minority oversampling may not be a feasible solution for parsing argument scramblings which are almost non-existent in the newswire treebanks (see).", "labels": [], "entities": [{"text": "parsing argument scramblings", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.9306839903195699}, {"text": "newswire treebanks", "start_pos": 167, "end_pos": 185, "type": "DATASET", "confidence": 0.8948347866535187}]}, {"text": "Newswire data usually represent only a sample of possible syntactic structures and, therefore, suffer from non-representation of certain classes that encode valid arc directionalities.", "labels": [], "entities": [{"text": "Newswire data", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9102863371372223}]}, {"text": "In the Hindi dependency treebank (HTB), for example, dependency relations such as source, time and place are never extraposed.", "labels": [], "entities": [{"text": "Hindi dependency treebank (HTB)", "start_pos": 7, "end_pos": 38, "type": "DATASET", "confidence": 0.7144246995449066}]}, {"text": "Therefore, we instead generate training examples for varied arc directionalities by transforming the gold syntactic trees in the training data.", "labels": [], "entities": []}, {"text": "We experiment with the Hindi dependency treebank and show that such transformations are indeed helpful when we deal with data with diverse word-orders such as movie dialogues.", "labels": [], "entities": []}, {"text": "Our work is in conformity with earlier attempts where modifying source syntactic trees to match target distributions benefited parsing of noisy, conversational data (Van der, Object (O) and Verb (V) in transitive sentences in the HTB training data with their percentages of occurrence.", "labels": [], "entities": [{"text": "HTB training data", "start_pos": 230, "end_pos": 247, "type": "DATASET", "confidence": 0.9396136005719503}]}], "datasetContent": [{"text": "The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9650704860687256}]}, {"text": "The monolingual models are trained on training files of HTB which uses the P\u00af aninian Grammar framework (PG) (, while the multilingual models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies ().", "labels": [], "entities": [{"text": "HTB", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9631651639938354}, {"text": "Universal Dependency Treebanks", "start_pos": 157, "end_pos": 187, "type": "DATASET", "confidence": 0.6981411973635355}, {"text": "Universal Dependencies", "start_pos": 239, "end_pos": 261, "type": "DATASET", "confidence": 0.8243408799171448}]}, {"text": "Parsing Models Our underlying parsing method is based on the arc-eager transition system.", "labels": [], "entities": []}, {"text": "The arc-eager system defines a set of configurations fora sentence,...,wn, where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A.", "labels": [], "entities": []}, {"text": "For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [ and A = \u2205 and terminates with a configuration C if the buffer is empty and the stack contains the ROOT.", "labels": [], "entities": []}, {"text": "The parse trees derived from transition sequences are given by A. To derive the parse tree, the arceager system defines four types of transitions (t): Shift, Left-Arc, Right-Arc, and Reduce.", "labels": [], "entities": []}, {"text": "We use a non-linear neural network to predict the transitions for the parser configurations.", "labels": [], "entities": []}, {"text": "The neural network model is the standard feedforward neural network with a single layer of hidden units.", "labels": [], "entities": []}, {"text": "We use 128 hidden units and the RelU activation function.", "labels": [], "entities": [{"text": "RelU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.7869902849197388}]}, {"text": "The output layer uses a softmax function for probabilistic multi-class classification.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6746216714382172}]}, {"text": "The model is trained by minimizing negative log-likelihood loss with l2-regularization over: Accuracy of our different parsing models on conversational data as well as newswire evaluation sets.", "labels": [], "entities": []}, {"text": "Improvements in superscript are over the newswire baseline.", "labels": [], "entities": []}, {"text": "We use Momentum SGD for optimization) and apply dropout ().", "labels": [], "entities": []}, {"text": "From each parser configuration, we extract features related to the top three nodes in the stack, the top node in the buffer and the leftmost and rightmost children of the top three nodes in the stack and the leftmost child of the top node in the buffer.", "labels": [], "entities": []}, {"text": "Similarly to, we use two stacked Bidirectional LSTMs with 128 hidden nodes for learning the feature representations over conjoined word-tag sequences for each training sentence.", "labels": [], "entities": []}, {"text": "We use an additional Bidirectional LSTM (64 nodes) for learning separate representations of words over their character sequences for capturing out-of-vocabulary (OOV) words at testing time.", "labels": [], "entities": []}, {"text": "We use word dropout with a dropout probability of 0.1 which enables character embeddings to drive the learning process around 10% of the time instead of full word representations.", "labels": [], "entities": []}, {"text": "This is important for evaluation on noisy data where OOV words are quite frequent.", "labels": [], "entities": []}, {"text": "The monolingual models are initialized using pre-trained 64 dimensional word embeddings of Hindi, while multilingual models use HindiEnglish bilingual embeddings from , while POS embeddings are randomly initialized within a range of -0.25 to +0.25 with 32 dimensions.", "labels": [], "entities": []}, {"text": "Moreover, we use pseudo-projective transformations of to handle a higher percentage of non-projective arcs in the evaluation data (6% as opposed to 2% in the training data).", "labels": [], "entities": []}, {"text": "We use the most informative scheme of head+path to store the transformation information.", "labels": [], "entities": []}, {"text": "Inverse transformations based on breadth-first search are applied to recover the non-projective arcs in a post-processing step.", "labels": [], "entities": []}, {"text": "We ran two experiments to evaluate the effectiveness of the tree transformations on the parsing 7 https://bitbucket.org/irshadbhat/indic-word2vec-embeddings of conversational data.", "labels": [], "entities": []}, {"text": "In the first, we leverage the monolingual annotations by applying syntactic transformations; in the second we use a crosslingual treebank with diverse word-orders.", "labels": [], "entities": []}, {"text": "For each experiment type, we report results using both predicted and gold POS tags.", "labels": [], "entities": []}, {"text": "The POS taggers are trained using an architecture similar to the parser's with a single layer MLP which takes its input from Bi-LSTM representation of the focus word (see Appendix for the results).", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7542156279087067}]}, {"text": "We used the newswire parsing models as the baseline for evaluating the impact of tree transformations and multilingual annotations.", "labels": [], "entities": []}, {"text": "The augmented models are trained on the union of the original newswire training data and the transformed trees.", "labels": [], "entities": [{"text": "newswire training data", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.772924025853475}]}, {"text": "We generated 9K trees from 4K representative sentences) which were projectivized before applying syntactic transformations to preserve non-projective arcs.", "labels": [], "entities": []}, {"text": "Our results are reported in.", "labels": [], "entities": []}, {"text": "As the table shows, our newswire models suffer heavily when applied to conversational data.", "labels": [], "entities": []}, {"text": "The parser indeed seems biased towards canonical structures of Hindi.", "labels": [], "entities": []}, {"text": "It could not correctly parse extraposed arguments, and could not even identify direct objects if they were not adjacent to the verb.", "labels": [], "entities": []}, {"text": "However, in both gold and predicted settings, our augmented parsing models produce results that are approximately 9% LAS points better than the state-of-the-art baseline newswire parsers.", "labels": [], "entities": [{"text": "LAS", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9927814602851868}]}, {"text": "Our augmented models even provided better results with UD dependencies.", "labels": [], "entities": []}, {"text": "Probably due to the increased structural ambiguity, augmenting transformed trees with the original training data led to a slight decrease in the results on the original Hindi test sets in both UD and PG dependencies.", "labels": [], "entities": []}, {"text": "Interestingly, our cross-lingual model also captured certain levels of scrambling which could be because the English treebank would at least provide training instances for SVO word order.", "labels": [], "entities": [{"text": "English treebank", "start_pos": 109, "end_pos": 125, "type": "DATASET", "confidence": 0.8195971250534058}, {"text": "SVO word order", "start_pos": 172, "end_pos": 186, "type": "TASK", "confidence": 0.7097981572151184}]}], "tableCaptions": [{"text": " Table 3: Accuracy of our different parsing models on conversational data as well as newswire evaluation sets.  Improvements in superscript are over the newswire baseline.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9700142741203308}]}]}