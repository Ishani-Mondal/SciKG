{"title": [{"text": "Coarse-To-Fine Parsing for Expressive Grammar Formalisms", "labels": [], "entities": []}], "abstractContent": [{"text": "We generalize coarse-to-fine parsing to grammar formalisms that are more expressive than PCFGs and/or describe languages of trees or graphs.", "labels": [], "entities": []}, {"text": "We evaluate our algorithm on PCFG, PTAG, and graph parsing.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8730521202087402}, {"text": "PTAG", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.7141228914260864}, {"text": "graph parsing", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.7126533836126328}]}, {"text": "While we achieve the expected performance gains on PCFGs, coarse-to-fine does not help for PTAG and can even slowdown parsing for graphs.", "labels": [], "entities": [{"text": "PTAG", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.7700777053833008}]}, {"text": "We discuss the implications of this finding.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coarse-to-fine (CTF) parsing) is one of the most effective pruning techniques for PCFG parsing.", "labels": [], "entities": [{"text": "Coarse-to-fine (CTF) parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5623854756355285}, {"text": "PCFG parsing", "start_pos": 82, "end_pos": 94, "type": "TASK", "confidence": 0.889192521572113}]}, {"text": "Its basic idea is to simplify a grammar by systematically merging nonterminals together.", "labels": [], "entities": []}, {"text": "One then parses the input with the simple grammar, and uses statistics calculated from the resulting parse chart to constrain parsing with the original grammar.", "labels": [], "entities": []}, {"text": "This can speedup parsing by an order of magnitude at no loss inaccuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9809330701828003}]}, {"text": "We present an algorithm for CTF parsing for grammar formalisms that are more expressive than PCFGs -to our knowledge, for the first time.", "labels": [], "entities": [{"text": "CTF parsing", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9463419318199158}]}, {"text": "More precisely, we extend CTF parsing to Interpreted Regular Tree Grammars), a very general grammar formalism which captures PCFGs, tree-adjoining grammars (TAGs,), hyperedge replacement graph grammars (HRGs,), and many others.", "labels": [], "entities": [{"text": "CTF parsing", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8499450087547302}]}, {"text": "Our direct application of CTF to expressive grammar formalism contrasts with related work) which limits entries for the parse chart of the expressive formalism using the parse chart of a PCFG approximation.", "labels": [], "entities": []}, {"text": "We then evaluate our generalized CTF algorithm on a number of grammar formalisms.", "labels": [], "entities": []}, {"text": "On PCFGs, we obtain the expected speedups.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8431022763252258}]}, {"text": "However, we observe no speedups for TAG parsing compared to unpruned parsing, and HRG parsing on abstract meaning representations () is even slowed down by CTF.", "labels": [], "entities": [{"text": "TAG parsing", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.8633388578891754}, {"text": "HRG parsing", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.7352443039417267}]}, {"text": "We discuss these findings and show how the efficacy of CTF parsing relies on specific properties of PCFG grammars derived from treebanks.", "labels": [], "entities": [{"text": "CTF parsing", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.921965479850769}]}, {"text": "Because these properties do not depend on the specifics of our formalism, they would generalize to formalism specific implementations of CTF for TAG or HRG.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using this algorithm, we can do CTF parsing for all grammar formalisms that can be encoded as IRTGs.", "labels": [], "entities": [{"text": "CTF parsing", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8821502029895782}]}, {"text": "We evaluate it on PCFG, TAG, and graph parsing, using the efficient algorithms of to compute the coarsest charts.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.9183667302131653}, {"text": "graph parsing", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.73812535405159}]}, {"text": "These algorithms are lazy and try to avoid computing rules of the inverse homomorphism grammar which cannot participate in a derivation.", "labels": [], "entities": []}, {"text": "This means that the number of rules in the inverse automaton differs depending on the grammar with which we are parsing.", "labels": [], "entities": []}, {"text": "The evaluation grammars and fine-to-coarse mappings are available as supplementary material for this paper, and our coarseto-fine parser is implemented as part of the Alto Toolkit.", "labels": [], "entities": [{"text": "Alto Toolkit", "start_pos": 167, "end_pos": 179, "type": "DATASET", "confidence": 0.918443888425827}]}, {"text": "First, we reproduce the known result that CTF parsing speeds up PCFG parsing.", "labels": [], "entities": [{"text": "CTF parsing", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.6551945209503174}, {"text": "PCFG parsing", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.7163230776786804}]}, {"text": "We read off a PCFG from the parse trees of the WSJ portion of the Penn Treebank (Sections 02-21), using the gold POS tags as terminal symbols; binarize it with the \"inside\" binarization strategy of; and convert it to an IRTG.", "labels": [], "entities": [{"text": "PCFG from the parse trees of the WSJ portion of the Penn Treebank (Sections 02-21)", "start_pos": 14, "end_pos": 96, "type": "DATASET", "confidence": 0.8214299959294936}]}, {"text": "This yields an IRTG grammar with 23817 rules and 8202 nonterminals, of which 8131 were created during binarization.", "labels": [], "entities": []}, {"text": "We then parsed the sentences in Section 23 of up to 40 words, both without pruning and with the CTF parser (longer sentences were infeasible with the unpruned parser).", "labels": [], "entities": [{"text": "Section", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9203628897666931}]}, {"text": "For CTF we used the fourlevel fine-to-coarse mapping from and a threshold of \u03b8 = 10 \u22125 . We also apply the fine-to-coarse mapping to the nonterminals introduced during binarization.", "labels": [], "entities": [{"text": "CTF", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.755769670009613}]}, {"text": "If the nonterminal 'NP' is mapped to 'HP' for the level k, then a nonterminal 'NPNP' -which is created during binarization to represent a sequence of two 'NP' children, signified with the notation -would correspond to a nonterminal 'HPHP' on the level, binarized with the \"inside\" strategy and converted to an IRTG.", "labels": [], "entities": []}, {"text": "To avoid data sparsity issues, we also evaluated the grammar on Section 00, thus f-scores should be read with care.", "labels": [], "entities": [{"text": "Section 00", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.8172652721405029}]}, {"text": "We used a variant of the four-level fineto-coarse mapping from, which always preserves the distinction between nonterminals at the root of initial trees and those at the root of auxiliary trees.", "labels": [], "entities": []}, {"text": "We tried the threshold values \u03b8 = 10 \u22125 and \u03b8 = 10 \u22129 . The results are shown in (bottom).", "labels": [], "entities": []}, {"text": "Unexpectedly, while CTF pruning with these thresholds already reduces the f-score, the parsing time barely improves.", "labels": [], "entities": [{"text": "f-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9870734214782715}, {"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.70234614610672}]}, {"text": "We also evaluated CTF on parsing Abstract Meaning Representation (AMR) graphs with hyperedge replacement grammars (HRGs,).", "labels": [], "entities": [{"text": "parsing Abstract Meaning Representation (AMR) graphs", "start_pos": 25, "end_pos": 77, "type": "TASK", "confidence": 0.8772737011313438}]}, {"text": "We use the HRG grammar of, which was induced from the \"Little Prince\" AMR corpus (Banarescu et al., 2013) and converted to an IRTG.", "labels": [], "entities": [{"text": "Little Prince\" AMR corpus", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.7350728213787079}]}, {"text": "This grammar describes how a graph can be constructed from atomic parts.", "labels": [], "entities": []}, {"text": "It uses complex nonterminal symbols such as N 0 {0, 1, 2}, indicating that the nonterminal should derive a subgraph with three sources 0, 1, and 2 (these describe nodes at which further edges can be added during the derivation), and the 1-source should be the AMR's \"root\" node.", "labels": [], "entities": []}, {"text": "The symbol before the curly brack-: Results for HRG parsing, with mean percent best found, runtime (in ms), invhom rules used in the chart, and saturation per graph.", "labels": [], "entities": [{"text": "HRG parsing", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.7960401475429535}]}, {"text": "ets can be one of N 0 or N 1 , to allow the grammar to make finer distinctions beyond the source information.", "labels": [], "entities": []}, {"text": "In total, the grammar has 39 nonterminals and 13681 rules.", "labels": [], "entities": []}, {"text": "We tried a number of fine-to-coarse mappings in parsing Groschwitz et al.'s corpus.", "labels": [], "entities": [{"text": "parsing Groschwitz et al.'s corpus", "start_pos": 48, "end_pos": 82, "type": "DATASET", "confidence": 0.795383632183075}]}, {"text": "The \"Unsplit\" mapping removes the distinction between N 0 and N 1 , so the above nonterminal coarsifies to N {0, 1, 2}.", "labels": [], "entities": []}, {"text": "\"Unroot\" removes the marking of the root source, i.e. coarsifies to N 0 {0, 1, 2}.", "labels": [], "entities": []}, {"text": "\"Both\" applies the two in sequence, i.e. coarsifies to N {0, 1, 2}.", "labels": [], "entities": []}, {"text": "As a sanity check, we also looked at a \"Self\" mapping, which \"coarsifies\" every nonterminal to itself.", "labels": [], "entities": []}, {"text": "We used an aggressive pruning threshold of \u03b8 = 10 \u22122 . The results are shown in.", "labels": [], "entities": []}, {"text": "Because we do not have access to gold standard derivation trees in this dataset, we report the percentage of sentences on which a CTF parser produced the same Viterbi derivation tree as the unpruned parser (\"best %\").", "labels": [], "entities": []}, {"text": "We find that the Unsplit and Unroot mappings produce high-quality parses.", "labels": [], "entities": [{"text": "Unsplit", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.8577010631561279}]}, {"text": "However, in striking contrast to the PCFG case, all nontrivial mappings make the parser slower than the unpruned one -in the case of Unroot and Both, dramatically so.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for PCFG and TAG parsing, with  mean runtime (in ms), invhom rules used in the  chart, and saturation per sentence.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.860056459903717}, {"text": "TAG parsing", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.7123237699270248}]}, {"text": " Table 2: Results for HRG parsing, with mean  percent best found, runtime (in ms), invhom rules  used in the chart, and saturation per graph.", "labels": [], "entities": [{"text": "HRG parsing", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.8565284311771393}]}]}