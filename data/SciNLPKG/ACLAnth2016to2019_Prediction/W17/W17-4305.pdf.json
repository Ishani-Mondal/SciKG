{"title": [{"text": "Syntax Aware LSTM model for Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7497663497924805}]}], "abstractContent": [{"text": "In Semantic Role Labeling (SRL) task, the tree structured dependency relation is rich in syntax information, but it is not well handled by existing models.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL) task", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.8290288022586277}]}, {"text": "In this paper , we propose Syntax Aware Long Short Time Memory (SA-LSTM).", "labels": [], "entities": [{"text": "Syntax Aware Long Short Time Memory", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.6264019111792246}]}, {"text": "The structure of SA-LSTM changes according to dependency structure of each sentence, so that SA-LSTM can model the whole tree structure of dependency relation in an architecture engineering way.", "labels": [], "entities": []}, {"text": "Experiments demonstrate that on Chinese Proposition Bank (CPB) 1.0, SA-LSTM improves F 1 by 2.06% than ordinary bi-LSTM with feature engineered dependency relation information , and gives state-of-the-art F 1 of 79.92%.", "labels": [], "entities": [{"text": "Chinese Proposition Bank (CPB) 1.0", "start_pos": 32, "end_pos": 66, "type": "DATASET", "confidence": 0.947842172213963}, {"text": "F 1", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9945014417171478}, {"text": "F 1", "start_pos": 205, "end_pos": 208, "type": "METRIC", "confidence": 0.9969685971736908}]}, {"text": "On English CoNLL 2005 dataset, SA-LSTM brings improvement (2.1%) to bi-LSTM model and also brings slight improvement (0.3%) when added to the state-of-the-art model.", "labels": [], "entities": [{"text": "English CoNLL 2005 dataset", "start_pos": 3, "end_pos": 29, "type": "DATASET", "confidence": 0.9450477063655853}, {"text": "SA-LSTM", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.7970286011695862}]}], "introductionContent": [{"text": "The task of Semantic Role Labeling (SRL) is to recognize arguments of a given predicate in a sentence and assign semantic role labels.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.8378847241401672}]}, {"text": "Many NLP works such as machine translation) benefit from SRL because of the semantic structure it provides.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7757140398025513}, {"text": "SRL", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9752112030982971}]}, {"text": "shows a sentence with semantic role label.", "labels": [], "entities": []}, {"text": "Dependency relation is considered important for SRL task), since it can provide rich structure and syntax information for SRL.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 48, "end_pos": 56, "type": "TASK", "confidence": 0.9125551283359528}, {"text": "SRL", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9675085544586182}]}, {"text": "At the bottom of shows dependency of the sentence.", "labels": [], "entities": []}, {"text": "* Corresponding Author) with semantic role labels and dependency.", "labels": [], "entities": []}, {"text": "Traditional methods ( do classification according to manually designed features.", "labels": [], "entities": []}, {"text": "Feature engineering requires expertise and is labor intensive.", "labels": [], "entities": [{"text": "Feature engineering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8989073038101196}]}, {"text": "Recent works based on Recurrent Neural Network (RNN) () extract features automatically, and significantly outperform traditional methods.", "labels": [], "entities": []}, {"text": "However, because RNN methods treat language as sequential data, they fail to integrate the tree structured dependency into RNN.", "labels": [], "entities": []}, {"text": "We propose Syntax Aware Long Short Time Memory (SA-LSTM) to directly model complex tree structure of dependency relation in an architecture engineering way.", "labels": [], "entities": []}, {"text": "Architecture of SA-LSTM is shown in.", "labels": [], "entities": []}, {"text": "SA-LSTM is based on bidirectional LSTM (bi-LSTM).", "labels": [], "entities": []}, {"text": "In order to model the whole dependency tree, we add additional directed connections between dependency related words in bi-LSTM.", "labels": [], "entities": []}, {"text": "SA-LSTM integrates the whole dependency tree directly into the model in an architecture engineering way.", "labels": [], "entities": []}, {"text": "Also, to take dependency relation type into account, we introduce trainable weights for different types of dependency relation.", "labels": [], "entities": []}, {"text": "The weights can be trained to indicate importance of a dependency type.", "labels": [], "entities": []}, {"text": "SA-LSTM is able to directly model the whole tree structure of dependency relation in an architecture engineering way.", "labels": [], "entities": []}, {"text": "Experiments show that SA-LSTM can model dependency relation better than traditional feature engineering way.", "labels": [], "entities": []}, {"text": "SA-LSTM gives state of the art F 1 on CPB 1.0 and also shows improvement on English CoNLL 2005 dataset.", "labels": [], "entities": [{"text": "F 1", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9907452166080475}, {"text": "CPB 1.0", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9751567542552948}, {"text": "English CoNLL 2005 dataset", "start_pos": 76, "end_pos": 102, "type": "DATASET", "confidence": 0.9245439022779465}]}], "datasetContent": [{"text": "In order to compare with previous Chinese SRL works, we choose to do experiment on CPB 1.0.", "labels": [], "entities": [{"text": "CPB 1.0", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.954683244228363}]}, {"text": "We also follow the same data setting as previous Chinese SRL work did.", "labels": [], "entities": []}, {"text": "Pre-trained 1 word embeddings are tested on SA-LSTM and shows improvement.", "labels": [], "entities": []}, {"text": "For English SRL, we test on CoNLL 2005 dataset.", "labels": [], "entities": [{"text": "SRL", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.6790117621421814}, {"text": "CoNLL 2005 dataset", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9796690940856934}]}, {"text": "We use Stanford Parser) to get dependency relation.", "labels": [], "entities": []}, {"text": "The training set of Chinese parser overlaps apart of CPB 1.0 test set, so we retrained the parser.", "labels": [], "entities": [{"text": "CPB 1.0 test set", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9464496225118637}]}, {"text": "Dimension of hyper parameters are tuned according to development set.", "labels": [], "entities": []}, {"text": "n 1 = 200, n h = 100, n 2 = 200, n 3 = 100, learning rate = 0.001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9715949892997742}]}], "tableCaptions": [{"text": " Table 1: Results comparison on CPB 1.0", "labels": [], "entities": [{"text": "CPB 1.0", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9560554325580597}]}, {"text": " Table 2: Results on English CoNLL 2005", "labels": [], "entities": [{"text": "English CoNLL 2005", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.907272736231486}]}]}