{"title": [{"text": "Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent applications of neural language models have led to an increased interest in the automatic generation of natural language.", "labels": [], "entities": [{"text": "automatic generation of natural language", "start_pos": 87, "end_pos": 127, "type": "TASK", "confidence": 0.8235095024108887}]}, {"text": "However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal.", "labels": [], "entities": []}, {"text": "Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text.", "labels": [], "entities": []}, {"text": "We focus on a specific aspect of neural language generation: its ability to reproduce authorial writing styles.", "labels": [], "entities": [{"text": "neural language generation", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.798829197883606}]}, {"text": "Using established models for authorship attribution, we empirically assess the stylistic qualities of neurally generated text.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7740508913993835}]}, {"text": "In comparison to conventional language models, neural models generate fuzzier text that is relatively harder to attribute correctly.", "labels": [], "entities": []}, {"text": "Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "In his landmark paper 'Computing Machinery and Intelligence', Turing (1950) quoted Jefferson's 'The Mind of: 'Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain'.", "labels": [], "entities": []}, {"text": "Strikingly, these early pioneers of modern AI considered the conscious creation of literature as a significant milestone on the long road towards general AI.", "labels": [], "entities": []}, {"text": "In recent years, the automated generation of text, such as literature, has received a significant impetus from research in the field of neural language modeling.", "labels": [], "entities": [{"text": "neural language modeling", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.6828963657220205}]}, {"text": "A variety of recent studies have demonstrated that neural language models can be used to synthesize new (literary) text, even at the character-level.", "labels": [], "entities": []}, {"text": "To a surprising extent, neurally generated text seems to make an authentic impression on readers, due to its ability to mimic certain properties of the text on which it was trained, without it degrading into in a mere reproduction or patchwork of verbatim passages in it.", "labels": [], "entities": []}, {"text": "In one particularly visible blog post, Karpathy (2015) demonstrated how a relatively simple character-level recurrent neural network, when trained on Shakespeare's oeuvre, was able to generate new, artificial text which, certainly in the eyes of non-experts, undeniably displayed some Shakespearean qualities.", "labels": [], "entities": []}, {"text": "This blog has inspired a wide array of other applicationsranging from cooking recipes) to Bach's sonatas.", "labels": [], "entities": []}, {"text": "Much of this work has so far been published in the online blogosphere and the assessment of the quality of neurally generated text has often remained fairly informal and anecdotal, apart from a number of more empirically oriented studies, for instance in the field of hiphop lyric generation (.", "labels": [], "entities": [{"text": "hiphop lyric generation", "start_pos": 268, "end_pos": 291, "type": "TASK", "confidence": 0.6772789259751638}]}, {"text": "In this paper, we report an attempt at a systematic assessment of the properties of neurally generated text in the context of style-based authorship attribution in stylometry.", "labels": [], "entities": []}, {"text": "We address the following research questions: (1) To which extent is the text, neurally generated on the basis of a single author's oeuvre, still attributable to the original input author? and (2) To which extent is the neural generation of text useful for training data augmentation in stylometry, e.g. for authors for whom little reference data is available?", "labels": [], "entities": [{"text": "training data augmentation", "start_pos": 256, "end_pos": 282, "type": "TASK", "confidence": 0.6487810711065928}]}, {"text": "Below, we first present the model architectures underlying our text generation, comparing a modern neural architecture to a more conventional ngram-based language model.", "labels": [], "entities": [{"text": "text generation", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7184506356716156}]}, {"text": "Next, we describe the Latin data set which we will use (Patrologia Latina) and discuss our experimental set-up (authorship attribution).", "labels": [], "entities": [{"text": "Latin data set", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.7816325426101685}, {"text": "Patrologia Latina)", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.8384945193926493}]}, {"text": "We goon to present our attribution results; in the discussion section, we in-terpret and visualize these results.", "labels": [], "entities": []}, {"text": "We conclude by pointing out viable future improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Patrologia Latina (PL) is a corpus containing texts of Latin ecclesiastical writers in 221 volumes ranging a time span of 10 centuries, from Late Antiquity to the High Middle Ages (3rd-13th century).", "labels": [], "entities": [{"text": "Patrologia Latina (PL)", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.4996328890323639}]}, {"text": "It was first published in two series halfway the 19th century by Jacques-Paul Migne, who mainly based the texts off of 17th and 18th-century prints.", "labels": [], "entities": []}, {"text": "Its digitized version is available since 1993, and it has remained one of the most sizable Latin corpora online (\u00b1113M words).", "labels": [], "entities": []}, {"text": "Performing this experiment on the PL, and not on an English corpus, for instance, has been a conscious decision to raise the bar.", "labels": [], "entities": []}, {"text": "It has been observed that state-of-the-art AA on an inflected language such as Latin yields poorer results when it is reliant on most frequent words ().", "labels": [], "entities": []}, {"text": "Moreover, the Latin that has comedown to us from the 1st century AD onwards is an institutionalized literary language, hardly a natural language, showing only far resemblance, or occasionally no resemblance at all, to the writer's mother tongue.", "labels": [], "entities": []}, {"text": "Tracing stylistic properties within a heavily formalized language, and attempting to resuscitate these through generation, is therefore challenging.", "labels": [], "entities": []}, {"text": "An additional obstacle for both language generation as AA is that many of the PL's authors cite from similar, authoritative sources such as the Bible or the church fathers' precursory texts, thereby having in common an ecclesiastical vocabulary that could complicate the detection of stable writing style patterns.", "labels": [], "entities": []}, {"text": "Not all authors in the PL have been equally prolific.", "labels": [], "entities": [{"text": "PL", "start_pos": 23, "end_pos": 25, "type": "DATASET", "confidence": 0.3983282446861267}]}, {"text": "These circumstances considerably limit the set of authors for whom our task is suited.", "labels": [], "entities": []}, {"text": "We set the condition that our text data include only texts by authors who dispose of at least 20 authentic, individual documents each.", "labels": [], "entities": []}, {"text": "As such we favored document counts over token counts, and lexical variety over mere word quantity.", "labels": [], "entities": []}, {"text": "A list of the 18 most prolific authors, their number of documents and the respective average length of these documents is given in.", "labels": [], "entities": []}, {"text": "It is not trivial to design an experiment that allows us to study the behavior of generated text in the context of AA.", "labels": [], "entities": []}, {"text": "shows the experimental setup which we propose, and in which we attempt to maximize the comparability of both generated and authentic data.", "labels": [], "entities": []}, {"text": "We start by splitting the full corpus into two equal-size document collections (stratified at the author level), \u03b1 and \u03c9.", "labels": [], "entities": []}, {"text": "Only \u03b1 will be used to train a LM, which then generates a third collection of synthetic documents.", "labels": [], "entities": []}, {"text": "For each author in \u03b1 and \u03c9, we aggregate all documents into a list of sentences per sub-corpus.", "labels": [], "entities": []}, {"text": "From these collections, we create 20 documents containing at least 5,000 words to create \u03b1 and \u03c9, through randomly sampling sentences (without replacement) from the author's sentence collection.", "labels": [], "entities": []}, {"text": "For the creation of \u00af \u03b1, we would also create 20 artificial 5,000-word documents, but this time through sampling new sentences from the LM.", "labels": [], "entities": []}, {"text": "This approach has its limitations, because we limit and balance the available data to a considerable extent.", "labels": [], "entities": []}, {"text": "Furthermore, the sampling procedure implies an underestimation in attribution performance, since it strips away all supra-sentential information.", "labels": [], "entities": []}, {"text": "Nevertheless, this setup guarantees that the authentic and generated corpora are maximally comparable in terms of number of documents, document length, topical diversity and style mixture-which is our focus in the present study.", "labels": [], "entities": []}, {"text": "Subsequently, 5 classification experiments are defined, where we train and and test on different 2-way combinations of the 3 datasets.", "labels": [], "entities": []}, {"text": "Ina first pair of experiments, < \u03b1, \u03c9 > and < \u03c9, \u03b1 >, we train and test a classifier on the authentic datasets to assess the classifier's performance under natural conditions.", "labels": [], "entities": []}, {"text": "(Note that we apply the classifier in both directions to account for any directionality artifacts.)", "labels": [], "entities": []}, {"text": "Ina third experiment, we train and test a classifier on the generated data only (< \u00af \u03b1, \u00af \u03b1 >) to establish to which extent the generated data preserves the data's stylistic structure at the author level (i.e. auto-classification).", "labels": [], "entities": []}, {"text": "Fourthly, we conduct an experiment where we train on the generated data in \u00af \u03b1 and test on the authentic data in \u03c9 (< \u00af \u03b1, \u03c9 >).", "labels": [], "entities": []}, {"text": "This allows us to verify whether the generated documents retain enough stylistic information to correctly attribute authentic documents.", "labels": [], "entities": []}, {"text": "Finally, we train a classifier on the authentic data in \u03c9 and test it on \u00af \u03b1: this setup (< \u03c9, \u00af \u03b1 >) allows to assess whether a classifier, trained on authentic data is still able to correctly attribute the generated materials.", "labels": [], "entities": []}, {"text": "In addition, we conduct a final experiment which can be characterized from the point of view of self-learning or co-learning)-a semi-supervised learning technique where a core of training data is expanded with examples from a related but unlabeled dataset that can be classified with high confidence by a classifier trained in the original labeled dataset.", "labels": [], "entities": []}, {"text": "In this experiment we compare the NGLM and RNNLM models with respect to their capacity to boost attribution performance by adding synthetic examples to the original training set-which might be a valuable strategy for real-life experiments.", "labels": [], "entities": [{"text": "NGLM", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9129277467727661}]}, {"text": "Specifically, we perform attribution on \u03c9 using a combination of \u03b1 and \u00af \u03b1 as training data (< \u03b1 + \u00af \u03b1, \u03c9 >).", "labels": [], "entities": []}, {"text": "Figure 2: Experimental setup.", "labels": [], "entities": []}, {"text": "\u03b1 and \u03c9 refer to 50% splits of the full corpus.", "labels": [], "entities": []}, {"text": "\u00af \u03b1 refers to the generated dataset (cf. dashed line).", "labels": [], "entities": []}, {"text": "Each classifier symbol refers to a classification experiment using the data at the arrow's source (first subscript) for training and the data at the arrow's target (second subscript) for testing (note that training only has to be performed 3 times, one per dataset).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mean F1, Precision (P) and Recall (R)  scores for all classification experiments.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9906189441680908}, {"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.7545418739318848}, {"text": "Precision (P)", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9550581276416779}, {"text": "Recall (R)  scores", "start_pos": 37, "end_pos": 55, "type": "METRIC", "confidence": 0.9708834767341614}]}]}