{"title": [{"text": "UWat-Emote at EmoInt-2017: Emotion Intensity Detection using Affect Clues, Sentiment Polarity and Word Embeddings", "labels": [], "entities": [{"text": "UWat-Emote at EmoInt-2017", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.805790921052297}, {"text": "Emotion Intensity Detection", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.8800237576166788}]}], "abstractContent": [{"text": "This paper describes the UWaterloo affect prediction system developed for EmoInt-2017.", "labels": [], "entities": [{"text": "UWaterloo affect prediction", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.782884438832601}, {"text": "EmoInt-2017", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9536595940589905}]}, {"text": "We delve into our feature selection approach for affect intensity, affect presence, sentiment intensity and sentiment presence lexica alongside pre-trained word embeddings, which are utilized to extract emotion intensity signals from tweets in an ensemble learning approach.", "labels": [], "entities": []}, {"text": "The system employs emotion specific model training, and utilizes distinct models for each of the emotion corpora in isolation.", "labels": [], "entities": []}, {"text": "Our system utilizes gradient boosted regression as the primary learning technique to predict the final emotion intensities .", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of this EmoInt task is to predict the intensity of affect expressions in a selection of tweets.", "labels": [], "entities": []}, {"text": "The intensity scores are floating point values between 0 and 1, representing low and high intensities of the emotion being expressed, respectively.", "labels": [], "entities": []}, {"text": "The emotions analyzed in this shared task are anger, fear, joy and sadness).", "labels": [], "entities": []}, {"text": "This paper describes the techniques used to clean tweets, build lexical features, find optimal combinations of features to produce a final vector representation of a tweet and train generalized regression, gradient boosted regression and neural-network computed regression models to fit the vector representations to the intensity scores.", "labels": [], "entities": []}, {"text": "The following sections describe each of these processes, followed by an enumeration of the parameters that worked in favor of the bestperforming models, a discussion of the results and potential approaches to boost model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9845751523971558}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training Cross-validated Accuracy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9571171998977661}]}, {"text": " Table 2: Testing Accuracy -Features + ML", "labels": [], "entities": [{"text": "Accuracy -Features + ML", "start_pos": 18, "end_pos": 41, "type": "METRIC", "confidence": 0.7697290301322937}]}, {"text": " Table 3: Testing Accuracy: Pre-trained Embedding Features + Shallow Neural Network", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9642357230186462}]}]}