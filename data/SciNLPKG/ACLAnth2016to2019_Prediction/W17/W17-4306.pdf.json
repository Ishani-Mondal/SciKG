{"title": [{"text": "Spatial Language Understanding with Multimodal Graphs using Declarative Learning based Programming", "labels": [], "entities": [{"text": "Spatial Language Understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8874945243199667}]}], "abstractContent": [{"text": "This work is on a previously formalized semantic evaluation task of spatial role labeling (SpRL) that aims at extraction of formal spatial meaning from text.", "labels": [], "entities": [{"text": "semantic evaluation task of spatial role labeling (SpRL)", "start_pos": 40, "end_pos": 96, "type": "TASK", "confidence": 0.7519826054573059}, {"text": "extraction of formal spatial meaning from text", "start_pos": 110, "end_pos": 156, "type": "TASK", "confidence": 0.7872441411018372}]}, {"text": "Here, we report the results of initial efforts towards exploiting visual information in the form of images to help spatial language understanding.", "labels": [], "entities": [{"text": "spatial language understanding", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.6628270844618479}]}, {"text": "We discuss the way of designing new models in the framework of declarative learning-based programming (DeLBP).", "labels": [], "entities": []}, {"text": "The DeLBP framework facilitates combining modalities and representing various data in a unified graph.", "labels": [], "entities": []}, {"text": "The learning and inference models exploit the structure of the unified graph as well as the global first order domain constraints beyond the data to predict the semantics which forms a structured meaning representation of the spatial context.", "labels": [], "entities": []}, {"text": "Continuous representations are used to relate the various elements of the graph originating from different modalities.", "labels": [], "entities": []}, {"text": "We improved over the state-of-the-art results on SpRL.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spatial language understanding is important in many real-world applications such as geographical information systems, robotics, and navigation when the robot has a camera on the head and receives instructions about grabbing objects and finding their locations, etc.", "labels": [], "entities": [{"text": "Spatial language understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9206132491429647}]}, {"text": "One approach towards spatial language understanding is to map the natural language to a formal spatial representation appropriate for spatial reasoning.", "labels": [], "entities": [{"text": "spatial language understanding", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6657443344593048}]}, {"text": "The previous research on spatial role labeling ( and ISOSpace ( aimed at formalizing such a problem and providing machine learning solutions to find such a mapping in a data-driven way ().", "labels": [], "entities": [{"text": "spatial role labeling", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.6554721295833588}, {"text": "ISOSpace", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.61424320936203}]}, {"text": "Such extractions are made from available textual resources.", "labels": [], "entities": []}, {"text": "However, spatial semantics are the most relevant and useful information for visualization of the language and, consequently, accompanying visual information could help disambiguation and extraction of the spatial meaning from text.", "labels": [], "entities": []}, {"text": "Recently, there has been a large community effort to prepare new resources for combining vision and language data ( though not explicitly focused on formal spatial semantic representations.", "labels": [], "entities": []}, {"text": "The current tasks are mostly image centered such as image captioning, that is, generating image descriptions (, image retrieval using textual descriptions, or visual question answering (.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7122828513383865}, {"text": "image retrieval", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.7413441836833954}, {"text": "question answering", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7170191258192062}]}, {"text": "In this work, we consider a different problem, that is, how images can help in the extraction of a structured spatial meaning representation from text.", "labels": [], "entities": [{"text": "extraction of a structured spatial meaning representation from text", "start_pos": 83, "end_pos": 150, "type": "TASK", "confidence": 0.6539165940549638}]}, {"text": "This task has been recently proposed as a CLEF pilot task 1 , the data is publicly available and the task overview will be published ().", "labels": [], "entities": []}, {"text": "Our interest informal meaning representation distinguishes our work from other vision and language tasks and the choice of the data since our future goal is to integrate explicit qualitative spatial reasoning models into learning and spatial language understanding.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7570231556892395}, {"text": "spatial language understanding", "start_pos": 234, "end_pos": 264, "type": "TASK", "confidence": 0.6650039752324423}]}, {"text": "The contribution of this paper is a) we report results on combining vision and language that extend and improve the spatial role labeling state-ofthe-art models, b) we model the task in the framework of declarative learning based programming and show its expressiveness in representing such complex structured output tasks.", "labels": [], "entities": []}, {"text": "DeLBP provides the possibility of seamless integration of heteroge- Figure 1: Given spatial ontology (  neous data in addition to considering domain ontological and linguistic knowledge in learning and inference.", "labels": [], "entities": [{"text": "DeLBP", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9154651761054993}]}, {"text": "To improve the state-of-the-art results in SpRL and exploiting the visual information we rely on existing techniques for continuous representations of image segments and text phrases, and measuring similarity to find the best alignments.", "labels": [], "entities": []}, {"text": "The challenging aspect of this work is that the formal representation of the textual spatial semantics is very different from the raw spatial information extracted from image segments using their geometrical relationships.", "labels": [], "entities": []}, {"text": "To alleviate this problem the embeddings of phrases as well as the embeddings of the relations helped connecting the two modalities.", "labels": [], "entities": []}, {"text": "This approach helped improving the state of the art results on spatial role labeling () for recognizing spatial roles.", "labels": [], "entities": [{"text": "spatial role labeling", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7056743701299032}]}], "datasetContent": [{"text": "In this section, we experimentally show the influence of our new features, constraints, phrase embeddings and image embeddings and compare them with the previous research.", "labels": [], "entities": []}, {"text": "We use the SemEval-2012 shared tasks data () that consists of textual descriptions of 613 images originally selected from the IAPR TC-12 dataset (), provided by the CLEF organization.", "labels": [], "entities": [{"text": "SemEval-2012 shared tasks data", "start_pos": 11, "end_pos": 41, "type": "DATASET", "confidence": 0.6156536862254143}, {"text": "IAPR TC-12 dataset", "start_pos": 126, "end_pos": 144, "type": "DATASET", "confidence": 0.9370404084523519}, {"text": "CLEF organization", "start_pos": 165, "end_pos": 182, "type": "DATASET", "confidence": 0.9157034456729889}]}, {"text": "In the previous works only the text part of this data has been used in various shared task settings () and with a variation in the annotation schemes.", "labels": [], "entities": []}, {"text": "This data includes about 1213 sentence containing 20,095 words with 1706 annotated relations.", "labels": [], "entities": []}, {"text": "We preferred this data compared to more recent related corpora () for two main reasons.", "labels": [], "entities": []}, {"text": "First is the availability of the aligned images and the second is the static nature of the most spatial descriptions.", "labels": [], "entities": []}, {"text": "As mentioned before, we used Saul (  framework that allows flexible relational feature extraction as well as declarative formulation of the global inference.", "labels": [], "entities": [{"text": "relational feature extraction", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.6840167244275411}]}, {"text": "We extend Saul's basic data structures and sensors to be able to work with multimodal data and to populate raw as well as annotated text easily into a Saul multimodal data-model.", "labels": [], "entities": []}, {"text": "The code is available in Github.", "labels": [], "entities": [{"text": "Github", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.9163450002670288}]}, {"text": "We face the following challenges when solving this problem: the training data is very small; the annotation schemes for the text and images are very different and they have been annotated independently; the image annotations regarding the spatial relations include very naively generated exhaustively pairwise relations which are not very relevant to what human describes by viewing the images.", "labels": [], "entities": []}, {"text": "We try to address these challenges by feature engineering, exploiting global constraints and using continuous representations for text and image segments.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7876673936843872}]}, {"text": "We report the results of the following models in BM: This is our baseline model built with extensive feature engineering as described in Section 3.2.1.", "labels": [], "entities": [{"text": "BM", "start_pos": 49, "end_pos": 51, "type": "DATASET", "confidence": 0.7446739077568054}]}, {"text": "We train independent classifiers for the roles and relations classification in this model; BM+C: This is the BM that uses global constraints to impose, for example, the integrity and consistency of the assignments of the roles and relation labels at the sentence level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results on SemEval-2012 data including images. BM: Baseline Model, C: Con- straints, E: Text Embeddings, I: Image Embeddings.", "labels": [], "entities": [{"text": "BM", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9467850923538208}]}]}