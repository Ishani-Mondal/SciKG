{"title": [{"text": "Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management", "labels": [], "entities": [{"text": "Sample-efficient Actor-Critic Reinforcement Learning", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7525944709777832}, {"text": "Dialogue Management", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8593385219573975}]}], "abstractContent": [{"text": "Deep reinforcement learning (RL) methods have significant potential for dialogue policy optimisation.", "labels": [], "entities": [{"text": "Deep reinforcement learning (RL)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7213485836982727}, {"text": "dialogue policy optimisation", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.8813808957735697}]}, {"text": "However, they suffer from a poor performance in the early stages of learning.", "labels": [], "entities": []}, {"text": "This is especially problematic for on-line learning with real users.", "labels": [], "entities": []}, {"text": "Two approaches are introduced to tackle this problem.", "labels": [], "entities": []}, {"text": "Firstly, to speedup the learning process, two sample-efficient neural networks algorithms: trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER) are presented.", "labels": [], "entities": [{"text": "experience replay (TRACER", "start_pos": 122, "end_pos": 147, "type": "METRIC", "confidence": 0.6299426406621933}]}, {"text": "For TRACER, the trust region helps to control the learning step size and avoid catastrophic model changes.", "labels": [], "entities": [{"text": "TRACER", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.879203200340271}]}, {"text": "For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speedup the convergence.", "labels": [], "entities": []}, {"text": "Both models employ off-policy learning with experience replay to improve sample-efficiency.", "labels": [], "entities": []}, {"text": "Secondly, to mitigate the cold start issue, a corpus of demonstration data is utilised to pre-train the models prior to on-line reinforcement learning.", "labels": [], "entities": []}, {"text": "Combining these two approaches, we demonstrate a practical approach to learning deep RL-based dialogue policies and demonstrate their effectiveness in a task-oriented information seeking domain.", "labels": [], "entities": [{"text": "RL-based dialogue policies", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.8516121308008829}]}], "introductionContent": [{"text": "Task-oriented Spoken Dialogue Systems (SDS) aim to assist users to achieve specific goals via speech, such as hotel booking, restaurant information and accessing bus-schedules.", "labels": [], "entities": []}, {"text": "These systems are typically designed according to a structured ontology (or a database schema), which defines the domain that the system can talk about.", "labels": [], "entities": []}, {"text": "The development of a robust SDS traditionally requires a substantial amount of hand-crafted rules combined with various statistical components.", "labels": [], "entities": [{"text": "SDS", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9652554988861084}]}, {"text": "This includes a spoken language understanding module (), a dialogue belief state tracker () to predict user intent and track the dialogue history, a dialogue policy ( to determine the dialogue flow, and a natural language generator () to convert conceptual representations into system responses.", "labels": [], "entities": [{"text": "dialogue belief state tracker", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.6010038554668427}]}, {"text": "Ina task-oriented SDS, teaching a system how to respond appropriately in all situations is nontrivial.", "labels": [], "entities": []}, {"text": "Traditionally, this dialogue management component has been designed manually using flow charts.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8260805904865265}]}, {"text": "More recently, it has been formulated as a planning problem and solved using reinforcement learning (RL) to optimise a dialogue policy through interaction with users (.", "labels": [], "entities": []}, {"text": "In this framework, the system learns by atrial and error process governed by a potentially delayed learning objective called the reward.", "labels": [], "entities": []}, {"text": "This reward is designed to encapsulate the desired behavioural features of the dialogue.", "labels": [], "entities": []}, {"text": "Typically it provides a positive reward for success plus a per turn penalty to encourage short dialogues.", "labels": [], "entities": []}, {"text": "To allow the system to be trained on-line, Bayesian sample-efficient learning algorithms have been proposed) which can learn policies from a minimal number of dialogues.", "labels": [], "entities": []}, {"text": "However, even with such methods, the initial performance is still relatively poor, and this can impact negatively on the user experience.", "labels": [], "entities": []}, {"text": "Supervised learning (SL) can also be used for dialogue action selection.", "labels": [], "entities": [{"text": "Supervised learning (SL)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7542418360710144}, {"text": "dialogue action selection", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.9210935235023499}]}, {"text": "In this case, the policy is trained to produce an appropriate response for any given dialogue state.", "labels": [], "entities": []}, {"text": "Wizard-of-Oz (WoZ) methods) have been widely used for collecting domain-specific training corpora.", "labels": [], "entities": [{"text": "collecting domain-specific training corpora", "start_pos": 54, "end_pos": 97, "type": "TASK", "confidence": 0.8192616403102875}]}, {"text": "Recently an emerging line of research has focused on training neural networkbased dialogue models, mostly in text-based systems (.", "labels": [], "entities": []}, {"text": "These systems are directly trained on past dialogues without detailed specification of the internal dialogue state.", "labels": [], "entities": []}, {"text": "However, there are two key limitations of using SL in SDS.", "labels": [], "entities": []}, {"text": "Firstly, the effect of selecting an action on the future course of the dialogue is not considered and this may result in sub-optimal behaviour.", "labels": [], "entities": []}, {"text": "Secondly, there will often be a large number of dialogue states which are not covered by the training data ().", "labels": [], "entities": []}, {"text": "Moreover, there is no reason to suppose that the recorded dialogue participants are acting optimally, especially in high noise levels.", "labels": [], "entities": []}, {"text": "These problems are exacerbated in larger domains where multi-step planning is needed.", "labels": [], "entities": []}, {"text": "In this paper, we propose a network-based approach to policy learning which combines the best of both SL-and RL-based dialogue management, and which capitalises on recent advances in deep RL (, especially off-policy algorithms (.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.8296359181404114}, {"text": "RL-based dialogue management", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.6984253426392873}]}, {"text": "The main contribution of this paper is two-fold: 1.", "labels": [], "entities": []}, {"text": "improving the sample-efficiency of actorcritic RL: trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER).", "labels": [], "entities": [{"text": "experience replay (TRACER", "start_pos": 82, "end_pos": 107, "type": "METRIC", "confidence": 0.6496060490608215}]}, {"text": "2. efficient utilisation of demonstration data for improved early stage policy learning.", "labels": [], "entities": [{"text": "early stage policy learning", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.640479065477848}]}, {"text": "The first part focusses primarily on increasing the RL learning speed.", "labels": [], "entities": [{"text": "RL learning", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.8884431719779968}]}, {"text": "For TRACER, trust regions are introduced to standard actor-critic to control the step size and thereby avoid catastrophic model changes.", "labels": [], "entities": [{"text": "TRACER", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.8686947226524353}]}, {"text": "For eNACER, the natural gradient identifies steepest ascent direction in policy space to ensure fast convergence.", "labels": [], "entities": []}, {"text": "Both models exploit the off-policy learning with experience replay (ER) to improve sample-efficiency.", "labels": [], "entities": [{"text": "experience replay (ER)", "start_pos": 49, "end_pos": 71, "type": "METRIC", "confidence": 0.8654099345207215}]}, {"text": "These are compared with various state-of-the-art RL methods.", "labels": [], "entities": [{"text": "RL", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9246106743812561}]}, {"text": "The second part aims to mitigate the cold start issue by using demonstration data to pre-train an RL model.", "labels": [], "entities": []}, {"text": "This resembles the training procedure adopted in recent game playing applications.", "labels": [], "entities": []}, {"text": "A key feature of this framework is that a single model is trained using both SL and RL with different training objectives but without modifying the architecture.", "labels": [], "entities": []}, {"text": "By combining the above, we demonstrate a practical approach to learning deep RL-based dialogue policies for new domains which can achieve competitive performance without significant detrimental impact on users.", "labels": [], "entities": [{"text": "RL-based dialogue", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.8794093728065491}]}], "datasetContent": [{"text": "Our experiments utilised the software tool-kit PyDial ( , which provides a platform for modular SDS.", "labels": [], "entities": [{"text": "PyDial", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.6758658289909363}]}, {"text": "The target application is a live telephone-based SDS providing restaurant information for the Cambridge (UK) area.", "labels": [], "entities": [{"text": "Cambridge (UK) area", "start_pos": 94, "end_pos": 113, "type": "DATASET", "confidence": 0.9359181880950928}]}, {"text": "The task is to learn a policy which manages the dialogue flow and delivers requested information to the user.", "labels": [], "entities": []}, {"text": "The domain consists of approximately 100 venues, each with 6 slots out of which 3 can be used by the system to constrain the search (food-type, area and price-range) and 3 are system-informable properties (phone-number, address and postcode) available once a database entity has been found.", "labels": [], "entities": []}, {"text": "The input for all models was the full dialogue belief state b of size 268 which includes the last system act and distributions over the user intention and the three requestable slots.", "labels": [], "entities": []}, {"text": "The output includes 14 restricted dialogue actions determining the system intent at the semantic level.", "labels": [], "entities": []}, {"text": "Combining the dialogue belief states and heuristic rules, it is then mapped into a spoken response using a natural language generator.", "labels": [], "entities": []}], "tableCaptions": []}