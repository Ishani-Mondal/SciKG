{"title": [{"text": "Tokyo Metropolitan University Neural Machine Translation System for WAT 2017", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6112805207570394}, {"text": "WAT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.970485270023346}]}], "abstractContent": [{"text": "In this paper, we describe our neural machine translation (NMT) system, which is based on the attention-based NMT (Luong et al., 2015) and uses long short-term memories (LSTM) as RNN.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.8428757190704346}]}, {"text": "We implemented beam search and ensemble decoding in the NMT system.", "labels": [], "entities": [{"text": "beam search", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.8389824032783508}, {"text": "NMT", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8180700540542603}]}, {"text": "The system was tested on the 4th Workshop on Asian Translation (WAT 2017) (Nakazawa et al., 2017) shared tasks.", "labels": [], "entities": [{"text": "Asian Translation (WAT 2017)", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.8144528369108835}]}, {"text": "In our experiments, we participated in the scientific paper subtasks and attempted Japanese-English, English-Japanese, and Japanese-Chinese translation tasks.", "labels": [], "entities": [{"text": "Japanese-Chinese translation", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.6892495453357697}]}, {"text": "The experimental results showed that implementation of beam search and ensemble decoding can effectively improve the translation quality.", "labels": [], "entities": [{"text": "beam search", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.8146106600761414}]}], "introductionContent": [{"text": "Recently, neural machine translation (NMT) has gained popularity in the field of machine translation.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8231426874796549}, {"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7828912436962128}]}, {"text": "The conventional encoder-decoder NMT) uses two recurrent neural networks (RNN); one is an encoder, which encodes a source sequence into a fixed-length vector; the other is a decoder, which decodes this vector into a target sequence.", "labels": [], "entities": []}, {"text": "Attention-based NMT () can predict output words by using the weights of each hidden state of the encoder as the context vector, thereby improving the adequacy of the translation.", "labels": [], "entities": []}, {"text": "Despite the success of attention-based models, several open questions remain in NMT.", "labels": [], "entities": []}, {"text": "In general, a unique output word is predicted at each time step.", "labels": [], "entities": []}, {"text": "Therefore, if a wrong word is predicted, subsequent words will not be correctly output.", "labels": [], "entities": []}, {"text": "To enable better predictions, best practices such as beam search and ensemble decoding are recommended to improve the robustness of the predictions.", "labels": [], "entities": [{"text": "beam search", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8815164864063263}]}, {"text": "Beam search keeps better hypotheses during decoding, while ensemble decoding reduces the variance of output during decoding.", "labels": [], "entities": []}, {"text": "In this paper, we describe the NMT system that was tested on the shared tasks at 4th Workshop on Asian Translation (WAT 2017) (.", "labels": [], "entities": [{"text": "Asian Translation (WAT 2017)", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.7711424181858698}]}, {"text": "We implemented beam search and ensemble decoding in our NMT system.", "labels": [], "entities": [{"text": "beam search", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.8431992530822754}]}, {"text": "We applied our NMT system to Japanese-English, EnglishJapanese, and Japanese-Chinese scientific paper translation subtasks.", "labels": [], "entities": [{"text": "Japanese-Chinese scientific paper translation subtasks", "start_pos": 68, "end_pos": 122, "type": "TASK", "confidence": 0.6433771908283233}]}, {"text": "The experimental results show that beam search and ensemble decoding improve the translation accuracy by 3.55 points in Japanese-English translation and 3.28 points in English-Japanese translation in terms of BLEU () scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9702212810516357}, {"text": "BLEU", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.9991668462753296}]}], "datasetContent": [{"text": "We experimented our NMT system on JapaneseEnglish, English-Japanese, and Japanese-Chinese scientific paper translation subtasks.", "labels": [], "entities": [{"text": "Japanese-Chinese scientific paper translation subtasks", "start_pos": 73, "end_pos": 127, "type": "TASK", "confidence": 0.6429632127285003}]}, {"text": "We used the Japanese-English and JapaneseChinese parallel corpora in Asian Scientific Paper Excerpt Corpus (ASPEC) ().", "labels": [], "entities": [{"text": "Asian Scientific Paper Excerpt Corpus (ASPEC)", "start_pos": 69, "end_pos": 114, "type": "DATASET", "confidence": 0.7869813181459904}]}, {"text": "As regards the Japanese-English parallel corpus, Japanese sentences were segmented by the morphological analyzer MeCab 1 (version 0.996, IPADIC) and English sentences were tokenized by tokenizer.perl of Moses 2 . On the other hand, as regards the Japanese-Chinese parallel corpus, Japanese and Chinese sentences were tokenized by SentencePiece 3 . The vocabulary size of the tokenizer was set to 50,000.", "labels": [], "entities": []}, {"text": "As regards the training data in Japanese-English parallel corpus, we used only the first 1.5 million sentences sorted by sentence-alignment similarity; sentences with more than 60 words were excluded.", "labels": [], "entities": []}, {"text": "On the other hand, as regards the training data in Japanese-Chinese parallel corpus, we used all the sentences.", "labels": [], "entities": []}, {"text": "shows the numbers of the sentences in each parallel corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Japanese-English translation results.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.5741220414638519}]}, {"text": " Table 3: English-Japanese translation results.", "labels": [], "entities": [{"text": "English-Japanese translation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.5430321842432022}]}, {"text": " Table 4: Examples of outputs of Japanese-English translation.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.6447144895792007}]}]}