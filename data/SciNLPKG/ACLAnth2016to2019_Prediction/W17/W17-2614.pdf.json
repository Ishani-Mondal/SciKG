{"title": [{"text": "DocTag2Vec: An Embedding Based Multi-label Learning Approach for Document Tagging", "labels": [], "entities": [{"text": "Document Tagging", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.8196237981319427}]}], "abstractContent": [{"text": "Tagging news articles or blog posts with relevant tags from a collection of prede-fined ones is coined as document tagging in this work.", "labels": [], "entities": [{"text": "Tagging news articles or blog posts with relevant tags", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8981879949569702}, {"text": "document tagging", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7073502987623215}]}, {"text": "Accurate tagging of articles can benefit several downstream applications such as recommendation and search.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel yet simple approach called DocTag2Vec to accomplish this task.", "labels": [], "entities": [{"text": "DocTag2Vec", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.9056107401847839}]}, {"text": "We substantially extend Word2Vec and Doc2Vec-two popular models for learning distributed representation of words and documents.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9562848210334778}, {"text": "learning distributed representation of words and documents", "start_pos": 68, "end_pos": 126, "type": "TASK", "confidence": 0.7477690279483795}]}, {"text": "In DocTag2Vec, we simultaneously learn the representation of words, documents, and tags in a joint vector space during training, and employ the simple k-nearest neighbor search to predict tags for unseen documents.", "labels": [], "entities": []}, {"text": "In contrast to previous multi-label learning methods, DocTag2Vec directly deals with raw text instead of provided feature vector, and in addition, enjoys advantages like the learning of tag representation , and the ability of handling newly created tags.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of our approach, we conduct experiments on several datasets and show promising results against state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Every hour, several thousand blog posts are actively shared on social media; for example, blogging sites such as Tumblr 1 had more than 70 billion posts by January 2014 across different communities (.", "labels": [], "entities": []}, {"text": "In order to reach * This work was done when the author was an intern at Yahoo.", "labels": [], "entities": []}, {"text": "1 tumblr.com right audience or community, authors often assign keywords or \"#tags\" (hashtags) to these blog posts.", "labels": [], "entities": []}, {"text": "Besides being topic-markers, it was shown that hashtags also serve as group identities, and as brand labels.", "labels": [], "entities": []}, {"text": "On Tumblr, authors are allowed to create their own tags or choose existing tags to label their blog.", "labels": [], "entities": []}, {"text": "Creating or choosing tags for maximum outreach can be a tricky task and authors may not be able to assign all the relevant tags.", "labels": [], "entities": []}, {"text": "To alleviate this problem, algorithm-driven document tagging has emerged as a potential solution in recent times.", "labels": [], "entities": [{"text": "document tagging", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6780758947134018}]}, {"text": "Automatically tagging these blogs has several downstream applications, e.g., blog search, cluster similar blogs, show topics associated with trending tags, and personalization of blog posts.", "labels": [], "entities": []}, {"text": "For better user engagement, the personalization algorithm could match user interests with the tags associated with a blog post.", "labels": [], "entities": []}, {"text": "From machine learning perspective, document tagging is by nature a multi-label learning (MLL) problem, where the input space is certain feature space X of document and the output space is the power set 2 Y of a finite set of tags Y.", "labels": [], "entities": [{"text": "document tagging", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.758164644241333}, {"text": "multi-label learning (MLL)", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.7230857491493226}]}, {"text": "Given training data Z \u2282 X \u00d7 2 Y , we want to learn a function f : X \u2192 2 Y that predicts tags for unseen documents.", "labels": [], "entities": []}, {"text": "As shown in, during training a standard MLL algorithm (big blue box) one typically attempts to fit the prediction function (small blue box) into feature vectors of documents and the corresponding tags.", "labels": [], "entities": []}, {"text": "Note that feature vectors are generated separately before training, and tags for each document are encoded as a |Y|-dimensional binary vector with one representing the presence and zero otherwise.", "labels": [], "entities": []}, {"text": "In prediction phase, the learned prediction function will output relevant tags for the input feature vector of an unseen document.", "labels": [], "entities": []}, {"text": "Following such a paradigm, many generic algorithms have been developed for MLL; Prabhu and Varma, . With a surge of text content created by users online, such as blog posts, Wikipedia entries, etc., the algorithms for document tagging has many challenges.", "labels": [], "entities": [{"text": "MLL", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9461643695831299}, {"text": "document tagging", "start_pos": 218, "end_pos": 234, "type": "TASK", "confidence": 0.7205361425876617}]}, {"text": "Firstly, time sensitive news articles are generated on a daily basis, and it is important for an algorithm to assign tags before they loose freshness.", "labels": [], "entities": []}, {"text": "Secondly, new tagged documents could be fed into the training system, thus incrementally adapting the system to new training data without re-training from scratch is also critical.", "labels": [], "entities": []}, {"text": "Thirdly, we might face a very large set of candidate tags that can change dynamically, as new things are being invented.", "labels": [], "entities": []}, {"text": "In view of the aforementioned challenges, in this paper we propose anew and simple approach for document tagging: DocTag2Vec.", "labels": [], "entities": [{"text": "document tagging", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.7319066524505615}, {"text": "DocTag2Vec", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.9171403646469116}]}, {"text": "Our approach is motivated by the line of works on learning distributed representation of words and documents, e.g.,) and Doc2Vec (a.k.a. Paragraph Vector) ().", "labels": [], "entities": []}, {"text": "Word2Vec and Doc2Vec aim at learning low-dimensional feature vectors (i.e., embeddings) for words and documents from large corpus in an unsupervised manner, such that similarity between words (or documents) can be reflected by some distance metric on their embeddings.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9744765758514404}, {"text": "Doc2Vec", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9142594337463379}]}, {"text": "The general assumption behind Word2Vec and Doc2Vec is that more frequent co-occurrence of two words inside a small neighborhood of document should imply higher semantic similarity between them (see Section 2.2 for details).", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9350741505622864}]}, {"text": "The DocTag2Vec extends this idea to document and tag by positing that document and its associated tags should share high semantic similarity, which allows us to learn the embeddings of tags along with documents (see Section 2.3 for details).", "labels": [], "entities": []}, {"text": "Our method has two striking differences compared with standard MLL frameworks: firstly, our method directly works with raw text and does not need feature vectors extracted in advance.", "labels": [], "entities": []}, {"text": "Secondly, our DocTag2Vec produces tag embeddings, which carry semantic information that are generally not available from standard MLL framework.", "labels": [], "entities": [{"text": "DocTag2Vec", "start_pos": 14, "end_pos": 24, "type": "DATASET", "confidence": 0.9383646249771118}]}, {"text": "During training, DocTag2Vec directly takes the raw documents and tags as input and learns their embeddings using stochastic gradient descent (SGD).", "labels": [], "entities": []}, {"text": "In terms of prediction, anew document will be first embedded using a Doc2Vec component inside the DocTag2Vec, and tags are then assigned by searching for the nearest tags embedded around the document.", "labels": [], "entities": [{"text": "prediction", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.9551131725311279}, {"text": "DocTag2Vec", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.9475365877151489}]}, {"text": "Overall the proposed approach has the following merits.", "labels": [], "entities": []}, {"text": "\u2022 The SGD training supports the incremental adjustment of DocTag2Vec to new data.", "labels": [], "entities": [{"text": "SGD", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.8721565008163452}]}, {"text": "\u2022 The prediction uses the simple k-nearest neighbor search among tags instead of documents, whose running time does not scale up as training data increase.", "labels": [], "entities": []}, {"text": "\u2022 Since our method represent each individual tag using its own embedding vector, it it easy to dynamically incorporate new tags.", "labels": [], "entities": []}, {"text": "\u2022 The output tag embeddings can be used in other applications.", "labels": [], "entities": []}, {"text": "Related Work: Multi-label learning has found several applications in social media and web, like sentiment and topic analysis, social text stream analysis, and online advertising (.", "labels": [], "entities": [{"text": "sentiment and topic analysis", "start_pos": 96, "end_pos": 124, "type": "TASK", "confidence": 0.8506718873977661}, {"text": "social text stream analysis", "start_pos": 126, "end_pos": 153, "type": "TASK", "confidence": 0.5973156914114952}]}, {"text": "MLL has also been applied to diverse Natural Language Processing (NLP) tasks.", "labels": [], "entities": [{"text": "MLL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8627634048461914}]}, {"text": "However to the best of our knowledge we are the first to propose embedding based MLL approach to a NLP task.", "labels": [], "entities": []}, {"text": "MLL has been applied to Word Sense Disambiguation (WSD) problem for polysemic adjectives.", "labels": [], "entities": [{"text": "MLL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6089478731155396}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.751100704073906}]}, {"text": "() proposed a joint model to predict sentiment and topic for tweets and () proposed a multiinstance MLL based approach for relation extraction with distant supervision.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8654274344444275}]}, {"text": "Recently learning embeddings of words and sentences from large unannotated corpus has gained immense popularity in many NLP tasks, such as Named Entity Recognition (, sentiment classification) and summarization (.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.6366218030452728}, {"text": "sentiment classification", "start_pos": 167, "end_pos": 191, "type": "TASK", "confidence": 0.8752579987049103}, {"text": "summarization", "start_pos": 197, "end_pos": 210, "type": "TASK", "confidence": 0.9913559556007385}]}, {"text": "Also, vector space modeling has been applied to search re-targeting () and query rewriting (.", "labels": [], "entities": []}, {"text": "Given many potential applications, document tagging has been a very active research area.", "labels": [], "entities": [{"text": "document tagging", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.8536624014377594}]}, {"text": "In information retrieval, it is often coined as contentbased tag recommendation problem (, for which numbers of approaches were proposed, such as ( Paper Organization: The rest of the paper is organized as follows.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.8138710260391235}, {"text": "contentbased tag recommendation problem", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.6765130460262299}]}, {"text": "In Section 2, we first give a brief review of Word2Vec and Doc2Vec models, and then present training and prediction step respectively for our proposed extension, DocTag2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.980655312538147}]}, {"text": "In Section 3, we demonstrate the effectiveness of our DocTag2Vec approach through experiments on several datasets.", "labels": [], "entities": []}, {"text": "In the end, Section 4 is dedicated to conclusions and future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this subsection, we briefly describe the datasets included in our experiments.", "labels": [], "entities": []}, {"text": "It is worth noting that DocTag2Vec method needs raw texts as input instead of extracted features.", "labels": [], "entities": []}, {"text": "Therefore many benchmark datasets for evaluating multi-label learning algorithms are not suitable for our setting.", "labels": [], "entities": []}, {"text": "For the experiment, we primarily focus on the diversity of the source of tags, which capture different aspects of documents.", "labels": [], "entities": []}, {"text": "The statistics of all datasets are provided in.", "labels": [], "entities": []}, {"text": "Public datasets: \u2022 Wiki10: The wiki10 dataset contains a subset of English Wikipedia documents, which are tagged collaboratively by users from the social bookmarking site Delicious 1 . We remove the two uninformative tags, \"wikipedia\" and \"wiki\", from the collected data.", "labels": [], "entities": []}, {"text": "\u2022 WikiNER: WikiNER has a larger set of English Wikipedia documents.", "labels": [], "entities": []}, {"text": "The tags for each document are the named entities inside it, which is detected automatically by some named entity recognition (NER) algorithm.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.8083499570687612}]}, {"text": "\u2022 Relevance Modeling (RM): The RM dataset consists of two sets of financial news article in Chinese and Korean respectively.", "labels": [], "entities": [{"text": "Relevance Modeling (RM)", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.7046049416065217}, {"text": "RM dataset", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.6734101176261902}]}, {"text": "Each article is tagged with related ticker symbols of companies given by editorial judgement.", "labels": [], "entities": []}, {"text": "\u2022 News Content Taxonomy (NCT): NCT dataset is a collection of news articles annotated by editors with topical tags from a taxonomy tree.", "labels": [], "entities": [{"text": "NCT dataset", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.7156303524971008}]}, {"text": "The closer the tag is to the root, the more general the topic is.", "labels": [], "entities": []}, {"text": "For such tags with hierarchical structure, we also evaluate our method separately for tags of general topics (depth=2) and specific topics (depth=3).", "labels": [], "entities": []}, {"text": "For NCT dataset, when we examine the prediction for individual articles, it turns out surprisingly that there area significant number of cases where DocTag2Vec outputs better tags than those by editorial judgement.", "labels": [], "entities": [{"text": "NCT dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9600756764411926}]}, {"text": "Among all these cases, we include a few in showing the superiority of the tags given by DocTag2Vec sometimes.", "labels": [], "entities": [{"text": "DocTag2Vec", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.9588302373886108}]}, {"text": "For the first article, we can see that the three predicted tags are all related to the topic, especially the one with highest similarity, /Nature & Environment/ Environment/Climate Change, seems more pertinent compared with the editor's.", "labels": [], "entities": []}, {"text": "Similarly, we predict /Finance/Investment & Company Information/Company Earnings as the most relevant topic for the second article, which is more precise than its parent /Finance/Investment & Company Information.", "labels": [], "entities": []}, {"text": "Besides our approach can even find the wrong tags assigned by the editor.", "labels": [], "entities": []}, {"text": "The last piece of news is apparently about NBA, which should have the tag /Sports & Recreation/Basketball as predicted, while the editor annotates them with the incorrect one, /Sports & Recreation/Baseball.", "labels": [], "entities": []}, {"text": "On the other hand, by looking at the similarity scores associated with the predicted tags, we can see that higher score in general implies higher aboutness, which can also be used as a quantification of prediction confidence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of datasets", "labels": [], "entities": []}, {"text": " Table 2: Overall Recall on News Content Taxonomy dataset", "labels": [], "entities": [{"text": "Recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.6681835651397705}, {"text": "News Content Taxonomy dataset", "start_pos": 28, "end_pos": 57, "type": "DATASET", "confidence": 0.9000068157911301}]}]}