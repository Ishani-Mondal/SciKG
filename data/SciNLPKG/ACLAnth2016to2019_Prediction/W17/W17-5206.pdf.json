{"title": [{"text": "IMS at EmoInt-2017: Emotion Intensity Prediction with Affective Norms, Automatically Extended Resources and Deep Learning", "labels": [], "entities": [{"text": "EmoInt-2017", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.5361353754997253}, {"text": "Emotion Intensity Prediction", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.9424397548039755}]}], "abstractContent": [{"text": "Our submission to the WASSA-2017 shared task on the prediction of emotion intensity in tweets is a supervised learning method with extended lexicons of affective norms.", "labels": [], "entities": [{"text": "WASSA-2017 shared task", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.5443888505299886}, {"text": "prediction of emotion intensity in tweets", "start_pos": 52, "end_pos": 93, "type": "TASK", "confidence": 0.8433493971824646}]}, {"text": "We combine three main information sources in a random forrest regressor, namely (1), manually created resources, (2) automatically extended lexicons, and (3) the output of a neural network (CNN-LSTM) for sentence regression.", "labels": [], "entities": [{"text": "sentence regression", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.7528165876865387}]}, {"text": "All three feature sets perform similarly well in isolation (\u2248 .67 macro average Pearson correlation).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 80, "end_pos": 99, "type": "METRIC", "confidence": 0.93199622631073}]}, {"text": "The combination achieves .72 on the official test set (ranked 2nd out of 22 participants).", "labels": [], "entities": [{"text": "official test set", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.7646737496058146}]}, {"text": "Our analysis reveals that performance is increased by providing cross-emotional intensity predictions.", "labels": [], "entities": []}, {"text": "The automatic extension of lexicon features benefit from domain specific embeddings.", "labels": [], "entities": []}, {"text": "Complementary ratings for affective norms increase the impact of lexicon features.", "labels": [], "entities": []}, {"text": "Our resources (ratings for 1.6 million twitter specific words) and our implementation is publicly available at http: //www.ims.uni-stuttgart.de/ data/ims_emoint.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural language processing, emotion recognition is the task of associating words, phrases or documents with predefined emotions from psychological models.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7024200956026713}, {"text": "emotion recognition", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7387810796499252}]}, {"text": "Typical discrete categories are those proposed by Ekman) and Plutchik, namely Anger, Anticipation, Disgust, Fear, Joy, Sadness, Surprise und Trust.", "labels": [], "entities": []}, {"text": "In contrast to sentiment analysis with its main task to recognize the polarity of text (e. g., positive, negative, neutral, mixed), only a few resources and domains have been subject of analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.942009449005127}]}, {"text": "Examples are, e. g., tales), blogs, and as a very popular domain, microblogs on Twitter ().", "labels": [], "entities": []}, {"text": "The latter in particular provides a large resource of data in the form of user messages (.", "labels": [], "entities": []}, {"text": "A common source of weak supervision for training classifiers are hashtags, emoticons, or emojis, which are interpreted as a weak form of author \"self-labeling\").", "labels": [], "entities": []}, {"text": "The classifier then learns the association of all other words in the message with the emotion ().", "labels": [], "entities": []}, {"text": "An alternative to discrete models are continuous models that map emotions to an n-dimensional space with valence, arousal and dominance (VAD) being usual dimensions.", "labels": [], "entities": [{"text": "dominance (VAD)", "start_pos": 126, "end_pos": 141, "type": "METRIC", "confidence": 0.8131970167160034}]}, {"text": "Previous works that rely on the VAD-scheme focus mainly on extending and adapting the affective lexicons, including to historical texts (, and on the prediction and extrapolation of affective ratings.", "labels": [], "entities": [{"text": "VAD-scheme", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.5391823053359985}, {"text": "prediction and extrapolation of affective ratings", "start_pos": 150, "end_pos": 199, "type": "TASK", "confidence": 0.7273299594720205}]}, {"text": "The WASSA-2017 shared task on the prediction of emotion intensity in tweets (EmoInt) aims at combining descrete emotion classes with different levels of activation.", "labels": [], "entities": [{"text": "WASSA-2017 shared task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.5802701314290365}, {"text": "prediction of emotion intensity in tweets", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.8446931640307108}]}, {"text": "Given a tweet and an emotion (anger, fear, joy, and sadness), the task requires to determine the intensity expressed regarding a particular emotion.", "labels": [], "entities": []}, {"text": "This score can be seen as an approximation of the emotion intensity felt by the reader or expressed by the author.", "labels": [], "entities": []}, {"text": "For a detailed task descriptions and background information on the data collection see Mohammad and Bravo-Marquez (2017).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Baseline features across training data us- ing support vector machines (SVM) and random  forest (RF). Pearson correlation based on 10-fold  cross validation. The column names denote anger  (a), fear (f), joy (j), sadness (s).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 112, "end_pos": 131, "type": "METRIC", "confidence": 0.8498260974884033}]}, {"text": " Table 3: Performance of lexicons and our automat- ically extended lexicons. Results are based on the  random forest classifier. Top part compares perfor- mance of lexicon features in isolation. Ext.News  and Ext.Twitter build on top of the baseline lex- icons and the ACVH lexicons. The bottom part  shows performance in combination with the origi- nal lexicons provided by the baseline (=BL).", "labels": [], "entities": [{"text": "ACVH lexicons", "start_pos": 269, "end_pos": 282, "type": "DATASET", "confidence": 0.9602309465408325}, {"text": "BL", "start_pos": 390, "end_pos": 392, "type": "METRIC", "confidence": 0.9789261221885681}]}, {"text": " Table 4: Comparing the performance of Tweet Re- gression Architectures.", "labels": [], "entities": []}, {"text": " Table 5: Overview IMS full system, features, fea- ture counts.", "labels": [], "entities": [{"text": "IMS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.6533580422401428}, {"text": "fea- ture counts", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.5740487053990364}]}, {"text": " Table 7: Overview IMS full and partial System  performance on Test data.", "labels": [], "entities": [{"text": "IMS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.860405445098877}, {"text": "Test data", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.7775530219078064}]}]}