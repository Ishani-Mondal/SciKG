{"title": [{"text": "Auxiliary Objectives for Neural Error Detection Models", "labels": [], "entities": [{"text": "Neural Error Detection", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7123905420303345}]}], "abstractContent": [{"text": "We investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to error detection in learner writing.", "labels": [], "entities": [{"text": "error detection", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.7087695747613907}]}, {"text": "Auxiliary costs provide the model with additional linguistic information, allowing it to learn general-purpose com-positional features that can then be exploited for other objectives.", "labels": [], "entities": []}, {"text": "Our experiments show that a joint learning approach trained with parallel labels on in-domain data improves performance over the previous best error detection system.", "labels": [], "entities": [{"text": "error detection", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.7099933177232742}]}, {"text": "While the resulting model has the same number of parameters, the additional objectives allow it to be optimised more efficiently and achieve better performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic error detection systems for learner writing need to identify various types of error in text, ranging from incorrect uses of function words, such articles and prepositions, to semantic anomalies in content words, such as adjectivenoun combinations.", "labels": [], "entities": []}, {"text": "To tackle the scarcity of errorannotated training data, previous work has investigated the utility of automatically generated ungrammatical data, as well as explored learning from native well-formed data.", "labels": [], "entities": []}, {"text": "In this work, we investigate the utility of supplementing error detection frameworks with additional linguistic information that can be extracted from the available error-annotated learner data.", "labels": [], "entities": [{"text": "error detection", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.7168312072753906}]}, {"text": "We construct a neural sequence labeling system for error detection that allows us to learn better representations of language composition and detect errors in context more accurately.", "labels": [], "entities": [{"text": "error detection", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.708192989230156}]}, {"text": "In addition to predicting the binary error labels, we experiment with also predicting additional information for each token, including token frequency and the specific error type, which can be extracted from the existing data, as well as part-of-speech (POS) tags and dependency relations, which can be generated automatically using readily available toolkits.", "labels": [], "entities": []}, {"text": "These auxiliary objectives provide the sequence labeling model with additional linguistic information, allowing it to learn useful compositional features that can then be exploited for error detection.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6480546593666077}, {"text": "error detection", "start_pos": 185, "end_pos": 200, "type": "TASK", "confidence": 0.7016962468624115}]}, {"text": "This can be seen as a type of multi-task learning, where the model learns better compositional features via shared representations with related tasks.", "labels": [], "entities": []}, {"text": "While common approaches to multitask learning require randomly switching between different tasks and datasets, we demonstrate that a joint learning approach trained on in-domain data with parallel labels substantially improves error detection performance on two different datasets.", "labels": [], "entities": [{"text": "error detection", "start_pos": 227, "end_pos": 242, "type": "TASK", "confidence": 0.6698341965675354}]}, {"text": "In addition, the auxiliary labels are only required during the training process, resulting in a better model with the same number of parameters.", "labels": [], "entities": []}, {"text": "In the following sections, we describe our approach to the task, systematically compare the informativeness of various auxiliary loss functions, investigate alternative training strategies, and examine the effect of additional training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Rei and Yannakoudakis (2016) investigate a number of compositional architectures for error detection, and present state-of-the-art results using a bidirectional LSTM.", "labels": [], "entities": [{"text": "error detection", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7084903120994568}]}, {"text": "We follow their experimental setup and investigate the impact of auxiliary loss functions on the same datasets: the First Certificate in English (FCE) dataset) and the CoNLL-14 shared task test set (.", "labels": [], "entities": [{"text": "First Certificate in English (FCE) dataset", "start_pos": 116, "end_pos": 158, "type": "DATASET", "confidence": 0.6770476251840591}, {"text": "CoNLL-14 shared task test set", "start_pos": 168, "end_pos": 197, "type": "DATASET", "confidence": 0.8598747611045837}]}, {"text": "FCE contains texts written by non-native learners of English in response to exam prompts eliciting free-text answers.", "labels": [], "entities": [{"text": "FCE contains texts written by non-native learners of English in response to exam prompts eliciting free-text answers", "start_pos": 0, "end_pos": 116, "type": "Description", "confidence": 0.7405314200064715}]}, {"text": "The texts have been manually annotated with error types and error spans by professional examiners, which convert to a binary correct/incorrect token-level labeling for error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 168, "end_pos": 183, "type": "TASK", "confidence": 0.6806618571281433}]}, {"text": "For missing-word errors, the error label is assigned to the next word in the sequence.", "labels": [], "entities": []}, {"text": "The released version contains 28,731 sentences for training, 2,222 sentences for development and 2,720 sentences for testing.", "labels": [], "entities": []}, {"text": "The development set was randomly sampled from the training data, and the test set contains texts from a different examination year.", "labels": [], "entities": []}, {"text": "The CoNLL-14 test set contains 50 texts annotated by two experts.", "labels": [], "entities": [{"text": "CoNLL-14 test set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9523359139760336}]}, {"text": "Compared to FCE, the texts are more technical and are written by higherproficiency learners.", "labels": [], "entities": [{"text": "FCE", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.7733229398727417}]}, {"text": "In order to make our results comparable to  Following the CoNLL-14 shared task, we also report F 0.5 as the main evaluation metric.", "labels": [], "entities": [{"text": "CoNLL-14 shared task", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.7550399899482727}, {"text": "F 0.5", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.98771932721138}]}, {"text": "However, while the shared task focused on correction and calculated F 0.5 over error spans using multiple annotations, we evaluate token-level error detection performance.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9701766669750214}, {"text": "token-level error detection", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.6243476768334707}]}, {"text": "Following recommendations by, we also report the raw counts for predicted and correct tokens.", "labels": [], "entities": []}, {"text": "For pre-processing, all the texts are lowercased and digits are replaced with zeros for the tokenlevel representations, although the character-based component has access to the original version of each token.", "labels": [], "entities": []}, {"text": "Tokens that occur only once are mapped to a single OOV token, which is then used to represent previously unseen tokens during testing.", "labels": [], "entities": []}, {"text": "The word embeddings have size 300 and are initialised with publicly available word2vec () embeddings trained on Google News.", "labels": [], "entities": []}, {"text": "The LSTM hidden layers have size 200 and the task-specific hidden layers have size 50 with tanh activation.", "labels": [], "entities": []}, {"text": "The model is optimised using Adadelta and training is stopped based on the error detection F 0.5 score on the development set.", "labels": [], "entities": [{"text": "error detection F 0.5 score", "start_pos": 75, "end_pos": 102, "type": "METRIC", "confidence": 0.9369171619415283}]}, {"text": "We implement the proposed framework using Theano and make the code publicly available online., trained on the same FCE data.", "labels": [], "entities": [{"text": "Theano", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.9822758436203003}, {"text": "FCE data", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.970285564661026}]}, {"text": "The main system in our experiments is the bi-directional LSTM error detection model with character-based representations, as described in Section 2.", "labels": [], "entities": [{"text": "LSTM error detection", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.8379641969998678}]}, {"text": "We then use this model and test the effect on performance when adding each of the auxiliary loss functions described in Section 3 to the training objective.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Alternative labels for an example sentence from the FCE training data.", "labels": [], "entities": [{"text": "FCE training data", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9311553438504537}]}, {"text": " Table 2: Error detection results on the FCE dataset using different auxiliary loss functions.", "labels": [], "entities": [{"text": "Error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8760882616043091}, {"text": "FCE dataset", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9573985934257507}]}, {"text": " Table 3: Error detection results on the CoNLL-14 test set using different auxiliary loss functions.", "labels": [], "entities": [{"text": "Error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.861784815788269}, {"text": "CoNLL-14 test set", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9661473035812378}]}, {"text": " Table 4: Results on error detection when the  model is pre-trained on different tasks.", "labels": [], "entities": [{"text": "error detection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.6769818067550659}]}, {"text": " Table 5: Results on error detection when training  is alternated between the two tasks (e.g., error de- tection and POS tagging) and datasets.", "labels": [], "entities": [{"text": "error detection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.681558832526207}, {"text": "POS tagging", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.7684753239154816}]}, {"text": " Table 6: Error detection results using auxiliary ob- jectives, trained on additional data.", "labels": [], "entities": [{"text": "Error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9068422913551331}]}]}