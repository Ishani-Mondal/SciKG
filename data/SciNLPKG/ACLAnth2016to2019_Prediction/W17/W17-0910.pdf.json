{"title": [{"text": "Resource-Lean Modeling of Coherence in Commonsense Stories", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a resource-lean neural recog-nizer for modeling coherence in common-sense stories.", "labels": [], "entities": []}, {"text": "Our lightweight system is inspired by successful attempts to mod-eling discourse relations and stands out due to its simplicity and easy optimization compared to prior approaches to narrative script learning.", "labels": [], "entities": []}, {"text": "We evaluate our approach in the Story Cloze Test 1 demonstrating an absolute improvement inaccuracy of 4.7% over state-of-the-art implementations.", "labels": [], "entities": [{"text": "Story Cloze Test 1", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.899331122636795}]}], "introductionContent": [{"text": "Semantic applications related to Natural Language Understanding have seen a recent surge of interest within the NLP community, and story understanding can be regarded as one of the supreme disciplines in that field.", "labels": [], "entities": [{"text": "Natural Language Understanding", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.6462026536464691}, {"text": "story understanding", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.8051196336746216}]}, {"text": "Closely related to Machine Reading () and script learning, it is a highly challenging task which is built on top of a cascade of core NLP applications, includingamong others-causal/temporal relation recognition, event extraction (UzZaman and Allen, 2010), (implicit) semantic role labeling ( or inter-sentential discourse parsing (.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7579216361045837}, {"text": "script learning", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.8580962121486664}, {"text": "temporal relation recognition", "start_pos": 181, "end_pos": 210, "type": "TASK", "confidence": 0.7235201597213745}, {"text": "event extraction", "start_pos": 212, "end_pos": 228, "type": "TASK", "confidence": 0.7516027688980103}, {"text": "semantic role labeling", "start_pos": 267, "end_pos": 289, "type": "TASK", "confidence": 0.643494576215744}, {"text": "inter-sentential discourse parsing", "start_pos": 295, "end_pos": 329, "type": "TASK", "confidence": 0.6200683812300364}]}, {"text": "Recent progress has been made in the field of narrative understanding: a variety of successful approaches have been introduced, ranging from narrative chains to script learning techniques (, or event schemas ( all these approaches have in common is that they ultimately seek to find away to prototypically model the causal and correlational relationships between events, and also to obtain a structured (ideally more compact and abstract) representation of the underlying commonsense knowledge which is encoded in the respective story.", "labels": [], "entities": [{"text": "narrative understanding", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8654845356941223}]}, {"text": "The downside of these approaches is that they are feature-rich (potentially hand-crafted) and therefore costly and domain-specific to a large extent.", "labels": [], "entities": []}, {"text": "On a related note, demonstrate that there is still room for improvement when testing the performances of these state-of-the-art techniques for learning procedural knowledge on an independent evaluation set.", "labels": [], "entities": []}, {"text": "Our Contribution: In this paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations.", "labels": [], "entities": []}, {"text": "We cast the problem of modeling text coherence as a special case of discourse processing in which our model jointly learns to distinguish correct from incorrect story endings.", "labels": [], "entities": []}, {"text": "Our approach is inspired by promising related attempts using event embeddings and neural methods for script learning.", "labels": [], "entities": [{"text": "script learning", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.88921919465065}]}, {"text": "Our system is an end-to-end implementation of the ideas sketched in of the joint paragraph and sentence level model (cf. Section 3 for details).", "labels": [], "entities": []}, {"text": "We evaluate our approach in the Story Cloze Test, a task for predicting story continuations.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.6423015793164571}, {"text": "predicting story continuations", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.9014307856559753}]}, {"text": "Despite its simplicity, our system demonstrates superior performance on the designated data over previous approaches to script learning and-due to its language and genre-independence-it also represents a solid basis for further optimization towards other textual domains.", "labels": [], "entities": []}, {"text": "Four-Sentence Core Story Quiz 1 Quiz 2 I asked Sarah out on a date.", "labels": [], "entities": []}, {"text": "I was so excited for our date together.", "labels": [], "entities": []}, {"text": "We went to dinner and then a movie.", "labels": [], "entities": []}, {"text": "I had a terrible time.", "labels": [], "entities": []}, {"text": "I got to kiss Sarah goodnight.", "labels": [], "entities": []}, {"text": "(wrong ending) (correct ending): An example of a ROCStory consisting of a core story and two alternative continuations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model intrinsically on both validation and test set provided by the shared task organizers.", "labels": [], "entities": []}, {"text": "As a reference, we also provide three baselines borrowed from at the time when the data set was released, namely the best-performing algorithms inspired by Semantic Model/DSSM) and Chambers and Jurafsky (2008, Narrative-Chains).", "labels": [], "entities": []}, {"text": "shows that correct endings appear almost equally often in either first or second position in the annotated data sets.", "labels": [], "entities": []}, {"text": "The majority class is only significantly beaten by the DSSM model.", "labels": [], "entities": []}, {"text": "Our approach (denoted by Neural-ROCStoriesOnly), however, can further improve upon the best system by an absolute increase inaccuracy of 4.7%.", "labels": [], "entities": []}, {"text": "Only the best configuration is shown and has been achieved with the 300-dimensional pre-trained Google News embeddings.", "labels": [], "entities": [{"text": "Google News embeddings", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.891205370426178}]}, {"text": "Interestingly, the performance of the model on the test set is slightly better that on the validation set but also very similar which suggests that it is able to generalize well to unseen data and is not prone to overfitting training or validation data.", "labels": [], "entities": []}, {"text": "A manual inspection of a subset of the misclassified items reveals that our neural recognizer is struggling to properly handle story continuations which change the underlying sentiment of the core story either towards negative or positive, e.g. fail test, study hard \u2192 pass test.", "labels": [], "entities": []}, {"text": "In future work we plan to address this issue in closer detail.", "labels": [], "entities": []}, {"text": "A Note on the Evaluation & Training Procedure: Although the task has been stated differently, it stands to reason that one could exploit the tiny amount of hand-annotated data in the validation set directly to train a classifier.", "labels": [], "entities": []}, {"text": "We have done so as aside experiment using as features the same 900-dimensional composition layer embeddings from Section 3.2 and optimized a minimalist SVM classifier by 10-fold cross-validation, with feature and parameter selection on the validation set.", "labels": [], "entities": []}, {"text": "The final model achieves a test set accuracy of 70.02%, cf. SVM-ManualLabels in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9871923923492432}]}, {"text": "Besides the relatively good performance obtained here, however, we want to emphasize thatwhen no hand-annotated labels for the correct position of the quizzes are available-the NeuralROCStories approach introduced in Section 3 represents a promising and more generic framework for coherence learning, incorporating the plain text ROCStories as only source of information.", "labels": [], "entities": []}], "tableCaptions": []}