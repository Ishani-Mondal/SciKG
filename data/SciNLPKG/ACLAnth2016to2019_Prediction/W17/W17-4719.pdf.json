{"title": [], "abstractContent": [{"text": "Automatic translation of documents is an important task in many domains, including the biological and clinical domains.", "labels": [], "entities": [{"text": "Automatic translation of documents", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8611232489347458}]}, {"text": "The second edition of the Biomed-ical Translation task in the Conference of Machine Translation focused on the automatic translation of biomedical-related documents between English and various European languages.", "labels": [], "entities": [{"text": "Biomed-ical Translation task", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.9028023680051168}, {"text": "Machine Translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7325110584497452}, {"text": "automatic translation of biomedical-related documents between English", "start_pos": 111, "end_pos": 180, "type": "TASK", "confidence": 0.8022384217807225}]}, {"text": "This year, we addressed ten languages: Czech, German, English, French, Hungarian, Polish, Por-tuguese, Spanish, Romanian and Swedish.", "labels": [], "entities": []}, {"text": "Test sets included both scientific publications (from the Scielo and EDP Sciences databases) and health-related news (from the Cochrane and UK National Health Service web sites).", "labels": [], "entities": [{"text": "EDP Sciences databases", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.906955897808075}, {"text": "UK National Health Service web", "start_pos": 140, "end_pos": 170, "type": "DATASET", "confidence": 0.8997340440750122}]}, {"text": "Seven teams participated in the task, submitting a total of 82 runs.", "labels": [], "entities": []}, {"text": "Herein we describe the test sets, participating systems and results of both the automatic and manual evaluation of the translations .", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic translation of texts allows readers to gain access to information present in documents written in a language in which the reader is not fluent.", "labels": [], "entities": []}, {"text": "We identify two main use cases of machine translation (MT) in the biomedical domain: (a) making health information available to health professionals and the general public in their own language; and (b) assisting health professionals and researchers in writing reports of their research in English.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8643117666244506}]}, {"text": "In addition, it creates an opportunity for natural language processing (NLP) tools to be applied to domain-specific texts in languages for which few domain-relevant tools are available; i.e., the texts can be translated into a language for which there are more resources.", "labels": [], "entities": []}, {"text": "The second edition of the Biomedical Translation Task in the Conference for Machine Translation (WMT) 1 builds on the first edition) by offering seven additional language pairs and new test sets.", "labels": [], "entities": [{"text": "Biomedical Translation Task in the Conference for Machine Translation (WMT)", "start_pos": 26, "end_pos": 101, "type": "TASK", "confidence": 0.8067495450377464}]}, {"text": "This year, we expanded to a total often languages in the biomedical task, namely, Czech (cs), German (de), English (en), French (fr), Hungarian (hu), Polish (pl), Portuguese (pt), Spanish (es), Romanian (ro) and Swedish (sv).", "labels": [], "entities": []}, {"text": "Test sets included scientific publications from the Scielo and EDP Sciences databases and health-related news from Cochrane and the UK National Health Service (NHS).", "labels": [], "entities": [{"text": "EDP Sciences databases", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.8956828713417053}, {"text": "UK National Health Service (NHS)", "start_pos": 132, "end_pos": 164, "type": "DATASET", "confidence": 0.9114315935543605}]}, {"text": "Participants were challenged to build systems to enable translation from English to all other lan-guages, as well as from French, Spanish and Portuguese to English.", "labels": [], "entities": []}, {"text": "We provided both training and development data but the teams were allowed to use additional in-domain or out-of-domain training data.", "labels": [], "entities": []}, {"text": "After release of the test sets, the participants had 10 days to submit results (automatic translations) for any of the test sets and languages.", "labels": [], "entities": []}, {"text": "We allowed up to three runs per team for each language pair and test sets.", "labels": [], "entities": []}, {"text": "We evaluated the submission both automatically and manually.", "labels": [], "entities": []}, {"text": "In this work, we report details on the challenge, test sets, participating teams, the results they obtained and the quality of the automatic translations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present an overview of the submissions to the Biomedical Task and results in terms of both automatic and manual evaluation.", "labels": [], "entities": [{"text": "Biomedical Task", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8198000192642212}]}, {"text": "In this section, we provide the results for the automatic evaluation and rank the various systems based on those results.", "labels": [], "entities": []}, {"text": "For the automatic evaluation, we computed BLEU scores at the sentence level using the multi-bleu and tokenization scripts as provided by Moses (tokenizer and truecase).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9975868463516235}]}, {"text": "For all test sets and language pairs, we compare the automatic translations to the reference one, as provided by each test set.", "labels": [], "entities": []}, {"text": "Results for the Scielo test sets are presented in.", "labels": [], "entities": [{"text": "Scielo test sets", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.7782365580399832}]}, {"text": "All three runs from the UHH team, for all four language pairs, obtained a much higher BLEU score than our baseline.", "labels": [], "entities": [{"text": "UHH team", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9773837924003601}, {"text": "BLEU score", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9850137829780579}]}, {"text": "However, this is not surprising given the simplicity of the methods used in the baseline system.", "labels": [], "entities": []}, {"text": "The BLEU scores for the EDP test set are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9986205101013184}, {"text": "EDP test set", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.922897458076477}]}, {"text": "While all system runs score above the baseline, only the Kyoto system outperforms the stronger baseline for en2fr.", "labels": [], "entities": []}, {"text": "We rank the various submissions as follows: \u2022 fr2en: Hunter (runs 1,2) < baseline < UHH (runs 1,2) < UHH (run 3) < kyoto (run 1).", "labels": [], "entities": [{"text": "UHH", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8837449550628662}, {"text": "UHH", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.9074023365974426}]}, {"text": "\u2022 en2fr: baseline < Hunter (runs 1,2) < UHH (runs 1,2,3) < LIMSI baseline < kyoto (run 1) < kyoto (run 2).", "labels": [], "entities": [{"text": "UHH", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8961743712425232}]}, {"text": "The BLEU scores for the Cochrane test sets are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9986107349395752}, {"text": "Cochrane test sets", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.8265047272046407}]}, {"text": "The scores range from as low as 12.45 (for Polish) to as high as 48.99 (for Spanish).", "labels": [], "entities": []}, {"text": "All scores were particularly high for Spanish (close to 50), but rather low for Polish and Czech (all below 30).", "labels": [], "entities": []}, {"text": "While the BLEU value did not vary much for French (all around 30), these went from a range of 14 to 41 for Romanian.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9997290968894958}]}, {"text": "We rank the various submissions for each language as below:: Results for the EDP test sets.", "labels": [], "entities": [{"text": "EDP test sets", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.921502153078715}]}, {"text": "* indicates the primary run as declared by the participants.", "labels": [], "entities": []}, {"text": "\u2022 cs: PJIIT (run 1) < uedin-nmt (run 1).", "labels": [], "entities": [{"text": "PJIIT", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.8757741451263428}]}, {"text": "\u2022 de: UHH (runs 1,2,3) < Hunter (run 1) < PI-IJT (run 1) < lilt (run 1,2) < LMU < uedinnmt (run 1).", "labels": [], "entities": [{"text": "UHH", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.9346084594726562}, {"text": "PI-IJT", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.8625588417053223}]}, {"text": "\u2022 fr: Hunter (runs 1,2) < UHH (runs 1,2,3).", "labels": [], "entities": [{"text": "Hunter", "start_pos": 6, "end_pos": 12, "type": "DATASET", "confidence": 0.9227463006973267}, {"text": "UHH", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.6684345602989197}]}, {"text": "\u2022 pl: PIIJT (run 2) < Hunter (run 1) < PIIJT (runs 1,3) < uedin-nmt (run 2) < uedin-nmt (run 1).", "labels": [], "entities": [{"text": "PIIJT", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.9072250127792358}, {"text": "PIIJT", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9135955572128296}]}, {"text": "\u2022 ro: Hunter (run 1) < PIIJT (run 1) < uedinnmt (run 2) < uedin-nmt (run 1).", "labels": [], "entities": [{"text": "PIIJT", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8809423446655273}]}, {"text": "Finally, the BLEU scores for the NHS dataset are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9984579086303711}, {"text": "NHS dataset", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.986803412437439}]}, {"text": "The scores range from as low as 10.56 (for Romanian, the lowest score across all test sets and languages) to as high as 41.22 (for Spanish).", "labels": [], "entities": []}, {"text": "All scores were particularly high for Spanish (around 40), but rather low for Polish, Czech and Romanian (all below 30).", "labels": [], "entities": []}, {"text": "We rank the various submissions for each language as shown below: \u2022 cs: PJIIT (run 1) < uedin-nmt (run 1).", "labels": [], "entities": [{"text": "PJIIT", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.841177761554718}]}, {"text": "\u2022 de: UHH (runs 1,2,3) < Hunter (run 1) < PI-IJT (run 1) < lilt (run 1,2) < LMU < uedinnmt (run 1).", "labels": [], "entities": [{"text": "UHH", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.9346084594726562}, {"text": "PI-IJT", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.8625588417053223}]}, {"text": "\u2022 fr: Hunter (run 1) < UHH (runs 1,2) < UHH (run 3).", "labels": [], "entities": [{"text": "UHH", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.9204064011573792}, {"text": "UHH", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8977647423744202}]}, {"text": "\u2022 pl: PIIJT (run 2) < Hunter (run 1), PIIJT (runs 1,3) < uedin-nmt (run 2) < uedin-nmt (run 1).", "labels": [], "entities": [{"text": "PIIJT", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.9007142186164856}, {"text": "PIIJT", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.890235960483551}]}, {"text": "\u2022 ro: Hunter (run 1) < PIIJT (run 1) < uedinnmt (run 2) < uedin-nmt (run 1).", "labels": [], "entities": [{"text": "PIIJT", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8809423446655273}]}, {"text": "The BLEU values were generally lower for NHS than the ones obtained for the same teams for the Cochrane test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9995633959770203}, {"text": "NHS", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.935059666633606}, {"text": "Cochrane test sets", "start_pos": 95, "end_pos": 113, "type": "DATASET", "confidence": 0.8800934155782064}]}, {"text": "However, the rankings of systems and runs are nearly the same for the Cochrane and NHS test sets.", "labels": [], "entities": [{"text": "Cochrane and NHS test sets", "start_pos": 70, "end_pos": 96, "type": "DATASET", "confidence": 0.8295186400413513}]}, {"text": "The only exceptions were in French, where run 3 from UHH was higher than the others from the team, and for Polish, where the scores for Hunter and PIIJT (runs 1,3) were nearly the same.", "labels": [], "entities": [{"text": "UHH", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9556227922439575}, {"text": "Hunter", "start_pos": 136, "end_pos": 142, "type": "DATASET", "confidence": 0.8235979676246643}, {"text": "PIIJT", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.7964320182800293}]}, {"text": "We required teams to identify a primary run for each language pair, in the case that they submitted more than one run.", "labels": [], "entities": []}, {"text": "These are the runs for which we performed manual evaluation.", "labels": [], "entities": []}, {"text": "The following runs were considered to be primary: Hunter (run1), kyoto (run2 for en/fr, run1 for fr/en), lilt (run1), LMU (run1), PJIIT (run3 for pl, otherwise, run1), uedin-nmt (run1), UHH (run3).", "labels": [], "entities": [{"text": "UHH", "start_pos": 186, "end_pos": 189, "type": "DATASET", "confidence": 0.840442955493927}]}, {"text": "We computed pairwise combinations of translations either between two automated systems, or one automated system and the reference translation.", "labels": [], "entities": []}, {"text": "We compared all systems (primary) to the reference translation, as well as to each other system.", "labels": [], "entities": []}, {"text": "We ran manual validation for all target languages and test sets.", "labels": [], "entities": []}, {"text": "The human validators were   native speakers of the languages and were either members of the participating teams or colleagues from the research community.", "labels": [], "entities": []}, {"text": "The validation task was carried out using the Appraise tool 15.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9819141030311584}, {"text": "Appraise tool 15", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.7741152048110962}]}, {"text": "For each pairwise comparison, we validated a total of 100 randomly-chosen sentence pairs.", "labels": [], "entities": []}, {"text": "The validation consisted of reading the two sentences (A and B), i.e., translations from two systems or from the reference, and choosing one of the options below: \u2022 A<B: when the quality of translation B was higher than A.", "labels": [], "entities": []}, {"text": "\u2022 A=B: when both translation had similar quality.", "labels": [], "entities": [{"text": "A=B", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.7146386305491129}]}, {"text": "\u2022 A>B: when the quality of translation A was higher than B.", "labels": [], "entities": []}, {"text": "\u2022 Flag error: when the translations did not seem to be derived from the same input sentence.", "labels": [], "entities": [{"text": "Flag error", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.771585613489151}]}, {"text": "This is usually derived from error in the corpus alignment (for the Scielo and EDP datasets).", "labels": [], "entities": [{"text": "EDP datasets", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.8319636881351471}]}, {"text": "The manual validation for the Scielo test sets is presented in, for the comparison of the only participating team (UHH) to the reference translation.", "labels": [], "entities": [{"text": "Scielo test sets", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.7964736819267273}]}, {"text": "For en2es, the automatic translation scored lower than the reference one in 53 out of 100 pairs, but could still beat the reference translation in 23 pairs.", "labels": [], "entities": []}, {"text": "For en2pt, the automatic translation was better only on 13 sentences pairs, while they could achieve similar quality to the reference translation on 31 cases.", "labels": [], "entities": []}, {"text": "In the case of translations from Spanish or Portuguese to English, the reference scored better than the UHH around the same proportion, while the latter could only beat the reference in very few cases.", "labels": [], "entities": [{"text": "translations from Spanish or Portuguese to English", "start_pos": 15, "end_pos": 65, "type": "TASK", "confidence": 0.7974353688103812}, {"text": "UHH", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.6226940751075745}]}, {"text": "We present the results for the manual evaluation of the EDP test sets in.", "labels": [], "entities": [{"text": "EDP test sets", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9157392779986063}]}, {"text": "Based on the number of times that a translation was validated as being better than another, we ranked the systems for each language as listed below: \u2022 en2fr: Hunter < UHH < kyoto = reference \u2022 fr2en: Hunter < UHH < kyoto < reference Results for manual validation of the Cochrane test sets are presented in: Results for the manual validation for the EDP test sets.", "labels": [], "entities": [{"text": "Cochrane test sets", "start_pos": 270, "end_pos": 288, "type": "DATASET", "confidence": 0.7856955528259277}, {"text": "EDP test sets", "start_pos": 349, "end_pos": 362, "type": "DATASET", "confidence": 0.9738978346188863}]}, {"text": "Values are absolute numbers (not percentages).", "labels": [], "entities": []}, {"text": "They might not sum up to 100 due to the skipped sentences.", "labels": [], "entities": []}, {"text": "\u2022 cs: PIIJT < uedin-nmt < reference \u2022 de: UHH < Hunter = PJIIT < Lilt < LMU < uedin-nmt = reference \u2022 fr: UHH < Hunter < reference \u2022 pl: Hunter = PIIJT < uedin < reference \u2022 es: UHH < reference \u2022 ro: Hunter < PIIJT < uedin < reference Results for manual validation of the NHS test sets are presented in.", "labels": [], "entities": [{"text": "UHH", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.958685040473938}, {"text": "UHH", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.9378419518470764}, {"text": "UHH", "start_pos": 178, "end_pos": 181, "type": "DATASET", "confidence": 0.9656209349632263}, {"text": "NHS test sets", "start_pos": 272, "end_pos": 285, "type": "DATASET", "confidence": 0.9776444633801779}]}, {"text": "We rank the various system as shown below: \u2022 cs: PIIJT < uedin-nmt < reference \u2022 de: Hunter = UHH < PIIJT < Lilt < LMU = uedin-nmt < reference \u2022 fr: UHH < Hunter < reference \u2022 pl: Hunter < PIIJT < uedin < reference \u2022 es: UHH < reference \u2022 ro: Hunter < PIIJT < uedin < reference For the Polish language in the NHS test set, the evaluator skipped too many sentences (68 out of 100) to enable a comparison between Hunter and PIIJT.", "labels": [], "entities": [{"text": "UHH", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9104634523391724}, {"text": "UHH", "start_pos": 221, "end_pos": 224, "type": "DATASET", "confidence": 0.9285922646522522}, {"text": "NHS test set", "start_pos": 309, "end_pos": 321, "type": "DATASET", "confidence": 0.9540105859438578}]}, {"text": "However, we ranked the PIIJT system higher than Hunter given that the former scored 21 times better that the latter (in contrast to 7).", "labels": [], "entities": [{"text": "PIIJT", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.6158031225204468}]}, {"text": "However, there is inadequate data to support assigning a clear difference between the two systems.", "labels": [], "entities": []}, {"text": "Indeed, both systems have similar quality for this language in the Cochrane test set.", "labels": [], "entities": [{"text": "Cochrane test set", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9183203975359598}]}, {"text": "Given that the methods and corpora seem to be largely the same fora particular language, differences in BLEU scores across the test sets are probably related to the the characteristics of these.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.998276948928833}]}, {"text": "Few teams submitted runs for more than one test set and only one team (UHH) submitted runs for all test set (for one particular language).", "labels": [], "entities": [{"text": "UHH", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.931669294834137}]}, {"text": "For Spanish, the UHH team obtained considerable differences in BLEU score for Scielo (around 36), NHS (around 41) and Cochrane (around 48).", "labels": [], "entities": [{"text": "UHH", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.9439152479171753}, {"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9649355411529541}, {"text": "NHS", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.5283881425857544}]}, {"text": "However, their system paper does not give much insight on the reason for such differences.", "labels": [], "entities": []}, {"text": "We can hypothesize that lower scores in the Scielo datasets are due to the fact that the reference translation is not a perfect translation of the source document and sentence alignment was performed automatically.", "labels": [], "entities": [{"text": "Scielo datasets", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8722964227199554}, {"text": "sentence alignment", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7235886454582214}]}, {"text": "For French, the Hunter team obtained lower scores in the EDP test set (around 17) and higher ones in the NHS (almost 23) and Cochrane test sets (around 30).", "labels": [], "entities": [{"text": "EDP test set", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9108162522315979}, {"text": "NHS", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8875422477722168}, {"text": "Cochrane test sets", "start_pos": 125, "end_pos": 143, "type": "DATASET", "confidence": 0.826983114083608}]}, {"text": "Similarly, the UHH team obtained lower scores for the EDP (around 22) and higher ones for Cochrane and NHS (around 31-32).", "labels": [], "entities": [{"text": "UHH", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9445583820343018}, {"text": "EDP", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.8710658550262451}, {"text": "Cochrane", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.8881649971008301}, {"text": "NHS", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.6865792870521545}]}, {"text": "The reason for these differences is probably the same as for the Scielo test set: this is an automatically acquired test set, whose documents were automatically aligned.", "labels": [], "entities": [{"text": "Scielo test set", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.829696257909139}]}, {"text": "While the quality of the automatic alignment is high (estimated at 88% accuracy for Scielo and 94% for EDP), we can also note that the translations in these test sets are created by the authors of the articles who are neither professional translators nor native speakers of the all the languages involved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9975946545600891}, {"text": "EDP", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.534146785736084}]}, {"text": "On the other hand, differences also occurred between the Cochrane and NHS test sets, although these were manually translated by professionals.", "labels": [], "entities": [{"text": "Cochrane and NHS test sets", "start_pos": 57, "end_pos": 83, "type": "DATASET", "confidence": 0.8122506618499756}]}, {"text": "Such differences were small for most systems (24 vs. 20 for Hunter, 22 vs. 19 for UHH, 25 vs. 21 for PIIJT), for German in the Cochrane and NHS test sets, respectively.", "labels": [], "entities": [{"text": "Hunter", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.8342397809028625}, {"text": "UHH", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9182371497154236}, {"text": "PIIJT", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.7613083720207214}, {"text": "NHS test sets", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.9072187741597494}]}, {"text": "However, some cases show larger differences, such as the uedin-nmt system for Romanian (41 vs. 29 for Cochrane and NHS, respectively).", "labels": [], "entities": [{"text": "NHS", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.7167354226112366}]}, {"text": "We observed that that the average sentence length is higher for Cochrane (with some very long sentences included) while there are many short sentence fragments in the NHS test set.", "labels": [], "entities": [{"text": "NHS test set", "start_pos": 167, "end_pos": 179, "type": "DATASET", "confidence": 0.9915355245272318}]}, {"text": "However, both can be problematic for MT as this can scramble long sentences, and trip up over sentence fragments since most of the training data consists of full sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9961490631103516}]}, {"text": "We checked for differences between the manual and automatic evaluations, i.e., whether a team performed better than another in the manual evaluation but the other way round in the automatic evaluation.", "labels": [], "entities": []}, {"text": "We observed small differences for Polish (Cochrane and NHS test sets) between the Hunter and PIIJT teams, but these are probably not significant and both systems have probably similar performance.", "labels": [], "entities": [{"text": "Cochrane and NHS test sets", "start_pos": 42, "end_pos": 68, "type": "DATASET", "confidence": 0.6996789455413819}, {"text": "Hunter and PIIJT teams", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.7489178255200386}]}, {"text": "We observed the same for the UHH and Hunter systems for German (NHS test set).", "labels": [], "entities": [{"text": "UHH", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9540209770202637}, {"text": "German (NHS test set", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.8462629079818725}]}, {"text": "However, we found a more interesting contradiction between Hunter and UHH systems for French in both Cohrane and NHS test sets.", "labels": [], "entities": [{"text": "UHH", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.4872783124446869}, {"text": "French", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.8565506935119629}, {"text": "Cohrane and NHS test sets", "start_pos": 101, "end_pos": 126, "type": "DATASET", "confidence": 0.8427009582519531}]}, {"text": "UHH obtained higher BLEU scores than Hunter (32-33 vs. 30 and 31-33 vs. 23, for Cochrane and NHS, respectively).", "labels": [], "entities": [{"text": "UHH", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9645695686340332}, {"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9997232556343079}, {"text": "NHS", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.47158369421958923}]}, {"text": "However, in the manual evaluation, our expert chose Hunter as being better than UHH in many more sentences (40 vs. 8 and 67 vs. 6, respectively).", "labels": [], "entities": [{"text": "UHH", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.833087682723999}]}], "tableCaptions": [{"text": " Table 1: Overview of the test sets. We present the number of documents and sentences in each test set.", "labels": [], "entities": []}, {"text": " Table 3: Overview of submissions for each language pair and test set: [E]DP, [S]cielo, [C]ochrane  and [N]HS. The number next to the letter indicates the number of runs that the team submitted for the  corresponding test set.", "labels": [], "entities": []}, {"text": " Table 5: Results for the EDP test sets. * indicates  the primary run as declared by the participants.", "labels": [], "entities": [{"text": "EDP test sets", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.8812802235285441}]}, {"text": " Table 6: Results for the Cochrane test sets. * indicates the primary run as informed by the participants.", "labels": [], "entities": [{"text": "Cochrane test sets", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.8501566052436829}]}, {"text": " Table 7: Results for the NHS test sets. * indicates the primary run as informed by the participants.", "labels": [], "entities": [{"text": "NHS test sets", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9880748987197876}]}, {"text": " Table 8: Results for the manual validation for the Scielo test sets. Values are absolute numbers (not  percentages). They might not sum up to 100 due to the skipped sentences.", "labels": [], "entities": [{"text": "Scielo test sets", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.728055993715922}]}, {"text": " Table 9: Results for the manual validation for the EDP test sets. Values are absolute numbers (not  percentages). They might not sum up to 100 due to the skipped sentences.", "labels": [], "entities": [{"text": "EDP test sets", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9121099511782328}]}, {"text": " Table 10: Results for the manual validation for the Cochrane test sets. Values are absolute numbers (not  percentages). They might not sum up to 100 due to the skipped sentences.", "labels": [], "entities": [{"text": "Cochrane test sets", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.8296756148338318}]}, {"text": " Table 11: Results for the manual validation for the NHS test sets. Values are absolute numbers (not  percentages). They might not sum up to 100 due to the skipped sentences.", "labels": [], "entities": [{"text": "NHS test sets", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9825634757677714}]}]}