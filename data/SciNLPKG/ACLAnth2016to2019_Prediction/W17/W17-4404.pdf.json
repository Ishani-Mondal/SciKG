{"title": [{"text": "The Impact of Normalization on Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7613269686698914}]}], "abstractContent": [{"text": "Does normalization help Part-of-Speech (POS) tagging accuracy on noisy, non-canonical data?", "labels": [], "entities": [{"text": "Part-of-Speech (POS) tagging", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.5719284772872925}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9293584823608398}]}, {"text": "To the best of our knowledge , little is known on the actual impact of normalization in a real-world scenario, where gold error detection is not available.", "labels": [], "entities": [{"text": "normalization", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9700416922569275}, {"text": "gold error detection", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.5709983209768931}]}, {"text": "We investigate the effect of automatic nor-malization on POS tagging of tweets.", "labels": [], "entities": [{"text": "POS tagging of tweets", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8628343194723129}]}, {"text": "We also compare normalization to strategies that leverage large amounts of unlabeled data kept in its raw form.", "labels": [], "entities": [{"text": "normalization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9839207530021667}]}, {"text": "Our results show that normalization helps, but does not add consistently beyond just word embedding layer initialization.", "labels": [], "entities": [{"text": "word embedding layer initialization", "start_pos": 85, "end_pos": 120, "type": "TASK", "confidence": 0.5894981324672699}]}, {"text": "The latter approach yields a tagging model that is competitive with a Twitter state-of-the-art tagger.", "labels": [], "entities": []}], "introductionContent": [{"text": "Non-canonical data poses a series of challenges to Natural Language Processing, as reflected in large performance drops documented in a variety of tasks, e.g., on POS tagging (), parsing) and named entity recognition.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6178135871887207}, {"text": "POS tagging", "start_pos": 163, "end_pos": 174, "type": "TASK", "confidence": 0.8251357972621918}, {"text": "parsing", "start_pos": 179, "end_pos": 186, "type": "TASK", "confidence": 0.9676390290260315}, {"text": "named entity recognition", "start_pos": 192, "end_pos": 216, "type": "TASK", "confidence": 0.6396855711936951}]}, {"text": "In this paper we focus on POS tagging and on a particular source of non-canonical language, namely Twitter data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8644854724407196}]}, {"text": "One obvious way to tackle the problem of processing non-canonical data is to build taggers that are specifically tailored to such text.", "labels": [], "entities": []}, {"text": "A prime example is the ARK POS tagger, designed especially to process English Twitter data (), on which it achieves state-of-the-art results.", "labels": [], "entities": [{"text": "ARK POS tagger", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.7269907792409261}]}, {"text": "One drawback of this approach is that non-canonical data is not all of the same kind, so that for non-canonical non-Twitter data or even collections of Twitter samples from  different times, typically anew specifically dedicated tool needs to be created.", "labels": [], "entities": []}, {"text": "The alternative route is to take a general purpose state-of-the-art POS tagger and adapt it to successfully tag non-canonical data.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.6695374846458435}]}, {"text": "In the case of Twitter, one way to go about this is lexical normalization.", "labels": [], "entities": []}, {"text": "It is the task of detecting \"ill-formed\" words (Han and Baldwin, 2011) and replacing them with their canonical counterpart.", "labels": [], "entities": []}, {"text": "To illustrate why this might help, consider the following tweet: \"new pix comming tomoroe\".", "labels": [], "entities": []}, {"text": "An off-the-shelf system such as the Stanford NLP suite 1 makes several mistakes on the raw input, e.g., the verb 'comming' as well as the plural noun 'pix' are tagged as singular noun.", "labels": [], "entities": [{"text": "Stanford NLP suite 1", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.8711957931518555}]}, {"text": "Instead, its normalized form is analyzed correctly, as shown in.", "labels": [], "entities": []}, {"text": "While being a promising direction, we see at least two issues with the assessment of normalization as a successful step in POS tagging noncanonical text.", "labels": [], "entities": [{"text": "normalization", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.9158504605293274}, {"text": "POS tagging noncanonical text", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.9109982401132584}]}, {"text": "Firstly, normalization experiments are usually carried out assuming that the tokens to be normalized are already detected (gold error detection).", "labels": [], "entities": [{"text": "normalization", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9637136459350586}, {"text": "gold error detection", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.6177453001340231}]}, {"text": "Thus little is known on how normalization impacts tagging accuracy in a realworld scenario (not assuming gold error detection).", "labels": [], "entities": [{"text": "tagging", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9649035334587097}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8692957758903503}, {"text": "gold error detection", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.5467210511366526}]}, {"text": "Secondly, normalization is one way to go about processing non-canonical data, but not the only one.", "labels": [], "entities": [{"text": "normalization", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9713322520256042}]}, {"text": "Indeed, alternative approaches leverage the abundance of unlabeled data kept in its raw form.", "labels": [], "entities": []}, {"text": "For instance, such data can be exploited with semi-supervised learning methods.", "labels": [], "entities": []}, {"text": "The advantage of this approach is that portability could be successful also towards domains where normalization is not necessary or crucial.", "labels": [], "entities": []}, {"text": "These observations lead us to the following research questions: Q1 Ina real-world setting, without assuming gold error detection, does normalization help in POS tagging of tweets?", "labels": [], "entities": [{"text": "gold error detection", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.605760415395101}, {"text": "POS tagging of tweets", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.8916347622871399}]}, {"text": "Q2 In the context of POS tagging, is it more beneficial to normalize input data or is it better to work with raw data and exploit large amounts of it in a semi-supervised setting?", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.8664228022098541}]}, {"text": "Q3 To what extent are normalization and semisupervised approaches complementary?", "labels": [], "entities": []}, {"text": "To answer these questions, we run a battery of experiments that evaluate different approaches.", "labels": [], "entities": []}, {"text": "We study the impact of normalization on POS tagging in a realistic setup, i.e., we compare normalizing only unknown words, or words for which we know they need correction; we compare this with a fully automatic normalization model (Section 3).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9223463535308838}]}, {"text": "2. We evaluate the impact of leveraging large amounts of unlabeled data using two approaches: a) deriving various word representations, and studying their effect for model initialization (Section 4.1); b) applying a bootstrapping approach based on selftraining to automatically derive labeled training data, evaluating a range of a-priori data selection mechanisms (Section 4.2).", "labels": [], "entities": []}, {"text": "3. We experiment with combining the most promising methods from both directions, to gain insights on their potential complementarity (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "We run two main sets of POS tagging experiments.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.8625803291797638}]}, {"text": "In the first one, we use normalization in a variety of settings (see Section 3).", "labels": [], "entities": []}, {"text": "In the second one, we leverage large amounts of unlabeled data that does not undergo any normalization but is used as training in a semi-supervised setting (Section 4).", "labels": [], "entities": []}, {"text": "For all experiments we use existing datasets as well as newly created resources, cf. Section 2.1.", "labels": [], "entities": []}, {"text": "The POS model used is described in Section 2.2.", "labels": [], "entities": []}, {"text": "Gray area: no gold normalization layer available.", "labels": [], "entities": []}, {"text": "In this section we report results on the test data, as introduced in Section 2.1.", "labels": [], "entities": []}, {"text": "Our main aim is to compare different approaches for successfully applying a generic stateof-the-art POS tagger to Twitter data.", "labels": [], "entities": []}, {"text": "Therefore we have to assess the contribution of the two methods we explore (normalization and using embeddings) and see how they fare, not only to each other but also in comparison to a state-of-the-art Twitter tagger.", "labels": [], "entities": []}, {"text": "We use the ARK tagger () and retrain it on our dataset for direct comparison with our models.", "labels": [], "entities": []}, {"text": "The ARK system is a conditional random fields tagger, which exploits clusters, lexical features and gazetteers.", "labels": [], "entities": [{"text": "conditional random fields tagger", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.6630805879831314}]}, {"text": "shows the performance of our best models and the ARK tagger on the test datasets.", "labels": [], "entities": [{"text": "ARK tagger", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.4702262878417969}]}, {"text": "Embeddings work considerably better than normalization, which confirms what we found on the DEV data.", "labels": [], "entities": [{"text": "DEV data", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.9824425876140594}]}, {"text": "The combined approach yields the highest accuracy overall evaluation sets, however, it significantly differs from embeddings only on TEST L.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9979363679885864}]}, {"text": "This can be explained by our earlier observation (cf. Table 3), which shows that COMB yields the highest improvement on non-canonical tokens, but the same does not hold for canonical tokens.", "labels": [], "entities": [{"text": "COMB", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.8368321657180786}]}, {"text": "Notice that TEST L does indeed contain the highest proportion of non-canonical tokens.", "labels": [], "entities": []}, {"text": "Our best results on all datasets are comparable to the state-of-the-art results achieved by the ARK tagger.", "labels": [], "entities": [{"text": "ARK tagger", "start_pos": 96, "end_pos": 106, "type": "TASK", "confidence": 0.6933511197566986}]}, {"text": "In we compare the errors made by our system (COMB in: Results on test data (average over 5 runs) compared to ARK-tagger (.", "labels": [], "entities": [{"text": "COMB", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9375497102737427}]}, {"text": "Bold: best result (in case of multiple: no stat.significant difference according to randomization test).", "labels": [], "entities": []}, {"text": "gers obtain the highest performance.", "labels": [], "entities": []}, {"text": "The ARK tagger has difficulties with prepositions (P), which are mistagged as numerals ($).", "labels": [], "entities": [{"text": "ARK tagger", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.6818511784076691}]}, {"text": "These are almost all cases of '2' and '4', which represent Twitter slang for 'to' and 'for', respectively.", "labels": [], "entities": []}, {"text": "Our system performs a lot better on these, due to the normalization model as already observed earlier.", "labels": [], "entities": []}, {"text": "Still regarding prepositions, ARK is better at distinguishing them from adverbs (R), which is a common mistake for our system.", "labels": [], "entities": [{"text": "ARK", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.40011096000671387}]}, {"text": "Our tagger makes more mistakes on confusing proper nouns (\u02c6) with nouns (N) in comparison to ARK.", "labels": [], "entities": [{"text": "ARK", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.8498535752296448}]}], "tableCaptions": [{"text": " Table 2: Accuracy on raw DEV: various pre- trained skip-gram embeddings for initialization.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9947249293327332}]}, {"text": " Table 3:  Effect of different models on  canonical/non-canonical words.", "labels": [], "entities": []}, {"text": " Table 4: Results on test data (average over 5 runs) compared to ARK-tagger (", "labels": [], "entities": [{"text": "ARK-tagger", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.7925847768783569}]}]}