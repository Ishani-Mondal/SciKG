{"title": [{"text": "Analyzing the Impact of Spelling Errors on POS-Tagging and Chunking in Learner English", "labels": [], "entities": []}], "abstractContent": [{"text": "Part-of-speech (POS) tagging and chunk-ing have been used in tasks targeting learner English; however, to the best our knowledge, few studies have evaluated their performance and no studies have revealed the causes of POS-tagging/chunking errors in detail.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5855342268943786}]}, {"text": "Therefore , we investigate performance and analyze the causes of failure.", "labels": [], "entities": []}, {"text": "We focus on spelling errors that occur frequently in learner English.", "labels": [], "entities": []}, {"text": "We demonstrate that spelling errors reduced POS-tagging performance by 0.23% owing to spelling errors , and that a spellchecker is not necessary for POS-tagging/chunking of learner English.", "labels": [], "entities": [{"text": "POS-tagging/chunking of learner English", "start_pos": 149, "end_pos": 188, "type": "TASK", "confidence": 0.7607421278953552}]}], "introductionContent": [{"text": "Part-of-speech (POS) tagging and chunking have been essential components of Natural Language Processing (NLP) techniques that target learner English, such as grammatical error correction and automated essay scoring.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5752402901649475}, {"text": "grammatical error correction", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.567092090845108}, {"text": "essay scoring", "start_pos": 201, "end_pos": 214, "type": "TASK", "confidence": 0.6435292959213257}]}, {"text": "In addition, they are frequently used to extract linguistic features relevant to the given task.", "labels": [], "entities": []}, {"text": "For example, in the CoNLL-2014 Shared Task (), 10 of the 12 teams used one or both POS-tagging and chunking to extract features for grammatical error correction.", "labels": [], "entities": [{"text": "CoNLL-2014 Shared Task", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.7539910872777303}, {"text": "grammatical error correction", "start_pos": 132, "end_pos": 160, "type": "TASK", "confidence": 0.6420368154843649}]}, {"text": "They have also been used for linguistic analysis of learner English, particularly in corpus-based studies.", "labels": [], "entities": []}, {"text": "explored characteristic POS patterns in learner English.", "labels": [], "entities": []}, {"text": "demonstrated that POS sequences obtained by POS-tagging can be used to distinguish between mother tongue interferences effectively.", "labels": [], "entities": []}, {"text": "The heavy dependence on POS-tagging and chunking suggests that failures could degrade the performance of NLP systems and linguistic analyses (.", "labels": [], "entities": []}, {"text": "For example, failure to recognize noun phrases in a sentence could lead to failure in correcting related errors in article use and noun number.", "labels": [], "entities": []}, {"text": "More importantly, such failures make it more difficult to simply count the number of POSs and chunks, thereby causing inaccurate estimates of their distributions.", "labels": [], "entities": []}, {"text": "Note that such estimates are often employed in linguistic analysis, including the above-mentioned studies.", "labels": [], "entities": [{"text": "linguistic analysis", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7199374586343765}]}, {"text": "Despite its importance in related tasks, we also note that few studies have focused on performance evaluations of POS-tagging and chunking.", "labels": [], "entities": []}, {"text": "Only a few studies, including, and, have reported the performance of POS taggers in learner English and found a performance gap between native and learner English.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.7011496722698212}]}, {"text": "However, none of those studies described the root causes of POS-tagging and chunking errors in detail.", "labels": [], "entities": []}, {"text": "Detailed investigations would certainly improve performance, which in turn, would improve related tasks.", "labels": [], "entities": []}, {"text": "Furthermore, to the best of our knowledge, no study has reported chunking performance when applied to learner English.", "labels": [], "entities": []}, {"text": "Unknown words area major cause of POStagging and chunking failures.", "labels": [], "entities": [{"text": "POStagging", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.5460875630378723}, {"text": "chunking", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.9350900650024414}]}, {"text": "In learner English, spelling errors, which occur frequently, area major source of unknown words.", "labels": [], "entities": []}, {"text": "Spell checkers (e.g., Aspell) are used to correct spelling errors prior to POS-tagging and chunking.", "labels": [], "entities": []}, {"text": "However, their effectiveness remains unclear.", "labels": [], "entities": []}, {"text": "Thus, we evaluate the extent to which spelling errors in learner English affect the POS tag-ging and chunking performance.", "labels": [], "entities": [{"text": "POS tag-ging", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.4824615716934204}]}, {"text": "More precisely, we analyze the performance analysis of POStagging/chunking to determine (1) the extent to which performance is reduced due to spelling errors, (2) what types of spelling errors impact the performance, and (3) the effect of correcting spelling errors using a spellchecker.", "labels": [], "entities": [{"text": "POStagging/chunking", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.662312388420105}]}, {"text": "Our analysis demonstrates that employing a spellchecker is not required preliminary step of POS-tagging and chunking for NLP analysis of learner English.", "labels": [], "entities": [{"text": "NLP analysis of learner English", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.8167192459106445}]}], "datasetContent": [{"text": "To evaluate the performance of POS-tagging and chunking, we used the Konan-JIEM (KJ) corpus) , which consists of 3,260 sentences and 30,517 tokens.", "labels": [], "entities": [{"text": "Konan-JIEM (KJ) corpus", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.8981823682785034}]}, {"text": "Note that the essays in the KJ corpus were written by Japanese university students.", "labels": [], "entities": [{"text": "KJ corpus", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.7525167465209961}]}, {"text": "The number of spelling errors targeted in this paper was 654 (i.e., 2.1% of all words).", "labels": [], "entities": [{"text": "spelling errors", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.8806566596031189}]}, {"text": "We used a proprietary dataset comprising English teaching materials for reading comprehen- We formulated the POS-tagging and chunking as a sequence labeling problem.", "labels": [], "entities": []}, {"text": "We used a conditional random field (CRF) () for sequence labeling and CRF++ 3 with default parameters as a CRF tool.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.5970855802297592}]}, {"text": "The features used for POS-tagging were based on the widely used features employed in.", "labels": [], "entities": []}, {"text": "These features consist of surface, original form, presence of specific characters (e.g., numbers, uppercase, and symbols), and prefix and suffix (i.e., affix) information.", "labels": [], "entities": []}, {"text": "In addition to, we used the original forms of words as features.", "labels": [], "entities": []}, {"text": "For the chunking task, we also employed generally used features in this case from.", "labels": [], "entities": [{"text": "chunking task", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.922114223241806}]}, {"text": "These features were based on surface, the original form of the words and POSs.", "labels": [], "entities": []}, {"text": "These features are used in which tools are commonly used for grammatical error correction tasks.", "labels": [], "entities": [{"text": "grammatical error correction tasks", "start_pos": 61, "end_pos": 95, "type": "TASK", "confidence": 0.7791114747524261}]}, {"text": "We also developed a spellchecker for our experiments.", "labels": [], "entities": []}, {"text": "We constructed the spellchecker based on a noisy channel model to capture the influence of spelling errors originating via the mother tongue.", "labels": [], "entities": []}, {"text": "summarizes the spelling correction performance of the spellchecker on the KJ corpus.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7924535870552063}, {"text": "KJ corpus", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.9141627252101898}]}, {"text": "As can be seen, better performance results is demonstrated compared to.", "labels": [], "entities": []}, {"text": "In most previous research into grammatical error correction, a spellchecker is used in a pipeline.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6159411569436392}]}, {"text": "Therefore, we used this pipeline method and treated spelling correction and POS-tagging and chunking as cascading problems.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.9215207099914551}]}, {"text": "For our evaluation metrics, we used accuracy (number of correct tokens / number of tokens in the corpus).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.999342143535614}]}, {"text": "In addition, we counted the number of correct tokens identified despite spelling errors, as well as their preceding and succeeding tokens, to observe the effect of spelling errors had on their surrounding words.", "labels": [], "entities": []}, {"text": "We conducted POS-tagging experiments to investigate the question introduced in Section 2.", "labels": [], "entities": []}, {"text": "We prepared the following five methods: 1.", "labels": [], "entities": []}, {"text": "A POS-tagging system trained with surface, original form, and presence of particular character features (Baseline) 2.", "labels": [], "entities": []}, {"text": "A system with prefix and suffix (affix) features added to the Baseline (Base+Aff) 3.", "labels": [], "entities": [{"text": "Aff", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.7998998165130615}]}, {"text": "The Baseline POS-tagging system with a spellchecker (Base+Checker) 4.", "labels": [], "entities": []}, {"text": "The Base+Aff POS-tagging system with a spellchecker (Base+Aff+Checker) 5.", "labels": [], "entities": []}, {"text": "The Base+Aff POS-tagging system without a spellchecker, i.e., errors were corrected manually (Base+Aff+Gold) summarizes the experimental results for POS-tagging.", "labels": [], "entities": [{"text": "Base+Aff+Gold)", "start_pos": 94, "end_pos": 108, "type": "METRIC", "confidence": 0.6116565267244974}]}, {"text": "The results show the same tendency for POS-tagging trained on in-house data and POS-tagging trained on Penn TreeBank, i.e., Base+Aff+Gold > Base+Aff+Checker > Base+Aff > Base+Checker > Baseline.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.995252788066864}]}, {"text": "Therefore, to simplifying analysis, we used results obtained with the in-house data.", "labels": [], "entities": []}, {"text": "First, we compared Base+Aff to Base+Aff+Gold to determine the influence of spelling errors.", "labels": [], "entities": [{"text": "Base+Aff", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8181848724683126}, {"text": "Base+Aff+Gold", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.7256633758544921}]}, {"text": "Base+Aff+Gold achieved a 0.23% improvement over Base+Aff.", "labels": [], "entities": [{"text": "Base+Aff+Gold", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.634937298297882}, {"text": "Aff", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.6246060729026794}]}, {"text": "From this, we conclude that the POS-tagging performance dropped 0.23% due to spelling errors.", "labels": [], "entities": []}, {"text": "This also indicates that an ideal spellchecker does have a positive impact on POS-tagging.", "labels": [], "entities": []}, {"text": "We also observed that Base+Aff demonstrated 1.3% higher accuracy compared to Baseline.", "labels": [], "entities": [{"text": "Base+Aff", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.7605866392453512}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9993107318878174}]}, {"text": "Similarly, Base+Aff showed higher accuracy than that of Base+Checker.", "labels": [], "entities": [{"text": "Base+Aff", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.6945501367251078}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.999479353427887}]}, {"text": "These results indicate that affix information is important to assigning corresponding POSs in learner English.", "labels": [], "entities": []}, {"text": "Furthermore, there was only a difference of only 0.06% between Base+Aff and Base+Aff+Checker, thereby demonstrating that a spellchecker is not necessary and that it is sufficient to assign POSs using affix information.", "labels": [], "entities": []}, {"text": "shows the number of correct POSs identified for misspelled and surrounding words.", "labels": [], "entities": []}, {"text": "As can be seen by comparing the Baseline to Base+Aff+Gold, the number of correct POSs for misspelled words increased.", "labels": [], "entities": []}, {"text": "In contrast, for the number of correct POSs identified for surrounding words, there was nearly no difference, implying that spelling errors do not influence the accuracy of estimating the POSs of their surrounding words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9980602860450745}]}, {"text": "As with the POS-tagging experiments, we performed chunking experiments on learner English.", "labels": [], "entities": []}, {"text": "As described in Section 1, we examined the performance of chunking in learner English for the first time.", "labels": [], "entities": []}, {"text": "We compared the following three systems: (1) a system using the features presented in Section 3 (Baseline), (2) a baseline chunking system with spell checking (Base+Checker), and (3) a baseline chunking system with no spelling errors, i.e., spelling errors were corrected manually (Base+Gold).", "labels": [], "entities": []}, {"text": "We used POSs that were automatically assigned by the POS-tagger to train our chunking model.", "labels": [], "entities": []}, {"text": "The experimental results on chunking are summarized in.", "labels": [], "entities": [{"text": "chunking", "start_pos": 28, "end_pos": 36, "type": "TASK", "confidence": 0.9819980263710022}]}, {"text": "As can be seen by comparing Baseline to Base+Checker, there was only a 0.03% difference, which has no statistical significance; thus, the spellchecker had nearly no practical effect.", "labels": [], "entities": []}, {"text": "Comparing Baseline to Base+Gold, there was a difference of 0.2% which is statistically significant even though it is only a small difference.", "labels": [], "entities": [{"text": "Base+Gold", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.8958574334780375}]}, {"text": "Thus, we conclude here that an ideal spellchecker has a positive effect on chunking.", "labels": [], "entities": [{"text": "chunking", "start_pos": 75, "end_pos": 83, "type": "TASK", "confidence": 0.9636521935462952}]}, {"text": "However, since chunking uses POSs identified by the POS-tagger as its features, it was assumed that POS-tagging errors would directly affect chunking.", "labels": [], "entities": [{"text": "chunking", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9630913138389587}]}, {"text": "shows the number of correctly identified chunks for misspelled and surrounding words.", "labels": [], "entities": []}, {"text": "As with POS-tagging, the number of correctly identified chunks for misspelled words increased, whereas there was nearly no difference in the number of correctly identified chunks for surrounding words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of spelling error correction", "labels": [], "entities": [{"text": "spelling error correction", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.67363374431928}]}, {"text": " Table 2: Results of POS-tagging. Accuracies of  POS-tagging trained on Penn TreeBank are shown  in parentheses.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9945825934410095}, {"text": "Penn TreeBank", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9957859218120575}]}, {"text": " Table 3: Results of POS tagging for misspelled  words and their surrounding words. s i indicates a  misspelled word.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.8476907908916473}]}, {"text": " Table 5: Results of chunking involving misspelled  words, as well as corresponding preceding and  succeeding words.", "labels": [], "entities": []}]}