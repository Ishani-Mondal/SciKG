{"title": [{"text": "Reconstruction of Word Embeddings from Sub-Word Parameters", "labels": [], "entities": [{"text": "Reconstruction of Word Embeddings from Sub-Word Parameters", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.8011234147208077}]}], "abstractContent": [{"text": "Pre-trained word embeddings improve the performance of a neural model at the cost of increasing the model size.", "labels": [], "entities": []}, {"text": "We propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level.", "labels": [], "entities": []}, {"text": "Our approach is quite simple: before task-specific training, we first optimize sub-word parameters to reconstruct pre-trained word embeddings using various distance measures.", "labels": [], "entities": []}, {"text": "We report interesting results on a variety of tasks: word similarity , word analogy, and part-of-speech tagging.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7813173234462738}, {"text": "word analogy", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.8052378296852112}, {"text": "part-of-speech tagging", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7046613991260529}]}], "introductionContent": [{"text": "Word embeddings trained from a large quantity of unlabled text are often important fora neural model to reach state-of-the-art performance.", "labels": [], "entities": []}, {"text": "They are shown to improve the accuracy of partof-speech (POS) tagging from 97.13 to 97.55, the F1 score of named-entity recognition (NER) from 83.63 to 90.94, and the UAS of dependency parsing from 93.1 to 93.9.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.999553382396698}, {"text": "partof-speech (POS) tagging", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6041923820972442}, {"text": "F1 score", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.982578694820404}, {"text": "named-entity recognition (NER)", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.768891590833664}, {"text": "UAS", "start_pos": 167, "end_pos": 170, "type": "METRIC", "confidence": 0.860578179359436}, {"text": "dependency parsing", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.7435449957847595}]}, {"text": "On the other hand, the benefit comes at the cost of a bigger model which now stores these embeddings as additional parameters.", "labels": [], "entities": []}, {"text": "In this study, we propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level.", "labels": [], "entities": []}, {"text": "Specifically, we optimize the character-level parameters of the model to reconstruct the word embeddings prior to task-specific training.", "labels": [], "entities": []}, {"text": "We frame the problem as distance minimization and consider various metrics suitable for different applications, for example Manhattan distance and negative cosine similarity.", "labels": [], "entities": [{"text": "distance minimization", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.8199816048145294}, {"text": "Manhattan distance", "start_pos": 124, "end_pos": 142, "type": "METRIC", "confidence": 0.6898372173309326}]}, {"text": "While our approach is simple, the underlying learning problem is a challenging one; the subword parameters must reproduce the topology of word embeddings which are not always morphologically coherent (e.g., the meaning of fox does not follow any common morphological pattern).", "labels": [], "entities": []}, {"text": "Nonetheless, we observe that the model can still learn useful patterns.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a variety of tasks: word similarity, word analogy, and POS tagging.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7507830560207367}, {"text": "word analogy", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.8030588626861572}, {"text": "POS tagging", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.8428745865821838}]}, {"text": "We report certain, albeit small, improvement on these tasks, which indicates that the word topology transformation based on pretraining can be beneficial.", "labels": [], "entities": [{"text": "word topology transformation", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6719862818717957}]}, {"text": "The aim and scope of our work are clearly different: we are interested in training a strictly sublexical model that only operates over characters (which has the benefit of smaller model size) and yet somehow exploit pre-trained word embeddings in the process.", "labels": [], "entities": []}], "datasetContent": [{"text": "Implementation We implement our models using the DyNet library.", "labels": [], "entities": [{"text": "DyNet library", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8587688207626343}]}, {"text": "We use the Adam optimizer ( and apply dropout at all LSTM layers ().", "labels": [], "entities": []}, {"text": "For POS tagging and parsing, we perform a 5 \u00d7 5 grid search over learning rates 0.0001 . .", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8904823064804077}, {"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.908259928226471}]}, {"text": "0.0005 and dropout rates 0.1 . .", "labels": [], "entities": []}, {"text": "0.5 and choose the configuration that gives the best performance on the dev set.", "labels": [], "entities": []}, {"text": "We use the highway network (3) for word analogy and parsing and (2) for others.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.792486310005188}]}, {"text": "Note that the character embedding dimension dc must match the dimension of the pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Teacher Word Embeddings We use 100-dimensional word embeddings identical to those used by  which are computed with a variant of the skip n-gram model ( ).", "labels": [], "entities": []}, {"text": "These embeddings have been shown to be effective in various tasks ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect of reconstruction on word simi- larity: the teacher word embeddings obtain score  0.50.", "labels": [], "entities": []}, {"text": " Table 2: Effect of reconstruction on word analogy  (10 reconstruction epochs).", "labels": [], "entities": [{"text": "word analogy", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.7838678061962128}]}, {"text": " Table 3: POS tagging accuracy with different defi- nitions of v w (see the main text). The final column  shows the number of lookup parameters.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7039986550807953}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9861939549446106}]}]}