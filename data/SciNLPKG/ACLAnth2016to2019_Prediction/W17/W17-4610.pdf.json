{"title": [{"text": "Improving coreference resolution with automatically predicted prosodic information", "labels": [], "entities": [{"text": "Improving coreference resolution", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8302196661631266}]}], "abstractContent": [{"text": "Adding manually annotated prosodic information , specifically pitch accents and phrasing, to the typical text-based feature set for coreference resolution has previously been shown to have a positive effect on German data.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.9866769313812256}]}, {"text": "Practical applications on spoken language, however, would rely on automatically predicted prosodic information.", "labels": [], "entities": []}, {"text": "In this paper we predict pitch accents (and phrase boundaries) using a con-volutional neural network (CNN) model from acoustic features extracted from the speech signal.", "labels": [], "entities": []}, {"text": "After an assessment of the quality of these automatic prosodic annotations , we show that they also significantly improve coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.9461506009101868}]}], "introductionContent": [{"text": "Noun phrase coreference resolution is the task of grouping noun phrases (NPs) together that refer to the same discourse entity in a text or dialogue.", "labels": [], "entities": [{"text": "Noun phrase coreference resolution", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7844985276460648}]}, {"text": "In Example (1), taken from, the question for the coreference resolver, besides linking the anaphoric pronoun he back to John, is to decide whether an old cottage and the shed refer to the same entity.", "labels": [], "entities": [{"text": "Example", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.8222798705101013}, {"text": "coreference resolver", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.958963930606842}]}, {"text": "(1) {John} 1 has {an old cottage} 2 . Last year {he} 1 reconstructed {the shed} ? .", "labels": [], "entities": []}, {"text": "Coreference resolution is an active NLP research area, with its own track at most NLP conferences and several shared tasks such as the CoNLL or SemEval shared tasks () or the CORBON shared task 2017 . Almost all work is based on text, although *The two first authors contributed equally to this work.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.927171140909195}, {"text": "CoNLL", "start_pos": 135, "end_pos": 140, "type": "DATASET", "confidence": 0.8388966917991638}, {"text": "CORBON shared task 2017", "start_pos": 175, "end_pos": 198, "type": "DATASET", "confidence": 0.7051620334386826}]}, {"text": "1 http://corbon.nlp.ipipan.waw.pl/ there exist a few systems for pronoun resolution in transcripts of spoken text.", "labels": [], "entities": [{"text": "pronoun resolution in transcripts of spoken text", "start_pos": 65, "end_pos": 113, "type": "TASK", "confidence": 0.7736313598496574}]}, {"text": "It has been shown that there are differences between written and spoken text that lead to a drop in performance when coreference resolution systems developed for written text are applied on spoken text (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.9286530017852783}]}, {"text": "For this reason, it may help to use additional information available from the speech signal, for example prosody.", "labels": [], "entities": []}, {"text": "In West-Germanic languages, such as English and German, there is a tendency for coreferent items, i.e. entities that have already been introduced into the discourse (their information status is given), to be deaccented, as the speaker assumes the entity to be salient in the listener's discourse model (cf.;;).", "labels": [], "entities": []}, {"text": "We can make use of this fact by providing prosodic information to the coreference resolver.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.9135566651821136}]}, {"text": "Example (2), this time marked with prominence information, shows that prominence can help us resolve cases where the transcription is potentially ambiguous 2 . (2) {John} 1 has {an old cottage} 2 . a. Last year {he} 1 reconstructed {the The pitch accent on shed in (2a) leads to the interpretation that the shed and the cottage refer to different entities, where the shed is apart of the cottage (they are in a bridging relation).", "labels": [], "entities": []}, {"text": "In contrast, in (2b), the shed is deaccented, which suggests that the shed and the cottage corefer.", "labels": [], "entities": []}, {"text": "A pilot study by has shown that enhancing the text-based feature set fora coreference resolver, consisting of e.g. automatic part-of-speech (POS) tags and syntactic information, with pitch accents and prosodic phrasing information helps to improve coreference resolution of German spoken text.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.8758638203144073}, {"text": "coreference resolution of German spoken text", "start_pos": 248, "end_pos": 292, "type": "TASK", "confidence": 0.8603337705135345}]}, {"text": "The prosodic labels used in the experiments were annotated manually, which is not only expensive but not applicable in an automatic pipeline setup.", "labels": [], "entities": []}, {"text": "In our paper, we present an experiment in which we replicate the main results from the pilot study by annotating the prosodic information automatically, thus omitting any manual annotations from the feature set.", "labels": [], "entities": []}, {"text": "We show that adding prosodic information significantly helps in all of our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our prosodic features by adding them to the feature set used in the baseline.", "labels": [], "entities": []}, {"text": "We define short NPs to be of length 3 or shorter . In this setup, we apply the feature only to short NPs.", "labels": [], "entities": []}, {"text": "In the all NP setting, the feature is used for all NPs.", "labels": [], "entities": []}, {"text": "The ratio of short vs. longer NPs in DIRNDL is roughly 3:1.", "labels": [], "entities": [{"text": "DIRNDL", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.842878520488739}]}, {"text": "Note that we evaluate on the whole test set in both cases.", "labels": [], "entities": []}, {"text": "We report how the performance of the coreference resolver is affected in three settings: (a) trained and tested on manual prosodic labels (short gold), (b) trained on manual prosodic labels, but tested on automatic labels (this simulates an application scenario where a pre-trained model is applied to new texts (short gold/auto) and (c) trained and tested on automatic prosodic labels (short auto).", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.9316610097885132}]}, {"text": "shows the effect of the pitch accent presence feature on our data.", "labels": [], "entities": []}, {"text": "All features perform significantly better than the baseline . As expected, the numbers are higher if we limit this feature to short NPs.", "labels": [], "entities": []}, {"text": "We believe that this is due to the fact that the feature contributes most when it is most meaningful: on short NPs, a pitch accent makes it more likely for the NP to contain new information, whereas long NPs almost always have at: Nuclear accent presence least one pitch accent, regardless of its information status.", "labels": [], "entities": []}, {"text": "We achieve the highest performance with gold labels, followed by the gold/auto version with a score that is not significantly worse than the gold version.", "labels": [], "entities": []}, {"text": "This is important for applications as it suggests that the loss in performance is small when training on gold data and testing on predicted data.", "labels": [], "entities": []}, {"text": "As expected, the version that is trained and tested on predicted data performs worse, but is still significantly better than the baseline.", "labels": [], "entities": []}, {"text": "Hence, prosodic information is helpful in all three settings.", "labels": [], "entities": []}, {"text": "It also shows that the assumption on short NPs in the pilot study is also true for automatic labels.", "labels": [], "entities": []}, {"text": "shows the effect of adding nuclear accent presence as a feature to the baseline.", "labels": [], "entities": []}, {"text": "Again, we report results that are all significantly better than the baseline.", "labels": [], "entities": []}, {"text": "The improvement is largest when we apply the feature to all NPs, i.e. also including long, complex NPs.", "labels": [], "entities": []}, {"text": "This is inline with the findings in the pilot study for long NPs.", "labels": [], "entities": []}, {"text": "If we restrict ourselves to just nuclear accents, this feature will receive the value true for only a few of the short NPs that would otherwise have been assigned true in terms of general pitch accent presence.", "labels": [], "entities": []}, {"text": "Therefore, nuclear pitch accents do not provide sufficient information fora majority of the short NPs.", "labels": [], "entities": [{"text": "NPs", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8272743821144104}]}, {"text": "For long NPs, however, the presence of a nuclear accent is more meaningful.", "labels": [], "entities": []}, {"text": "The performance of the different systems follows the pattern present for pitch accent type: gold > gold/auto > auto.", "labels": [], "entities": []}, {"text": "Again, automatic prosodic information contributes to the system's performance.", "labels": [], "entities": []}, {"text": "The highest score when using automatic labels is 50.64, as compared to 53.99 with gold labels.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, these are the best results reported on the DIRNDL anaphora dataset so far.", "labels": [], "entities": [{"text": "DIRNDL anaphora dataset", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.8933592240015665}]}], "tableCaptions": [{"text": " Table 1: Pitch accent presence", "labels": [], "entities": [{"text": "Pitch accent", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.664064347743988}]}, {"text": " Table 2: Nuclear accent presence", "labels": [], "entities": [{"text": "Nuclear accent", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6827199012041092}]}]}