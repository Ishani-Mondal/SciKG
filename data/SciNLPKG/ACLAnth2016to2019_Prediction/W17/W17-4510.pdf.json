{"title": [{"text": "Learning to Score System Summaries for Better Content Selection Evaluation", "labels": [], "entities": [{"text": "Learning to Score System Summaries", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.3898888289928436}, {"text": "Content Selection Evaluation", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.7553189396858215}]}], "abstractContent": [{"text": "The evaluation of summaries is a challenging but crucial task of the summarization field.", "labels": [], "entities": [{"text": "evaluation of summaries", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7134556372960409}, {"text": "summarization", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.9864370226860046}]}, {"text": "In this work, we propose to learn an automatic scoring metric based on the human judgements available as part of classical summarization datasets like TAC-2008 and TAC-2009.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 151, "end_pos": 159, "type": "DATASET", "confidence": 0.9190874695777893}, {"text": "TAC-2009", "start_pos": 164, "end_pos": 172, "type": "DATASET", "confidence": 0.9073740839958191}]}, {"text": "Any existing automatic scoring metrics can be included as features , the model learns the combination exhibiting the best correlation with human judgments.", "labels": [], "entities": []}, {"text": "The reliability of the new metric is tested in a further manual evaluation where we ask humans to evaluate summaries covering the whole scoring spectrum of the metric.", "labels": [], "entities": [{"text": "reliability", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9597510695457458}]}, {"text": "We release the trained metric as an open-source tool.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of automatic multi-document summarization is to convert source documents into a condensed text containing the most important information.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.587007611989975}]}, {"text": "In particular, the question of evaluation is notably difficult due to the inherent lack of gold standard.", "labels": [], "entities": [{"text": "evaluation", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.9682238101959229}, {"text": "gold standard", "start_pos": 91, "end_pos": 104, "type": "METRIC", "confidence": 0.8046202063560486}]}, {"text": "The evaluation can be done manually by involving humans in the process of scoring a given system summary.", "labels": [], "entities": []}, {"text": "For example, with the Responsiveness metric, human annotators score summaries on a LIKERT scale ranging from 1 to 5.", "labels": [], "entities": [{"text": "LIKERT scale", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9656099081039429}]}, {"text": "Later, the Pyramid scheme was introduced to evaluate content selection with high inter-annotator agreement (.", "labels": [], "entities": [{"text": "content selection", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7168934941291809}]}, {"text": "Manual evalations are meaningful and reliable but are also expensive and not reproducible.", "labels": [], "entities": []}, {"text": "This makes them unfit for systematic comparison.", "labels": [], "entities": [{"text": "systematic comparison", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.546746015548706}]}, {"text": "Due to the necessity of having cheap and reproducible metrics, a significant body of research was dedicated to the study of automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "Automatic metrics aim to produce a semantic similarity score between the candidate summary and a pool of reference summaries previously written by human annotators.", "labels": [], "entities": []}, {"text": "Some variants rely only on the source documents and the candidate summary ignoring the reference summaries (.", "labels": [], "entities": []}, {"text": "In order to select the best automatic metric, we typically consider manual evalution metrics as our gold standard, then a good automatic metric should reliably predict how well a summarizer would perform if human evaluation was conducted.", "labels": [], "entities": []}, {"text": "In practice, we use the human judgment datasets like the ones constructed during the manual evaluation of the Text Analysis Conference (TAC).", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.8259377976258596}]}, {"text": "The system summaries submitted to the shared tasks were manually scored by trained human annotators following the Responsiveness and/or the Pyramid schemes.", "labels": [], "entities": []}, {"text": "An automatic metric is considered good if it ranks the system summaries similarly as humans did.", "labels": [], "entities": []}, {"text": "Currently, ROUGE) is the accepted standard for automatic evaluation of content selection because of its simplicity and its good correlation with human judgments.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9838501214981079}, {"text": "evaluation of content selection", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6049962118268013}]}, {"text": "However, previous works on evaluation metrics comparison averaged scores of summaries over topics for each system and then computed the correlation with averaged scores given by humans.", "labels": [], "entities": []}, {"text": "ROUGE works well in this scenario which compares only systems after aggregating their scores for many summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9476602673530579}]}, {"text": "We call this scenario system-level correlation analysis.", "labels": [], "entities": [{"text": "system-level correlation analysis", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.6777829627195994}]}, {"text": "A more natural analysis, which we use in this work, is to compute the correlation between the candidate metric and human judgments for each topic indivually and then average these correlations over topics.", "labels": [], "entities": []}, {"text": "In this scenario, which we call summary-level correlation analysis, the performance of ROUGE significantly drops meaning that on average ROUGE does not really identify summary quality, it can only rank systems after aggregation of many topics.", "labels": [], "entities": [{"text": "summary-level correlation analysis", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.6768887341022491}, {"text": "ROUGE", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.8318327069282532}]}, {"text": "In order to advance the field of summarization we need to have more consistent metrics correlating well with humans on every topic and capable of estimating the quality of individual summaries (not just systems).", "labels": [], "entities": [{"text": "summarization", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9922614693641663}]}, {"text": "We propose to rely on human judgment datasets to learn an automatic scoring metric.", "labels": [], "entities": []}, {"text": "The learned metric presents the advantage of being explicitly trained to exhibit high correlation with the \"goldstandard\" human judgments at the summary level (and not just at the system level).", "labels": [], "entities": []}, {"text": "The setup is also convenient because any already existing automatic metric can be incorporated as a feature and the model learns the best combination of features matching human judgments.", "labels": [], "entities": []}, {"text": "We should worry whether the learned metric is reliable.", "labels": [], "entities": []}, {"text": "Indeed, typical human judgment datasets (like the ones from TAC-2008 or TAC-2009) contain manual scores only for several system summaries which have a limited range of quality.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9069721698760986}, {"text": "TAC-2009", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.6679972410202026}]}, {"text": "We conduct a manual evaluation specifically designed to test the metric accross its whole scoring spectrum.", "labels": [], "entities": []}, {"text": "To summarize our contributions: We performed a summary-level correlation analysis to compare a large set of existing evaluation metrics.", "labels": [], "entities": []}, {"text": "We learned anew evaluation metric as a combination of existing ones to maximize the summary-level correlation with human judgments.", "labels": [], "entities": []}, {"text": "We conducted a manual evaluation to test whether learning from available human judgment datasets yields a reliable metric accross its whole scoring spectrum.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted both automatic and manual testing of the learned metric.", "labels": [], "entities": []}, {"text": "We present here the datasets and results of the experiments.", "labels": [], "entities": []}, {"text": "We use two multi-document summarization datasets from the Text Analysis Conference (TAC) shared tasks: TAC-2008 and TAC-2009.", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.7709216227134069}, {"text": "TAC-2008", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.7835109233856201}, {"text": "TAC-2009", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.9025452136993408}]}, {"text": "TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9648954272270203}, {"text": "TAC-2009", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8658496141433716}]}, {"text": "Each topic consists of 10 news articles to be summarized in a maximum of 100 words.", "labels": [], "entities": []}, {"text": "We use only the so-called initial summaries (A summaries), but not the update part.", "labels": [], "entities": []}, {"text": "For each topic, there are 4 human reference summaries.", "labels": [], "entities": []}, {"text": "In both editions, all system summaries and the 4 reference summaries were manually evaluated by NIST assessors for readability, content selection (with Pyramid) and overall responsiveness.", "labels": [], "entities": [{"text": "content selection", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.681937038898468}]}, {"text": "At the time of the shared tasks, 57 systems were submitted to TAC-2008 and 55 to TAC-2009.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9572486281394958}, {"text": "TAC-2009", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.973704993724823}]}, {"text": "For our experiments, we use the Pyramid and the responsiveness annotations.", "labels": [], "entities": [{"text": "Pyramid", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9685639142990112}]}, {"text": "With our notations, for example with TAC-2009, we haven = 55 scored system summaries, m = 44 topics, Di contains 10 documents and \u03b8 i contains 4 reference summaries.", "labels": [], "entities": [{"text": "TAC-2009", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8934145569801331}]}, {"text": "We also use the recently created German dataset DBS-corpus (.", "labels": [], "entities": [{"text": "German dataset DBS-corpus", "start_pos": 33, "end_pos": 58, "type": "DATASET", "confidence": 0.8666089574495951}]}, {"text": "It contains 10 topics consisting of 4 to 14 documents each.", "labels": [], "entities": []}, {"text": "The summaries have variable sizes and are about 500 words long.", "labels": [], "entities": []}, {"text": "For each topic, 5 summaries were evaluated by trained human annotators but only for content selection with Pyramid.", "labels": [], "entities": []}, {"text": "We experiment with this dataset because it contains heterogeneous sources (different text types) in German about the educational domain.", "labels": [], "entities": []}, {"text": "This contrasts with the English homogeneous news documents from TAC-2008 and TAC-2009.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.5397976636886597}, {"text": "TAC-2009", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.8937570452690125}]}, {"text": "Thus, we can test our technique in a different summarization setup.", "labels": [], "entities": [{"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9641796946525574}]}], "tableCaptions": [{"text": " Table 1: Correlation of automatic metrics with human judgments for TAC-2008 and TAC-2009.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.7306367754936218}, {"text": "TAC-2009", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.8608487844467163}]}, {"text": " Table 3: Percentage of topics for which the correlation between the metric and human judgments is  below the chosen thresholds for", "labels": [], "entities": []}, {"text": " Table 4: Correlation of automatic metrics with hu- man accross the whole scoring spectrum of S 3  best .", "labels": [], "entities": []}]}