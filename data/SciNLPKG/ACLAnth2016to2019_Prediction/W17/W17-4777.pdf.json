{"title": [], "abstractContent": [{"text": "Following upon the last year's CUNI system for automatic post-editing of machine translation output, we focus on exploiting the potential of sequence-to-sequence neural models for this task.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.7408217986424764}]}, {"text": "In this system description paper, we compare several encoder-decoder architectures on a smaller-scale models and present the system we submitted to WMT 2017 Automatic Post-Editing shared task based on this preliminary comparison.", "labels": [], "entities": [{"text": "WMT 2017 Automatic Post-Editing shared task", "start_pos": 148, "end_pos": 191, "type": "DATASET", "confidence": 0.8408196369806925}]}, {"text": "We also show how simple inclusion of synthetic data can improve the overall performance as measured by an automatic evaluation metric.", "labels": [], "entities": []}, {"text": "Lastly, we list few example outputs generated by our post-editing system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Even with the recent substantial improvements of the machine translation (MT) quality mainly thanks to the increasingly popular neural models (neural MT, NMT), many errors still remain in the output require further post-editing.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8143625676631927}]}, {"text": "This can be done manually, or as the automatic post-editing (APE) task expects, automatically.", "labels": [], "entities": []}, {"text": "When phrase-based machine translation (PBMT) was the indisputable state of the art, some automatic post-editing (APE) systems were based on the PBMT techniques (.", "labels": [], "entities": [{"text": "phrase-based machine translation (PBMT)", "start_pos": 5, "end_pos": 44, "type": "TASK", "confidence": 0.7760056356588999}]}, {"text": "With source-sentence information), post-editing results were quite promising.", "labels": [], "entities": []}, {"text": "It is therefore not surprising that with the rise of the neural machine translation, neural APE systems based on the findings in NMT research were built ( and even won last year's WMT16 Shared Task.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6993415355682373}, {"text": "WMT16 Shared Task", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.49297388394673664}]}, {"text": "In this paper, we present a baseline comparison of several recent neural sequence-to-sequence architectures, motivations behind our primary submission for the WMT17 Shared Task and further improvements of this submission with regard to model size and additional synthetic data.", "labels": [], "entities": [{"text": "WMT17 Shared Task", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.48135635256767273}]}], "datasetContent": [{"text": "In automatic post-editing, we are expected to take the output of an MT system that usually contains various errors (morphological, lexical etc.) and to generate a corrected version of the output.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9568718075752258}]}, {"text": "Most of the time, there is also additional information available, e.g. the original sentence in the source language and sometimes also some internal scores or features from the primary MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 185, "end_pos": 187, "type": "TASK", "confidence": 0.9581800699234009}]}, {"text": "We evaluated these three models using the WMT16 APE test set 3 , computing the BLEU score on the produced outputs: baseline CHAR2CHAR setup (Baseline), the model trained with synthetic data (Synth) and the model which produces edit operations instead of complete sentences (Synth+editops).", "labels": [], "entities": [{"text": "WMT16 APE test set 3", "start_pos": 42, "end_pos": 62, "type": "DATASET", "confidence": 0.8771266579627991}, {"text": "BLEU score", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9769443571567535}, {"text": "baseline CHAR2CHAR setup", "start_pos": 115, "end_pos": 139, "type": "METRIC", "confidence": 0.8105531533559164}]}, {"text": "shows the results of the evaluation.", "labels": [], "entities": []}, {"text": "We can see that even when we choose the best architecture based on the relative comparison and increase the model capacity (\"Baseline\"), it is still not enough to even get close to the original MT output quality (\"Original MT\").", "labels": [], "entities": []}, {"text": "Introducing additional synthetic data: Automatic evaluation of the final 8GB APE setups.", "labels": [], "entities": []}, {"text": "The score of the original MT output is shown for comparison.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9530901908874512}]}, {"text": "The \u00b1 values are empirical confidence intervals reflecting the variance in the test set).", "labels": [], "entities": []}, {"text": "We chose this system as our primary submission for the WMT16 APE task.", "labels": [], "entities": [{"text": "WMT16 APE task", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7990167935689291}]}, {"text": "We were a little surprised that there was no improvement when using model that learned to generate post-editing operations (\"Synth+editops\").", "labels": [], "entities": []}, {"text": "When we manually examined the generated output, we found out that the system took the safer path of keeping most of the machine translation output because it probably resulted in fewer errors than trying to change it.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.6419617533683777}]}, {"text": "This could be probably avoided by discouraging the model from keeping the whole MT output unchanged and we plan investigating this approach in the future.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.8889195919036865}]}, {"text": "Even though we did not perform a thorough manual evaluation, we present some examples of our submitted system (\"Synth\") outputs to give the reader some insight to the model performance in.", "labels": [], "entities": []}, {"text": "Our post-editing helped with the main verb, but in other cases, it also damaged the sentence structure or introduced spelling errors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluation of the proposed ar- chitectures we trained. The model size was down- scaled to 5GB due to the limited computation re- sources.", "labels": [], "entities": []}]}