{"title": [{"text": "CHRF ++ : words helping character n-grams", "labels": [], "entities": []}], "abstractContent": [{"text": "Character n-gram F-score (CHRF) is shown to correlate very well with human relative rankings of different machine translation outputs, especially for morphologically rich target languages.", "labels": [], "entities": [{"text": "Character n-gram F-score (CHRF)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.7345990588267645}]}, {"text": "However, its relation with direct human assessments is not yet clear.", "labels": [], "entities": []}, {"text": "In this work, Pearson's correlation coefficients for direct assessments are investigated for two currently available target languages, English and Russian.", "labels": [], "entities": []}, {"text": "First, different \u03b2 parameters (in range from 1 to 3) are re-investigated with direct assessment, and it is confirmed that \u03b2 = 2 is the optimal option.", "labels": [], "entities": []}, {"text": "Then separate character and word n-grams are investigated , and the main finding is that, apart from character n-grams, word 1-grams and 2-grams also correlate rather well with direct assessments.", "labels": [], "entities": []}, {"text": "Further experiments show that adding word unigrams and bi-grams to the standard CHRF score improves the correlations with direct assessments , though it is still not clear which option is better, unigrams only (CHRF+) or unigrams and bigrams (CHRF++).", "labels": [], "entities": []}, {"text": "This should be investigated in future work on more target languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent investigations have shown that the character n-gram F-score (CHRF) represents a very promising evaluation metric for machine translation, especially for morphologically rich target languages -it is fast, it does not require any additional tools or information, it is language independent and tokenisation independent, and it correlates very well with human relative rankings (RR).", "labels": [], "entities": [{"text": "character n-gram F-score (CHRF)", "start_pos": 42, "end_pos": 73, "type": "METRIC", "confidence": 0.749208057920138}, {"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7790908217430115}, {"text": "relative rankings (RR)", "start_pos": 364, "end_pos": 386, "type": "METRIC", "confidence": 0.6781572222709655}]}, {"text": "In order to produce these rankings, human annotators have to decide which sentence translation is better/worse than another without giving any note about the absolute quality of any of the evaluated translations.", "labels": [], "entities": []}, {"text": "This type of human judgment has been the offical evaluation metric and gold standard for all automatic metrics at WMT shared tasks from 2008 until 2016.", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7417001724243164}]}, {"text": "Another type of human judgment, direct human assessment (DA) (, has become additional official evaluation metric for, and the only one for WMT-17.", "labels": [], "entities": [{"text": "direct human assessment (DA)", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.5388004978497823}, {"text": "WMT-17", "start_pos": 139, "end_pos": 145, "type": "DATASET", "confidence": 0.6731154322624207}]}, {"text": "These assessments consist of absolute quality scores for each translated sentence.", "labels": [], "entities": []}, {"text": "Contrary to RR, the relation between CHRF and DA has still not been investigated systematically.", "labels": [], "entities": [{"text": "RR", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.7166888117790222}]}, {"text": "Preliminary experiments in previous work shown that, concerning DA, the main advantage of character-based Fscore CHRF in comparison to word-based F-score WORDF is better correlation for good translations for which WORDF often assigns too low scores.", "labels": [], "entities": [{"text": "Fscore CHRF", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.8696529269218445}, {"text": "correlation", "start_pos": 170, "end_pos": 181, "type": "METRIC", "confidence": 0.967607319355011}]}, {"text": "In this work, we systematically investigate relations between DA and both character and word n-grams, as well as their combinations.", "labels": [], "entities": []}, {"text": "The scores are calculated for all available translation outputs from the WMT-15 and WMT-16 shared tasks () which contain two target languages, English (translated from Czech, German, Finnish, Romanian, Russian and Turkish) and Russian (translated from English), and then compared with DAs on segment level using Pearsons's correlation coefficient.", "labels": [], "entities": [{"text": "WMT-15", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9233689308166504}, {"text": "WMT-16", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.61037677526474}, {"text": "DAs", "start_pos": 285, "end_pos": 288, "type": "METRIC", "confidence": 0.9903815388679504}, {"text": "Pearsons's correlation coefficient", "start_pos": 312, "end_pos": 346, "type": "METRIC", "confidence": 0.9094771146774292}]}, {"text": "2 n-gram based F-scores The general formula for an n-gram based F-score is: where ngrP and ngrR stand for n-gram precision and recall arithmetically averaged overall ngrams from n = 1 to N: \u2022 ngrP n-gram precision: percentage of n-grams in the hypothesis which have a counterpart in the reference; \u2022 ngrR n-gram recall: percentage of n-grams in the reference which are also present in the hypothesis. and \u03b2 is a parameter which assigns \u03b2 times more weight to recall than to precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9421367049217224}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9526609182357788}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.6892827749252319}, {"text": "recall", "start_pos": 312, "end_pos": 318, "type": "METRIC", "confidence": 0.9060532450675964}, {"text": "precision", "start_pos": 474, "end_pos": 483, "type": "METRIC", "confidence": 0.9910412430763245}]}, {"text": "WORDF is then calculated on word n-grams and CHRF is calculated on character n-grams.", "labels": [], "entities": [{"text": "WORDF", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9749674201011658}, {"text": "CHRF", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.99565190076828}]}, {"text": "As for maximum n-gram length N, previous work reported that there is no need to go beyond N=4 for WORDF) and N=6 for CHRF.", "labels": [], "entities": []}, {"text": "CHRF++ score is obtained when the word ngrams are added to the character n-grams and averaged together.", "labels": [], "entities": [{"text": "CHRF++ score", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8534395893414816}]}, {"text": "The best maximum n-gram lengths for such combinations are again N=6 for character n-grams and N=2 or N=1 for word ngrams, which will be discussed in Section 4.3. 3 Motivation for adding word n-grams to CHRF A preliminary experiment on a small set of texts reported in previous work) with different target languages and different types of DA 1 shown that for poorly rated sentences, the standard deviations of CHRF and WORDF scores are similar -both metrics assign relatively similar (low) scores.", "labels": [], "entities": []}, {"text": "On the other hand, for the sentences with higher human rates, the deviations for CHRF are (much) lower.", "labels": [], "entities": [{"text": "CHRF", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9004545211791992}]}, {"text": "In addition, the higher the human rating is, the greater is the difference between the WORDF and CHRF deviations.", "labels": [], "entities": [{"text": "WORDF", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.5340807437896729}, {"text": "CHRF", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.5676920413970947}]}, {"text": "These results indicate that CHRF is better than WORDF mainly for segments/systems of higher translation quality -the CHRF scores for good translations are more concentrated in the higher range, whereas the WORDF scores are often too low.", "labels": [], "entities": [{"text": "CHRF", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9068824052810669}]}, {"text": "In order to further investigate these premises, scatter plots in are produced for CHRF and WORDF with DA for the Russian\u2192English and English\u2192Russian WMT-16 data.", "labels": [], "entities": [{"text": "CHRF", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.9510457515716553}, {"text": "WORDF", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.7935824990272522}, {"text": "DA", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9943523406982422}, {"text": "WMT-16 data", "start_pos": 149, "end_pos": 160, "type": "DATASET", "confidence": 0.7774547934532166}]}, {"text": "1 none of them equal to the variant used in WMT confirms the findings from previous work, since a number of WORDF values is indeed pessimistic -high DA but low WORDF, whereas CHRF values are more concentrated, i.e. correlate better with DA values.", "labels": [], "entities": [{"text": "WMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.6883957386016846}, {"text": "WORDF", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.9463762640953064}, {"text": "DA", "start_pos": 149, "end_pos": 151, "type": "METRIC", "confidence": 0.9600157737731934}, {"text": "WORDF", "start_pos": 160, "end_pos": 165, "type": "METRIC", "confidence": 0.9835038185119629}]}, {"text": "However, these plots raised another question -are CHRF scores maybe too optimistic (i.e. segments with high CHRF score and low DA score)?", "labels": [], "entities": [{"text": "CHRF score and low DA score", "start_pos": 108, "end_pos": 135, "type": "METRIC", "confidence": 0.7555971095959345}]}, {"text": "Certainly not to such extent as WORDF scores are pessimistic, but still, could some combination of character and word n-grams improve the correlations of CHRF?", "labels": [], "entities": [{"text": "correlations", "start_pos": 138, "end_pos": 150, "type": "METRIC", "confidence": 0.9542551636695862}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pearson's correlation coefficients of CHRF and WORDF with direct human assessments (DA)  for different \u03b2 parameters. Bold represents the best character level value and underline represents the  best word level value. The best \u03b2 values are 2 and 3.", "labels": [], "entities": []}, {"text": " Table 2: Pearson's correlation coefficients of CHRF and WORDF with direct human assessments (DA)  for individual character and word n-grams. Bold represents the best character level value and underline  represents the best word level value.", "labels": [], "entities": []}, {"text": " Table 3: Pearson's correlation coefficients with direct human assessments (DA) of CHRF enhanced with  word n-grams together with CHRF and two variants of WORDF: N=4 and N=2. Bold represents the best  overall value.", "labels": [], "entities": []}]}