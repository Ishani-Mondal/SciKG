{"title": [{"text": "TAG Parser Evaluation using Textual Entailments", "labels": [], "entities": [{"text": "TAG Parser Evaluation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.846047600110372}]}], "abstractContent": [{"text": "Parser Evaluation using Textual Entail-ments (PETE, Yuret et al.", "labels": [], "entities": [{"text": "Parser Evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8322210013866425}]}, {"text": "(2013)) is a restricted textual entailment task designed to evaluate in a uniform manner parsers that produce different representations of syntactic structure.", "labels": [], "entities": []}, {"text": "In PETE, entailments can be resolved using syntactic relations alone, and do not implicate lexical semantics or world knowledge.", "labels": [], "entities": []}, {"text": "We evaluate TAG parsers on the PETE task, and compare our results to the state-of-the-art.", "labels": [], "entities": [{"text": "TAG parsers", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.7817402482032776}, {"text": "PETE task", "start_pos": 31, "end_pos": 40, "type": "TASK", "confidence": 0.4703514873981476}]}, {"text": "Our TAG parser combined with structural transformations to compute entailments outper-forms the CCG-based results on the development set, though it falls behind these results on the test set.", "labels": [], "entities": []}, {"text": "The CCG parser makes use of a number of heuristics for entailment comparison, however.", "labels": [], "entities": [{"text": "entailment comparison", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.929936021566391}]}, {"text": "Adding such heuristics to our best TAG parser yields state-of-the-art results on the test set when using accuracy as a metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.997334361076355}]}, {"text": "This sensitivity to heuristics suggests that the PETE task may suffer from an unrepresentative development set, and that we need to improve upon formalism-independent parsing evaluation methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been a flurry of recent work, involving neural network architectures, on parsing that has improved performance across a variety of frameworks that make different assumptions about the target output for the parsing process: Dependency grammar (, Combinatory Categorial Grammar (CCG) (, Tree Adjoining Grammar (TAG) (), Constituent structure ( ).", "labels": [], "entities": [{"text": "parsing", "start_pos": 83, "end_pos": 90, "type": "TASK", "confidence": 0.9737489819526672}]}, {"text": "However, it is as yet unknown the degree to which these improvements in parsing scores contribute to downstream NLP tasks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9737681150436401}]}, {"text": "Moreover, since the different frameworks make different representational assumptions about the target of the parsing process, these results are not directly comparable.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.9133213758468628}]}, {"text": "Parser Evaluation using Textual Entailments (PETE) is a shared task from the SemEval-2010 Exercises on Semantic Evaluation ().", "labels": [], "entities": [{"text": "Parser Evaluation using Textual Entailments (PETE)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8205415233969688}, {"text": "SemEval-2010 Exercises on Semantic Evaluation", "start_pos": 77, "end_pos": 122, "type": "TASK", "confidence": 0.6428351879119873}]}, {"text": "The task was intended to evaluate syntactic parsers across different formalisms, focusing on entailments that could be determined entirely on the basis of the syntactic representations of the sentences that are involved, without recourse to lexical semantics logical reasoning or world knowledge.", "labels": [], "entities": []}, {"text": "For instance, syntactic knowledge alone tells us that the sentence Peter, who loves Mary, left the room entails Peter left the room and Peter loves Mary but not, for example, that Peter knows Mary or that Peter was no longer in the room.", "labels": [], "entities": []}, {"text": "In this paper, we apply a number of TAG parsers to the PETE task.", "labels": [], "entities": [{"text": "TAG parsers", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.6477445811033249}, {"text": "PETE task", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.6934851109981537}]}, {"text": "In the next section, we discuss the PETE task in further detail.", "labels": [], "entities": [{"text": "PETE task", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.7918769717216492}]}, {"text": "Then in Section 3 we describe how we apply TAG parses to this task.", "labels": [], "entities": [{"text": "TAG parses", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.8161222338676453}]}, {"text": "Doing so requires a means of determining whether one TAG derivation entails another syntactically.", "labels": [], "entities": [{"text": "TAG derivation entails", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.8651490410168966}]}, {"text": "We do this through a set of taskindependent, linguistically-motivated transformations.", "labels": [], "entities": []}, {"text": "After reviewing the TAG supertaggers and parsers we evaluate in Sections 4 and 5, we discuss our results in Section 6.", "labels": [], "entities": []}, {"text": "We demonstrate that improvements in TAG parsing and supertagging do indeed contribute to improvements in the extrinsic PETE task, reaching state-of-the-art results inaccuracy and near state-of-the-art in fmeasure.", "labels": [], "entities": [{"text": "TAG parsing", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.7789356708526611}]}, {"text": "In particular, we compare our results to the top-scoring systems of and SCHWA (, both based on the CCG parser, as well as a later system based on an HPSG-Minimal Recursion Semantics parser).", "labels": [], "entities": [{"text": "SCHWA", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.9617751836776733}]}, {"text": "We also conduct an error analysis and discuss limitations of TAG parsing in the context of this task.", "labels": [], "entities": [{"text": "TAG parsing", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9715088605880737}]}, {"text": "2 The PETE Task) is a restricted instance of the recognizing textual entailment (RTE) task, aimed at evaluating syntactic parsers.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE) task", "start_pos": 49, "end_pos": 90, "type": "TASK", "confidence": 0.7198371035712106}]}, {"text": "As in other RTE tasks, the task includes a set of TextHypothesis pairs, for which a system must determine whether or not the content of the Text entails the content of the Hypothesis.", "labels": [], "entities": [{"text": "RTE tasks", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.9229060709476471}]}, {"text": "For PETE, the texts are individual sentences that were drawn from one of three sources: the Unbounded Dependency Corpus (, the Brown section of the Penn Treebank, and a list of sentences in the Penn Treebank on which the Charniak parser) performed poorly.", "labels": [], "entities": [{"text": "Unbounded Dependency Corpus", "start_pos": 92, "end_pos": 119, "type": "DATASET", "confidence": 0.6448621650536855}, {"text": "Brown section of the Penn Treebank", "start_pos": 127, "end_pos": 161, "type": "DATASET", "confidence": 0.849125862121582}, {"text": "Penn Treebank", "start_pos": 194, "end_pos": 207, "type": "DATASET", "confidence": 0.9941969811916351}]}, {"text": "A sentence S from these sources were selected as a candidate Text if S was misparsed by at least one phrase structure or dependency parsers that was state-of-the-art in 2009.", "labels": [], "entities": []}, {"text": "Given a candidate Text T, an associated Hypothesis H was constructed by identifying a pair of content words in T whose syntactic relationship is implicated in the difference between the gold parse and the incorrect parse.", "labels": [], "entities": []}, {"text": "These words were then used to form a minimal sentence.", "labels": [], "entities": []}, {"text": "If such a sentence involved a verb that required additional arguments, these could be filled in with indefinite expressions (somebody, someone, and something) or the verb could be passivized.", "labels": [], "entities": []}, {"text": "Similarly, in the case of ahead and modifier, a copular sentence, or one involving existential there, could be constructed.", "labels": [], "entities": []}, {"text": "The resulting T-H pair would then be assigned the label 'YES' if the content words in H stand in a relation that is also present in the gold parse of T, and is otherwise assigned the label 'NO'.", "labels": [], "entities": [{"text": "YES", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9575422406196594}, {"text": "NO", "start_pos": 190, "end_pos": 192, "type": "METRIC", "confidence": 0.8746799826622009}]}, {"text": "Each of the resulting T-H pairs were then given to five untrained annotators on Amazon Mechanical Turk and was retained in the dataset if three of them agreed on the presence or absence of the entailment.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.9643680055936178}]}, {"text": "This left a dataset containing 367 T-H pairs (of which 51.83% were labeled 'YES').", "labels": [], "entities": [{"text": "YES", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.8681426644325256}]}, {"text": "These were then randomly divided into a development set containing 66 sentences and a test set containing 301 sentences.", "labels": [], "entities": []}, {"text": "For more details on the construction process, see.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Intrinsic task parsing results on the development and test sets.", "labels": [], "entities": [{"text": "Intrinsic task parsing", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.5923230151335398}]}, {"text": " Table 2: PETE task previous system scores and TAG system scores on dev and test sets, using structural  transformations together with either our notion of subderivation or Cambridge's heuristics. Accuracy  (A) gives the percentage of correct answers for both YES and NO. Precision (P), recall (R) and F1 are  calculated for YES.", "labels": [], "entities": [{"text": "TAG", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9444192051887512}, {"text": "Accuracy  (A)", "start_pos": 197, "end_pos": 210, "type": "METRIC", "confidence": 0.9528226107358932}, {"text": "Precision (P)", "start_pos": 272, "end_pos": 285, "type": "METRIC", "confidence": 0.9590187966823578}, {"text": "recall (R)", "start_pos": 287, "end_pos": 297, "type": "METRIC", "confidence": 0.9617560654878616}, {"text": "F1", "start_pos": 302, "end_pos": 304, "type": "METRIC", "confidence": 0.9964684247970581}]}]}