{"title": [{"text": "An Exploration of Word Embedding Initialization in Deep-Learning Tasks", "labels": [], "entities": [{"text": "Word Embedding Initialization", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.7361510396003723}]}], "abstractContent": [{"text": "Word embeddings are the interface between the world of discrete units of text processing and the continuous, differen-tiable world of neural networks.", "labels": [], "entities": []}, {"text": "In this work, we examine various random and pretrained initialization methods for em-beddings used in deep networks and their effect on the performance on four NLP tasks with both recurrent and convolu-tional architectures.", "labels": [], "entities": []}, {"text": "We confirm that pre-trained embeddings area little better than random initialization, especially considering the speed of learning.", "labels": [], "entities": []}, {"text": "On the other hand, we do not see any significant difference between various methods of random initialization, as long as the variance is kept reasonably low.", "labels": [], "entities": []}, {"text": "High-variance ini-tialization prevents the network to use the space of embeddings and forces it to use other free parameters to accomplish the task.", "labels": [], "entities": []}, {"text": "We support this hypothesis by observing the performance in learning lexical relations and by the fact that the network can learn to perform reasonably in its task even with fixed random embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Embeddings or lookup tables () are used for units of different granularity, from characters () to subword units () up to words.", "labels": [], "entities": []}, {"text": "In this paper, we focus solely on word embeddings (embeddings attached to individual token types in the text).", "labels": [], "entities": []}, {"text": "In their highly dimensional vector space, word embeddings are capable of representing many aspects of similarities between words: semantic relations or morphological properties () in one language or cross-lingually (.", "labels": [], "entities": []}, {"text": "Embeddings are trained fora task.", "labels": [], "entities": []}, {"text": "In other words, the vectors that embeddings assign to each word type are almost never provided manually but always discovered automatically in a neural network trained to carryout a particular task.", "labels": [], "entities": []}, {"text": "The well known embeddings are those by, where the task is to predict the word from its neighboring words (CBOW) or the neighbors from the given word (Skip-gram).", "labels": [], "entities": []}, {"text": "Trained on a huge corpus, these \"Word2Vec\" embeddings show an interesting correspondence between lexical relations and arithmetic operations in the vector space.", "labels": [], "entities": []}, {"text": "The most famous example is the following: v(king) \u2212 v(man) + v(woman) \u2248 v(queen) In other words, adding the vectors associated with the words 'king' and 'woman' while subtracting 'man' should be equal to the vector associated with the word 'queen'.", "labels": [], "entities": []}, {"text": "We can also say that the difference vectors v(king) \u2212 v(queen) and v(man) \u2212 v(woman) are almost identical and describe the gender relationship.", "labels": [], "entities": []}, {"text": "Word2Vec is not trained with a goal of proper representation of relationships, therefore the absolute accuracy scores around 50% do not allow to rely on these relation predictions.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9540797472000122}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.8884583711624146}]}, {"text": "Still, it is a rather interesting property observed empirically in the learned space.", "labels": [], "entities": []}, {"text": "Another extensive study of embedding space has been conducted by.", "labels": [], "entities": []}, {"text": "Word2Vec embeddings as well as GloVe embeddings () became very popular and they were tested in many tasks, also because for English they can be simply downloaded as pretrained on huge corpora.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9521859884262085}]}, {"text": "Word2Vec was trained on 100 billion words Google News 56 dataset 1 and GloVe embeddings were trained on 6 billion words from the Wikipedia.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9629292488098145}, {"text": "Google News 56 dataset", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.9621903747320175}]}, {"text": "Sometimes, they are used as a fixed mapping fora better robustness of the system), but they are more often used to seed the embeddings in a system and they are further trained in the particular end-to-end application.", "labels": [], "entities": []}, {"text": "In practice, random initialization of embeddings is still more common than using pretrained embeddings and it should be noted that pretrained embeddings are not always better than random initialization ().", "labels": [], "entities": []}, {"text": "We are not aware of any study of the effects of various random embeddings initializations on the training performance.", "labels": [], "entities": []}, {"text": "In the first part of the paper, we explore various English word embeddings initializations in four tasks: neural machine translation (denoted MT in the following for short), language modeling (LM), part-of-speech tagging (TAG) and lemmatization (LEM), covering both common styles of neural architectures: the recurrent and convolutional neural networks, RNN and CNN, resp.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 106, "end_pos": 132, "type": "TASK", "confidence": 0.6967895030975342}, {"text": "language modeling (LM)", "start_pos": 174, "end_pos": 196, "type": "TASK", "confidence": 0.7712925136089325}, {"text": "part-of-speech tagging (TAG)", "start_pos": 198, "end_pos": 226, "type": "TASK", "confidence": 0.8244233191013336}]}, {"text": "In the second part, we explore the obtained embeddings spaces in an attempt to better understand the networks have learned about word relations.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the neural models we use for our four tasks and the training and testing datasets.", "labels": [], "entities": []}, {"text": "We use CzEng 1.6 (   For TAG, we use manually annotated English tags from PCEDT 5.", "labels": [], "entities": [{"text": "PCEDT 5", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9270487725734711}]}, {"text": "From this dataset, we drop all sentences containing the tag \"-NONE-\" which is not part of the standard tags.", "labels": [], "entities": [{"text": "NONE-\"", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9205560684204102}]}, {"text": "This leads to the testset of 13060 sentences of 228k running words.", "labels": [], "entities": []}, {"text": "In this section, we experimentally evaluate embedding initialization methods across four different tasks and two architectures: the recurrent and convolutional neural networks.", "labels": [], "entities": []}, {"text": "The experiments are performed on the NVidia GeForce 1080 graphic card.", "labels": [], "entities": [{"text": "NVidia GeForce 1080 graphic card", "start_pos": 37, "end_pos": 69, "type": "DATASET", "confidence": 0.9131168007850647}]}, {"text": "Note that each run of MT takes a week of training, LM takes a day and a half and TAG and LEM need several hours each.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9811774492263794}, {"text": "TAG", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8830862641334534}, {"text": "LEM", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9395152926445007}]}, {"text": "We run the training for one epoch and evaluate the performance regularly throughout the training on the described test set.", "labels": [], "entities": []}, {"text": "For MT and LM, the epoch amounts to 25M sentence pairs and for TAG and LEM to 3M sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8676388263702393}]}, {"text": "The epoch size is set empirically so that the models already reach a stable level of performance and further improvement does not increase the performance too much.", "labels": [], "entities": []}, {"text": "MT and LM exhibit performance fluctuation throughout the training.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6181231737136841}]}, {"text": "Therefore, we average the results over five consecutive evaluation scores that this difference will have no effect on the comparison of embeddings initializations and we prefer to use the same training dataset for all our tasks.", "labels": [], "entities": []}, {"text": "http://www.statmt.org/wmt16/translation-task.html 5 https://ufal.mff.cuni.cz/pcedt2.0/en/index.html spread across 500k training examples to avoid local fluctuations.", "labels": [], "entities": []}, {"text": "This can be seen as a simple smoothing method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task performance with various embedding initializations. Except for LM, higher is better. The  best results for random (upper part) and pretrained (lower part) embedding initializations are in bold.", "labels": [], "entities": []}, {"text": " Table 3: The accuracy in percent on the (seman- tic; morphosyntactic) questions. We do not re- port TAG since its accuracy was less than 1% on  all questions. *For comparison, we present results  of Word2Vec trained on our training set and offi- cial trained embeddings before applying them on  training of particular task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995505213737488}, {"text": "TAG", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.7490869164466858}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9977920055389404}]}, {"text": " Table 4: Spearman's correlation \u03c1 on word simi- larities. The results are multiplied by 100.", "labels": [], "entities": [{"text": "Spearman's correlation \u03c1", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6651177704334259}]}, {"text": " Table 5: The results of the experiment when  learned with non-trainable embeddings.", "labels": [], "entities": []}]}