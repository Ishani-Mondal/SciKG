{"title": [], "abstractContent": [{"text": "While word embeddings have been shown to implicitly encode various forms of attribu-tional knowledge, the extent to which they capture relational information is far more limited.", "labels": [], "entities": []}, {"text": "In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding.", "labels": [], "entities": []}, {"text": "Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of related-ness.", "labels": [], "entities": []}, {"text": "As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding.", "labels": [], "entities": []}, {"text": "This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available.", "labels": [], "entities": []}, {"text": "Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings are paramount to the success of current natural language processing (NLP) methods.", "labels": [], "entities": []}, {"text": "Apart from the fact that they provide a convenient mechanism for encoding textual information in neural network models, their importance mainly stems from the remarkable amount of linguistic and semantic information that they capture.", "labels": [], "entities": []}, {"text": "For instance, the vector representation of the word Paris implicitly encodes that this word is a noun, and more specifically a capital city, and that it describes a location in France.", "labels": [], "entities": []}, {"text": "This information arises because word embeddings are learned from co-occurrence counts, and properties such as being a capital city are reflected in such statistics.", "labels": [], "entities": []}, {"text": "However, the extent to which relational knowledge (e.g. Trump was the successor of Obama) can be learned in this way is limited.", "labels": [], "entities": []}, {"text": "Previous work has addressed this by incorporating external knowledge graphs () or relations extracted from text (.", "labels": [], "entities": []}, {"text": "However, the success of such approaches depends on the amount of available relational knowledge.", "labels": [], "entities": []}, {"text": "Moreover, they only consider well-defined discrete relation types (e.g. is the capital of, or is apart of ), whereas the appeal of vector space representations largely comes from their ability to capture subtle aspects of meaning that go beyond what can be expressed symbolically.", "labels": [], "entities": []}, {"text": "For instance, the relationship between popcorn and cinema is intuitively clear, but it is more subtle than the assertion that \"popcorn is located at cinema\", which is how ConceptNet (, for example, encodes this relationship . In fact, regardless of how a word embedding is learned, if its primary aim is to capture similarity, there are inherent limitations on the kinds of relations they can capture.", "labels": [], "entities": []}, {"text": "For instance, such word embeddings can only encode similarity preserving relations (i.e. similar entities have to be related to similar entities) and it is often difficult to encode that w is in a particular relationship while preventing the inference that words with similar vectors tow are also in this relationship; e.g.  found that both (Berlin,Germany) and (Moscow,Germany) were predicted to be instances of the capital-of relation due to the similarity of the word vectors for Berlin and Moscow.", "labels": [], "entities": []}, {"text": "Furthermore, while the ability to capture word analogies (e.g. king-man+woman\u2248queen) emerged as a successful illustration of how word embeddings can encode some types of relational information (, the generalization of this interesting property has proven to be less successful than initially anticipated (.", "labels": [], "entities": []}, {"text": "This suggests that relational information has to be encoded separately from standard similaritycentric word embeddings.", "labels": [], "entities": []}, {"text": "One appealing strategy is to represent relational information by learning, for each pair of related words, a vector that encodes how the words are related.", "labels": [], "entities": []}, {"text": "This strategy was first adopted by, and has recently been revisited by a number of authors ().", "labels": [], "entities": []}, {"text": "However, in many applications, word vectors are easier to deal with than vector representations of word pairs.", "labels": [], "entities": []}, {"text": "The research question we consider in this paper is whether it is possible to learn word vectors that capture relational information.", "labels": [], "entities": []}, {"text": "Our aim is for such relational word vectors to be complementary to standard word vectors.", "labels": [], "entities": []}, {"text": "To make relational information available to NLP models, it then suffices to use a standard architecture and replace normal word vectors by concatenations of standard and relational word vectors.", "labels": [], "entities": []}, {"text": "In particular, we show that such relational word vectors can be learned directly from a given set of relation vectors.", "labels": [], "entities": []}], "datasetContent": [{"text": "In what follows, we detail the resources and training details that we used to obtain the relational word vectors.", "labels": [], "entities": []}, {"text": "We followed the setting of and used the English Wikipedia as input corpus.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.8357856869697571}]}, {"text": "Multiwords (e.g. Manchester United) were grouped together as a single token by following the same approach described in.", "labels": [], "entities": []}, {"text": "As word embeddings, we used 300-dimensional FastText vectors () trained on Wikipedia with standard hyperparameters.", "labels": [], "entities": []}, {"text": "These embeddings are used as input to construct the relation vectors r wv (see Section 3.1), which are in turn used to learn relational word embeddings e w (see Section 3.2).", "labels": [], "entities": []}, {"text": "The FastText vectors are additionally used as our baseline word embedding model.", "labels": [], "entities": []}, {"text": "As our core vocabulary V, we selected the 100, 000 most frequent words from Wikipedia.", "labels": [], "entities": []}, {"text": "To construct the set of word pairs R, for each word from V, we selected the 100 most closely related words (cf. Section 3.1), considering only consider word pairs that co-occur at least 25 times in the same sentence throughout the Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 231, "end_pos": 247, "type": "DATASET", "confidence": 0.8315877020359039}]}, {"text": "This process yielded relation vectors for 974,250 word pairs.", "labels": [], "entities": []}, {"text": "To learn our relational word embeddings we use the model described in Section 3.2.", "labels": [], "entities": []}, {"text": "The embedding layer is initialized with the standard FastText 300-dimensional vectors trained on Wikipedia.", "labels": [], "entities": []}, {"text": "The method was implemented in PyTorch, employing standard hyperparameters, using ReLU as the non-linear activation function f (Equation 3).", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9081220030784607}, {"text": "ReLU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9247845411300659}]}, {"text": "The hidden layer of the model was fixed to the same dimensionality as the embedding layer (i.e. 600).", "labels": [], "entities": []}, {"text": "The stopping criterion was decided based on a small development set, by setting aside 1% of the relation vectors.", "labels": [], "entities": [{"text": "stopping", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9248140454292297}]}, {"text": "Code to reproduce our experiments, as well as pre-trained models and details of the implementation such as other network hyperparameters, are available at https://github.com/pedrada88/rwe.", "labels": [], "entities": []}, {"text": "A natural way to assess the quality of word vectors is to test them in lexical semantics tasks.", "labels": [], "entities": []}, {"text": "However, it should be noted that relational word vectors behave differently from standard word vectors, and we should not expect the relational word vectors to be meaningful in unsupervised tasks such as semantic relatedness).", "labels": [], "entities": []}, {"text": "In particular, note that a high similarity between e wand e v should mean that relationships which hold for w have a high probability of holding for v as well.", "labels": [], "entities": []}, {"text": "Words which are related, but not syn-onymous, may thus have very dissimilar relational word vectors.", "labels": [], "entities": []}, {"text": "Therefore, we test our proposed models on a number of different supervised tasks for which accurately capturing relational information is crucial to improve performance.", "labels": [], "entities": []}, {"text": "Standard FastText vectors, which were used to construct the relation vectors, are used as our main baseline.", "labels": [], "entities": []}, {"text": "In addition, we also compare with the word embeddings that were learned by the Pair2Vec system 7 (see Section 2).", "labels": [], "entities": [{"text": "Pair2Vec system 7", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.8014318744341532}]}, {"text": "All comparison systems are 300-dimensional and trained on the same Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.8515178263187408}]}], "tableCaptions": [{"text": " Table 1: Accuracy and macro-averaged F-Measure, precision and recall on BLESS and DiffVec. Models marked  with  \u2020 use external resources. The results with * indicate that WordNet was used for both the development of the  model and the construction of the dataset. All models concatenate their encoded representations with the baseline  vector difference of standard FastText word embeddings.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9975507855415344}, {"text": "F-Measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9281163811683655}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9995334148406982}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9995105266571045}, {"text": "BLESS", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.8728007078170776}, {"text": "WordNet", "start_pos": 172, "end_pos": 179, "type": "DATASET", "confidence": 0.9491013884544373}]}, {"text": " Table 2: Results on the McRae feature norms dataset (Macro F-Score) and QVEC (correlation score). Models  marked with  \u2020 use external resources. The results with * indicate that WordNet was used for both the development  of the model and the construction of the dataset.", "labels": [], "entities": [{"text": "McRae feature norms dataset", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.8176883608102798}, {"text": "F-Score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.573027491569519}, {"text": "QVEC (correlation score)", "start_pos": 73, "end_pos": 97, "type": "METRIC", "confidence": 0.8497363328933716}, {"text": "WordNet", "start_pos": 179, "end_pos": 186, "type": "DATASET", "confidence": 0.95160973072052}]}, {"text": " Table 4: Three nearest neighbours for selected word pairs using our relational word vector's relation encoding  (RWE) and the standard vector difference encoding of FastText word embeddings. In each column only the word  pairs which were on the top 50 NNs of the given model but not in the other are listed. Relations which include one  word from the original pair were not considered.", "labels": [], "entities": []}, {"text": " Table 5: Pearson (r) and Spearman (\u03c1) correlation on  a subset of the HyperLex lexical split. Models marked  with  \u2020 use external resources. All models concatenate  their encoded representations with the baseline vector  difference of standard FastText word embeddings.", "labels": [], "entities": [{"text": "Pearson (r)", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9598623365163803}, {"text": "Spearman (\u03c1) correlation", "start_pos": 26, "end_pos": 50, "type": "METRIC", "confidence": 0.9332914233207703}, {"text": "HyperLex lexical split", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.9154939651489258}]}]}