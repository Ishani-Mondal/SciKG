{"title": [{"text": "Improved Language Modeling by Decoding the Past", "labels": [], "entities": [{"text": "Improved Language Modeling", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9132546583811442}]}], "abstractContent": [{"text": "Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling.", "labels": [], "entities": []}, {"text": "We propose anew regu-larization method based on decoding the last token in the context using the predicted distribution of the next token.", "labels": [], "entities": []}, {"text": "This biases the model towards retaining more contextual information , in turn improving its ability to predict the next token.", "labels": [], "entities": []}, {"text": "With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method improves perplexity on the Penn Treebank dataset by up to 1.8 points and by up to 2.3 points on the WikiText-2 dataset, over strong regularized baselines using a single softmax.", "labels": [], "entities": [{"text": "Past Decode Regularization", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.5776773591836294}, {"text": "Penn Treebank dataset", "start_pos": 143, "end_pos": 164, "type": "DATASET", "confidence": 0.996084988117218}, {"text": "WikiText-2 dataset", "start_pos": 216, "end_pos": 234, "type": "DATASET", "confidence": 0.9435991644859314}]}, {"text": "With a mixture-of-softmax model, we show gains of up to 1.0 perplexity points on these datasets.", "labels": [], "entities": []}, {"text": "In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.", "labels": [], "entities": [{"text": "Penn Treebank Character dataset", "start_pos": 65, "end_pos": 96, "type": "DATASET", "confidence": 0.9876200258731842}, {"text": "character level language modeling", "start_pos": 101, "end_pos": 134, "type": "TASK", "confidence": 0.6062216386198997}]}, {"text": "Each of these results constitute improvements over models without PDR in their respective settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modeling is a fundamental task in natural language processing.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7433139979839325}, {"text": "natural language processing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.650903046131134}]}, {"text": "Given a sequence of tokens, its joint probability distribution can be modeled using the auto-regressive conditional factorization.", "labels": [], "entities": []}, {"text": "This leads to a convenient formulation where a language model has to predict the next token given a sequence of tokens as context.", "labels": [], "entities": []}, {"text": "Recurrent neural networks are an effective way to compute distributed representations of the context by sequentially operating on the embeddings of the tokens.", "labels": [], "entities": []}, {"text": "These representations can then be used to predict the next token as a probability distribution over a fixed vocabulary using a linear decoder followed by Softmax.", "labels": [], "entities": [{"text": "Softmax", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9337660670280457}]}, {"text": "Starting from the work of (, there has been along list of works that seek to improve language modeling performance using more sophisticated recurrent neural networks (RNNs) (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7347101271152496}]}, {"text": "However, in more recent work vanilla LSTMs) with relatively large number of parameters have been shown to achieve state-of-the-art performance on several standard benchmark datasets both in wordlevel and character-level perplexity (.", "labels": [], "entities": []}, {"text": "A key component in these models is the use of several forms of regularization e.g. variational dropout on the token embeddings (, dropout on the hidden-tohidden weights in the LSTM (, norm regularization on the outputs of the LSTM and classical dropout ().", "labels": [], "entities": []}, {"text": "By carefully tuning the hyperparameters associated with these regularizers combined with optimization algorithms like NT-ASGD (a variant of Averaged SGD), it is possible to achieve very good performance.", "labels": [], "entities": []}, {"text": "Each of these regularizations address different parts of the LSTM model and are general techniques that could be applied to any other sequence modeling problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose a regularization technique that exploits asymmetry in language models.", "labels": [], "entities": []}, {"text": "A unique aspect of language modeling using LSTMs (or any RNN) is that at each time step t, the model takes as input a particular token x t from a vocabulary W and using the hidden state of the LSTM (which encodes the context till x t\u22121 ) predicts a probability distribution w t+1 on the next token x t+1 over the same vocabulary as output.", "labels": [], "entities": []}, {"text": "Since x t can be mapped to a trivial probability distribution over W , this operation can be interpreted as transforming distributions over W (.", "labels": [], "entities": []}, {"text": "Clearly, the output distribution is dependent on and is a function of x t and the context further in the past and encodes information about it.", "labels": [], "entities": []}, {"text": "We ask the following question -How much information is it possible to decode about the input distribution (and hence x t ) from the output distribution w t+1 ? In general, it is impossible to decode x t unambiguously.", "labels": [], "entities": []}, {"text": "Even if the language model is perfect and correctly predicts x t+1 with probability 1, there could be many tokens preceding it.", "labels": [], "entities": []}, {"text": "However, in this case the number of possibilities for x twill be limited, as dictated by the bigram statistics of the corpus and the language in general.", "labels": [], "entities": []}, {"text": "We argue that biasing the language model such that it is possible to decode more information about the past tokens from the predicted next token distribution is beneficial.", "labels": [], "entities": []}, {"text": "We incorporate this intuition into a regularization term in the loss function of the language model.", "labels": [], "entities": []}, {"text": "The symmetry in the inputs and outputs of the language model at each step lends itself to a simple decoding operation.", "labels": [], "entities": []}, {"text": "It can be cast as a (pseudo) language modeling problem in \"reverse\", where the future prediction w t+1 acts as the input and the last token x t acts as the target of prediction.", "labels": [], "entities": []}, {"text": "The token embedding matrix and weights of the linear decoder of the main language model can be reused in the past decoding operation.", "labels": [], "entities": []}, {"text": "We only need a few extra parameters to model the nonlinear transformation performed by the LSTM, which we do by using a simple stateless layer.", "labels": [], "entities": []}, {"text": "We compute the cross-entropy loss between the decoded distribution for the past token and x t and add it to the main loss function after suitable weighting.", "labels": [], "entities": []}, {"text": "The extra parameters used in the past decoding are discarded during inference time.", "labels": [], "entities": []}, {"text": "We call our method Past Decode Regularization or PDR for short.", "labels": [], "entities": [{"text": "Past Decode Regularization", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.7248552739620209}]}, {"text": "We conduct extensive experiments on four benchmark datasets for word level and character level language modeling by combining PDR with existing LSTM based language models and achieve improved performance on three of them.", "labels": [], "entities": [{"text": "character level language modeling", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.5946629047393799}]}], "datasetContent": [{"text": "We present extensive experimental results to show the efficacy of using PDR for language modeling on four standard benchmark datasets -two each for word level and character level language modeling.", "labels": [], "entities": [{"text": "character level language modeling", "start_pos": 163, "end_pos": 196, "type": "TASK", "confidence": 0.5890822038054466}]}, {"text": "For the former, we evaluate our method on the Penn Treebank (PTB) ( and the WikiText-2 (WT2) ( datasets.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.9704287052154541}, {"text": "WikiText-2 (WT2) ( datasets", "start_pos": 76, "end_pos": 103, "type": "DATASET", "confidence": 0.7354046553373337}]}, {"text": "For the latter, we use the Penn Treebank Character (PTBC) () and the Hutter Prize Wikipedia Prize (also known as Enwik8) datasets.", "labels": [], "entities": [{"text": "Penn Treebank Character (PTBC)", "start_pos": 27, "end_pos": 57, "type": "DATASET", "confidence": 0.969362735748291}, {"text": "Hutter Prize Wikipedia Prize (also known as Enwik8) datasets", "start_pos": 69, "end_pos": 129, "type": "DATASET", "confidence": 0.7786676829511469}]}, {"text": "Key statistics for these datasets is presented in.", "labels": [], "entities": []}, {"text": "As mentioned in the introduction, some of the best existing results on these datasets are obtained by using extensive regularization techniques on relatively large LSTMs (.", "labels": [], "entities": []}, {"text": "We apply our regularization technique to these models, the so called AWD-LSTM.", "labels": [], "entities": [{"text": "AWD-LSTM", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.7577794194221497}]}, {"text": "We consider two versions of the modelone with a single softmax (AWD-LSTM) and one with a mixture-of-softmaxes (AWD-LSTM-MoS).", "labels": [], "entities": [{"text": "AWD-LSTM", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.4824797511100769}]}, {"text": "The PDR regularization term is computed according to Eq.(4) and Eq.(5).", "labels": [], "entities": [{"text": "PDR regularization", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.5957982391119003}]}, {"text": "We call our model AWD-LSTM+PDR when using a single softmax and AWD-LSTM-MoS+PDR when using a mixtureof-softmaxes.", "labels": [], "entities": []}, {"text": "We largely follow the experimental procedure of the original models and incorporate their dropouts and regularizations in our experiments.", "labels": [], "entities": []}, {"text": "For completeness, we briefly mention the set of dropouts and regularizations reused from AWD-LSTM in our experiments.", "labels": [], "entities": [{"text": "AWD-LSTM", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.7888965010643005}]}, {"text": "1. Embedding dropout -Variational or locked dropout applied to the token embedding matrix.", "labels": [], "entities": []}, {"text": "2. Word dropout -Dropout applied to entire tokens.", "labels": [], "entities": []}, {"text": "3. LSTM layer dropout -Dropout between layers of the LSTM.", "labels": [], "entities": []}, {"text": "4. LSTM weight dropout -Dropout applied to the hidden-to-hidden connections in the LSTM.", "labels": [], "entities": []}, {"text": "5. LSTM output dropout -Dropout applied to the final output of the LSTM.", "labels": [], "entities": []}, {"text": "6. Alpha/beta regularization -Activation and temporal activation regularization applied to the LSTM states.", "labels": [], "entities": []}, {"text": "7. Weight decay -L2 regularization on the parameters of the model.", "labels": [], "entities": []}, {"text": "Note that these regularizations are applied to the input, hidden state and output of the LSTM and do not exploit the special structure of language modeling, which PDR does.", "labels": [], "entities": []}, {"text": "The relative contribution of these existing regularizations and PDR will be analyzed in Section 6.", "labels": [], "entities": [{"text": "PDR", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.8280235528945923}]}, {"text": "There are 7 hyperparameters associated with the regularizations used in AWD-LSTM (and one extra with MoS).", "labels": [], "entities": [{"text": "AWD-LSTM", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.8986489176750183}]}, {"text": "PDR also has an associated weighting coefficient \u03bb P DR . For our experiments, we set \u03bb P DR = 0.001 which was determined by a coarse search on the PTB and WT2 validation sets.", "labels": [], "entities": [{"text": "PDR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7500380277633667}, {"text": "weighting coefficient \u03bb P DR", "start_pos": 27, "end_pos": 55, "type": "METRIC", "confidence": 0.8693491101264954}, {"text": "PTB", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.953303337097168}, {"text": "WT2 validation sets", "start_pos": 156, "end_pos": 175, "type": "DATASET", "confidence": 0.8620587388674418}]}, {"text": "For the remaining ones, we perform light hyperparameter search in the vicinity of those reported for AWD-LSTM in (Merity et al., 2018b,a) and for AWD-LSTM-MoS in ().", "labels": [], "entities": [{"text": "AWD-LSTM", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.7335174679756165}, {"text": "AWD-LSTM-MoS", "start_pos": 146, "end_pos": 158, "type": "DATASET", "confidence": 0.8301317691802979}]}, {"text": "We consider the Gigaword dataset () with a truncated vocabulary of about 100K tokens with the highest frequency and apply PDR to a baseline 2-layer LSTM language model with embedding and hidden dimensions set to 1024.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.9551958441734314}]}, {"text": "We use all the shards from the training set for training and a few shards from the heldout set for validation (heldout-0,10) and test.", "labels": [], "entities": []}, {"text": "We tuned the PDR coefficient coarsely in the vicinity of 0.001.", "labels": [], "entities": [{"text": "PDR coefficient", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9629469513893127}]}, {"text": "While the baseline model achieved a validation (test) perplexity of 44.3 (43.1), on applying PDR, the model achieved a perplexity of 44.0 (42.5).", "labels": [], "entities": []}, {"text": "Thus, PDR is relatively less effective on larger datasets, a fact also observed for other regularization techniques on such datasets (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the language modeling benchmark datasets.", "labels": [], "entities": []}, {"text": " Table 2: Perplexities on Penn Treebank (PTB) for single and mixture-of-softmaxes models. Values in parentheses  show gain over respective models without using PDR. The number of parameters during training is 24.4M. Our  models do not use frequency agnostic word embeddings.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.9599989295005799}]}, {"text": " Table 3: Perplexities on WikiText-2 (WT2) for single and mixture-of-softmaxes models. Values in parentheses  show gain over respective models without using PDR. The number of parameters during training is 33.8M. We do  not use frequency agnostic word embeddings.", "labels": [], "entities": []}, {"text": " Table 4: Bits-per-character on the PTBC test set.", "labels": [], "entities": [{"text": "PTBC test set", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9851790269215902}]}, {"text": " Table 5: Bits-per-character on Enwik8 test set.", "labels": [], "entities": [{"text": "Enwik8 test set", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9907818833986918}]}, {"text": " Table 6: Validation perplexities for AWD-LSTM with- out any regularization and with only PDR.", "labels": [], "entities": [{"text": "Validation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9008693695068359}, {"text": "AWD-LSTM", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.7360618710517883}, {"text": "PDR", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.925523579120636}]}, {"text": " Table 7: Ablation experiments on the PTB and WT2 validation and test sets.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9883020520210266}, {"text": "PTB and WT2 validation and test sets", "start_pos": 38, "end_pos": 74, "type": "DATASET", "confidence": 0.8455067191805158}]}]}