{"title": [{"text": "Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions", "labels": [], "entities": []}], "abstractContent": [{"text": "Several deep learning models have been proposed for solving math word problems (MWPs) automatically.", "labels": [], "entities": [{"text": "solving math word problems (MWPs) automatically", "start_pos": 52, "end_pos": 99, "type": "TASK", "confidence": 0.8018193244934082}]}, {"text": "Although these models have the ability to capture features without manual efforts, their approaches to capturing features are not specifically designed for MWPs.", "labels": [], "entities": [{"text": "MWPs", "start_pos": 156, "end_pos": 160, "type": "TASK", "confidence": 0.9015370011329651}]}, {"text": "To utilize the merits of deep learning models with simultaneous consideration of MWPs' specific features, we propose a group attention mechanism to extract global features, quantity-related features, quantity-pair features and question-related features in MWPs respectively.", "labels": [], "entities": []}, {"text": "The experimental results show that the proposed approach performs significantly better than previous state-of-the-art methods, and boost performance from 66.9% to 69.5% on Math23K with training-test split, from 65.8% to 66.9% on Math23K with 5-fold cross-validation and from 69.2% to 76.1% on MAWPS.", "labels": [], "entities": [{"text": "Math23K", "start_pos": 172, "end_pos": 179, "type": "DATASET", "confidence": 0.9165935516357422}, {"text": "Math23K", "start_pos": 229, "end_pos": 236, "type": "DATASET", "confidence": 0.9377893209457397}, {"text": "MAWPS", "start_pos": 293, "end_pos": 298, "type": "DATASET", "confidence": 0.9764777421951294}]}], "introductionContent": [{"text": "Computer systems, dating back to 1960s, have been developing to automatically solve math word problems (MWPs).", "labels": [], "entities": [{"text": "solve math word problems (MWPs)", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.6746848693915776}]}, {"text": "As illustrated in, when solving this problem, machines are asked to infer \"how many shelves would Tom fill up \" based on the textual problem description.", "labels": [], "entities": []}, {"text": "It requires systems having the ability to map the natural language text into the machine-understandable form, reason in terms of sets of numbers or unknown variables, and then derive the numeric answer.", "labels": [], "entities": []}, {"text": "In recent years, a growing number of deep learning models for MWPs () have drawn inspiration from advances in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7235228419303894}]}, {"text": "* corresponding author Problem: For a birthday party Tom bought 4 regular sodas and 52 diet sodas.", "labels": [], "entities": []}, {"text": "If his fridge would only hold 7 on each shelf, how many shelves would he fill up?", "labels": [], "entities": []}, {"text": "Equation: x = (4.0 + 52.0)/7.0 Solution: 8 The core idea is to leverage the immense capacity of neural networks to strengthen the process of equation generating.", "labels": [], "entities": [{"text": "equation generating", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.8629734814167023}]}, {"text": "Compared to statistical machine learning-based methods ( and semantic parsing-based methods (, these methods do not need hand-crafted features and achieve high performance on large datasets.", "labels": [], "entities": [{"text": "semantic parsing-based", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7682823538780212}]}, {"text": "However, they lack in capturing the specific MWPs features, which are an evidently vital component in solving MWP.", "labels": [], "entities": [{"text": "MWP", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.7042390704154968}]}, {"text": "More related work and feature-related information can be found in . Inspired by recent work on modeling locality using multi-head attention (, we introduce a group attention that contains different attention mechanisms to extract various types of MWPs features.", "labels": [], "entities": []}, {"text": "More explicitly, there are four kinds of attention mechanisms: 1) Global attention to grab global information; 2) Quantity-related attention to model the relations between the current quantity and its neighbor-words; 3) Quantity-pair attention to acquire the relations between quantities; 4) Question-related attention to capture the connections between the question and quantities.", "labels": [], "entities": []}, {"text": "The experimental results show that the proposed model establishes the state-of-the-art performance on both Math23K and MAWPS datasets.", "labels": [], "entities": [{"text": "Math23K", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9562022089958191}, {"text": "MAWPS datasets", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.9429729580879211}]}, {"text": "In addtion, we release the source code of our model in Github 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed model on these datasets, Math23K ( and MAWPS (.", "labels": [], "entities": [{"text": "Math23K", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9501267075538635}, {"text": "MAWPS", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8247522711753845}]}, {"text": "Datasets: Math23K is collected from multiple online educational websites.", "labels": [], "entities": []}, {"text": "This dataset contains 23,162 Chinese elementary school level MWPs.", "labels": [], "entities": [{"text": "Chinese elementary school level MWPs", "start_pos": 29, "end_pos": 65, "type": "DATASET", "confidence": 0.662259429693222}]}, {"text": "MAWPS is another large scale dataset which owns 2,373 arithmetic word problems after harvesting ones with a single unknown variable.", "labels": [], "entities": [{"text": "MAWPS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9334294199943542}]}, {"text": "Evaluation Metrics: We use answer accuracy to evaluate our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8178221583366394}]}, {"text": "The accuracy calculation follows a simple formula.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995094537734985}]}, {"text": "If a generated equation produces an answer equal to the corresponding ground truth answer, we consider it to be right.", "labels": [], "entities": []}, {"text": "Implementation details: For Math23K, we follow the training and test set released by (, and we also evaluate our proposed method with 5-fold cross-validation in main results table.", "labels": [], "entities": []}, {"text": "We adopt the pre-trained word embeddings with dimension set to 128 and use a twolayer Bi-LSTM with 256 hidden units and a group attention with four different functional 2-head attention as the encoder, and a two-layer LSTM with 512 hidden units as the decoder.", "labels": [], "entities": []}, {"text": "Dropout probabilities for word embeddings, LSTM and group attention are all set to 0.3.", "labels": [], "entities": []}, {"text": "The number of epochs and mini-batch size are set to 300 and 128 respectively.", "labels": [], "entities": []}, {"text": "As to the optimizer, we use the Adam optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and e = 10 \u22129 . Refer to (, we use the same policy to vary the learning rate with warmup steps=2000.", "labels": [], "entities": []}, {"text": "For MAWPS, we use 5-fold cross-validation, and the parameter setting is similar to those on Math23K.", "labels": [], "entities": [{"text": "MAWPS", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8028033971786499}, {"text": "Math23K", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9615676999092102}]}, {"text": "Baselines: We compare our approach with retrieval models, deep learning based solvers.", "labels": [], "entities": []}, {"text": "The retrieval models) find the most similar math word problem in training set under a distance metric and use its equation template to compute the result.", "labels": [], "entities": []}, {"text": "DNS () first applies a vanilla SEQ2SEQ model with GRU as encoder and LSTM as the decoder to solve MWPs.", "labels": [], "entities": []}, {"text": "In (), the authors apply Bi-LSTM with equation normalization to reinforce the vanilla SEQ2SEQ model.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9697265625}]}, {"text": "T-RNN ( ) launches a two-stage system named as T-RNN that first predicts a tree-structure template to be filled, and then accomplishes the template with operators predicted by the recursive neural network.", "labels": [], "entities": []}, {"text": "In S-Aligned (, the encoder is designed to understand the semantics of problems, and the decoder focuses on deciding which symbol to generate next over semantic meanings of the generated symbols.", "labels": [], "entities": []}, {"text": "As illustrated in, we can see that retrieval approaches work poorly on both two datasets.", "labels": [], "entities": []}, {"text": "Our method named as GROUP-ATT performs substantially better than existing deep learning based methods, increasing the accuracy from 66.9% to 69.5% on Math23K based on trainingtest split, from 65.8% to 66.9% on Math23K with 5-fold cross-validation and from 69.2% to 76.1% on MAWPS.", "labels": [], "entities": [{"text": "GROUP-ATT", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8239082098007202}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9996719360351562}, {"text": "Math23K", "start_pos": 210, "end_pos": 217, "type": "DATASET", "confidence": 0.9338396787643433}, {"text": "MAWPS", "start_pos": 274, "end_pos": 279, "type": "DATASET", "confidence": 0.9748308062553406}]}, {"text": "In addition, DNS and T-RNN also boost the performance by integrating with retrieval methods, while () improves the performance by combining different SEQ2SEQ models.", "labels": [], "entities": []}, {"text": "However, we only focus on improving the performance of single model.", "labels": [], "entities": []}, {"text": "It is worth noting that GROUP-ATT also achieves higher accuracy than the state-of-the-art ensemble models (   In addition, we perform an ablation study to empirically examine the ability of designed group attentions.", "labels": [], "entities": [{"text": "GROUP-ATT", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.8106666207313538}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9992567896842957}]}, {"text": "We adopt the same parameter settings as GROUP-ATT while applying a single kind of attention with 8 heads.", "labels": [], "entities": [{"text": "GROUP-ATT", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.5804951786994934}]}, {"text": "shows the results of ablation study on Math23K.", "labels": [], "entities": [{"text": "Math23K", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9797345995903015}]}, {"text": "Although each specified attention tries to catch related information alone, it still outperforms Bi-LSTM by a margin from 1.0% to 1.5%, showing its effectiveness.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.6962189078330994}]}, {"text": "Ina parking lot, there are ! \" cars and motorcycles in total, each car has ! # wheels, and each motorcycle has n & wheels.", "labels": [], "entities": []}, {"text": "These cars have ! ' wheels in total, so how many motorcycles are therein the parking lot?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Model comparison. Notice that Math23K  means the open training-test split and Math23K*  means 5-fold cross-validation.", "labels": [], "entities": [{"text": "Math23K", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.8577529191970825}]}, {"text": " Table 3: The ablation study to quantify the role of each  type of attention in group attention.", "labels": [], "entities": []}]}