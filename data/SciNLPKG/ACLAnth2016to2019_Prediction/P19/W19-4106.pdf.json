{"title": [{"text": "Conversational Response Re-ranking Based on Event Causality and Role Factored Tensor Event Embedding", "labels": [], "entities": [{"text": "Conversational Response Re-ranking", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7790560921033224}]}], "abstractContent": [{"text": "We propose a novel method for selecting coherent and diverse responses fora given dialogue context.", "labels": [], "entities": []}, {"text": "The proposed method re-ranks response candidates generated from conversational models by using event causality relations between events in a dialogue history and response candidates (e.g., \"be stressed out\" precedes \"relieve stress\").", "labels": [], "entities": []}, {"text": "We use distributed event representation based on the Role Factored Tensor Model fora robust matching of event causality relations due to limited event causality knowledge of the system.", "labels": [], "entities": []}, {"text": "Experimental results showed that the proposed method improved coherency and dialogue continuity of system responses.", "labels": [], "entities": []}], "introductionContent": [{"text": "While a variety of dialogue models such as the neural conversational model (NCM)) have been researched widely, such dialogue models often generate simple and dull responses due to the limitation of their ability to take dialogue context into account.", "labels": [], "entities": []}, {"text": "It is very difficult for these models to generate coherent responses to a dialogue history.", "labels": [], "entities": []}, {"text": "We tackle this problem with anew architecture by incorporating event causality relations between response candidates and a dialogue history.", "labels": [], "entities": []}, {"text": "Typical event causality relations are cause-effect relations between two events, such as \"be stressed out\" precedes \"relieve stress.\"", "labels": [], "entities": []}, {"text": "In this paper, event causality relations are defined that an effect event is likely to happen after a corresponding cause event happens ().", "labels": [], "entities": []}, {"text": "Event causality relations have been used in why-question answering systems to focus on causalities between questions and answers (.", "labels": [], "entities": [{"text": "why-question answering", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.8127102851867676}]}, {"text": "It is also reported that a conversational model using event causality relations can generate diverse and coherent responses).", "labels": [], "entities": []}, {"text": "However, the relation between dialogue continuity and the coherency of system responses is still an underlying problem.", "labels": [], "entities": [{"text": "dialogue continuity", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7988860309123993}]}, {"text": "In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs.", "labels": [], "entities": []}, {"text": "We define a score for re-ranking to select a response that has an event causality relation to a dialogue history.", "labels": [], "entities": []}, {"text": "Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (.", "labels": [], "entities": [{"text": "language generation tasks", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.7846338252226511}, {"text": "why-question answering", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8402727246284485}]}, {"text": "We used event causality pairs extracted from a large-scale corpus ().", "labels": [], "entities": []}, {"text": "We also use distributed event representation based on the Role Factored Tensor Model (RFTM) ( to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs.", "labels": [], "entities": []}, {"text": "In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted automatic and human evaluations to compare responses with and without the reranking.", "labels": [], "entities": []}, {"text": "We evaluated our proposed re-ranking method on a conventional Encoder-Decoder with Attention (EncDec) model  It is difficult to evaluate system performances only with automatic metrics ().", "labels": [], "entities": []}, {"text": "Hence, we compared a baseline model and our models in a human evaluation to confirm coherency and dialogue continuity of responses selected by our proposed methods.", "labels": [], "entities": []}, {"text": "We compared baseline HRED model with our proposed models, re-ranked without embedding and with embedding using the last   five histories.", "labels": [], "entities": []}, {"text": "To reduce evaluators' workload, we used test data whose the number of user utterances is less than three, and removed dialogues which need external knowledge to evaluate.", "labels": [], "entities": []}, {"text": "We used crowdsourcing for the human evaluation.", "labels": [], "entities": []}, {"text": "Ten crowd-workers compared responses selected by two of three models in the following two subjective criteria.", "labels": [], "entities": []}, {"text": "The first one is \"which words in a response are more related to a dialogue history\" (word coherency), which indicates system response coherency to dialogue histories.", "labels": [], "entities": []}, {"text": "The second criterion is \"which response is easier to respond to\" (dialogue continuity), which indicates how much dialogue continuity system responses have.", "labels": [], "entities": [{"text": "dialogue continuity)", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.5992728670438131}]}, {"text": "We were inspired to make these criteria by those of the Alexa Prize (.", "labels": [], "entities": [{"text": "Alexa Prize", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9760287702083588}]}, {"text": "The results are shown in, 5, and 6.", "labels": [], "entities": []}, {"text": "Word coherency was improved by our model without embedding, but lowered by the model with embedding.", "labels": [], "entities": []}, {"text": "This is because workers acknowledged causality relations included in the event causality pair pool, but did not acknowledge generalized causalities with event embedding.", "labels": [], "entities": []}, {"text": "However, dialogue continuity was improved by the proposed re-ranking model with embedding, it is probably because the proposed model reduced the number of dull responses.", "labels": [], "entities": [{"text": "dialogue continuity", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7706399857997894}]}, {"text": "We need to investigate the better threshold in the event embedding to balance out the coherency and the continuity as the future work.", "labels": [], "entities": [{"text": "continuity", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.979184627532959}]}], "tableCaptions": [{"text": " Table 4: 1-best v.s. Re-ranking; # dialogues is 100.", "labels": [], "entities": [{"text": "Re-ranking", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9851845502853394}]}, {"text": " Table 5: 1-best v.s. Re-ranking (emb); # dialogues is  100.", "labels": [], "entities": [{"text": "Re-ranking (emb)", "start_pos": 22, "end_pos": 38, "type": "METRIC", "confidence": 0.8526127636432648}]}, {"text": " Table 6: Re-ranking v.s. Re-ranking (emb); # dialogues  is 100.", "labels": [], "entities": []}]}