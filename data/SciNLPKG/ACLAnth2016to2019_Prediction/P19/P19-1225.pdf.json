{"title": [{"text": "Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction", "labels": [], "entities": [{"text": "Answering while Summarizing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8881934285163879}]}], "abstractContent": [{"text": "Question answering (QA) using textual sources for purposes such as reading comprehension (RC) has attracted much attention.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.943038821220398}, {"text": "reading comprehension (RC)", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.6814757585525513}]}, {"text": "This study focuses on the task of explainable multi-hop QA, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts.", "labels": [], "entities": [{"text": "explainable multi-hop QA", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.49233072996139526}]}, {"text": "It proposes the Query Focused Extractor (QFE) model for evidence extraction and uses multi-task learning with the QA model.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.9054920971393585}]}, {"text": "QFE is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence.", "labels": [], "entities": [{"text": "QFE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.842867374420166}]}, {"text": "It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence.", "labels": [], "entities": []}, {"text": "Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.657000795006752}, {"text": "HotpotQA", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.9921155571937561}]}, {"text": "Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database.", "labels": [], "entities": [{"text": "RC", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9544317126274109}, {"text": "evidence extraction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6877520680427551}, {"text": "FEVER", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.994403064250946}]}], "introductionContent": [{"text": "Reading comprehension (RC) is a task that uses textual sources to answer any question.", "labels": [], "entities": [{"text": "Reading comprehension (RC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8238126158714294}]}, {"text": "It has seen significant progress since the publication of numerous datasets such as SQuAD (.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.7773388028144836}]}, {"text": "To achieve the goal of RC, systems must be able to reason over disjoint pieces of information in the reference texts.", "labels": [], "entities": [{"text": "RC", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9793986082077026}]}, {"text": "Recently, multi-hop question answering (QA) datasets focusing on this capability, such as QAngaroo (  and HotpotQA (, have been released.", "labels": [], "entities": [{"text": "multi-hop question answering (QA)", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.78355340162913}]}, {"text": "Multi-hop QA faces two challenges.", "labels": [], "entities": [{"text": "Multi-hop QA", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.706046998500824}]}, {"text": "The first is the difficulty of reasoning.", "labels": [], "entities": []}, {"text": "It is difficult for the Given a question and multiple textual sources, the system extracts evidence sentences from the sources and returns the answer and the evidence.", "labels": [], "entities": []}, {"text": "system to find the disjoint pieces of information as evidence and reason using the multiple pieces of such evidence.", "labels": [], "entities": []}, {"text": "The second challenge is interpretability.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.9740821123123169}]}, {"text": "The evidence used to reason is not necessarily located close to the answer, so it is difficult for users to verify the answer.", "labels": [], "entities": []}, {"text": "released HotpotQA, an explainable multi-hop QA dataset, as shown in Figure 1.", "labels": [], "entities": [{"text": "HotpotQA", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.9247934222221375}, {"text": "QA dataset", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.6660215854644775}]}, {"text": "Hotpot QA provides the evidence sentences of the answer for supervised learning.", "labels": [], "entities": [{"text": "Hotpot QA", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9415676891803741}]}, {"text": "The evidence extraction in multi-hop QA is more difficult than that in other QA problems because the question itself may not provide a clue for finding evidence sentences.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8234466910362244}]}, {"text": "As shown in, the system finds an evidence sentence (Evidence 2) by relying on another evidence sentence (Evidence 1).", "labels": [], "entities": []}, {"text": "The capability of being able to explicitly extract evidence is an advance towards meeting the above two challenges.", "labels": [], "entities": []}, {"text": "Here, we propose a Query Focused Extractor (QFE) that is based on a summarization model.", "labels": [], "entities": []}, {"text": "We regard the evidence extraction of the explainable multi-hop QA as a query-focused summarization task.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7139688283205032}]}, {"text": "Query-focused summarization is the task of summarizing the source document with regard to the given query.", "labels": [], "entities": [{"text": "Query-focused summarization", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6412196159362793}]}, {"text": "QFE sequentially extracts the evidence sentences by using an RNN with an attention mechanism on the question sentence, while the existing method extracts each evidence sentence independently.", "labels": [], "entities": []}, {"text": "This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence.", "labels": [], "entities": []}, {"text": "Our overall model uses multi-task learning with a QA model for answer selection and QFE for evidence extraction.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.9198267161846161}, {"text": "evidence extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8308328986167908}]}, {"text": "The multi-task learning with QFE is general in the sense that it can be combined with any QA model.", "labels": [], "entities": []}, {"text": "Moreover, we find that the recognizing textual entailment (RTE) task on a large textual database, FEVER, can be regarded as an explainable multi-hop QA task.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE) task", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.7709472860608783}, {"text": "FEVER", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.9746383428573608}]}, {"text": "We confirm that QFE effectively extracts the evidence both on HotpotQA for RC and on FEVER for RTE.", "labels": [], "entities": [{"text": "QFE", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.5993514060974121}, {"text": "HotpotQA", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9867189526557922}, {"text": "FEVER", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.996246874332428}, {"text": "RTE", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.6655025482177734}]}, {"text": "Our main contributions are as follows.", "labels": [], "entities": []}, {"text": "\u2022 We propose QFE for explainable multi-hop QA.", "labels": [], "entities": []}, {"text": "We use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.952784925699234}, {"text": "evidence extraction", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8276194036006927}]}, {"text": "\u2022 QFE adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question.", "labels": [], "entities": []}, {"text": "\u2022 QFE achieves state-of-the-art performance on both HotpotQA and FEVER in terms of the evidence extraction score and comparable performance to competitive models in terms of the answer selection score.", "labels": [], "entities": [{"text": "HotpotQA", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9731706976890564}, {"text": "FEVER", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9818387031555176}]}, {"text": "QFE is the first model that outperformed the baseline on HotpotQA.", "labels": [], "entities": [{"text": "QFE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9368447065353394}, {"text": "HotpotQA", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9856179356575012}]}], "datasetContent": [{"text": "In HotpotQA, the query Q is created by crowd workers, on the condition that answering Q requires reasoning over two paragraphs in Wikipedia.", "labels": [], "entities": []}, {"text": "The candidates of A T are 'Yes', 'No', and 'Span'.", "labels": [], "entities": [{"text": "A T", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9551883041858673}]}, {"text": "The answer string A S , if it exists, is a span in the two paragraphs.", "labels": [], "entities": []}, {"text": "The context C is ten paragraphs, and its content has two settings.", "labels": [], "entities": []}, {"text": "In the distractor setting, C consists of the two gold paragraphs used to create Q and eight paragraphs retrieved from Wikipedia by using TF-IDF with Q. shows the statistics of the distractor setting.", "labels": [], "entities": []}, {"text": "In the fullwiki setting, all ten paragraphs of C are retrieved paragraphs.", "labels": [], "entities": []}, {"text": "Hence, C may not include two gold paragraphs, and in that case, A Sand E cannot be extracted.", "labels": [], "entities": [{"text": "A Sand E", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.8594028751055399}]}, {"text": "Therefore, the oracle model does not achieve 100 % accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9986013770103455}]}, {"text": "HotpotQA does not provide the training data for the fullwiki setting, and the training data in the fullwiki setting is the same as the distractor setting.", "labels": [], "entities": [{"text": "HotpotQA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9596033692359924}]}, {"text": "Because C is large, we used the NSMN document retriever  We evaluated the prediction of A T and the evidence E by using the official metrics in FEVER.", "labels": [], "entities": [{"text": "NSMN document retriever", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7702030340830485}, {"text": "prediction of A T", "start_pos": 74, "end_pos": 91, "type": "METRIC", "confidence": 0.9068485796451569}, {"text": "FEVER", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.929338812828064}]}, {"text": "A T was evaluated in terms of the label accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9255557656288147}]}, {"text": "E was evaluated in terms of precision, recall and F1, which were measured by sentence id.", "labels": [], "entities": [{"text": "E", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9747434258460999}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9997226595878601}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9997559189796448}, {"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9995927214622498}]}, {"text": "The FEVER score was used as a metric accounting for both A T and E.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9833367764949799}, {"text": "A T", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9493562281131744}, {"text": "E", "start_pos": 65, "end_pos": 66, "type": "METRIC", "confidence": 0.5121543407440186}]}, {"text": "The FEVER score of a sample is 1 if the predicted evidence includes all gold evidence and the answer is correct.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9901213943958282}]}, {"text": "That is, the FEVER score emphasizes the recall of extracting evidence sentences over the precision.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9844195246696472}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9992384910583496}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.999085545539856}]}, {"text": "In FEVER, the query Q is created by crowd workers.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.5267854332923889}]}, {"text": "Annotators are given a randomly sampled sen-  tence and a corresponding dictionary.", "labels": [], "entities": []}, {"text": "The given sentence is from Wikipedia.", "labels": [], "entities": []}, {"text": "The key-value of the corresponding dictionary consists of an entity and a description of the entity.", "labels": [], "entities": []}, {"text": "Entities are those that have a hyperlink from the given sentence.", "labels": [], "entities": []}, {"text": "The description is the first sentence of the entity's Wikipedia page.", "labels": [], "entities": []}, {"text": "Only using the information in the sentence and the dictionary, annotators create a claim as Q.", "labels": [], "entities": []}, {"text": "The candidates of A T are 'Supports', 'Refutes' and 'Not Enough Info (NEI)'.", "labels": [], "entities": [{"text": "A T", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.5611673295497894}, {"text": "Refutes'", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9645944237709045}]}, {"text": "The proportion of samples with more than one evidence sentence is 27.3% in the samples whose label is not 'NEI'.", "labels": [], "entities": []}, {"text": "The context C is the Wikipedia database shared among all samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of HotpotQA (the development set  in the distractor setting).", "labels": [], "entities": []}, {"text": " Table 2: Performance of the models on the HotpotQA  distractor setting leaderboard 1 (4 March 2019). The  models except for the baseline were unpublished at the  time of submission of this paper. Our model was sub- mitted on 21 November 2018, three months before the  other submissions.", "labels": [], "entities": [{"text": "HotpotQA  distractor setting leaderboard 1", "start_pos": 43, "end_pos": 85, "type": "DATASET", "confidence": 0.9134223222732544}]}, {"text": " Table 3: Performance of the models on the HotpotQA  fullwiki setting leaderboard 1 (4 March 2019). The  models except for the baseline were unpublished at the  time of submission of this paper. Our model was sub- mitted on 25 November 2018, three months before the  other submissions.", "labels": [], "entities": [{"text": "HotpotQA  fullwiki setting leaderboard 1", "start_pos": 43, "end_pos": 83, "type": "DATASET", "confidence": 0.9245871186256409}]}, {"text": " Table 4: Performance of our models and the baseline  models on the development set in the distractor setting.", "labels": [], "entities": []}, {"text": " Table 5: Performance of our model and the baseline in  evidence extraction on the development set in the dis- tractor setting. The correlation is the Kendall tau cor- relation of the number of predicted evidence sentences  and that of gold evidence.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8219984173774719}, {"text": "Kendall tau cor- relation", "start_pos": 151, "end_pos": 176, "type": "METRIC", "confidence": 0.7704884171485901}]}, {"text": " Table 6: Performance of our model in terms of the num- ber of gold evidence sentences on the development set  in the distractor setting. # sample, Num, P and R mean  the proportion in the dataset, number of predicted evi- dence sentences, precision, and recall, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 240, "end_pos": 249, "type": "METRIC", "confidence": 0.9997732043266296}, {"text": "recall", "start_pos": 255, "end_pos": 261, "type": "METRIC", "confidence": 0.9997450709342957}]}, {"text": " Table 7: Performance of our model for each reasoning  type on the development set in the distractor setting.", "labels": [], "entities": []}, {"text": " Table 8: Outputs of QFE. The sentences are extracted in the order shown in the predicted column. The extraction  scores of the sentences at each step are in the probability column.", "labels": [], "entities": []}, {"text": " Table 9: Statistics of FEVER (the development set).", "labels": [], "entities": [{"text": "FEVER", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9705801606178284}]}, {"text": " Table 10: Performance of the models on the FEVER  leaderboard 3 (4 March 2019). The top two rows are  the models submitted during the FEVER Shared Task  that have higher FEVER scores than ours. The middle  three rows are the top-three FEVER models submitted  after the Shared Task. The rows next to the bottom and  the bottom row (ours) show the top-three F1 models  submitted after the Shared Task. None of the models  submitted after the Shared Task has paper information.", "labels": [], "entities": [{"text": "FEVER  leaderboard 3 (4 March 2019)", "start_pos": 44, "end_pos": 79, "type": "DATASET", "confidence": 0.9370150715112686}, {"text": "FEVER Shared Task", "start_pos": 135, "end_pos": 152, "type": "DATASET", "confidence": 0.7230454683303833}, {"text": "FEVER scores", "start_pos": 171, "end_pos": 183, "type": "METRIC", "confidence": 0.9765556752681732}, {"text": "FEVER", "start_pos": 236, "end_pos": 241, "type": "METRIC", "confidence": 0.75480055809021}]}, {"text": " Table 11: Performance of evidence extraction. The top  five rows are evaluated on the test set. The comparison  of our models is on the development set. The models  submitted after the Shared Task have no information  about precision or recall.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.773021787405014}, {"text": "precision", "start_pos": 225, "end_pos": 234, "type": "METRIC", "confidence": 0.998956561088562}, {"text": "recall", "start_pos": 238, "end_pos": 244, "type": "METRIC", "confidence": 0.9674558043479919}]}]}