{"title": [{"text": "Second-order Co-occurrence Sensitivity of Skip-Gram with Negative Sampling", "labels": [], "entities": []}], "abstractContent": [{"text": "We simulate first-and second-order context overlap and show that Skip-Gram with Negative Sampling is similar to Singular Value Decomposition in capturing second-order co-occurrence information, while Pointwise Mutual Information is agnostic to it.", "labels": [], "entities": []}, {"text": "We support the results with an empirical study finding that the models react differently when provided with additional second-order information.", "labels": [], "entities": []}, {"text": "Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks.", "labels": [], "entities": [{"text": "Skip-Gram with Negative Sampling", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.5523398816585541}]}], "introductionContent": [{"text": "The idea of second-order co-occurrence vectors was introduced by for word sense discrimination and has since then been extended and applied to a variety of tasks (; Schulte im.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7287868062655131}]}, {"text": "The basic idea is to represent a word w not by a vector of the counts of context words it directly co-occurs with, but instead by a count vector of the context words of the context words, i.e., the second-order context words of w.", "labels": [], "entities": []}, {"text": "These second-order vectors are supposed to be less sparse and more robust than firstorder vectors.", "labels": [], "entities": []}, {"text": "Moreover, capturing second-order co-occurrence information can be seen as away of generalization.", "labels": [], "entities": []}, {"text": "To see this, cf. examples (1) and (2) inspired by.", "labels": [], "entities": []}, {"text": "(1) As far as the Soviet Communist Party and the Comintern were concerned . .", "labels": [], "entities": []}, {"text": "this is precisely the approach taken by the British Government.", "labels": [], "entities": []}, {"text": "The nouns Party and Government have similar meanings in these contexts, although they have little contextual overlap: A frequent topic in the British corpus used by Sch\u00fctze and Pedersen is the Communist Party of the Soviet Union, but governments are rarely qualified as communist in the corpus.", "labels": [], "entities": [{"text": "British corpus", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.9165886640548706}]}, {"text": "Hence, there is little overlap in first-order context words of Party and Government.", "labels": [], "entities": []}, {"text": "However, their context words Communist and British in turn have a greater overlap, because they are frequently used to qualify the same nouns from the political domain, as in Communist authorities and British authorities.", "labels": [], "entities": []}, {"text": "Hence, although Party and Government may have no first-order context overlap, they do have second-order context overlap.", "labels": [], "entities": []}, {"text": "According to this information corresponds to the generalization \"occurring with apolitical adjective.\"", "labels": [], "entities": []}, {"text": "While most traditional count-based vector learning techniques such as raw count vectors or Point-wise Mutual Information (PPMI) do not capture second-order co-occurrence information, truncated Singular Value Decomposition (SVD) has been shown to do so.", "labels": [], "entities": []}, {"text": "Regarding the more recently developed embeddings based on shallow neural networks, such as Skip-Gram with Negative Sampling (SGNS), it is presently unknown whether they capture higher-order co-occurrence information.", "labels": [], "entities": []}, {"text": "So far, this question has been neglected as a research topic, although the answer is crucial to explain performance differences: show that SGNS performs similarly to SVD and differently from PPMI across semantic similarity data sets.", "labels": [], "entities": []}, {"text": "If SGNS captures secondorder co-occurrence information, this provides a possible explanation for the observed performance differences.", "labels": [], "entities": []}, {"text": "We examine this question in two experiments: (i) We create an artificial data set with target words displaying context overlap in different orders of co-occurrence and show that SGNS behaves similarly to SVD in capturing second-order co-occurrence information.", "labels": [], "entities": []}, {"text": "The experiment supplies additional and striking evidence to prior work on SVD and introduces a method to further investigate questions more precisely than done before.", "labels": [], "entities": [{"text": "SVD", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9282345771789551}]}, {"text": "(ii) We transfer second-order context information to the first-order level in a small corpus and test the models' reaction on a standard evaluation data set when provided with the additional information.", "labels": [], "entities": []}, {"text": "We find that SGNS and SVD, already capturing second-order information, do not benefit, whereas PPMI benefits.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to see whether SGNS captures secondorder co-occurrence information, we artificially simulate context overlap for first-and secondorder co-occurrence separately.", "labels": [], "entities": []}, {"text": "This allows us to simulate clear cases of overlap controlling for confounding factors which are present in empirical data.", "labels": [], "entities": []}, {"text": "We generate target-context pairs in such away that specific target words have either context word overlap in first-order co-occurrence, or by contrast in second order.", "labels": [], "entities": []}, {"text": "We compare the behavior of PPMI, SVD and SGNS on three groups of such target words (see): first-order overlap (1ST): Target words T occurring with the same context words C1 in the first order, while in the second order all context words from C1 have distinct context words C2.", "labels": [], "entities": []}, {"text": "2nd-order overlap (2ND): Target words T occurring with distinct context words C1 in the first order, while all context words from C1 have the same context words C2.", "labels": [], "entities": []}, {"text": "no overlap (NONE): Target words T occurring with distinct context words C1 in the first order and also all context words in C1 have distinct context words C2.", "labels": [], "entities": [{"text": "overlap", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.954130232334137}, {"text": "NONE)", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9311317205429077}]}, {"text": "As an example, consider the column 2ND in Table 1.", "labels": [], "entities": []}, {"text": "The target words T area and b.", "labels": [], "entities": [{"text": "T area", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9479395151138306}]}, {"text": "Each has distinct context words: a occurs only with c, d \u2208 C1, while b occurs only withe, f \u2208 C1.", "labels": [], "entities": []}, {"text": "However, the first-order context words c, d, e, f \u2208 C1 do have context overlap: c, d, e, f occur all with u, v \u2208 C2, i.e., they have the same second-order context words.", "labels": [], "entities": []}, {"text": "For each group we generate 10 target words (T ).", "labels": [], "entities": []}, {"text": "Per target word, each of C1, C2 is constructed by first generating 1000 context words C, assigning a sampling probability from a lognormal distribution to each context word in C and then sampling 1000 times from C.", "labels": [], "entities": []}, {"text": "For the 1ST-group, the set of context words C will be shared across target words, meaning that they have a context word overlap.", "labels": [], "entities": []}, {"text": "For the target words in the 2ND-group this will not be the case, but instead their firstorder context words (C1) will have context overlap (see).", "labels": [], "entities": []}, {"text": "In this way, we simulate context overlap in first vs. second order.", "labels": [], "entities": []}, {"text": "For the target words in the NONE-group, C will instead be completely disjunct in both orders.", "labels": [], "entities": [{"text": "NONE-group", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.8630602359771729}]}, {"text": "Because cooccurrence is symmetric (if a occurs with c, c also occurs with a), for each pair (a,c) generated by the above-described process, we also add the reverse pair (c,a).", "labels": [], "entities": []}, {"text": "To make sure that the pairs from the different groups (1ST, 2ND, NONE) do not interfere with each other, each string generated fora group is unique to the group.", "labels": [], "entities": [{"text": "NONE", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.8833516240119934}]}, {"text": "Finally, we mix and randomly shuffle the pairs from all groups.", "labels": [], "entities": []}, {"text": "In this way, we generate 10 x 1000 x 1000 x 2 targetcontext pairs for each group: 10 target words occurring with 1000 context words in C1 where each in turn occurs with 1000 context words in C2, plus each of these pairs reversed.", "labels": [], "entities": []}, {"text": "Our main hypothesis is that SGNS and SVD will predict target words from the 2ND-group to be more similar on average than target words from the NONE-group (although both groups have no first-order context overlap), while PPMI will predict similar averages for both groups.", "labels": [], "entities": []}, {"text": "shows the average cosine distance between the target words in each of the three target word groups with context overlap in different orders (1ST, 2ND and NONE).", "labels": [], "entities": [{"text": "NONE", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.8943493366241455}]}, {"text": "As expected, PPMI predicts the target words without contextual overlap in any order (NONE) to be orthogonal to each other (1.0).", "labels": [], "entities": [{"text": "NONE)", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.960554301738739}]}, {"text": "Further, PPMI is sensitive to first-order overlap, but not at all to second-order overlap (0.51 vs. 1.0).", "labels": [], "entities": [{"text": "second-order overlap", "start_pos": 69, "end_pos": 89, "type": "METRIC", "confidence": 0.8330012857913971}]}, {"text": "SVD also predicts orthogonality for the NONE-group (1.0) and shows sensitivity to first-order overlap (0.34), but is extremely 2 By sampling from a lognormal distribution we aim to approximate the empirical frequency distribution of context words.", "labels": [], "entities": [{"text": "SVD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7136856913566589}, {"text": "NONE-group", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8522799611091614}]}, {"text": "Context words receive probabilities by randomly sampling 1000 values from f (x) = 1 and normalizing them to a probability distribution.", "labels": [], "entities": []}, {"text": "3 Find the code generating the artificial pairs under: https://github.com/Garrafao/SecondOrder.: Artificial co-occurrence pairs with context overlap in different orders of co-occurrence (1ST, 2ND and NONE).", "labels": [], "entities": [{"text": "NONE", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9357250928878784}]}, {"text": "C1 and C2 give co-occurrence in first and second order respectively.", "labels": [], "entities": []}, {"text": "For each pair (a,c) shown above we also add the reverse pair (c,a).", "labels": [], "entities": []}, {"text": "sensitive to second-order overlap: it predicts the target words in 2ND to be perfectly similar to each other (0.0), notwithstanding the fact that they have no first-order context word overlap.", "labels": [], "entities": []}, {"text": "SGNS shows a similar behavior, although its vectors are distributed more densely: target words in NONE are predicted to be least similar (0.79), while target words in 1ST are more similar (0.11) and in 2ND they are predicted to be completely similar (0.0).", "labels": [], "entities": [{"text": "NONE", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.889711856842041}]}, {"text": "We further hypothesize that the fact that for SGNS and SVD the average cosine distance in 1ST is higher than in 2ND is related to our choice to make the context words C1 of the target words in 1ST dissimilar to each other by assigning completely distinct context words C2 (see).", "labels": [], "entities": []}, {"text": "We test this hypothesis by creating a second artificial set of target-context pairs completely parallel to the above-described set with the only difference that 1ST has additional context overlap in C2.", "labels": [], "entities": []}, {"text": "On these targets words with overlap in both orders we find that PPMI makes similar predictions (0.56) as before, while for SVD and SGNS predictions drop to 0.0, confirming our hypothesis.", "labels": [], "entities": []}, {"text": "SGNS and SVD capture secondorder co-occurrence information.", "labels": [], "entities": []}, {"text": "Notably, they are more sensitive to the similarity of context words than to the words themselves (2ND vs. 1ST), which means that they abstract over mere co-occurrence with specific context words and take into account the co-occurrence structure of these words in the second order (and potentially higher orders).", "labels": [], "entities": []}, {"text": "PPMI does not have this property and only measures context overlap in first order.", "labels": [], "entities": [{"text": "PPMI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6867757439613342}]}, {"text": "We now propagate second-order information to the first-order level by extracting second-order word-context pairs and adding them to the firstorder pairs.", "labels": [], "entities": []}, {"text": "We hypothesize that the additional second-order information will impact PPMI representations positively and stronger than SVD and SGNS, because we saw that the latter already capture second-order information.", "labels": [], "entities": []}, {"text": "We reckon that the additional information is beneficial for PPMI in two ways: (i) it helps to generalize as described in and, and (ii) it overcomes data sparsity for low-frequency words.", "labels": [], "entities": []}, {"text": "Note that these two aspects are often highly related: with only a limited amount of data available it is more likely that similar words do not have exactly the same, but still similar context words.", "labels": [], "entities": []}, {"text": "Generalization then helps to overcome sparsity.", "labels": [], "entities": [{"text": "Generalization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9264218211174011}]}, {"text": "We use ukWaC (), a > 1B token web-crawled corpus of English.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.9471298456192017}]}, {"text": "The corpus provides part-of-speech tagging and lemmatization.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6768491715192795}]}, {"text": "We keep only the lemmas which are tagged as nouns, adjectives or verbs.", "labels": [], "entities": []}, {"text": "In order to assure we have low-frequency words in the test data, we create a small corpus by randomly choosing 1M sentences and shuffling them.", "labels": [], "entities": []}, {"text": "The final corpus contains roughly 10M tokens.", "labels": [], "entities": []}, {"text": "Under these sparse conditions we expect to observe strong effects on model performance highlighting the differences between the models.", "labels": [], "entities": []}], "tableCaptions": []}