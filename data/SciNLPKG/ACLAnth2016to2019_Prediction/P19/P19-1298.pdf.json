{"title": [{"text": "Lattice-Based Transformer Encoder for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7570496002833048}]}], "abstractContent": [{"text": "Neural machine translation (NMT) takes deterministic sequences for source representations.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8029364844163259}]}, {"text": "However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes.", "labels": [], "entities": []}, {"text": "We hypothesize that the diversity in segmentations may affect the NMT performance.", "labels": [], "entities": []}, {"text": "To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training.", "labels": [], "entities": [{"text": "word or subword representation", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.6975335478782654}]}, {"text": "We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention.", "labels": [], "entities": [{"text": "lattice positional encoding", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6811720927556356}]}, {"text": "These two methods can be used together and show complementary to each other to further improve translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.965401828289032}]}, {"text": "Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has achieved great progress with the evolvement of model structures under an encoder-decoder framework.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8477144738038381}]}, {"text": "Recently, the self-attention based Transformer model has achieved state-of-theart performance on multiple language pairs (.", "labels": [], "entities": []}, {"text": "Both representations of source and target sentences in * Corresponding author.", "labels": [], "entities": []}, {"text": "This paper was partially supported by National Key Research and Development Program of China ( and key projects of National Natural Science Foundation of China.", "labels": [], "entities": []}, {"text": "Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): \"Unsupervised Neural Machine Translation in Universal Scenarios\" and NICT tenure-track researcher startup fund \"Toward Intelligent Machine Translation\".", "labels": [], "entities": [{"text": "JSPS grant-in-aid", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.8437007367610931}, {"text": "Unsupervised Neural Machine Translation in Universal Scenarios", "start_pos": 95, "end_pos": 157, "type": "TASK", "confidence": 0.7120296103613717}, {"text": "Intelligent Machine Translation", "start_pos": 213, "end_pos": 244, "type": "TASK", "confidence": 0.6550037165482839}]}, {"text": "NMT can be factorized in character (Costa-Jussa and Fonollosa, 2016), word), or subword () level.", "labels": [], "entities": []}, {"text": "However, only using 1-best segmentation as inputs limits NMT encoders to express source sequences sufficiently and reliably.", "labels": [], "entities": []}, {"text": "Many East Asian languages, including Chinese are written without explicit word boundary, so that their sentences need to be segmented into words firstly (.", "labels": [], "entities": []}, {"text": "By different segmentors, each sentence can be segmented into multiple forms as shown in.", "labels": [], "entities": []}, {"text": "Even for those alphabetical languages with clear word boundary like English, there is still an issue about selecting a proper subword vocabulary size, which determines the segmentation granularities for word representation.", "labels": [], "entities": [{"text": "word representation", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.7058120965957642}]}, {"text": "In order to handle this problem, used hierarchical subword features to represent sequence with different subword granularities.", "labels": [], "entities": []}, {"text": "proposed the first word-lattice based recurrent neural network (RNN) encoders which extended Gated Recurrent Units (GRUs) ( ) to take in multiple sequence segmentation representations.", "labels": [], "entities": []}, {"text": "incorporated posterior scores to Tree-LSTM for building a lattice encoder in speech translation.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7179471403360367}]}, {"text": "All these existing methods serve for RNN-based NMT model, where lattices can be formulized as directed graphs and the inherent directed structure of RNN facilitates the construction of lattice.", "labels": [], "entities": []}, {"text": "Meanwhile, the selfattention mechanism is good at learning the dependency between characters in parallel, which can partially compare and learn information from multiple segmentations.", "labels": [], "entities": []}, {"text": "Therefore, it is challenging to directly apply the lattice structure to Transformer.", "labels": [], "entities": []}, {"text": "In this work, we explore an efficient way of integrating lattice into Transformer.", "labels": [], "entities": []}, {"text": "Our method cannot only process multiple sequences segmented in different ways to improve translation quality, but also maintain the characteristics of parallel computation in the Transformer.", "labels": [], "entities": [{"text": "translation", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.9523094296455383}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Evaluation of translation performance on NIST Zh-En dataset. RNN and Lattice-RNN results are from  (Su et al., 2017). We highlight the highest BLEU score in bold for each set. \u2191 indicates statistically significant  difference (p <0.01) from best baseline.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9608938097953796}, {"text": "NIST Zh-En dataset", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.9223266045252482}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9987801909446716}]}, {"text": " Table 3: Evaluation of translation performance on  IWSLT2016 En-De dataset. RNN results are reported  from Morishita et al. (2018). \u2191 indicates statistically  significant difference (p <0.01) from best baseline.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9685314893722534}, {"text": "IWSLT2016 En-De dataset", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.8980316122372946}]}]}