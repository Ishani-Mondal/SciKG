{"title": [{"text": "What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of Slovenian, Croatian and Serbian", "labels": [], "entities": []}], "abstractContent": [{"text": "We present experiments on Slovenian, Croa-tian and Serbian morphosyntactic annotation and lemmatisation between the former state-of-the-art for these three languages and one of the best performing systems at the CoNLL 2018 shared task, the Stanford NLP neural pipeline.", "labels": [], "entities": [{"text": "Stanford NLP neural pipeline", "start_pos": 240, "end_pos": 268, "type": "DATASET", "confidence": 0.8736365437507629}]}, {"text": "Our experiments show significant improvements in morphosyntactic annotation, especially on categories where either semantic knowledge is needed, available through word embeddings, or where long-range dependencies have to be modelled.", "labels": [], "entities": []}, {"text": "On the other hand, on the task of lemmatisation no improvements are obtained with the neural solution, mostly due to the heavy dependence of the task on the lookup in an external lexicon, but also due to obvious room for improvements in the Stan-ford NLP pipeline's lemmatisation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphosyntactic annotation and lemmatisation are crucial tasks for languages that are rich in inflectional morphology, such as Slavic languages.", "labels": [], "entities": [{"text": "Morphosyntactic annotation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7131187915802002}]}, {"text": "These tasks are far from solved, and the recent CoNLL 2017  and) shared tasks on multilingual parsing from raw text to Universal Dependencies () have given the necessary spotlight to these problems.", "labels": [], "entities": [{"text": "CoNLL 2017", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9440975785255432}, {"text": "multilingual parsing from raw text", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.7946789383888244}]}, {"text": "In addition to the advances due to multi-and cross-lingual settings, the participating systems have also confirmed the predominance of neural network approaches in the field of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 177, "end_pos": 204, "type": "TASK", "confidence": 0.639479527870814}]}, {"text": "In this paper we compare the improvements obtained on these two tasks in three South Slavic languages (Slovenian, Croatian and Serbian) by moving from traditional approaches to the neural ones.", "labels": [], "entities": []}, {"text": "The tool that we use as the representative of the traditional approaches is reldi-tagger, the previous state-of-theart for morphosyntactic tagging and lemmatisation of the three focus languages due to (1) carefully engineered features for the CRF-based tagger, (2) integration of an inflectional lexicon both for the morphosyntactic tagging and the lemmatisation task and (3) lemma guessing for unknown word forms via morphosyntactic-tagspecific Naive Bayes classifiers, predicting the transformation of the surface form.", "labels": [], "entities": [{"text": "lemma guessing for unknown word forms", "start_pos": 376, "end_pos": 413, "type": "TASK", "confidence": 0.7505969405174255}]}, {"text": "The tool that we use as the representative for the neural approaches is stanfordnlp, the Stanford NLP pipeline (, a state-of-the-art in neural morphosyntactic and dependency syntax text annotation.", "labels": [], "entities": []}, {"text": "The system took part in the CoNLL 2018 shared task ( as one of the best-performing systems, which would have, with \"an unfortunate bug fixed\", placed among the top-three for all evaluation metrics, including lemmatisation and morphology prediction.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.7385252714157104}, {"text": "morphology prediction", "start_pos": 226, "end_pos": 247, "type": "TASK", "confidence": 0.8906043767929077}]}, {"text": "The tool is, additionally, released as open source and has a vivid development community, 1 with a named entity recognition module being in development.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform our comparison of the traditional and the neural tool of choice on the two tasks on data splits defined in the babushka-bench benchmarking platform 2 which currently hosts data and results for the three South Slavic languages we use in these experiments, namely Slovenian, Croatian and Serbian.", "labels": [], "entities": []}, {"text": "It is organised as a git repository, with scripts for transferring datasets from the CLARIN.SI repository, and splitting them into training, development, and testing portions.", "labels": [], "entities": [{"text": "CLARIN.SI repository", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.9392209649085999}]}, {"text": "While the primary usage of this platform are in-house experiments on the available and emerging technologies, other researchers are more than welcome to further enrich the repository.", "labels": [], "entities": []}, {"text": "The name of the repository has its roots in the erroneous, but popular naming of the Matryoshka doll in South Slavic languages, as the datasets are split into train, dev and test portions in a random fashion, but with a fixed random seed.", "labels": [], "entities": []}, {"text": "This enables splitting the same datasets on the annotation layers that were not applied over the whole dataset (as is often the case with costly annotations of syntax, semantic etc.), and simultaneously ensuring that no spillage between train, dev and test between the various layers would occur.", "labels": [], "entities": []}, {"text": "There are many cases where such a split comes handy for benchmarking, one example being using the whole datasets for training taggers and just portions of the datasets (i.e. the manually parsed subsets) to train parsers that require tagging as upstream processing.", "labels": [], "entities": []}, {"text": "For evaluating morphosyntactic tagging and lemmatisation in babushka-bench, we use a modified CoNLL 2018 shared task evaluation script to enable evaluation without parsing present.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task evaluation script", "start_pos": 94, "end_pos": 134, "type": "DATASET", "confidence": 0.8875540991624197}]}, {"text": "This script calculates the F1 metric between the gold and the real annotations, taking into account the possibility of different segmentation, which is not the casein these experiments as we use gold segmentation from the datasets to focus on the tasks of morphosyntactic tagging and lemmatisation.", "labels": [], "entities": [{"text": "F1 metric", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9763307273387909}, {"text": "morphosyntactic tagging", "start_pos": 256, "end_pos": 279, "type": "TASK", "confidence": 0.6345958411693573}]}, {"text": "When modelling morphosyntax, we predict morphosyntactic descriptions (MSDs), position-based encodings of partof-speech and feature-value pairs, as defined in the MULTEXT-East tagset.", "labels": [], "entities": [{"text": "MULTEXT-East tagset", "start_pos": 162, "end_pos": 181, "type": "DATASET", "confidence": 0.9222406446933746}]}, {"text": "The training-data-defined size of the tagset for each of the three languages lies between 600 and 1300 MSDs, depending on the language and the size of the training data.", "labels": [], "entities": []}, {"text": "This is the default tagset for the reldi-tagger and is also supported by the stanfordnlp tool, where language-specific tags (XPOS) are predicted as one of the three outputs by the tagging module (the other two being UD parts-of-speech (UPOS) and features (FEATS)).", "labels": [], "entities": [{"text": "FEATS", "start_pos": 256, "end_pos": 261, "type": "METRIC", "confidence": 0.9938918352127075}]}, {"text": "The datasets we use for our experiments are the three official datasets for training standard language technologies for these languages.", "labels": [], "entities": []}, {"text": "These are the ssj500k dataset for Slovenian ( , the hr500k dataset for Croatian ) and the SETimes.SR dataset for Serbian . While the Slovenian and Croatian datasets are both around 500 thousand tokens in size, the Serbian dataset is significantly smaller with only 87 thousand tokens in size.", "labels": [], "entities": [{"text": "ssj500k dataset", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.698855072259903}, {"text": "hr500k dataset", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.7957203388214111}, {"text": "SETimes.SR dataset", "start_pos": 90, "end_pos": 108, "type": "DATASET", "confidence": 0.8719774484634399}]}, {"text": "We additionally make use of the inflectional lexicons of these three languages, Sloleks for Slovenian ( , hrLex for Croatian (Ljube\u0161i\u00b4Ljube\u0161i\u00b4c, 2019a) and srLex for Serbian), all containing more than 100 thousand lemmas with around 3 million inflected forms.", "labels": [], "entities": []}, {"text": "While learning neural morphosyntactic taggers, we also experiment with various embeddings, mostly (1) the original CoNLL 2017 word2vec (w2v) embeddings for Slovenian and Croatian () (there are none available for Serbian), based on the CommonCrawl data, and (2) the CLARIN.SI embeddings for Slovenian , Croatian (Ljube\u0161i\u00b4Ljube\u0161i\u00b4c, 2018a) and Serbian), either trained with fastText (fT) or with word2vec (w2v) 4 on large, just partially publicly available texts due to copyright restrictions.", "labels": [], "entities": [{"text": "CoNLL 2017 word2vec (w2v) embeddings", "start_pos": 115, "end_pos": 151, "type": "DATASET", "confidence": 0.8996219549860273}, {"text": "CommonCrawl data", "start_pos": 235, "end_pos": 251, "type": "DATASET", "confidence": 0.9189677536487579}]}, {"text": "Our experiments are split into two main parts: experiments on morphosyntactic tagging in Section 3.1, backed with the comparison of the difference of the most frequent errors in the traditional and neural approaches, and the experiments on lemmatisation in Section 3.2.", "labels": [], "entities": [{"text": "morphosyntactic tagging", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.6952262818813324}]}], "tableCaptions": [{"text": " Table 1: F1 results in morphosyntactic annotation with the traditional and neural tool and different distributional  information.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9964056015014648}]}, {"text": " Table 3: F1 results in lemmatisation with the traditional and neural tool and different upstream processing.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9962700605392456}]}]}