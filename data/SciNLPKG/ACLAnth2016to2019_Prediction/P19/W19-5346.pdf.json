{"title": [{"text": "IITP-MT System for Gujarati-English News Translation Task at WMT 2019", "labels": [], "entities": [{"text": "IITP-MT", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9161232709884644}, {"text": "Gujarati-English News Translation", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.7138031323750814}, {"text": "WMT", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9554365873336792}]}], "abstractContent": [{"text": "We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair.", "labels": [], "entities": [{"text": "WMT 2019 News translation shared task", "start_pos": 30, "end_pos": 67, "type": "DATASET", "confidence": 0.9139306247234344}]}, {"text": "We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data.", "labels": [], "entities": []}, {"text": "We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolin-gual data.", "labels": [], "entities": [{"text": "subword-level neural machine translation (NMT)", "start_pos": 27, "end_pos": 73, "type": "TASK", "confidence": 0.7766970225742885}]}, {"text": "Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati\u2192English and English\u2192Gujarati, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9992202520370483}]}, {"text": "We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9786532819271088}, {"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9385950565338135}]}], "introductionContent": [{"text": "In this paper, we describe the system that we submit to the WMT 2019 1 news translation shared task.", "labels": [], "entities": [{"text": "WMT 2019 1 news translation shared task", "start_pos": 60, "end_pos": 99, "type": "TASK", "confidence": 0.8275307246616909}]}, {"text": "We participate in Gujarati-English language pair and submit two systems: English\u2192Gujarati and Gujarati\u2192English.", "labels": [], "entities": []}, {"text": "Gujarati language belongs to Indo-Aryan language family and is spoken predominantly in the Indian state of Gujarat.", "labels": [], "entities": []}, {"text": "It is a low-resource language as only a few thousands parallel sentences are available, which are not enough to train a neural machine translation (NMT) system as well statistical machine translation (SMT) system.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 120, "end_pos": 152, "type": "TASK", "confidence": 0.8259873886903127}, {"text": "statistical machine translation (SMT)", "start_pos": 168, "end_pos": 205, "type": "TASK", "confidence": 0.7732945481936137}]}, {"text": "Gujarati-English is a distant language pair and they have different linguistic properties including syntax, morphology, word order etc.", "labels": [], "entities": []}, {"text": "English follows subject-verb-object order while Gujarati follows subject-object-verb order.", "labels": [], "entities": []}, {"text": "http://www.statmt.org/wmt19/ translation-task.html NMT ( has recently become dominant paradigm for machine translation (MT) achieving state-ofthe-art on standard benchmark data sets for many language pairs.", "labels": [], "entities": [{"text": "translation-task.html NMT", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7362931668758392}, {"text": "machine translation (MT)", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.8437384128570556}]}, {"text": "As opposed to SMT, NMT systems are trained in an end-to-end manner.", "labels": [], "entities": [{"text": "SMT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9888416528701782}]}, {"text": "Training an effective NMT requires a huge amount of high-quality parallel corpus and in absence of that, an NMT system tends to perform poorly (.", "labels": [], "entities": []}, {"text": "However, back-translation () has been shown to improve NMT systems in such a situation.", "labels": [], "entities": []}, {"text": "In this work, we train a SMT system and an NMT system for both English\u2192Gujarati and Gujarati\u2192English using the original training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9869205951690674}]}, {"text": "SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9594635367393494}]}, {"text": "These corpora along with the original training corpora are used to improve the baseline NMT systems.", "labels": [], "entities": []}, {"text": "All the SMT and NMT systems are trained at subword level.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9716224074363708}]}, {"text": "Our SMT systems are standard phrase-based SMT systems (, and NMT systems are based on Transformer ( architecture.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9700142741203308}, {"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.8542028069496155}]}, {"text": "Experiments show that NMT systems achieve BLEU () scores of 10.4 and 8.1 for Gujarati\u2192English and English\u2192Gujarati, respectively, outperforming the baseline SMT systems even in the absence of enough-sized parallel data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9993894100189209}, {"text": "SMT", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.9773110747337341}]}, {"text": "Rest of the paper is arranged in following manner: Section 2 gives brief introduction of the Transformer architecture that we used for NMT training, Section 3 describes the task, Section 4 describes the submitted systems, Section 5 gives various evaluation scores for English-Gujarati translation pair, and finally, Section 6 concludes the work.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 135, "end_pos": 147, "type": "TASK", "confidence": 0.9437068998813629}]}], "datasetContent": [{"text": "We train phrase based statistical system (PB-SMT) () as well as Transformer (Vaswani et al., 2017) based neural system for comparing their performance under low-resource conditions.", "labels": [], "entities": []}, {"text": "In addition to that, PBSMT are used to genrate synthetic parallel data.", "labels": [], "entities": []}, {"text": "PBSMT systems are trained only on original training data, while neural based models are trained on original training data), and also with synthetic parallel data in addition to original data).", "labels": [], "entities": []}, {"text": "Synthetic parallel data are obtained through back-translation of a target monolingual corpus into source using PB-SMT system.", "labels": [], "entities": []}, {"text": "We use Moses () toolkit for PBSMT training and Sockeye (Hieber et al., 2017) toolkit for NMT training.", "labels": [], "entities": [{"text": "Sockeye (Hieber et al., 2017) toolkit", "start_pos": 47, "end_pos": 84, "type": "DATASET", "confidence": 0.9185388485590616}, {"text": "NMT training", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.8360287845134735}]}, {"text": "Some preprocessing of data is required before using it for experiment.", "labels": [], "entities": []}, {"text": "English data is tokenized using moses tokenizer, and truecased.", "labels": [], "entities": []}, {"text": "For tokenizing Gujarati data, we use indic nlp library 8 . After tokeninzation and truecasing, we subword () all original data.", "labels": [], "entities": [{"text": "tokenizing Gujarati data", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7393825650215149}]}, {"text": "We apply 10,000 BPE merge operations over English and Gujarati data independently.", "labels": [], "entities": []}, {"text": "For back-translation of monolingual data, two PBSMT models English\u2192Gujarati and Gujarati\u2192English are trained over original available parallel subworded corpora.", "labels": [], "entities": []}, {"text": "4-gram lan-8 https://github.com/anoopkunchukuttan/indic nlp library guage model is trained using KenLM.", "labels": [], "entities": []}, {"text": "For word alignment, we use GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8074660301208496}]}, {"text": "Model is tuned with Minimum Error Rate Training.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 20, "end_pos": 47, "type": "METRIC", "confidence": 0.909573033452034}]}, {"text": "After these two models are trained, monolingual subworded data from both English and Gujarati are back-translated using English\u2192Gujarati and Gujarati\u2192English PB-SMT model, respectively.", "labels": [], "entities": []}, {"text": "We merge the back translated data with original parallel data to have larger parallel corpora for Gujarati\u2192English and English\u2192Gujarati translation directions.", "labels": [], "entities": []}, {"text": "Finally, with the augmented parallel corpora, we train one Transformer based NMT model for each direction.", "labels": [], "entities": []}, {"text": "We use the following hyperparameters values of Sockeye toolkit: 6 layers in both encoder and decoder, word embedding size of 512, hidden size of 512, maximum input length of 50 tokens, Adam optimizer, word batch size 1000, attention type is dot, learning rate of 0.0002.", "labels": [], "entities": [{"text": "Sockeye toolkit", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9087170958518982}, {"text": "learning rate", "start_pos": 246, "end_pos": 259, "type": "METRIC", "confidence": 0.9442247152328491}]}, {"text": "The rest of the hyper-parameters are set to the default values in Sockeye.", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9873791337013245}]}, {"text": "We use early stopping criteria for terminating the training on the validation set of 1,998 parallel sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU scores of different SMT and NMT based systems. Synth: Synthetic data", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989402890205383}, {"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9731630682945251}]}, {"text": " Table 8: Preliminary results of WMT19 News Translation Task. Systems ordered by DA score z-score", "labels": [], "entities": [{"text": "WMT19 News Translation Task", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.7604579627513885}, {"text": "DA score z-score", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.9005675315856934}]}, {"text": " Table 3: Preliminary official results of WMT 2019  news translation task for Gujarati-English pair. Sys- tems ordered by DA score z-score; systems within a  cluster are considered tied; lines indicate clusters ac- cording to Wilcoxon rank-sum test p < 0.05; grayed  entry indicates resources that fall outside the con- straints provided.", "labels": [], "entities": [{"text": "WMT 2019  news translation task", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.727313244342804}, {"text": "DA score z-score", "start_pos": 122, "end_pos": 138, "type": "METRIC", "confidence": 0.9356020092964172}]}]}