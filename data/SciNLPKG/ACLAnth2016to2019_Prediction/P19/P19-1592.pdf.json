{"title": [{"text": "Optimal Transport-based Alignment of Learned Character Representations for String Similarity", "labels": [], "entities": [{"text": "Optimal Transport-based Alignment of Learned Character Representations", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.709245102746146}, {"text": "Similarity", "start_pos": 82, "end_pos": 92, "type": "TASK", "confidence": 0.6708255410194397}]}], "abstractContent": [{"text": "String similarity models are vital for record linkage, entity resolution, and search.", "labels": [], "entities": [{"text": "entity resolution", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.798231691122055}]}, {"text": "In this work, we present STANCE-a learned model for computing the similarity of two strings.", "labels": [], "entities": [{"text": "STANCE-a", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8535372018814087}]}, {"text": "Our approach encodes the characters of each string, aligns the encodings using Sinkhorn Iteration (alignment is posed as an instance of optimal transport) and scores the alignment with a convolutional neural network.", "labels": [], "entities": []}, {"text": "We evaluate STANCE's ability to detect whether two strings can refer to the same entity-a task we term alias detection.", "labels": [], "entities": [{"text": "STANCE", "start_pos": 12, "end_pos": 18, "type": "TASK", "confidence": 0.8823851346969604}, {"text": "alias detection", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.711785227060318}]}, {"text": "We construct five new alias detection datasets (and make them publicly available).", "labels": [], "entities": [{"text": "alias detection", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7271785140037537}]}, {"text": "We show that STANCE (or one of its variants) outperforms both state-of-the-art and classic, parameter-free similarity models on four of the five datasets.", "labels": [], "entities": [{"text": "STANCE", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.5164099335670471}]}, {"text": "We also demonstrate STANCE's ability to improve downstream tasks by applying it to an instance of cross-document coreference and show that it leads to a 2.8 point improvement in B 3 F1 over the previous state-of-the-art approach.", "labels": [], "entities": [{"text": "B 3 F1", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9330376386642456}]}], "introductionContent": [{"text": "String similarity models are crucial in record linkage, data integration, search and entity resolution systems, in which they are used to determine whether two strings refer to the same entity ().", "labels": [], "entities": [{"text": "data integration", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7828555405139923}, {"text": "entity resolution", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7225432842969894}]}, {"text": "In the context of these systems, measuring string similarity is complicated by a variety of factors including: the use of nicknames (e.g., Bill Clinton instead of William Clinton), token permutations (e.g., US Navy and Naval Forces of the US) and noise, among others.", "labels": [], "entities": [{"text": "measuring string similarity", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.7057204246520996}]}, {"text": "Many state-of-the-art systems employ either classic similarity models, such as Levenshtein, longest common subsequence, and Jaro-Winkler, or learned models for string similarity (.", "labels": [], "entities": []}, {"text": "While classic and learned approaches can be effective, they both have a number of shortcomings.", "labels": [], "entities": []}, {"text": "First, the classic approaches have few parameters making them inflexible and unlikely to succeed across languages or across domains with unique characteristics (e.g. company names, music album titles, etc.).", "labels": [], "entities": []}, {"text": "Classic models also assume that each edit has equal cost, which is unrealistic.", "labels": [], "entities": []}, {"text": "For example, consider the names Chun How and Chun Hao-which can refer to the same entity-and the names John A. Smith and John B. Smith, which cannot.", "labels": [], "entities": []}, {"text": "Even though the first pair differ by 2 edits and the second pair by 1, transforming ow to ao in the first pair should cost less than transforming A to B in the second.", "labels": [], "entities": []}, {"text": "Learned string similarity models address these problems by learning distinct costs for various edits and have thus proven successful in a number of domains (.", "labels": [], "entities": []}, {"text": "Some learned string similarity models, such as the SVM ( and CRFbased () approaches, use edit patterns akin to insertions/swaps/deletions, which may lead to strong inductive biases.", "labels": [], "entities": []}, {"text": "For example, even when costs are learned, two strings related by a token permutation-e.g., Grace Hopper and Hopper, Grace-are likely to have high cost even though they clearly refer to the same entity., on the other hand, provide less structure, encoding each string with a single vector embedding and measuring similarity between the embedded representations.", "labels": [], "entities": []}, {"text": "In this paper, we present a learned string similarity model that is flexible, captures sequential dependencies of characters, and is readily able to learn a wide range of edit patterns-such as token permutations.", "labels": [], "entities": []}, {"text": "Our approach is comprised of three components: the first encodes each character in both strings using a recurrent neural network; the second softly aligns the two encoded sequences by solving an instance of optimal transport; the third scores the alignment with a convolutional neural network.", "labels": [], "entities": []}, {"text": "Each component is differentiable, allowing for end-to-end training.", "labels": [], "entities": []}, {"text": "Our model is called STANCE-an acronym that stands for: Similarity of Transport-Aligned Neural Character Encodings.", "labels": [], "entities": [{"text": "STANCE-an", "start_pos": 20, "end_pos": 29, "type": "TASK", "confidence": 0.5118768811225891}, {"text": "Similarity of Transport-Aligned Neural Character Encodings", "start_pos": 55, "end_pos": 113, "type": "TASK", "confidence": 0.5854168087244034}]}, {"text": "We evaluate STANCE's ability to capture string similarity in a task we term alias detection.", "labels": [], "entities": [{"text": "STANCE", "start_pos": 12, "end_pos": 18, "type": "TASK", "confidence": 0.8791800737380981}, {"text": "alias detection", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7138763219118118}]}, {"text": "The input to alias detection is a query mention (i.e., a string) and a set of candidate mentions, and the goal is to score querycandidate pairs that can refer to the same entity higher than pairs that cannot.", "labels": [], "entities": [{"text": "alias detection", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.725204274058342}]}, {"text": "For example, an accurate model scores the query Philips with candidates Philips Corporation and Katherine Philips higher than with M.", "labels": [], "entities": []}, {"text": "Alias detection differs from both coreference and entity linking in that neither surrounding natural language context of the mention nor external knowledge are available.", "labels": [], "entities": [{"text": "Alias detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8686778843402863}, {"text": "entity linking", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.717142641544342}]}, {"text": "A similar task is studied in recent work (.", "labels": [], "entities": []}, {"text": "In experiments, we compare STANCE to stateof-the-art and classic models of string similarity in alias detection on 5 newly constructed datasetswhich we make publicly available.", "labels": [], "entities": [{"text": "alias detection", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.7146248370409012}]}, {"text": "Our results demonstrate that STANCE outperforms all other approaches on 4 out of 5 datasets in terms of Hits@1 and 3 out of 5 datasets in terms of mean average precision.", "labels": [], "entities": [{"text": "STANCE", "start_pos": 29, "end_pos": 35, "type": "TASK", "confidence": 0.7001570463180542}, {"text": "mean average precision", "start_pos": 147, "end_pos": 169, "type": "METRIC", "confidence": 0.7940932512283325}]}, {"text": "Of the two cases in which STANCE is outperformed by other methods in terms of mean average precision, one is by a variant of STANCE in an ablation study.", "labels": [], "entities": [{"text": "STANCE", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.8293362855911255}, {"text": "mean average precision", "start_pos": 78, "end_pos": 100, "type": "METRIC", "confidence": 0.8182215094566345}]}, {"text": "We also demonstrate STANCE's capacity for supporting downstream tasks by using it in cross-document coreference for the Twitter at the Grammy's dataset (.", "labels": [], "entities": [{"text": "Twitter at the Grammy's dataset", "start_pos": 120, "end_pos": 151, "type": "DATASET", "confidence": 0.7319410045941671}]}, {"text": "Using STANCE improves upon the state-of-the-art by 2.8 points of B 3 F1.", "labels": [], "entities": [{"text": "STANCE", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.8615260720252991}, {"text": "B 3 F1", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.8230640490849813}]}, {"text": "Analyzing our trained model reveals STANCE effectively learns sequence-aware character similarities, filters noise with optimal transport, and uses the CNN scoring component to detect unconventional similarity-preserving edit patterns.", "labels": [], "entities": [{"text": "STANCE", "start_pos": 36, "end_pos": 42, "type": "TASK", "confidence": 0.9136783480644226}]}], "datasetContent": [{"text": "We evaluate STANCE directly via alias detection and also indirectly via cross document coreference.", "labels": [], "entities": [{"text": "STANCE", "start_pos": 12, "end_pos": 18, "type": "TASK", "confidence": 0.7882623672485352}, {"text": "alias detection", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7893693149089813}, {"text": "cross document coreference", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.6244157453378042}]}, {"text": "We also conduct an ablation study in order to understand the contribution of each of STANCE's three components to its overall performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Qualities of the 5 created datasets. True positive are correct entity aliases included in the dev or test set.", "labels": [], "entities": []}, {"text": " Table 2: Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9215956628322601}]}, {"text": " Table 3: Hits at K.", "labels": [], "entities": [{"text": "K.", "start_pos": 18, "end_pos": 20, "type": "DATASET", "confidence": 0.8640714287757874}]}, {"text": " Table 4: Cross Document Coreference Results on Twit- ter at the Grammy's Dataset. Baseline results from  (Dredze et al., 2016).", "labels": [], "entities": [{"text": "Cross Document Coreference", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6748399138450623}, {"text": "Twit- ter at the Grammy's Dataset", "start_pos": 48, "end_pos": 81, "type": "DATASET", "confidence": 0.8271584883332253}]}]}