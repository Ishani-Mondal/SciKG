{"title": [{"text": "Corpus Creation and Analysis for Named Entity Recognition in Telugu-English Code-Mixed Social Media Data", "labels": [], "entities": [{"text": "Corpus Creation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6770058870315552}, {"text": "Named Entity Recognition", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8113041718800863}, {"text": "Telugu-English Code-Mixed Social Media Data", "start_pos": 61, "end_pos": 104, "type": "DATASET", "confidence": 0.5575588822364808}]}], "abstractContent": [{"text": "Named Entity Recognition(NER) is one of the important tasks in Natural Language Process-ing(NLP) and also is a sub task of Information Extraction.", "labels": [], "entities": [{"text": "Named Entity Recognition(NER)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7514473696549734}, {"text": "Information Extraction", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.8108518719673157}]}, {"text": "In this paper we present our work on NER in Telugu-English code-mixed social media data.", "labels": [], "entities": [{"text": "NER", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9887679219245911}, {"text": "Telugu-English code-mixed social media data", "start_pos": 44, "end_pos": 87, "type": "DATASET", "confidence": 0.541181868314743}]}, {"text": "Code-Mixing, a progeny of multilingualism is away in which multilingual people express themselves on social media by using linguistics units from different languages within a sentence or speech context.", "labels": [], "entities": []}, {"text": "Entity Extraction from social media data such as tweets(twitter) 1 is in general difficult due to its informal nature, code-mixed data further complicates the problem due to its informal , unstructured and incomplete information.", "labels": [], "entities": [{"text": "Entity Extraction from social media", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.891939914226532}]}, {"text": "We present a Telugu-English code-mixed corpus with the corresponding named entity tags.", "labels": [], "entities": []}, {"text": "The named entities used to tag data are Person('Per'), Organization('Org') and Loca-tion('Loc').", "labels": [], "entities": []}, {"text": "We experimented with the machine learning models Conditional Random Fields(CRFs), Decision Trees and Bidirec-tional LSTMs on our corpus which resulted in a F1-score of 0.96, 0.94 and 0.95 respectively.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.999273955821991}]}], "introductionContent": [{"text": "People from Multilingual societies often tend to switch between languages while speaking or writing.", "labels": [], "entities": []}, {"text": "This phenomenon of interchanging languages is commonly described by two terms \"codemixing\" and \"code-switching\".", "labels": [], "entities": []}, {"text": "Code-Mixing refers to the placing or mixing of various linguistic units such as affixes, words, phrases and clauses from two different grammatical systems within the same sentence and speech context.", "labels": [], "entities": [{"text": "Code-Mixing refers to the placing or mixing of various linguistic units such as affixes, words, phrases and clauses from two different grammatical systems within the same sentence and speech context", "start_pos": 0, "end_pos": 198, "type": "Description", "confidence": 0.7874673893675208}]}, {"text": "CodeSwitching refers to the placing or mixing of units such as words, phrases and sentences from two codes within the same speech context.", "labels": [], "entities": [{"text": "placing or mixing of units such as words, phrases and sentences from two codes within the same speech context", "start_pos": 28, "end_pos": 137, "type": "Description", "confidence": 0.6204029276967049}]}, {"text": "The structural difference between code-mixing and code-1 https://twitter.com/ switching can be understood in terms of the position of altered elements.", "labels": [], "entities": []}, {"text": "Intersentential modification of codes occurs in code-switching whereas the modification of codes is intrasentential in code-mixing..", "labels": [], "entities": []}, {"text": "Both codemixing and code-switching can be observed in social media platforms like Twitter and Facebook, In this paper, we focus on the code-mixing aspect between Telugu and English Languages.", "labels": [], "entities": []}, {"text": "Telugu is a Dravidian language spoken majorly in the Indian states of Andhra Pradesh and Telangana.", "labels": [], "entities": []}, {"text": "A significant amount of linguistic minorities are present in the neighbouring states.", "labels": [], "entities": []}, {"text": "It is one of six languages designated as a classical language of India by the Indian government The following is an instance taken from Twitter depicting Telugu-English code-mixing, each word in the example is annotated with its respective Named Entity and Language Tags ('Eng' for English and 'Tel' for Telugu).", "labels": [], "entities": []}, {"text": "Translation: \"Sir it has been a year that this government school in Rajanna Siricilla district has got computers and fans still there is no permanent electricity, Could you please respond @KTRTRS @Collector RSL \"", "labels": [], "entities": [{"text": "KTRTRS @Collector RSL", "start_pos": 189, "end_pos": 210, "type": "DATASET", "confidence": 0.7528870850801468}]}], "datasetContent": [{"text": "In this section we present the experiments using different combinations of features and systems.", "labels": [], "entities": []}, {"text": "In order to determine the effect of each feature and parameters of the model we performed several experiments using some set of features at once and all at a time simultaneously changing the parameters of the model, like criterion ('Information gain', 'gini') and maximum depth of the tree for decision tree model, regularization parameters and algorithms of optimization like 'L2 regularization' 4 , 'Avg.", "labels": [], "entities": []}, {"text": "Perceptron' and 'Passive Aggressive' for CRF.", "labels": [], "entities": [{"text": "CRF", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8133344054222107}]}, {"text": "Optimization algorithms and loss functions in LSTM.", "labels": [], "entities": []}, {"text": "We used 5 fold cross validation in order to validate our classification models.", "labels": [], "entities": []}, {"text": "We used 'scikit-learn' and 'keras' libraries for the implementation of the above algorithms.", "labels": [], "entities": []}, {"text": "Conditional Random Field (CRF) : Conditional Random Fields (CRF's) area class of statistical modelling methods applied in machine learning and often used for structured prediction tasks.", "labels": [], "entities": []}, {"text": "In sequence labelling tasks like POS Tagging, adjective is more likely to be followed by a noun than a verb.", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.7965028584003448}]}, {"text": "In NER using the BIO standard annotation, I-ORG cannot follow I-PER.", "labels": [], "entities": [{"text": "NER", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9068934917449951}, {"text": "BIO standard annotation", "start_pos": 17, "end_pos": 40, "type": "DATASET", "confidence": 0.893553634484609}]}, {"text": "We wish to look at sentence level rather than just word level as looking at the correlations between the labels in sentence is beneficial, so we chose to work with CRF's in this problem of named entity tagging.", "labels": [], "entities": [{"text": "named entity tagging", "start_pos": 189, "end_pos": 209, "type": "TASK", "confidence": 0.6129903097947439}]}, {"text": "We have experimented with regularization parameters and algorithms of optimization like 'L2 regularization', 'Avg.", "labels": [], "entities": []}, {"text": "Perceptron' and 'Passive Aggressive' for CRF.", "labels": [], "entities": [{"text": "CRF", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8133344054222107}]}], "tableCaptions": [{"text": " Table 1: Inter Annotator Agreement.", "labels": [], "entities": [{"text": "Inter Annotator Agreement", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.8146728277206421}]}, {"text": " Table 2: Tags and their Count in Corpus", "labels": [], "entities": [{"text": "Count", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9845842719078064}, {"text": "Corpus", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.30343371629714966}]}, {"text": " Table 3: CRF Model with 'c2=0.1' and 'l2sgd' algo.", "labels": [], "entities": []}, {"text": " Table 4: Decision Tree Model with 'max-depth=32'", "labels": [], "entities": []}, {"text": " Table 5: Feature Specific Results for CRF", "labels": [], "entities": [{"text": "Feature Specific", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6653833240270615}, {"text": "CRF", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8469958305358887}]}, {"text": " Table 6: Feature Specific Results for Decision tree", "labels": [], "entities": []}, {"text": " Table 7: Bi-LSTM model with optimizer = 'adam' and  has a weighted f1-score of 0.95", "labels": [], "entities": []}]}