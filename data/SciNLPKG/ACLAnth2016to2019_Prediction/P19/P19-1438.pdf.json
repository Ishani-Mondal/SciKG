{"title": [], "abstractContent": [{"text": "We consider a zero-shot semantic parsing task: parsing instructions into compositional logical forms, in domains that were not seen during training.", "labels": [], "entities": [{"text": "zero-shot semantic parsing task", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.7101976498961449}, {"text": "parsing instructions into compositional logical forms", "start_pos": 47, "end_pos": 100, "type": "TASK", "confidence": 0.8292109866937002}]}, {"text": "We present anew dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application's initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction.", "labels": [], "entities": []}, {"text": "We introduce anew training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain.", "labels": [], "entities": []}, {"text": "We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation.", "labels": [], "entities": [{"text": "zero-shot adaptation", "start_pos": 183, "end_pos": 203, "type": "TASK", "confidence": 0.7465153336524963}]}, {"text": "Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser.", "labels": [], "entities": [{"text": "zero-shot adaptation", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7462798058986664}]}], "introductionContent": [{"text": "The idea of interacting with machines via natural language instructions and queries has fascinated researchers for decades.", "labels": [], "entities": []}, {"text": "Recent years have seen an increasing number of applications that have a natural language interface, either in the form of chatbots or via \"intelligent personal assistants\" such as Alexa (Amazon), Google Assistant, Siri (Apple), and Cortana (Microsoft).", "labels": [], "entities": []}, {"text": "In the near future, we may find ourselves in a world where even more functionality could be accessed via a natural language user interface (NLUI).", "labels": [], "entities": []}, {"text": "If so, we better seek answers to the following questions: Will every developing team need to hire NLP experts to develop a NLUI for Our code and data are available at: https://github.com/givoli/TechnionNLI.", "labels": [], "entities": []}, {"text": "Can we hope fora general framework that once trained on annotated data from a set of domains, does not require annotated data from a newly presented domain?", "labels": [], "entities": []}, {"text": "Previous work on tasks related to NLUI for applications mostly relied on in-domain data (e.g. ;), and papers that did not rely on in-domain data did not attempt to parse instructions into compositional logical forms.", "labels": [], "entities": [{"text": "parse instructions into compositional logical forms", "start_pos": 164, "end_pos": 215, "type": "TASK", "confidence": 0.7668551206588745}]}, {"text": "To fill this gap, we address the task of zeroshot semantic parsing for instructions: training a parser so that it can parse instructions into compositional logical forms, where the instructions are from domains that were not seen during training.", "labels": [], "entities": [{"text": "zeroshot semantic parsing", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.598252127567927}, {"text": "parse instructions into compositional logical forms", "start_pos": 118, "end_pos": 169, "type": "TASK", "confidence": 0.7726204295953115}]}, {"text": "Formally, our task assumes a set D = {d 1 , ..., d n } of source domains, each corresponding to a simple application (e.g. a calendar or a file manager) and an application program interface (API) consisting of a set of interface methods.", "labels": [], "entities": []}, {"text": "Each interface method is augmented with a list of description phrases that are expected to be used by the users of the application to ask for the invocation of that method.", "labels": [], "entities": []}, {"text": "These instructions are to be parsed into logical forms that denote a method call with specific arguments.", "labels": [], "entities": []}, {"text": "We collected anew dataset of 1,390 examples from 7 domains.", "labels": [], "entities": []}, {"text": "Each example in the dataset is a triplet consisting of (a) the application's initial state, (b) an instruction, to be carried out in context of that state, and (c) the state of the application after carrying out the instruction, also referred to as the desired state.", "labels": [], "entities": []}, {"text": "The instructions were provided by MTurk workers, one for each pair of initial and desired states.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.7349439263343811}]}, {"text": "demonstrates examples from two of the domains in our dataset.", "labels": [], "entities": []}, {"text": "We present anew training algorithm for zeroshot semantic parsing, which involves learning the weights in two steps, such that in each step different source domains are used.", "labels": [], "entities": [{"text": "zeroshot semantic parsing", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.773928185304006}]}, {"text": "Our training algo-: Two examples from our dataset.", "labels": [], "entities": []}, {"text": "Annotators are presented with a visualization of initial and desired states, and are asked to write an instruction that will transfer the system from the former state to the latter.", "labels": [], "entities": []}, {"text": "rithm is motivated by the goal of optimizing the weights for unseen domains rather than for the source domains, and is integrated into the floating parser: a parser that was designed for question answering, but is easy to adjust to instruction execution (see \u00a7 4.1).", "labels": [], "entities": [{"text": "question answering", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.7981044054031372}]}, {"text": "To further assist the parser in dealing with the zero-shot setup, we extract additional features, mostly based on co-occurrence of primitive logical forms and the description phrases that are provided for the interface methods.", "labels": [], "entities": []}, {"text": "We also use the application logic to dismiss candidate logical forms that represent a method call which does not modify the application state or results in an exception being thrown from the application logic.", "labels": [], "entities": []}, {"text": "Our training algorithm yields an averaged accuracy of 44.5%, compared to 39.1% of the parser when trained with its original AdaGrad training algorithm), but with our features and filtering logic.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9966011047363281}]}, {"text": "Further exclusion of our features and filtering logic decreases accuracy to 28.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9996161460876465}]}, {"text": "We demonstrate that, relative to the baseline, our training algorithm yields smaller weights to some features in away that can be expected to benefit previously unseen domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "Models and Baselines We compare eight combinations of parsers.", "labels": [], "entities": []}, {"text": "Four models use our GMDP training algorithm: (a) GMDP: the full model ( \u00a7 4.2); (b) GMDP\u2212A: where we do not apply the application logic filtering ( \u00a7 4.2.2); (c) GMDP\u2212F: where we do not use our features ( \u00a7 4.2.3); and (d) GMDP\u2212FA: where we omit both our new features and the application logic filtering.", "labels": [], "entities": [{"text": "GMDP", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.839594841003418}, {"text": "FA", "start_pos": 228, "end_pos": 230, "type": "METRIC", "confidence": 0.8492156267166138}]}, {"text": "The other four models are identical, but they use the original ADAGRAD training.", "labels": [], "entities": [{"text": "ADAGRAD training", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.6611845940351486}]}, {"text": "Note that ADAGRAD\u2212FA corresponds to the original FParser (with minimal modifications to support instruction parsing).", "labels": [], "entities": [{"text": "ADAGRAD\u2212", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9509676992893219}, {"text": "FA", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.5439587235450745}, {"text": "FParser", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9289836287498474}]}, {"text": "Notice that in some real-world settings our application logic filtering, which requires invoking interface methods hundreds of times per inference, might be impractical (e.g. if executing the interface methods is computationally intensive).", "labels": [], "entities": [{"text": "application logic filtering", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6617866357167562}]}, {"text": "This motivates us to consider the results of the ablated model that does not use the application logic.", "labels": [], "entities": []}, {"text": "Experiments In each experiment we train the parsers on examples from 6 application domains and test them on the remaining domain.", "labels": [], "entities": []}, {"text": "Our evaluation metric is accuracy: the fraction of the test examples where a correct denotation (desired state) is predicted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9994403719902039}]}, {"text": "For examples where multiple logical forms achieve maximum score, we consider the fraction that yields the desired state.", "labels": [], "entities": []}, {"text": "While in our main results we report the accuracy of the parsers on the target domain's test set, for the error and qualitative analyses we report the accuracy on the target domain's training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9989681243896484}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9987664222717285}]}, {"text": "We do that in order to avoid multiple runs on the test sets; we do not use the target domain's training set for other purposes (e.g. hyper-parameter tuning).", "labels": [], "entities": [{"text": "hyper-parameter tuning", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.6792682111263275}]}, {"text": "The average number of training and test examples per domain is 101 and 97.6, respectively.", "labels": [], "entities": []}, {"text": "Hyper-parameter tuning We use a grid search and leave-one-out cross-validation over the source domains to tune the hyper-parameters.", "labels": [], "entities": []}, {"text": "We tune the following hyper-parameters: the L1 regularization coefficient, the initial step-size, the number of training iterations (for the GMDP algorithm: the number of training iterations in the second step), and for the GMDP algorithm also: the number and identity of training domains used in the first (D 1 ) and second (D 2 ) steps, and the number of training iterations during the first step.", "labels": [], "entities": []}, {"text": "We use abeam size of 200 and limit the number of rule applications per derivation to 15.", "labels": [], "entities": []}, {"text": "We provide more details about the values of the hyper-parameters we consider in the appendix.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The domains in our dataset. The interface method parameters are presented in Java syntax. In the second  column, the properties of the non-primitive entities appear in parentheses.", "labels": [], "entities": []}, {"text": " Table 2: Test set accuracy. In parenthesis: result for the in-domain setup (training with the 96-104 training ex- amples of the target domain). GMDP results marked with * represent a statistically significant difference from  ADAGRAD (\u03b1 = 0.05, using the paired bootstrap test", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.984889805316925}, {"text": "ADAGRAD", "start_pos": 227, "end_pos": 234, "type": "METRIC", "confidence": 0.9841645359992981}]}]}