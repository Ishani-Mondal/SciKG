{"title": [{"text": "A Self-Training Approach for Short Text Clustering", "labels": [], "entities": [{"text": "Short Text Clustering", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.6133674184481303}]}], "abstractContent": [{"text": "Short text clustering is a challenging problem when adopting traditional bag-of-words or TF-IDF representations, since these lead to sparse vector representations for short texts.", "labels": [], "entities": [{"text": "Short text clustering", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7336375514666239}]}, {"text": "Low-dimensional continuous representations or em-beddings can counter that sparseness problem: their high representational power is exploited in deep clustering algorithms.", "labels": [], "entities": []}, {"text": "While deep clustering has been studied extensively in computer vision, relatively little work has fo-cused on NLP.", "labels": [], "entities": []}, {"text": "The method we propose, learns discriminative features from both an autoen-coder and a sentence embedding, then uses assignments from a clustering algorithm as supervision to update weights of the encoder network.", "labels": [], "entities": []}, {"text": "Experiments on three short text datasets empirically validate the effectiveness of our method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text clustering groups semantically similar text without using supervision or manually assigned labels.", "labels": [], "entities": [{"text": "Text clustering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7312376499176025}]}, {"text": "Text clusters have proven to be beneficial in many applications including news recommendation (, language modeling (), query expansion (, visualization (, and corpus summarization.", "labels": [], "entities": [{"text": "news recommendation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7628193199634552}, {"text": "language modeling", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7169367223978043}, {"text": "query expansion", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.7433792948722839}, {"text": "corpus summarization", "start_pos": 159, "end_pos": 179, "type": "TASK", "confidence": 0.6395604610443115}]}, {"text": "Due to the popularity of social media and online fora such as Twitter and Reddit, texts containing only few words have become prevalent on the web.", "labels": [], "entities": []}, {"text": "Compared to clustering of long documents, Short Text Clustering (STC) introduces additional challenges.", "labels": [], "entities": [{"text": "Short Text Clustering (STC)", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.571361169219017}]}, {"text": "Traditionally, text is represented as a bag-of-words (BOW) or termfrequency inverse-document-frequency (TF-IDF) vectors, after which a clustering algorithm such as k-means is applied to partition the texts into homogeneous groups (.", "labels": [], "entities": []}, {"text": "Due to the short lengths of such texts, their vector representations tend to become very sparse.", "labels": [], "entities": []}, {"text": "As a result, traditional measures for similarity, which rely on word overlap or distance between high-dimensional vectors, become ineffective (.", "labels": [], "entities": []}, {"text": "Previous work on STC enriched short text representations by incorporating features from external resources. and extended short texts using articles from Wikipedia.", "labels": [], "entities": [{"text": "STC", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9228087067604065}]}, {"text": "In similar fashion, and proposed different methods to enrich text representation using ontologies.", "labels": [], "entities": []}, {"text": "More recently, low-dimensional representations have shown potential to counter the sparsity problem in STC.", "labels": [], "entities": [{"text": "STC", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.8377360701560974}]}, {"text": "Combined with neural network architectures, embeddings of words (), sentences () and documents) were proven to be effective on a variety of tasks in machine learning for NLP.", "labels": [], "entities": []}, {"text": "Deep clustering methods first embed the highdimensional data into a lower dimensional space, after which a clustering algorithm is applied.", "labels": [], "entities": []}, {"text": "These methods either perform clustering after having trained the embedding transformation, or jointly optimize both the embedding and clustering, and we situate our method in the former.", "labels": [], "entities": []}, {"text": "Closely related to our work is the method of Deep Embedded Clustering (DEC) (, which learns feature representations and cluster assignments using deep neural networks.", "labels": [], "entities": [{"text": "learns feature representations and cluster assignments", "start_pos": 85, "end_pos": 139, "type": "TASK", "confidence": 0.6189847886562347}]}, {"text": "DEC learns a mapping from the data space to a lower-dimensional feature space while iteratively optimizing a clustering objective.", "labels": [], "entities": [{"text": "DEC", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6781805157661438}]}, {"text": "The self-taught convolutional neural network (STC 2 ) framework proposed by uses a dimensionality reduction technique to generate auxiliary targets fora neural network architecture.", "labels": [], "entities": []}, {"text": "A convolutional neural network (CNN) learns feature rep- resentations in order to reconstruct these auxiliary targets.", "labels": [], "entities": []}, {"text": "Trained representations from the CNN are clustered using the k-means algorithm.", "labels": [], "entities": []}, {"text": "Two recent surveys provide an overview of research on deep clustering methods).", "labels": [], "entities": []}, {"text": "Similar to, we follow a multiphase approach and train a neural network (which we will refer to as the encoder) to transform embeddings to a latent space before clustering.", "labels": [], "entities": []}, {"text": "However, we apply two crucial modifications.", "labels": [], "entities": []}, {"text": "As opposed to CNN-based encoders (, we propose the use of Smooth Inverse Frequency (SIF) embeddings () in order to simplify and make clustering more efficient while maintaining performance.", "labels": [], "entities": []}, {"text": "During the second stage of clustering, we apply self-training using soft cluster assigments to fine-tune the encoder before applying a final clustering.", "labels": [], "entities": [{"text": "clustering", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.9773563146591187}]}, {"text": "We describe our methodology in more detail in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we evaluate our method using three short text datasets, measuring for clustering accuracy and normalized mutual information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9803161025047302}]}, {"text": "Our model matches or produces better results compared to more sophisticated neural network architectures.", "labels": [], "entities": []}], "datasetContent": [{"text": "After describing the datasets (Section 3.1) and the experiment design (Section 3.2), we will present the results of these experiments (Section 3.3).", "labels": [], "entities": []}, {"text": "We compare our method to baselines for STC including clustering of TF and TF-IDF representations, Skip-thought Vectors ( and the best reported STC 2 model by.", "labels": [], "entities": [{"text": "STC", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9859429597854614}]}, {"text": "Following (Van Der Maaten, 2009; Xie et al., 2016), we set sizes of hidden layers to d:500:500:2000:20 for all datasets, where dis the short text embedding dimension for all datasets.", "labels": [], "entities": []}, {"text": "We used pre-trained word2vec embeddings 3 with fixed \u03b1 = 0.1 value for all corpora.", "labels": [], "entities": []}, {"text": "We set the batch size to 64 and pre-trained the autoencoder for 15 epochs.", "labels": [], "entities": []}, {"text": "We initialized stochastic gradient descent with a learning rate of 0.01 and momentum value of 0.9.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.969901978969574}, {"text": "momentum", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.969801127910614}]}, {"text": "During experiments, the choice of initial centroids had considerable impact on clustering performance when applying the k-means algorithm.", "labels": [], "entities": []}, {"text": "To reduce this influence of initialization, we restarted k-means 100 times with different initial centroids, as;, and selected the best centroids, which obtained the lowest sum of squared distances of samples to their closest cluster center.", "labels": [], "entities": []}, {"text": "Similar to, results are averaged over 5 trials and we also report the standard deviation on the scores.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 70, "end_pos": 88, "type": "METRIC", "confidence": 0.9355680346488953}]}], "tableCaptions": [{"text": " Table 1: Clustering results (accuracy ACC and normalized mutal information NMI) for three short text collections  using various representations and self-training methods. STC 2 and our method involve additional fine-tuning of  encoders, others apply k-means directly on short text representations. Performance results are average and standard  deviations over 5 runs.", "labels": [], "entities": [{"text": "accuracy ACC", "start_pos": 30, "end_pos": 42, "type": "METRIC", "confidence": 0.9824986755847931}]}, {"text": " Table 2: Statistics for the short text clustering datasets  as used by Xu et al. (2017): number of classes (C),  number of short texts (N ), average number of tokens  per text (T ) and vocabulary size (|V |).", "labels": [], "entities": []}]}