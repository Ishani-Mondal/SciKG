{"title": [], "abstractContent": [{"text": "We present open domain response generation with meta-words.", "labels": [], "entities": [{"text": "open domain response generation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.6430744826793671}]}, {"text": "A meta-word is a structured record that describes various attributes of a response , and thus allows us to explicitly model the one-to-many relationship within open domain dialogues and perform response generation in an explainable and controllable manner.", "labels": [], "entities": [{"text": "response generation", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.7323834002017975}]}, {"text": "To incorporate meta-words into generation , we enhance the sequence-to-sequence architecture with a goal tracking memory network that formalizes meta-word expression as a goal and manages the generation process to achieve the goal with a state memory panel and a state controller.", "labels": [], "entities": []}, {"text": "Experimental results on two large-scale datasets indicate that our model can significantly outperform several state-of-the-art generation models in terms of response relevance, response diversity, accuracy of one-to-many modeling, accuracy of meta-word expression , and human evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9986076951026917}, {"text": "accuracy", "start_pos": 231, "end_pos": 239, "type": "METRIC", "confidence": 0.9983069896697998}]}], "introductionContent": [{"text": "Human-machine conversation is a fundamental problem in NLP.", "labels": [], "entities": []}, {"text": "Traditional research focuses on building task-oriented dialog systems () to achieve specific user goals such as restaurant reservation through limited turns of dialogues within specific domains.", "labels": [], "entities": [{"text": "restaurant reservation", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.6990139037370682}]}, {"text": "Recently, building a chatbot for open domain conversation has attracted increasing attention, not only owing to the availability of large amount of human-human conversation data on internet, but also because of the success of such systems in real products such as the social bot XiaoIce () from Microsoft.", "labels": [], "entities": [{"text": "XiaoIce", "start_pos": 279, "end_pos": 286, "type": "DATASET", "confidence": 0.9072811603546143}]}, {"text": "A common approach to implementing a chatbot is to learn a response generation model within an encoder-decoder framework (Vinyals and Le,: An example of response generation with metawords.", "labels": [], "entities": [{"text": "response generation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.72147436439991}]}, {"text": "The underlined word means it is copied from the message, and the word in bold means it corresponds to high specificity.", "labels": [], "entities": []}, {"text": "Although the architecture can naturally model the correspondence between a message and a response, and is easy to extend to handle conversation history () and various constraints (, it is notorious for generating safe responses such as \"I don't know\" and \"me too\" in practice.", "labels": [], "entities": []}, {"text": "A plausible reason for the \"safe response\" issue is that there exists one-to-many relationship between messages and responses.", "labels": [], "entities": []}, {"text": "One message could correspond to many valid responses and vice versa ().", "labels": [], "entities": []}, {"text": "The vanilla encoder-decoder architecture is prone to memorize high-frequency patterns in data, and thus tends to generate similar and trivial responses for different messages.", "labels": [], "entities": []}, {"text": "A typical method for modeling the relationship between messages and responses is to introduce latent variables into the encoder-decoder framework.", "labels": [], "entities": []}, {"text": "It is, however, difficult to explain what relationship a latent variable represents, nor one can control responses to generate by manipulating the latent variable.", "labels": [], "entities": []}, {"text": "Although a recent study ( ) replaces continuous latent variables with discrete ones, it still needs a lot of post human effort to explain the meaning of the variables.", "labels": [], "entities": []}, {"text": "In this work, we aim to model the one-to-many relationship in open domain dialogues in an explainable and controllable way.", "labels": [], "entities": []}, {"text": "Instead of using latent variables, we consider explicitly representing the relationship between a message and a response with meta-words . A meta-word is a structured record that characterizes the response to generate.", "labels": [], "entities": []}, {"text": "The record consists of a group of variables with each an attribute of the response.", "labels": [], "entities": []}, {"text": "Each variable is in a form of (key, type, value) where \"key\" defines the attribute, \"value\" specifies the attribute, and \"type\" \u2208 {r, c} indicates whether the variable is real-valued (r) or categorical (c).", "labels": [], "entities": []}, {"text": "Given a message, a meta-word corresponds to one kind of relationship between the message and a response, and by manipulating the meta-word (e.g., values of variables or combination of variables), one can control responses in abroad way.", "labels": [], "entities": []}, {"text": "gives an example of response generation with various meta-words, where \"Act\", \"Len\", \"Copy\", \"Utts\", and \"Spe\" are variables of a meta-word and refer to dialogue act, response length (including punctuation marks), if copy from the message, if made up of multiple utterances, and specificity level () respectively 2 . Advantages of response generation with meta-words are three-folds: (1) the generation model is explainable as the meta-words inform the model, developers, and even end users what responses they will have before the responses are generated; (2) the generation process is controllable.", "labels": [], "entities": [{"text": "response generation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7661121189594269}, {"text": "Utts", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9074222445487976}]}, {"text": "The metaword system acts as an interface that allows developers to customize responses by tailoring the set of attributes; (3) the generation approach is general.", "labels": [], "entities": []}, {"text": "By taking dialogue acts ( ), personas (), emotions ( , and specificity () as attributes, our approach can address the problems in the existing literature in a unified form; and generation-based open domain dialogue systems now become scalable, since the model supports feature engineering on meta-words.", "labels": [], "entities": []}, {"text": "The challenge of response generation with meta-words lies in how to simultaneously ensure relevance of a response to the message and fidelity of the response to the meta-word.", "labels": [], "entities": [{"text": "response generation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7548181116580963}]}, {"text": "To tackle the challenge, we propose equipping the vanilla sequence-to-sequence architecture with a novel goal tracking memory network (GTMN) and crafting anew loss item for learning GTMN.", "labels": [], "entities": []}, {"text": "GTMN sets meta-word expression as a goal of generation and dynamically monitors expression of each variable in the meta-word during the decoding process.", "labels": [], "entities": []}, {"text": "Specifically, GTMN consists of a state memory panel and a state controller where the former records status of meta-word expression and the latter manages information exchange between the state memory panel and the decoder.", "labels": [], "entities": []}, {"text": "In decoding, the state controller updates the state memory panel according to the generated sequence, and reads out difference vectors that represent the residual of the meta-word.", "labels": [], "entities": []}, {"text": "The next word from the decoder is predicted based on attention on the message representations, attention on the difference vectors, and the word predicted in the last step.", "labels": [], "entities": []}, {"text": "In learning, besides the negative log likelihood, we further propose minimizing a state update loss that can directly supervise the learning of the memory network under the ground truth.", "labels": [], "entities": []}, {"text": "We also propose a meta-word prediction method to make the proposed approach complete in practice.", "labels": [], "entities": [{"text": "meta-word prediction", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.723857656121254}]}, {"text": "We test the proposed model on two large-scale open domain conversation datasets built from Twitter and Reddit, and compare the model with several state-of-the-art generation models in terms of response relevance, response diversity, accuracy of one-to-many modeling, accuracy of metaword expression, and human judgment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.9986646175384521}, {"text": "accuracy", "start_pos": 267, "end_pos": 275, "type": "METRIC", "confidence": 0.9982026815414429}]}, {"text": "Evaluation results indicate that our model can significantly outperform the baseline models over most of the metrics on both datasets.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are three-folds: (1) proposal of explicitly modeling one-to-many relationship and explicitly controlling response generation in open domain dialogues with multiple variables (a.k.a., meta-word); (2) proposal of a goal tracking memory network that naturally allows a meta-word to guide response generation; and (3) empirical verification of the effectiveness of the proposed model on two large-scale datasets.", "labels": [], "entities": [{"text": "response generation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7394008636474609}, {"text": "response generation", "start_pos": 317, "end_pos": 336, "type": "TASK", "confidence": 0.7160293310880661}]}], "datasetContent": [{"text": "We test GTMNES2S on two large-scale datasets.", "labels": [], "entities": [{"text": "GTMNES2S", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8638644814491272}]}, {"text": "We mine 10 million message-response pairs from Twitter FireHose, covering 2-month period from June 2016 to July 2016, and sample 10 million pairs from the full Reddit data . As preprocessing, we remove duplicate pairs, pairs with a message or a response having more than 30 words, and messages that correspond to more than 20 responses to prevent them from dominating learning.", "labels": [], "entities": []}, {"text": "After that, there are 4, 759, 823 pairs left for Twitter and 4, 246, 789 pairs left for Reddit.", "labels": [], "entities": []}, {"text": "On average, each message contains 10.78 words in the Twitter data and 12.96 words in the Reddit data.", "labels": [], "entities": [{"text": "Twitter data", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9162378907203674}, {"text": "Reddit data", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9118108451366425}]}, {"text": "The average lengths of responses in the Twitter data and the Reddit data are 11.03 and 12.75 respectively.", "labels": [], "entities": [{"text": "Twitter data", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.868134617805481}, {"text": "Reddit data", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9417992234230042}]}, {"text": "From the pairs after pre-processing, we randomly sample 10k pairs as a validation set and 10k pairs as a test set for each data, and make sure that there is no overlap between the two sets.", "labels": [], "entities": []}, {"text": "After excluding pairs in the validation sets and the test sets, the left pairs are used for model training.", "labels": [], "entities": []}, {"text": "The test sets are built for calculating automatic metrics.", "labels": [], "entities": []}, {"text": "Besides, we randomly sample 1000 distinct messages from each of the two test sets and recruit human annotators to judge the quality of responses generated for these messages.", "labels": [], "entities": []}, {"text": "For both the Twitter data and the Reddit data, top 30, 000 most frequent words in messages and responses in the training sets are kept as message vocabularies and response vocabularies.", "labels": [], "entities": [{"text": "Twitter data", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.766171932220459}, {"text": "Reddit data", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.893955409526825}]}, {"text": "In the Twitter data, the message vocabulary and the response vocabulary cover 99.17% and 98.67% words appearing in messages and responses respectively.", "labels": [], "entities": []}, {"text": "The two ratios are 99.52% and 98.8% respectively in the Reddit data.", "labels": [], "entities": [{"text": "Reddit data", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9310105741024017}]}, {"text": "Other words are marked as \"UNK\".", "labels": [], "entities": []}, {"text": "We conduct both automatic evaluation and human evaluation.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.6754381358623505}]}, {"text": "In terms of automatic ways, we evaluate models from four aspects: relevance, diversity, accuracy of one-to-many modeling, and accuracy of meta-word expression.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9990725517272949}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9984257221221924}]}, {"text": "For relevance, besides BLEU (), we follow () and employ Embedding Average (Average), Embedding Extrema (Extrema), Embedding Greedy (Greedy) as metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9987152814865112}, {"text": "Embedding Average (Average)", "start_pos": 56, "end_pos": 83, "type": "METRIC", "confidence": 0.9016676545143127}]}, {"text": "To evaluate diversity, we follow () and use Distinct-1 (Dist1) and Distinct-2 (Dist2) as metrics which are calculated as the ratios of distinct unigrams and bigrams in the generated responses.", "labels": [], "entities": []}, {"text": "For accuracy of one-to-many modeling, we utilize A-bow precision (A-prec), A-bow recall (A-rec), E-bow precision, and E-bow recall (Erec) proposed in (  as metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9971265196800232}, {"text": "A-bow precision (A-prec)", "start_pos": 49, "end_pos": 73, "type": "METRIC", "confidence": 0.8179111838340759}, {"text": "A-bow recall (A-rec)", "start_pos": 75, "end_pos": 95, "type": "METRIC", "confidence": 0.8236266493797302}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.5518082976341248}, {"text": "recall (Erec)", "start_pos": 124, "end_pos": 137, "type": "METRIC", "confidence": 0.8809936195611954}]}, {"text": "For accuracy of meta-word expression, we measure accuracy for categorical variables and square deviation for real-valued variables.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.99837327003479}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9995071887969971}]}, {"text": "Metrics of relevance, diversity, and accuracy of meta-word expression are calculated on the 10k test data based on top 1 responses from beam search.", "labels": [], "entities": [{"text": "relevance", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9581906199455261}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9990742206573486}]}, {"text": "To measure the accuracy of meta-word expression fora generated response, we extract values of the metaword of the response with the methods described in Section 6.2, and compare these values with the oracle ones sampled from distributions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9988547563552856}]}, {"text": "Metrics of accuracy of one-to-many modeling require a test message to have multiple reference responses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9978814721107483}]}, {"text": "Thus, we filter the test sets by picking out messages that have at least 2 responses, and form two subsets with 166 messages for Twitter and 135 messages for Reddit respectively.", "labels": [], "entities": []}, {"text": "On average, each message corresponds to 2.8 responses in the Twitter data and 2.92 responses in the Reddit data.", "labels": [], "entities": [{"text": "Twitter data", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9195221364498138}, {"text": "Reddit data", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.9386826455593109}]}, {"text": "For each message, 10 responses from a model are used for evaluation.", "labels": [], "entities": []}, {"text": "In kg-CVAE, we follow ( ) and sample 10 times from the latent variable; in SC-Seq2Seq, we vary the specificity in {0.1, 0.2, . .", "labels": [], "entities": []}, {"text": ", 1}; and in both CT and GTMNES2S, we sample 10 times from the distributions.", "labels": [], "entities": [{"text": "CT", "start_pos": 18, "end_pos": 20, "type": "DATASET", "confidence": 0.8356858491897583}, {"text": "GTMNES2S", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8615830540657043}]}, {"text": "Top 1 response from beam search under each sampling or specificity setting are collected as the set for evaluation.", "labels": [], "entities": []}, {"text": "In terms of human evaluation, we recruit 3 native speakers to label top 1 responses of beam search from different models.", "labels": [], "entities": [{"text": "beam search", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.8012388944625854}]}, {"text": "Responses from all models for all the 1000 test messages in both data are pooled, randomly shuffled, and presented to each of the annotators.", "labels": [], "entities": []}, {"text": "The annotators judge the quality of the responses according to the following criteria: +2: the response is not only relevant and natural, but also informative and interesting; +1: the response can be used as a reply, but might not be informative enough (e.g.,\"Yes, I see\" etc.); 0: the response makes no sense, is irrelevant, or is grammatically broken.", "labels": [], "entities": []}, {"text": "Each response receives 3 labels.", "labels": [], "entities": []}, {"text": "Agreements among the annotators are measured by).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on relevance, diversity, and accuracy of one-to-many modeling. Numbers in bold mean that  improvement over the best baseline is statistically significant (t-test, p-value < 0.01).", "labels": [], "entities": [{"text": "relevance", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9663101434707642}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9991082549095154}]}, {"text": " Table 3: Results on accuracy of meta-word expression. Numbers in bold mean that improvement over the best  baseline is statistically significant (t-test, p-value < 0.01).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9983453750610352}]}, {"text": " Table 4: Results on the human evaluation. Ratios are  calculated by combining labels from the three judges.", "labels": [], "entities": []}, {"text": " Table 5: Contribution of different attributes.", "labels": [], "entities": []}]}