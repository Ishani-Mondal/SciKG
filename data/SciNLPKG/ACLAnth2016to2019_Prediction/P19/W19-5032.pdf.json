{"title": [{"text": "Embedding Biomedical Ontologies by Jointly Encoding Network Structure and Textual Node Descriptors", "labels": [], "entities": [{"text": "Embedding Biomedical Ontologies", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7960752050081888}]}], "abstractContent": [{"text": "Network Embedding (NE) methods, which map network nodes to low-dimensional feature vectors, have wide applications in network analysis and bioinformatics.", "labels": [], "entities": [{"text": "network analysis", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7591673135757446}]}, {"text": "Many existing NE methods rely only on network structure , overlooking other information associated with the nodes, e.g., text describing the nodes.", "labels": [], "entities": []}, {"text": "Recent attempts to combine the two sources of information only consider local network structure.", "labels": [], "entities": []}, {"text": "We extend NODE2VEC, a well-known NE method that considers broader network structure , to also consider textual node descriptors using recurrent neural encoders.", "labels": [], "entities": [{"text": "NODE2VEC", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.75904780626297}]}, {"text": "Our method is evaluated on link prediction in two networks derived from UMLS.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7571029365062714}, {"text": "UMLS", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9578059315681458}]}, {"text": "Experimental results demonstrate the effectiveness of the proposed approach compared to previous work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Network Embedding (NE) methods map each node of a network to an embedding, meaning a low-dimensional feature vector.", "labels": [], "entities": []}, {"text": "They are highly effective in network analysis tasks involving predictions over nodes and edges, for example link prediction (, and node classification (.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.7597511112689972}, {"text": "node classification", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.6998468339443207}]}, {"text": "Early NE methods, such as DEEPWALK (), LINE (), (,, leverage information from the network structure to produce embeddings that can reconstruct node neighborhoods.", "labels": [], "entities": [{"text": "LINE", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.939899742603302}]}, {"text": "The main advantage of these structure-oriented methods is that they encode the network context of the nodes, which can be very informative.", "labels": [], "entities": []}, {"text": "The downside is that they typically treat each node as anatomic unit, directly mapped to an embedding in a lookup table.", "labels": [], "entities": []}, {"text": "There is no attempt to model information other than the network structure, such as textual descriptors (labels) or other meta-data associated with the nodes.", "labels": [], "entities": []}, {"text": "More recent NE methods, e.g., CANE (),, produce embeddings by combining the network structure and the text associated with the nodes.", "labels": [], "entities": []}, {"text": "These contentoriented methods embed networks whose nodes are rich textual objects (often whole documents).", "labels": [], "entities": []}, {"text": "They aim to capture the compositionality and semantic similarities in the text, encoding them with deep learning methods.", "labels": [], "entities": []}, {"text": "This approach is illustrated in.", "labels": [], "entities": []}, {"text": "However, previous methods of this kind considered impoverished network contexts when embedding nodes, usually single-edge hops, as opposed to the non-local structure considered by most structure-oriented methods.", "labels": [], "entities": []}, {"text": "When embedding biomedical ontologies, it is important to exploit both wider network contexts and textual node descriptors.", "labels": [], "entities": []}, {"text": "The benefit of the latter is evident, for example, in 'acute leukemia' IS-A 'leukemia'.", "labels": [], "entities": []}, {"text": "To be able to predict (or reconstruct) this IS-A relation from the embeddings of 'acute leukemia' and 'leukemia' (and the word embeddings of their textual descriptors in), a NE method only needs to model the role of 'acute' as a modifier that can be included in the descriptor of anode (e.g., a disease node) to specify a sub-type.", "labels": [], "entities": []}, {"text": "This property can be learned (and encoded in the word embedding of 'acute') if several similar IS-A edges, with 'acute' being the only extra word in the descriptor of the sub-type, exist in the network.", "labels": [], "entities": []}, {"text": "This strategy would not however be successful in 'p53' (a protein) IS-A 'tumor suppressor', where no word in the descriptors frequently denotes sub-typing.", "labels": [], "entities": [{"text": "tumor suppressor", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.6672030389308929}]}, {"text": "Instead, by considering the broader network context of the nodes (i.e. longer paths that connect them), a NE method can detect that the two nodes have common neighbors and, hence, adjust the two node embeddings (and the word embeddings of their descriptors) to be close in the representation space, making it more likely to predict an IS-A relation between them.", "labels": [], "entities": []}, {"text": "We propose anew NE method that leverages the strengths of both structure and content-oriented approaches.", "labels": [], "entities": [{"text": "NE", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9461635947227478}]}, {"text": "To exploit wide network contexts, we follow NODE2VEC ( and generate random walks to construct the network neighborhood of each node.", "labels": [], "entities": [{"text": "NODE2VEC", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.5344319343566895}]}, {"text": "The SKIP-GRAM model ( is then used to learn node embeddings that successfully predict the nodes in each walk, from the node at the beginning of the walk.", "labels": [], "entities": []}, {"text": "To enrich the node embeddings with information from their textual descriptors, we replace the NODE2VEC look-up table with various architectures that operate on the word embeddings of the descriptors.", "labels": [], "entities": [{"text": "NODE2VEC look-up table", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.7342268228530884}]}, {"text": "These include simply averaging the word embeddings of a descriptor, and applying recurrent deep learning encoders.", "labels": [], "entities": []}, {"text": "The proposed method can be seen as an extension of NODE2VEC that incorporates textual node descriptors.", "labels": [], "entities": [{"text": "NODE2VEC", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8509535193443298}]}, {"text": "We evaluate several variants of the proposed method on link prediction, a standard evaluation task for NE methods.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.8057971596717834}]}, {"text": "We use two biomedical networks extracted from UMLS (), with PART-OF and IS-A relations, respectively.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.910871684551239}, {"text": "PART-OF", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9969364404678345}]}, {"text": "Our method outperforms several existing structure and content-oriented methods on both datasets.", "labels": [], "entities": []}, {"text": "We make our datasets and source code available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate the effectiveness of our proposed approach by conducting link prediction experiments on two biomedical datasets derived from UMLS.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.700627475976944}, {"text": "UMLS", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.9321734309196472}]}, {"text": "Furthermore, we devise anew approach of generating negative edges for the link prediction evaluation -beyond just random negativesthat makes the problem more difficult and aligns more with real-world use-cases.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7845972180366516}]}, {"text": "We also conduct a qualitative analysis, showing that the proposed framework does indeed leverage both the textual descriptors and the network structure.", "labels": [], "entities": []}, {"text": "We created our datasets from the UMLS ontology, which contains approx. 3.8 million biomedical concepts and 54 semantic relationships.", "labels": [], "entities": [{"text": "UMLS ontology", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8925256133079529}]}], "tableCaptions": [{"text": " Table 1: Statistics of the two datasets (IS-A, PART-OF).  The true positive and true negative edges are used in  the link prediction experiments.", "labels": [], "entities": [{"text": "PART-OF", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9687950015068054}, {"text": "link prediction", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.8102639317512512}]}, {"text": " Table 2: AUC scores (%) for the IS-A dataset. Best  scores per link predictor (CS, LR) shown in bold.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9673237204551697}, {"text": "IS-A dataset", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.8278544545173645}]}, {"text": " Table 3: AUC scores (%) for the PART-OF dataset. Best  scores per link predictor (CS, LR) shown in bold.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.977150559425354}, {"text": "PART-OF dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.893266886472702}]}, {"text": " Table 4: Examples of nodes whose embeddings are  closest (cosine similarity, Cos) to the embedding of  a target node in the PART-OF (top) and IS-A (bottom)  datasets. We also show the distances (number of edges,  Hops) between the nodes in the networks.", "labels": [], "entities": [{"text": "Cos", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.8723291754722595}]}, {"text": " Table 5: Examples of true positive edges, showing how  structure and textual descriptors affect node embed- dings. The first two edges are IS-A, the third one is  PART-OF. The NE methods used are BIGRU-MAX-RES- N2V (BN2V), CANE and NODE2VEC (N2V). We report  cosine similarities between node embeddings and the  distances between the nodes (number of edges, Hops)  in the networks after removing true positive edges.", "labels": [], "entities": [{"text": "PART-OF", "start_pos": 164, "end_pos": 171, "type": "METRIC", "confidence": 0.846885621547699}, {"text": "BIGRU-MAX-RES", "start_pos": 197, "end_pos": 210, "type": "METRIC", "confidence": 0.991486668586731}]}]}