{"title": [{"text": "Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks", "labels": [], "entities": [{"text": "Question Deduplication", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8187609314918518}, {"text": "Textual Entailment Tasks", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.767368366320928}]}], "abstractContent": [{"text": "Adversarial domain adaptation has been recently introduced as an effective technique for textual matching tasks, such as question dedu-plication (Shah et al., 2018).", "labels": [], "entities": [{"text": "Adversarial domain adaptation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7614227334658304}]}, {"text": "Here we investigate the use of gradient reversal on adversar-ial domain adaptation to explicitly learn both shared and unshared (domain specific) representations between two textual domains.", "labels": [], "entities": [{"text": "adversar-ial domain adaptation", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7412235935529073}]}, {"text": "In doing so, gradient reversal learns features that explicitly compensate for domain mismatch, while still distilling domain specific knowledge that can improve target domain accuracy.", "labels": [], "entities": [{"text": "gradient reversal", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7790179252624512}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9549368023872375}]}, {"text": "We evaluate reversing gradients for adversarial adaptation on multiple domains, and demonstrate that it significantly outperforms other methods on question deduplication as well as on recognizing textual entailment (RTE) tasks, achieving up to 7% absolute boost in base model accuracy on some datasets.", "labels": [], "entities": [{"text": "question deduplication", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.7357746362686157}, {"text": "recognizing textual entailment (RTE) tasks", "start_pos": 184, "end_pos": 226, "type": "TASK", "confidence": 0.7303065998213631}, {"text": "accuracy", "start_pos": 276, "end_pos": 284, "type": "METRIC", "confidence": 0.9707223773002625}]}], "introductionContent": [{"text": "Domain adaptation is a flexible machine learning approach that allows the transfer of category independent information between domains.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8159746825695038}]}, {"text": "Through domain adaptation we can leverage source task representations to bring the source and target distributions closer in a learned joint feature space.", "labels": [], "entities": []}, {"text": "In this paper we are focused only on semi-supervised domain adaptation -when knowledge from a large labeled dataset in a source domain can be somewhat transferred to help improve the same task on a target domain, which typically has a significantly smaller number of labels.", "labels": [], "entities": [{"text": "semi-supervised domain adaptation", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.6687492827574412}]}, {"text": "In particular, this paper focuses on domain adaptation for the detection of question duplicates in community question answering forums (, as well as for RTE tasks.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7307189702987671}, {"text": "detection of question duplicates in community question answering forums", "start_pos": 63, "end_pos": 134, "type": "TASK", "confidence": 0.8096178571383158}, {"text": "RTE tasks", "start_pos": 153, "end_pos": 162, "type": "TASK", "confidence": 0.8997768759727478}]}, {"text": "Generally speaking, the effectiveness of domain adaptation depends essentially on two factors: the similarity between source and target domains, and representation strategy to transfer the source domain knowledge.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7212905883789062}]}, {"text": "Long et al. showed transferring features across domains becomes increasingly difficult as domain discrepancy increases (, since the features learned by models gradually transition from general to highly domain specific as training progresses.", "labels": [], "entities": []}, {"text": "Recent domain adaptation strategies attempt to counter this issue by making certain features invariant across source and target domains using distribution matching or minimizing distance metrics between the representations (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.725795716047287}]}, {"text": "The idea of generating domain invariant features was further enhanced by the use of adversarial learning methods.", "labels": [], "entities": []}, {"text": "Recent work has advocated for tuning networks using a loss functions that reduce the mismatch between source and target data distributions.", "labels": [], "entities": []}, {"text": "Others have proposed a domain discriminator that maximizes the domain classification loss between source and target domains (.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.734584629535675}]}, {"text": "One particular limitation of these approaches is that they are restricted to using only the shared domain invariant features and hence can't benefit from target domain specific information.", "labels": [], "entities": []}, {"text": "Small amounts of labeled target domain data could in principle be used to fine-tune learned shared representations and improve the target task, however this could also lead to overfitting.", "labels": [], "entities": []}, {"text": "To address this issue, Qiu et al. used both shared domain invariant and domain specific features: while the shared features are learned by maximizing domain discriminator loss, the domain specific features are learned by jointly minimizing the task loss and the domain classification loss by domain specific discriminators (.", "labels": [], "entities": []}, {"text": "Similar ideas were put forth by Peng et al for cross-domain sentiment classification where they demonstrate the effectiveness of using both domain specific and domain invariant features ().", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.8397648731867472}]}, {"text": "Moreover, Bousmalis et al have made similar observations in domain adaptation for image classification and related vision tasks.", "labels": [], "entities": [{"text": "image classification", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.7366167902946472}]}, {"text": "All these studies follow similar approach of learning shared feature space by maximizing domain classification loss.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7013450413942337}]}, {"text": "In contrast, our work here enhances the ideas from from Qiu et al. by utilizing a Gradient Reversal Layer (GRL) ( to train the domain discriminator in a minimax game manner, and show that it results in significantly better transfer performance to multiple target domains.", "labels": [], "entities": [{"text": "Gradient Reversal Layer (GRL)", "start_pos": 82, "end_pos": 111, "type": "METRIC", "confidence": 0.8244731823603312}]}, {"text": "The use of gradient reversal layer is further advocated by works of and Fu et al () for removal of demographic attributes from text, and relation extraction from text, respectively.", "labels": [], "entities": [{"text": "relation extraction from text", "start_pos": 137, "end_pos": 166, "type": "TASK", "confidence": 0.8503245711326599}]}, {"text": "To the best of our knowledge, the use of Gradient Reversal in textual matching tasks, such as question deduplication and RTE, is novel and may trigger further applications of this approach in other language tasks.", "labels": [], "entities": [{"text": "textual matching tasks", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7208955387274424}, {"text": "question deduplication", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7961138486862183}, {"text": "RTE", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.8802237510681152}]}, {"text": "To summarize our contributions, (1) we propose a novel approach for adversarial domain adaptation that uses gradient reversal layers to discover shared representations between source and target domains on textual matching tasks, and elegantly combines domain specific and domain invariant shared features.", "labels": [], "entities": [{"text": "adversarial domain adaptation", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.6506355504194895}]}, {"text": "(2) We apply it to question deduplication tasks and empirically confirm that it outperforms all other strong baselines and feature sets on five different domains, with absolute accuracy gains of up to 4.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9264849424362183}]}, {"text": "(3) We further apply the same approach to two different textual entailment domains, where it again outperforms other baselines by as much as 7% absolute accuracy points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9332003593444824}]}], "datasetContent": [{"text": "For question duplicate detection, we use the Quora question pairs dataset as the source domain dataset and 5 datasets that are from different and diverse set of domains as our target domains.", "labels": [], "entities": [{"text": "question duplicate detection", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8808387517929077}, {"text": "Quora question pairs dataset", "start_pos": 45, "end_pos": 73, "type": "DATASET", "confidence": 0.8945162743330002}]}, {"text": "The Android, Mathematica, Programmers and Unix question datasets were used from the Stack Exchange dataset.", "labels": [], "entities": [{"text": "Programmers and Unix question datasets", "start_pos": 26, "end_pos": 64, "type": "DATASET", "confidence": 0.6256584525108337}, {"text": "Stack Exchange dataset", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.8304468790690104}]}, {"text": "We obtained the Tax Domain Qs from a popular forum for tax related question answers, which we plan to make public shortly.", "labels": [], "entities": [{"text": "Tax Domain Qs", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.8905278444290161}]}, {"text": "For RTE, the Stanford Natural Language Inference (SNLI) has been used as source domain, and for target domains we used The Guardian Headlines RTE and SICK (SICK, 2014) datasets.", "labels": [], "entities": [{"text": "RTE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9574043154716492}, {"text": "Guardian Headlines RTE and SICK (SICK, 2014) datasets", "start_pos": 123, "end_pos": 176, "type": "DATASET", "confidence": 0.6724365895444696}]}, {"text": "The size for all these datasets has been mentioned in in the (train/ validation/ test) format.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of Accuracy for different domain adaptation methods; Source domain for question duplicate  detection: Quora (240k/ 80k/ 80k), Source domain for RTE: SNLI (550k/ 10k/ 10k); SF: shared features, DSF:  domain specific features, maxLoss: maximizing domain discriminator loss, GRL: gradient reversal layer", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9835820198059082}, {"text": "question duplicate  detection", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.8510646025339762}, {"text": "RTE", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.70915687084198}, {"text": "GRL", "start_pos": 293, "end_pos": 296, "type": "METRIC", "confidence": 0.7686189413070679}]}]}