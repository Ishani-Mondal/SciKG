{"title": [{"text": "On the Word Alignment from Neural Machine Translation *", "labels": [], "entities": [{"text": "Word Alignment from Neural Machine Translation", "start_pos": 7, "end_pos": 53, "type": "TASK", "confidence": 0.7358571390310923}]}], "abstractContent": [{"text": "Prior researches suggest that neural machine translation (NMT) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some NMT models.", "labels": [], "entities": [{"text": "neural machine translation (NMT) captures word alignment", "start_pos": 30, "end_pos": 86, "type": "TASK", "confidence": 0.8294509450594584}, {"text": "word alignment", "start_pos": 183, "end_pos": 197, "type": "TASK", "confidence": 0.7203964740037918}]}, {"text": "This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7853916883468628}]}, {"text": "Experiments show that both methods induce much better word alignment than attention.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7154534608125687}]}, {"text": "This paper further visualizes the translation through the word alignment induced by NMT.", "labels": [], "entities": []}, {"text": "In particular , it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences, and word alignment tries to model the semantic equivalence between a pair of source and target words.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7807605564594269}, {"text": "word alignment", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.7660440802574158}]}, {"text": "As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7392562478780746}, {"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7541022896766663}, {"text": "statistical machine translation (SMT)", "start_pos": 150, "end_pos": 187, "type": "TASK", "confidence": 0.7743333975474039}, {"text": "word alignment", "start_pos": 195, "end_pos": 209, "type": "TASK", "confidence": 0.6976177841424942}, {"text": "SMT", "start_pos": 226, "end_pos": 229, "type": "TASK", "confidence": 0.9818137884140015}, {"text": "accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9968000650405884}]}, {"text": "In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.863273044427236}, {"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7798195481300354}, {"text": "word alignment", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.7263138741254807}]}, {"text": "Prior researches * Work done while X.", "labels": [], "entities": []}, {"text": "Li interning at Tencent AI Lab.", "labels": [], "entities": []}, {"text": "L. Liu is the corresponding author.", "labels": [], "entities": []}, {"text": "observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7735264897346497}]}, {"text": "Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as TRANSFORMER (, as demonstrated in our experiments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.7007834166288376}, {"text": "TRANSFORMER", "start_pos": 149, "end_pos": 160, "type": "METRIC", "confidence": 0.6680846810340881}]}, {"text": "In this paper, we propose two methods to induce word alignment from general NMT models and answer a fundamental question that how much word alignment NMT models can learn ( \u00a7 3).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7899152040481567}]}, {"text": "The first method explicitly builds a word alignment model between a pair of source and target word representations encoded by NMT models, and then it learns additional parameters for this word alignment model with the supervision from an external aligner similar to and.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7280828952789307}, {"text": "word alignment", "start_pos": 188, "end_pos": 202, "type": "TASK", "confidence": 0.7002715468406677}]}, {"text": "The second method is more intuitive and flexible: it is parameter-free and thus does not need retraining and external aligner.", "labels": [], "entities": []}, {"text": "Its key idea is to measure the prediction difference of a target word if a source word is removed, inspired by and.", "labels": [], "entities": []}, {"text": "Experiments on an advanced NMT model show that both methods achieve much better word alignment than the method by attention ( \u00a7 4.1).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.731767863035202}]}, {"text": "In addition, our experiments demonstrate that NMT captures good word alignment for those words mostly contributed from source (CFS), while their word alignment is much worse for those words mostly contributed from target (CFT).", "labels": [], "entities": []}, {"text": "This finding offers a reason why advanced NMT models delivering excellent translation capture worse word alignment than statistical aligners in SMT, which was observed in prior researches yet without deep explanation (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.9841403961181641}]}, {"text": "Furthermore, we understand and interpret NMT from the viewpoint of word alignment induced from NMT ( \u00a7 4.2).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7255494743585587}]}, {"text": "Unlike existing researches on interpreting NMT by accessing few examples as case study, we aim to provide quantitatively analysis for interpreting NMT by accessing many testing examples, which makes our findings more general.", "labels": [], "entities": [{"text": "interpreting NMT", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.8474873900413513}, {"text": "interpreting NMT", "start_pos": 134, "end_pos": 150, "type": "TASK", "confidence": 0.8450540900230408}]}, {"text": "To this end, we firstly compare the effects of both approaches to interpreting NMT and find the prediction difference is better for understanding NMT.", "labels": [], "entities": [{"text": "interpreting NMT", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.7912794947624207}]}, {"text": "Consequently, we propose to quantitatively analyze the translation errors by using alignment from prediction difference.", "labels": [], "entities": []}, {"text": "Since it is far from trivial to measure the translation errors at the word level, we design experiments by using two metrics to detect translation errors.", "labels": [], "entities": []}, {"text": "Our empirical results consistently show that wrong alignment is more likely to induce the translation errors meanwhile right alignment favors to encourage the translation quality.", "labels": [], "entities": []}, {"text": "Our analysis further suggest that word alignment errors for CFS words are responsible for translation errors in some extent.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7138225883245468}]}, {"text": "This paper makes the two-fold contributions: \u2022 It systematically studies word alignment from NMT and proposes two approaches to induce word alignment which are agnostic to specific NMT models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.7936422228813171}, {"text": "word alignment", "start_pos": 135, "end_pos": 149, "type": "TASK", "confidence": 0.7490781545639038}]}, {"text": "\u2022 It understands NMT from the viewpoint of word alignment and investigates the effect of alignment errors on translation errors via quantitative analysis over many testing examples.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7470381855964661}]}], "datasetContent": [{"text": "In this section, we conduct extensive experiments on ZH\u21d2EN and DE\u21d2EN translation tasks to evaluate different methods for word alignment induced from the NMT model and compare them with a statistical alignment model FAST ALIGN ().", "labels": [], "entities": [{"text": "DE\u21d2EN translation tasks", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.7609034419059754}, {"text": "word alignment", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.7875199913978577}, {"text": "FAST ALIGN", "start_pos": 215, "end_pos": 225, "type": "METRIC", "confidence": 0.6752241551876068}]}, {"text": "Then, we use the induced word alignment to understand translation errors both qualitatively and quantitatively.", "labels": [], "entities": []}, {"text": "The alignment performance is evaluated by alignment error rate (AER).", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 42, "end_pos": 68, "type": "METRIC", "confidence": 0.9334603250026703}]}, {"text": "The proposed methods are implemented on top of TRANS-FORMER ( which is a state-ofthe-art NMT system.", "labels": [], "entities": [{"text": "TRANS-FORMER", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.4822005033493042}]}, {"text": "We report AER on NIST05 test set and RWTH data, whose reference alignment was manually annotated by experts (.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9979842901229858}, {"text": "NIST05 test set", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.9769797722498575}, {"text": "RWTH data", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.7719067633152008}]}, {"text": "More details on data and training these systems are described in Appendix A.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.6385376453399658}]}], "tableCaptions": [{"text": " Table 1: AER of the proposed methods.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9988784193992615}]}, {"text": " Table 2: EAM on translation models with different  number of layer.", "labels": [], "entities": [{"text": "EAM", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9154601693153381}]}, {"text": " Table 3: Comparison between sampling and deterministic methods for prediction difference.", "labels": [], "entities": [{"text": "prediction difference", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.9219908714294434}]}, {"text": " Table 4: AER of CFS and CFT words.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9989745616912842}, {"text": "CFS", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.6628657579421997}]}, {"text": " Table 5: Alignment of Real Translation.", "labels": [], "entities": [{"text": "Alignment of Real Translation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.867760181427002}]}, {"text": " Table 6: Forced decoding translation error rate for  CFS/CFT words with right/wrong alignment.", "labels": [], "entities": [{"text": "decoding translation error rate", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.5730557441711426}, {"text": "CFS/CFT words", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.5801962316036224}]}, {"text": " Table 7: Real decoding translation error rate for  CFS/CFT words with right/wrong alignment.", "labels": [], "entities": [{"text": "CFS/CFT words", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.5809323787689209}]}]}