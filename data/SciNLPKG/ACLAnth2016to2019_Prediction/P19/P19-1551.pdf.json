{"title": [{"text": "Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming", "labels": [], "entities": [{"text": "Differentiable Dynamic Programming", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.7107868989308676}]}], "abstractContent": [{"text": "We treat projective dependency trees as latent variables in our probabilistic model and induce them in such away as to be beneficial fora downstream task, without relying on any direct tree supervision.", "labels": [], "entities": []}, {"text": "Our approach relies on Gum-bel perturbations and differentiable dynamic programming.", "labels": [], "entities": []}, {"text": "Unlike previous approaches to latent tree learning, we stochastically sample global structures and our parser is fully differ-entiable.", "labels": [], "entities": []}, {"text": "We illustrate its effectiveness on sentiment analysis and natural language inference tasks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9780974090099335}]}, {"text": "We also study its properties on a synthetic structure induction task.", "labels": [], "entities": [{"text": "synthetic structure induction task", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.7478065490722656}]}, {"text": "Ablation studies emphasize the importance of both stochas-ticity and constraining latent structures to be projective trees.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discrete structures are ubiquitous in the study of natural languages, for example in morphology, syntax and discourse analysis.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7641967535018921}]}, {"text": "In natural language processing, they are often used to inject linguistic prior knowledge into statistical models.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6593822340170542}]}, {"text": "For examples, syntactic structures have been shown beneficial in question answering (, sentiment analysis, machine translation ( and relation extraction (), among others.", "labels": [], "entities": [{"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8685871362686157}, {"text": "sentiment analysis", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.9595292806625366}, {"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8359425365924835}, {"text": "relation extraction", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.8296667039394379}]}, {"text": "However, linguistic tools producing these structured representations (e.g., syntactic parsers) are not available for many languages and not robust when applied outside of the domain they were trained on ().", "labels": [], "entities": []}, {"text": "Moreover, linguistic structures do not always seem suitable in downstream applications, with simpler alternatives sometimes yielding better performance (.", "labels": [], "entities": []}, {"text": "Indeed, a parallel line of work focused on inducing task-specific structured representations of language (.", "labels": [], "entities": []}, {"text": "In these approaches, no syntactic or semantic annotation is needed for training: representation is induced from scratch in an end-to-end fashion, in such away as to benefit a given downstream task.", "labels": [], "entities": []}, {"text": "In other words, these approaches provide an inductive bias specifying that (hierarchical) structures are appropriate for representing a natural language, but do not make any further assumptions regarding what the structures represent.", "labels": [], "entities": []}, {"text": "Structures induced in this way, though useful for the task, tend not to resemble any accepted syntactic or semantic formalisms ().", "labels": [], "entities": []}, {"text": "Our approach falls under this category.", "labels": [], "entities": []}, {"text": "In our method, projective dependency trees (see for examples) are treated as latent variables within a probabilistic model.", "labels": [], "entities": []}, {"text": "We rely on differentiable dynamic programming which allows for efficient sampling of dependency trees.", "labels": [], "entities": []}, {"text": "Intuitively, sampling a tree involves stochastically perturbing dependency weights and then running a relaxed form of the Eisner dynamic programming algortihm.", "labels": [], "entities": []}, {"text": "A sampled tree (or its continuous relaxation) can then be straightforwardly integrated in a neural sentence encoder fora target task using graph convolutional networks).", "labels": [], "entities": []}, {"text": "The entire model, including the parser and GCN parameters, are estimated jointly while minimizing the loss for the target task.", "labels": [], "entities": []}, {"text": "What distinguishes us from previous work is that we stochastically sample global structures and do it in a differentiable fashion.", "labels": [], "entities": []}, {"text": "For example, the structured attention method ( does not sample entire trees but rather computes arc marginals, and hence does not faithfully represent higher-order statistics.", "labels": [], "entities": []}, {"text": "Much of other previous work relies either on reinforce-ment learning) or does not treat the latent structure as a random variable (.", "labels": [], "entities": []}, {"text": "marginalizes over latent structures, however, this necessitates strong sparsity assumptions on the posterior distributions which may inject undesirable biases in the model.", "labels": [], "entities": []}, {"text": "Overall, differential dynamic programming has not been actively studied in the task-specific tree induction context.", "labels": [], "entities": [{"text": "differential dynamic programming", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.8826555411020914}]}, {"text": "Most previous work also focused on constituent trees rather than dependency ones.", "labels": [], "entities": []}, {"text": "We study properties of our approach on a synthetic structure induction task and experiment on sentiment classification) and natural language inference.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.8696497976779938}, {"text": "natural language inference", "start_pos": 124, "end_pos": 150, "type": "TASK", "confidence": 0.637695719798406}]}, {"text": "Our experiments confirm that the structural bias encoded in our approach is beneficial.", "labels": [], "entities": []}, {"text": "For example, our approach achieves a 4.9% improvement on multi-genre natural language inference (MultiNLI) over a structure-agnostic baseline.", "labels": [], "entities": []}, {"text": "We show that stochastisticity and higher-order statistics given by the global inference are both important.", "labels": [], "entities": []}, {"text": "In ablation experiments, we also observe that forcing the structures to be projective dependency trees rather than permitting any general graphs yields substantial improvements without sacrificing execution time.", "labels": [], "entities": []}, {"text": "This confirms that our inductive bias is useful, at least in the context of the considered downstream applications.", "labels": [], "entities": []}, {"text": "Our main contributions can be summarized as follows: 1.", "labels": [], "entities": []}, {"text": "we show that a latent tree model can be estimated by drawing global approximate samples via Gumbel perturbation and differentiable dynamic programming; 2.", "labels": [], "entities": []}, {"text": "we demonstrate that constraining the structures to be projective dependency trees is beneficial; 3.", "labels": [], "entities": []}, {"text": "we show the effectiveness of our approach on two standard tasks used in latent structure modelling and on a synthetic dataset.", "labels": [], "entities": [{"text": "latent structure modelling", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.6278081635634104}]}], "datasetContent": [{"text": "We first experiment on a toy task.", "labels": [], "entities": []}, {"text": "The task is designed in such away that there exists a simple projective dependency grammar which turns it into a trivial problem.", "labels": [], "entities": []}, {"text": "We can therefore perform thorough analysis of the latent tree induction method.", "labels": [], "entities": [{"text": "latent tree induction", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.583247572183609}]}, {"text": "The ListOps dataset has been built specifically to test structured latent variable models.", "labels": [], "entities": [{"text": "ListOps dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9003957211971283}]}, {"text": "The task is to compute the result of a mathematical expression written in prefix notation.", "labels": [], "entities": []}, {"text": "It has been shown easy fora Tree-LSTM that follows the gold underlying structure but most latent variable models fail to induce it.", "labels": [], "entities": []}, {"text": "Unfortunately, the task is not compatible with our neural network because it requires propagation of information from the leafs to the root node, which is not possible fora GCN with a fixed number of layers.", "labels": [], "entities": []}, {"text": "Instead, we transform the computation problem into a tagging problem: the task is to tag the valency of operations, i.e. the number of operands they have.", "labels": [], "entities": []}, {"text": "We transform the original unlabelled binary phrase-structure into a dependency structure by following a simple head-percolation table: the head of a phrase is always the head of its left argument.", "labels": [], "entities": []}, {"text": "The resulting dependencies represent two kinds of relation: operand to argument and operand to closing parenthesis.", "labels": [], "entities": []}, {"text": "Therefore, this task is trivial fora GCN trained with gold dependencies: it simply needs to count the number of outgoing arcs minus one (for operation nodes).", "labels": [], "entities": []}, {"text": "In practice, we observe 100% tagging accuracy with the gold dependencies.", "labels": [], "entities": [{"text": "tagging", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9265432953834534}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.978960394859314}]}, {"text": "We evaluate our method on two real-world problems: a sentence comparison task (natural language inference, see Section 6.1) and a sentence classification problem (sentiment classification, see Section 6.2).", "labels": [], "entities": [{"text": "sentence comparison task", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7575604915618896}, {"text": "sentence classification", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.7213073819875717}, {"text": "sentiment classification", "start_pos": 163, "end_pos": 187, "type": "TASK", "confidence": 0.7673813700675964}]}, {"text": "Besides using the differentiable dynamic programming method, our approach also differs from previous work in that we use GCNs followed by a pooling operation, whereas most previous work used Tree-LSTMs.", "labels": [], "entities": []}, {"text": "Unlike Tree-LSTMs, GCNs are trivial to parallelize over batches on GPU.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ListOps results: tagging accuracy (Acc.) and  attachment score for the latent tree grammar (Att.).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9552359580993652}, {"text": "Acc.", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9578262567520142}, {"text": "attachment score", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.9696212708950043}, {"text": "Att.", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9535514712333679}]}, {"text": " Table 2: SNLI results and number of network param- eters (discarding word embeddings). Stars indicate la- tent tree models.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.8423349857330322}]}, {"text": " Table 3: (a) SST results. Stars indicate latent tree models. (b) MultiNLI results. Stars indicate latent tree models.  (c) Ablation tests on MultiNLI (results on the matched and mismatched development sets).", "labels": [], "entities": [{"text": "SST", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9761633276939392}, {"text": "Ablation", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.989452064037323}]}]}