{"title": [{"text": "Simple Unsupervised Summarization by Contextual Matching", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose an unsupervised method for sentence summarization using only language modeling.", "labels": [], "entities": [{"text": "sentence summarization", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7276621162891388}]}, {"text": "The approach employs two language models, one that is generic (i.e. pre-trained), and the other that is specific to the target domain.", "labels": [], "entities": []}, {"text": "We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency.", "labels": [], "entities": []}, {"text": "Experiments on both abstractive and extractive sentence sum-marization data sets show promising results of our method without being exposed to any paired data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic text summarization is the process of formulating a shorter output text than the original while capturing its core meaning.", "labels": [], "entities": [{"text": "Automatic text summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6087519923845927}]}, {"text": "We study the problem of unsupervised sentence summarization with no paired examples.", "labels": [], "entities": [{"text": "sentence summarization", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6543362140655518}]}, {"text": "While datadriven approaches have achieved great success based on various powerful learning frameworks such as sequence-to-sequence models with attention (, variational auto-encoders, and reinforcement learning (, they usually require a large amount of parallel data for supervision to do well.", "labels": [], "entities": []}, {"text": "In comparison, the unsupervised approach reduces the human effort for collecting and annotating large amount of paired training data.", "labels": [], "entities": []}, {"text": "Recently researchers have begun to study the unsupervised sentence summarization tasks.", "labels": [], "entities": [{"text": "sentence summarization tasks", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.748767097791036}]}, {"text": "These methods all use parameterized unsupervised learning methods to induce a latent variable model: for example uses a length controlled variational autoencoder, Fevry and Phang (2018) use a denoising autoencoder but only for extractive summarization, and apply a reinforcement learning procedure combined with GANs, which takes a further step to the goal of using language as latent representations for semisupervised learning.", "labels": [], "entities": []}, {"text": "This work instead proposes a simple approach to this task that does not require any joint training.", "labels": [], "entities": []}, {"text": "We utilize a generic pretrained language model to enforce contextual matching between sentence prefixes.", "labels": [], "entities": []}, {"text": "We then use a smoothed problem specific target language model to guide the fluency of the generation process.", "labels": [], "entities": []}, {"text": "We combine these two models in a product-of-experts objective.", "labels": [], "entities": []}, {"text": "This approach does not require any task-specific training, yet experiments show results on par with or better than the best unsupervised systems while producing qualitatively fluent outputs.", "labels": [], "entities": []}, {"text": "The key aspect of this technique is the use of a pretrained language model for unsupervised contextual matching, i.e. unsupervised paraphrasing.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the contextual matching model's similarity function S, we adopt the forward language model of ELMo ( to encode tokens to corresponding hidden states in the sequence, resulting in a three-layer representation each of dimension 512.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.766453206539154}]}, {"text": "The bottom layer is a fixed character embedding layer, and the above two layers are LSTMs associated with the generic unsupervised language model trained on a large amount of text data.", "labels": [], "entities": []}, {"text": "We explicitly manage the ELMo hidden states to allow our model to generate contextual embeddings sequentially for efficient beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.7871730625629425}]}, {"text": "The fluency language model component lm is task specific, and pretrained on a corpus of summarizations.", "labels": [], "entities": []}, {"text": "We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024.", "labels": [], "entities": []}, {"text": "It is trained using dropout rate 0.5 and SGD combined with gradient clipping.", "labels": [], "entities": [{"text": "dropout rate 0.5", "start_pos": 20, "end_pos": 36, "type": "METRIC", "confidence": 0.8538011908531189}, {"text": "SGD", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9954258799552917}]}, {"text": "We test our method on both abstractive and extractive sentence summarization tasks.", "labels": [], "entities": [{"text": "extractive sentence summarization", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.6376630862553915}]}, {"text": "For abstractive summarization, we use the English Gigaword data set pre-processed by.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7511614561080933}, {"text": "English Gigaword data set", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.7799288481473923}]}, {"text": "We train p fm using its 3.8 million headlines in the training set, and generate summaries for the input in test set.", "labels": [], "entities": []}, {"text": "For extractive summarization, we use the Google data set from Filippova and Altun (2013).", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.825413316488266}, {"text": "Google data set from Filippova and Altun (2013)", "start_pos": 41, "end_pos": 88, "type": "DATASET", "confidence": 0.9586942732334137}]}, {"text": "We train p fm on 200K compressed sentences in the training set and test on the first 1000 pairs of evaluation set consistent with previous works.", "labels": [], "entities": []}, {"text": "For generation, we set \u03bb = 0.11 in (1) and beam size to 10.", "labels": [], "entities": [{"text": "generation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9630207419395447}]}, {"text": "Each source sentence is tokenized and lowercased, with periods deleted and a special end of sentence token appended.", "labels": [], "entities": []}, {"text": "In abstractive summarization, we use K = 6 in the candidate list and use the fixed embeddings at the bottom layer of ELMo language model for similarity.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.6304009556770325}]}, {"text": "Larger K has only small impact on performance but makes the generation more expensive.", "labels": [], "entities": []}, {"text": "The hyper-parameter \u03b1 for length penalty ranges from -0.1 to 0.1 for different tasks, mainly for desired output length as we find ROUGE scores are not sensitive to it.", "labels": [], "entities": [{"text": "hyper-parameter \u03b1", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9487529397010803}, {"text": "ROUGE", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.935908317565918}]}, {"text": "We use concatenation of all ELMo layers as default in p cm .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results of abstractive summa- rization on Gigaword test set with ROUGE metric. The  top section is prefix baselines, the second section is re- cent unsupervised methods and ours, the third section  is state-of-the-art supervised method along with our  implementation of a seq-to-seq model with attention,  and the bottom section is our model's oracle perfor- mance. Wang and Lee (2018) is by author correspon- dence (scores differ because of evaluation setup). For  another unsupervised work Fevry and Phang (2018),  we attempted to replicate on our test set, but were un- able to obtain results better than the baselines.", "labels": [], "entities": [{"text": "Gigaword test set", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9232826232910156}]}, {"text": " Table 2: Experimental results of extractive summariza- tion on Google data set. F1 is the token overlapping  score, and CR is the compression rate. F&A is an unsu- pervised baseline used in", "labels": [], "entities": [{"text": "Google data set", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9596828023592631}, {"text": "F1", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9982640147209167}, {"text": "CR", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9844323396682739}, {"text": "compression rate", "start_pos": 131, "end_pos": 147, "type": "METRIC", "confidence": 0.9457425475120544}, {"text": "F&A", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.5617972413698832}]}]}