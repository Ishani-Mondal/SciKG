{"title": [{"text": "Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7297366658846537}]}], "abstractContent": [{"text": "Dialogue contexts are proven helpful in the spoken language understanding (SLU) system and they are typically encoded with explicit memory representations.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.8246301015218099}]}, {"text": "However, most of the previous models learn the context memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew dialogue logistic inference (DLI) task to consolidate the context memory jointly with SLU in the multi-task framework.", "labels": [], "entities": []}, {"text": "DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model.", "labels": [], "entities": [{"text": "sorting a shuffled dialogue session", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.7601252675056458}]}, {"text": "Our experimental results show that various popular contextual SLU models can benefit from our approach, and improvements are quite impressive, especially in slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 157, "end_pos": 169, "type": "TASK", "confidence": 0.9207361042499542}]}], "introductionContent": [{"text": "Spoken language understanding (SLU) is a key technique in today's conversational systems such as Apple Siri, Amazon Alexa, and Microsoft Cortana.", "labels": [], "entities": [{"text": "Spoken language understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8923464119434357}]}, {"text": "A typical pipeline of SLU includes domain classification, intent detection, and slot filling), to parse user utterances into semantic frames.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7307512015104294}, {"text": "intent detection", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7086989730596542}, {"text": "slot filling", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.6693337857723236}, {"text": "parse user utterances into semantic frames", "start_pos": 98, "end_pos": 140, "type": "TASK", "confidence": 0.8369322319825491}]}, {"text": "Example semantic frames ) are shown in fora restaurant reservation.", "labels": [], "entities": []}, {"text": "Traditionally, domain classification and intent detection are treated as classification tasks with popular classifiers such as support vector machine and deep neural network ().", "labels": [], "entities": [{"text": "domain classification", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.798340916633606}, {"text": "intent detection", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7538728415966034}]}, {"text": "They can also be combined into one task if there are not many intents of each domain(.", "labels": [], "entities": []}, {"text": "Slot filling task is usually treated as a sequence labeling task.", "labels": [], "entities": [{"text": "Slot filling task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9252725044886271}, {"text": "sequence labeling task", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7302545507748922}]}, {"text": "Popular approaches for slot filling include conditional random fields (CRF) and recurrent neural network (RNN) B-time Yao et al., 2014).", "labels": [], "entities": [{"text": "slot filling", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9233020544052124}]}, {"text": "Considering that pipeline approaches usually suffer from error propagation, the joint model for slot filling and intent detection has been proposed to improve sentence-level semantics via mutual enhancement between two tasks (, which is a direction we follow.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.7714453637599945}, {"text": "intent detection", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.6974922269582748}]}, {"text": "To create a more effective SLU system, the contextual information has been shown useful, as natural language utterances are often ambiguous.", "labels": [], "entities": [{"text": "SLU", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9639468789100647}]}, {"text": "For example, the number 6 of utterance u 2 in may refer to either B-time or B-people without considering the context.", "labels": [], "entities": []}, {"text": "Popular contextual SLU models) exploit the dialogue history with the memory network), which covers all three main stages of memory process: encoding (write), storage (save) and retrieval (read).", "labels": [], "entities": []}, {"text": "With such a memory mechanism, SLU model can retrieve context knowledge to reduce the ambiguity of the current utterance, contributing to a stronger SLU model.", "labels": [], "entities": []}, {"text": "However, the memory consolidation, a well-recognized operation for maintaining and updating memory in cognitive psy- Figure 2: Architecture of our proposed contextual SLU with memory consolidation.", "labels": [], "entities": [{"text": "memory consolidation", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.6707591563463211}, {"text": "memory consolidation", "start_pos": 176, "end_pos": 196, "type": "TASK", "confidence": 0.6838338524103165}]}, {"text": "chology, is underestimated in previous models.", "labels": [], "entities": []}, {"text": "They update memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited.", "labels": [], "entities": []}, {"text": "In this paper, we propose a multi-task learning approach for multi-turn SLU by consolidating context memory with an additional task: dialogue logistic inference (DLI), defined as sorting a shuffled dialogue session into its original logical order.", "labels": [], "entities": [{"text": "multi-turn SLU", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.5938200652599335}]}, {"text": "DLI can be trained with contextual SLU jointly if utterances are sorted one by one: selecting the right utterance from remaining candidates based on previously sorted context.", "labels": [], "entities": []}, {"text": "In other words, given a response and its context, the DLI task requires our model to infer whether the response is the right one that matches the dialogue context, similar to the next sentence prediction task.", "labels": [], "entities": [{"text": "sentence prediction task", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.7909117639064789}]}, {"text": "We conduct our experiments on the public multi-turn dialogue dataset KVRET (, with two popular memory based contextual SLU models.", "labels": [], "entities": [{"text": "multi-turn dialogue dataset KVRET", "start_pos": 41, "end_pos": 74, "type": "DATASET", "confidence": 0.6147280484437943}]}, {"text": "According to our experimental results, noticeable improvements are observed, especially on slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 91, "end_pos": 103, "type": "TASK", "confidence": 0.9108532965183258}]}], "datasetContent": [{"text": "In this section, we first introduce datasets we used, then present our experimental setup and results on these datasets.", "labels": [], "entities": []}, {"text": "KVRET () is a multi-turn task-oriented dialogue dataset for an in-car assistant.", "labels": [], "entities": [{"text": "KVRET", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8343966007232666}]}, {"text": "This dataset was collected with the Wizardof-Oz scheme  We conduct extensive experiments on intent detection and slot filling with datasets described above.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.8713542520999908}, {"text": "slot filling", "start_pos": 113, "end_pos": 125, "type": "TASK", "confidence": 0.8387834131717682}]}, {"text": "The domain classification is skipped because intents and domains are the same for KVRET.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6838845163583755}, {"text": "KVRET", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.939346432685852}]}, {"text": "For training model, our training batch size is 64, and we train all models with Adam optimizer with default parameters).", "labels": [], "entities": []}, {"text": "For each model, we conduct training up to 30 epochs with five epochs' early stop on validation loss.", "labels": [], "entities": []}, {"text": "The word embedding size is 100, and the hidden size of all RNN layer is 64.", "labels": [], "entities": []}, {"text": "The \u03bb is set to be 0.3.", "labels": [], "entities": []}, {"text": "The dropout rate is set to be 0.3 to avoid over-fitting.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9649845063686371}]}], "tableCaptions": [{"text": " Table 1: Detailed information of KVRET and  KVRET* datasets, including train/dev/test size and av- erage turns per conversation.", "labels": [], "entities": [{"text": "KVRET", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9236412048339844}, {"text": "KVRET* datasets", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9035013119379679}, {"text": "av- erage turns", "start_pos": 96, "end_pos": 111, "type": "METRIC", "confidence": 0.7926757484674454}]}, {"text": " Table 2: SLU results on original KVRET and multi-domain KVRET*, including accuracy of intent detection and  average precision, recall and F1 score of slot filling.", "labels": [], "entities": [{"text": "KVRET", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8586244583129883}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9992238283157349}, {"text": "intent detection", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.6733949780464172}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.8713600635528564}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9997121691703796}, {"text": "F1 score", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9798784852027893}, {"text": "slot filling", "start_pos": 151, "end_pos": 163, "type": "TASK", "confidence": 0.7423343658447266}]}]}