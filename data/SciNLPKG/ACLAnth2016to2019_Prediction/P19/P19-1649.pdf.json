{"title": [], "abstractContent": [{"text": "Recent advances in sequence modeling have highlighted the strengths of the transformer architecture , especially in achieving state-of-the-art machine translation results.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7832109928131104}, {"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7228461802005768}]}, {"text": "However, depending on the upstream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.788039892911911}, {"text": "word segmentation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7913884520530701}]}, {"text": "The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input.", "labels": [], "entities": []}, {"text": "We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (ASR) which contains multiple paths and posterior scores.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7352803349494934}, {"text": "automatic speech recognition (ASR)", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.7675204575061798}]}, {"text": "To leverage the extra information from the lattice structure, we develop a novel control-lable lattice attention mechanism to obtain latent representations.", "labels": [], "entities": []}, {"text": "On the LDC Spanish-English speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM.", "labels": [], "entities": [{"text": "LDC Spanish-English speech translation corpus", "start_pos": 7, "end_pos": 52, "type": "DATASET", "confidence": 0.7286678671836853}]}, {"text": "Additionally, we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmen-tations.", "labels": [], "entities": [{"text": "WMT 2017 Chinese-English translation task", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.7322442889213562}]}, {"text": "In this task, we also observe the improvements over strong baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transformer based encoder-decoder framework () for Neural Machine Translation (NMT) has currently become the state-ofthe-art in many translation tasks, significantly improving translation quality in text () as well as in speech).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.8242480059464773}]}, {"text": "Most NMT systems fall into the category of Sequence-to-Sequence (Seq2Seq) model), because both the input and * indicates equal contribution.", "labels": [], "entities": []}, {"text": "output consist of sequential tokens.", "labels": [], "entities": []}, {"text": "Therefore, inmost neural speech translation, such as that of, the input to the translation system is usually the 1-best hypothesis from the ASR instead of the word lattice output with its corresponding probability scores.", "labels": [], "entities": []}, {"text": "How to consume word lattice rather than sequential input has been substantially researched in several natural language processing (NLP) tasks, such as language modeling), Chinese Named Entity Recognition (NER) (, and NMT (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7348804771900177}, {"text": "Chinese Named Entity Recognition (NER)", "start_pos": 171, "end_pos": 209, "type": "TASK", "confidence": 0.6939094705241067}]}, {"text": "Additionally, some pioneering works () demonstrated the potential improvements in speech translation by leveraging the additional information and uncertainty of the packed lattice structure produced by ASR acoustic model.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.751600831747055}, {"text": "ASR acoustic", "start_pos": 202, "end_pos": 214, "type": "TASK", "confidence": 0.8339549005031586}]}, {"text": "Efforts have since continued to push the boundaries of long short-term memory (LSTM)) models.", "labels": [], "entities": []}, {"text": "More precisely, most previous works are inline with the existing method Tree-LSTMs (, adapting to task-specific variant LatticeLSTMs that can successfully handle lattices and robustly establish better performance than the original models.", "labels": [], "entities": []}, {"text": "However, the inherently sequential nature still remains in Lattice-LSTMs due to the topological representation of the lattice graph, precluding long-path dependencies ( and parallelization within training examples that are the fundamental constraint of LSTMs.", "labels": [], "entities": []}, {"text": "In this work, we introduce a generalization of the standard transformer architecture to accept lattice-structured network topologies.", "labels": [], "entities": []}, {"text": "The standard transformer is a transduction model relying entirely on attention modules to compute latent representations, e.g., the self-attention requires to calculate the intra-attention of every two tokens for each sequence example.", "labels": [], "entities": []}, {"text": "Latest works such as () empirically find that transformer can outperform LSTMs by a large margin, and the success is mainly attributed to selfattention.", "labels": [], "entities": []}, {"text": "In our lattice transformer, we propose a lattice relative positional attention mechanism that can incorporate the probability scores of ASR word lattices.", "labels": [], "entities": [{"text": "ASR word lattices", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.7991005976994833}]}, {"text": "The major difference with the selfattention in transformer encoder is illustrated in.", "labels": [], "entities": []}, {"text": "We first borrow the idea from the relative positional embedding) to maximally encode the information of the lattice graph into its corresponding relative positional matrix.", "labels": [], "entities": []}, {"text": "This design essentially does not allow a token to pay attention to any token that has not appeared in a shared path.", "labels": [], "entities": []}, {"text": "Secondly, the attention weights depend not only on the query and key representations in the standard attention module, but also on the marginal / forward / backward probability scores) derived from the upstream systems (such as ASR).", "labels": [], "entities": []}, {"text": "Instead of 1-best hypothesis alone (though it is based on forward scores), the additional probability scores have rich information about the distribution of each path ().", "labels": [], "entities": []}, {"text": "It is in principle possible to use them, for example in attention weights reweighing, to increase the uncertainty of the attention for other alternative tokens.", "labels": [], "entities": []}, {"text": "Our lattice attention is controllable and flexible enough for the utilization of each score.", "labels": [], "entities": []}, {"text": "The lattice transformer can readily consume the lattice input alone if the scores are unavailable.", "labels": [], "entities": []}, {"text": "A common application is found in the Chinese NER task, in which a Chinese sentence could possibly have multiple word segmentation possibilities.", "labels": [], "entities": [{"text": "Chinese NER task", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.535359650850296}, {"text": "word segmentation", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.762361466884613}]}, {"text": "Furthermore, different BPE operations ( or probabilistic subwords can also bring similar uncertainty to subword candidates and form a compact lattice structure.", "labels": [], "entities": []}, {"text": "In summary, this paper makes the following main contributions.", "labels": [], "entities": []}, {"text": "i) To our best knowledge, we are the first to propose a novel attention mechanism that consumes a word lattice and the probability scores from the ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.8791123032569885}]}, {"text": "ii) The proposed approach is naturally applied to both the encoder self-attention and encoder-decoder attention.", "labels": [], "entities": []}, {"text": "iii) Another appealing feature is that the lattice transformer can be reduced to standard latticeto-sequence model without probability scores, fitting the text translation task.", "labels": [], "entities": [{"text": "text translation", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.8009063899517059}]}, {"text": "iv) Extensive experiments on speech translation datasets demonstrate that our method outperforms the previous transformer and Lattice-LSTMs.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7192374914884567}]}, {"text": "The experiment on the WMT 2017 Chinese-English translation task shows the reduced model can improve many strong baselines such as the transformer.", "labels": [], "entities": [{"text": "WMT 2017 Chinese-English translation task", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.7554765820503235}]}], "datasetContent": [{"text": "We mainly validate our model in two scenarios, speech translation with word lattices and posterior scores, and Chinese to English text translation with different BPE operations on the source side.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7693467140197754}, {"text": "Chinese to English text translation", "start_pos": 111, "end_pos": 146, "type": "TASK", "confidence": 0.6453436613082886}]}, {"text": "The Fisher corpus includes the contents between strangers, while the Callhome corpus is primarily between friends and family members.", "labels": [], "entities": [{"text": "Fisher corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8818604946136475}, {"text": "Callhome corpus", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.9809474945068359}]}, {"text": "The numbers of sentence pairs of the two datasets are respectively 138,819 and 15,080.", "labels": [], "entities": []}, {"text": "The source side Spanish corpus consists of four data types: reference (human transcripts), oracle of ASR lattices (the optimal path with the lowest word error rate (WER)), ASR 1-best hypothesis, and ASR lattice.", "labels": [], "entities": [{"text": "Spanish corpus", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.6999058723449707}, {"text": "word error rate (WER))", "start_pos": 148, "end_pos": 170, "type": "METRIC", "confidence": 0.8464844723542532}]}, {"text": "For the data processing, we make caseinsensitive tokenization with the standard moses 3 tokenizer for both the source and target transcripts, and remove the punctuation in source sides.", "labels": [], "entities": []}, {"text": "The sentences of the other three types have been already been lowercased and punctuation-removed.", "labels": [], "entities": []}, {"text": "To keep consistent with the lattices, we add a token \"<s>\" at the beginning for all cases.", "labels": [], "entities": []}, {"text": "4 systems in are trained for both LatticeLSTMs and Lattice Transformer.", "labels": [], "entities": [{"text": "LatticeLSTMs", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.929349422454834}]}, {"text": "For fair and comprehensive comparison, we also evaluate all algorithms on the inputs of four types.", "labels": [], "entities": []}, {"text": "We initially train the baseline of our lattice transformer with the human transcripts on Fisher/Train data alone, which is equivalent to the modified transformer ().", "labels": [], "entities": [{"text": "Fisher/Train data", "start_pos": 89, "end_pos": 106, "type": "DATASET", "confidence": 0.6220884397625923}]}, {"text": "Then we fine-tune the pre-trained model with 1-best hypothesis or word lattices (and probability scores) for either Fisher or Callhome dataset.", "labels": [], "entities": [{"text": "Callhome dataset", "start_pos": 126, "end_pos": 142, "type": "DATASET", "confidence": 0.8584998548030853}]}, {"text": "The To better analyze the performance of our approach, we use an intensive cross-evaluation https://github.com/moses-smt/mosesdecoder method, i.e., we feed 4 possible inputs to test different models.", "labels": [], "entities": []}, {"text": "The cross-evaluation results are put into several 4 \u00d7 4 blocks in and 3.", "labels": [], "entities": []}, {"text": "As the aforementioned discussion, if the input is not ASR lattice, the evaluation on the model R+L+S needs to set w m = sf = s b = 0, s m = 1.", "labels": [], "entities": []}, {"text": "If the input is an ASR lattice but fed into the other three models, the probability scores are in fact discarded.", "labels": [], "entities": []}, {"text": "The Chinese to English parallel corpus for WMT 2017 news task contains about 20 million sentences after deduplication.", "labels": [], "entities": [{"text": "WMT 2017 news task", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.8123094737529755}]}, {"text": "For Chinese word segmentation, we use Jieba 4 as the baseline (, while the English sentences are tokenized by moses tokenizer.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6038519938786825}]}, {"text": "Some data filtering tricks have been applied, such as the ratio within of lengths between source and target sentence pairs and the count of tokens in both sides (\u2264 200).", "labels": [], "entities": [{"text": "data filtering", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.7212991416454315}]}, {"text": "Then for the Chinese source corpus, we learn the BPE tokenization with 16K / 32K / 48K operations, while for the English target corpus, we only learn the BPE tokenization with 32K operations.", "labels": [], "entities": [{"text": "Chinese source corpus", "start_pos": 13, "end_pos": 34, "type": "DATASET", "confidence": 0.7222228050231934}, {"text": "BPE tokenization", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.5612781345844269}]}, {"text": "In this way, each Chinese input can be represented as three different tokenized results, thus being ready to construct a word lattice.", "labels": [], "entities": []}, {"text": "The hyper-parameters of our model are the src transcript qu\u00e9 tal , eh , yo soy guillermo , \u00bf c\u00f3mo est\u00e1s ? porque como esto tiene que ir avanzando \u00bf no ? pues , \u00bf y llevas muchos a\u00f1osaq\u00f9 \u0131 en atlanta ? quererlo y tener fe . tgt reference how are you , eh i 'm guillermo , how are you ? because like this has to be moving forward , no ? well . and you 've been many years herein atlanta ? to love him and have faith . ASR 1-best quedar eh yo soy guillermo c\u00f3mo est\u00e1s porque como esto tiene que ir avanzando no pas lleva muchos aos aqu en atlanta quieren lo y tener fe mt from R+1 stay . eh , i 'm guillermo . how are you ? why do you have to move forward or not ? country has been many years herein atlanta they want to have faith ASR lattice quedar que qu\u00e9 eh yo soy dar eh yo tal eh yo soy guillermo cmo comprar con como est\u00e1 est\u00e1s porque como esto tiene que ir avanzando no pa\u00eds well lleva lleva muchos a\u00f1os aqu\u00ed en atlanta quieren quererlo lo y tener tenerse fey tener tenerse fe mt from R+L+S how are you ? i 'm guillermo . how are you ? because since this has to move forward , right ? well , you 've been here many years in atlanta loving him and having faith  same as the setting with the speech translation in previous experiments.", "labels": [], "entities": []}, {"text": "We follow the optimization convention in ( to use ADAM optimizer with Noam invert squared decay.", "labels": [], "entities": []}, {"text": "All of our lattice transformers are trained on 4 P-100 GPU cards.", "labels": [], "entities": []}, {"text": "Similar to our comparison method, detokenized cased-sensitive BLEU is reported in our experiment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9619342088699341}]}], "tableCaptions": [{"text": " Table 2: Cross-Evaluation of BLEU on Fisher. Note that for the lattice transformer architecture with R or R+1 set- ting, the resulted model is equivalent to a standard transformer with relative positional embeddings. The evaluation  of oracle inputs is similar to ASR 1-best, but it can indicate an upper bound of the performance. The evaluation  results of Lattice LSTM on Fisher dev are not reported in (Sperber et al., 2017).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9939484596252441}, {"text": "ASR", "start_pos": 265, "end_pos": 268, "type": "METRIC", "confidence": 0.8135513067245483}]}, {"text": " Table 3: Cross-Evaluation of BLEU on Callhome.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9975038170814514}, {"text": "Callhome", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9770520925521851}]}, {"text": " Table 5: Ablation Experiment BLEU Results. The  rows of the Lattice LSTM and the Lattice Transformer  represent the 1-best hypothesis fine-tuning, and the  BLEUs are evaluated on 1-best inputs and on lattice  inputs for the others. The colored BLEU values come  from Table 2 and 3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.4617134630680084}, {"text": "BLEUs", "start_pos": 157, "end_pos": 162, "type": "METRIC", "confidence": 0.9889625906944275}, {"text": "BLEU", "start_pos": 245, "end_pos": 249, "type": "METRIC", "confidence": 0.9832421541213989}]}, {"text": " Table 6: BLEU on WMT 2017 Chinese English", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998984158039093}, {"text": "WMT 2017 Chinese English", "start_pos": 18, "end_pos": 42, "type": "DATASET", "confidence": 0.951972246170044}]}]}