{"title": [{"text": "SRILM -An extensible language modeling toolkit", "labels": [], "entities": [{"text": "SRILM", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.5565981268882751}]}], "abstractContent": [{"text": "This paper describes the UdS-DFKI submission to the WMT2019 news translation task for Gujarati-English (low-resourced pair) and German-English (document-level evaluation).", "labels": [], "entities": [{"text": "WMT2019 news translation task", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.761864498257637}]}, {"text": "Our systems rely on the on-line extraction of parallel sentences from comparable corpora for the first scenario and on the inclusion of coreference-related information in the training data in the second one.", "labels": [], "entities": []}, {"text": "Neural Machine Translation of Rare Words with Subword Units.", "labels": [], "entities": [{"text": "Neural Machine Translation of Rare Words", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8333971798419952}]}], "introductionContent": [{"text": "This document describes the systems and experiments conducted to participate in the news translation tasks of WMT 2019 for GujaratiEnglish (gu-en, low-resourced language pair) and German-English (de-en, document-level evaluation).", "labels": [], "entities": [{"text": "news translation", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.6919633001089096}, {"text": "WMT 2019", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.7124382257461548}]}, {"text": "We use different approaches to tackle each setting.", "labels": [], "entities": []}, {"text": "Machine translation (neural, statistical or rulebased), usually operates on a sentence-bysentence basis.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7959220111370087}]}, {"text": "However, when translating a coherent document, surrounding sentences may contain information that needs to be reflected in a local sentence.", "labels": [], "entities": []}, {"text": "In our experiments for the document-level task in en2de, we explore how the information beyond sentence level can be made available to a neural machine translation (NMT) system by modifying -tagging-the data in order to include this knowledge.", "labels": [], "entities": []}, {"text": "Ina similar way, multilingual NMT systems have already been successfully built by only tagging the source data with the knowledge of the target language.", "labels": [], "entities": []}, {"text": "With this approach, we incorporate the knowledge that carries coreferences through a text in every sentence.", "labels": [], "entities": []}, {"text": "We expect to improve the translation of ambiguous items such as pronouns in English, so we just tackle a specific number of problems and not translation quality in general.", "labels": [], "entities": [{"text": "translation of ambiguous items such as pronouns", "start_pos": 25, "end_pos": 72, "type": "TASK", "confidence": 0.8398488334247044}]}, {"text": "The approach for the low-resource setting is completely different.", "labels": [], "entities": []}, {"text": "In this case, we use a neural architecture that allows us to extract parallel data from comparable corpora and filter noise from the available parallel data.", "labels": [], "entities": []}, {"text": "The additional data obtained in this way is then used to train SMT models, which we compare to a baseline trained on the available parallel data only to observe the effects of the extraction and filtering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9957749247550964}]}, {"text": "Below, we describe our coreference-aware system for en2de (Section 2) and our low-resourced approach for en-gu (Section 3).", "labels": [], "entities": []}, {"text": "Finally we summarise our findings in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Model names are structured as  architectureVocabulary-Annotation  -Embeddings-Corpus.", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores of the models trained for the  en2de translation task. The boldfaced ensembled  model was submitted as the primary submission; the  best performing model with boldfaced BLEU scores  was not ready at submission time.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990259408950806}, {"text": "en2de translation task", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7590705454349518}, {"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9948114156723022}]}, {"text": " Table 4: BLEU scores achieved on the internal devel- opment set and the official newstest2019. Scores on the  development set are calculated using multi-bleu  on the tokenized outputs, while the results on new- stest2019 are those calculated by the WMT matrix. Pri- mary system submissions are in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9976179003715515}]}]}