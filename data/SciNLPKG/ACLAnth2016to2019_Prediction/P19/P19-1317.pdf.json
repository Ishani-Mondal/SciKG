{"title": [{"text": "Robust Representation Learning of Biomedical Names", "labels": [], "entities": [{"text": "Robust Representation Learning of Biomedical Names", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8735466400782267}]}], "abstractContent": [{"text": "Biomedical concepts are often mentioned in medical documents under different name variations (synonyms).", "labels": [], "entities": []}, {"text": "This mismatch between surface forms is problematic, resulting in difficulties pertaining to learning effective representations.", "labels": [], "entities": []}, {"text": "Consequently, this has tremendous implications such as rendering downstream applications inefficacious and/or potentially unreliable.", "labels": [], "entities": []}, {"text": "This paper proposes anew framework for learning robust representations of biomedical names and terms.", "labels": [], "entities": [{"text": "learning robust representations of biomedical names and terms", "start_pos": 39, "end_pos": 100, "type": "TASK", "confidence": 0.6520187109708786}]}, {"text": "The idea behind our approach is to consider and encode contextual meaning, conceptual meaning , and the similarity between synonyms during the representation learning process.", "labels": [], "entities": []}, {"text": "Via extensive experiments, we show that our proposed method outperforms other baselines on a battery of retrieval, similarity and relatedness benchmarks.", "labels": [], "entities": [{"text": "similarity", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9348491430282593}]}, {"text": "Moreover, our proposed method is also able to compute meaningful representations for unseen names, resulting in high practical utility in real-world applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representation learning of words (, and/or sentences ( forms the bedrock of many modern NLP applications.", "labels": [], "entities": [{"text": "Representation learning of words", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9236196875572205}]}, {"text": "These techniques, largely relying on context information, have a huge impact on downstream applications.", "labels": [], "entities": []}, {"text": "To this end, learning effective and useful representations has been a highly fruitful area of research.", "labels": [], "entities": []}, {"text": "Biomedical names 1 , however, are different from standard words and sentences.", "labels": [], "entities": []}, {"text": "These names have both contextual and conceptual meanings.", "labels": [], "entities": []}, {"text": "Contextual meaning reflects the contexts where the names appear, and it is specifically granted to each Biomedical names refer to surface forms that represent biomedical concepts.", "labels": [], "entities": []}, {"text": "They can be official names in biomedical vocabularies or unofficial names mentioned in text.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first detail the implementations of baselines and the proposed BNE model.", "labels": [], "entities": [{"text": "BNE", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.5904757380485535}]}, {"text": "We then evaluates all the models with 4 different tasks in retrieval, embedding similarity and relatedness benchmarks.", "labels": [], "entities": []}, {"text": "We consider three variants of skip-gram (with negative sampling).", "labels": [], "entities": []}, {"text": "SG W obtains word embeddings by training the very basic skip-gram model (see Equation 1).", "labels": [], "entities": []}, {"text": "To get the representation fora name, we simply take the average of its associated word embeddings.", "labels": [], "entities": []}, {"text": "SG S is another variant that considers names as special tokens.", "labels": [], "entities": [{"text": "SG S", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7236078381538391}]}, {"text": "The model obtains embeddings for word and names concurrently (see Equation 2).", "labels": [], "entities": []}, {"text": "SG S training requires input text to be segmented into names and regular words.", "labels": [], "entities": [{"text": "SG S", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9680432379245758}]}, {"text": "SG S.C is our proposed extension of skip-gram model.", "labels": [], "entities": []}, {"text": "As introduced in Section 3.1, this baseline requires an annotated corpus where the names are labeled with their associated concepts.", "labels": [], "entities": []}, {"text": "We use PubMed corpus, which consists of 29 million biomedical abstracts, to train SG W . For SG Sand SG S.C , we further utilize the annotations provided in Pubtator ().", "labels": [], "entities": [{"text": "PubMed corpus", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9438886940479279}, {"text": "SG W", "start_pos": 82, "end_pos": 86, "type": "TASK", "confidence": 0.7971738874912262}]}, {"text": "The annotations (names and their associated concepts) come with five categories: disease, chemical, gene, species, and mutation.", "labels": [], "entities": []}, {"text": "We use annotations of the two popular classes: disease and chemical.", "labels": [], "entities": []}, {"text": "In preprocessing, text is tokenized and lowercased.", "labels": [], "entities": []}, {"text": "Words that appear less than 3 times are ignored.", "labels": [], "entities": []}, {"text": "We use spaCy library for this parsing.", "labels": [], "entities": [{"text": "spaCy library", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9296348989009857}]}, {"text": "In total, our vocabulary contains approximately 3 millions words, 700 thousand names, and 85 thousand CUIs.", "labels": [], "entities": []}, {"text": "We use Gensim library to train all the skip-gram baselines.", "labels": [], "entities": [{"text": "Gensim library", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.8790865242481232}]}, {"text": "The embedding dimension is 200, and the context window size is 6.", "labels": [], "entities": []}, {"text": "Negative sampling is used with the number of negatives set to 5.", "labels": [], "entities": []}, {"text": "Biomedical Named Encoder (BNE).", "labels": [], "entities": [{"text": "Biomedical Named Encoder (BNE)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.572778508067131}]}, {"text": "We set the character embedding dimension to 50, and initialize their values randomly.", "labels": [], "entities": []}, {"text": "We use 200 dimensions for the outputted name embeddings.", "labels": [], "entities": []}, {"text": "The hidden states' dimensions for both character and wordlevel BiLSTM are 200.", "labels": [], "entities": []}, {"text": "We use Adam optimizer with the learning rate of 0.001, and gradient clipping threshold set to 5.0.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9301992058753967}]}, {"text": "Training batch size is 64.", "labels": [], "entities": []}, {"text": "Dropout with the rate of 0.5 is used to regularize the model.", "labels": [], "entities": []}, {"text": "Average performance on validation sets of biomedical name normalization experiment (see Section 4.3) is used as a criteria to stop the model training.", "labels": [], "entities": [{"text": "biomedical name normalization experiment", "start_pos": 42, "end_pos": 82, "type": "TASK", "confidence": 0.6777661293745041}]}, {"text": "Our proposed model is trained using only the synonym sets in UMLS 2 , i.e., U = {S c }.", "labels": [], "entities": [{"text": "UMLS 2", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.891141951084137}]}, {"text": "We limit the synonyms to those of disease concepts 3 . We intentionally leave the chemical concepts out for out-domain evaluation.", "labels": [], "entities": []}, {"text": "As a result, approximately 16 thousand synonym sets (associated to that number of disease concepts) are collected for training.", "labels": [], "entities": []}, {"text": "These synonym sets include 156 thousand disease names in total.", "labels": [], "entities": []}, {"text": "In each training batch, one positive and one negative pairs are sampled separately for each loss.", "labels": [], "entities": []}, {"text": "The pre-trained word (or name/concept) embeddings are taken from the skip-gram baselines as described before.", "labels": [], "entities": []}, {"text": "We denote two configurations, associated to Options 1 and 2 (see Section 3.2), as BNE + SG S.C and BNE + SG W , respectively.", "labels": [], "entities": [{"text": "BNE", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9386260509490967}]}, {"text": "Next, we present the evaluations of these models.: t-SNE visualization of 254 name embeddings.", "labels": [], "entities": []}, {"text": "These names belong to 10 disease concepts in which 5 of these concepts appear in the training data, while the other 5 concepts (marked with (*)) do not.", "labels": [], "entities": []}, {"text": "It can be observed that BNE projects names of the same concept close to each others.", "labels": [], "entities": [{"text": "BNE", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.5735615491867065}]}, {"text": "The model also retains closeness between names of related concepts, such as 'parkinson disease' and 'paranoid disorders' (see the blue and olive plus signs).: Mean coverage at k: average ratio of correct synonyms that are found in k-nearest neighbors, which are estimated by cosine similarity of name embeddings.", "labels": [], "entities": [{"text": "Mean coverage", "start_pos": 159, "end_pos": 172, "type": "METRIC", "confidence": 0.8808831870555878}]}, {"text": "Note that names in these disease and chemical test sets are not seen in the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Mean average precision (MAP) performance  on the synonym retrieval task. The best and second best  results are in boldface and underlined, respectively.", "labels": [], "entities": [{"text": "Mean average precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9246223866939545}, {"text": "synonym retrieval task", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.910004715124766}]}, {"text": " Table 3: Name normalization accuracy on disease (Di)  and chemical (Ch) datasets. The last row group in- cludes the results of supervised models that utilize  training annotations in each specific dataset. XM de- notes the use of 'exact match' rule to assign the corre- sponding concept to a mention if the mention is found  in the training data.  \u2020 indicates the results reported  by Wright et al. (2019).", "labels": [], "entities": [{"text": "Name normalization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8093622028827667}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8500260710716248}]}]}