{"title": [{"text": "Towards Explainable NLP: A Generative Explanation Framework for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7643058896064758}]}], "abstractContent": [{"text": "Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.7139127850532532}]}, {"text": "Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs.", "labels": [], "entities": []}, {"text": "However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time.", "labels": [], "entities": [{"text": "generative explanation", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9604485034942627}]}, {"text": "More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations.", "labels": [], "entities": []}, {"text": "We construct two new datasets that contain summaries, rating scores, and fine-grained reasons.", "labels": [], "entities": []}, {"text": "We conduct experiments on both datasets, comparing with several strong neural network base-line systems.", "labels": [], "entities": []}, {"text": "Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep learning methods have produced state-ofthe-art results in many natural language processing (NLP) tasks (.", "labels": [], "entities": []}, {"text": "Though these deep neural network models achieve impressive performance, it is relatively difficult to convince people to trust the predictions of such neural networks since they are actually black boxes for human beings).", "labels": [], "entities": []}, {"text": "For instance, if an essay scoring system only tells the scores of a given essay without providing explicit reasons, the users can hardly be convinced of the judgment.", "labels": [], "entities": []}, {"text": "Therefore, the ability to explain the rationale is essential fora NLP system, a need which requires traditional NLP models to provide human-readable explanations.", "labels": [], "entities": []}, {"text": "In recent years, lots of works have been done to solve text classification problems, but just a few of them have explored the explainability of their systems (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7950076758861542}]}, {"text": "try to identify an interpretable model over the interpretable representation that is locally faithful to the classifier.", "labels": [], "entities": []}, {"text": "use heatmap to visualize how much each hidden element contributes to the predicted results.", "labels": [], "entities": []}, {"text": "Although these systems are somewhat promising, they typically do not consider finegrained information that may contain information for interpreting the behavior of models.", "labels": [], "entities": []}, {"text": "However, if a human being wants to rate a product, s/he may first write down some reviews, and then score or summarize some attributes of the product, like price, packaging, and quality.", "labels": [], "entities": []}, {"text": "Finally, the overall rating for the product will be given based on the fine-grained information.", "labels": [], "entities": []}, {"text": "Therefore, it is crucial to build trustworthy explainable text classification models that are capable of explicitly generating fine-grained information for explaining their predictions.", "labels": [], "entities": [{"text": "text classification", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7093241363763809}]}, {"text": "To achieve these goals, in this paper, we propose a novel generative explanation framework for text classification, where our model is capable of not only providing the classification predictions but also generating fine-grained information as explanations for decisions.", "labels": [], "entities": [{"text": "generative explanation", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.9296848475933075}, {"text": "text classification", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7979379296302795}]}, {"text": "The novel idea behind our hybrid generative-discriminative method is to explicitly capture the fine-grained information inferred from raw texts, utilizing the information to help interpret the predicted classification results and improve the overall performance.", "labels": [], "entities": []}, {"text": "Specifically, we introduce the notion of an explainable factor and a minimum risk training method that learn to generate reasonable explanations for the overall predict results.", "labels": [], "entities": []}, {"text": "Meanwhile, such a strategy brings strong connections between the explanations and predictions, which in return leads to better performance.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to explicitly explain the predicted results by utilizing the abstractive generative fine-grained information.", "labels": [], "entities": []}, {"text": "In this work, we regard the summaries (texts) and rating scores (numbers) as the fine-grained information.", "labels": [], "entities": []}, {"text": "Two datasets that contain these kinds of fine-grained information are collected to evaluate our method.", "labels": [], "entities": []}, {"text": "More specifically, we construct a dataset crawled from a website called PCMag . Each item in this dataset consists of three parts: along review text for one product, three short text comments (respectively explains the property of the product from positive, negative and neutral perspectives) and an overall rating score.", "labels": [], "entities": [{"text": "PCMag", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.9324404001235962}]}, {"text": "We regard the three short comments as fine-grained information for the long review text.", "labels": [], "entities": []}, {"text": "Besides, we also conduct experiments on the Skytrax User Reviews Dataset 2 , where each case consists of three parts: a review text fora flight, five sub-field rating scores (seat comfortability, cabin stuff, food, in-flight environment, ticket value) and an overall rating score.", "labels": [], "entities": [{"text": "Skytrax User Reviews Dataset 2", "start_pos": 44, "end_pos": 74, "type": "DATASET", "confidence": 0.9140690922737121}]}, {"text": "As for this dataset, we regard the five sub-field rating scores as fine-grained information for the flight review text.", "labels": [], "entities": [{"text": "flight review text", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.6174163023630778}]}, {"text": "Empirically, we evaluate our model-agnostic method on several neural network baseline models for both datasets.", "labels": [], "entities": []}, {"text": "Experimental results suggest that our approach substantially improves the performance over baseline systems, illustrating the advantage of utilizing fine-grained information.", "labels": [], "entities": []}, {"text": "Meanwhile, by providing the fine-grained information as explanations for the classification results, our model is an understandable system that is worth trusting.", "labels": [], "entities": []}, {"text": "Our major contributions are three-fold: \u2022 We are the first to leverage the generated finegrained information for building a generative explanation framework for text classification, propose an explanation factor, and introduce minimum risk training for this hybrid generative-discriminative framework; \u2022 We evaluate our model-agnostic explanation framework with different neural network architectures, and show considerable improvements over baseline systems on two datasets; \u2022 We provide two new publicly available explainable NLP datasets that contain finegrained information as explanations for text classification.", "labels": [], "entities": [{"text": "generative explanation", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.8844267427921295}, {"text": "text classification", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7574869990348816}, {"text": "text classification", "start_pos": 598, "end_pos": 617, "type": "TASK", "confidence": 0.7669997215270996}]}], "datasetContent": [{"text": "We conduct experiments on two datasets where we use texts and numerical ratings to represent finegrained information respectively.", "labels": [], "entities": []}, {"text": "The first one is crawled from a website called PCMag, and the other one is the Skytrax User Reviews Dataset.", "labels": [], "entities": [{"text": "PCMag", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9225296378135681}, {"text": "Skytrax User Reviews Dataset", "start_pos": 79, "end_pos": 107, "type": "DATASET", "confidence": 0.904479444026947}]}, {"text": "Note that all the texts in the two datasets are preprocessed by the Stanford Tokenizer 3 (Manning et al., 2014).", "labels": [], "entities": [{"text": "Stanford Tokenizer 3", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.9409162600835165}]}, {"text": "This dataset is crawled from the website PCMag.", "labels": [], "entities": [{"text": "PCMag", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9620879292488098}]}, {"text": "It is a website providing reviews for electronic products, like laptops, smartphones, cameras and soon.", "labels": [], "entities": []}, {"text": "Each item in the dataset consists of three parts: along review text, three short comments, and an overall rating score for the product.", "labels": [], "entities": []}, {"text": "Three short comments are summaries of the long review respectively from positive, negative, neutral perspectives.", "labels": [], "entities": []}, {"text": "An overall rating score is a number ranging from 0 to 5, and the possible values that the score could be are {1.0, 1.5, 2.0, ..., 5.0}.", "labels": [], "entities": []}, {"text": "Since long text generation is not what we focus on, the items where review text contains more than 70 sentences or comments contain greater than 75 tokens are filtered.", "labels": [], "entities": [{"text": "long text generation", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.6844974160194397}]}, {"text": "We randomly split the dataset into 10919/1373/1356 pairs for train/dev/test set.", "labels": [], "entities": []}, {"text": "The distribution of the overall rating scores within this corpus is shown in.", "labels": [], "entities": []}, {"text": "We incorporate an airline review dataset scraped from Skytraxs Web portal.", "labels": [], "entities": [{"text": "airline review dataset", "start_pos": 18, "end_pos": 40, "type": "DATASET", "confidence": 0.6753774086634318}, {"text": "Skytraxs Web portal", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.9310964544614156}]}, {"text": "Each item in this dataset consists of three parts: i.e., a review text, five sub-field scores and an overall rating score.", "labels": [], "entities": []}, {"text": "The five sub-field scores respectively stand for the user's ratings for seat comfortability, cabin stuff, food, in-flight environment, and ticket value, and each score is an integer between 0 and 5.", "labels": [], "entities": []}, {"text": "The overall score is an integer between 1 and 10.", "labels": [], "entities": []}, {"text": "Similar to the PCMag Review Dataset, we filter out the items where the review contains more than.", "labels": [], "entities": [{"text": "PCMag Review Dataset", "start_pos": 15, "end_pos": 35, "type": "DATASET", "confidence": 0.9580374161402384}]}, {"text": "As the goal of this study is to propose an explanation framework, in order to test the effectiveness of proposed GEF, we use the same experimental settings on the base model and on the base model+GEF.", "labels": [], "entities": [{"text": "GEF", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.6408404111862183}, {"text": "GEF", "start_pos": 196, "end_pos": 199, "type": "DATASET", "confidence": 0.9661556482315063}]}, {"text": "We use GloVe () word embedding for PCMag dataset and minimize the objective function using Adam).", "labels": [], "entities": [{"text": "PCMag dataset", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.953973114490509}]}, {"text": "The hyperparameter settings for both datasets are listed in.", "labels": [], "entities": []}, {"text": "Meanwhile, since the generation loss is larger than classification loss for text explanations, we stop updating the predictor after classification loss reaches a certain threshold (adjusted based on dev set) to avoid overfitting.", "labels": [], "entities": []}, {"text": "In order to prove our model-agnostic framework can make the basic model generate explanations more closely aligned with the classification results, we employ crowdsourced judges to evaluate   a random sample of 100 items in the form of text, each being assigned to 5 judges on the Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 281, "end_pos": 303, "type": "DATASET", "confidence": 0.9551761746406555}]}, {"text": "All the items are correctly classified both using the basic model and using GEF, so that we can clearly compare the explainability of these generated text explanations.", "labels": [], "entities": [{"text": "GEF", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.5125032067298889}]}, {"text": "We report the results in, and we can see that over half of the judges think that our GEF can generate explanations more related to the classification results.", "labels": [], "entities": [{"text": "GEF", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.8600656390190125}]}, {"text": "In particular, for 57.62% of the tested items, our GEF can generate better or equal explanations comparing with the basic model.", "labels": [], "entities": [{"text": "GEF", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.7124587893486023}]}, {"text": "In addition, we show some the examples of text explanations generated by CVAE+GEF in.", "labels": [], "entities": [{"text": "CVAE+GEF", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.675224244594574}]}, {"text": "We can see that our model can accurately capture some key points in the golden explanations.", "labels": [], "entities": []}, {"text": "And it can learn to generate grammatical comments that are logically reasonable.", "labels": [], "entities": []}, {"text": "All these illustrate the efficient of our method.", "labels": [], "entities": []}, {"text": "We will demonstrate more of our results in the supplementary materials.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of examples by each overall rating score in PCMag Review Dataset.", "labels": [], "entities": [{"text": "PCMag Review Dataset", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.9722974499066671}]}, {"text": " Table 2: Distribution of examples by each overall rating score in Skytrax User Reviews Dataset.", "labels": [], "entities": [{"text": "Skytrax User Reviews Dataset", "start_pos": 67, "end_pos": 95, "type": "DATASET", "confidence": 0.9230770617723465}]}, {"text": " Table 3: Experimental settings for our experiments.  Note that for CNN, we additionally set filter number to  be 256 and filter sizes to be [3, 4, 5, 6].", "labels": [], "entities": [{"text": "CNN", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.7034242749214172}]}, {"text": " Table 3. Mean- while, since the generation loss is larger than clas- sification loss for text explanations, we stop updat- ing the predictor after classification loss reaches  a certain threshold (adjusted based on dev set) to  avoid overfitting.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9974226951599121}]}, {"text": " Table 4: BLEU scores for generated explanations.  Pos., Neg., Neu. respectively stand for positive, neg- ative and neural explanations. The low BLEU-3 and  BLEU-4 scores are because the target explanations  contain many domain-specific words with low fre- quency, which makes it hard for the model to generate  accurate explanations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978965520858765}, {"text": "BLEU-3", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9967255592346191}, {"text": "BLEU-4", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9964721202850342}]}, {"text": " Table 5: Classification accuracy on PCMag Review  Dataset. Oracle means if we feed ground-truth text  explanations to the Classifier C, the accuracy C can  achieve to do classification. Oracle confirms our as- sumption that explanations can do better in classifica- tion than the original text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9292347431182861}, {"text": "PCMag Review  Dataset", "start_pos": 37, "end_pos": 58, "type": "DATASET", "confidence": 0.9451894760131836}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9850313663482666}]}, {"text": " Table 6: Accuracy of sub-field numerical explanations  on Skytrax User Reviews Dataset. s, c, f, t, v stand for  seat comfortability, cabin stuff, food, in-flight environ- ment and ticket value, respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9833968281745911}, {"text": "Skytrax User Reviews Dataset", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.9547652453184128}, {"text": "ticket value", "start_pos": 182, "end_pos": 194, "type": "METRIC", "confidence": 0.8665856420993805}]}, {"text": " Table 7: Classification accuracy on Skytrax User Re- views Dataset. Oracle means if we feed ground-truth  numerical explanation to the Classifier C, the accuracy  C can achieve to do classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9130634665489197}, {"text": "Skytrax User Re- views Dataset", "start_pos": 37, "end_pos": 67, "type": "DATASET", "confidence": 0.8837425907452902}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9646514654159546}]}, {"text": " Table 8: Results of human evaluation. Tests are con- ducted between the text explanations generated by ba- sic CVAE and CVAE+GEF.", "labels": [], "entities": [{"text": "CVAE+GEF", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.7279514074325562}]}, {"text": " Table 10: Examples from the results on Skytrax User  Reviews Dataset. s, c, f, i, t stand for seat comfortabil- ity, cabin stuff, food, in-flight environment and ticket  value, respectively.", "labels": [], "entities": [{"text": "Skytrax User  Reviews Dataset", "start_pos": 40, "end_pos": 69, "type": "DATASET", "confidence": 0.9174462556838989}]}]}