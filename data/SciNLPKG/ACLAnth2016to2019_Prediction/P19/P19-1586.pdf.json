{"title": [{"text": "Low-resource Deep Entity Resolution with Transfer and Active Learning", "labels": [], "entities": [{"text": "Deep Entity Resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6384763022263845}]}], "abstractContent": [{"text": "Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases.", "labels": [], "entities": [{"text": "Entity resolution (ER)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8498597800731659}]}, {"text": "It is a key step for knowledge base creation and text mining.", "labels": [], "entities": [{"text": "knowledge base creation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7045899232228597}, {"text": "text mining", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8723246455192566}]}, {"text": "Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records.", "labels": [], "entities": []}, {"text": "While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications.", "labels": [], "entities": []}, {"text": "In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning.", "labels": [], "entities": [{"text": "ER", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9632946848869324}]}, {"text": "We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one.", "labels": [], "entities": []}, {"text": "To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model.", "labels": [], "entities": []}, {"text": "Empirical evaluation demonstrates that our method achieves comparable, if not better , performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity Resolution (ER), also known as entity matching, record linkage, reference reconciliation (), and merge-purge (, identifies and links different representations of the same real-world entities.", "labels": [], "entities": [{"text": "Entity Resolution (ER)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8603139340877533}, {"text": "entity matching", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7294902950525284}, {"text": "reference reconciliation", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.6839313358068466}]}, {"text": "ER yields a unified and consistent view of data and serves as a crucial step in downstream applications, including knowledge base creation, text mining (, and social media analysis (Campbell * Work done during summer internship at IBM et al., 2016).", "labels": [], "entities": [{"text": "knowledge base creation", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.6326572199662527}, {"text": "text mining", "start_pos": 140, "end_pos": 151, "type": "TASK", "confidence": 0.8444632887840271}, {"text": "social media analysis", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.6815628409385681}]}, {"text": "For instance, seen in are citation data records from two databases, DBLP and Google Scholar.", "labels": [], "entities": [{"text": "DBLP", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.9606395363807678}]}, {"text": "If one intends to build a system that analyzes citation networks of publications, it is essential to recognize publication overlaps across the databases and to integrate the data records).", "labels": [], "entities": []}, {"text": "Recent work demonstrated that deep learning (DL) models with distributed representations of words are viable alternatives to other machine learning algorithms, including support vector machines and decision trees, for performing ER (.", "labels": [], "entities": []}, {"text": "The DL models provide a universal solution to ER across all kinds of datasets that alleviates the necessity of expensive feature engineering, in which a human designer explicitly defines matching functions for every single ER scenario.", "labels": [], "entities": []}, {"text": "However, DL is well known to be data hungry; in fact, the DL models proposed in; achieve state-of-the-art performance by learning from thousands of labels.", "labels": [], "entities": []}, {"text": "Unfortunately, realistic ER tasks have limited access to labeled data and would require substantial labeling effort upfront, before the actual learning of the ER models.", "labels": [], "entities": []}, {"text": "Creating a representative training set is especially challenging in ER problems due to the data distribution, which is heavily skewed towards negative pairs (i.e. non-matches) as opposed to positive pairs (i.e. matches).", "labels": [], "entities": [{"text": "ER", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9725871682167053}]}, {"text": "This problem limits the applicability of DL methods in low-resource ER scenarios.", "labels": [], "entities": []}, {"text": "Indeed, we will show in a later section that the performance of DL models degrades significantly as compared to other machine learning algorithms when only a limited amount of labeled data is available.", "labels": [], "entities": []}, {"text": "To address this issue, we propose a DLbased method that combines transfer learning and: Data record examples from DBLP-Scholar (citation genre).", "labels": [], "entities": [{"text": "DBLP-Scholar", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.8969603180885315}]}, {"text": "The first records from DBLP and Google Scholar (red) refer to the same publication even though the information is not identical.", "labels": [], "entities": [{"text": "DBLP", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.9799507856369019}, {"text": "Google Scholar", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.8731714487075806}]}, {"text": "The second ones (blue and brown) record different papers with the same authors and year.", "labels": [], "entities": []}, {"text": "We first develop a transfer learning methodology to leverage a few pre-existing scenarios with abundant labeled data, in order to use them in other settings of similar nature but with limited or no labeled data.", "labels": [], "entities": []}, {"text": "More concretely, through a carefully crafted neural network architecture, we learn a transferable model from multiple source datasets with cumulatively abundant labeled data.", "labels": [], "entities": []}, {"text": "Then we use active learning to identify informative examples from the target dataset to further adapt the transferred model to the target setting.", "labels": [], "entities": []}, {"text": "This novel combination of transfer and active learning in ER settings enables us to learn a comparable or better performing DL model while using significantly fewer target dataset labels in comparison to state-of-the-art DL and even non-DL models.", "labels": [], "entities": []}, {"text": "We also note that the two techniques are not dependent on each other.", "labels": [], "entities": []}, {"text": "For example, one could skip transfer learning if no high-resource dataset is available and directly use active learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.9364388883113861}]}, {"text": "Conversely, one could use transfer learning directly without active learning.", "labels": [], "entities": []}, {"text": "We evaluate these cases in the experiments.", "labels": [], "entities": []}, {"text": "Specifically, we make the following contributions: \u2022 We propose a DL architecture for ER that learns attribute agnostic and transferable representations from multiple source datasets using dataset (domain) adaptation.", "labels": [], "entities": []}, {"text": "\u2022 To the best of our knowledge, we are the first to design an active learning algorithm for deep ER models.", "labels": [], "entities": []}, {"text": "Our active learning algorithm searches for high-confidence examples and uncertain examples, which provide a guided way to improve the precision and recall of the transferred model to the target dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9989094734191895}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9917805790901184}]}, {"text": "\u2022 We perform extensive empirical evaluations over multiple benchmark datasets and demonstrate that our method outperforms state-ofthe-art learning-based models while using an order of magnitude fewer labels.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all datasets, we first conduct blocking to reduce the Cartesian product to a candidate set.", "labels": [], "entities": []}, {"text": "Then, we randomly split the candidate set into training, development, and test data with a ratio of 3:1:1.", "labels": [], "entities": []}, {"text": "For the datasets used in Mudgal et al.", "labels": [], "entities": []}, {"text": "(2018) (DBLP-ACM, DBLP-Scholar, Fodors-Zagats, and Amazon-Google), we adopted the same feature-based blocking strategies and random splits to ensure comparability with the state-of-the-art method.", "labels": [], "entities": [{"text": "DBLP-Scholar", "start_pos": 18, "end_pos": 30, "type": "DATASET", "confidence": 0.8714868426322937}]}, {"text": "The candidate set of Cora was obtained by randomly sampling 50,000 pairs from the result of the jaccard similarity-based blocking strategy described in.", "labels": [], "entities": []}, {"text": "The candidate set of Zomato-Yelp was taken from.", "labels": [], "entities": []}, {"text": "All dataset statistics are given in.", "labels": [], "entities": []}, {"text": "For evaluation, we compute precision, recall, and F1 score on the test sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996236562728882}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9994420409202576}, {"text": "F1 score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9857767522335052}]}, {"text": "In the active learning experiments, we holdout the test sets a priori and sample solely from the training data to ensure fair comparison with non-active learning methods.", "labels": [], "entities": []}, {"text": "The sampling size K for active learning is 20.", "labels": [], "entities": []}, {"text": "As preprocessing, we tokenize with NLTK () and lowercase all attribute values.", "labels": [], "entities": []}, {"text": "For every configuration, we run experiments with 5 random initializations and report the average.", "labels": [], "entities": []}, {"text": "Our DL models are all implemented using the publicly available deepmatcher library.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Post-blocking statistics of the ER datasets we  used. (attr denotes the number of attributes.)", "labels": [], "entities": []}, {"text": " Table 3: Transfer learning results (citation genre). We report standard deviations of the F1 scores. For each target  dataset, the source is given by the other two datasets (e.g., the source for DBLP-ACM is DBLP-Scholar and Cora.)", "labels": [], "entities": [{"text": "F1", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9969825148582458}, {"text": "DBLP-ACM", "start_pos": 196, "end_pos": 204, "type": "DATASET", "confidence": 0.9553273320198059}, {"text": "DBLP-Scholar", "start_pos": 208, "end_pos": 220, "type": "DATASET", "confidence": 0.9466180205345154}]}, {"text": " Table 4: Low-resource (shaded) and high-resource (full  training data) performance comparison. DTAL, DAL,  and DL denote deep transfer active learning, deep ac- tive learning, and deep learning (random sampling).", "labels": [], "entities": [{"text": "DTAL", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.8386576175689697}]}, {"text": " Table 5: Transfer and active learning results in the  restaurant genre. The target and source datasets are  Fodors-Zagats and Zomato-Yelp respectively.", "labels": [], "entities": []}, {"text": " Table 6: Low-resource performance (300 labeled ex- amples) of different sampling strategies (DBLP-ACM).", "labels": [], "entities": []}, {"text": " Table 7: Breakdown of 300 labeled samples (uncertain  samples) from deep transfer active learning in DBLP- ACM. Part, FP, TP, FN, and TN denote the partition  mechanism, false positives, true positives, false nega- tives, and true negatives respectively.", "labels": [], "entities": [{"text": "DBLP- ACM", "start_pos": 102, "end_pos": 111, "type": "DATASET", "confidence": 0.8235851724942526}, {"text": "FP", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9803747534751892}, {"text": "FN", "start_pos": 127, "end_pos": 129, "type": "METRIC", "confidence": 0.8924750089645386}, {"text": "TN", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.9136463403701782}]}, {"text": " Table 8: Deep ER hyperparameters.", "labels": [], "entities": []}]}