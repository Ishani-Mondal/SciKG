{"title": [{"text": "A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning", "labels": [], "entities": [{"text": "Neural Grammatical Error Correction", "start_pos": 2, "end_pos": 37, "type": "TASK", "confidence": 0.766753189265728}, {"text": "Sequential Transfer", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8590717017650604}]}], "abstractContent": [{"text": "Grammatical error correction can be viewed as a low-resource sequence-to-sequence task, because publicly available parallel corpora are limited.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7754372159639994}]}, {"text": "To tackle this challenge, we first generate erroneous versions of large unanno-tated corpora using a realistic noising function.", "labels": [], "entities": []}, {"text": "The resulting parallel corpora are subsequently used to pre-train Transformer models.", "labels": [], "entities": []}, {"text": "Then, by sequentially applying transfer learning, we adapt these models to the domain and style of the test set.", "labels": [], "entities": []}, {"text": "Combined with a context-aware neural spellchecker, our system achieves competitive results in both restricted and low resource tracks in ACL 2019 BEA Shared Task.", "labels": [], "entities": [{"text": "ACL 2019 BEA Shared Task", "start_pos": 137, "end_pos": 161, "type": "DATASET", "confidence": 0.7870728015899658}]}, {"text": "We release all of our code and materials for reproducibility.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction (GEC) is the task of correcting various grammatical errors in text, as illustrated by the following example: by bus is, and annoying.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8178547223409017}]}, {"text": "While the dominant approach following the has been different adaptations of phrase-based and statistical machine translation (PBSMT) models, more recent work on GEC increasingly adopted partial () or exclusive () use of deep sequence-to-sequence (seq2seq) architectures (), which showed immense success in neural machine translation (NMT) ().", "labels": [], "entities": [{"text": "statistical machine translation (PBSMT)", "start_pos": 93, "end_pos": 132, "type": "TASK", "confidence": 0.7824661036332449}, {"text": "GEC", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.8087722063064575}, {"text": "neural machine translation (NMT)", "start_pos": 306, "end_pos": 338, "type": "TASK", "confidence": 0.8176940083503723}]}, {"text": "In GEC, unlike NMT between major languages, there are not enough publicly available corpora (GEC's hundreds of thousands to NMT's tens of millions).", "labels": [], "entities": [{"text": "GEC", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.9296260476112366}]}, {"text": "This motivates the use of pre-training and transfer learning, which has shown to be highly effective in many natural language processing (NLP) scenarios in which there is not enough annotated data, notably in low-resource machine translation (MT) (.", "labels": [], "entities": [{"text": "low-resource machine translation (MT)", "start_pos": 209, "end_pos": 246, "type": "TASK", "confidence": 0.8057257930437723}]}, {"text": "As a result, recent GEC systems also include pre-training on various auxiliary tasks, such as language modeling (LM)), text revision (, and denoising (.", "labels": [], "entities": [{"text": "language modeling (LM))", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.8270807027816772}, {"text": "text revision", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.7952356040477753}]}, {"text": "In this paper, we introduce a neural GEC system that combines the power of pre-training and transfer learning.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows: \u2022 We pre-train our model for the denoising task using a novel noising function, which gives us a parallel corpus that includes realistic grammatical errors; \u2022 We leverage the idea of sequential transfer learning, thereby effectively adapting our pre-trained model to the domain as well as the writing and annotation styles suitable for our final task.", "labels": [], "entities": [{"text": "sequential transfer learning", "start_pos": 228, "end_pos": 256, "type": "TASK", "confidence": 0.7878031730651855}]}, {"text": "\u2022 We introduce a context-aware neural spellchecker, which improves upon an off-the-shelf spellchecker by incorporating context into spellchecking using a pre-trained neural language model (LM).", "labels": [], "entities": []}], "datasetContent": [{"text": "Throughout our experiments, we use fairseq 3 (Ott et al., 2019), a publicly available sequenceto-sequence modeling toolkit based on PyTorch (.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.9179383516311646}]}, {"text": "Specifically, we take fairseq-0.6.1 and add our own implementations of a copy-augmented transformer model as well as several GEC-specific auxiliary losses.", "labels": [], "entities": []}, {"text": "In, we summarize all relevant data sources, their sizes, whether they are public, and the number of annotators.", "labels": [], "entities": []}, {"text": "For pre-training, we use the Gutenberg dataset, the Tatoeba 4 dataset, and the WikiText-103 dataset ().", "labels": [], "entities": [{"text": "Gutenberg dataset", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.7086343169212341}, {"text": "Tatoeba 4 dataset", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9472453395525614}, {"text": "WikiText-103 dataset", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.941065102815628}]}, {"text": "We learned through initial experiments that the quality of pre-training data is crucial to the final model's performance, because our DAE model assumes \u00a74 that these unannotated corpora contain little grammatical errors.", "labels": [], "entities": []}, {"text": "Our choice of corpora is based on both the quality and diversity of text: Guten- Gutenberg, Tatoeba, WikiText-103: Datasets used for each set of results.", "labels": [], "entities": [{"text": "Tatoeba", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9352105855941772}]}, {"text": "For the W&I+L development set, Dev-3K and Dev-1K respectively indicate a 3:1 train-test random split of the development set, such that the original proportions of English proficiency (A, B, C, N) are kept the same in each split.", "labels": [], "entities": [{"text": "W&I+L development set", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.7848922099385943}]}, {"text": "See for more information about each dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: ACL 2019 BEA Workshop Restricted Track results. For each training step, we only list results from the  model configuration that achieved the best F 0.5 test set score. All evaluation is done using ERRANT's span-based  correction scorer. Pre-processing and post-processing are included in the first step and last steps, respectively.", "labels": [], "entities": [{"text": "ACL 2019 BEA Workshop Restricted Track", "start_pos": 10, "end_pos": 48, "type": "DATASET", "confidence": 0.8492584725220998}, {"text": "F 0.5 test set score", "start_pos": 156, "end_pos": 176, "type": "METRIC", "confidence": 0.9489879250526428}]}, {"text": " Table 5: Results on CoNLL-2014 as point of comparison. \"W&I+L\" indicates whether the approach made use of  the (newly released) W&I+L dataset. Evaluation is done using the MaxMatch (M 2 ) scorer, rather than ERRANT.  Pre-processing & post-processing are included before the first step and after the last step, respectively. See  \u00a77.7  for details and references.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8943988680839539}, {"text": "W&I+L dataset", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.6251971970001856}, {"text": "MaxMatch (M 2 ) scorer", "start_pos": 173, "end_pos": 195, "type": "METRIC", "confidence": 0.5411204198996226}, {"text": "ERRANT", "start_pos": 209, "end_pos": 215, "type": "METRIC", "confidence": 0.956642210483551}]}, {"text": " Table 6: Comparison of realistic and random error gen- eration on Restricted Track. \u2206 means the difference  between Ours and Random.", "labels": [], "entities": []}, {"text": " Table 7: Comparison of realistic and random error gen- eration on Low Resource Track. \u2206 means the differ- ence between Ours and Random.", "labels": [], "entities": []}, {"text": " Table 8: Effect of incorporating context into a standard  spellchecker.", "labels": [], "entities": []}, {"text": " Table 9: Comparing the average number of edits per  sentence, normalized by sentence length, between the  W&I training set and other available training data  sources for the Restricted Track. \"vs. W&I\" refers  to the result of an approximate permutation test (10k  rounds) against that in the W&I training set. Under the  significance level of \u03b1 = 0.05, the number for FCE,  NUCLE, and Lang-8 are all significantly different from  that for the W&I training set.", "labels": [], "entities": [{"text": "W&I training set", "start_pos": 107, "end_pos": 123, "type": "DATASET", "confidence": 0.8377450108528137}, {"text": "W&I training set", "start_pos": 294, "end_pos": 310, "type": "DATASET", "confidence": 0.7470943450927734}, {"text": "FCE", "start_pos": 370, "end_pos": 373, "type": "DATASET", "confidence": 0.5579745769500732}, {"text": "NUCLE", "start_pos": 376, "end_pos": 381, "type": "DATASET", "confidence": 0.6488032341003418}, {"text": "W&I training set", "start_pos": 445, "end_pos": 461, "type": "DATASET", "confidence": 0.817073404788971}]}, {"text": " Table 10: Results on error types.", "labels": [], "entities": []}, {"text": " Table 11: Training progress on CoNLL-2014. No  W&I+Locness datasets were used in these results. 'b'", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.9038931131362915}, {"text": "W&I+Locness datasets", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.45917338877916336}]}, {"text": " Table 12: Single-model ERRANT scores for Re- stricted Track, using a large Transformer and a copy- augmented Transformer.", "labels": [], "entities": [{"text": "ERRANT", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9362678527832031}, {"text": "Re- stricted Track", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.47866567969322205}]}]}