{"title": [{"text": "NRC Parallel Corpus Filtering System for WMT 2019", "labels": [], "entities": [{"text": "WMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8343729376792908}]}], "abstractContent": [{"text": "We describe the National Research Council Canada team's submissions to the parallel corpus filtering task at the Fourth Conference on Machine Translation.", "labels": [], "entities": [{"text": "National Research Council Canada team", "start_pos": 16, "end_pos": 53, "type": "DATASET", "confidence": 0.9101236462593079}, {"text": "parallel corpus filtering task at the Fourth Conference on Machine Translation", "start_pos": 75, "end_pos": 153, "type": "TASK", "confidence": 0.7405357496304945}]}], "introductionContent": [{"text": "The WMT19 shared task on parallel corpus filtering was essentially the same as last year's edition (), except under lowresource conditions: the language pairs were Nepali-English and Sinhala-English instead of German-English, and the data participants were allowed to use was constrained.", "labels": [], "entities": [{"text": "WMT19", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7174843549728394}, {"text": "parallel corpus filtering", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.5601244966189066}]}, {"text": "The aim of the challenge was to identify high-quality sentence pairs in a noisy corpus crawled from the web using ParaCrawl (, in order to train machine translation (MT) systems on the clean data.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.8455053329467773}]}, {"text": "Specifically, participating systems must produce a score for each sentence pair in the test corpora, this score indicating the quality of that pair.", "labels": [], "entities": []}, {"text": "Then samples containing 1M or 5M words would be used to train MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9912410378456116}]}, {"text": "Participants were ranked based on the performance of these MT systems on a test set of Wikipedia translations, as measured by BLEU ().", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9788180589675903}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9973050355911255}]}, {"text": "Participants were provided with a few small sources of parallel data, covering different domains, for each of the two low-resource languages, as well as a third, related language, Hindi (which uses the same script as Nepali).", "labels": [], "entities": []}, {"text": "The provided data also included much larger monolingual corpora for each of the four languages (en, hi, ne, si).", "labels": [], "entities": []}, {"text": "Cleanliness or quality of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9888961315155029}]}, {"text": "Previous work ( showed that different types of errors in the parallel training data degrade MT quality in different ways.", "labels": [], "entities": [{"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9912241697311401}]}, {"text": "Intuitively, cross-lingual semantic textual similarity is one of the most important properties of high-quality sentence pairs.", "labels": [], "entities": [{"text": "cross-lingual semantic textual similarity", "start_pos": 13, "end_pos": 54, "type": "TASK", "confidence": 0.6556452214717865}]}, {"text": "scored cross-lingual semantic textual similarity in two ways, either using a semantic MT quality estimation metric, or by first translating one of the sentences using MT, and then comparing the result to the other sentence, using a semantic MT evaluation metric.", "labels": [], "entities": [{"text": "cross-lingual semantic textual similarity", "start_pos": 7, "end_pos": 48, "type": "TASK", "confidence": 0.6357235759496689}]}, {"text": "At last year's edition of the corpus filtering task,'s supervised submissions were developed in the same philosophy using anew semantic MT evaluation metric, YiSi-1.", "labels": [], "entities": [{"text": "corpus filtering task", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.8353731830914816}, {"text": "YiSi-1", "start_pos": 158, "end_pos": 164, "type": "DATASET", "confidence": 0.9217049479484558}]}, {"text": "This year, the National Research Council (NRC) Canada team submitted 4 systems to the corpus filtering task, which use different strategies to evaluate the parallelism of sentence pairs.", "labels": [], "entities": [{"text": "National Research Council (NRC) Canada team", "start_pos": 15, "end_pos": 58, "type": "DATASET", "confidence": 0.8658998236060143}, {"text": "corpus filtering task", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.8175600171089172}]}, {"text": "Two of these systems exploit the quality estimation metric YiSi-2, the third uses a deep Transformer network (, and the fourth is an ensemble combining these approaches.", "labels": [], "entities": [{"text": "YiSi-2", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.8716781735420227}]}, {"text": "In this paper, we describe the 4 systems we submitted, which have three main components: pre-filtering rules, sentence pair scoring, and reranking to improve vocabulary coverage.", "labels": [], "entities": [{"text": "sentence pair scoring", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7407037019729614}]}, {"text": "The systems vary in the way they score sentence pairs.", "labels": [], "entities": []}, {"text": "Official results indicate our best systems were ranked 3rd or 4th out of over 20 submissions inmost test settings, the ensemble system providing the most robust results.", "labels": [], "entities": []}], "datasetContent": [{"text": "YiSi 2 is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources.", "labels": [], "entities": [{"text": "MT quality evaluation and estimation", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.8229647874832153}]}, {"text": "YiSi-1 measures the similarity between a machine translation and human references by aggregating weighted distributional (lexical) semantic similarities, and optionally incorporating shallow semantic structures.", "labels": [], "entities": []}, {"text": "YiSi-2 is the bilingual, reference-less version, which uses bilingual word embeddings to evaluate cross-lingual lexical semantic similarity between the input and MT output.", "labels": [], "entities": [{"text": "YiSi-2", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8775565028190613}]}, {"text": "While YiSi-1 successfully served in the WMT2018 parallel corpus filtering task, YiSi-2 showed comparable accuracy on identifying clean parallel sentences on a handannotated subset of test data in our internal experiments (.", "labels": [], "entities": [{"text": "WMT2018 parallel corpus filtering task", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.6735447585582733}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9978448152542114}]}, {"text": "Like YiSi-1, YiSi-2 can exploit shallow semantic structures as well.", "labels": [], "entities": []}, {"text": "However, there is no semantic role labeler for Nepali/Sinhalese readily available off-the-shelf, thus the version of YiSi-2 used in this work is purely based on cross-lingual lexical semantic similarity.", "labels": [], "entities": []}, {"text": "In addition, instead of evaluating through the bag of trigrams to reward the same word order between the two sentences as in YiSi-1, YiSi-2 evaluates through the bag of unigrams to allow reordering between the two sentences in the two languages.", "labels": [], "entities": []}, {"text": "Here is a simplified version of YiSi without using shallow semantic structures and bag of n-grams (it is the same as the original version of YiSi (Lo, 2019) with the hyperparameter \u03b2 set to 0 and n to 1): where U is the set of all tested sentences in the same language of the word unit u; \u03b1 is the ratio of precision and recall in the final YiSi score.", "labels": [], "entities": [{"text": "precision", "start_pos": 307, "end_pos": 316, "type": "METRIC", "confidence": 0.9990598559379578}, {"text": "recall", "start_pos": 321, "end_pos": 327, "type": "METRIC", "confidence": 0.9986760020256042}]}, {"text": "In this experiment, we set \u03b1 to 0.5 fora balanced ratio of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9997174143791199}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9988110065460205}]}, {"text": "This year, we experimented with two methods to build the bilingual word embeddings for evaluating cross-lingual lexical semantic similarity in YiSi-2.", "labels": [], "entities": [{"text": "cross-lingual lexical semantic similarity", "start_pos": 98, "end_pos": 139, "type": "TASK", "confidence": 0.6217749118804932}]}, {"text": "The supervised bilingual word embeddings are trained on the parallel data provided using bivec ().", "labels": [], "entities": []}, {"text": "The unsupervised (weakly supervised, to be precise) bilingual word embeddings are built by transforming monolingual) embeddings of each language into the same vector space using vecmap (.", "labels": [], "entities": []}, {"text": "shows the statistics of the data used to train the two bilingual word embedding models.", "labels": [], "entities": []}, {"text": "Common Crawl data was not used to train the bilingual word embeddings.", "labels": [], "entities": [{"text": "Common Crawl data", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8566527565320333}]}, {"text": "To evaluate the deep Transformer model intrinsically, we can look at its accuracy on the sentence pair classification task used to fine-tune it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.999508261680603}, {"text": "sentence pair classification", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.6820771793524424}]}, {"text": "Table 3 shows the accuracy on the dev sets for all three language pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9996439218521118}]}, {"text": "The table shows the results obtained using 4 different configurations for training, with the confounding sentences being drawn either from the training data or test data, and using either sentence pair classification (SPC) only or both SPC and the (monolingual) masked language model (MLM) for fine-tuning.", "labels": [], "entities": [{"text": "sentence pair classification", "start_pos": 188, "end_pos": 216, "type": "TASK", "confidence": 0.6651617685953776}]}, {"text": "First, we see that the accuracy scores are high, 9 so the model is good at discriminating real translations from procedurally generated bad ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9992782473564148}]}, {"text": "The results also suggest that including the (monolingual) MLM task during fine-tuning is a hindrance, since the model achieves lower accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9959828853607178}]}, {"text": "However, it is important to note that we did no hyperparameter tuning, had to use a smaller model because of time and resource limitations, and did not have time to fully train any of the models tested.", "labels": [], "entities": []}, {"text": "More extensive testing would be required to assess the usefulness of multi-task finetuning.", "labels": [], "entities": []}, {"text": "If we analyze the scores output by the model on the test data (i.e. the predicted probability of SMT NMT langs system 1M-word 5M-word 1M-word 5M-word ne-en YiSi-2-sup 3.55   the positive class), we see that the model predicts that avast majority of sentences pairs are not valid translations of each other, their score being below 0.5.", "labels": [], "entities": [{"text": "SMT NMT", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.8224978744983673}]}, {"text": "We briefly inspected the top-scoring sentences in the test set, and in the case of ne-en, these seem to contain a lot of biblical texts, which suggests a domain bias, as the ne-en fine-tuning data included biblical texts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Sentence pair classification accuracy of XLM model on dev sets. Confounders are sentences that we draw  at random to create inadequate translations.", "labels": [], "entities": [{"text": "Sentence pair classification", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.833876371383667}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9071006178855896}]}, {"text": " Table 4: Uncased BLEU scores on the official dev (\"dev-test\") sets achieved by the SMT systems trained on the  1M-and 5M-word corpora subselected by the scoring systems. For XLM, v2 is the version that selects confounders  from the test corpora, whereas v3 selects them from the training data, and spc-mlm means that both SPC and MLM  were used for fine-tuning. *These results for the Zipporah baseline were reported by the task organizers, and the  SMT architecture was different from our systems. We obtained Zipporah's score lists and trained our own SMT  systems using the data selected from those lists, and results are shown in the third row.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9764094352722168}, {"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9599084258079529}]}]}