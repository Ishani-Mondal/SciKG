{"title": [{"text": "Assessing the Ability of Self-Attention Networks to Learn Word Order", "labels": [], "entities": [{"text": "Learn Word Order", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.5349285006523132}]}], "abstractContent": [{"text": "Self-attention networks (SAN) have attracted a lot of interests due to their high parallelization and strong performance on a variety of NLP tasks, e.g. machine translation.", "labels": [], "entities": [{"text": "Self-attention networks (SAN)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6353989362716674}, {"text": "machine translation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.8267191052436829}]}, {"text": "Due to the lack of recurrence structure such as recurrent neural networks (RNN), SAN is ascribed to be weak at learning positional information of words for sequence modeling.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.6943165063858032}]}, {"text": "However, neither this speculation has been empirically confirmed , nor explanations for their strong performances on machine translation tasks when \"lacking positional information\" have been explored.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.8082656661669413}]}, {"text": "To this end, we propose a novel word reordering detection task to quantify how well the word order information learned by SAN and RNN.", "labels": [], "entities": [{"text": "word reordering detection", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.8402290940284729}]}, {"text": "Specifically, we randomly move one word to another position, and examine whether a trained model can detect both the original and inserted positions.", "labels": [], "entities": []}, {"text": "Experimental results reveal that: 1) SAN trained on word reordering detection indeed has difficulty learning the positional information even with the position embedding; and 2) SAN trained on machine translation learns better positional information than its RNN counterpart, in which position embedding plays a critical role.", "labels": [], "entities": [{"text": "word reordering detection", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.8570020198822021}, {"text": "machine translation", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.8014554381370544}]}, {"text": "Although recurrence structure make the model more universally-effective on learning word order, learning objectives matter more in the downstream tasks such as machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.7595590651035309}]}], "introductionContent": [{"text": "Self-attention networks) have shown promising empirical results in a variety of natural language processing (NLP) tasks, such as machine translation), semantic role labelling (, and language representations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7814719974994659}, {"text": "semantic role labelling", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.6392892301082611}]}, {"text": "The popularity of SAN lies in its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals.", "labels": [], "entities": [{"text": "SAN", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9472048282623291}]}, {"text": "Position embedding) is generally deployed to capture sequential information for SAN (.", "labels": [], "entities": []}, {"text": "Recent studies claimed that SAN with position embedding is still weak at learning word order information, due to the lack of recurrence structure that is essential for sequence modeling).", "labels": [], "entities": []}, {"text": "However, such claims are mainly based on a theoretical argument, which have not been empirically validated.", "labels": [], "entities": []}, {"text": "In addition, this cannot explain well why SAN-based models outperform their RNN counterpart in machine translation -a benchmark sequence modeling task (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7585911154747009}]}, {"text": "Our goal in this work is to empirically assess the ability of SAN to learn word order.", "labels": [], "entities": []}, {"text": "We focus on asking the following research questions: Q1: Is recurrence structure obligate for learning word order, and does the conclusion hold in different scenarios (e.g., translation)?", "labels": [], "entities": []}, {"text": "Q2: Is the model architecture the critical factor for learning word order in the downstream tasks such as machine translation?", "labels": [], "entities": [{"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7719831466674805}]}, {"text": "Q3: Is position embedding powerful enough to capture word order information for SAN?", "labels": [], "entities": []}, {"text": "We approach these questions with a novel probing task -word reordering detection (WRD), which aims to detect the positions of randomly reordered words in the input sentence.", "labels": [], "entities": [{"text": "word reordering detection (WRD)", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.7945512433846792}]}, {"text": "We compare SAN with RNN, as well as directional SAN) that augments SAN with recurrence modeling.", "labels": [], "entities": []}, {"text": "In this study, we focus on the encoders implemented with different architectures, so as to investigate their abilities to learn: Illustration of (a) the position detector, where (b) the output layer is build upon a randomly initialized or pre-trained encoder.", "labels": [], "entities": []}, {"text": "In this example, the word \"hold\" is moved to another place.", "labels": [], "entities": []}, {"text": "The goal of this task is to predict the inserted position \"I\" and the original position \"O\" of \"hold\".", "labels": [], "entities": [{"text": "inserted position \"I", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.8499133586883545}]}, {"text": "word order information of the input sequence.", "labels": [], "entities": []}, {"text": "The encoders are trained on objectives like detection accuracy and machine translation, to study the influences of learning objectives.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8681758046150208}, {"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7680165469646454}]}, {"text": "Our experimental results reveal that: (Q1) SAN indeed underperforms the architectures with recurrence modeling (i.e. RNN and DiSAN) on the WRD task, while this conclusion does not hold in machine translation: SAN trained with the translation objective outperforms both RNN and DiSAN on detection accuracy; (Q2) Learning objectives matter more than model architectures in downstream tasks such as machine translation; and (Q3) Position encoding is good enough for SAN in machine translation, while DiSAN is a more universally-effective mechanism to learn word order information for SAN.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.7831485569477081}, {"text": "accuracy", "start_pos": 296, "end_pos": 304, "type": "METRIC", "confidence": 0.9212188720703125}, {"text": "machine translation", "start_pos": 396, "end_pos": 415, "type": "TASK", "confidence": 0.8143421113491058}, {"text": "machine translation", "start_pos": 470, "end_pos": 489, "type": "TASK", "confidence": 0.7341314107179642}]}], "datasetContent": [{"text": "We return to the central questions originally posed, that is, whether SAN is indeed weak at learning positional information.", "labels": [], "entities": [{"text": "SAN", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9774126410484314}]}, {"text": "Using the above experimental design, we give the following answers: A1: SAN-based encoder trained on the WRD data is indeed harder to learn positional information than the recurrence architectures (Section 4.1), while there is no evidence that: Accuracy on the WRD task.", "labels": [], "entities": [{"text": "WRD data", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.7461987435817719}, {"text": "Accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9736798405647278}, {"text": "WRD task", "start_pos": 261, "end_pos": 269, "type": "TASK", "confidence": 0.8001909554004669}]}, {"text": "\"Insert\" and \"Original\" denotes the accuracies of detecting the inserted and original positions respectively, and \"Both\" denotes detecting both positions.).", "labels": [], "entities": [{"text": "Insert", "start_pos": 1, "end_pos": 7, "type": "METRIC", "confidence": 0.990151047706604}]}], "tableCaptions": [{"text": " Table 1: Accuracy on the WRD task. \"Insert\" and  \"Original\" denotes the accuracies of detecting the in- serted and original positions respectively, and \"Both\"  denotes detecting both positions.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978600144386292}, {"text": "WRD task", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.6372868418693542}, {"text": "Insert", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9681678414344788}]}, {"text": " Table 2: Performances of NMT encoders pre-trained on WMT14 En\u21d2De and WAT17 En\u21d2Ja data. \"Translation\"  denotes translation quality measured in BLEU scores, while \"Detection\" denotes the accuracies on WRD task.  \"En\u21d2De Enc.\" denotes NMT encoder trained with translation objective on the En\u21d2De data. We also list the  detection accuracies of WRD encoders (\"WRD Enc.\") for comparison. \"-Pos Emb\" indicates removing posi- tional embeddings from SAN-or DiSAN-based encoder. Surprisingly, SAN-based NMT encoder achieves the best  accuracy on the WRD task, which contrasts with the performances of WRD encoders (the last column).", "labels": [], "entities": [{"text": "WMT14", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.8917811512947083}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.998599112033844}, {"text": "accuracy", "start_pos": 524, "end_pos": 532, "type": "METRIC", "confidence": 0.997226893901825}]}]}