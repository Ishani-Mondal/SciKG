{"title": [{"text": "Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System", "labels": [], "entities": [{"text": "Attribute Aware Response Generation", "start_pos": 12, "end_pos": 47, "type": "TASK", "confidence": 0.5923475325107574}]}], "abstractContent": [{"text": "Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "The state-of-the-art dialogue systems are primarily based on uni-modal sources, predominantly the text, and hence cannot capture the information present in the other sources such as videos, audios, images etc.", "labels": [], "entities": []}, {"text": "With the availability of large scale multimodal dialogue dataset (MMD) (Saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user.", "labels": [], "entities": []}, {"text": "Without capturing the information from both the text and image, the system will be incapable of generating correct and desirable responses.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance.", "labels": [], "entities": []}, {"text": "Our evaluation shows that the proposed model can generate appropriate responses while preserving the position and attribute information.", "labels": [], "entities": []}, {"text": "Experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the advancement in Artificial Intelligence (AI), dialogue systems have become a prominent part in today's virtual assistant, which helps users to converse naturally with the system for effective task completion.", "labels": [], "entities": []}, {"text": "Dialogue systems focus on two broad categories -open domain conversations with casual chitchat and goal-oriented systems where the system is designed to solve a particular task for the user belonging to a specific domain.", "labels": [], "entities": []}, {"text": "Response generation is a crucial component of every conversational agent.", "labels": [], "entities": [{"text": "Response generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8968513607978821}]}, {"text": "The task of \"how to say\" * * First two authors are jointly the first authors the information to the user is the primary objective of every response generation module.", "labels": [], "entities": [{"text": "response generation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.730585366487503}]}, {"text": "One of the running goals of AI is to bring language and vision together in building robust dialogue systems.", "labels": [], "entities": []}, {"text": "Advances in visual question answering (VQA) (, and image captioning ( have ensured interdisciplinary research in natural language processing (NLP) and computer vision.", "labels": [], "entities": [{"text": "visual question answering (VQA)", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.8057059248288473}, {"text": "image captioning", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7262085676193237}]}, {"text": "Recently, several works in dialogue systems incorporating both vision and language () have shown promising research directions.", "labels": [], "entities": []}, {"text": "Goal oriented dialogue systems are majorly based on textual data (unimodal source).", "labels": [], "entities": []}, {"text": "With increasing demands in the domains like retail, travel, entertainment, conversational agents that can converse by combining different modalities is an essential requirement for building the robust systems.", "labels": [], "entities": []}, {"text": "Knowledge from different modalities carries complementary information about the various aspects of a product, event or activity of interest.", "labels": [], "entities": []}, {"text": "By combining information from different modalities to learn better representation is crucial for creating robust dialogue systems.", "labels": [], "entities": []}, {"text": "Ina multimodal setup, the provision of different modalities assists both the user and the agent in achieving the desired goal.", "labels": [], "entities": []}, {"text": "Our work is established upon the recently proposed Multimodal Dialogue (MMD) dataset (, consisting of ecommerce (fashion domain) related conversations.", "labels": [], "entities": []}, {"text": "The work focused on generating textual responses conditioned on the conversational history consisting of both text and image.", "labels": [], "entities": []}, {"text": "In the existing task-oriented dialogue systems, the inclusion of visually grounded dialogues-as in the case of MMD dataset-has provided exciting new challenges in the field of interactive dialogue systems.", "labels": [], "entities": [{"text": "MMD dataset-has", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.7259725481271744}]}, {"text": "In contrast to VQA, multimodal dialogues have conversations with more extended contextual dependency, and a clear end-goal.", "labels": [], "entities": []}, {"text": "As opposed to a static image in VQA, MMD deals with dynamic images making the task even more challenging.", "labels": [], "entities": [{"text": "VQA", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.854205846786499}]}, {"text": "In comparison to the previous slotfilling dialogue systems on textual data (), MMD provides an additional visual modality to drive the conversation forward.", "labels": [], "entities": [{"text": "slotfilling dialogue", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.8917131125926971}]}, {"text": "In this work, we propose an entirely data-driven response generation model in a multi-modal setup by combining the modalities of text and images.", "labels": [], "entities": [{"text": "response generation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.693260133266449}]}, {"text": "In, we present an example from the MMD dataset.", "labels": [], "entities": [{"text": "MMD dataset", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8679920136928558}]}, {"text": "It is a conversation between the user and the system in a multimodal setting on the fashion domain.", "labels": [], "entities": []}, {"text": "From the example, it is understood that the position of images is essential for the system to fulfill the demands of the user.", "labels": [], "entities": []}, {"text": "For example, in figure, the U3 utterance \"Can you tell me the type of colour in the 1st image\" needs position information of the particular image from the given set of images.", "labels": [], "entities": []}, {"text": "To handle such situations, we incorporate position embeddings to capture ordered visual information.", "labels": [], "entities": []}, {"text": "The underlying motivation was to capture the correct image information from the text; hence, we use position aware attention mechanism.", "labels": [], "entities": []}, {"text": "From, in utterance U5, we can see that the user is keen on different aspects of the image as well.", "labels": [], "entities": []}, {"text": "In this case, user is interested in the \"print as in the 2nd image\".", "labels": [], "entities": []}, {"text": "To focus and capture the different attributes from the image representation being considered in the text, we apply attribute aware attention on the image representation.", "labels": [], "entities": []}, {"text": "Hence in order to handle such situations present in the dataset, we apply both position and attribute aware attention mechanisms to capture intricate details from the image and textual features.", "labels": [], "entities": []}, {"text": "For effective interaction among the modalities, we use Multimodal Factorized Bilinear (MFB) () pooling mechanism.", "labels": [], "entities": []}, {"text": "Since multimodal feature distribution varies dramatically, hence the integrated image-text representations obtained by such linear models may not be sufficient in capturing the complex interactions between the visual and textual modalities.", "labels": [], "entities": []}, {"text": "The information of the present utterance, image and the contextual history are essential for better response generation (.", "labels": [], "entities": [{"text": "response generation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7255337983369827}]}, {"text": "The key contributions/highlights of our current work are as follows: \u2022 We employ a position-aware attention mechanism to incorporate the ordered visual information and attribute-aware attention mechanism to focus on image conditioned on the attributes discussed in the text.", "labels": [], "entities": []}, {"text": "\u2022 We utilize Multi-modal Factorized Bilinear (MFB) model to fuse the contextual information along with image and utterance representation.", "labels": [], "entities": []}, {"text": "\u2022 We achieve state-of-the-art performance for the textual response generation task on the MMD dataset.", "labels": [], "entities": [{"text": "textual response generation task", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.7707239836454391}, {"text": "MMD dataset", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.8779124915599823}]}, {"text": "The rest of the paper is structured as follows: In section 2, we discuss the related works.", "labels": [], "entities": []}, {"text": "In Section 3, we explain the proposed methodology followed by the dataset description in Section 4.", "labels": [], "entities": []}, {"text": "Experimental details and evaluation metrics are reported in Section 5.", "labels": [], "entities": []}, {"text": "Results along with necessary analysis are presented in Section 6.", "labels": [], "entities": []}, {"text": "In Section 7, we conclude the paper along with future research direction.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our work is built upon the Multimodal Dialogue (MMD) dataset ().", "labels": [], "entities": [{"text": "Multimodal Dialogue (MMD) dataset", "start_pos": 27, "end_pos": 60, "type": "DATASET", "confidence": 0.631340021888415}]}, {"text": "The MMD dataset comprises of 150k chat sessions between the customer and sales agent.", "labels": [], "entities": [{"text": "MMD dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7491708695888519}]}, {"text": "lists the detailed information about the MMD dataset.", "labels": [], "entities": [{"text": "MMD dataset", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.8579460978507996}]}, {"text": "Domain-specific knowledge in the fashion domain was captured during the series of customer-agent interactions.", "labels": [], "entities": []}, {"text": "The dialogues incorporate text and image information seamlessly in a conversation bringing together multiple modalities for creating advanced dialogue systems.", "labels": [], "entities": []}, {"text": "The dataset poses new challenges for multimodal, goal-oriented dialogue containing complex user utterances.", "labels": [], "entities": []}, {"text": "For example, \"Can you show me the 5th image in different orientations within my budget?\", requires quantitative inference such as filtering, counting and sorting.", "labels": [], "entities": []}, {"text": "Bringing the textual and image modalities together, multimodal inference makes the task of generation even more challenging, for example, \"See the second stilettos, I want to see more like it but in a different colour\".", "labels": [], "entities": []}, {"text": "In our work,  In this section we present the implementation details and the evaluation metrics (automatic and human) that we use for measuring the model performance.", "labels": [], "entities": []}, {"text": "For evaluating the model we report the standard metrics like BLEU-4 (), ROUGE-L () and METEOR (Lavie and Agarwal, 2007) employing the evaluation scripts made available by (Sharma et al., 2017).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9986094236373901}, {"text": "ROUGE-L", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9974341988563538}, {"text": "METEOR", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.995935320854187}]}, {"text": "To understand the quality of responses, we adopt human evaluation to compare the performance of different models.", "labels": [], "entities": []}, {"text": "We randomly sample 700 responses from the test set for human evaluation.", "labels": [], "entities": []}, {"text": "Given an utterance, image along with the conversation history were presented to three human annotators, with post-graduate level of exposure.", "labels": [], "entities": []}, {"text": "They were asked to measure the correctness and relevance of the responses generated by the different models with respect to the following three metrics: The generated response is grammatically correct and is free of any errors.", "labels": [], "entities": []}, {"text": "2. Relevance (R): The generated response is in accordance to the aspect being discussed (style, colour, material, etc.), and contains the information with respect to the conversational history.", "labels": [], "entities": [{"text": "Relevance (R)", "start_pos": 3, "end_pos": 16, "type": "METRIC", "confidence": 0.8967618346214294}]}, {"text": "Also, there is no loss of attributes/information in the generated response.", "labels": [], "entities": []}, {"text": "We follow the scoring scheme for fluency and relevance as-0: incorrect or incomplete, 1: moderately correct, and 2: correct.", "labels": [], "entities": [{"text": "relevance", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9590219259262085}]}, {"text": "We compute the Fleiss' kappa for the above metrics to measure inter-rater consistency.", "labels": [], "entities": []}, {"text": "The kappa score for fluency is 0.75 and relevance is 0.77 indicating \"substantial agreement\".", "labels": [], "entities": [{"text": "kappa score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9421387612819672}, {"text": "relevance", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.997738242149353}]}, {"text": "Results of the different models are presented in Table 2.", "labels": [], "entities": []}, {"text": "The proposed model performs better than the other baselines for all the evaluation metrics, and we find this improvement to be statistically    In, we show the attention visualization to demonstrate the effectiveness of our proposed position and attribute aware attention mechanisms.", "labels": [], "entities": []}, {"text": "Example 1 in the figure shows that the model can focus on the correct image (in this case, the 3rd image) with the help of position-aware attention mechanism as the focus is given to the word 3rd in the utterance.", "labels": [], "entities": []}, {"text": "Example 2 shows the effect of both position and attribute aware attention mechanism that helps in more accurate response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7161322236061096}]}, {"text": "The positional word 2nd along with the attribute rubber has obtained maximum focus in the given example.", "labels": [], "entities": [{"text": "focus", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9551975727081299}]}, {"text": "While in Example 3, we can seethe effect of attribute aware attention mechanism with maximum attention given to the keywords such as dark, red, frame in the utterance.", "labels": [], "entities": []}, {"text": "In, we present the evaluation results of human.", "labels": [], "entities": []}, {"text": "In case of fluency, the baseline MHRED model and the proposed model have shown quite similar performance.", "labels": [], "entities": [{"text": "MHRED", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.7590638995170593}]}, {"text": "While for the relevance metric our proposed model has shown better performance with an improvement of 7.47% in generating the correct responses.", "labels": [], "entities": []}, {"text": "This maybe due the reason that our proposed model focuses on the relevant information in the text as well as the image, and generate more accurate and informative responses.", "labels": [], "entities": []}, {"text": "All the results are statistically significant as we perform Welch's t-test and it is conducted at 5% (0.05) significance level.", "labels": [], "entities": [{"text": "significance level", "start_pos": 108, "end_pos": 126, "type": "METRIC", "confidence": 0.9503751993179321}]}], "tableCaptions": [{"text": " Table 1: Dataset statistics of MMD", "labels": [], "entities": [{"text": "MMD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9468602538108826}]}, {"text": " Table 3: Human evaluation results for Fluency and Rel- evance (All values are in percentages.)", "labels": [], "entities": [{"text": "Fluency", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.8421367406845093}, {"text": "Rel- evance", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.958069920539856}]}]}