{"title": [], "abstractContent": [{"text": "The paper presents a first attempt towards un-supervised neural text simplification that relies only on unlabeled text corpora.", "labels": [], "entities": [{"text": "un-supervised neural text simplification", "start_pos": 43, "end_pos": 83, "type": "TASK", "confidence": 0.7164207398891449}]}, {"text": "The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and de-noising.", "labels": [], "entities": []}, {"text": "The framework is trained using unla-beled text collected from en-Wikipedia dump.", "labels": [], "entities": []}, {"text": "Our analysis (both quantitative and qualitative involving human evaluators) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods.", "labels": [], "entities": []}, {"text": "It also outperforms viable un-supervised baselines.", "labels": [], "entities": []}, {"text": "Adding a few labeled pairs helps improve the performance further.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text Simplification (TS) deals with transforming the original text into simplified variants to increase its readability and understandability.", "labels": [], "entities": [{"text": "Text Simplification (TS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8457954883575439}]}, {"text": "TS is an important task in computational linguistics, and has numerous use-cases in fields of education technology, targeted content creation, language learning, where producing variants of the text with varying degree of simplicity is desired.", "labels": [], "entities": [{"text": "TS", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9567789435386658}, {"text": "computational linguistics", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.7464447915554047}, {"text": "targeted content creation", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.7419719099998474}]}, {"text": "TS systems are typically designed to simplify from two different linguistic aspects: (a) Lexical aspect, by replacing complex words in the input with simpler The crux of the (unsupervised) auto-encoding framework is a shared encoder and a pair of attention-based decoders (one for each type of corpus).", "labels": [], "entities": []}, {"text": "The encoder attempts to produce semanticspreserving representations which can be acted upon by the respective decoders (simple or complex) to generate the appropriate text output they are designed for.", "labels": [], "entities": []}, {"text": "The framework is crucially supported by two kinds of losses: (1) adversarial loss -to distinguish between the real or fake attention context vectors for the simple decoder, and (2) diversification loss -to distinguish between attention context vectors of the simple decoder and the complex decoder.", "labels": [], "entities": []}, {"text": "The first loss ensures that only the aspects of semantics that are necessary for simplification are passed to the simple decoder in the form of the attention context vectors.", "labels": [], "entities": []}, {"text": "The second loss, on the other hand, facilitates passing different semantic aspects to the different decoders through their respective context vectors.", "labels": [], "entities": []}, {"text": "Also we employ denoising in the auto-encoding setup for enabling syntactic transformations.", "labels": [], "entities": []}, {"text": "The framework is trained using unlabeled text collected from Wikipedia (complex) and Simple Wikipedia (simple).", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.9619522094726562}, {"text": "Simple Wikipedia", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.7553280591964722}]}, {"text": "It attempts to perform simplification both lexically and syntactically unlike prevalent systems which mostly target them separately.", "labels": [], "entities": []}, {"text": "We demonstrate the competitiveness of our unsupervised framework alongside supervised skylines through both automatic evaluation metrics and human evaluation studies.", "labels": [], "entities": []}, {"text": "We also outperform another unsupervised baseline (), first proposed for neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.6441633800665537}]}, {"text": "Further, we demonstrate that by leveraging a small amount of labeled parallel data, performance can be improved further.", "labels": [], "entities": []}, {"text": "Our code and anew dataset containing partitioned unlabeled sets of simple and complex sentences is publicly available 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the dataset, architectural choices, and model hyperparameters.", "labels": [], "entities": []}, {"text": "The implementation of the experimental setup is publicly available .  For training our system, we created an unlabeled dataset of simple and complex sentences by partitioning the standard en-wikipedia dump.", "labels": [], "entities": []}, {"text": "Since partitioning requires a metric for measuring text simpleness we categorize sentences based on their  readability scores.", "labels": [], "entities": []}, {"text": "For this we use the Flesch Readability Ease (henceforth abbreviated as FE).", "labels": [], "entities": [{"text": "Flesch Readability Ease", "start_pos": 20, "end_pos": 43, "type": "METRIC", "confidence": 0.9157484968503317}, {"text": "FE", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.870393693447113}]}, {"text": "Sentences with lower FE values (up to 10) are categorized as complex and sentences with FE values greater than 70 are categorized as simple . The FE bounds are decided through trial and error through manual inspection of the categorized sentences.", "labels": [], "entities": [{"text": "FE", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9982607960700989}, {"text": "FE", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.994795024394989}, {"text": "FE", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.997698962688446}]}, {"text": "Even though the dataset was created with some level of human mediation, the manual effort is insignificant compared to that needed to create a parallel corpus.", "labels": [], "entities": []}, {"text": "To train the system with minimal supervision (Section 4.5), we extract 10, 000 pairs of sentences from various datasets such as WikipediaSimpleWikipedia dataset introduced in and the Split-Rephrase dataset by . The WikipediaSimpleWikipedia was filtered following and 4000 examples were randomly picked from the filtered set.", "labels": [], "entities": [{"text": "WikipediaSimpleWikipedia dataset", "start_pos": 128, "end_pos": 160, "type": "DATASET", "confidence": 0.9840257167816162}, {"text": "Split-Rephrase dataset", "start_pos": 183, "end_pos": 205, "type": "DATASET", "confidence": 0.6894730925559998}, {"text": "WikipediaSimpleWikipedia", "start_pos": 215, "end_pos": 239, "type": "DATASET", "confidence": 0.9660029411315918}]}, {"text": "From the SplitRephrase dataset, examples containing one compound/complex sentence at the source side and two simple sentences at the target side were selected and 6000 examples were randomly picked from the selected set.", "labels": [], "entities": [{"text": "SplitRephrase dataset", "start_pos": 9, "end_pos": 30, "type": "DATASET", "confidence": 0.8856869637966156}]}, {"text": "The Split-Rephrase dataset is used to promote sentence splitting in the proposed system.", "labels": [], "entities": [{"text": "Split-Rephrase dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.6778468936681747}, {"text": "sentence splitting", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7476788759231567}]}, {"text": "To select and evaluate our models, we use the test and development sets 7 released by (.", "labels": [], "entities": []}, {"text": "The test set (359 sentences) and development set (2000 sentences) have 8 simplified reference sentences for each source sentence.", "labels": [], "entities": []}, {"text": "For automatic evaluation of our system on the test data, we used four metrics, (a) SARI (b) BLEU (c) FE Difference (d) Word Difference, which are briefly explained below.", "labels": [], "entities": [{"text": "SARI", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9970824122428894}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9556763768196106}, {"text": "FE", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9884201288223267}, {"text": "Word Difference", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.590501070022583}]}, {"text": "SARI () is an automatic evaluation metric designed to measure the simpleness of the generated sentences.", "labels": [], "entities": [{"text": "SARI", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7427185773849487}]}, {"text": "SARI requires access to source, predictions and references for evaluation.", "labels": [], "entities": [{"text": "SARI", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.5549466013908386}]}, {"text": "Computing SARI involves penalizing the n-gram additions to source which are inconsistent with the references.", "labels": [], "entities": [{"text": "SARI", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.6971153020858765}]}, {"text": "Similarly, deletions and keep operations are penalized.", "labels": [], "entities": [{"text": "keep", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9531596899032593}]}, {"text": "The overall score is a balanced sum of all the penalties.", "labels": [], "entities": []}, {"text": "BLEU (), a popular metric to evaluate generations and translations is used to measure the correctness of the generations by measuring overlaps between the generated sentences and (multiple) references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9816229343414307}]}, {"text": "We also compute the average FE score difference between predictions and source in our evaluations.", "labels": [], "entities": [{"text": "FE score difference", "start_pos": 28, "end_pos": 47, "type": "METRIC", "confidence": 0.9810891350110372}]}, {"text": "FE-difference measures whether the changes made by the model increase the readability ease of the generated sentence.", "labels": [], "entities": [{"text": "FE-difference", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9896044731140137}]}, {"text": "Word Difference is the average difference between number of words in the source sentence and generation.", "labels": [], "entities": [{"text": "Word Difference", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.61642225086689}]}, {"text": "It is a simple and approximate metric proposed to detect if sentence shortening is occurring or not.", "labels": [], "entities": []}, {"text": "Generations with lesser number of changes can still have high SARI and BLEU.", "labels": [], "entities": [{"text": "SARI", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9989356398582458}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9988332390785217}]}, {"text": "Models with such generations can be ruled out by imposing a threshold on the word-diff metric.", "labels": [], "entities": []}, {"text": "Models with high word-diff, SARI and BLEU are picked during model-selection (with validation data).", "labels": [], "entities": [{"text": "SARI", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9684181809425354}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9983661770820618}]}, {"text": "Model selection also involved manually examining the quality and relevance of generations.", "labels": [], "entities": [{"text": "Model selection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8120097815990448}]}, {"text": "We carryout a qualitative analysis of our system through human evaluation.", "labels": [], "entities": []}, {"text": "For this the first 50 test samples were selected from the test data.", "labels": [], "entities": []}, {"text": "Output of the seven systems reported in with the sources are presented to two native English speakers who would provide two ratings for each output: (a) Simpleness, a binary score indicating whether the output is a simplified version of the input or not, (b) Grammaticality of the output in the range of, in the increasing order of fluency (c) Relatedness score in the range of showing if the overall semantics of the input is preserved in the output or not.", "labels": [], "entities": [{"text": "Simpleness", "start_pos": 153, "end_pos": 163, "type": "METRIC", "confidence": 0.9791689515113831}, {"text": "Grammaticality", "start_pos": 259, "end_pos": 273, "type": "METRIC", "confidence": 0.9460682272911072}, {"text": "Relatedness", "start_pos": 344, "end_pos": 355, "type": "METRIC", "confidence": 0.9832183718681335}]}], "tableCaptions": [{"text": " Table 1: Statistics showing number of sentences, av- erage words per sentence, and average FE score, FE  score limits for complex and simple datasets used for  training.", "labels": [], "entities": [{"text": "FE score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9903673827648163}, {"text": "FE  score limits", "start_pos": 102, "end_pos": 118, "type": "METRIC", "confidence": 0.9842154184977213}]}, {"text": " Table 2: Comparison of evaluation metrics for  proposed systems (UNTS), unsupervised baseline  (UNMT,USMT, and ST) and existing supervised  and the unsupervised lexical simplification system  LIGHTLS.", "labels": [], "entities": [{"text": "UNMT,USMT", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.8881233930587769}]}, {"text": " Table 3: Average human evaluation scores for simple- ness and grammatical correctness (fluency) and seman- tic relatedness between the output and input.", "labels": [], "entities": []}, {"text": " Table 6: UNTS-ADV does not use the adversarial loss,  UNTS-DIV does not use the diversification loss.", "labels": [], "entities": [{"text": "UNTS-ADV", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.9128949642181396}, {"text": "UNTS-DIV", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.8940442800521851}]}, {"text": " Table 7: Effect of variation in labeled data considered  as additional help during training the unsupervised sys- tems.", "labels": [], "entities": []}]}