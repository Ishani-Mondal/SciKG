{"title": [{"text": "Debiasing Embeddings for Reduced Gender Bias in Text Classification", "labels": [], "entities": [{"text": "Reduced Gender Bias in Text Classification", "start_pos": 25, "end_pos": 67, "type": "TASK", "confidence": 0.6531148105859756}]}], "abstractContent": [{"text": "(Bolukbasi et al., 2016) demonstrated that pre-trained word embeddings can inherit gender bias from the data they were trained on.", "labels": [], "entities": []}, {"text": "We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019).", "labels": [], "entities": [{"text": "occupation classification", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.6920491009950638}]}, {"text": "We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information.", "labels": [], "entities": []}, {"text": "With a relatively minor adjustment , however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy .", "labels": [], "entities": [{"text": "bias", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9693976640701294}, {"text": "classification", "start_pos": 140, "end_pos": 154, "type": "TASK", "confidence": 0.9253427982330322}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.8841923475265503}]}], "introductionContent": [{"text": "A trend in the construction of deep learning models for natural language processing tasks is the use of pre-trained embeddings at the input layer (.", "labels": [], "entities": []}, {"text": "These embeddings are usually learned by solving a language modeling task on a large unsupervised corpus, allowing downstream models to leverage the semantic and syntactic relationships learned from this corpus.", "labels": [], "entities": []}, {"text": "One issue with using such embeddings, however, is that the model might inherit unintended biases from this corpus.", "labels": [], "entities": []}, {"text": "In (, the authors highlight some gender bias at the embedding layer through analogy and occupational stereotyping tasks, but do not investigate how these biases affect modeling on downstream tasks.", "labels": [], "entities": []}, {"text": "It has been argued) that such debiasing approaches only mask the bias in embeddings and that bias remains in a form that downstream algorithms can still pickup.", "labels": [], "entities": []}, {"text": "This paper investigate the impact of gender bias in these pre-trained word embeddings on down- * Equal contribution.", "labels": [], "entities": [{"text": "Equal contribution", "start_pos": 97, "end_pos": 115, "type": "METRIC", "confidence": 0.8491426110267639}]}, {"text": "We build deep neural network classifiers to perform occupation classification on the recently released \"Bias in Bios\" dataset) using a variety of different debiasing techniques for these embeddings introduced in ( and comparing them to the scrubbing of gender indicators.", "labels": [], "entities": [{"text": "occupation classification", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.7048985511064529}]}, {"text": "The main contributions of this paper are: \u2022 Comparing the efficacy of embedding based debiasing techniques to manual word scrubbing techniques on both overall model performance and fairness.", "labels": [], "entities": [{"text": "word scrubbing", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.7086262553930283}]}, {"text": "\u2022 Demonstrating that standard debiasing approaches like those introduced in) actually worsen the bias of downstream tasks by providing a denoised channel for communicating demographic information.", "labels": [], "entities": []}, {"text": "\u2022 Highlight that a simple modification of this debiasing technique which aims to completely remove gender information can simultaneously improve fairness criteria and maintain a high level of task accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9854162335395813}]}], "datasetContent": [{"text": "We will evaluate our models on the dimensions of overall performance and fairness.", "labels": [], "entities": []}, {"text": "For the overall performance of these models, we will use the standard accuracy metric of multi-class classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9986649751663208}, {"text": "multi-class classification", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.7272323369979858}]}, {"text": "There area number of metrics and criteria that offer different interpretations of model fairness (.", "labels": [], "entities": []}, {"text": "In this work, we use the method introduced by) as Equality of Opportunity.", "labels": [], "entities": []}, {"text": "If your data has binary labels Y and some demographic variable A, in our case whether the biography is about a female, then Equality of Opportunity is defined as i.e. the true positive rate of the model should be independent of the demographic variable conditioned on the true label.", "labels": [], "entities": [{"text": "Equality", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9806729555130005}]}, {"text": "In order to measure deviation from this ideal criteria we follow a number of other authors ( and define the True Positive Rate Gap (TPR gap ) to be: Since in our context, we are dealing with a multi-class classifier, we will measure the TPR gap for each class as a separate binary decision and will aggregate by averaging overall occupations.", "labels": [], "entities": [{"text": "True Positive Rate Gap (TPR gap )", "start_pos": 108, "end_pos": 141, "type": "METRIC", "confidence": 0.8885759338736534}]}, {"text": "We can analagously define the TNR gap with the True Negative Rates taking the place of True Positive Rates in the above discussion.", "labels": [], "entities": []}, {"text": "We will also report the average TNR gap across occupations.", "labels": [], "entities": [{"text": "TNR gap", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9655880630016327}]}, {"text": "To understand how the embedding layer affects our deep learning classifiers, we will train classifiers with a variety of embeddings.", "labels": [], "entities": []}, {"text": "As baselines, we will use the GloVe embeddings with and without the gender indicator scrubbing described in).", "labels": [], "entities": [{"text": "GloVe", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.6579492688179016}]}, {"text": "Additionally, we train a classifier on GloVe embeddings debiased using both techniques discussed in Section 3.", "labels": [], "entities": []}, {"text": "These embeddings are fixed (rather than trainable) parameters of our network.", "labels": [], "entities": []}, {"text": "For each of these models, we evaluate their classification performance (accuracy) alongside their overall fairness (TPR gap ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9924226403236389}, {"text": "fairness (TPR gap )", "start_pos": 106, "end_pos": 125, "type": "METRIC", "confidence": 0.834335196018219}]}], "tableCaptions": [{"text": " Table 3: Gender classifier accuracy", "labels": [], "entities": [{"text": "Gender classifier", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6554429680109024}]}, {"text": " Table 4: Ablation study: Metrics for projection and  equalization step", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9950189590454102}]}]}