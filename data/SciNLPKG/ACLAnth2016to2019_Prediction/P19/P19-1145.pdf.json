{"title": [{"text": "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks", "labels": [], "entities": [{"text": "Neural Natural Language Processing", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.6342516839504242}]}], "abstractContent": [{"text": "Many state-of-the-art neural models for NLP are heavily parameterized and thus memory inefficient.", "labels": [], "entities": []}, {"text": "This paper proposes a series of lightweight and memory efficient neural ar-chitectures fora potpourri of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "To this end, our models exploit computation using Quaternion algebra and hypercomplex spaces, enabling not only expressive inter-component interactions but also significantly (75%) reduced parameter size due to lesser degrees of freedom in the Hamilton product.", "labels": [], "entities": []}, {"text": "We propose Quaternion variants of models, giving rise to new architec-tures such as the Quaternion attention Model and Quaternion Transformer.", "labels": [], "entities": []}, {"text": "Extensive experiments on a battery of NLP tasks demonstrates the utility of proposed Quaternion-inspired models, enabling up to 75% reduction in parameter size without significant loss in performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural network architectures such as Transformers ( and attention networks () are dominant solutions in natural language processing (NLP) research today.", "labels": [], "entities": []}, {"text": "Many of these architectures are primarily concerned with learning useful feature representations from data in which providing a strong architectural inductive bias is known to be extremely helpful for obtaining stellar results.", "labels": [], "entities": []}, {"text": "Unfortunately, many of these models are known to be heavily parameterized, with state-of-the-art models easily containing millions or billions of parameters (.", "labels": [], "entities": []}, {"text": "This renders practical deployment challenging.", "labels": [], "entities": []}, {"text": "As such, the enabling of efficient and lightweight * Work done while at University of adaptations of these models, without significantly degrading performance, would certainly have a positive impact on many real world applications.", "labels": [], "entities": []}, {"text": "To this end, this paper explores anew way to improve/maintain the performance of these neural architectures while substantially reducing the parameter cost (compression of up to 75%).", "labels": [], "entities": [{"text": "compression", "start_pos": 157, "end_pos": 168, "type": "METRIC", "confidence": 0.9690399765968323}]}, {"text": "In order to achieve this, we move beyond real space, exploring computation in Quaternion space (i.e., hypercomplex numbers) as an inductive bias.", "labels": [], "entities": []}, {"text": "Hypercomplex numbers comprise of areal and three imaginary components (e.g., i, j, k) in which interdependencies between these components are encoded naturally during training via the Hamilton product \u2297.", "labels": [], "entities": []}, {"text": "Hamilton products have fewer degrees of freedom, enabling up to four times compression of model size.", "labels": [], "entities": []}, {"text": "Technical details are deferred to subsequent sections.", "labels": [], "entities": []}, {"text": "While Quaternion connectionist architectures have been considered in various deep learning application areas such as speech recognition (), kinematics/human motion ( and computer vision (, our work is the first hypercomplex inductive bias designed fora widespread of NLP tasks.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7494587302207947}]}, {"text": "Other fields have motivated the usage of Quaternions primarily due to their natural 3 or 4 dimensional input features (e.g., RGB scenes or 3D human poses).", "labels": [], "entities": []}, {"text": "Ina similar vein, we can similarly motivate this by considering the multi-sense nature of natural language (.", "labels": [], "entities": []}, {"text": "In this case, having multiple embeddings or components per token is well-aligned with this motivation.", "labels": [], "entities": []}, {"text": "Latent interactions between components may also enjoy additional benefits, especially pertaining to applications which require learning pairwise affinity scores (.", "labels": [], "entities": []}, {"text": "Intuitively, instead of regular (real) dot products, Hamilton products \u2297 extensively learn representations by matching across multiple (inter-latent) components in hypercomplex space.", "labels": [], "entities": []}, {"text": "Alternatively, the effectiveness of multi-view and multi-headed () approaches may also explain the suitability of Quaternion spaces in NLP models.", "labels": [], "entities": []}, {"text": "The added advantage to multi-headed approaches is that Quaternion spaces explicitly encodes latent interactions between these components or heads via the Hamilton product which intuitively increases the expressiveness of the model.", "labels": [], "entities": []}, {"text": "Conversely, multi-headed embeddings are generally independently produced.", "labels": [], "entities": []}, {"text": "To this end, we propose two Quaternioninspired neural architectures, namely, the Quaternion attention model and the Quaternion Transformer.", "labels": [], "entities": []}, {"text": "In this paper, we devise and formulate anew attention (and self-attention) mechanism in Quaternion space using Hamilton products.", "labels": [], "entities": []}, {"text": "Transformation layers are aptly replaced with Quaternion feed-forward networks, yielding substantial improvements in parameter size (of up to 75% compression) while achieving comparable (and occasionally better) performance.", "labels": [], "entities": []}, {"text": "Contributions All in all, we make the following major contributions: \u2022 We propose Quaternion neural models for NLP.", "labels": [], "entities": []}, {"text": "More concretely, we propose a novel Quaternion attention model and Quaternion Transformer fora wide range of NLP tasks.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first formulation of hypercomplex Attention and Quaternion models for NLP.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate our Quaternion NLP models on a wide range of diverse NLP tasks such as pairwise text classification (natural language inference, question answering, paraphrase identification, dialogue prediction), neural machine translation (NMT), sentiment analysis, mathematical language understanding (MLU), and subject-verb agreement (SVA).", "labels": [], "entities": [{"text": "pairwise text classification", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6842665076255798}, {"text": "question answering, paraphrase identification", "start_pos": 143, "end_pos": 188, "type": "TASK", "confidence": 0.6413787364959717}, {"text": "dialogue prediction)", "start_pos": 190, "end_pos": 210, "type": "TASK", "confidence": 0.7845776279767355}, {"text": "neural machine translation (NMT)", "start_pos": 212, "end_pos": 244, "type": "TASK", "confidence": 0.8251976370811462}, {"text": "sentiment analysis", "start_pos": 246, "end_pos": 264, "type": "TASK", "confidence": 0.9666211009025574}, {"text": "mathematical language understanding (MLU)", "start_pos": 266, "end_pos": 307, "type": "TASK", "confidence": 0.7723766515652338}]}, {"text": "\u2022 Our experimental results show that Quaternion models achieve comparable or better performance to their real-valued counterparts with up to a 75% reduction in parameter costs.", "labels": [], "entities": []}, {"text": "The key advantage is that these models are expressive (due to Hamiltons) and also parameter efficient.", "labels": [], "entities": []}, {"text": "Moreover, our Quaternion components are self-contained and play well with real-valued counterparts.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our experimental setup across multiple diverse NLP tasks.", "labels": [], "entities": []}, {"text": "All experiments were run on NVIDIA Titan X hardware.", "labels": [], "entities": [{"text": "NVIDIA Titan X hardware", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.8550820499658585}]}, {"text": "Our Models On pairwise text classification, we benchmark Quaternion attention model (Q-Att), testing the ability of Quaternion models on pairwise representation learning.", "labels": [], "entities": [{"text": "pairwise text classification", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6603325804074606}]}, {"text": "On all the other tasks, such as machine translation and subjectverb agreement, we evaluate Quaternion Transformers.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.825759619474411}]}, {"text": "We evaluate two variations of Transformers, full and partial.", "labels": [], "entities": []}, {"text": "The full setting converts all linear transformations into Quaternion space and is approximately 25% of the actual Transformer size.", "labels": [], "entities": []}, {"text": "The second setting (partial) only reduces the linear transforms at the self-attention mechanism.", "labels": [], "entities": []}, {"text": "Tensor2Tensor 1 is used for Transformer benchmarks, which uses its default Hyperparameters and encoding for all experiments.", "labels": [], "entities": [{"text": "Transformer benchmarks", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7208132445812225}]}], "tableCaptions": [{"text": " Table 1: Experimental results on pairwise text classification and ranking tasks. Q-Att achieves comparable or  competitive results compared with DeAtt with approximately one third of the parameter cost.", "labels": [], "entities": [{"text": "pairwise text classification", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.6063896119594574}]}, {"text": " Table 2: Experimental results on sentiment analysis on IMDb and Stanford Sentiment Treebank (SST) data sets.  Evaluation measure is accuracy.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9600145518779755}, {"text": "Stanford Sentiment Treebank (SST) data sets", "start_pos": 65, "end_pos": 108, "type": "DATASET", "confidence": 0.8231289647519588}, {"text": "Evaluation", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9489529132843018}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9995390176773071}]}, {"text": " Table 3: Experimental results on neural machine translation (NMT). Results of Transformer Base on EN-VI  (IWSLT 2015), EN-RO (WMT 2016) and EN-ET (WMT 2018). Parameter size excludes word embeddings. Our  proposed Quaternion Transformer achieves comparable or higher performance with only 67.9% parameter costs  of the base Transformer model.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.8178405066331228}, {"text": "EN-ET (WMT 2018)", "start_pos": 141, "end_pos": 157, "type": "DATASET", "confidence": 0.7914979100227356}]}, {"text": " Table 4: Experimental results on mathematical language understanding (MLU). Both Quaternion models outper- form the base Transformer model with up to 75% parameter savings.", "labels": [], "entities": [{"text": "mathematical language understanding (MLU)", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.850414494673411}]}, {"text": " Table 5: Experimental results on subject-verb agree- ment (SVA) number prediction task.", "labels": [], "entities": [{"text": "subject-verb agree- ment (SVA) number prediction task", "start_pos": 34, "end_pos": 87, "type": "TASK", "confidence": 0.5679382532835007}]}]}