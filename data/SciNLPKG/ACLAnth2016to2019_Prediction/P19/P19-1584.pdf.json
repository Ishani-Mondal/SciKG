{"title": [{"text": "Adversarial Learning of Privacy-Preserving Text Representations for De-Identification of Medical Records", "labels": [], "entities": [{"text": "Adversarial Learning of Privacy-Preserving Text Representations", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.793452779452006}]}], "abstractContent": [{"text": "De-identification is the task of detecting protected health information (PHI) in medical text.", "labels": [], "entities": [{"text": "detecting protected health information (PHI) in medical text", "start_pos": 33, "end_pos": 93, "type": "TASK", "confidence": 0.8021650433540344}]}, {"text": "It is a critical step in sanitizing electronic health records (EHRs) to be shared for research.", "labels": [], "entities": [{"text": "sanitizing electronic health records (EHRs)", "start_pos": 25, "end_pos": 68, "type": "TASK", "confidence": 0.8750347920826503}]}, {"text": "Automatic de-identification classifiers can significantly speedup the sanitization process.", "labels": [], "entities": [{"text": "sanitization", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.9708271622657776}]}, {"text": "However, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records.", "labels": [], "entities": []}, {"text": "We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization.", "labels": [], "entities": []}, {"text": "These representations can be shared between organizations to create unified datasets for training de-identification models.", "labels": [], "entities": []}, {"text": "Our representation allows training a simple LSTM-CRF de-identification model to an F 1 score of 97.4%, which is comparable to a strong baseline that exposes private information in its representation.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9912743965784708}]}, {"text": "A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Electronic health records (EHRs) are are valuable resource that could potentially be used in largescale medical research (.", "labels": [], "entities": []}, {"text": "In addition to structured medical data, EHRs contain free-text patient notes that area rich source of information ().", "labels": [], "entities": []}, {"text": "However, due to privacy and data protection laws, medical records can only be shared and used for research if they are sanitized to not include information potentially identifying patients.", "labels": [], "entities": []}, {"text": "The PHI that may not be shared includes potentially identifying information such as names, geographic identifiers, dates, and account numbers; the American Health Insurance Portability Accountability Act 1) defines 18 categories of PHI.", "labels": [], "entities": [{"text": "American Health Insurance Portability Accountability Act 1", "start_pos": 147, "end_pos": 205, "type": "DATASET", "confidence": 0.8051962171282087}]}, {"text": "De-identification is the task of finding and labeling PHI in medical text as a step toward sanitization.", "labels": [], "entities": [{"text": "labeling PHI in medical text", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7850019812583924}]}, {"text": "As the information to be removed is very sensitive, sanitization always requires final human verification.", "labels": [], "entities": [{"text": "sanitization", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.9731205701828003}]}, {"text": "Automatic deidentification labeling can however significantly speedup the process, as shown for other annotation tasks in e.g..", "labels": [], "entities": [{"text": "deidentification labeling", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8616186678409576}]}, {"text": "Trying to create an automatic classifier for deidentification leads to a \"chicken and egg problem\": without a comprehensive training set, an automatic de-identification classifier cannot be developed, but without access to automatic de-identification, it is difficult to share large corpora of medical text in a privacypreserving way for research (including for training the classifier itself).", "labels": [], "entities": []}, {"text": "The standard method of data protection compliant sharing of training data fora de-identification classifier requires humans to pseudonymize protected information with substitutes in a document-coherent way.", "labels": [], "entities": []}, {"text": "This includes replacing e.g. every person or place name with a different name, offsetting dates by a random amount while retaining date intervals, and replacing misspellings with similar misspellings of the pseudonym (.", "labels": [], "entities": []}, {"text": "In 2019, a pseudonymized dataset for deidentification from a single source, the i2b2 2014 dataset, is publicly available ( . However, de-identification classifiers trained on this dataset do not generalize well to data from other sources (.", "labels": [], "entities": [{"text": "i2b2 2014 dataset", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.7600359618663788}]}, {"text": "To obtain a universal de-identification classifier, many medical institutions would have to pool their data.", "labels": [], "entities": []}, {"text": "But, preparing this data for sharing using the document-coherent pseudonymization ap-James was admitted to St. Thomas.", "labels": [], "entities": [{"text": "St. Thomas", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.8237298429012299}]}], "datasetContent": [{"text": "We evaluate our approaches using the i2b2 2014 dataset ( , which was released as part of the 2014 i2b2/UTHealth shared task track 1 and is the largest publicly available dataset for de-identification today.", "labels": [], "entities": [{"text": "i2b2 2014 dataset", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.7540981471538544}, {"text": "UTHealth shared task track 1", "start_pos": 103, "end_pos": 131, "type": "DATASET", "confidence": 0.8468127012252807}]}, {"text": "It contains 1304 free-text documents with PHI annotations.", "labels": [], "entities": []}, {"text": "The i2b2 dataset uses the 18 categories of PHI defined by HIPAA as a starting point for its own set of PHI categories.", "labels": [], "entities": [{"text": "i2b2 dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7402162253856659}, {"text": "HIPAA", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.8902298212051392}]}, {"text": "In addition to the HIPAA set of categories, it includes (sub-)categories such as doctor names, professions, states, countries, and ages under 90.", "labels": [], "entities": [{"text": "HIPAA set", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.948068380355835}]}, {"text": "We compare three different approaches: a nonprivate de-identification classifier and two privacyenabled extensions, automatic pseudonymization (Section 4) and adversarially learned representations (Section 5).", "labels": [], "entities": []}, {"text": "Our non-private system as well as the privacyenabled extensions are based on a bidirectional LSTM-CRF architecture that has been proven to work well in sequence tagging () and de-identification.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.7029208838939667}]}, {"text": "We only use pre-trained FastText ( or GloVe () word embeddings, not explicit character embeddings, as we suspect that these may allow easy re-identification of private information if used in shared representations.", "labels": [], "entities": []}, {"text": "In place of learned character features, we provide the casing feature from as an additional input.", "labels": [], "entities": []}, {"text": "The feature maps words to a one-hot representation of their casing (numeric, mainly numeric, all lower, all upper, initial upper, contains digit, or other).", "labels": [], "entities": []}, {"text": "shows our raw de-identification model's hyperparameter configuration that was determined through a random hyperparameter search.", "labels": [], "entities": []}, {"text": "To evaluate our approaches, we perform experiments using the i2b2 2014 dataset.", "labels": [], "entities": [{"text": "i2b2 2014 dataset", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.8540646632512411}]}, {"text": "Preprocessing: We apply aggressive tokenization similarly to, including splitting at all punctuation marks and mid-word e.g. if a number is followed by a word (\"25yo\" is split into \"25\", \"yo\") in order to minimize the amount of GloVe out-of-vocabulary tokens.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9439141750335693}]}, {"text": "We extend spaCy's 3 sentence splitting heuristics with additional rules for splitting at multiple blank lines as well as bulleted and numbered list items.", "labels": [], "entities": [{"text": "3 sentence splitting heuristics", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.6725711300969124}]}, {"text": "Deep Learning Models: We use the Keras framework) with the TensorFlow backend () to implement our deep learning models.", "labels": [], "entities": [{"text": "Keras framework", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.9095251262187958}]}, {"text": "Evaluation: In order to compare our results to the state of the art, we use the token-based binary HIPAA F 1 score as our main metric for deidentification performance.", "labels": [], "entities": [{"text": "token-based binary HIPAA F 1 score", "start_pos": 80, "end_pos": 114, "type": "METRIC", "confidence": 0.6171947667996088}]}, {"text": "deem it the most important metric: deciding if an entity is PHI or not is generally more important than assigning the correct category of PHI, and only HIPAA categories of PHI are required to be removed by American law.", "labels": [], "entities": []}, {"text": "Non-PHI tokens are not incorporated in the F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9029046893119812}]}, {"text": "We perform the evaluation with the official i2b2 evaluation script 5 . In the case of binary classification: Lrandom = \u2212 log 1 2 shows de-identification performance results for the non-private de-identification classifier (upper part, in comparison to the state of the art) as well as the two privacy-enabled extensions (lower part).", "labels": [], "entities": [{"text": "Lrandom", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9943740367889404}]}, {"text": "The results are average values out of five experiment runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameter configuration of our de- identification model.", "labels": [], "entities": []}, {"text": " Table 2: Binary HIPAA F 1 scores of our non-private  (top) and private (bottom) de-identification approaches  on the i2b2 2014 test set in comparison to non-private  the state of the art. Our private approaches use N =  100 neighbors as a privacy criterion.", "labels": [], "entities": [{"text": "Binary HIPAA F 1", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.6852517053484917}, {"text": "i2b2 2014 test set", "start_pos": 118, "end_pos": 136, "type": "DATASET", "confidence": 0.8329092636704445}]}]}