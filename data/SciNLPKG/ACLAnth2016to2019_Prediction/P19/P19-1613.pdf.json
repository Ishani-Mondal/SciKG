{"title": [{"text": "Multi-hop Reading Comprehension through Question Decomposition and Rescoring", "labels": [], "entities": [{"text": "Question Decomposition", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7415257692337036}, {"text": "Rescoring", "start_pos": 67, "end_pos": 76, "type": "TASK", "confidence": 0.5050484538078308}]}], "abstractContent": [{"text": "Multi-hop Reading Comprehension (RC) requires reasoning and aggregation across several paragraphs.", "labels": [], "entities": [{"text": "Multi-hop Reading Comprehension (RC)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7879700710376104}]}, {"text": "We propose a system for multi-hop RC that decomposes a composi-tional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models.", "labels": [], "entities": [{"text": "multi-hop RC", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.723466545343399}]}, {"text": "Since annotations for such decomposition are expensive, we recast sub-question generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as human-authored sub-questions.", "labels": [], "entities": [{"text": "sub-question generation", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7323414087295532}, {"text": "span prediction", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7128943055868149}]}, {"text": "We also introduce anew global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance.", "labels": [], "entities": [{"text": "global rescoring", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7223387062549591}]}, {"text": "Our experiments on HOTPOTQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-hop reading comprehension (RC) is challenging because it requires the aggregation of evidence across several paragraphs to answer a question.", "labels": [], "entities": [{"text": "Multi-hop reading comprehension (RC)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8364838262399038}]}, {"text": "shows an example of multi-hop RC, where the question \"Which team does the player named 2015 Diamond Head Classics MVP play for?\" requires first finding the player who won MVP from one paragraph, and then finding the team that player plays for from another paragraph.", "labels": [], "entities": [{"text": "multi-hop RC", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.812533050775528}]}, {"text": "In this paper, we propose DECOMPRC, a system for multi-hop RC, that learns to break compositional multi-hop questions into simpler, singlehop sub-questions using spans from the original question.", "labels": [], "entities": [{"text": "DECOMPRC", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8297815918922424}, {"text": "multi-hop RC", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.7351335883140564}]}, {"text": "For example, for the question in Table 1, we can create the sub-questions \"Which player named 2015 Diamond Head Classics MVP?\" and \"Which team does ANS play for?\",: An example of multi-hop question from HOT-POTQA.", "labels": [], "entities": [{"text": "2015 Diamond Head Classics MVP", "start_pos": 94, "end_pos": 124, "type": "DATASET", "confidence": 0.5953908562660217}]}, {"text": "The first cell shows given question and two of given paragraphs (other eight paragraphs are not shown), where the red text is the groundtruth answer.", "labels": [], "entities": []}, {"text": "Our system selects a span over the question and writes two sub-questions shown in the second cell.", "labels": [], "entities": []}, {"text": "where the token ANS is replaced by the answer to the first sub-question.", "labels": [], "entities": [{"text": "ANS", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9610584378242493}]}, {"text": "The final answer is then the answer to the second sub-question.", "labels": [], "entities": []}, {"text": "Recent work on question decomposition relies on distant supervision data created on top of underlying relational logical forms), making it difficult to generalize to diverse natural language questions such as those on HOTPOTQA (.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.8281070590019226}]}, {"text": "In contrast, our method presents anew approach which simplifies the process as a span prediction, thus requiring only 400 decomposition examples to train a competitive decomposition neural model.", "labels": [], "entities": [{"text": "span prediction", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7529177069664001}]}, {"text": "Furthermore, we propose a rescoring approach which obtains answers from different possible decompositions and rescores each decomposition with the answer to decide on the final answer, rather than deciding on the decomposition in the beginning.", "labels": [], "entities": []}, {"text": "Our experiments show that DECOMPRC outperforms other published methods on HOT-POTQA (, while providing explainable evidence in the form of sub-questions.", "labels": [], "entities": [{"text": "DECOMPRC", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8987734913825989}]}, {"text": "In addition, we evaluate with alternative distrator paragraphs and questions and show that our decomposition-based approach is more ro-bust than an end-to-end BERT baseline).", "labels": [], "entities": []}, {"text": "Finally, our ablation studies show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance.", "labels": [], "entities": []}, {"text": "Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: F1 scores on the dev set of HOTPOTQA in both distractor (left) and full wiki settings (right). We compare  DECOMPRC (our model), BERT, and BiDAF, and variants of the models that are only trained on single-hop QA  data (1hop train). Bridge and Comp indicate original splits in HOTPOTQA; Single and Multi refer to dev set splits  that can be solved (or not) by all of three BERT models trained on single-hop QA data.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9991152882575989}, {"text": "DECOMPRC", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9424483180046082}, {"text": "BERT", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9964893460273743}, {"text": "BiDAF", "start_pos": 149, "end_pos": 154, "type": "METRIC", "confidence": 0.9340638518333435}]}, {"text": " Table 4: F1 score on the test set of HOTPOTQA distrac- tor and full wiki setting. All numbers from the official  leaderboard. All models except BiDAF are concurrent  work (not published). DECOMPRC achieves the best  result out of models reported to both distractor and full  wiki setting.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9997221827507019}]}, {"text": " Table 5: Left: modifying distractor paragraphs. F1 score on the original dev set and the new dev set made up  with a different set of distractor paragraphs. DECOMPRC is our model and DECOMPRC-1hop train is DECOM- PRC trained on only single-hop QA data and 400 decomposition annotations. BERT and BERT-1hop train are  the baseline models, trained on HOTPOTQA and single-hop data, respectively. Right: adversarial comparison  questions. F1 score on a subset of binary comparison questions. Orig F1, Inv F1 and Joint F1 indicate F1 score  on the original example, the inverted example and the joint of two (example-wise minimum of two), respectively.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.982765257358551}, {"text": "BERT", "start_pos": 288, "end_pos": 292, "type": "METRIC", "confidence": 0.9891062378883362}, {"text": "BERT-1hop", "start_pos": 297, "end_pos": 306, "type": "METRIC", "confidence": 0.9600637555122375}, {"text": "F1 score", "start_pos": 436, "end_pos": 444, "type": "METRIC", "confidence": 0.9812737703323364}, {"text": "F1 score", "start_pos": 527, "end_pos": 535, "type": "METRIC", "confidence": 0.9812802076339722}]}, {"text": " Table 6: An example of the original question, span-based human-annotated sub-questions and free-form human- authored sub-questions.", "labels": [], "entities": []}, {"text": " Table 7: Left: ablations in sub-questions. F1 score on a sample of 50 bridging questions from the dev set  of HOTPOTQA, Pointer c is our span-based model trained with 200 or 400 annotations. Right: ablations in  decomposition decision method. F1 score on the dev set of HOTPOTQA with ablating decomposition decision  method. Oracle indicates that the ground truth reasoning type is selected.", "labels": [], "entities": [{"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9997029900550842}, {"text": "Pointer", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9557957053184509}, {"text": "F1", "start_pos": 244, "end_pos": 246, "type": "METRIC", "confidence": 0.99915611743927}]}]}