{"title": [{"text": "Combining Local and Document-Level Context: The LMU Munich Neural Machine Translation System at WMT19", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.7921767830848694}, {"text": "WMT19", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.9071142673492432}]}], "abstractContent": [{"text": "We describe LMU Munich's machine translation system for English\u2192German translation which was used to participate in the WMT19 shared task on supervised news translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6554296165704727}, {"text": "English\u2192German translation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6670458912849426}, {"text": "WMT19 shared task on supervised news translation", "start_pos": 120, "end_pos": 168, "type": "TASK", "confidence": 0.5141090963568006}]}, {"text": "We specifically participated in the document-level MT track.", "labels": [], "entities": [{"text": "MT track", "start_pos": 51, "end_pos": 59, "type": "TASK", "confidence": 0.793086588382721}]}, {"text": "The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contex-tual information and integration of large-scale document-level context with a less rich representation.", "labels": [], "entities": []}, {"text": "We train this model by fine-tuning a big Transformer baseline.", "labels": [], "entities": []}, {"text": "Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.", "labels": [], "entities": [{"text": "translation", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.9629256725311279}]}], "introductionContent": [{"text": "In this paper we describe the system we developed at the LMU Munich Center for Information and Language Processing, which we used to participate in the news translation task at WMT19.", "labels": [], "entities": [{"text": "news translation task at WMT19", "start_pos": 152, "end_pos": 182, "type": "TASK", "confidence": 0.7519736647605896}]}, {"text": "We submitted system runs for the English\u2192German translation direction and specifically focus on the document-level translation track.", "labels": [], "entities": [{"text": "English\u2192German translation direction", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.6205326557159424}, {"text": "document-level translation", "start_pos": 100, "end_pos": 126, "type": "TASK", "confidence": 0.6285567432641983}]}, {"text": "The goal of the document-level track is to train machine translation models capable of taking into account larger context or even entire documents when translating sentences.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.690880298614502}]}, {"text": "Supervised NMT has achieved state-of-the-art results (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.5806832909584045}]}, {"text": "Several works have claimed translation quality on a level similar to human translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9535378217697144}]}, {"text": "report translation quality on par with average bilingual human translators and argue for parity to professional human translators on news translation from Chinese to English.", "labels": [], "entities": [{"text": "news translation from Chinese to English", "start_pos": 133, "end_pos": 173, "type": "TASK", "confidence": 0.8535208702087402}]}, {"text": "However, these claims have been challenged in several ways with recent work.", "labels": [], "entities": []}, {"text": "One challenge is that these evaluations were done without giving evaluators access to the whole document-level context.", "labels": [], "entities": []}, {"text": "They further show that human translations are preferred over automatic ones if evaluators are given document-level context.", "labels": [], "entities": []}, {"text": "This is precisely the motivation for the document-level MT track in this year's WMT19.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.8180209994316101}, {"text": "WMT19", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9129149913787842}]}, {"text": "One of the reasons for the failure of NMT in these context-dependent cases is not being able to model discourse-level phenomena.", "labels": [], "entities": [{"text": "NMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9193337559700012}]}, {"text": "The straightforward reason for this is that traditional NMT does not have access to the context.", "labels": [], "entities": []}, {"text": "As a result, it fails to account for several discourse-level phenomena, prominent ones being coreference resolution and coherence.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.9148121178150177}]}, {"text": "Coreference resolution has a particular impact on English\u2192German translation, specifically for pronoun translation.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8937246203422546}, {"text": "English\u2192German translation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6068167984485626}, {"text": "pronoun translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7778614461421967}]}, {"text": "English has only one third person singular pronoun that is routinely used for non-human references (\"it\"), while German has three, each representing a specific gender: masculine, feminine and neuter.", "labels": [], "entities": []}, {"text": "Consider the following sentence: We know it won't change students' behaviour instantly.", "labels": [], "entities": []}, {"text": "The translation of it into German can be, er, sie or es depending on the gender of the noun the English it is referencing.", "labels": [], "entities": []}, {"text": "Since traditional NMT is working on the sentence-level, it has noway of ascertaining the appropriate gender and usually falls back to the data-driven prior, which is the neuter es.", "labels": [], "entities": []}, {"text": "Coherence is important in order to provide coherent translations across the whole given document.", "labels": [], "entities": []}, {"text": "It is usually undesirable to produce translations with different meanings within a single document for the same ambiguous word.", "labels": [], "entities": []}, {"text": "Taking into account the whole document when generating translations will address some of the relevant discourse-level phenomena.", "labels": [], "entities": []}, {"text": "An implicit effect that one could expect by modeling the whole document is also modeling the underlying domain.", "labels": [], "entities": []}, {"text": "On an abstract level, one can presume that this is happening in sentence-level models as well, however access to larger context is likely to improve the ability to implicitly identify the domain.", "labels": [], "entities": []}, {"text": "Domain adaptation and multi-domain NMT have been extensively studied (.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7738921344280243}]}, {"text": "However, most previous works assume that the domain of each sentence is known at training time, which is often not the case.", "labels": [], "entities": []}, {"text": "Taking into consideration different discourselevel phenomena, we develop a Transformer () which can richly model the previous sentence, but also takes advantage of larger context.", "labels": [], "entities": []}, {"text": "We borrow on previous work on context-aware NMT ( and add additional parameters in the encoder and decoder to account for the previous sentence.", "labels": [], "entities": []}, {"text": "We limit the context since we want this part of the model to be able to do coreference resolution which very often can be addressed by looking at the first previous sentence.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.9530906081199646}]}, {"text": "We additionally take the 10 previous sentences and create a simple document representation by averaging their embeddings.", "labels": [], "entities": []}, {"text": "This embedding is subsequently added to each source token in the sentence to be translated in the same fashion as positional embeddings are added to the token-level embeddings in the Transformer.", "labels": [], "entities": []}, {"text": "We assume that this representation can help provide a clear domain signal.", "labels": [], "entities": []}, {"text": "The remainder of the paper outlines the model in detail, and presents the experimental setup and obtained results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present the results we obtain with our models in.", "labels": [], "entities": []}, {"text": "We report results on the English\u2192German newstest2017, newstest2018 and newstest2019.", "labels": [], "entities": []}, {"text": "We report BLEU scores using sacreBLEU 2 (Post, 2018) on detokenized text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986405968666077}]}, {"text": "For the final submission, we processed quotation marks to match the German style.", "labels": [], "entities": []}, {"text": "We train our baseline on the data presented in.", "labels": [], "entities": []}, {"text": "We initially train on the ParaCrawl dataset and an oversampled version of the other datasets.", "labels": [], "entities": [{"text": "ParaCrawl dataset", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9638919830322266}]}, {"text": "We train this baseline until convergence with early-stopping based on the perplexity on the development set.", "labels": [], "entities": []}, {"text": "As a development set, we use newstest2018.", "labels": [], "entities": []}, {"text": "After convergence, we fine-tune with 9.3M NewsCrawl backtranslations in addition to the dataset we used for the initial baseline.", "labels": [], "entities": [{"text": "9.3M NewsCrawl backtranslations", "start_pos": 37, "end_pos": 68, "type": "DATASET", "confidence": 0.6965436339378357}]}, {"text": "This baseline is used to initialize all the other context-aware models.", "labels": [], "entities": []}, {"text": "It is interesting to observe that fine-tuning with NewsCrawl backtranslations and WMT data improves on newstest2017 and newstest2018, but significantly decreases the BLEU score on newstest2019.", "labels": [], "entities": [{"text": "NewsCrawl", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.9190632104873657}, {"text": "WMT data", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.7242487668991089}, {"text": "newstest2018", "start_pos": 120, "end_pos": 132, "type": "DATASET", "confidence": 0.9103710651397705}, {"text": "BLEU score", "start_pos": 166, "end_pos": 176, "type": "METRIC", "confidence": 0.9804642200469971}]}, {"text": "For training the context-aware models, we ignore the ParaCrawl data and use the remaining datasets.", "labels": [], "entities": [{"text": "ParaCrawl data", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9120218455791473}]}, {"text": "Depending on the setup, we either use the 16.9M NewsCrawl backtranslations with document boundaries or completely ignore them.", "labels": [], "entities": []}, {"text": "Our previous sentence context-aware Transformer trained with NewsCrawl backtranslations do not provide for significant improvements.", "labels": [], "entities": []}, {"text": "It increases the BLEU score from 38.5 to 38.6.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9528332054615021}]}, {"text": "However, the document-level model with averaging context embeddings obtains a BLEU score of 39.3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9995912909507751}]}, {"text": "We also remove the NewsCrawl backtranslations when fine-tuning our average context embedding Transformer.", "labels": [], "entities": [{"text": "NewsCrawl", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.9286841750144958}]}, {"text": "This proves to be very helpful and we manage to obtain 40.0 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9925767779350281}]}, {"text": "It is interesting that this model also substantially improves the BLEU score on newstest2017 and newstest2018.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.976323813199997}, {"text": "newstest2018", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9272029399871826}]}, {"text": "One possible explanation of the adverse effect of using backtranslations is that our document-level model is more sensitive to noisy input.", "labels": [], "entities": []}, {"text": "We leave a further examination of the issue for future work.", "labels": [], "entities": []}, {"text": "Finally, we train a joint model where we combine the average context embedding approach with the previous-sentence context-aware Transformer where we employ a separate encoder and modify the decoder.", "labels": [], "entities": []}, {"text": "This further pushes the BLEU score to 40.3 on newstest2019 and slightly improves results on the other test sets.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9816676676273346}, {"text": "newstest2019", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9831486940383911}]}, {"text": "This is the system we used for the primary submission.", "labels": [], "entities": []}, {"text": "We also tried ensembling context-aware joint models.", "labels": [], "entities": []}, {"text": "However, due to time constraints we only managed to train a single baseline.", "labels": [], "entities": []}, {"text": "Therefore, all context-aware models were trained by fine-tuning on top of the single baseline.", "labels": [], "entities": []}, {"text": "As a result, these models were not diverse enough and ensembling did not help.", "labels": [], "entities": []}, {"text": "After the evaluation period, we also tried averaging the last 5 checkpoints of a single run of the joint model.", "labels": [], "entities": []}, {"text": "This improved the score on newstest2019 to 40.8 BLEU.", "labels": [], "entities": [{"text": "newstest2019", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9532514214515686}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9957922697067261}]}], "tableCaptions": [{"text": " Table 1: Training data sizes after filtering. x2 -over- sampling factor.", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores on newstest2017, newstest2018  and newstest2019. * -model trained with NewsCrawl  backtranslations. All context-aware models fine-tuned  on baseline*.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994943141937256}, {"text": "newstest2018", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9227128624916077}, {"text": "newstest2019", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9163915514945984}]}]}