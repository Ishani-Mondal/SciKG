{"title": [{"text": "Accelerating Sparse Matrix Operations in Neural Networks on Graphics Processing Units", "labels": [], "entities": [{"text": "Accelerating", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9521519541740417}]}], "abstractContent": [{"text": "Graphics Processing Units (GPUs) are commonly used to train and evaluate neural networks efficiently.", "labels": [], "entities": []}, {"text": "While previous work in deep learning has focused on accelerating operations on dense matrices/tensors on GPUs, efforts have concentrated on operations involving sparse data structures.", "labels": [], "entities": []}, {"text": "Operations using sparse structures are common in natural language models at the input and output layers, because these models operate on sequences over discrete alphabets.", "labels": [], "entities": []}, {"text": "We present two new GPU algorithms: one at the input layer, for multiplying a matrix by a few-hot vector (gen-eralizing the more common operation of multiplication by a one-hot vector) and one at the output layer, fora fused softmax and top-N selection (commonly used in beam search).", "labels": [], "entities": [{"text": "beam search", "start_pos": 270, "end_pos": 281, "type": "TASK", "confidence": 0.8056182265281677}]}, {"text": "Our methods achieve speedups over state-of-the-art parallel GPU baselines of up to 7\u00d7 and 50\u00d7, respectively.", "labels": [], "entities": []}, {"text": "We also illustrate how our methods scale on different GPU architectures.", "labels": [], "entities": []}], "introductionContent": [{"text": "The speedups introduced by parallel architectures inspired the development of accelerators tailored towards specialized functions.", "labels": [], "entities": []}, {"text": "Graphics Processing Units (GPUs) are now a standard platform for deep learning.", "labels": [], "entities": []}, {"text": "GPUs provide faster model training and inference times compared to serial processors, because they can parallelize the linear algebra operations used so heavily in neural networks (.", "labels": [], "entities": []}, {"text": "Currently, major open source toolkits () provide additional layers of abstraction to support one or more parallel GPU architectures.", "labels": [], "entities": []}, {"text": "The seamless compatibility with multiple GPUs allows researchers to train a single model on multiple hardware platforms with no significant changes to their code base and no specialized knowledge about the targeted architectures.", "labels": [], "entities": []}, {"text": "The disadvantage of hardware agnostic APIs is the lack of optimizations fora set of task-specific functions.", "labels": [], "entities": []}, {"text": "Adapting parallel neural operations to a specific hardware platform is required to obtain optimal speed.", "labels": [], "entities": []}, {"text": "Since matrix operations are used heavily in deep learning, much research has been done on optimizing them on GPUs (.", "labels": [], "entities": []}, {"text": "Recently, some efforts have been made to other kinds of operations: serial operations running on the GPU (, operations not involving matrix multiplications (, and models using sparse structures (.", "labels": [], "entities": [{"text": "GPU", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.9474149346351624}]}, {"text": "In this paper, we focus on sparse operations running exclusively on the GPU architecture.", "labels": [], "entities": []}, {"text": "Much recent work in High Performance Computing (HPC) and Natural Language Processing (NLP) focuses on an expensive step of a model or models and optimizes it fora specific architecture.", "labels": [], "entities": []}, {"text": "The lookup operation used in the input layer and the softmax function used in the output are two examples seen in machine translation, language modeling, and other tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8224232196807861}, {"text": "language modeling", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.716004952788353}]}, {"text": "Previous work has accelerated the softmax step by skipping it entirely), or approximating it.", "labels": [], "entities": []}, {"text": "Another strategy is to fuse multiple tasks into a single step.", "labels": [], "entities": []}, {"text": "This approach increases the room for parallelism.", "labels": [], "entities": []}, {"text": "Recent efforts have fused the softmax and top-N operations to accelerate beam search on the GPU using similar approaches (.", "labels": [], "entities": [{"text": "beam search", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.8446381986141205}]}, {"text": "Our approach differs from former methods in the following aspects: We deliver a novel method tailored towards scenarios seen in Neural Machine Translation (NMT), we introduce anew GPU-specific method to obtain the top-N elements from a list of hypotheses using a different sorting mechanism, and we introduce a sparse lookup method for GPUs.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 128, "end_pos": 160, "type": "TASK", "confidence": 0.8480840524037679}]}, {"text": "NMT uses beam search during inference to limit the full set of potential output translations explored during decoding (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8254693746566772}]}, {"text": "This algorithm is widely used to obtain state-of-the-art results during test time.", "labels": [], "entities": []}, {"text": "At each decoding time-step t, the top-N hypotheses are chosen for further expansion and the rest are discarded.", "labels": [], "entities": []}, {"text": "The top-N selection part of the search has been accelerated using hashing methods to avoid a full sort ().", "labels": [], "entities": []}, {"text": "The aim of this paper is to both combine softmax and top-N operations seen in the last layer of a neural network and optimize the top-N selection operation used by several NMT models.", "labels": [], "entities": []}, {"text": "Our work uses ideas from previous work to accelerate two different operations.", "labels": [], "entities": []}, {"text": "We focus on operations that manipulate sparse structures.", "labels": [], "entities": []}, {"text": "By sparse, we mean operations that only require a small fraction of the elements in a tensor to output the correct result.", "labels": [], "entities": []}, {"text": "We propose two different optimizations for sparse scenarios in deep learning: The first operation involves the first layer of a neural network.", "labels": [], "entities": []}, {"text": "We accelerate the first matrix multiplication using batched sparse vectors as input.", "labels": [], "entities": []}, {"text": "The second operation is the computation of the softmax used for beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.939915269613266}]}, {"text": "We combine the softmax and the top-N selection into one operation obtaining a speedup over a parallel stateof-the-art baseline.", "labels": [], "entities": []}, {"text": "We show that our fused top-N selection and sparse lookups achieve speedups of 7\u00d7 and 50\u00d7 relative to other parallel NVIDIA baselines.", "labels": [], "entities": [{"text": "speedups", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9818226099014282}]}], "datasetContent": [{"text": "We run experiments on two different GPU configurations.", "labels": [], "entities": []}, {"text": "The first setup is a 16 core Intel(R) Xeon(R) Silver 4110 CPU connected to a Tesla V100 CPU, and the second set is a 16-core Intel(R) Xeon(R) CPU E5-2630 connected to a GeForce GTX TITAN X.", "labels": [], "entities": [{"text": "GeForce GTX TITAN X", "start_pos": 169, "end_pos": 188, "type": "DATASET", "confidence": 0.8623229116201401}]}, {"text": "The dense matrices we use are randomly generated with different floating point values.", "labels": [], "entities": []}, {"text": "We assume the dense representations contain no values equal to zero.", "labels": [], "entities": []}, {"text": "The sparse minibatches used for the top-N experiments are randomly generated to contain a specific amount of non-zero values per element.", "labels": [], "entities": []}, {"text": "The indices for all non-zero values are selected at random.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Fused softmax and top-N performance comparison against the method of Milakov and Gimelshein (2018)  using different values of N and different batch sizes. For all experiments, we set the vocabulary size to K = 10240.  Each time (in ms) is an average over ten runs. Fastest times are shown in bold.", "labels": [], "entities": []}]}