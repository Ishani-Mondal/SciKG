{"title": [{"text": "Annotation and Automatic Classification of Aspectual Categories", "labels": [], "entities": [{"text": "Automatic Classification of Aspectual Categories", "start_pos": 15, "end_pos": 63, "type": "TASK", "confidence": 0.7315470576286316}]}], "abstractContent": [{"text": "We present the first annotated resource for the aspectual classification of German verb tokens in their clausal context.", "labels": [], "entities": [{"text": "aspectual classification of German verb tokens", "start_pos": 48, "end_pos": 94, "type": "TASK", "confidence": 0.8659693400065104}]}, {"text": "We use aspec-tual features compatible with the plurality of aspectual classifications in previous work and treat aspectual ambiguity systematically.", "labels": [], "entities": []}, {"text": "We evaluate our corpus by using it to train supervised classifiers to automatically assign aspec-tual categories to verbs in context, permitting favourable comparisons to previous work.", "labels": [], "entities": []}], "introductionContent": [{"text": "The universal linguistic category of aspect describes how a verb or a verbal projection (including sentences, 'predicates' for short) characterises the temporal course of a state of affairs or 'eventuality'.", "labels": [], "entities": [{"text": "universal linguistic category of aspect describes how a verb or a verbal projection (including sentences, 'predicates' for short) characterises the temporal course of a state of affairs or 'eventuality", "start_pos": 4, "end_pos": 205, "type": "Description", "confidence": 0.792741183848942}]}, {"text": "Such information is relevant for tasks that extract temporal information from texts, such as information extraction, question answering, and document summarisation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.7811877727508545}, {"text": "question answering", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.9093808829784393}, {"text": "document summarisation", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.6497179269790649}]}, {"text": "Further tasks in which aspectual information plays a crucial role include computational semantic analysis, zoning (analysing the argumentative and rhetorical structure of texts;, and the analysis of specific textual elements, e.g., captions.", "labels": [], "entities": [{"text": "computational semantic analysis", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7673957546552023}]}, {"text": "Aspect must also be considered in event annotation ().", "labels": [], "entities": [{"text": "Aspect", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9365730881690979}]}, {"text": "Aspect is a universal semantic category; thus, the same aspectual patterns reappear across languages.", "labels": [], "entities": []}, {"text": "We created the first resource of German verbs annotated for aspectual class in context.", "labels": [], "entities": []}, {"text": "We use aspectual features compatible with various different previously published aspectual classifications, and model the pervasive phenomenon of aspectual ambiguity.", "labels": [], "entities": []}, {"text": "We evaluate the resource by using it in supervised aspectual classifiers for verbs in context.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the validity and utility of our annotated corpus, we trained supervised classifiers on the dataset.", "labels": [], "entities": [{"text": "validity", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9726731777191162}]}, {"text": "The fine granularity of our classification allows us to define several tasks.", "labels": [], "entities": []}, {"text": "We use a logistic regression classifier with L2 regularisation (\u03bb \u22121 = 2.78) and employ sentence-level features derived from the automatic parse of the clause: the verb lemma; POS; tense; use of the passive; a word embedding for the verb ; a bag of words to represent the sentence context; the lemmas of the verb's grammatical dependants; the GermaNet () semantic class for the verb and its subject and object; the adverb modifying the verb, if available; and the subcategorisation frame of the verb, given by the rule-based classifier of.", "labels": [], "entities": [{"text": "POS", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.9520393013954163}]}, {"text": "Training and testing use 10-fold cross validation.", "labels": [], "entities": []}, {"text": "shows accuracies and baselines, which always predict the training set's most frequent label.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9826304316520691}]}, {"text": "The first classifier predicts the full 6-way classification of the annotation.", "labels": [], "entities": []}, {"text": "To handle aspectual ambiguity, each verb instance maps to an ambiguity class consisting of one or more aspectual class labels.", "labels": [], "entities": []}, {"text": "The distribution of ambiguity classes is long-tailed, and we discard data points with labels less frequent than a threshold set to 10.", "labels": [], "entities": []}, {"text": "In the case of the 6-way classifier, this removes 40 data points, and results in 10 ambiguity classes in total.", "labels": [], "entities": []}, {"text": "The second and the third classifier test our expectation that our resource is useful for less finegrained aspectual classifications, too.", "labels": [], "entities": [{"text": "aspectual classifications", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.7093949615955353}]}, {"text": "The second classifier disregards the punctual-extended feature (collapsing the two change and the two nonchange classes), i.e., follows Egg's (2005) classification.", "labels": [], "entities": []}, {"text": "18 data points are dropped, leaving 7 possible labels.", "labels": [], "entities": []}, {"text": "The third classifier disregards the change/no-change distinction, corresponding to classes.", "labels": [], "entities": []}, {"text": "26 data points are dropped, resulting in 7 possible labels.", "labels": [], "entities": []}, {"text": "These three models achieve similar error rate reductions over the baseline of about 60%.", "labels": [], "entities": [{"text": "error rate reductions", "start_pos": 35, "end_pos": 56, "type": "METRIC", "confidence": 0.9742964108784994}]}, {"text": "The 4-way classifier, which ignores the extended-punctual distinction, outperforms the Vendlerian classifier, which includes it; this suggests that the extendedpunctual distinction is more difficult to identify and to model.", "labels": [], "entities": []}, {"text": "The following three classifiers are motivated by classifications in prior work.", "labels": [], "entities": []}, {"text": "The fourth one ('Stativity') predicts whether a token is stative (1077), dynamic (2915), or ambiguous in context (60).", "labels": [], "entities": []}, {"text": "This corresponds to Experiment 1 of.", "labels": [], "entities": []}, {"text": "Their baseline of 0.725 and their classifier accuracy of 0.841 are both similar to our results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9527187943458557}]}, {"text": "We can also replicate their Experiment 2 by stratifying the cross validation folds by verb lemma, showing the performance of the classifier on unseen verbs.", "labels": [], "entities": []}, {"text": "Our accuracy here is 0.811, almost identical to their reported 0.819.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996020197868347}]}, {"text": "The fifth classifier approximates the classification task of, distinguishing 'atelic' (1707, our stative and unbounded verbs), 'telic' (1794, our change of state verbs), and 'variable telicity' (551, our no-change verbs, plus verbs that are ambiguous between the other two categories).", "labels": [], "entities": []}, {"text": "Our results exceed theirs (0.675 accuracy with a 0.484 baseline).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9941390752792358}]}, {"text": "The sixth classifier predicts whether a verb token is 'culminated' or 'non-culminated', corresponding to the task of Experiment 2 of, p.618).", "labels": [], "entities": []}, {"text": "Culminated verbs (1834) are our change verbs, and non-culminated verbs (1077), the union of our unbounded and no-change verbs; 59 verbs are ambiguous in context.", "labels": [], "entities": []}, {"text": "Siegel and McKeown report a baseline of 0.633, similar to ours, and their classifier achieves 0.740, which we outperform.", "labels": [], "entities": []}, {"text": "These experiments support several conclusions.", "labels": [], "entities": []}, {"text": "First, we have shown our resource can be used to build machine learning classifiers of high quality, speaking to the validity of our corpus.", "labels": [], "entities": []}, {"text": "While we can only draw indirect comparisons to previous work in English and French, the accuracies achieved by our classifiers suggest that we go beyond the state of the art in our work.", "labels": [], "entities": []}, {"text": "Second, our resource has proven to be very flexible in that it can be broken down in different ways to capture different aspectual distinctions, which is very welcome considering the wide range of aspectual classifications.", "labels": [], "entities": []}, {"text": "Finally, the better performance of the 4-way classifier compared to the Venderian classifier, combined with the \u03ba value for the extended-punctual distinction () seems to indicate that both machines and human annotators find it hard to judge the length of time of a reported event.", "labels": [], "entities": []}, {"text": "As hypothesised, this distinction has proved to be the most difficult of our four aspectual features; this finding accords with, who report that durativity is the hardest aspectual feature to classify.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Agreement on the aspectual class annotation.", "labels": [], "entities": []}, {"text": " Table 3: Classifier accuracy on aspect labelling tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9596751928329468}, {"text": "aspect labelling tasks", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8045750657717387}]}]}