{"title": [{"text": "Self-Attentive, Multi-Context One-Class Classification for Unsupervised Anomaly Detection on Text", "labels": [], "entities": [{"text": "Unsupervised Anomaly Detection", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6677277783552805}]}], "abstractContent": [{"text": "There exist few text-specific methods for un-supervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words.", "labels": [], "entities": [{"text": "un-supervised anomaly detection", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7225544452667236}]}, {"text": "In this paper we introduce anew anomaly detection method-Context Vector Data Description (CVDD)-which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism.", "labels": [], "entities": []}, {"text": "Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an un-labeled text corpus.", "labels": [], "entities": [{"text": "contextual anomaly detection of sentences and phrases", "start_pos": 49, "end_pos": 102, "type": "TASK", "confidence": 0.7907707861491612}]}, {"text": "These contexts in combination with the self-attention weights make our method highly interpretable.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the well-known Reuters, 20 Newsgroups, and IMDB Movie Reviews datasets.", "labels": [], "entities": [{"text": "Reuters, 20 Newsgroups", "start_pos": 99, "end_pos": 121, "type": "DATASET", "confidence": 0.7414597868919373}, {"text": "IMDB Movie Reviews datasets", "start_pos": 127, "end_pos": 154, "type": "DATASET", "confidence": 0.9001370668411255}]}], "introductionContent": [{"text": "Anomaly Detection (AD) ( is the task of discerning rare or unusual samples in a corpus of unlabeled data.", "labels": [], "entities": [{"text": "Anomaly Detection (AD)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8954623103141784}]}, {"text": "A common approach to AD is one-class classification (, where the objective is to learn a model that compactly describes \"normality\"-usually assuming that most of the unlabeled training data is \"normal,\" i.e. non-anomalous.", "labels": [], "entities": [{"text": "AD", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9741882085800171}, {"text": "one-class classification", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.684584230184555}]}, {"text": "Deviations from this description are then deemed to be anomalous.", "labels": [], "entities": []}, {"text": "Examples of one-class classification methods are the well-known One-Class SVM (OC-SVM) () and Support Vector Data Description (SVDD) (.", "labels": [], "entities": [{"text": "Support Vector Data Description (SVDD)", "start_pos": 94, "end_pos": 132, "type": "TASK", "confidence": 0.6833243753228869}]}, {"text": "Applying AD to text is useful for many applications including discerning anomalous web content (e.g. posts, reviews, or product descriptions), automated content management, spam detection, and characterizing news articles so as to identify similar or dissimilar novel topics.", "labels": [], "entities": [{"text": "Applying AD", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8300141990184784}, {"text": "discerning anomalous web content (e.g. posts, reviews, or product descriptions)", "start_pos": 62, "end_pos": 141, "type": "TASK", "confidence": 0.768130110842841}, {"text": "automated content management", "start_pos": 143, "end_pos": 171, "type": "TASK", "confidence": 0.690756102403005}, {"text": "spam detection", "start_pos": 173, "end_pos": 187, "type": "TASK", "confidence": 0.8842205107212067}]}, {"text": "Recent work has found that proper text representation is critical for designing well-performing machine learning algorithms.", "labels": [], "entities": [{"text": "text representation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7123228460550308}]}, {"text": "Given the exceptional impact that universal vector embeddings of words () such as word2vec (,, and FastText (  or dynamic vector embeddings of text by language models such as ELMo () and BERT () have had on NLP, it is somewhat surprising that there has been no work on adapting AD techniques to use such unsupervised pre-trained models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.9720419049263}]}, {"text": "Existing AD methods for text still typically rely on bag-of-words (BoW) text representations.", "labels": [], "entities": []}, {"text": "In this work, we introduce a novel one-class classification method that takes advantage of pretrained word embedding models for performing AD on text.", "labels": [], "entities": [{"text": "one-class classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7239169776439667}]}, {"text": "Starting with pre-trained word embeddings, our method-Context Vector Data Description (CVDD)-finds a collection of transforms to map variable-length sequences of word embeddings to a collection of fixed-length text representations via a multi-head self-attention mechanism.", "labels": [], "entities": []}, {"text": "These representations are trained along with a collection of context vectors such that the context vectors and representations are similar while keeping the context vectors diverse.", "labels": [], "entities": []}, {"text": "Training these representations and context vectors jointly allows our algorithm to capture multiple modes of normalcy which may, for example, correspond to a collection of distinct yet non-anomalous topics.", "labels": [], "entities": []}, {"text": "Disentangling such modes allows for contextual anomaly detection with sample-based explanations and enhanced model interpretability.", "labels": [], "entities": [{"text": "contextual anomaly detection", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.7127690513928732}]}, {"text": "shows the most important words from three of the CVDD contexts.", "labels": [], "entities": [{"text": "CVDD contexts", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.869756430387497}]}, {"text": "Our model successfully disentangles positive and negative sentiments as well as cinematic language in an unsupervised manner.", "labels": [], "entities": []}, {"text": "(c) shows the most normal examples w.r.t. those contexts with explanations given by the highlighted self-attention weights.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of CVDD quantitatively in one-class classification experiments on the Reuters-21578 1 and 20 Newsgroups 2 datasets as well as qualitatively in an application on IMDB Movie Reviews 3 to detect anomalous reviews.", "labels": [], "entities": [{"text": "Reuters-21578 1 and 20 Newsgroups 2 datasets", "start_pos": 98, "end_pos": 142, "type": "DATASET", "confidence": 0.9468553236552647}, {"text": "IMDB Movie Reviews 3", "start_pos": 189, "end_pos": 209, "type": "DATASET", "confidence": 0.8995869904756546}]}, {"text": "Pre-trained Models We employ the pre-trained GloVe () as well as fastText (  word embeddings in our experiments.", "labels": [], "entities": [{"text": "fastText", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8457897901535034}]}, {"text": "For GloVe we consider the 6B tokens vector embeddings of p = 300 dimensions that have been trained on the Wikipedia and Gigaword 5 corpora.", "labels": [], "entities": [{"text": "Gigaword 5 corpora", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.8826625148455302}]}, {"text": "For fastText we consider the English word vectors that also have p = 300 dimensions which have been trained on the Wikipedia and English webcrawl.", "labels": [], "entities": []}, {"text": "We also experimented with dynamic word embeddings from the BERT () language model but did not observe improvements over GloVe or fastText on the considered datasets that would justify the added computational cost.", "labels": [], "entities": [{"text": "BERT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9153763651847839}]}, {"text": "Baselines We consider three baselines for aggregating word vector embeddings to fixed-length sentence representations: (i) mean, (ii) tf-idf weighted mean, and (iii) max-pooling.", "labels": [], "entities": []}, {"text": "It has been repeatedly observed that the simple average sentence embedding proves to be a strong baseline in many tasks (.", "labels": [], "entities": []}, {"text": "Max-pooling is commonly applied over hidden activations (.", "labels": [], "entities": []}, {"text": "The tf-idf weighted mean is a natural sentence embedding baseline that includes document-to-term co-occurrence statistics.", "labels": [], "entities": []}, {"text": "For AD, we then consider the OC-SVM () with cosine kernel (which in this case is equivalent to SVDD ()) on these sentence embeddings where we always train for hyperparameters \u03bd \u2208 {0.05, 0.1, 0.2, 0.5} and report the best result.", "labels": [], "entities": []}, {"text": "CVDD configuration We employ self-attention with d a = 150 for CVDD and present results for r \u2208 {3, 5, 10} number of attention heads.", "labels": [], "entities": []}, {"text": "We use Adam () with a batch size of 64 for optimization and first train for 40 epochs with a learning rate of \u03b7 = 0.01 after which we train another 60 epochs with \u03b7 = 0.001, i.e. we establish a simple two-phase learning rate schedule.", "labels": [], "entities": []}, {"text": "For weighting contexts, we consider the case of equal weights (\u03b1 = 0) as well as a logarithmic annealing strategy \u03b1 \u2208 {0, 10 \u22124 , 10 \u22123 , 10 \u22122 , 10 \u22121 } where we update \u03b1 every 20 epochs.", "labels": [], "entities": []}, {"text": "For multicontext regularization, we choose \u03bb \u2208 {1, 10}.", "labels": [], "entities": [{"text": "multicontext regularization", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8364723026752472}]}, {"text": "Data pre-processing On all three datasets, we always lowercase text and strip punctuation, numbers, as well as redundant whitespaces.", "labels": [], "entities": []}, {"text": "Moreover, we remove stopwords using the stopwords list from the nltk library () and only consider words with a minimum length of 3 characters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: AUCs (in %) of one-class classification experiments on Reuters and 20 Newsgroups.", "labels": [], "entities": [{"text": "AUCs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8685349822044373}, {"text": "one-class classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.674932062625885}, {"text": "Reuters and 20 Newsgroups", "start_pos": 65, "end_pos": 90, "type": "DATASET", "confidence": 0.9104134291410446}]}]}