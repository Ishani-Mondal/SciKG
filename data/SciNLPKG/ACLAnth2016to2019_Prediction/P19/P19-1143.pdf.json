{"title": [{"text": "Training Hybrid Language Models by Marginalizing over Segmentations", "labels": [], "entities": [{"text": "Marginalizing over Segmentations", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.9216694633165995}]}], "abstractContent": [{"text": "In this paper, we study the problem of hybrid language modeling, that is using models which can predict both characters and larger units such as character ngrams or words.", "labels": [], "entities": [{"text": "hybrid language modeling", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6502081453800201}]}, {"text": "Using such models, multiple potential segmenta-tions usually exist fora given string, for example one using words and one using characters only.", "labels": [], "entities": []}, {"text": "Thus, the probability of a string is the sum of the probabilities of all the possible segmentations.", "labels": [], "entities": []}, {"text": "Here, we show how it is possible to marginalize over the segmentations efficiently, in order to compute the true probability of a sequence.", "labels": [], "entities": []}, {"text": "We apply our technique on three datasets, comprising seven languages, showing improvements over a strong character level language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical language modeling is the problem of estimating a probability distribution over text data ().", "labels": [], "entities": [{"text": "Statistical language modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7746886213620504}]}, {"text": "Most approaches formulate this problem at the word level, by first segmenting the text using a fixed vocabulary.", "labels": [], "entities": []}, {"text": "A limitation of these methods is that they cannot generate new words, or process out of vocabulary words.", "labels": [], "entities": []}, {"text": "A popular alternative is to directly model sequences at the character level.", "labels": [], "entities": []}, {"text": "These models can potentially generate any sequence, and are thus sometimes referred to as open vocabulary.", "labels": [], "entities": []}, {"text": "However, they tend to underperform compared to word level models when trained on the same data.", "labels": [], "entities": []}, {"text": "For these reasons, a few works have proposed hybrid models, that work both at the character and word level (or sometimes groups of characters).", "labels": [], "entities": []}, {"text": "A first class of hybrid models switch between word and character level representations, depending on whether they predict that the upcoming word is in the vocabulary or not (.", "labels": [], "entities": []}, {"text": "For example, a first model can be trained on tokenized data, where outof-vocabulary words are replaced by the <unk> token.", "labels": [], "entities": []}, {"text": "A second model is then used to generate the character sequences corresponding to out-ofvocabulary words.", "labels": [], "entities": []}, {"text": "Another approach, which does not require tokenization, is to process groups of characters, which are obtained based on linguistic knowledge or low level statistics.", "labels": [], "entities": []}, {"text": "These include merging characters using mutual information () or the byte pair encoding algorithm.", "labels": [], "entities": []}, {"text": "This approach first produces a segmentation for the text, and then learns a language model on it.", "labels": [], "entities": []}, {"text": "However, some sequences have multiple possible segmentations, and a model considering a single one might underestimate the true probability of the sequence.", "labels": [], "entities": []}, {"text": "Thus, it is important to marginalize over the set of segmentations to obtain the true probability of a sequence.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative approach to address this limitation, and in particular, to train models by marginalizing over the set of segmentations.", "labels": [], "entities": []}, {"text": "As the number of possible segmentations grows exponentially with the sequence size, using an efficient algorithm such as dynamic programming is important.", "labels": [], "entities": []}, {"text": "Computing the representation of the context at the character level allows to apply dynamic programming to this problem, without using approximations.", "labels": [], "entities": []}, {"text": "This technique was previously considered in the context of automatic speech recognition ( or to copy tokens from the input for code generation (.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6336987117926279}]}, {"text": "We evaluate our method on three datasets for character level language modeling, showing that adding n-grams to the predictions improve the perplexity of the model.", "labels": [], "entities": [{"text": "character level language modeling", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.6804948151111603}]}], "datasetContent": [{"text": "In this section, we describe the experiments that we performed to evaluate our approach on character level language modeling.", "labels": [], "entities": [{"text": "character level language modeling", "start_pos": 91, "end_pos": 124, "type": "TASK", "confidence": 0.7186682522296906}]}, {"text": "We consider 3 datasets derived from Wikipedia articles, but with different preprocessing.", "labels": [], "entities": []}, {"text": "The text8 dataset of M.", "labels": [], "entities": [{"text": "text8 dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8752849698066711}]}, {"text": "Mahoney 1 contains 100 million characters from Wikipedia, and was preprocessed to only contains the lowercase letters a-z and nonconsecutive spaces.", "labels": [], "entities": [{"text": "Mahoney 1", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9409795105457306}]}, {"text": "The WikiText2 dataset was introduced by with a different preprocessing from text8: numbers, capital letters and special characters are kept.", "labels": [], "entities": [{"text": "WikiText2 dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9518709182739258}]}, {"text": "The vocabulary size is 1152.", "labels": [], "entities": []}, {"text": "We use the raw version of the dataset, which is tokenized but where rare words are not replaced by the <unk> token.", "labels": [], "entities": []}, {"text": "The training data contains 10.9 millions characters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set bpc on the MWC dataset. The hyperparameters for our method are chosen on the validation set of  WikiText2. Note that Mielke and Eisner (2019) applied the BPE baseline and their method to both tokenized  and non-tokenized data. All the other methods were applied on non-tokenized data only.", "labels": [], "entities": [{"text": "MWC dataset", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9776949882507324}, {"text": "BPE baseline", "start_pos": 173, "end_pos": 185, "type": "DATASET", "confidence": 0.817872017621994}]}, {"text": " Table 2: Test set bpc on the text8 dataset.", "labels": [], "entities": [{"text": "text8 dataset", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.900879830121994}]}, {"text": " Table 3: Test set bpc on the WikiText2 dataset.", "labels": [], "entities": [{"text": "WikiText2 dataset", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9718583226203918}]}]}