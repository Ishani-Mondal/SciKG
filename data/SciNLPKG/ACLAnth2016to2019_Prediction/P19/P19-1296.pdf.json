{"title": [{"text": "Sentence-Level Agreement for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.8076624671618143}]}], "abstractContent": [{"text": "The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.8397849599520365}]}, {"text": "In NMT, there is a natural correspondence between the source sentence and the target sentence.", "labels": [], "entities": []}, {"text": "However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level.", "labels": [], "entities": []}, {"text": "In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence.", "labels": [], "entities": []}, {"text": "The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences.", "labels": [], "entities": [{"text": "NMT", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.8541399836540222}]}, {"text": "Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance.", "labels": [], "entities": [{"text": "NIST", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9406400322914124}, {"text": "WMT English-to-German tasks", "start_pos": 53, "end_pos": 80, "type": "DATASET", "confidence": 0.6714601119359335}]}], "introductionContent": [{"text": "Neural network based methods have been applied to several natural language processing tasks (.", "labels": [], "entities": []}, {"text": "In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.870014101266861}, {"text": "phrase-based statistical machine translation", "start_pos": 57, "end_pos": 101, "type": "TASK", "confidence": 0.6291406452655792}]}, {"text": "It is based on the estimation of a probability distribution overall input words for each target word.", "labels": [], "entities": []}, {"text": "However, source and target words are in different representation space, and they still have to go through along information processing procedure that may lead to the source words are incorrectly translated into the target words.", "labels": [], "entities": []}, {"text": "Based on this hypothesis, proposed a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation.", "labels": [], "entities": []}, {"text": "incorporated a reconstructor module into NMT, which reconstructs the input source sentence from the hidden layer of the output target sentence to enhance source representation.", "labels": [], "entities": []}, {"text": "However, in previous studies, the training objective function was usually based on word-level and lacked explicit sentencelevel relationships.", "labels": [], "entities": []}, {"text": "Although Transformer model ( has archived state-of-the-art performance of NMT, more attention is paid to the words-level relationship via self-attention networks.", "labels": [], "entities": []}, {"text": "Sentence-level agreement method has been applied to many natural language processing tasks.", "labels": [], "entities": []}, {"text": "used sentence similarity measure technique for automatic text summarization.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.5843145747979482}]}, {"text": "have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7402373254299164}, {"text": "VSM", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.6861245036125183}, {"text": "FAQ", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.7202414274215698}]}, {"text": "presented a sentence similarity method for spoken dialogue system to improve accuracy.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.666061207652092}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.998237133026123}]}, {"text": "proposed sentence similarity measures to improve the estimation of topical relevance.", "labels": [], "entities": [{"text": "estimation of topical relevance", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.7370171248912811}]}, {"text": "used sentence similarity to select sentences with the similar domains.", "labels": [], "entities": []}, {"text": "The above methods only considered monolingual sentence-level agreement.", "labels": [], "entities": []}, {"text": "In human translation, a translator's primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning.", "labels": [], "entities": [{"text": "human translation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.716189980506897}]}, {"text": "Therefore, in early machine translation studies, such as example-based machine translation, use the sentence similarity matching between the sentences to be translated and the sentences in the bilingual corpora to extract translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7327688038349152}, {"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7267474383115768}, {"text": "extract translation", "start_pos": 214, "end_pos": 233, "type": "TASK", "confidence": 0.6698573231697083}]}, {"text": "Inspired by these studies, we establish a sentence-level agreement channel directly in the deep neural network to shorten the distance between the source and target sentence-level embeddings.", "labels": [], "entities": []}, {"text": "Specifically, our model can be effectively applied to NMT in two aspects: \u2022 Sentence-Level Agreement as Training Objective: we use the sentence-level agreement as apart of the training objective function.", "labels": [], "entities": []}, {"text": "In this way, we not only consider the translation of the word level but also consider the sentence level.", "labels": [], "entities": []}, {"text": "\u2022 Enhance Source Representation: As our model can make the vector distribution of the sentence-level between source-side and target-side closer, we can combine their sentence-level embeddings to enhance the source representation.", "labels": [], "entities": [{"text": "Enhance Source Representation", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.6596211493015289}]}, {"text": "Experimental results on Chinese-to-English and English-to-German translation tasks demonstrate that our model is able to effectively improve the performance of NMT.", "labels": [], "entities": [{"text": "English-to-German translation tasks", "start_pos": 47, "end_pos": 82, "type": "TASK", "confidence": 0.7578948537508646}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Translation results for Chinese-English and English-German translation task. \" \u2020\": indicates statistically  better than Transformer(Base/Big) (\u03c1 < 0.01).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9546434879302979}, {"text": "English-German translation task", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6886396209398905}]}, {"text": " Table 2: The efficiency analysis on English-German task. \"Param\" denotes the trainable parameter size of each  model (M=million) and Beam=10.", "labels": [], "entities": [{"text": "Param", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.985341489315033}, {"text": "Beam", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.998980700969696}]}, {"text": " Table 3: Source-to-target sentence-level similarity analysis on Chinese-English and English-German translation  task.", "labels": [], "entities": []}]}