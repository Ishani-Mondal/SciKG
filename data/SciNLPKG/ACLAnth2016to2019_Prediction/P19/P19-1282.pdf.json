{"title": [], "abstractContent": [{"text": "Attention mechanisms have recently boosted performance on a range of NLP tasks.", "labels": [], "entities": []}, {"text": "Because attention layers explicitly weight input compo-nents' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens).", "labels": [], "entities": []}, {"text": "We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions.", "labels": [], "entities": [{"text": "text classification", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.727925717830658}]}, {"text": "While we observe some ways in which higher attention weights correlate with greater impact on model predictions , we also find many ways in which this does not hold, i.e., where gradient-based rank-ings of attention weights better predict their effects than their magnitudes.", "labels": [], "entities": []}, {"text": "We conclude that while attention noisily predicts input compo-nents' overall importance to a model, it is by no means a fail-safe indicator.", "labels": [], "entities": []}], "introductionContent": [{"text": "Interpretability is a pressing concern for many current NLP models.", "labels": [], "entities": []}, {"text": "As they become increasingly complex and learn decision-making functions from data, ensuring our ability to understand why a particular decision occurred is critical.", "labels": [], "entities": []}, {"text": "Part of that development has been the incorporation of attention mechanisms ( into models fora variety of tasks.", "labels": [], "entities": []}, {"text": "For many different problems-to name a few, machine translation (, syntactic parsing (, reading comprehension (), and language modeling (-incorporating attention mechanisms into models has proven beneficial for performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8268287181854248}, {"text": "syntactic parsing", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7471588850021362}, {"text": "language modeling", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7469387650489807}]}, {"text": "While there are many variants of attention (, each for-mulation consists of the same high-level goal: calculating nonnegative weights for each input component (e.g., word) that together sum to 1, multiplying those weights by their corresponding representations, and summing the resulting vectors into a single fixed-length representation.", "labels": [], "entities": []}, {"text": "Since attention calculates a distribution over inputs, prior work has used attention as a tool for interpretation of model decisions (.", "labels": [], "entities": []}, {"text": "The existence of so much work on visualizing attention weights is a testament to attention's popularity in this regard; to name just a few examples of these weights being examined to understand a model, recent work has focused on goals from explaining and debugging the current system's decision ( to distilling important traits of a dataset.", "labels": [], "entities": []}, {"text": "Despite this, existing work on interpretability is only beginning to assess what computed attention weights actually communicate.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.9621610641479492}]}, {"text": "In an independent and contemporaneous study, Jain and Wallace (2019) explore whether attention mechanisms can identify the relative importance of inputs to the full model, finding them to be highly inconsistent predictors.", "labels": [], "entities": []}, {"text": "In this work, we apply a different analysis based on intermediate representation erasure to assess whether attention weights can instead be relied upon to explain the relative importance of the inputs to the attention layer itself.", "labels": [], "entities": []}, {"text": "We find similar cause for concern: attention weights are only noisy predictors of even intermediate components' importance, and should not be treated as justification fora decision.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Percent of test instances in each decision-flip  indicator variable category for each HANrnn.", "labels": [], "entities": [{"text": "HANrnn", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.5621752142906189}]}]}