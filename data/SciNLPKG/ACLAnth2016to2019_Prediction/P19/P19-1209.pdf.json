{"title": [{"text": "Scoring Sentence Singletons and Pairs for Abstractive Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.5870940685272217}]}], "abstractContent": [{"text": "When writing a summary, humans tend to choose content from one or two sentences and merge them into a single summary sentence.", "labels": [], "entities": []}, {"text": "However, the mechanisms behind the selection of one or multiple source sentences remain poorly understood.", "labels": [], "entities": []}, {"text": "Sentence fusion assumes multi-sentence input; yet sentence selection methods only work with single sentences and not combinations of them.", "labels": [], "entities": [{"text": "Sentence fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9434320330619812}, {"text": "sentence selection", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7097979038953781}]}, {"text": "There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7025030255317688}, {"text": "summarizing", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9901220202445984}]}, {"text": "This paper attempts to bridge the gap by ranking sentence singletons and pairs together in a unified space.", "labels": [], "entities": []}, {"text": "Our proposed framework attempts to model human methodology by selecting either a single sentence or a pair of sentences, then compressing or fusing the sentence(s) to produce a summary sentence.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments on both single-and multi-document summarization datasets and report findings on sentence selection and abstraction.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7471566796302795}]}], "introductionContent": [{"text": "Abstractive summarization aims at presenting the main points of an article in a succinct and coherent manner.", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7070409655570984}]}, {"text": "To achieve this goal, a proficient editor can rewrite a source sentence into a more succinct form by dropping inessential sentence elements such as prepositional phrases and adjectives.", "labels": [], "entities": []}, {"text": "She can also choose to fuse multiple source sentences into one by reorganizing the points in a coherent manner.", "labels": [], "entities": []}, {"text": "In fact, it appears to be common practice to summarize by either compressing single sentences or fusing multiple sentences.", "labels": [], "entities": [{"text": "summarize", "start_pos": 45, "end_pos": 54, "type": "TASK", "confidence": 0.9901062846183777}]}, {"text": "We investigate this hypothesis by analyzing human-written abstracts contained in three large datasets: DUC-04 (), CNN/Daily Mail, and XSum (.", "labels": [], "entities": [{"text": "DUC-04", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.9424669742584229}, {"text": "CNN/Daily Mail", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.9095211923122406}]}, {"text": "For every summary sentence, we find its ground-truth set containing one or more source Figure 1: Portions of summary sentences generated by compression (content is drawn from 1 source sentence) and fusion (content is drawn from 2 or more source sentences).", "labels": [], "entities": []}, {"text": "Humans often grab content from 1 or 2 document sentences when writing a summary sentence.", "labels": [], "entities": []}, {"text": "sentences that exhibit a high degree of similarity with the summary sentence (details in \u00a74).", "labels": [], "entities": []}, {"text": "As shown in, across the three datasets, 60-85% of summary sentences are generated by fusing one or two source sentences.", "labels": [], "entities": []}, {"text": "Selecting summary-worthy sentences has been studied in the literature, but there lacks a mechanism to weigh sentence singletons and pairs in a unified space.", "labels": [], "entities": []}, {"text": "Extractive methods focus on selecting sentence singletons using greedy, optimization-based (, and (non-)autoregressive methods (.", "labels": [], "entities": []}, {"text": "In contrast, existing sentence fusion studies tend to assume ground sets of source sentences are already provided, and the system fuses each set of sentences into a single one.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.751564085483551}]}, {"text": "There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7025030255317688}, {"text": "summarizing", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9901220202445984}]}, {"text": "This paper attempts to bridge the gap by ranking singletons and pairs together by their likelihoods of producing summary sentences.", "labels": [], "entities": []}, {"text": "The selection of sentence singletons and pairs can bring benefit to neural abstractive summarization, as a number of studies seek to separate content selection from summary generation.", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 68, "end_pos": 100, "type": "TASK", "confidence": 0.6899885733922323}, {"text": "summary generation", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.7109719812870026}]}, {"text": "Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary.", "labels": [], "entities": [{"text": "Content selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7569916546344757}, {"text": "summary generation", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7650013267993927}]}, {"text": "Despite having local coherence, system summaries can sometimes contain erroneous details () and forged content.", "labels": [], "entities": []}, {"text": "Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer.", "labels": [], "entities": [{"text": "content selection", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7291957437992096}, {"text": "summary generation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.770758330821991}]}, {"text": "In this paper we propose a method to learn to select sentence singletons and pairs, which then serve as the basis for an abstractive summarizer to compose a summary sentence-by-sentence, where singletons are shortened (i.e., compressed) and pairs are merged (i.e., fused).", "labels": [], "entities": []}, {"text": "We exploit stateof-the-art neural representations and traditional vector space models to characterize singletons and pairs; we then provide suggestions on the types of representations useful for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.9894902110099792}]}, {"text": "Experiments are performed on both single-and multi-document summarization datasets, where we demonstrate the efficacy of selecting sentence singletons and pairs as well as its utility to abstractive summarization.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 187, "end_pos": 212, "type": "TASK", "confidence": 0.5303907096385956}]}, {"text": "Our research contributions can be summarized as follows: \u2022 the present study fills an important gap by selecting sentence singletons and pairs jointly, assuming a summary sentence can be created by either shortening a singleton or merging a pair.", "labels": [], "entities": []}, {"text": "Compared to abstractive summarizers that perform content selection implicitly, our method is flexible and can be extended to multi-document summarization where training data is limited; \u2022 we investigate the factors involved in representing sentence singletons and pairs.", "labels": [], "entities": [{"text": "content selection", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6874267011880875}, {"text": "multi-document summarization", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.6000079959630966}]}, {"text": "We perform extensive experiments and report findings on sentence selection and abstraction.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7616719603538513}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Instance selection results; evaluated for primary, secondary, and all ground-truth sentences. Our BERT- SingPairMix method achieves strong performance owing to its capability of building effective representations for  both singletons and pairs.", "labels": [], "entities": [{"text": "BERT", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9807093143463135}]}, {"text": " Table 3: Summarization results on various datasets.  Whether abstractive summaries (BERT-Abst) outper- form its extractive variant (BERT-Extr) appears to be  related to the amount of sentence pairs selected by  BERT-SingPairMix. Selecting more pairs than single- tons seems to hurt the abstractor.", "labels": [], "entities": [{"text": "BERT-Abst", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9867570400238037}, {"text": "BERT-Extr", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9907077550888062}]}]}