{"title": [{"text": "Improved Generalization of Arabic Text Classifiers", "labels": [], "entities": [{"text": "Improved Generalization of Arabic Text Classifiers", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8564573923746744}]}], "abstractContent": [{"text": "While transfer learning for text has been very active in the English language, progress in Arabic has been slow, including the use of Domain Adaptation (DA).", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.9077270030975342}]}, {"text": "Domain Adaptation is used to generalize the performance of any clas-sifier by trying to balance the classifier's accuracy fora particular task among different text domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9947496056556702}]}, {"text": "In this paper, we propose and evaluate two variants of a domain adaptation technique: the first is abase model called Domain Adversarial Neural Network (DANN), while the second is a variation that incorporates representational learning.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.720978170633316}]}, {"text": "Similar to previous approaches, we propose the use of proxy A-distance as a metric to assess the success of generalization.", "labels": [], "entities": [{"text": "A-distance", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.7346271276473999}, {"text": "generalization", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.973742663860321}]}, {"text": "We make use of ArSentD-LEV, a multi-topic dataset collected from the Levantine countries, to test the performance of the models.", "labels": [], "entities": [{"text": "ArSentD-LEV", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9656935334205627}]}, {"text": "We show the superiority of the proposed method inaccuracy and robustness when dealing with the Arabic language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Processing (NLP) for Arabic is challenging due to the complexity of the language.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.714818278948466}]}, {"text": "Additionally, resources in Arabic are scarce making it difficult to achieve NLP progress at the pace of other resource-rich languages such as English (.", "labels": [], "entities": []}, {"text": "As a result, there is a need for transfer learning methods that can overcome the resource limitations.", "labels": [], "entities": []}, {"text": "In this paper, we propose the use of domain adaptation to address this challenge while considering the task of sentiment analysis (SA) also referred to as Opinion Mining (OM).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7505049705505371}, {"text": "sentiment analysis (SA)", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.832192063331604}, {"text": "Opinion Mining (OM)", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7304256319999695}]}, {"text": "When training over a dataset with multiple domains, different domains have different data distributions.", "labels": [], "entities": []}, {"text": "This has a negative impact when training on one domain and testing on another, since the model would not be able to generalize well.", "labels": [], "entities": []}, {"text": "Although domains within the same dataset have differences, they share some characteristics.", "labels": [], "entities": []}, {"text": "For example, consider reviews of Amazon products: reviews of electronic products are different from book reviews, but these two domains share the general structure of reviews.", "labels": [], "entities": []}, {"text": "We say there exists a shift in the data's distribution between the two domains.", "labels": [], "entities": []}, {"text": "To solve this problem, many approaches were proposed within the field of Domain Adaptation (DA).", "labels": [], "entities": [{"text": "Domain Adaptation (DA)", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.8760650992393494}]}, {"text": "This field is receiving a lot of attention in English, a lot more than its Arabic counterpart.", "labels": [], "entities": []}, {"text": "Solving the data shift problem is of interest for many reasons.", "labels": [], "entities": []}, {"text": "First, it is harder for machine learning to learn good internal representations on the Arabic text as opposed to English text.", "labels": [], "entities": []}, {"text": "This is due to the sparsity of the Arabic language, and its morphological complexity compared to English.", "labels": [], "entities": []}, {"text": "Another reason is the limited amount of available data, especially for dialects, which causes deep learning models to perform bad on any task.", "labels": [], "entities": []}, {"text": "Lastly, we are not aware of domain adaptation techniques for the Arabic language, and thus much work needs to be done in this area to catch up with the research in English.", "labels": [], "entities": []}, {"text": "Traditionally, researchers focused their efforts on extracting features shared between the source and target domains.", "labels": [], "entities": []}, {"text": "After the advancement of representational learning (, several algorithms were introduced.", "labels": [], "entities": [{"text": "representational learning", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.932942122220993}]}, {"text": "The most notable approaches are Stacked Denoising Autoencoder (SDA)).", "labels": [], "entities": [{"text": "Stacked Denoising Autoencoder (SDA))", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.674043208360672}]}, {"text": "Later, a modified version was introduced by.", "labels": [], "entities": []}, {"text": "This version, called marginalized Stacked Denoising Autoencoder (mSDA), introduced a speedup compared to the original SDA since the input/output relation was provided in closed form.", "labels": [], "entities": []}, {"text": "After Generative Adversarial Nets () were introduced, the interest in adversarial training increased.", "labels": [], "entities": []}, {"text": "Researchers developed new approaches that solve the DA problem through adversarial training, with emphasis on applications in computer vision and limited exploration for NLP.", "labels": [], "entities": [{"text": "DA problem", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.9519656896591187}]}, {"text": "The most notable approaches are Domain Adversarial Neural Networks (DANN) (, Domain Separation Network (DSN) (, Adversarial Discriminative Domain Adaptation (ADDA) ( and Conditional Adversarial Domain Adaptation (.", "labels": [], "entities": [{"text": "Domain Separation Network (DSN)", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.8243278861045837}, {"text": "Adversarial Discriminative Domain Adaptation (ADDA)", "start_pos": 112, "end_pos": 163, "type": "TASK", "confidence": 0.7424768720354352}, {"text": "Conditional Adversarial Domain Adaptation", "start_pos": 170, "end_pos": 211, "type": "TASK", "confidence": 0.6848014891147614}]}, {"text": "Although limited in Arabic, some efforts have been spent to solve the domain shift problem (.", "labels": [], "entities": []}, {"text": "In this paper, we propose and evaluate some adversarial approaches for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7662236988544464}]}, {"text": "The first is a regular DANN model while the second is a variant of DANN that incorporates representational learning.", "labels": [], "entities": []}, {"text": "To assess the success of domain adaptation, we use the proxy A-distance as a matrix.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7892606556415558}, {"text": "A-distance", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.8077347278594971}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents different approaches for DA.", "labels": [], "entities": [{"text": "DA", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9925978183746338}]}, {"text": "Section 3 introduces the algorithms to be evaluated, and describes the dataset.", "labels": [], "entities": []}, {"text": "Section 4 presents the experiments and the results.", "labels": [], "entities": []}, {"text": "We finally summarize our work and conclude the paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the effectiveness of the proposed approach, we conduct a 5-point sentiment classification on ArSentD-LEV 1, once using the country of origin of the tweet as domain, and once the category to which the tweet belongs.", "labels": [], "entities": [{"text": "ArSentD-LEV 1", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.8060263097286224}]}, {"text": "We then show the effect of the data size on the performance of the adaptation algorithms.", "labels": [], "entities": []}, {"text": "We start by describing the available dataset, then we describe each experiment alongside its results and we include some insights.", "labels": [], "entities": []}, {"text": "ArSentD-LEV is a multi-domain dataset containing almost 4,000 tweets collected equally from the 4 Levantine countries: Jordan, Lebanon, Palestine and Syria.", "labels": [], "entities": [{"text": "ArSentD-LEV", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.7331352233886719}]}, {"text": "For each tweet, the following labels are available: the country of origin, the sentiment conveyed by the tweet on 5-point scale (from very negative to very positive), the way of expressing the sentiment (explicit vs implicit) and the category to which the tweet belongs.", "labels": [], "entities": []}, {"text": "The tweets were divided into 5 categories: politics, personal, sports, religious and other.", "labels": [], "entities": []}, {"text": "The distribution of the tweets amongst these categories is shown in.", "labels": [], "entities": []}, {"text": "Following the approach used by, we extract from the dataset the 5,000 most frequent unigrams and bigrams, as was adopted in ( for English.", "labels": [], "entities": []}, {"text": "We then form, using these unigrams and bigrams, a bag-of-words matrix that will be used as input data for the learned models.", "labels": [], "entities": []}, {"text": "Although many models represent text better (e.g. sequence models, tree models, etc...) we limit ourselves to a simpler model to show the improvement by the domain adaptation technique rather than by the text model.", "labels": [], "entities": []}, {"text": "The different experiments evaluated the performance of four models.", "labels": [], "entities": []}, {"text": "A Linear SVM was used as a baseline and representative of feature based models.", "labels": [], "entities": []}, {"text": "For the deep learning models, we consider a fully-connected neural network) consisting of a hidden layer of 100 neurons and a label predictor of size 2.", "labels": [], "entities": []}, {"text": "The setup of DANN is similar to that in (.", "labels": [], "entities": [{"text": "DANN", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.6673778295516968}]}, {"text": "The hidden layer is composed of 100 neurons, and the label predictor is of size 2.", "labels": [], "entities": []}, {"text": "The domain classifier of DANN (of size 2) is preceded by a GRL.", "labels": [], "entities": [{"text": "DANN", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8020389676094055}]}, {"text": "The proposed model is identical to the description in section 3.", "labels": [], "entities": []}, {"text": "All neural networks were trained using ADAM optimizer (  For this experiment, we evaluate the adaptation task between tweets from different countries.", "labels": [], "entities": []}, {"text": "This means the source domain will consist of tweets from one of the 4 Levantine countries, and the target domain will consist of tweets coming from other countries.", "labels": [], "entities": []}, {"text": "We thus have a total of 12 adaptation tasks.", "labels": [], "entities": []}, {"text": "Baly et al. showed that Twitter is used for different purposes in different countries (, which presents an additional challenge.", "labels": [], "entities": []}, {"text": "The result of the domain adaptation tasks are shown in.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7263666540384293}]}, {"text": "The proposed method outperformed all other models inmost of the adaptation tasks.", "labels": [], "entities": []}, {"text": "Although many real-life applications showed that traditional machine learning models are usually better when the available data is little (), the proposed model was able to outperform the linear SVM inmost of the tasks in our experiment.", "labels": [], "entities": []}, {"text": "This means it was able to extract useful representation from the data.", "labels": [], "entities": []}, {"text": "The model was also able to outperform DANN, which shows that the representational learning provides intrinsic representation of the data.", "labels": [], "entities": []}, {"text": "In this second experiment, we consider the task of adapting tweets from different topics.", "labels": [], "entities": []}, {"text": "ArSentD-LEV (Baly et al., 2018) contains 5 classes for topic: politics, personal, religious, sports and other.", "labels": [], "entities": [{"text": "ArSentD-LEV", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9174246191978455}]}, {"text": "This means we have a total of 20 tasks.", "labels": [], "entities": []}, {"text": "The models evaluated are the linear SVM, DANN and the proposed model.", "labels": [], "entities": [{"text": "DANN", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.8432745933532715}]}, {"text": "The models' structure is identical to the one defined in section 4.2.", "labels": [], "entities": []}, {"text": "The results of the experiment are shown in.", "labels": [], "entities": []}, {"text": "The behavior of the algorithms is significantly different in these categories.", "labels": [], "entities": []}, {"text": "This is caused by the unbalanced data distribution amongst the different topics, as can be seen in.", "labels": [], "entities": []}, {"text": "We can see that whenever the data is very limited, the linear SVM outperforms the deep learning models.", "labels": [], "entities": []}, {"text": "This is expected since neural networks cannot learn well the underlying representation when the data is scarce.", "labels": [], "entities": []}, {"text": "Looking at the radar plot in, we can find the following interesting property.", "labels": [], "entities": []}, {"text": "The higher the PAD distance between the source and target domains, the better the performance of the proposed model.", "labels": [], "entities": [{"text": "PAD distance", "start_pos": 15, "end_pos": 27, "type": "METRIC", "confidence": 0.9119158685207367}]}, {"text": "This can be related to the fact that the proposed model tries to find a hidden representation that combines features from both source and target domains, i.e. decrease the distance between the 2 domains.", "labels": [], "entities": []}, {"text": "Whenever the distance is low, the proposed model cannot thus decrease it much further.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies of linear SVM, NN, DANN and  the proposed approach for Cross-Country adaptation  on ArSentD-LEV. We can see that the proposed vari- ant outperforms other models in almost all DA tasks.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9880324602127075}, {"text": "DANN", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.927424430847168}, {"text": "Cross-Country adaptation", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.8754890561103821}, {"text": "ArSentD-LEV", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.8502854108810425}]}, {"text": " Table 2: Accuracies of linear SVM, DANN and the  proposed approach for Cross-Topic on ArSentD-LEV.  We can see that the SVM and the proposed variant are  performing better than DANN, with SVM performing  better when available data is little.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.977700412273407}]}]}