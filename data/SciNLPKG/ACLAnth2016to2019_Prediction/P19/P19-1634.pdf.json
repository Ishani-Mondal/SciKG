{"title": [{"text": "Bias Analysis and Mitigation in the Evaluation of Authorship Verification", "labels": [], "entities": [{"text": "Bias Analysis", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9350608885288239}, {"text": "Evaluation of Authorship Verification", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.6480432525277138}]}], "abstractContent": [{"text": "The PAN series of shared tasks is well known for its continuous and high quality research in the field of digital text forensics.", "labels": [], "entities": [{"text": "digital text forensics", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.6059171458085378}]}, {"text": "Among others, PAN contributions include original corpora , tailored benchmarks, and standardized experimentation platforms.", "labels": [], "entities": []}, {"text": "In this paper we review, theoretically and practically, the authorship verification task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art-in fact, it allows for top benchmarking with a surprisingly straightforward approach.", "labels": [], "entities": []}, {"text": "In this regard , we present a \"Basic and Fairly Flawed\" (BAFF) authorship verifier that is on a par with the best approaches submitted so far, and that illustrates sources of bias that should be eliminated.", "labels": [], "entities": []}, {"text": "We pinpoint these sources in the evaluation chain and present a refined authorship corpus as effective countermeasure.", "labels": [], "entities": []}], "introductionContent": [{"text": "When tackling a problem in empirical research, a sound and reliable evaluation of competing solution approaches is a prerequisite to achieve agreement on the state-of-the-art performance.", "labels": [], "entities": []}, {"text": "For authorship verification, the PAN series of shared tasks caters for the most important benchmarks to which new approaches refer and compare against.", "labels": [], "entities": [{"text": "authorship verification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9713714122772217}]}, {"text": "The fundamental problem in authorship verification is to decide whether two given texts were written by the same author.", "labels": [], "entities": [{"text": "authorship verification", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8112335801124573}]}, {"text": "When experimenting within the PAN setting, we learned that one can quickly achieve a competitive performance for this task-with one of the most basic approaches: a TFIDF-weighted character 3-gram model.", "labels": [], "entities": []}, {"text": "By extending this model with a few additional features, such as the KullbackLeibler divergence and related measures, we were able to reach the performance of the best verifiers submitted so far.", "labels": [], "entities": []}, {"text": "However, reality caught up with us when we applied our verifier to other authorship verification problems with little success.", "labels": [], "entities": []}, {"text": "To 1 https://www.tira.io/task/authorship-verification/ get to the bottom of this rather baffling outcome, we carried out a systematic analysis of the entire evaluation chain, its problem definition, its corpora, its evaluation procedure, and of course our model, in search of any sources of bias that may have artificially inflated the performance of our approach.", "labels": [], "entities": []}, {"text": "The paper in hand introduces our \"Basic and Fairly Flawed\" (BAFF) model and reports on our bias analysis.", "labels": [], "entities": [{"text": "Basic and Fairly Flawed\" (BAFF)", "start_pos": 34, "end_pos": 65, "type": "METRIC", "confidence": 0.7163977660238743}]}, {"text": "Moreover, in an attempt to improve the situation and call for better data, we not only contribute anew and carefully curated authorship verification corpus, 2 but also collect a few best practices for the creation of such corpora.", "labels": [], "entities": []}, {"text": "The outlined situation calls into question a lot of what we believed to know about the state of the art, and future PAN tasks on verification will have to rectify these issues in order to provide fora more valid assessment of the state of the art.", "labels": [], "entities": []}], "datasetContent": [{"text": "Lastly, the evaluation procedure itself is biased.", "labels": [], "entities": []}, {"text": "(B6) Test conflation.", "labels": [], "entities": []}, {"text": "At testing time, authorship verifiers can usually access the entire test dataset.", "labels": [], "entities": []}, {"text": "This is unrealistic; a forensic linguist works on a case-by-case basis, and cases are independent of one another, or their underlying population is unknown.", "labels": [], "entities": []}, {"text": "Emulating this scenario, a verifier should process only one test case at a time, without referring to previously processed cases to solve the next one.", "labels": [], "entities": []}, {"text": "Incidentally, this policy would mitigate many of the aforementioned biases.", "labels": [], "entities": []}, {"text": "While not enforcible in individual evaluations and shared tasks with run submissions, at PAN, it may indeed be, by adjusting the TIRA platform (  to handle the software runs accordingly.", "labels": [], "entities": [{"text": "PAN", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.8226715326309204}]}], "tableCaptions": [{"text": " Table 1: Column 1 shows the results of different classifiers on the PAN15 (a) and our Gutenberg corpus (b), an  analysis of BAFF on the PAN15 corpus with different IDF values (c)", "labels": [], "entities": [{"text": "BAFF", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9912742972373962}, {"text": "PAN15 corpus", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.841296911239624}]}]}