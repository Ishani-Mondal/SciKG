{"title": [{"text": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale", "labels": [], "entities": [{"text": "Multi-Hop Reading Comprehension", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.7828013698259989}]}], "abstractContent": [{"text": "We propose anew CogQA framework for multi-hop question answering in web-scale documents.", "labels": [], "entities": [{"text": "multi-hop question answering", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.6365092098712921}]}, {"text": "Founded on the dual process theory in cognitive science, the framework gradually builds a cognitive graph in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2).", "labels": [], "entities": []}, {"text": "While giving accurate answers, our framework further provides explainable reasoning paths.", "labels": [], "entities": []}, {"text": "Specifically , our implementation 1 based on BERT and graph neural network (GNN) efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint F 1 score of 34.9 on the leaderboard, compared to 23.6 of the best competitor.", "labels": [], "entities": [{"text": "BERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9693519473075867}, {"text": "HotpotQA fullwiki dataset", "start_pos": 164, "end_pos": 189, "type": "DATASET", "confidence": 0.9254377285639445}, {"text": "F 1 score", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9889708360036215}]}], "introductionContent": [{"text": "Deep learning models have made significant strides in machine reading comprehension and even outperformed human on single paragraph question answering (QA) benchmarks including SQuAD (.", "labels": [], "entities": [{"text": "single paragraph question answering (QA)", "start_pos": 115, "end_pos": 155, "type": "TASK", "confidence": 0.7792974455016}]}, {"text": "However, to cross the chasm of reading comprehension ability between machine and human, three main challenges lie ahead: 1) Reasoning ability.", "labels": [], "entities": [{"text": "Reasoning", "start_pos": 124, "end_pos": 133, "type": "TASK", "confidence": 0.960077166557312}]}, {"text": "As revealed by adversarial tests, models for single paragraph QA tend to seek answers in sentences matched by the question, which does not involve complex reasoning.", "labels": [], "entities": []}, {"text": "Therefore, multi-hop QA becomes the next frontier to conquer).", "labels": [], "entities": []}, {"text": "Explicit reasoning paths, which enable verification of logical rigor, are vital for the reliability of QA systems.", "labels": [], "entities": []}, {"text": "HotpotQA () requires models to provide supporting sentences, which: An example of cognitive graph for multi-hop QA.", "labels": [], "entities": []}, {"text": "Each hop node corresponds to an entity (e.g., \"Los Angeles\") followed by its introductory paragraph.", "labels": [], "entities": []}, {"text": "The circles mean ans nodes, answer candidates to the question.", "labels": [], "entities": []}, {"text": "Cognitive graph mimics human reasoning process.", "labels": [], "entities": []}, {"text": "Edges are built when calling an entity to \"mind\".", "labels": [], "entities": []}, {"text": "The solid black edges are the correct reasoning path.", "labels": [], "entities": []}, {"text": "means unordered and sentence-level explainability, yet humans can interpret answers with step by step solutions, indicating an ordered and entitylevel explainability.", "labels": [], "entities": []}, {"text": "For any practically useful QA system, scalability is indispensable.", "labels": [], "entities": []}, {"text": "Existing QA systems based on machine comprehension generally follow retrievalextraction framework in DrQA ( , reducing the scope of sources to a few paragraphs by pre-retrieval.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.9508145451545715}]}, {"text": "This framework is a simple compromise between single paragraph QA and scalable information retrieval, compared to human's ability to breeze through reasoning with knowledge in massive-capacity memory (.", "labels": [], "entities": []}, {"text": "Therefore, insights on the solutions to these challenges can be drawn from the cognitive process of humans.", "labels": [], "entities": []}, {"text": "Dual process theory suggests that our brains first retrieve relevant information following attention via an implicit, unconscious and intu-itive process called System 1, based on which another explicit, conscious and controllable reasoning process, System 2, is then conducted.", "labels": [], "entities": []}, {"text": "System 1 could provide resources according to requests, while System 2 enables diving deeper into relational information by performing sequential thinking in the working memory, which is slower but with human-unique rationality.", "labels": [], "entities": []}, {"text": "For complex reasoning, the two systems are coordinated to perform fast and slow thinking) iteratively.", "labels": [], "entities": []}, {"text": "In this paper, we propose a framework, namely Cognitive Graph QA (CogQA), contributing to tackling all challenges above.", "labels": [], "entities": []}, {"text": "Inspired by the dual process theory, the framework comprises functionally different System 1 and 2 modules.", "labels": [], "entities": []}, {"text": "System 1 extracts question-relevant entities and answer candidates from paragraphs and encodes their semantic information.", "labels": [], "entities": []}, {"text": "Extracted entities are organized as a cognitive graph), which resembles the working memory.", "labels": [], "entities": []}, {"text": "System 2 then conducts the reasoning procedure over the graph, and collects clues to guide System 1 to better extract next-hop entities.", "labels": [], "entities": []}, {"text": "The above process is iterated until all possible answers are found, and then the final answer is chosen based on reasoning results from System 2.", "labels": [], "entities": []}, {"text": "An efficient implementation based on BERT () and graph neural network (GNN) () is introduced.", "labels": [], "entities": [{"text": "BERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9852994680404663}]}, {"text": "Our contributions are as follows: \u2022 We propose the novel CogQA framework for multi-hop reading comprehension QA at scale according to human cognition.", "labels": [], "entities": []}, {"text": "\u2022 We show that the cognitive graph structure in our framework offers ordered and entitylevel explainability and suits for relational reasoning.", "labels": [], "entities": [{"text": "relational reasoning", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.8049013912677765}]}, {"text": "\u2022 Our implementation based on BERT and GNN surpasses previous works and other competitors substantially on all the metrics.", "labels": [], "entities": [{"text": "BERT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9849180579185486}]}], "datasetContent": [{"text": "We use the full-wiki setting of HotpotQA to conduct our experiments.", "labels": [], "entities": []}, {"text": "112,779 questions are collected by crowdsourcing based on the first paragraphs in Wikipedia documents, 84% of which require multi-hop reasoning.", "labels": [], "entities": []}, {"text": "The data are split into a training set (90,564 questions), a development set (7,405 questions) and a test set (7,405 questions).", "labels": [], "entities": []}, {"text": "All questions in development and test sets are hard multi-hop cases.", "labels": [], "entities": []}, {"text": "In the training set, for each question, an answer and paragraphs of 2 gold (useful) entities are provided, with multiple supporting facts, sentences containing key information for reasoning, marked out.", "labels": [], "entities": []}, {"text": "There are also 8 unhelpful negative paragraphs for training.", "labels": [], "entities": []}, {"text": "During evaluation, only questions are offered and meanwhile supporting facts are required besides the answer.", "labels": [], "entities": []}, {"text": "To construct cognitive graphs for training, edges in gold-only cognitive graphs are inferred from supporting facts by fuzzy matching based on Levenshtein distance).", "labels": [], "entities": []}, {"text": "For each supporting fact in para, if any gold entity or the answer, denoted as y, is fuzzy matched with a span in the supporting fact, edge (x, y) is added.", "labels": [], "entities": []}, {"text": "We use pre-trained BERT-base model released by) in System 1.", "labels": [], "entities": [{"text": "BERT-base", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9963008165359497}]}, {"text": "The hidden size H is 768, unchanged in node vectors of GNN and predictors.", "labels": [], "entities": []}, {"text": "All the activation functions in our model are gelu.", "labels": [], "entities": []}, {"text": "We train models on Task #1 for 1 epoch and then on Task #1 and #2 jointly for 1 epoch.", "labels": [], "entities": []}, {"text": "Hyperparameters in training are as follows: Model Task batch size learning rate weight decay BERT #1,#2 10 BERT and GNN are optimized by two different Adam optimizers, where \u03b2 1 = 0.9, \u03b2 2 = 0.999.", "labels": [], "entities": [{"text": "weight decay BERT #1", "start_pos": 80, "end_pos": 100, "type": "METRIC", "confidence": 0.7805109620094299}, {"text": "BERT", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.997517466545105}]}, {"text": "The predictors share the same optimizer as GNN.", "labels": [], "entities": [{"text": "GNN", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9285661578178406}]}, {"text": "The learning rate for parameters in BERT warmup over the first 10% steps, and then linearly decays to zero.", "labels": [], "entities": [{"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9241012930870056}]}, {"text": "To select out supporting facts, we just regard the sentences in the clues of any node in graph as supporting facts.", "labels": [], "entities": []}, {"text": "In the initialization of G, these 1-hop spans exist in the question and can also be detected by fuzzy matching with supporting facts in training set.", "labels": [], "entities": []}, {"text": "The extracted 1-hop entities by our framework can improve the retrieval phase of other models (See \u00a7 4.3), which motivated us to separate out the extraction of 1-hop entities to another BERT-base model for the purpose of reuse in implementation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on HotpotQA (fullwiki setting). The test set is not public. The maintainer of HotpotQA only  offers EM and F 1 for every submission. N/A means the model cannot find supporting facts.", "labels": [], "entities": [{"text": "EM", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.996221661567688}, {"text": "F 1", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9336159229278564}, {"text": "N/A", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.708694338798523}]}]}