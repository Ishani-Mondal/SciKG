{"title": [{"text": "Correlating neural and symbolic representations of language", "labels": [], "entities": []}], "abstractContent": [{"text": "Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP.", "labels": [], "entities": []}, {"text": "Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees.", "labels": [], "entities": [{"text": "Representational Similarity Analysis (RSA)", "start_pos": 37, "end_pos": 79, "type": "TASK", "confidence": 0.7786654382944107}]}, {"text": "We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics , and show that they exhibit the expected pattern of results.", "labels": [], "entities": []}, {"text": "We then apply our methods to correlate neural representations of English sentences with their constituency parse trees.", "labels": [], "entities": []}], "introductionContent": [{"text": "Analysis methods which allow us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach to natural language processing.", "labels": [], "entities": []}, {"text": "A popular technique for analyzing neural representations involves predicting information of interest from the activation patterns, typically using a simple predictive model such as a linear classifier or regressor.", "labels": [], "entities": []}, {"text": "If the model is able to predict this information with high accuracy, the inference is that the neural representation encodes it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9902400970458984}]}, {"text": "We refer to these as diagnostic models.", "labels": [], "entities": []}, {"text": "One important limitation of this method of analysis is that it is only easily applicable to relatively simple types of target information, which are amenable to be predicted via linear regression or classification.", "labels": [], "entities": []}, {"text": "Should we wish to decode activation patterns into a structured target such as a syntax tree, we would need to resort to complex structure prediction algorithms, running the risk that the analytic method becomes no simpler than the actual neural model.", "labels": [], "entities": []}, {"text": "Here we introduce an alternative approach based on correlating neural representations of sentences and structured symbolic representations commonly used in linguistics.", "labels": [], "entities": []}, {"text": "Crucially, the correlation is in similarity space rather than in the original representation space, removing most constraints on the types of representations we can use.", "labels": [], "entities": []}, {"text": "Our approach is an extension of the Representational Similarity Analysis (RSA) method, initially introduced by in the context of understanding neural activation patterns inhuman brains.", "labels": [], "entities": [{"text": "Representational Similarity Analysis (RSA)", "start_pos": 36, "end_pos": 78, "type": "TASK", "confidence": 0.7736387799183527}]}, {"text": "In this work we propose to apply RSA to neural representations of strings from a language on one side, and to structured symbolic representations of these strings on the other side.", "labels": [], "entities": [{"text": "RSA", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9762545824050903}]}, {"text": "To capture the similarities between these symbolic representations, we use a tree kernel, a metric to compute the proportion of common substructures between trees.", "labels": [], "entities": []}, {"text": "This approach enables straightforward comparison of neural and symbolic-linguistic representations.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce RSA REGRESS , a similarity-based analytic method which combines features of RSA and of diagnostic models.", "labels": [], "entities": [{"text": "REGRESS", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.5993286967277527}]}, {"text": "We validate both techniques on neural models which process a synthetic language for arithmetic expressions with a simple syntax and semantics and show that they behave as expected in this controlled setting.", "labels": [], "entities": []}, {"text": "We further apply our techniques to two neural models trained on English text, Infersent () and BERT, and show that both models encode a substantial amount of syntactic information compared to random models and simple bag-of-words representations; we also show that according to our metrics syntax is most salient in the intermediate layers of BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9930104613304138}]}], "datasetContent": [{"text": "Data We use a sample of data from the English Web Treebank (EWT) () which contains a mix of English weblogs, newsgroups, email, reviews and question-answers manually annotated for syntactic constituency structure.", "labels": [], "entities": [{"text": "English Web Treebank (EWT)", "start_pos": 38, "end_pos": 64, "type": "DATASET", "confidence": 0.8986801505088806}]}, {"text": "We use the 2,002 sentences corresponding to the development section of the EWT Universal Dependencies (), plus 200 sentences from the training section as reference sentences when fitting RSA REGRESS . Tree Kernel Prior to computing the Tree Kernel scores we delexicalize the constituency trees by replacing all terminals (i.e. words) with a single placeholder value X.", "labels": [], "entities": [{"text": "EWT Universal Dependencies", "start_pos": 75, "end_pos": 101, "type": "DATASET", "confidence": 0.9492831428845724}]}, {"text": "This ensures that only syntactic structure, and not lexical overlap, contributes to kernel scores.", "labels": [], "entities": []}, {"text": "We compute kernels for the values of \u03bb \u2208 {1, 1 2 }.", "labels": [], "entities": []}, {"text": "Embeddings For the BERT embeddings we use the vector associated with the first token (CLS) fora given layer.", "labels": [], "entities": [{"text": "BERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.6270788908004761}]}, {"text": "For Infersent, we use the default max-pooled representation.", "labels": [], "entities": []}, {"text": "Fitting When fitting RSA REGRESS we use L2-penalized multivariate linear regression.", "labels": [], "entities": [{"text": "RSA REGRESS", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.48364292085170746}]}, {"text": "We report the results for the value of the penalty = 10 n , for n \u2208 {\u22123, \u22122, \u22121, 0, 1, 2}, with the highest 10-fold cross-validated Pearson's r between target and predicted similarity-embedded vectors.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.8917582432428995}]}, {"text": "shows the results of applying RSA and RSA REGRESS on five different sentence encoders, using the Tree Kernel reference.", "labels": [], "entities": []}, {"text": "Results are reported using two different values for the Tree Kernel parameter \u03bb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Scores for diagnostic regression, RSA, and RSA REGRESS with respect to expression value, expression tree  depth and the Tree Kernel (TK) with \u03bb = 1 and \u03bb = 0.5. All scores are Pearson's correlation coefficients. For the  diagnostic model and RSA REGRESS they are cross-validated correlations between target and predicted values. The  randomly initialized encoder is the same for all encoder types, and thus there is only a single row for the RANDOM  encoder. The loss column shows the loss of the full model on the test data: mean squared error for SEMANTIC  EVALUATION and TREE DEPTH, and cross-entropy for INFIX-TO-PREFIX.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 186, "end_pos": 207, "type": "METRIC", "confidence": 0.7222093542416891}, {"text": "RANDOM  encoder", "start_pos": 452, "end_pos": 467, "type": "DATASET", "confidence": 0.870863676071167}, {"text": "mean squared error", "start_pos": 536, "end_pos": 554, "type": "METRIC", "confidence": 0.8982756535212199}, {"text": "TREE", "start_pos": 584, "end_pos": 588, "type": "METRIC", "confidence": 0.9725464582443237}, {"text": "DEPTH", "start_pos": 589, "end_pos": 594, "type": "METRIC", "confidence": 0.5686575174331665}]}, {"text": " Table 3: Correlation scores for encoders against Tree  Kernel with varying \u03bb. Scores for both RSA and  RSA REGRESS are Pearson's r. The column Train indi- cates whether the encoder (including the word embed- dings) is randomly initialized (\u2212), or trained (+). For  BERT, we report scores for the topmost (last) layer and  for the layer which maximizes the given score (best).", "labels": [], "entities": [{"text": "REGRESS", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.7578624486923218}, {"text": "BERT", "start_pos": 266, "end_pos": 270, "type": "METRIC", "confidence": 0.8897765874862671}]}]}