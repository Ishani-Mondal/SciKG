{"title": [{"text": "Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text Generation", "labels": [], "entities": [{"text": "Low Resource Table-to-Text Generation", "start_pos": 41, "end_pos": 78, "type": "TASK", "confidence": 0.59683558344841}]}], "abstractContent": [{"text": "Table-to-text generation aims to translate the structured data into the unstructured text.", "labels": [], "entities": [{"text": "Table-to-text generation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.799895167350769}]}, {"text": "Most existing methods adopt the encoder-decoder framework to learn the transformation, which requires large-scale training samples.", "labels": [], "entities": []}, {"text": "However , the lack of large parallel data is a major practical problem for many domains.", "labels": [], "entities": []}, {"text": "In this work, we consider the scenario of low resource table-to-text generation, where only limited parallel data is available.", "labels": [], "entities": [{"text": "table-to-text generation", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.7504501044750214}]}, {"text": "We propose a novel model to separate the generation into two stages: key fact prediction and surface realization.", "labels": [], "entities": [{"text": "key fact prediction", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.6153495907783508}, {"text": "surface realization", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7453190088272095}]}, {"text": "It first predicts the key facts from the tables, and then generates the text with the key facts.", "labels": [], "entities": []}, {"text": "The training of key fact prediction needs much fewer annotated data, while surface realization can be trained with pseudo parallel corpus.", "labels": [], "entities": [{"text": "key fact prediction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.6377686758836111}, {"text": "surface realization", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7709124982357025}]}, {"text": "We evaluate our model on a biography generation dataset.", "labels": [], "entities": [{"text": "biography generation dataset", "start_pos": 27, "end_pos": 55, "type": "DATASET", "confidence": 0.7242918014526367}]}, {"text": "Our model can achieve 27.34 BLEU score with only 1, 000 parallel data, while the baseline model only obtain the performance of 9.71 BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9788940846920013}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9961121678352356}]}], "introductionContent": [{"text": "shows an example of table-to-text generation.", "labels": [], "entities": [{"text": "table-to-text generation", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.788327544927597}]}, {"text": "The table provides some structured information about a person named \"Denise Margaret Scott\", and the corresponding text describes the person with the key information in the table.to-text generation can be applied in many scenarios, including weather report generation (), NBA news writing (), biography generation, and soon.", "labels": [], "entities": [{"text": "weather report generation", "start_pos": 242, "end_pos": 267, "type": "TASK", "confidence": 0.6262672444184622}, {"text": "NBA news writing", "start_pos": 272, "end_pos": 288, "type": "TASK", "confidence": 0.6995747486750284}, {"text": "biography generation", "start_pos": 293, "end_pos": 313, "type": "TASK", "confidence": 0.7588919997215271}]}, {"text": "Moreover, table-to-text genera-tion is a good testbed of a model's ability of understanding the structured knowledge.", "labels": [], "entities": []}, {"text": "Most of the existing methods for table-totext generation are based on the encoder-decoder framework).", "labels": [], "entities": [{"text": "table-totext generation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7765699625015259}]}, {"text": "They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation.", "labels": [], "entities": []}, {"text": "Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (, it requires a large parallel corpus, and is known to fail when the corpus is not big enough.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.8299373288949331}]}, {"text": "shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework.", "labels": [], "entities": []}, {"text": "We can see that the performance is poor when the parallel data size is low.", "labels": [], "entities": []}, {"text": "In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus.", "labels": [], "entities": []}, {"text": "This work focuses on the task of low resource table-to-text generation, where only limited paral- lel data is available.", "labels": [], "entities": [{"text": "low resource table-to-text generation", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.6507453173398972}]}, {"text": "Some previous work) formulates the task as the combination of content selection and surface realization, and models them with an end-to-end model.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7878769338130951}]}, {"text": "Inspired by these work, we breakup the table-to-text generation into two stages, each of which is performed by a model trainable with only a few annotated data.", "labels": [], "entities": [{"text": "table-to-text generation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7879401743412018}]}, {"text": "Specifically, it first predicts the key facts from the tables, and then generates the text with the key facts, as shown in.", "labels": [], "entities": []}, {"text": "The two-stage method consists of two separate models: a key fact prediction model and a surface realization model.", "labels": [], "entities": []}, {"text": "The key fact prediction model is formulated as a sequence labeling problem, so it needs much fewer annotated data than the encoder-decoder models.", "labels": [], "entities": [{"text": "key fact prediction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6544095377127329}]}, {"text": "According to our experiments, the model can obtain 87.92% F1 score with only 1, 000 annotated data.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9660015106201172}]}, {"text": "As for the surface realization model, we propose a method to construct a pseudo parallel dataset without the need of labeled data.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7583456635475159}]}, {"text": "In this way, our model can make full use of the unlabeled text, and alleviate the heavy need of the parallel data.", "labels": [], "entities": []}, {"text": "The contributions of this work are as follows: \u2022 We propose to breakup the table-to-text generation into two stages with two separate models, so that the model can be trained with fewer annotated data.", "labels": [], "entities": [{"text": "table-to-text generation", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.7636938095092773}]}, {"text": "\u2022 We propose a method to construct a pseudo parallel dataset for the surface realization model, without the need of labeled data.", "labels": [], "entities": []}, {"text": "\u2022 Experiments show that our proposed model can achieve 27.34 BLEU score on a biography generation dataset with only 1, 000 tabletext samples.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9723834097385406}]}], "datasetContent": [{"text": "We evaluate our model on a table-to-text generation benchmark.", "labels": [], "entities": []}, {"text": "We denote the PIVOT model under the vanilla Seq2Seq framework as PIVOTVanilla, and that under the Transformer framework as PIVOT-Trans.", "labels": [], "entities": [{"text": "PIVOTVanilla", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.8409102559089661}]}, {"text": "We use WIKIBIO dataset ( as our benchmark dataset.", "labels": [], "entities": [{"text": "WIKIBIO dataset", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9717755615711212}]}, {"text": "The dataset contains 728, 321 articles from English Wikipedia, which uses the first sentence of each article as the description of the related infobox.", "labels": [], "entities": []}, {"text": "There are an average of 26.1 words in each description, of which 9.5 words also appear in the table.", "labels": [], "entities": []}, {"text": "The table contains 53.1 words and 19.7 attributes on average.", "labels": [], "entities": []}, {"text": "Following the previous work (, we split the dataset into 80% training set, 10% testing set, and 10% validation set.", "labels": [], "entities": []}, {"text": "In order to simulate the low resource scenario, we randomly sample 1, 000 parallel sample, and remove the tables from the rest of the training data.", "labels": [], "entities": []}, {"text": "Following the previous work (, we use BLEU-4 (), ROUGE-4 (F measure) (, and NIST-4 () as the evaluation metrics.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9987905621528625}, {"text": "ROUGE-4 (F measure)", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.8453149914741516}, {"text": "NIST-4", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.7973439693450928}]}], "tableCaptions": [{"text": " Table 1: Results of our model and the baselines. Above  is the performance of the key fact prediction compo- nent (F1: F1 score, P: precision, R: recall). Middle  is the comparison between models under the Vanilla  Seq2Seq framework. Below is the models imple- mented with the transformer framework.", "labels": [], "entities": [{"text": "F1: F1 score", "start_pos": 116, "end_pos": 128, "type": "METRIC", "confidence": 0.8589458763599396}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.6179429888725281}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.5981031656265259}, {"text": "Vanilla  Seq2Seq framework", "start_pos": 207, "end_pos": 233, "type": "DATASET", "confidence": 0.8444798191388448}]}, {"text": " Table 2: Ablation study on the 1k training set for the  effect of pseudo parallel data.", "labels": [], "entities": []}, {"text": " Table 3: Ablation study on the 1k training set for the  effect of the denoising data augmentation.", "labels": [], "entities": []}, {"text": " Table 4. Under the low resource setting, the Trans- former can not produce a fluent sentence, and  also fails to select the proper fact from the ta- ble. Thanks to the unlabeled data, the SemiMT  model can generate a fluent, human-like descrip- tion. However, it suffers from the hallucination  problem so that it generates some unseen facts,  which is not faithful to the source input. Although", "labels": [], "entities": []}]}