{"title": [{"text": "Saama Research at MEDIQA 2019: Pre-trained BioBERT with Attention Visualisation for Medical Natural Language Inference", "labels": [], "entities": [{"text": "MEDIQA 2019", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.8905691802501678}, {"text": "Medical Natural Language Inference", "start_pos": 84, "end_pos": 118, "type": "TASK", "confidence": 0.5648682191967964}]}], "abstractContent": [{"text": "Natural Language inference is the task of identifying relation between two sentences as en-tailment, contradiction or neutrality.", "labels": [], "entities": [{"text": "Natural Language inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.592728724082311}]}, {"text": "MedNLI is a biomedical flavour of NLI for clinical domain.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8593632578849792}]}, {"text": "This paper explores the use of Bidirectional Encoder Representation from Transformer (BERT) for solving MedNLI.", "labels": [], "entities": [{"text": "BERT", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.4679088592529297}, {"text": "solving MedNLI", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.6285965889692307}]}, {"text": "The proposed model, BERT pre-trained on PMC, PubMed and fine-tuned on MIMIC-III v1.4, achieves state of the art results on MedNLI (83.45%) and an accuracy of 78.5% in MEDIQA challenge.", "labels": [], "entities": [{"text": "BERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9974794983863831}, {"text": "PMC", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8805045485496521}, {"text": "PubMed", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.9229384660720825}, {"text": "MedNLI", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.7644962072372437}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9994076490402222}, {"text": "MEDIQA challenge", "start_pos": 167, "end_pos": 183, "type": "DATASET", "confidence": 0.7477059960365295}]}, {"text": "The authors present an analysis of the attention patterns that emerged as a result of training BERT on MedNLI using a visualization tool, bertviz.", "labels": [], "entities": [{"text": "BERT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9657936096191406}]}], "introductionContent": [{"text": "Natural Language Inference (NLI) is a fundamental task in Natural Language Processing in which the objective is to determine if the hypothesis is true (entailment), false (contradiction) or undetermined (neutral), given a premise.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7853848338127136}]}, {"text": "Entailment, Contradiction and Neutral (semantic independence) are semantic concepts that represent the relationship between sentences.", "labels": [], "entities": []}, {"text": "The ability to infer these relations between sentences or pieces of text, is crucial in tasks like Information Retrieval, Semantic Parsing, Commonsense Reasoning, etc.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.8303274214267731}, {"text": "Semantic Parsing", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.8495270013809204}, {"text": "Commonsense Reasoning", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.759295791387558}]}, {"text": "NLI, like most NLP tasks, is challenging due to the ambiguous nature of natural language.", "labels": [], "entities": []}, {"text": "A particular meaning can be expressed in multiple linguistic forms.", "labels": [], "entities": []}, {"text": "This calls for methods that can capture meaningful semantic concepts from text.", "labels": [], "entities": []}, {"text": "Stanford Natural Language Inference (SNLI) corpus) is a collection of * *Equal Contribution: Kamal had sole access to MIMIC and MEDIQA data, focussed on the algorithm development and implementation.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 0, "end_pos": 49, "type": "DATASET", "confidence": 0.762765534222126}, {"text": "MEDIQA data", "start_pos": 128, "end_pos": 139, "type": "DATASET", "confidence": 0.7214452773332596}]}, {"text": "Suriyadeepan and Archana focussed on the attention visualisation and writing.", "labels": [], "entities": []}, {"text": "Soham and Malaikannan focussed on reviewing sentence pairs labeled for entailment, contradiction, and semantic independence.", "labels": [], "entities": []}, {"text": "It contains approximately 550,000 labeled hypothesis/premise pairs.", "labels": [], "entities": []}, {"text": "Multi-Genre Natural Language Inference (Multi-NLI) corpus () contains 433,000 samples, covering a wide range of (10) genres of written and spoken English.", "labels": [], "entities": []}, {"text": "Multi-NLI, in its complexity, is closer to Natural Language than SNLI.", "labels": [], "entities": []}, {"text": "MedNLI (Romanov and Shivade) is a dataset for natural language inference in clinical domain, analogous to SNLI.", "labels": [], "entities": [{"text": "MedNLI (Romanov and Shivade)", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8785600960254669}, {"text": "SNLI", "start_pos": 106, "end_pos": 110, "type": "TASK", "confidence": 0.8021307587623596}]}, {"text": "Romanov et al in (Romanov and Shivade), used InferSent (, a bidirectional LSTM based model for achieving an accuracy of 73.5% in MedNLI.", "labels": [], "entities": [{"text": "InferSent", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9316658973693848}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9989413619041443}, {"text": "MedNLI", "start_pos": 129, "end_pos": 135, "type": "DATASET", "confidence": 0.8307758569717407}]}, {"text": "In (Jin et al., 2019), Jin et al make use of BioBERT (), a biomedical version of BERT along with pre-trained LMs(Language Models) as feature extractors, to achieve an accuracy of 81.7% on MedNLI.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9462267756462097}, {"text": "BERT", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9415817260742188}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.99909508228302}, {"text": "MedNLI", "start_pos": 188, "end_pos": 194, "type": "DATASET", "confidence": 0.9680764675140381}]}, {"text": "This work uses BERT pre-trained on PMC and PubMed corpus, and fine-tuned on MIMIC-III v1.4 data (BioBERT) to solve MedNLI.", "labels": [], "entities": [{"text": "BERT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.993453860282898}, {"text": "PubMed corpus", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9160212576389313}, {"text": "MIMIC-III v1.4 data (BioBERT", "start_pos": 76, "end_pos": 104, "type": "DATASET", "confidence": 0.8048956513404846}, {"text": "MedNLI", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.8483801484107971}]}, {"text": "This approach achieves new state of the art results when evaluated on MedNLI test set (83.4%).", "labels": [], "entities": [{"text": "MedNLI test set", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.9778729279836019}]}, {"text": "Evaluation on MEDIQA) test set results in an accuracy of 78.5 %.", "labels": [], "entities": [{"text": "MEDIQA) test set", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.9523292928934097}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9997465014457703}]}], "datasetContent": [{"text": "The MedNLI dataset used over 4 clinicians working on a total of 4,683 premises over a period of 6 weeks with 14,049 unique sentence pairs.", "labels": [], "entities": [{"text": "MedNLI dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9711637794971466}]}, {"text": "The dataset was then split into training, development, and test sets.", "labels": [], "entities": []}, {"text": "The class distribution is even across all classes, throughout training, development and test sets (  All the experiments in this paper are done with BERT pre-trained on unlabelled biomedical dataBioBERT (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9762312769889832}]}, {"text": "Fine tuning on BioBERT was done using TensorFlow with three GeForce GTX 1080Ti GPUs for 2 weeks.", "labels": [], "entities": []}, {"text": "The model on MIMIC III v1.4 is trained with maximum sequence length 128 with batch size 32 and learning rate 2e-5 for 200,000 steps.", "labels": [], "entities": [{"text": "MIMIC III v1.4", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.8071960608164469}]}, {"text": "The sequence length is limited such that it can fit into GPU memory.", "labels": [], "entities": []}, {"text": "The pretraining data from MIMIC III v1.4 is prepared using scripts from the original BERT github repository) with the default parameters.", "labels": [], "entities": [{"text": "MIMIC III v1.4", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.6629784901936849}, {"text": "BERT github repository", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.7435739239056905}]}, {"text": "Further fine tuning on MedNLI task is done with one GeForce GTX 1080Ti GPU with 11 GB of RAM.", "labels": [], "entities": []}, {"text": "One epoch on MedNLI takes around 3 minutes on a single GPU 1 .", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9630761742591858}]}], "tableCaptions": [{"text": " Table 1: Examples from Development set of MedNLI", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.45884665846824646}]}, {"text": " Table 4: Comparision of Results", "labels": [], "entities": []}]}