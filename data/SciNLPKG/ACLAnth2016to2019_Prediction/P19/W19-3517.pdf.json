{"title": [{"text": "Detecting Aggression and Toxicity using a Multi Dimension Capsule Network", "labels": [], "entities": [{"text": "Detecting Aggression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.901218980550766}]}], "abstractContent": [{"text": "In the era of social media, hate speech, trolling and verbal abuse have become a common issue.", "labels": [], "entities": []}, {"text": "We present an approach to automatically classify such statements, using anew deep learning architecture.", "labels": [], "entities": []}, {"text": "Our model comprises of a Multi Dimension Capsule Network that generates the representation of sentences which we use for classification.", "labels": [], "entities": []}, {"text": "We further provide an analysis of our model's interpretation of such statements.", "labels": [], "entities": []}, {"text": "We compare the results of our model with state-of-art classification algorithms and demonstrate our model's ability.", "labels": [], "entities": []}, {"text": "It also has the capability to handle comments that are written in both Hindi and English, which are provided in the TRAC dataset.", "labels": [], "entities": [{"text": "TRAC dataset", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.9443272650241852}]}, {"text": "We also compare results on Kaggle's Toxic comment classification dataset.", "labels": [], "entities": [{"text": "Kaggle's Toxic comment classification dataset", "start_pos": 27, "end_pos": 72, "type": "DATASET", "confidence": 0.8735268910725912}]}], "introductionContent": [{"text": "Many people refrain from expressing themselves or giving opinions online for the fear of harassment and abuse.", "labels": [], "entities": []}, {"text": "Twitter admitted that such behavior is resulting in users quitting from their platform and sometimes they are even forced to change their location.", "labels": [], "entities": []}, {"text": "Due to this, combating hate speech and abusive behavior has become a high priority area for major companies like Facebook, Twitter, Youtube, and Microsoft.", "labels": [], "entities": []}, {"text": "With an ever-increasing content on such platforms, it makes impossible to manually detect toxic comments or hate speech.", "labels": [], "entities": []}, {"text": "Earlier works in Capsule network based deep learning architecture to classify toxic comments have proved that these networks work well as compared to other deep learning architectures.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the performance of a multi-dimension Capsule network as opposed to using a fixed dimension Capsule network for capturing a sentence representation and we shall discuss how well it captures features necessary for classification of such sentences.", "labels": [], "entities": []}, {"text": "For our experiments we have taken up two different datasets, namely, TRAC-1, which has comments in Hindi and English both scraped from Facebook and Twitter and, Kaggle's Toxic Comment Classification Challenge which is a multilabel classification task.", "labels": [], "entities": [{"text": "TRAC-1", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8161628246307373}, {"text": "Kaggle's Toxic Comment Classification Challenge", "start_pos": 161, "end_pos": 208, "type": "TASK", "confidence": 0.6979171931743622}, {"text": "multilabel classification task", "start_pos": 220, "end_pos": 250, "type": "TASK", "confidence": 0.7246035734812418}]}, {"text": "In our experiments, we discovered that our model is capable of handling transliterated comments, which is another major challenge in this task.", "labels": [], "entities": []}, {"text": "Since one of the datasets we used, TRAC-1, was crawled from public Facebook Pages and Twitter, mainly on Indian topics, hence there is a presence of code-mixed text.", "labels": [], "entities": [{"text": "TRAC-1", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.6379886865615845}]}, {"text": "This type of data is more observed in a real-world scenario.", "labels": [], "entities": []}], "datasetContent": [{"text": "It is a dataset for Aggression identification 2 , and contains 15,000 comments in both Hindi and English.", "labels": [], "entities": [{"text": "Aggression identification 2", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.9299314022064209}]}, {"text": "The task is to classify the comments into the following categories, Overtly Aggressive (OAG), Covertly Aggressive (CAG), and Non-aggressive (NAG).", "labels": [], "entities": [{"text": "Covertly Aggressive (CAG)", "start_pos": 94, "end_pos": 119, "type": "METRIC", "confidence": 0.8807687997817993}]}, {"text": "We used the train, dev and test data as provided by the organizers of the task.", "labels": [], "entities": []}, {"text": "As a preprocessing step, we performed casefolding of all the words and removal of punctuations.", "labels": [], "entities": []}, {"text": "The code for tokenization was taken from) which seems to properly separate the word tokens and special characters.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9747936129570007}]}, {"text": "For training all our classification models, we have used fastText embeddings of dimension 300 trained on a common crawl.", "labels": [], "entities": []}, {"text": "For out of vocabulary (OOV) words we initialized the embeddings randomly.", "labels": [], "entities": []}, {"text": "For feature extraction, we used 200 LSTM units, each for capturing forward and backward contexts (total of 400).", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8377254903316498}]}, {"text": "We used 20 capsules of dimension 15 and another 20 of dimension 20 for all the experiments.", "labels": [], "entities": []}, {"text": "We kept the number of routings to be 3 as more routings could introduce overfitting.", "labels": [], "entities": []}, {"text": "To further avoid overfitting, we adjusted the dropout values to 0.4.", "labels": [], "entities": []}, {"text": "We used cross-entropy as the loss function and Adam as an optimizer (with default values) for all the models.", "labels": [], "entities": []}, {"text": "We obtained all these hyperparameters values by tuning several models on the validation set and then finally selecting the model with minimum validation loss.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results Of various architectures on publicly available datasets", "labels": [], "entities": []}]}