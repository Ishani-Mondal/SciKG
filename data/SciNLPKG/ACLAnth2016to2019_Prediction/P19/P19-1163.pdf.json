{"title": [{"text": "The Risk of Racial Bias in Hate Speech Detection", "labels": [], "entities": [{"text": "Racial Bias", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.735956996679306}, {"text": "Hate Speech Detection", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7105581561724345}]}], "abstractContent": [{"text": "We investigate how annotators' insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models , potentially amplifying harm against minority populations.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.6180260678132375}]}, {"text": "We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets.", "labels": [], "entities": []}, {"text": "Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others.", "labels": [], "entities": [{"text": "AAE tweets and tweets by self-identified African Americans", "start_pos": 97, "end_pos": 155, "type": "TASK", "confidence": 0.7973730936646461}]}, {"text": "Finally, we propose dialect and race priming as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet's dialect they are significantly less likely to label the tweet as offensive.", "labels": [], "entities": []}], "introductionContent": [{"text": "Toxic language (e.g., hate speech, abusive speech, or other offensive speech) primarily targets members of minority groups and can catalyze reallife violence towards them.", "labels": [], "entities": []}, {"text": "Social media platforms are under increasing pressure to respond, but automated removal of such content risks further suppressing alreadymarginalized voices).", "labels": [], "entities": []}, {"text": "Thus, great care is needed when developing automatic toxic language identification tools.", "labels": [], "entities": [{"text": "toxic language identification", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.7479816277821859}]}, {"text": "The task is especially challenging because what is considered toxic inherently depends on social context (e.g., speaker's identity or dialect).", "labels": [], "entities": []}, {"text": "Indeed, terms previously used to disparage communities (e.g., \"n*gga\", \"queer\") have been reclaimed by those communities while remaining offensive when used by outsiders.", "labels": [], "entities": []}, {"text": "Non-toxic tweets Figure 1: Phrases in African American English (AAE), their non-AAE equivalents, and toxicity scores from PerspectiveAPI.com.", "labels": [], "entities": [{"text": "Phrases in African American English (AAE)", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.5751697011291981}]}, {"text": "Perspective is a tool from Jigsaw/Alphabet that uses a convolutional neural network to detect toxic language, trained on crowdsourced data where annotators were asked to label the toxicity of text without metadata.", "labels": [], "entities": [{"text": "Jigsaw/Alphabet", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.8488784432411194}]}, {"text": "more toxic than general American English equivalents, despite their being understood as non-toxic by AAE speakers.", "labels": [], "entities": []}, {"text": "In this work, we first empirically characterize the racial bias present in several widely used Twitter corpora annotated for toxic content, and quantify the propagation of this bias through models trained on them ( \u00a73).", "labels": [], "entities": []}, {"text": "We establish strong associations between AAE markers (e.g., \"n*ggas\", \"ass\") and toxicity annotations, and show that models acquire and replicate this bias: in other corpora, tweets inferred to be in AAE and tweets from self-identifying African American users are more likely to be classified as offensive.", "labels": [], "entities": [{"text": "AAE", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.7993203401565552}]}, {"text": "Second, through an annotation study, we introduce away of mitigating annotator bias through dialect and race priming.", "labels": [], "entities": []}, {"text": "Specifically, by designing tasks that explicitly highlight the inferred dialect of a tweet or likely racial background of its author, we show that annotators are significantly less likely to label an AAE tweet as offensive than when not shown this information ( \u00a74).", "labels": [], "entities": []}, {"text": "Our findings show that existing approaches to toxic language detection have racial biases, and that text alone does not determine offensiveness.", "labels": [], "entities": [{"text": "toxic language detection", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6460650165875753}]}, {"text": "Therefore, we encourage paying greater attention to the confounding effects of dialect and a speaker's social identity (e.g., race) so as to avoid unintended negative impacts.", "labels": [], "entities": []}], "datasetContent": [{"text": "To understand the racial and dialectic bias in toxic language detection, we focus our analyses on two corpora of tweets () that are widely used in hate speech detection; Lee 1 Of course, many African Americans might not use AAE in every context, or at all.", "labels": [], "entities": [{"text": "toxic language detection", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.6453009148438772}, {"text": "hate speech detection", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.737326999505361}]}, {"text": "For further discussion of AAE, please refer to.", "labels": [], "entities": [{"text": "AAE", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.5793799161911011}]}, {"text": "The model yields AAE, Hispanic, Asian/Other and White-aligned dialect probabilities, but for the purpose of our study we only focus on AAE and White-aligned dialects.", "labels": [], "entities": [{"text": "AAE", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9229320883750916}, {"text": "AAE", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.6601434350013733}]}, {"text": "tweets annotated with four labels: hateful, abusive, spam or none.", "labels": [], "entities": []}, {"text": "Authors used a bootstrapping approach to sampling tweets, which were then labelled by five crowdsource workers.", "labels": [], "entities": []}, {"text": "For each dataset, we randomly split the data into train/dev./test sets (73/12/15%), and perform early stopping when classification accuracy on dev.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9460852742195129}]}, {"text": "For DWMW17, which has multicategory count AAE corr.", "labels": [], "entities": [{"text": "DWMW17", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9368917346000671}, {"text": "AAE corr", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9487725794315338}]}, {"text": "ple annotations per instance, we use the majority class as the label, dropping instances that are tied.", "labels": [], "entities": []}, {"text": "For both datasets, we preprocess the text using an adapted version of the script for Twitter GloVe vectors.", "labels": [], "entities": [{"text": "Twitter GloVe vectors", "start_pos": 85, "end_pos": 106, "type": "DATASET", "confidence": 0.8405758937199911}]}, {"text": "In our experiments, we set H = 64, and use a vocabulary size of |V | = 19k and |V | = 74k for DWMW17 and FDCL18, respectively, and initialize the embedding layer with 300-dimensional GloVe vectors trained on 840 billion tokens.", "labels": [], "entities": [{"text": "DWMW17", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.9173045754432678}, {"text": "FDCL18", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.6349163055419922}]}, {"text": "We experimented with using ELMo embeddings, but found that they did not boost performance for this task.", "labels": [], "entities": []}, {"text": "We optimize these models using Adam with a learning rate of 0.001, and a batch size of 64.", "labels": [], "entities": []}, {"text": "We collected annotations from 110 (76% White), 143 (77% White), and 81 (72% White) workers in the control, dialect, and race priming conditions, respectively.", "labels": [], "entities": []}, {"text": "shows the instruction snippet related to dialect and race shown to workers in the two treatment conditions.", "labels": [], "entities": []}, {"text": "Additionally, shows the annotation interface, with (a) and without priming (b,c).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of tweets in each category, and cor- relation with AAE (Pearson r, p 0.001). We assign  tweets to categories based on the label for FDCL18, and  majority class for DWMW17. Correlations are colored  for interpretability.", "labels": [], "entities": [{"text": "AAE", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9917300343513489}, {"text": "Pearson r", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9055004417896271}, {"text": "FDCL18", "start_pos": 149, "end_pos": 155, "type": "DATASET", "confidence": 0.8031483292579651}, {"text": "DWMW17", "start_pos": 181, "end_pos": 187, "type": "DATASET", "confidence": 0.9785497784614563}]}, {"text": " Table 2: Data statistics in WH16, as well as the Pearson  r correlations with the labels and inferred AAE dialect.  All correlations are p 0.001.", "labels": [], "entities": [{"text": "WH16", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9667204022407532}, {"text": "Pearson  r correlations", "start_pos": 50, "end_pos": 73, "type": "METRIC", "confidence": 0.9268484910329183}, {"text": "AAE", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.8362162709236145}]}]}