{"title": [{"text": "AdaNSP: Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing", "labels": [], "entities": [{"text": "Neural Semantic Parsing", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.684118906656901}]}], "abstractContent": [{"text": "Neural semantic parsers utilize the encoder-decoder framework to learn an end-to-end model for semantic parsing that transduces a natural language sentence to the formal semantic representation.", "labels": [], "entities": [{"text": "Neural semantic parsers", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.777324378490448}, {"text": "semantic parsing", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7685465812683105}]}, {"text": "To keep the model aware of the underlying grammar in target sequences , many constrained decoders were devised in a multi-stage paradigm, which decode to the sketches or abstract syntax trees first, and then decode to target semantic tokens.", "labels": [], "entities": []}, {"text": "We instead to propose an adaptive decoding method to avoid such intermediate representations.", "labels": [], "entities": []}, {"text": "The decoder is guided by model uncertainty and automatically uses deeper computations when necessary.", "labels": [], "entities": []}, {"text": "Thus it can predict tokens adaptively.", "labels": [], "entities": []}, {"text": "Our model outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or sketches in the meantime.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Parsing (SP) maps a natural language utterance into a formal language, which is crucial in abundant tasks, such as question answering and code generation.", "labels": [], "entities": [{"text": "Semantic Parsing (SP) maps a natural language utterance", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8675246894359588}, {"text": "question answering", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.8939723670482635}, {"text": "code generation", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.7369990646839142}]}, {"text": "The prevailing neural semantic parsers view semantic parsing as a sequence transduction task, and adopt the encoder-decoder framework similar to machine translation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7329917699098587}, {"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7278916388750076}]}, {"text": "The distinguishing difference of semantic parsing, however, is in its target sequences, which are token sequences of well-formed semantic representations.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7392634153366089}]}, {"text": "SQL language and lambda expressions are typical examples of SP targets.", "labels": [], "entities": []}, {"text": "The \"SELECT..FROM..WHERE\" pattern in SQL and the paired parentheses in lambda expressions are consequences of underlying grammars.", "labels": [], "entities": [{"text": "SELECT.", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.8943099975585938}, {"text": "FROM.", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.899325430393219}]}, {"text": "However, standard Seq2Seq models ignore the patterns and may give ill-formed results.", "labels": [], "entities": []}, {"text": "To better model the grammatical and semantical constraints, many decoding methods were devised.", "labels": [], "entities": []}, {"text": "proposed to generate tokens of an intermediate sketch first, followed by decoding into final formal targets.", "labels": [], "entities": []}, {"text": "Others chose to gradually build abstract syntax trees using a transition-based paradigm, and tokens are generated at the tree leaves or in the middle of the transitions (.", "labels": [], "entities": []}, {"text": "There are also some decoders comprised of several submodules which are intended to generate different parts of the semantic output, respectively (.", "labels": [], "entities": []}, {"text": "However, the aforementioned methods still have the following key issue.", "labels": [], "entities": []}, {"text": "They explicitly require the expertise to design intermediate representations or model structures, which is not ideal or acceptable for scenarios with Domain Specific Languages (DSL) or new representations because of domain alterations and the incompleteness of the expert knowledge.", "labels": [], "entities": []}, {"text": "To follow the successful idea and overcome the above issue, we introduce a novel adaptive decoding mechanism.", "labels": [], "entities": []}, {"text": "Inspired by adaptive computing, pervasive tokens in training data will be generated immediately with no doubt.", "labels": [], "entities": []}, {"text": "But for tokens seen less often, the model maybe pondering and less confident, and it will be better to carryout more computations.", "labels": [], "entities": []}, {"text": "In this way, it is unnecessary to pre-build any intermediate supervision for training, such as preprocessed sketches and predesigned grammars (, which must be manually redesigned for an unseen kind of target language.", "labels": [], "entities": []}, {"text": "Furthermore, we use the model uncertainty estimates to reflect its prediction confidence.", "labels": [], "entities": []}, {"text": "Although different uncertainty estimates have been explored in semantic parsing (, we use Dropout () as the uncertainty signal () due to its simplicity, and use policy gradient algorithm to guide the model search.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7700569927692413}]}, {"text": "Our contributions are thus three-fold.", "labels": [], "entities": []}, {"text": "\u2022 We introduce the adaptive decoding mechanism into semantic parsing, which is well rid of intermediate representations and easily adaptable to new target languages.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7109850645065308}]}, {"text": "\u2022 We adopt uncertainty estimates to bias the decoder search, which is not covered in architecture searching literature to our best knowledge.", "labels": [], "entities": []}, {"text": "\u2022 Our model outperforms the state-of-the-art neural models without other intermediate supervisions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our method with other models on two datasets.", "labels": [], "entities": []}, {"text": "Our codes could be obtained via https://github.com/zxteloiv/AdaNSP.", "labels": [], "entities": []}, {"text": "We use the preprocessed ATIS and GeoQuery datasets kindly provided by Dong and Lapata (2018).", "labels": [], "entities": [{"text": "GeoQuery datasets", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.8502023220062256}]}, {"text": "All natural language sentences are converted to lower cases and stemmed with NLTK ().", "labels": [], "entities": []}, {"text": "Entity mentions like city codes, flight numbers are anonymized using numbered placeholders.", "labels": [], "entities": []}, {"text": "We choose hyperparameters on the ATIS dataset with the validation set.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9774093925952911}]}, {"text": "For the GeoQuery dataset that doesn't come with a validation set, we randomly shuffle the training set and select the top 100 records as the validation set, and the remaining as the new training data.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9557828009128571}]}, {"text": "After choosing the best hyperparameters, we resort back to train on the original set.", "labels": [], "entities": []}, {"text": "The Dropout ratio is selected from {0.5, 0.6, 0.7, 0.8}, and the embedding dimension dis chosen from {64, 128, 256, 512}.", "labels": [], "entities": []}, {"text": "We fix the batch size to 20, and both the encoder and decoder cell are two stacked LSTM layers.", "labels": [], "entities": []}, {"text": "We apply scheduled sampling () with the ratio 0.2 during training.", "labels": [], "entities": []}, {"text": "We run F = 5 forward passes before computing the variance.", "labels": [], "entities": [{"text": "F", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.9570245146751404}]}, {"text": "We use Adam ( for the optimizer, and use its default parameters from the paper.", "labels": [], "entities": []}, {"text": "We use the logical form accuracy as the evaluation metric, which is computed with parsed trees of the predictions and true labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9561566710472107}]}, {"text": "Two trees are considered identical as long as their structures are the same, i.e., the order to sibling predicates doesn't matter.", "labels": [], "entities": []}, {"text": "We reuse the STree parser code from Dong and Lapata (2018).", "labels": [], "entities": [{"text": "STree parser code from Dong and Lapata", "start_pos": 13, "end_pos": 51, "type": "DATASET", "confidence": 0.8571873903274536}]}], "tableCaptions": [{"text": " Table 1: Results on GeoQuery and ATIS datasets", "labels": [], "entities": [{"text": "ATIS datasets", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8200958371162415}]}]}