{"title": [{"text": "Are Red Roses Red? Evaluating Consistency of Question-Answering Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Although current evaluation of question-answering systems treats predictions in isolation , we need to consider the relationship between predictions to measure true understanding.", "labels": [], "entities": []}, {"text": "A model should be penalized for answering \"no\" to \"Is the rose red?\" if it answers \"red\" to \"What color is the rose?\".", "labels": [], "entities": []}, {"text": "We propose a method to automatically extract such implications for instances from two QA datasets, VQA and SQuAD, which we then use to evaluate the consistency of models.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.7525115311145782}, {"text": "VQA", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.7533024549484253}]}, {"text": "Human evaluation shows these generated implications are well formed and valid.", "labels": [], "entities": []}, {"text": "Consistency evaluation provides crucial insights into gaps in existing models, and retraining with implication-augmented data improves consistency on both synthetic and human-generated implications.", "labels": [], "entities": [{"text": "consistency", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.9639689326286316}]}], "introductionContent": [{"text": "Question-answering (QA) systems have become popular benchmarks for AI systems, as they require the ability to comprehend and employ complex reasoning about the question and the associated context.", "labels": [], "entities": [{"text": "Question-answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6397993937134743}]}, {"text": "In order to really excel in machine comprehension (, for example, models need to understand the entities, coreferences, and relations in the paragraph, and align them to the information need encoded in the question.", "labels": [], "entities": []}, {"text": "Similarly, Visual Question Answering requires not only perception abilities (fine-grained recognition, object detection), but also \"higher level reasoning\" about how the question is related to the visual information, commonsense reasoning, knowledge based reasoning, and the understanding of location/color/size attributes.", "labels": [], "entities": [{"text": "Visual Question Answering", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.6699532866477966}, {"text": "object detection", "start_pos": 103, "end_pos": 119, "type": "TASK", "confidence": 0.7086623311042786}, {"text": "commonsense reasoning", "start_pos": 217, "end_pos": 238, "type": "TASK", "confidence": 0.8395231068134308}]}, {"text": "However, recent work has shown that popular benchmarks have crucial limitations in their ability to test reasoning and comprehension.", "labels": [], "entities": []}, {"text": "For example, show that models can do well in the SQuAD dataset by using heuristic   lexical and type overlap between the context and the question.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.7592899799346924}]}, {"text": "Biases have also been observed in the popular VQA dataset, e.g. answering questions starting with \"Do you see a ...\" with \"yes\" results in 87% accuracy, and \"tennis\" is the correct answer for 41% of questions starting with \"What sport is ...\".", "labels": [], "entities": [{"text": "VQA dataset", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9588254988193512}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9956990480422974}]}, {"text": "While there are laudable efforts to try to diminish such biases (), they do not address a fundamental evaluation question: it is not only individual predictions that matter, but also whether multiple answers reflect a consistent and coherent model.", "labels": [], "entities": []}, {"text": "For example, in, models answer original questions correctly but answer follow-up questions in an inconsistent manner, which indicates they do not really understand the context or the questions (e.g. simultaneously predicting 0, 1, and 2 birds in.", "labels": [], "entities": []}, {"text": "In this paper, we propose evaluation for QA systems that measures the extent to which model predictions are consistent.", "labels": [], "entities": []}, {"text": "We first automatically generate new question-answer pairs that are implied by existing instances from the dataset (such as the ones in).", "labels": [], "entities": []}, {"text": "We use this generated dataset to evaluate models by penalizing them when their predictions are not consistent with these implications.", "labels": [], "entities": []}, {"text": "Human evaluation verifies that the generated implications are valid and well formed when compared to original instances, and thus can be used to evaluate and gain insights into models for VQA and SQuAD.", "labels": [], "entities": []}, {"text": "Finally, we propose a simple data augmentation procedure that results in models nearly as accurate as the original models on the original data, while being more consistent when measured by our implications and by human generated implications (and thus expected to generalize better in the real world).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we assess the quality of the generated (q , a ) pairs, measure consistency of models for VQA and SQuAD, and evaluate whether data  augmentation with implications can improve the consistency of existing models.", "labels": [], "entities": [{"text": "consistency", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.984693169593811}, {"text": "VQA", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.8801477551460266}]}], "tableCaptions": [{"text": " Table 3: Consistency of VQA Models.", "labels": [], "entities": []}, {"text": " Table 4: Consistency of SQuAD Models.", "labels": [], "entities": []}, {"text": " Table 5. Accuracy on the validation set remains  comparable after augmentation, while consistency  on both generated and worker-provided implica- tions improves across models and tasks. We also  evaluate SAAA on the GQA dataset (Hudson and  Manning, 2019) (Count and BAN use features that  are not allowed in GQA): while accuracy is com- parable (41.4% before augmentation, 40.4% after),  consistency goes up significantly (59.3% before,  64.7% after). These results indicate that data aug- mentation is useful for increasing consistency with", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9883111119270325}, {"text": "consistency", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.984454870223999}, {"text": "SAAA", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.9980341792106628}, {"text": "GQA dataset", "start_pos": 217, "end_pos": 228, "type": "DATASET", "confidence": 0.9749305844306946}, {"text": "Count", "start_pos": 258, "end_pos": 263, "type": "METRIC", "confidence": 0.9863318204879761}, {"text": "BAN", "start_pos": 268, "end_pos": 271, "type": "METRIC", "confidence": 0.962692379951477}, {"text": "accuracy", "start_pos": 322, "end_pos": 330, "type": "METRIC", "confidence": 0.9983761310577393}, {"text": "consistency", "start_pos": 390, "end_pos": 401, "type": "METRIC", "confidence": 0.9966031312942505}]}]}