{"title": [{"text": "Self-Supervised Learning for Contextualized Extractive Summarization", "labels": [], "entities": [{"text": "Contextualized Extractive Summarization", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.6751831869284312}]}], "abstractContent": [{"text": "Existing models for extractive summarization are usually trained from scratch with a cross-entropy loss, which does not explicitly capture the global context at the document level.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6455009877681732}]}, {"text": "In this paper, we aim to improve this task by introducing three auxiliary pre-training tasks that learn to capture the document-level context in a self-supervised fashion.", "labels": [], "entities": []}, {"text": "Experiments on the widely-used CNN/DM dataset validate the effectiveness of the proposed auxiliary tasks.", "labels": [], "entities": [{"text": "CNN/DM dataset", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.8910903781652451}]}, {"text": "Furthermore, we show that after pre-training, a clean model with simple building blocks is able to outperform previous state-of-the-art that are carefully designed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extractive summarization aims at shortening the original article while retaining the key information through the way of selection sentences from the original articles.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9016114473342896}]}, {"text": "This paradigm has been proven effective by many previous systems.", "labels": [], "entities": []}, {"text": "In order to decide whether to choose a particular sentence, the system should have a global view of the document context, e.g., the subject and structure of the document.", "labels": [], "entities": []}, {"text": "However, previous works) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context.", "labels": [], "entities": []}, {"text": "We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this Code can be found in this repository: https:// github.com/hongwang600/Summarization Last week, I went to attend a one-day meeting.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 309, "end_pos": 322, "type": "TASK", "confidence": 0.8286194205284119}]}, {"text": "I booked the flight in advanced.", "labels": [], "entities": []}, {"text": "[masked sentence] The earliest next flight will be a few days later.", "labels": [], "entities": []}, {"text": "I had to use the online discussion instead.", "labels": [], "entities": []}, {"text": "But the flight was cancelled due to the weather.", "labels": [], "entities": []}, {"text": "But I lost my passport.", "labels": [], "entities": []}, {"text": "The weather is good today.", "labels": [], "entities": []}], "datasetContent": [{"text": "To show the effectiveness of the pre-training method (Mask, Replace and Switch), we conduct experiments on the commonly used dataset CNN/DM (, and compare them with a popular baseline, which selects first three sentences as the summary, and the state-of-theart extractive summarization method NEUSUM ( , which jointly scores and selects sentences using pointer network.", "labels": [], "entities": [{"text": "CNN/DM", "start_pos": 133, "end_pos": 139, "type": "DATASET", "confidence": 0.7832253376642863}]}, {"text": "Model and training details We use the rulebased system from ( ) to label sentences in a document, e.g., sentences to be extracted will be labeled as 1.", "labels": [], "entities": []}, {"text": "Rouge score 3) is used to evaluate the performance of the model, and we report Rouge-1, Rouge-2, and Rouge-L as in prior work.", "labels": [], "entities": []}, {"text": "We use the pretrained glove embedding () with 100 dimensions to initialize the word embedding.", "labels": [], "entities": []}, {"text": "A one-layer bidirectional LSTM) is used as the sentence encoder, and the size of hidden state is 200.", "labels": [], "entities": []}, {"text": "A 5-layer Transformer encoder () with 4 heads is used as the document-level selfattention module.", "labels": [], "entities": []}, {"text": "A linear classification layer is used to predict whether to choose the sentence.", "labels": [], "entities": []}, {"text": "The training process consists of two phrases.", "labels": [], "entities": []}, {"text": "First, we use the pre-training task to pre-train the basic model using the raw article from the", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The Rouge (Lin", "labels": [], "entities": [{"text": "The Rouge", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.9200294613838196}]}]}