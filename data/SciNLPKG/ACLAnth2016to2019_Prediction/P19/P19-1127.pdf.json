{"title": [{"text": "Global Textual Relation Embedding for Relational Understanding", "labels": [], "entities": [{"text": "Global Textual Relation Embedding", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7386363446712494}, {"text": "Relational Understanding", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8413170576095581}]}], "abstractContent": [{"text": "Pre-trained embeddings such as word embed-dings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks.", "labels": [], "entities": []}, {"text": "In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities.", "labels": [], "entities": []}, {"text": "Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text.", "labels": [], "entities": [{"text": "Textual relation embedding", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6733484963575999}]}, {"text": "To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase.", "labels": [], "entities": [{"text": "English ClueWeb09 corpus to Freebase", "start_pos": 100, "end_pos": 136, "type": "DATASET", "confidence": 0.8114588260650635}]}, {"text": "We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding.", "labels": [], "entities": []}, {"text": "Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding.", "labels": [], "entities": [{"text": "relational understanding", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.7905656397342682}]}, {"text": "The data and code can be found at https://github.com/czyssrs/GloREPlus", "labels": [], "entities": []}], "introductionContent": [{"text": "Pre-trained embeddings such as word embeddings) and sentence embeddings () have become fundamental NLP tools.", "labels": [], "entities": []}, {"text": "Learned with large-scale (e.g., up to 800 billion tokens () open-domain corpora, such embeddings serve as a good prior fora wide range of downstream tasks by endowing task-specific models with general lexical, syntactic, and semantic knowledge.", "labels": [], "entities": []}, {"text": "Inspecting the spectrum of granularity, a representation between lexical (and phrasal) level and sentence level is missing.", "labels": [], "entities": []}, {"text": "Many tasks require relational understanding of the entities mentioned in the text, e.g., relation extraction and knowledge base completion.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.9254344999790192}, {"text": "knowledge base completion", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.6747681597868601}]}, {"text": "Textual relation (), defined as the shortest path between two entities in the dependency parse tree of a sentence, has been widely shown to be the main bearer of relational information in text and proved effective in relation extraction tasks ().", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 217, "end_pos": 236, "type": "TASK", "confidence": 0.7777502834796906}]}, {"text": "If we can learn a general-purpose embedding for textual relations, it may facilitate many downstream relational understanding tasks by providing general relational knowledge.", "labels": [], "entities": [{"text": "relational understanding", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7017527371644974}]}, {"text": "Similar to language modeling for learning general-purpose word embeddings, distant supervision () is a promising way to acquire supervision, at no cost, for training general-purpose embedding of textual relations.", "labels": [], "entities": []}, {"text": "Recently propose to leverage global co-occurrence statistics of textual and KB relations to learn embeddings of textual relations, and show that it can effectively combat the wrong labeling problem of distant supervision (see for example).", "labels": [], "entities": []}, {"text": "While their method, named GloRE, achieves the state-of-the-art performance on the popular New York Times (NYT) dataset (, the scope of their study is limited to relation extraction with smallscale in-domain training data.", "labels": [], "entities": [{"text": "GloRE", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8369619250297546}, {"text": "NYT) dataset", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.7885880867640177}, {"text": "relation extraction", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.8641630709171295}]}, {"text": "In this work, we take the GloRE approach further and apply it to large-scale, domainindependent data labeled with distant supervision, with the goal of learning general-purpose textual relation embeddings.", "labels": [], "entities": []}, {"text": "Specifically, we create the largest ever distant supervision dataset by linking the entire English ClueWeb09 corpus (half a billion of web documents) to the latest version of Freebase (, which contains 45 million entities and 3 billion relational facts.", "labels": [], "entities": [{"text": "English ClueWeb09 corpus", "start_pos": 91, "end_pos": 115, "type": "DATASET", "confidence": 0.8780564268430074}]}, {"text": "After filtering, we get a dataset with over 5 million unique textual relations and around 9 million cooccurring textual and KB relation pairs.", "labels": [], "entities": []}, {"text": "We then train textual relation embedding on the collected  The wrong labeling problem of distant supervision.", "labels": [], "entities": []}, {"text": "The Ford Motor Company is both founded by and named after Henry Ford.", "labels": [], "entities": []}, {"text": "The KB relation founder and named after are thus both mapped to all of the sentences containing the entity pair, resulting in many wrong labels (red dashed arrows).", "labels": [], "entities": []}, {"text": "Right: Global co-occurrence statistics from our distant supervision dataset, which clearly distinguishes the two textual relations.", "labels": [], "entities": [{"text": "distant supervision dataset", "start_pos": 48, "end_pos": 75, "type": "DATASET", "confidence": 0.719298928976059}]}, {"text": "dataset in away similar to (), but using Transformer () instead of vanilla RNN as the encoder for better training efficiency.", "labels": [], "entities": []}, {"text": "To demonstrate the usefulness of the learned textual relation embedding, we experiment on two relational understanding tasks, relation extraction and knowledge base completion.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.873128354549408}, {"text": "knowledge base completion", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.670512874921163}]}, {"text": "For relation extraction, we use the embedding to augment PCNN+ATT () and improve the precision for top 1000 predictions from 83.9% to 89.8%.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9455989599227905}, {"text": "PCNN", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.7750054597854614}, {"text": "ATT", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.7054651379585266}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.99949049949646}]}, {"text": "For knowledge base completion, we replace the neural network in () with our pre-trained embedding followed by a simple projection layer, and gain improvements on both MRR and HITS@10 measures.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.8043035268783569}, {"text": "MRR", "start_pos": 167, "end_pos": 170, "type": "METRIC", "confidence": 0.6596287488937378}, {"text": "HITS", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9624223709106445}]}, {"text": "Our major contributions are summarized as following: \u2022 We propose the novel task of learning general-purpose embedding of textual relations, which has the potential to facilitate a wide range of relational understanding tasks.", "labels": [], "entities": [{"text": "relational understanding", "start_pos": 195, "end_pos": 219, "type": "TASK", "confidence": 0.7572828829288483}]}, {"text": "\u2022 To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase.", "labels": [], "entities": [{"text": "English ClueWeb09 corpus to Freebase", "start_pos": 102, "end_pos": 138, "type": "DATASET", "confidence": 0.8257079601287842}]}, {"text": "The dataset is publicly available 1 . \u2022 Based on the global co-occurrence statistics of textual and KB relations, we learn a textual relation embedding on the collected dataset and demonstrate its usefulness on relational understanding tasks.", "labels": [], "entities": [{"text": "relational understanding", "start_pos": 211, "end_pos": 235, "type": "TASK", "confidence": 0.8212458491325378}]}, {"text": "Another line of research that relates to ours is the universal schema () for relation extraction, KB completion, as well as its extensions (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8409373164176941}, {"text": "KB completion", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.671590581536293}]}, {"text": "Wrong labeling problem still exists since their embedding is learned based on individual relation facts.", "labels": [], "entities": []}, {"text": "In contrast, we use the global cooccurrence statistics as explicit supervision signal.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the usefulness of the learned textual relation embedding on two popular relational understanding tasks, relation extraction and knowledge base completion.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.877782791852951}, {"text": "knowledge base completion", "start_pos": 157, "end_pos": 182, "type": "TASK", "confidence": 0.6736613313357035}]}, {"text": "We do not fine-tune the embedding, and only use in-domain data to train a single feedforward layer to project the embedding to the target relations of the domain.", "labels": [], "entities": []}, {"text": "We compare this with models that are specifically designed for those tasks and trained using in-domain data.", "labels": [], "entities": []}, {"text": "If we can achieve comparable or better results, it demonstrates that the general-purpose embedding captures useful information for downstream tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Relation extraction manual evaluation results:  Precision of top 1000 predictions.", "labels": [], "entities": [{"text": "Relation extraction manual evaluation", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.8453909307718277}, {"text": "Precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9727600812911987}]}, {"text": " Table 2: Results of KB completion on FB15k-237 dataset 4 , measured by MRR and HITS@10 (Both scaled by  100).", "labels": [], "entities": [{"text": "KB completion", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.6076669096946716}, {"text": "FB15k-237 dataset 4", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.9764817754427592}, {"text": "MRR", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.939008891582489}, {"text": "HITS", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9917120933532715}]}]}