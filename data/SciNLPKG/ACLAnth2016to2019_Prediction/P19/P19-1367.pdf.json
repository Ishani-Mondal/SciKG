{"title": [{"text": "Vocabulary Pyramid Network: Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation", "labels": [], "entities": [{"text": "Vocabulary Pyramid Network", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.6372036437193552}, {"text": "Response Generation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.887593686580658}]}], "abstractContent": [{"text": "We study the task of response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.9526439011096954}]}, {"text": "Conventional methods employ a fixed vocabulary and one-pass decoding, which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence.", "labels": [], "entities": []}, {"text": "To tackle the above two problems, we present a Vocabulary Pyramid Network (VPN) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.7686966061592102}]}, {"text": "Specifically, the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words.", "labels": [], "entities": []}, {"text": "Then, multi-pass encoding and decoding are conducted on the multi-level vocabularies.", "labels": [], "entities": [{"text": "multi-pass encoding", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.6567366570234299}]}, {"text": "Since VPN is able to leverage rich encoding and decoding information with multi-level vocabularies, it has the potential to generate better responses.", "labels": [], "entities": []}, {"text": "Experiments on English Twitter and Chinese Wei-bo datasets demonstrate that VPN remarkably outperforms strong baselines.", "labels": [], "entities": [{"text": "Chinese Wei-bo datasets", "start_pos": 35, "end_pos": 58, "type": "DATASET", "confidence": 0.6354325513044993}]}], "introductionContent": [{"text": "As one of the long-term goals in AI and NLP, automatic conversation devotes to constructing automatic dialogue systems to communicate with humans.", "labels": [], "entities": []}, {"text": "Benefited from large-scale human-human conversation data available on the Internet, data-driven dialog systems have attracted increasing attention of both academia and industry.", "labels": [], "entities": []}, {"text": "Recently, a popular approach to build dialog engines is to learn a response generation model within an encoder-decoder framework such as sequence-to-sequence (Seq2Seq) model).", "labels": [], "entities": []}, {"text": "In such a framework, an encoder transforms the source sequence into hidden vectors, and a decoder generates the targeted sequence based on the encoded vectors and previ-: Vocabulary pyramid networks for response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.7954900860786438}]}, {"text": "The dialogue input (context) and output (response) are represented by multi-level vocabularies (e.g., raw words, low-level clusters and high-level clusters) and then processed by multi-pass encoder and ously generated words.", "labels": [], "entities": []}, {"text": "In this process, the encoder and decoder share a vocabulary (word list) , and the targeted words are typically performed by a softmax classifier over the vocabulary word-byword.", "labels": [], "entities": []}, {"text": "However, such typical Seq2Seq model is prone to generate safe and repeated responses, such as \"Me too\" and \"I don't know\".", "labels": [], "entities": []}, {"text": "In addition to the exposure bias issue 2 , the main reasons of this problem include: 1) a fixed (single) vocabulary (word list) in decoding, which usually covers high-frequency words, so it is easy to capture high-frequency patterns (e.g., \"Me too\") and lose a great deal of content information in middle and low-frequency patterns; 2) one-pass decoding, where word-by-word generation from left to right is prone to error accumulation since previously generated erroneous words will greatly affect future un-generated words.", "labels": [], "entities": []}, {"text": "More importantly, it can leverage only the previously generated words but not the future un-generated words.", "labels": [], "entities": []}, {"text": "In fact, there are some researches in text generation tasks such as dialogue generation, machine translation and text summarization, are dedicated to solving the above issues.", "labels": [], "entities": [{"text": "text generation", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7730451226234436}, {"text": "dialogue generation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8637061715126038}, {"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.8116999268531799}, {"text": "text summarization", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7739045917987823}]}, {"text": "In order to alleviate issues on the fixed vocabulary, incorporated dynamic vocabulary mechanism into Seq2Seq models, which dynamically allocates vocabularies for each input by a vocabulary prediction model.", "labels": [], "entities": []}, {"text": "presented topic aware response generation by incorporating topic words obtained from a pre-trained LDA model (.", "labels": [], "entities": [{"text": "topic aware response generation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6425980031490326}]}, {"text": "Besides, several works attempted to solve the dilemma of one-pass decoding.", "labels": [], "entities": []}, {"text": "Xia et al. proposed deliberation network for sequence generation, where the first-pass decoder generates a rough sequence and then the secondpass decoder refines the rough sequence.", "labels": [], "entities": [{"text": "sequence generation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7482423186302185}]}, {"text": "However, so far there has been no unified framework to solve both of the aforementioned problems.", "labels": [], "entities": []}, {"text": "In this study, we present Vocabulary Pyramid Networks (VPN) to tackle the issues of one fixed vocabulary and one-pass decoding simultaneously.", "labels": [], "entities": []}, {"text": "Specifically, VPN incorporates multipass encoding and decoding with multi-level vocabularies into response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7371104657649994}]}, {"text": "As depicted in, the multi-level vocabularies contain raw words, low-level clusters and high-level clusters, where low-level and high-level clusters are obtained from hierarchical clustering of raw words.", "labels": [], "entities": []}, {"text": "Afterward, the multi-pass encoder (rawword encoder, low-level encoder, and high-level encoder) gradually works on diminishing vocabularies from raw words to low-level clusters until to high-level clusters, and it looks like a \"pyramid\" concerning the vocabulary size.", "labels": [], "entities": []}, {"text": "On the other side, the multi-pass decoder gradually increases the size of processed vocabularies from high-level clusters to low-level clusters and finally to raw words.", "labels": [], "entities": []}, {"text": "From a theoretical point of view, people usually associate raw input words with low-level or highlevel abstractions like semantic meanings and concepts on human-human conversations.", "labels": [], "entities": []}, {"text": "Based on the abstractive cognition, people organize contents and select the expressive words as the response (.", "labels": [], "entities": []}, {"text": "From a practical perspective, VPN is able to capture much more sequence information with multi-level vocabularies.", "labels": [], "entities": [{"text": "VPN", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9342345595359802}]}, {"text": "As a result, VPN has the potential to generate better responses.", "labels": [], "entities": []}, {"text": "To verify the effectiveness of the proposed model, we conduct experiments on two public response generation datasets: English Twitter and Chinese Weibo.", "labels": [], "entities": [{"text": "Chinese Weibo", "start_pos": 138, "end_pos": 151, "type": "DATASET", "confidence": 0.719102531671524}]}, {"text": "Both automatic and manual evaluations demonstrate that the proposed VPN is remarkably better than the state-of-the-art.", "labels": [], "entities": [{"text": "VPN", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8859035968780518}]}], "datasetContent": [{"text": "There are large-scale message-response pairs on social websites, which consist of informational text from different topics ().", "labels": [], "entities": []}, {"text": "Our experimental data comes from two public corpus: English \"Twitter\" and Chinese \"Weibo\" ().", "labels": [], "entities": []}, {"text": "In order to improve the quality of datasets, some noisy message-response pairs are filtered (e.g., containing too many punctuations or emoticons), and the datasets are randomly split into Train/Dev/Test by a proportion (9:0.5:0.5).", "labels": [], "entities": []}, {"text": "Evaluation for generative responses is a challenging and under-researching problem).", "labels": [], "entities": [{"text": "generative responses", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.8815388083457947}]}, {"text": "Similar to (, we borrow two well-established automatic evaluation metrics from machine translation and text summarization: BLEU () and ROUGE (Lin, 2004) 7 , which could be leveraged to analyze the co-occurrences of n-gram between the generated responses and references.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7290324568748474}, {"text": "text summarization", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.6897506415843964}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9987873435020447}, {"text": "ROUGE", "start_pos": 135, "end_pos": 140, "type": "METRIC", "confidence": 0.9920746088027954}]}, {"text": "In addition to automatic evaluations, we also leverage manual evaluations to enhance the evaluations.", "labels": [], "entities": []}, {"text": "Following previous studies, we employ three metrics for manual evaluations as follows.", "labels": [], "entities": []}, {"text": "1) Fluency (Flu.): measuring the grammaticality and fluency of generated responses, where too short responses are regarded as lack of fluency.", "labels": [], "entities": [{"text": "Fluency (Flu.)", "start_pos": 3, "end_pos": 17, "type": "METRIC", "confidence": 0.933256670832634}]}, {"text": "2) Consistency (Con.): measuring whether the generated responses are consistent with the inputs or not.", "labels": [], "entities": []}, {"text": "3) Informativeness (Inf.): measuring whether the response provides informative (knowledgeable) contents or not.", "labels": [], "entities": [{"text": "Informativeness (Inf.)", "start_pos": 3, "end_pos": 25, "type": "METRIC", "confidence": 0.6488594487309456}]}, {"text": "(1) S2SA: Sequence-to-Sequence () with attention mechanisms (.", "labels": [], "entities": []}, {"text": "Similar to manual evaluations used in, we conduct a pair-wise comparison between the response generated by VPN and the one for the same input by two typical baselines: S2STA and DelNet.", "labels": [], "entities": []}, {"text": "we sample 100 responses from each system, then two curators judge (win, tie and lose) between these two methods.", "labels": [], "entities": []}, {"text": "The results of manual evaluations are shown in, where the score is the percentage that VPN wins a baseline after removing \"tie\" pairs.", "labels": [], "entities": []}, {"text": "The Cohen Kappa for interannotator statistics is 61.2, 62.1 and 70.8 for fluency, consistency and informativeness, respectively.", "labels": [], "entities": [{"text": "consistency", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.9977890253067017}]}, {"text": "We can see that our VPN is significantly (sign test, p-value < 0.01) better than all baselines in terms of the three metrics, which further demonstrates that VPN is able to deliver fluent, consistent and informative responses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performances on whether using multi-level  vocabularies or not, where \"SV\" represents single vo- cabulary (from raw words), and \"MVs\" means multi- level vocabularies obtained from hierarchical cluster- ing. \"enc\" and \"dec\" denote encoder and decoder, re- spectively, and numbers after them represent how many  passes. For example, \"enc1-dec3\" means a encoder a- long with three passes of decoders.", "labels": [], "entities": []}, {"text": " Table 3: Influences of multi-pass encoding and decod- ing, where \"w/o\" indicates without, \"ED\" represents  encoder and decoder. For example, \"w/o low-level ED\"  means removing low-level encoder and low-level de- coder.", "labels": [], "entities": [{"text": "multi-pass encoding", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8073899447917938}, {"text": "decod- ing", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.6456483900547028}]}, {"text": " Table 3. We can clearly see that  removing any encoder and decoder causes obvious  performance degradation. Specifically, \"w/o high- level ED\" obtains worse performances than \"w/o  low-level ED\". We guess that the high-level en- coder and decoder are well trained since they have  the smallest vocabulary (the size of high-level  clusters is only 340), so removing the well-trained  component (\"w/o high-level ED\") performs poor- ly (Details in Section 4.8). Furthermore, \"w/o  low&high-level ED\" performs the worst. This fur- ther indicates that multi-pass encoder and decoder  contribute to generating better responses.", "labels": [], "entities": []}, {"text": " Table 4: Manual evaluations with fluency (Flu.), con- sistency (Con.), and informativeness (Inf.). The score is  the percentage that VPN wins a baseline after removing  \"tie\" pairs. VPN is clearly better than all baselines on  the three metrics, and all results are at 99% confidence  intervals.", "labels": [], "entities": [{"text": "Flu.", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9688163995742798}, {"text": "con- sistency (Con.)", "start_pos": 50, "end_pos": 70, "type": "METRIC", "confidence": 0.9181468884150187}, {"text": "Inf.", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.8604345321655273}]}, {"text": " Table 5: Performances on each decoder in VPN 8 .", "labels": [], "entities": []}]}