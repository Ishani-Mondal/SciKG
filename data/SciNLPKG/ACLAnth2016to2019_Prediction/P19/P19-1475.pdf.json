{"title": [{"text": "Learning to Rank for Plausible Plausibility", "labels": [], "entities": []}], "abstractContent": [{"text": "Researchers illustrate improvements in contex-tual encoding strategies via resultant performance on a battery of shared Natural Language Understanding (NLU) tasks.", "labels": [], "entities": [{"text": "contex-tual encoding", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7082541882991791}]}, {"text": "Many of these tasks are of a categorical prediction variety: given a conditioning context (e.g., an NLI premise), provide a label based on an associated prompt (e.g., an NLI hypothesis).", "labels": [], "entities": []}, {"text": "The categorical nature of these tasks has led to common use of across entropy log-loss objective during training.", "labels": [], "entities": []}, {"text": "We suggest this loss is intuitively wrong when applied to plausibility tasks, where the prompt by design is neither categorically entailed nor contradictory given the context.", "labels": [], "entities": []}, {"text": "Log-loss naturally drives models to assign scores near 0.0 or 1.0, in contrast to our proposed use of a margin-based loss.", "labels": [], "entities": []}, {"text": "Following a discussion of our intuition, we describe a confirmation study based on an extreme , synthetically curated task derived from MultiNLI.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.938975989818573}]}, {"text": "We find that a margin-based loss leads to a more plausible model of plausibil-ity.", "labels": [], "entities": []}, {"text": "Finally, we illustrate improvements on the Choice Of Plausible Alternative (COPA) task through this change in loss.", "labels": [], "entities": []}], "introductionContent": [{"text": "Contextualized encoders such as GPT () and BERT) have led to improvements on various structurally similar Natural Language Understanding (NLU) tasks such as variants of Natural Language Inference (NLI).", "labels": [], "entities": [{"text": "BERT", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9956210255622864}]}, {"text": "Such tasks model the conditional interpretation of a sentence (e.g., an NLI hypothesis) based on some other context (usually some other sentence, e.g., an NLI premise).", "labels": [], "entities": []}, {"text": "The structural similarity of these tasks points to a structurally similar modeling approach: (1) concatenate the conditioning context (premise) to a sentence to be interpreted, p I just stopped where I wash E I stopped in my tracks h N I stopped running right were I wash N I stopped running right were I wash C I continued on my way: COPA-like pairs maybe constructed from datasets such as MultiNLI, where a premise and two hypotheses are presented, where the correct -most plausible -item depends on the competing hypothesis.: Dev set score distribution on COPA-pairs derived from MNLI, after training with cross entropy logloss and margin-loss.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 391, "end_pos": 399, "type": "DATASET", "confidence": 0.9406054019927979}, {"text": "MNLI", "start_pos": 583, "end_pos": 587, "type": "DATASET", "confidence": 0.9017206430435181}]}, {"text": "Margin-loss leads to a more intuitively plausible encoding of read this pair using a contextualized encoder, then (3) employ the resultant representation to support classification under the label set of the task.", "labels": [], "entities": []}, {"text": "NLI datasets employ a categorical label scheme (Entailment, Neutral, Contradiction) which has led to the use of a cross-entropy log-loss objective at training time: learn to maximize the probability of the correct label, and thereby minimize the probability of the competing labels.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9174398183822632}]}, {"text": "We suggest that this approach is intuitively problematic when applied to a task such as COPA (Choice Of Plausible Alternative) by, where one is provided with a premise and two or more alternatives, and the model must select the most sensible hypothesis, with respect to the premise and the other options.", "labels": [], "entities": []}, {"text": "As compared to NLI datasets, COPA was designed to have alternatives that are neither strictly true nor false in context: a procedure that maximizes the probability of the correct item at training time, thereby minimizing the probability of the other alternative(s), will seemingly learn to misread future examples.", "labels": [], "entities": []}, {"text": "We argue that COPA-style tasks should intuitively be approached as learning to rank problems (, where an encoder on competing items is trained to assign relatively higher or lower scores to candidates, rather than maximizing or minimizing probabilities.", "labels": [], "entities": []}, {"text": "In the following we investigate three datasets, beginning with a constructed COPA-style variant of MultiNLI (, later MNLI), designed to be adversarial (see.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9236840009689331}, {"text": "MNLI", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.6924142837524414}]}, {"text": "Results on this dataset support our intuition (see).", "labels": [], "entities": []}, {"text": "We then construct a second synthetic dataset based on JOCI (, which employed a finer label set than NLI, and a margin-based approach strictly outperforms log-loss in this case.", "labels": [], "entities": [{"text": "JOCI", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8308264017105103}]}, {"text": "Finally, we demonstrate state-of-the-art on COPA, showing that a BERT-based model trained with margin-loss significantly outperforms a log-loss alternative.", "labels": [], "entities": [{"text": "COPA", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.7173788547515869}, {"text": "BERT-based", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9970691800117493}]}], "datasetContent": [{"text": "We consider three datasets: MNLI, JOCI, and COPA.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.8921445608139038}, {"text": "JOCI", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.5996866822242737}, {"text": "COPA", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.9194249510765076}]}, {"text": "These are all cast as plausibility datasets, into a format comprising (p, h, h ) triples, where h is more plausible than h under the context of premise p.", "labels": [], "entities": []}, {"text": "MNLI In MNLI, each premise p is paired with 3 hypotheses.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9540075063705444}, {"text": "MNLI", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.7599666714668274}]}, {"text": "We cast the label on each hypothesis as a relative plausibility judgment, where entailment > neutral > contradiction (we label them as 2, 1, and 0).", "labels": [], "entities": []}, {"text": "We construct two 2-choice plausibility tasks from MNLI: MNLI 1 comprises all pairs labeled with 2/1, 2/0, or 1/0; whereas MNLI 2 removes the presumably easier 2/0 pairs.", "labels": [], "entities": []}, {"text": "For MNLI 1 , the training set is constructed from the original MNLI training dataset, and the dev set for MNLI 1 is derived from the original MNLI matched dev dataset.", "labels": [], "entities": [{"text": "MNLI training dataset", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.9358406861623129}, {"text": "MNLI", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.7740663886070251}, {"text": "MNLI matched dev dataset", "start_pos": 142, "end_pos": 166, "type": "DATASET", "confidence": 0.8628606647253036}]}, {"text": "For MNLI 2 , all of the examples in our training and dev sets is taken from the original MNLI training dataset, hence the same premise exists in both training and dev.", "labels": [], "entities": [{"text": "MNLI training dataset", "start_pos": 89, "end_pos": 110, "type": "DATASET", "confidence": 0.9054024616877238}]}, {"text": "This is by our adversarial design: each neutral hypothesis appears either as the preferred (beating contradiction), or dispreferred alternative (beaten by entailment), which is flipped at evaluation time.", "labels": [], "entities": []}, {"text": "JOCI In JOCI, every inference pair is labeled with their ordinal inference Likert-scale labels 5, 4, 3, 2, or 1.", "labels": [], "entities": [{"text": "JOCI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9442842602729797}]}, {"text": "Similar to MNLI, we cast these to 2-choice problems under the following conditions: We ignore inference pairs with scores below 3, aiming for sets akin to COPA, where even the dispreferred option is still often semi-plausible.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.889611542224884}]}, {"text": "COPA We label alternatives as 1 (the more plausible one) and 0 (otherwise).", "labels": [], "entities": [{"text": "COPA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8355792164802551}]}, {"text": "The original dev set in COPA is used as the training set.", "labels": [], "entities": [{"text": "COPA", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.9045937657356262}]}, {"text": "shows the statistics of these datasets.", "labels": [], "entities": []}, {"text": "Setup We fine-tune the BERT-BASE-UNCASED (Devlin et al., 2019) using our proposed margin-  based loss, and perform hyperparameter search on the margin parameter \u03be.", "labels": [], "entities": [{"text": "BERT-BASE-UNCASED", "start_pos": 23, "end_pos": 40, "type": "METRIC", "confidence": 0.9974034428596497}]}, {"text": "For the recast MNLI and JOCI datasets, the margin hyperparameter \u03be = 0.2.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.8823004364967346}, {"text": "JOCI datasets", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.8970783054828644}, {"text": "margin hyperparameter \u03be", "start_pos": 43, "end_pos": 66, "type": "METRIC", "confidence": 0.9444229602813721}]}, {"text": "Since COPA does not have a training set, we use the original dev set as the training set, and perform 10-fold cross validation to find the best hyperparameter \u03be = 0.37.", "labels": [], "entities": [{"text": "COPA", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.9004271626472473}]}, {"text": "We employ the Adam optimizer () with initial learning rate \u03b7 = 3 \u00d7 10 \u22125 , finetune for at most 3 epochs and use early-stopping to select the best model.", "labels": [], "entities": [{"text": "initial learning rate \u03b7", "start_pos": 37, "end_pos": 60, "type": "METRIC", "confidence": 0.7243957668542862}, {"text": "finetune", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9709281921386719}]}, {"text": "shows results on the recast MNLI and JOCI datasets.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.8119233250617981}, {"text": "JOCI datasets", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.9132682681083679}]}, {"text": "We find that for the two synthetic MNLI datasets, margin-loss performs similarly to cross entropy log-loss.", "labels": [], "entities": [{"text": "MNLI datasets", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8518145084381104}, {"text": "margin-loss", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9692378044128418}]}, {"text": "Shifting to the JOCI datasets, with less extreme (contradiction / entailed) hypotheses, especially in the adversarial JOCI 2 variant, marginloss outperforms log-loss.", "labels": [], "entities": [{"text": "JOCI datasets", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.9777324795722961}, {"text": "marginloss", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9523212313652039}]}], "tableCaptions": [{"text": " Table 1: Statistics of various plausibility datasets. All  numbers are numbers of (p, h, h ) triplets.", "labels": [], "entities": []}, {"text": " Table 2: Results on recast MNLI and JOCI.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.7385793924331665}, {"text": "JOCI", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9016937017440796}]}, {"text": " Table 3: Experimental results on COPA test set.", "labels": [], "entities": [{"text": "COPA test set", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.910348653793335}]}, {"text": " Table 4: Examples of premises and their corresponding hypotheses in various plausibility datasets, with gold labels  and scores given by the log-loss and margin-loss trained models.", "labels": [], "entities": []}]}