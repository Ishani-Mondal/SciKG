{"title": [{"text": "Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural NLG", "labels": [], "entities": [{"text": "Neural NLG", "start_pos": 85, "end_pos": 95, "type": "TASK", "confidence": 0.7057112157344818}]}], "abstractContent": [{"text": "Neural natural language generation (NNLG) from structured meaning representations has become increasingly popular in recent years.", "labels": [], "entities": [{"text": "Neural natural language generation (NNLG)", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.841517048222678}]}, {"text": "While we have seen progress with generating syntactically correct utterances that preserve semantics, various shortcomings of NNLG systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and maybe dull and repetitive.", "labels": [], "entities": []}, {"text": "This paper addresses these two critical challenges in NNLG by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews, and (2) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output.", "labels": [], "entities": []}, {"text": "We present YELPNLG, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel methodology that can be scalably reused to generate NLG datasets for other domains.", "labels": [], "entities": []}, {"text": "The experiments show that the models control important aspects, including lexical choice of adjectives , output length, and sentiment, allowing the models to successfully hit multiple style targets without sacrificing semantics.", "labels": [], "entities": []}], "introductionContent": [{"text": "The increasing popularity of personal assistant dialog systems and the success of end-to-end neural models on problems such as machine translation has lead to a surge of interest around data-totext neural natural language generation (NNLG).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.8130986094474792}, {"text": "data-totext neural natural language generation (NNLG)", "start_pos": 186, "end_pos": 239, "type": "TASK", "confidence": 0.7274063676595688}]}, {"text": "State-of-the-art NNLG models commonly use a sequence-to-sequence framework for end-to-end neural language generation, taking a meaning representation (MR) as input, and generating a natural language (NL) realization as output).", "labels": [], "entities": [{"text": "end-to-end neural language generation", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.769393265247345}]}, {"text": "shows some examples of MR to human and system NL realizations from recently popular NNLG datasets.", "labels": [], "entities": [{"text": "MR to human and system NL realizations", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.7562504410743713}, {"text": "NNLG datasets", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9679877161979675}]}, {"text": "The real power of NNLG models over traditional statistical generators is their ability to produce natural language output from structured input in a completely data-driven way, without needing hand-crafted rules or templates.", "labels": [], "entities": []}, {"text": "However, these models suffer from two critical bottlenecks: (1) a data bottleneck, i.e. the lack of large parallel training data of MR to NL, and (2) a control bottleneck, i.e. the inability to systematically control important aspects of the generated output to allow for more stylistic variation.", "labels": [], "entities": []}, {"text": "Recent efforts to address the data bottleneck with large corpora for training neural generators have relied almost entirely on high-effort, costly crowdsourcing, asking humans to write references given an input MR.", "labels": [], "entities": []}, {"text": "shows two recent efforts: the E2E NLG challenge () and the WEBNLG challenge (, both with an example of an MR, human reference, and system realization.", "labels": [], "entities": [{"text": "E2E NLG challenge", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.8462215065956116}, {"text": "WEBNLG", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.6376431584358215}, {"text": "system realization", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.6690144389867783}]}, {"text": "The largest dataset, E2E, consists of 50k instances.", "labels": [], "entities": [{"text": "E2E", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.9324837327003479}]}, {"text": "Other datasets, such as the Laptop (13k) and TV (7k) product review datasets, are similar but smaller.", "labels": [], "entities": [{"text": "TV (7k) product review datasets", "start_pos": 45, "end_pos": 76, "type": "DATASET", "confidence": 0.7331496562276568}]}, {"text": "These datasets were created primarily to focus on the task of semantic fidelity, and thus it is very evident from comparing the human and system outputs from each system that the model realizations are less fluent, descriptive, and natural than the human reference.", "labels": [], "entities": []}, {"text": "Also, the nature of the domains (restaurant description, Wikipedia infoboxes, and technical product reviews) are not particularly descriptive, exhibiting little variation.", "labels": [], "entities": []}, {"text": "Other work has also focused on the control bottleneck in NNLG, but has zoned in on one particular dimension of style, such as sentiment, length, 1 -E2E () 50k -Crowdsourcing (Domain: Restaurant Description) MR: name[Blue Spice], eatType, food, area, familyFriendly, near Human: Situated near the Rainbow Vegetarian Cafe in the riverside area of the city, The Blue Spice restaurant is ideal if you fancy traditional English food whilst outwith the kids.", "labels": [], "entities": [{"text": "NNLG", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.9466153383255005}, {"text": "length", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9475672245025635}, {"text": "MR", "start_pos": 207, "end_pos": 209, "type": "METRIC", "confidence": 0.9801226258277893}]}, {"text": "System: Blue Spice is a family friendly English restaurant in the riverside area near Rainbow Vegetarian Cafe.", "labels": [], "entities": [{"text": "Rainbow Vegetarian Cafe", "start_pos": 86, "end_pos": 109, "type": "DATASET", "confidence": 0.9506750504175822}]}, {"text": "2 -WebNLG ( 21k -DBPedia and Crowdsourcing (Domain: Wikipedia) MR: (Buzz-Aldrin, mission, Apollo-11), (Buzz-Aldrin, birthname, \"Edwin Eugene Aldrin Jr.\"),, (Apollo-11, operator, NASA) Human: Buzz Aldrin (born as Edwin Eugene Aldrin Jr) was a crew member for NASA's Apollo 11 and had 20 awards.", "labels": [], "entities": [{"text": "MR", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9881572127342224}]}, {"text": "System: Buzz aldrin, who was born in edwin eugene aldrin jr., was a crew member of the nasa operated apollo 11.", "labels": [], "entities": [{"text": "edwin eugene aldrin jr.", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.7285801321268082}]}, {"text": "he was awarded 20 by nasa.", "labels": [], "entities": []}, {"text": "3 -YelpNLG (this work) 300k -Auto.", "labels": [], "entities": []}, {"text": "Extraction (Domain: Restaurant Review) MR: (attr=food, val=taco, adj=no-adj, mention=1), (attr=food, val=flour-tortilla, adj=small, mention=1), (attr=food, val=beef, adj=marinated, mention=1), (attr=food, val=sauce, adj=spicy, mention=1) +[sentiment=positive, len=long, first-person=false, exclamation=false] Human: The taco was a small flour tortilla topped with marinated grilled beef, asian slaw and a spicy delicious sauce.", "labels": [], "entities": [{"text": "MR", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9924778342247009}]}, {"text": "System: The taco was a small flour tortilla with marinated beef and a spicy sauce that was a nice touch.  or formality.", "labels": [], "entities": []}, {"text": "However, human language actually involves a constellation of interacting aspects of style, and NNLG models should be able to jointly control these multiple interacting aspects.", "labels": [], "entities": []}, {"text": "In this work, we tackle both bottlenecks simultaneously by leveraging masses of freely available, highly descriptive user review data, such as that shown in.", "labels": [], "entities": []}, {"text": "These naturally-occurring examples show a highly positive and highly negative review for the same restaurant, with many examples of rich language and detailed descriptions, such as \"absurdly overpriced\", and \"ridiculously delicious\".", "labels": [], "entities": []}, {"text": "Given the richness of this type of free, abundant data, we ask: (1) can this freely available data be used for training NNLG models?, and (2) is it possible to exploit the variation in the data to develop models that jointly control multiple interacting aspects of semantics and style?", "labels": [], "entities": []}, {"text": "We address these questions by creating the YELPNLG corpus, consisting of 300k MR to reference pairs for training NNLGs, collected completely automatically using freely available data (such as that in), and off-the-shelf tools.", "labels": [], "entities": [{"text": "YELPNLG corpus", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.8648270964622498}]}, {"text": "Rather than starting with a meaning representation and collecting human references, we begin with the references (in the form of review sentences), and work backwards -systematically constructing meaning representations for the sentences using dependency parses and rich sets of lexical, syntactic, and sentiment information, including ontological knowledge from DBPedia.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 363, "end_pos": 370, "type": "DATASET", "confidence": 0.9547988772392273}]}, {"text": "This method uniquely exploits existing data which is naturally rich in semantic content, emotion, and varied language.", "labels": [], "entities": []}, {"text": "Row 3 of shows an example MR from YELPNLG, consisting of relational tuples of attributes, values, adjectives, and order information, as well as sentence-level information including sentiment, length, and pronouns.", "labels": [], "entities": [{"text": "YELPNLG", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.6221158504486084}]}, {"text": "Once we have created the YELPNLG corpus, we are in the unique position of being able to explore, for the first time, how varying levels of supervision in the encoding of content, lexical choice, and sentiment can be exploited to control style in NNLG.", "labels": [], "entities": [{"text": "YELPNLG corpus", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.8455440998077393}]}, {"text": "Our contributions include: \u2022 A new corpus, YELPNLG, larger and more lexically and stylistically varied than existing NLG datasets; \u2022 A method for creating corpora such as YELPNLG, which should be applicable to other domains; \u2022 Experiments on controlling multiple interacting aspects of style with an NNLG while maintaining semantic fidelity, and results using abroad range of evaluation methods; \u2022 The first experiments, to our knowledge, showing that an NNLG can be trained to control lexical choice of adjectives.", "labels": [], "entities": []}, {"text": "We leave a detailed review of prior work to Section 5 where we can compare it with our own.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.9396257996559143}]}], "datasetContent": [{"text": "We examine the quality of the MR extraction with a qualitative study evaluating YELPNLG MR to NL pairs on various dimensions.", "labels": [], "entities": [{"text": "MR extraction", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.9631749391555786}, {"text": "YELPNLG MR", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.8733359575271606}]}, {"text": "Specifically, we evaluate content preservation (how much of the MR content appears in the NL, specifically, nouns and their corresponding adjectives from our parses), fluency (how \"natural sounding\" the NL is, aiming for both grammatical errors and general fluency), and sentiment (what the perceived sentiment of the NL is).", "labels": [], "entities": [{"text": "content preservation", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.6885192692279816}]}, {"text": "We note that we conduct the same study over our NNLG test outputs when we generate data using YELPNLG in Section 4.3.", "labels": [], "entities": [{"text": "NNLG test outputs", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.8946288625399271}]}, {"text": "We randomly sample 200 MRs from the YELPNLG dataset, along with their corresponding NL references, and ask 5 annotators on Mechanical Turk to rate each output on a 5 point Likert scale (where 1 is low and 5 is high for content and fluency, and where 1 is negative and 5 is positive for sentiment).", "labels": [], "entities": [{"text": "YELPNLG dataset", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9420167803764343}, {"text": "Mechanical Turk", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.8715221285820007}]}, {"text": "For content and fluency, we compute the average score across all 5 raters for each item, and average those scores to get a final rating for each model, such that higher content and fluency scores are better.", "labels": [], "entities": []}, {"text": "We compute sentiment error by converting the judgments into 3 bins to match the Yelp review scores (as we did during MR creation), finding the average rating for all 5 annotators per item, then computing the difference between their average score and the true sentiment rating in the reference text (from the original review), such that lower sentiment error is better.", "labels": [], "entities": []}, {"text": "The average ratings for content and fluency are high, at 4.63 and 4.44 out of 5, respectively, meaning that there are few mistakes in marking attribute and value pairs in the NL references, and that the references are also fluent.", "labels": [], "entities": []}, {"text": "This is an important check because correct grammar/spelling/punctuation is not a restriction in Yelp reviews.", "labels": [], "entities": [{"text": "Yelp reviews", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.9352601170539856}]}, {"text": "For sentiment, the largest error is 0.58 (out of 3), meaning that the perceived sentiment by raters does not diverge greatly, on average, from the Yelp review sentiment assigned in the MR, and indicates that inheriting sentence sentiment from the review is a reasonable heuristic.", "labels": [], "entities": [{"text": "Yelp review sentiment assigned in the MR", "start_pos": 147, "end_pos": 187, "type": "DATASET", "confidence": 0.718087055853435}]}, {"text": "To evaluate whether the models effectively hit semantic and stylistic targets, we randomly split the YELPNLG corpus into 80% train (\u223c235k instances), 10% dev and test (\u223c30k instances each), and create 4 versions of the corpus: BASE, +ADJ, +SENT, and +STYLE, each with the same split.", "labels": [], "entities": [{"text": "YELPNLG corpus", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.9038216173648834}, {"text": "BASE", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9965205192565918}, {"text": "SENT", "start_pos": 240, "end_pos": 244, "type": "METRIC", "confidence": 0.9564394354820251}, {"text": "STYLE", "start_pos": 251, "end_pos": 256, "type": "METRIC", "confidence": 0.7425509095191956}]}, {"text": "7 shows examples of output generated by the models fora given test MR, showing the effects of training models with increasing information.", "labels": [], "entities": []}, {"text": "Note that we present the longest version of the MR (that used for the +STYLE model), so the BASE, +ADJ, and +SENT models use the same MR minus the additional information.", "labels": [], "entities": [{"text": "MR", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9964017868041992}, {"text": "BASE", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9965282082557678}, {"text": "SENT", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9863879680633545}]}, {"text": "Row 1 shows an example of partially correct sentiment for BASE, and fully correct sentiment for the rest; +ADJ gets the adjectives right, +SENT is more descriptive, and +STYLE hits all targets.", "labels": [], "entities": [{"text": "BASE", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.7086362838745117}, {"text": "ADJ", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9646899104118347}, {"text": "SENT", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9964336156845093}, {"text": "STYLE", "start_pos": 170, "end_pos": 175, "type": "METRIC", "confidence": 0.9294418692588806}]}, {"text": "Row 2 gives an example of extra length in +STYLE, \"the meat was so ten- Since we randomly split the data, we compute the overlap between train and test for each corpus version, noting that around 14% of test MRs exist in training for the most specific +STYLE version (around 4.3k of the 30k), but that less than 0.5% of the 30k full MR-ref pairs from test exist in train.", "labels": [], "entities": []}, {"text": "der and juicy that it melted in your mouth\".", "labels": [], "entities": [{"text": "juicy", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9555283188819885}]}, {"text": "Row 3 shows an example of a negative sentiment target, which is achieved by both the +SENT and +STYLE models, with interesting descriptions such as \"the breakfast pizza was a joke\", and \"the pizza crust was a little on the bland side\".", "labels": [], "entities": [{"text": "SENT", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9944994449615479}, {"text": "STYLE", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9509152173995972}]}, {"text": "We show more +STYLE model outputs in Appendix C.  Machine Translation Metrics.", "labels": [], "entities": [{"text": "STYLE", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9795093536376953}, {"text": "Machine Translation Metrics", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6858483950297037}]}, {"text": "We begin with an automatic evaluation using standard metrics frequently used for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7430105805397034}]}, {"text": "We use the script provided by the E2E Generation Challenge 8 to compute scores for each of the 4 model test outputs compared to the original Yelp review sentences in the corresponding test set.", "labels": [], "entities": [{"text": "E2E Generation Challenge 8", "start_pos": 34, "end_pos": 60, "type": "DATASET", "confidence": 0.889652892947197}]}, {"text": "Rows 1-4 of summarize the results for BLEU (n-gram precision), METEOR (n-grams with synonym recall), CIDEr (weighted n-gram cosine similarity), and NIST (weighted n-gram precision), where higher numbers indicate better overlap (shown with the \u2191).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9986698627471924}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.87687087059021}, {"text": "METEOR", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9941469430923462}, {"text": "NIST", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.8036168217658997}, {"text": "overlap", "start_pos": 219, "end_pos": 226, "type": "METRIC", "confidence": 0.962780237197876}]}, {"text": "We note that while these measures are common for machine translation, they are not well-suited to this task, since they are based on ngram overlap which is not a constraint within the model; we include them for comparative purposes.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.76322141289711}]}, {"text": "From the table, we observe that across all metrics, we see a steady increase as more information is added.", "labels": [], "entities": []}, {"text": "Overall, the +STYLE model has the highest scores for all metrics, i.e. +STYLE model outputs are most lexically similar to the references.", "labels": [], "entities": []}, {"text": "The types of semantic errors the models make are more relevant than how well they conform to test references.", "labels": [], "entities": []}, {"text": "We calculate average Semantic Error Rate (SER), which is a function of the number of semantic mistakes the model makes (.", "labels": [], "entities": [{"text": "average Semantic Error Rate (SER)", "start_pos": 13, "end_pos": 46, "type": "METRIC", "confidence": 0.8482563112463269}]}, {"text": "We find counts of two types of common mistakes: deletions, where the model fails to realize a value from the input MR, and repetitions, where the model repeats the same value more than once.", "labels": [], "entities": [{"text": "repetitions", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.995142936706543}]}, {"text": "Thus, we compute SER per MR as SER = D+R N , where D and R are the number of deletions and repetitions, and the N is the number of tuples in the MR, and average across the test outputs.", "labels": [], "entities": [{"text": "SER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9951618313789368}, {"text": "SER", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9975265860557556}]}, {"text": "We compute stylistic metrics to compare the model outputs, with results shown in.", "labels": [], "entities": []}, {"text": "For vocab, we find the number of unique words in all outputs for each model.", "labels": [], "entities": []}, {"text": "We find the average sentence length (SentLen) by counting the number of words, and find the total number of times an adjective is used (Row 3) and average number of adjectives per reference for each model (Row 4).", "labels": [], "entities": [{"text": "SentLen)", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.7462339401245117}]}, {"text": "We compute Shannon text entropy (E) as: where V is the vocab size in all outputs generated by the model, f is the frequency of a term (in this case, a trigram), and t counts the number of terms in all outputs.", "labels": [], "entities": []}, {"text": "Finally, we count the instances of contrast (e.g. \"but\" and \"although\"), and aggregation (e.g. \"both\" and \"also\").", "labels": [], "entities": []}, {"text": "For all metrics, higher scores indicate more variability (indicated by \u2191).", "labels": [], "entities": []}, {"text": "From the table, we see that overall the vocabulary is large, even when compared to the training data for E2E and Laptop, as shown in.", "labels": [], "entities": [{"text": "vocabulary", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9712995290756226}]}, {"text": "First, we see that the simplest, least constrained BASE model has the largest vocabulary, since it has the most freedom in terms of word choice, while the model with the largest amount of supervision, +STYLE, has the smallest vocab, since we provide it with the most constraints on word choice.", "labels": [], "entities": [{"text": "STYLE", "start_pos": 202, "end_pos": 207, "type": "METRIC", "confidence": 0.764413595199585}]}, {"text": "For all other metrics, we see that the +STYLE  model scores highest: these results are especially interesting when considering that +STYLE has the smallest vocab; even though word choice is constrained with richer style markup, +STYLE is more descriptive on average (more adjectives used), and has the highest entropy (more diverse word collocations).", "labels": [], "entities": []}, {"text": "This is also very clear from the significantly higher number of contrast and aggregation operations in the +STYLE outputs.", "labels": [], "entities": []}, {"text": "Since our test set consists of 30k MRs, we are able to broadly characterize and quantify the kinds of sentence constructions we get for each set of model outputs.", "labels": [], "entities": []}, {"text": "To make generalized sentence templates, we delexicalize each reference in the model outputs, i.e. we replace any food item with a token, any service item with, etc.", "labels": [], "entities": []}, {"text": "Then, we find the total number of unique templates each model produces, finding that each \"more informed\" model produces more unique templates: BASE produces 18k, +ADJ produces 22k, +SENT produces 23k, and +STYLE produces 26k unique templates.", "labels": [], "entities": [{"text": "BASE", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9527076482772827}]}, {"text": "In other words, given the test set of 30k, +STYLE produces a novel templated output for over 86% of the input MRs.", "labels": [], "entities": [{"text": "STYLE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9720737934112549}]}, {"text": "While it is interesting to note that each \"more informed\" model produces more unique templates, we also want to characterize how frequently templates are reused.", "labels": [], "entities": []}, {"text": "shows the number of times each model repeats its top 20 most frequently used templates.", "labels": [], "entities": []}, {"text": "For example, the Rank 1 most frequently used template for the BASE model is \"I had the [FOOD].\", and it is used 550 times (out of the 30k outputs).", "labels": [], "entities": [{"text": "BASE", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.39643266797065735}, {"text": "FOOD", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9897298812866211}]}, {"text": "For +STYLE, the Rank 1 most frequently used template is \"I had the [FOOD] and it was delicious.\", and it is only used 130 times.", "labels": [], "entities": [{"text": "STYLE", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.5281694531440735}, {"text": "FOOD", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9971266388893127}]}, {"text": "The number of repetitions decreases as the template rank moves from 1 to 20, and repetition count is always significantly lower for +STYLE, indicating more variation.", "labels": [], "entities": [{"text": "repetitions", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9747626185417175}, {"text": "repetition count", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.9824987649917603}, {"text": "STYLE", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9504106044769287}]}, {"text": "Examples of frequent templates from the BASE and +STYLE models are are shown in Appendix B. Achieving Other Style Goals.", "labels": [], "entities": [{"text": "BASE", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9019666314125061}, {"text": "Appendix", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.8849889636039734}]}, {"text": "The +STYLE model is the only one with access to first-person, length, and exclamation markup, so we also measure its ability to hit these stylistic goals.", "labels": [], "entities": [{"text": "STYLE", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.9131604433059692}, {"text": "length", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9555707573890686}]}, {"text": "The average sentence length for the +STYLE model for LEN=SHORT is 7.06 words, LEN=MED is 13.08, and LEN=LONG is 22.74, closely matching the average lengths of the test references in those cases, i.e. 6.33, 11.05, and 19.03, respectively.", "labels": [], "entities": [{"text": "STYLE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9153783917427063}, {"text": "MED", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.502320408821106}, {"text": "LEN=LONG", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.7844156622886658}]}, {"text": "The model correctly hits the target 99% of the time for first person (it is asked to produce this for 15k of the 30k test instances), and 100% of the time for exclamation (2k instances require exclamation).", "labels": [], "entities": []}, {"text": "We evaluate output quality using human annotators on Mechanical Turk.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.8912728428840637}]}, {"text": "As in our corpus quality evaluation from Section 2.2, we randomly sample 200 MRs from the test set, along with the corresponding outputs for each of the 4 models, and ask 5 annotators to rate each output on a 1-5 Likert scale for content, fluency, and sentiment (1 for very negative, 5 for very positive 11 ).", "labels": [], "entities": []}, {"text": "shows the average scores by criteria and model.", "labels": [], "entities": []}, {"text": "For content and fluency, all average ratings are very high, above 4.3 (out of 5).", "labels": [], "entities": []}, {"text": "The differences between models are small, but it is interesting to note that the BASE and +STYLE models are almost tied on fluency (although BASE outputs may appear more fluent due to their comparably shorter length).", "labels": [], "entities": [{"text": "BASE", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.8417670130729675}, {"text": "STYLE", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.8692010641098022}]}, {"text": "In the case of sentiment error, the largest error is 0.75 (out of 3), with the smallest sentiment error (0.56) achieved by the +STYLE model.", "labels": [], "entities": [{"text": "sentiment error", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.8820600211620331}, {"text": "sentiment error", "start_pos": 88, "end_pos": 103, "type": "METRIC", "confidence": 0.858224481344223}]}, {"text": "Examination of the outputs reveals that the most common sentiment error is producing a neutral sentence when negative sentiment is specified.", "labels": [], "entities": []}, {"text": "This maybe due to the lower frequency of negative sentiment in the corpus as well as noise in automatic sentiment annotation.: Human quality evaluation (higher is better for content and fluency, lower is better for sentiment error).", "labels": [], "entities": []}, {"text": "Paired t-test for each model vs.+STYLE, * is p < 0.05.", "labels": [], "entities": [{"text": "STYLE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.8745911717414856}]}], "tableCaptions": [{"text": " Table 4: NLG corpus statistics from E2E (Novikova  et al., 2017a), LAPTOP (Wen et al., 2016), and  YELPNLG (this work).", "labels": [], "entities": [{"text": "LAPTOP", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9256899356842041}, {"text": "YELPNLG", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.7785397171974182}]}, {"text": " Table 5: Sample test MR and corresponding outputs for each model. Note that the MR presented is for +STYLE:  the other models all provide less information as described in Section 2.", "labels": [], "entities": [{"text": "MR", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.7426556944847107}, {"text": "MR", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9422546625137329}, {"text": "STYLE", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9955399632453918}]}, {"text": " Table 6: Automatic semantic evaluation (higher is bet- ter for all but SER).", "labels": [], "entities": [{"text": "semantic evaluation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.652436226606369}, {"text": "SER", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9878678321838379}]}, {"text": " Table 7: Automatic stylistic evaluation metrics (higher  is better). Paired t-test BASE vs. +STYLE all p < 0.05.", "labels": [], "entities": [{"text": "Paired t-test", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.810535728931427}, {"text": "BASE", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.7904680967330933}, {"text": "STYLE", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.9629281163215637}]}, {"text": " Table 8: Human quality evaluation (higher is better for  content and fluency, lower is better for sentiment error).  Paired t-test for each model vs.+STYLE, * is p < 0.05.", "labels": [], "entities": [{"text": "Paired t-test", "start_pos": 118, "end_pos": 131, "type": "METRIC", "confidence": 0.9040693640708923}, {"text": "STYLE", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.8986797332763672}]}, {"text": " Table 9: Sample of 10 \"most repeated\" templates from  BASE and +STYLE.", "labels": [], "entities": [{"text": "BASE", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.8950633406639099}, {"text": "STYLE", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.8735587000846863}]}]}