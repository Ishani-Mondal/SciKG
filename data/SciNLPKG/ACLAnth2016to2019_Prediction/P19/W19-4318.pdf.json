{"title": [{"text": "Probing Multilingual Sentence Representations With X-PROBE", "labels": [], "entities": [{"text": "Probing Multilingual Sentence Representations", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.49855489283800125}]}], "abstractContent": [{"text": "This paper extends the task of probing sentence representations for linguistic insight in a multilingual domain.", "labels": [], "entities": [{"text": "probing sentence representations", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.7078621685504913}, {"text": "linguistic insight", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.736160010099411}]}, {"text": "In doing so, we make two contributions: first, we provide datasets for multilingual probing, derived from Wikipedia, in five languages, viz.", "labels": [], "entities": []}, {"text": "English, French, German, Spanish and Russian.", "labels": [], "entities": []}, {"text": "Second , we evaluate six sentence encoders for each language, each trained by mapping sentence representations to English sentence representations , using sentences in a parallel corpus.", "labels": [], "entities": []}, {"text": "We discover that cross-lingually mapped representations are often better at retaining certain linguistic information than representations derived from English encoders trained on natural language inference (NLI) as a downstream task.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, there has been a considerable amount of research into attempting to represent contexts longer than single words with fixedlength vectors.", "labels": [], "entities": []}, {"text": "These representations typically tend to focus on attempting to represent sentences, although phrase-and paragraph-centric mechanisms do exist.", "labels": [], "entities": []}, {"text": "These have moved well beyond relatively na\u00a8\u0131vena\u00a8\u0131ve compositional methods, such as additive and multiplicative methods, one of the earlier papers on the subject.", "labels": [], "entities": []}, {"text": "There have been several proposed approaches to learning these representations since, both unsupervised and supervised.", "labels": [], "entities": []}, {"text": "Naturally, this has also sparked interest in evaluation methods for sentence representations; the focus of this paper is on probing-centric evaluations, and their extension to a multilingual domain.", "labels": [], "entities": [{"text": "sentence representations", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.7246294915676117}]}, {"text": "In Section 2, we provide a literature review of prior work in the numerous domains that our paper builds upon.", "labels": [], "entities": []}, {"text": "Section 3 motivates the principle of cross-lingual probing and describes our goals.", "labels": [], "entities": []}, {"text": "In Section 4, we describe our probing tasks and relevant modifications, if any.", "labels": [], "entities": []}, {"text": "Section 5 describes our sentence encoders, as well as the procedure we follow for training, mapping and probing.", "labels": [], "entities": []}, {"text": "Section 6 describes our data and relevant preprocessing methods we applied.", "labels": [], "entities": []}, {"text": "Section 7 presents a detailed evaluation from several perspectives, which we discuss in Section 8.", "labels": [], "entities": []}, {"text": "We conclude, as well as describe avenues for future work, in Section 9.", "labels": [], "entities": []}, {"text": "Our hyperparameters are described in Appendix A.1, and further detailed results that are not critical to the paper are tabulated in A.2.", "labels": [], "entities": [{"text": "Appendix A.1", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.6632521450519562}, {"text": "A.2", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.8938977122306824}]}], "datasetContent": [{"text": "Work on evaluating sentence representations was encouraged by the release of the SentEval toolkit (, which provided an easy-to-use framework that sentence representations could be 'plugged' into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks.", "labels": [], "entities": [{"text": "textual entailment and similarity", "start_pos": 291, "end_pos": 324, "type": "TASK", "confidence": 0.7107150331139565}, {"text": "paraphrase detection task", "start_pos": 334, "end_pos": 359, "type": "TASK", "confidence": 0.8390414118766785}, {"text": "caption/image retrieval", "start_pos": 365, "end_pos": 388, "type": "TASK", "confidence": 0.7644152343273163}]}, {"text": "(2018a) also created a set of 'probing tasks', a variant on the theme of diagnostic classification (, that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations.", "labels": [], "entities": [{"text": "diagnostic classification", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7360844612121582}]}, {"text": "The authors, whose work focussed on evaluating representations for English, provided Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Along similar lines, and contemporaneously with this work, probe three pretrained contextualised word representation models -ELMo (), BERT) and the OpenAI transformer () -with a \"suite of sixteen diverse probing tasks\".", "labels": [], "entities": [{"text": "BERT", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9962323307991028}]}, {"text": "On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated probing corpora.", "labels": [], "entities": []}, {"text": "They motivate the use of SVCCA ( to quantify precisely what an encoder learns by comparing the representations it generates with representations generated by an architecture trained specifically fora certain task, with the intuition that a higher similarity between the representations generated by the generic encoder and the specialised representations would indicate that the encoder is capable of encapsulating more taskrelevant information.", "labels": [], "entities": []}, {"text": "Their method has numerous advantages over traditional diagnostic classification, such as the elimination of the classifier, which reduces the risk of an additional component obfuscating results.", "labels": [], "entities": [{"text": "diagnostic classification", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.9474065601825714}]}, {"text": "A visible limitation of the datasets provided by these probing tasks is that most of them were created with the idea of evaluating representations built for English language data.", "labels": [], "entities": []}, {"text": "In this spirit, what we propose is analogous to work on generating multilingual evaluation corpora for word representations.", "labels": [], "entities": []}, {"text": "Within the realm of evaluating multilingual sentence representations, describe the XNLI dataset, a set of translations of the development and test portions of the multi-genre MultiNLI inference dataset (.", "labels": [], "entities": [{"text": "XNLI dataset", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8539140522480011}]}, {"text": "This, in a sense, is an extension of a predominantly monolingual task to the multilingual domain; the authors evaluate sentence representations derived by mapping nonEnglish representations to an English representation space.", "labels": [], "entities": []}, {"text": "The original XNLI paper provides a baseline representation mapping technique, based on minimising the mean-squared error (MSE) loss between sentence representations across a parallel corpus.", "labels": [], "entities": [{"text": "representation mapping", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.6751758456230164}, {"text": "mean-squared error (MSE) loss", "start_pos": 102, "end_pos": 131, "type": "METRIC", "confidence": 0.9451674520969391}]}, {"text": "Their English language sentence representations are derived from an encoder trained on NLI data (, and their target language representations are randomly initialised fora parallel sentence.", "labels": [], "entities": [{"text": "NLI data", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.832962840795517}]}, {"text": "While this system does perform reasonably well, a more naive machinetranslation based approach performs better.", "labels": [], "entities": []}, {"text": "The focus of this paper is twofold.", "labels": [], "entities": []}, {"text": "First, we provide five datasets for probing mapped sentence representations, in five languages (including an additional dataset for English), drawn from a different domain to Conneau et al.'s probing dataset: specifically, from Wikipedia.", "labels": [], "entities": []}, {"text": "Second, we probe a selection of mapped sentence representations, in an attempt to answer precisely what linguistic features are retained, and to what extent, post mapping.", "labels": [], "entities": []}, {"text": "The emphasis of this evaluation is therefore, crucially, not a probing-oriented analysis of representations trained on different languages, but an analysis of the effects of MSE-based mapping procedures on the ability of sentence representations to retain linguistic features.", "labels": [], "entities": [{"text": "MSE-based mapping", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.9016236662864685}]}, {"text": "In this sense, our focus is lesson the correlation between probing performance and downstream performance, and more on the relative performance of our representations on probing tasks.", "labels": [], "entities": []}, {"text": "Despite having described (in Section 2) numerous methods, both for learning monolingual sentence representations, and for mapping them cross-linguistically, we restrict our work to a smaller subset of these.", "labels": [], "entities": []}, {"text": "Specifically, we evaluate six encoders, each trained in a supervised fashion on NLI data.", "labels": [], "entities": []}, {"text": "Whilst our choice of languages could have been more typologically diverse, we were restricted by three factors: 1.", "labels": [], "entities": []}, {"text": "the availability of a parallel corpus with English for our mapping procedure 2.", "labels": [], "entities": []}, {"text": "the availability of a large enough Wikipedia to allow us to extract sufficient data (for instance, the Arabic Wikipedia was not large enough to fully extract data for all our tasks) 3.", "labels": [], "entities": []}, {"text": "the inclusion of the language in XNLI.", "labels": [], "entities": [{"text": "XNLI", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8380023241043091}]}, {"text": "Despite not being necessary, we believed it would be interesting to have a 'real' downstream task to compare to.", "labels": [], "entities": []}, {"text": "As a preface to this section, we reiterate that the goal of this work was not to attempt to reach stateof-the-art on the tasks we describe; our goal was primarily to study the effect of transfer on sentence representations.", "labels": [], "entities": []}, {"text": "Our first step during evaluation, therefore, was to probe all our encoders using Conneau et al.'s original probing corpus, and compare these results to our English-language results on our Wikipediagenerated corpus.", "labels": [], "entities": [{"text": "Conneau et al.'s original probing corpus", "start_pos": 81, "end_pos": 121, "type": "DATASET", "confidence": 0.7557508945465088}, {"text": "Wikipediagenerated corpus", "start_pos": 188, "end_pos": 213, "type": "DATASET", "confidence": 0.8941182196140289}]}, {"text": "We present these results in the form of a heatmap in.", "labels": [], "entities": []}, {"text": "Similarities between results on our corpora are instantly visible; these also appear to hold across encoders.", "labels": [], "entities": []}, {"text": "Tasks with minor visible differences include WC, the most 'difficult' classification task (1k classes), and TreeDepth, where we use dependency tree depth instead of constituency tree depth, as well as a different sampling mechanism.", "labels": [], "entities": []}, {"text": "Next, we present Spearman correlations between the performance of our encoders on probing tasks and on the only 'true' cross-lingual downstream task we evaluated our systems on: cross-lingual natural language inference, via the XNLI () corpus.", "labels": [], "entities": [{"text": "XNLI () corpus", "start_pos": 228, "end_pos": 242, "type": "DATASET", "confidence": 0.8108321030934652}]}, {"text": "A caveat here is that we make no claims about the statistical significance of these results; given just six data points per language per task, our p-values tend to be well below acceptable for statistical significance.", "labels": [], "entities": []}, {"text": "We refer the reader to Conneau et al.'s original probing work, where despite having results for 30 encoders, correlations between many downstream and probing tasks were not statisti- cally significant.", "labels": [], "entities": []}, {"text": "Our correlations are presented, again in the form of a heatmap, in.", "labels": [], "entities": []}, {"text": "Our absolute results on XNLI are presented in the appendix.", "labels": [], "entities": [{"text": "XNLI", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.6761404275894165}]}, {"text": "These are not a focus for this work: we did not attempt to obtain state-of-the-art, nor, indeed, perform any sort of hyperparameter optimisation to get the 'best' possible results.", "labels": [], "entities": []}, {"text": "Given these caveats, we draw the reader's attention to the fact that the overwhelming majority of correlations are negative.", "labels": [], "entities": []}, {"text": "Finally, and most importantly, we measure downstream performance on probing tasks for all our cross-lingually mapped encoders.", "labels": [], "entities": []}, {"text": "For visualisation relevant to our goals, and for brevity, we present these results, in, as a heatmap of probing results relative to (our) English probing results; a full table with numeric results is presented in Appendix A.2.", "labels": [], "entities": [{"text": "Appendix A.2", "start_pos": 213, "end_pos": 225, "type": "DATASET", "confidence": 0.8897822499275208}]}], "tableCaptions": [{"text": " Table 1: Mean and standard deviations for the absolute  performance for each probing task, across languages  and encoders", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9845219850540161}]}, {"text": " Table 2: Hyperparameters, divided by the 'component'  that each layer belongs to. Note that biRNN dims are  per direction.", "labels": [], "entities": []}, {"text": " Table 3: Language-specific results on relevant XNLI  splits for each encoder", "labels": [], "entities": []}, {"text": " Table 4: Complete set of absolute results per probing task, per encoder, per language. For English, these numbers  are for unmapped, NLI-based encoders; for all other languages, these are post-mapping numbers", "labels": [], "entities": []}]}