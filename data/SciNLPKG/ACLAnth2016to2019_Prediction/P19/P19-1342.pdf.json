{"title": [{"text": "Tree Communication Models for Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9793137907981873}]}], "abstractContent": [{"text": "Tree-LSTMs have been used for tree-based sentiment analysis over Stanford Sentiment Treebank, which allows the sentiment signals over hierarchical phrase structures to be calculated simultaneously.", "labels": [], "entities": [{"text": "tree-based sentiment analysis", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7108386357625326}, {"text": "Stanford Sentiment Treebank", "start_pos": 65, "end_pos": 92, "type": "DATASET", "confidence": 0.8668640454610189}]}, {"text": "However, traditional tree-LSTMs capture only the bottom-up dependencies between constituents.", "labels": [], "entities": []}, {"text": "In this paper , we propose a tree communication model using graph convolutional neural network and graph recurrent neural network, which allows rich information exchange between phrases constituent tree.", "labels": [], "entities": []}, {"text": "Experiments show that our model outperforms existing work on bidirec-tional tree-LSTMs in both accuracy and efficiency , providing more consistent predictions on phrase-level sentiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9994319081306458}]}], "introductionContent": [{"text": "There has been increasing research interest investigating sentiment classification over hierarchical phrases.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.9416332542896271}]}, {"text": "As shown in, the goal is to predict the sentiment class over a sentence and each phrase in its constituent tree.", "labels": [], "entities": []}, {"text": "There have been methods that classify each phrase independently ().", "labels": [], "entities": []}, {"text": "However, sentiments over hierarchical phrases can have dependencies.", "labels": [], "entities": []}, {"text": "For example, in, both sentences have a phrase \"an awesome day\", but the polarities of which are different according to their sentence level contexts.", "labels": [], "entities": []}, {"text": "To better represent such sentiment dependencies, one can encode a constituency tree holistically using a neural encoder.", "labels": [], "entities": []}, {"text": "To this end, treestructured LSTMs have been investigated as a dominant approach).", "labels": [], "entities": []}, {"text": "Such methods work by encoding hierarchical phrases bottom-up, so that sub constituents can be used as inputs for representing a constituent.", "labels": [], "entities": []}, {"text": "However, they cannot pass information from a constituent node to its children, which can be necessary for cases similar to.", "labels": [], "entities": []}, {"text": "In this example, sentence level information from toplevel nodes is useful for disambiguating \"an awesome day\".", "labels": [], "entities": []}, {"text": "Bi-directional tree LSTMs provide a solution, using a separate top-down LSTM to augment a tree-LSTM (.", "labels": [], "entities": []}, {"text": "This method has achieved highly competitive accuracies, at the cost of doubling the runtime.", "labels": [], "entities": []}, {"text": "Intuitively, information exchange between tree nodes can happen beyond bottom-up and topdown directions.", "labels": [], "entities": []}, {"text": "For example, direct communication between sibling nodes, such as (\"an awesome day\", \"winning the game\") and (\"an awesome day\", \"experiencing the tsunami\") can also bring benefits to tree representation.", "labels": [], "entities": []}, {"text": "Recent advances of graph neural networks, such as graph convolutional neural network (GCN) ( and graph recurrent neural network (GRN) offer rich node communication patterns over graphs.", "labels": [], "entities": []}, {"text": "For relation extraction, for example, GCNs have been shown superior to tree LSTMs for encoding a dependency tree ( We investigate both GCNs and GRNs as tree communication models for tree sentiment classification.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9577561318874359}, {"text": "tree sentiment classification", "start_pos": 182, "end_pos": 211, "type": "TASK", "confidence": 0.7238936722278595}]}, {"text": "In particular, initialized with a vanilla tree LSTM representation, each node repeatedly exchanges information with its neighbours using graph neural networks.", "labels": [], "entities": []}, {"text": "Such multi-pass information exchange can allow each node to be more informed about its sentence-level context through rich communication patterns.", "labels": [], "entities": []}, {"text": "In addition, the number of time steps does not scale with the height of the tree.", "labels": [], "entities": []}, {"text": "To allow better interaction, we further propose a novel time-wise attention mechanism over GRN, which summarizes the representation after each communication step.", "labels": [], "entities": []}, {"text": "Experiments on Stanford Sentiment Treebank (SST;) show that our model outperforms standard bottom-up tree-LSTM ( and also recent work on bidirectional tree-LSTM (.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank (SST", "start_pos": 15, "end_pos": 47, "type": "DATASET", "confidence": 0.8357396721839905}]}, {"text": "In addition, our model allows a more holistic prediction of phase-level sentiments over the tree with a high degree of node sentiment consistency.", "labels": [], "entities": []}, {"text": "To our knowledge, we are the first to investigate graph NNs for tree sentiment classification, and the first to discuss phrase level sentiment consistency over a constituent tree for SST.", "labels": [], "entities": [{"text": "tree sentiment classification", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.8162648479143778}, {"text": "SST", "start_pos": 183, "end_pos": 186, "type": "TASK", "confidence": 0.9769573211669922}]}, {"text": "We release our code and models at https://github.com/fred2008/TCMSA.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the effectiveness of TCM by comparing its performance with a standard tree-LSTM () as well as a state-of-the-art bidirectional tree-LSTM (.", "labels": [], "entities": []}, {"text": "A series of analysis is conducted for measuring the holistic representation of sentiment in a tree via phrase-level sentiments consistency.", "labels": [], "entities": []}, {"text": "Hyper-parameters We initialize word embeddings using GloVe () 300-dimensional embeddings.", "labels": [], "entities": []}, {"text": "Embeddings are finetuned during training.", "labels": [], "entities": []}, {"text": "The size of LSTM hidden states are set to 300.", "labels": [], "entities": []}, {"text": "We thus fix the number to 9.", "labels": [], "entities": []}, {"text": "Training In order to obtain a good representation for an initial constituent state, we first train an independent bottom-up tree-LSTM, over which we train our tree communication models.", "labels": [], "entities": []}, {"text": "To avoid over-fitting, we adopt dropout on the embedding layer, with a rate of 0.5.", "labels": [], "entities": []}, {"text": "Training is done on minibatches through Adagrad () with a learning rate of 0.05.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9733297824859619}]}, {"text": "We adopt gradient clipping with a threshold of 1.0.", "labels": [], "entities": []}, {"text": "The L2 regularization parameter is set to 0.001.", "labels": [], "entities": []}, {"text": "Hyper-parameters We investigate the effect of recurrent steps of RTCM as shown in Block A of.", "labels": [], "entities": []}, {"text": "As the number of steps increases from 1, the accuracy increases, showing the effectiveness of tree node communication.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9997263550758362}, {"text": "tree node communication", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.6696812113126119}]}, {"text": "A recurrent step of 9 gives the best accuracies, and a larger number of steps does not give further improvements.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.990919828414917}]}, {"text": "This is consistent with observations of, which shows that sufficient context information can be collected over a small number of iterations.", "labels": [], "entities": []}, {"text": "The effectiveness of TCM Block B in Block Model SST-5 SST-2 A  shows the performance of different models.", "labels": [], "entities": [{"text": "SST-5 SST-2", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.6996962130069733}]}, {"text": "TreeLSTMs with different TCMs outperform the baseline tree-LSTM on both datasets.", "labels": [], "entities": []}, {"text": "In addition, the time-wise attention mechanism in Section 4.2.1 improves performance on both SST-5 and SST-2.", "labels": [], "entities": []}, {"text": "In the remaining experiments, we use RTCM with time wise-attention.", "labels": [], "entities": [{"text": "RTCM", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.7726079225540161}]}, {"text": "shows the overall performances for sentiment classification on both SST-5 and SST-2.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8534573912620544}, {"text": "SST-5", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.6733570098876953}, {"text": "SST-2", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.6489853858947754}]}, {"text": "We report accuracies on both the sentence level and the phrase level.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.982050895690918}]}, {"text": "Compared with previous methods based on constituent tree-LSTM, our model improves the preformance on different datasets and settings.", "labels": [], "entities": []}, {"text": "In particular, it outperforms BiConL-STM (, which use bidirectional tree-LSTM.", "labels": [], "entities": []}, {"text": "This demostrates the advantage of graph neural networks compared to a top-down LSTM for tree communication.", "labels": [], "entities": []}, {"text": "Our model gives the state-of-the-art accuracies on phrase-level settings.", "labels": [], "entities": []}, {"text": "Note that we do not leverage character representation or external resources such as sentiment lexicons and large-scale corpuses.", "labels": [], "entities": []}, {"text": "There has also been work using large-scale external datasets to improve performance.", "labels": [], "entities": []}, {"text": "pretrain their model on large parallel bilingual datasets and exploit character ngram features.", "labels": [], "entities": []}, {"text": "They report an accuracy of 53.7 on sentence-level SST-5 and an accuracy of 90.3 on sentence-level SST-2, which are lower than our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9995193481445312}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9995055198669434}]}, {"text": "pretrain a language model with character convolutions on a large-scale corpus and report an accuracy of 54.7 on sentencelevel SST-5, which is slightly higher than our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9994638562202454}]}, {"text": "Large-scale pretraining is orthogonal to our method.", "labels": [], "entities": []}, {"text": "For a fair comparison, we do not list their results on   spect to different sentence lengths.", "labels": [], "entities": []}, {"text": "On both datasets, the performance of tree-LSTM on sentences of lengths less than 10 (l = 5 in the is much better than that of longer sentences.", "labels": [], "entities": []}, {"text": "There is a tendency of decreasing accuracies as the sentence length increases.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.981634795665741}]}, {"text": "As the length of sentences increases, there are longerrange dependencies along the depth of tree structure, which is more difficult to model than short sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Phrase level performances on the dev set.", "labels": [], "entities": []}, {"text": " Table 4: Rates of holistically-labeled sentences with  sentence-level phrase accuracy SPAcc \u03b1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.872577428817749}, {"text": "SPAcc \u03b1", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9578951597213745}]}, {"text": " Table 5: Deviation statistics. Values in units of \u00d710 \u22122", "labels": [], "entities": []}, {"text": " Table 6: Sentence-level phrase accuracy (SPAcc) and  phrase error deviation (PEDev) comparison on SST-5  between bi-tree-LSTM and TCM.", "labels": [], "entities": [{"text": "accuracy (SPAcc)", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.8831948637962341}, {"text": "phrase error deviation (PEDev)", "start_pos": 54, "end_pos": 84, "type": "METRIC", "confidence": 0.785686249534289}]}]}