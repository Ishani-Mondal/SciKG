{"title": [], "abstractContent": [{"text": "A lot of work has been done in the field of image compression via machine learning, but not much attention has been given to the compression of natural language.", "labels": [], "entities": [{"text": "image compression", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7601831555366516}]}, {"text": "Compressing text into lossless representations while making features easily retrievable is not a trivial task, yet has huge benefits.", "labels": [], "entities": []}, {"text": "Most methods designed to produce feature rich sentence embeddings focus solely on performing well on downstream tasks and are unable to properly reconstruct the original sequence from the learned embedding.", "labels": [], "entities": []}, {"text": "In this work, we propose a near lossless method for encoding long sequences of texts as well as all of their sub-sequences into feature rich representations.", "labels": [], "entities": []}, {"text": "We test our method on sentiment analysis and show good performance across all sub-sentence and sentence embeddings.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9649100601673126}]}], "introductionContent": [{"text": "Compressing information by encoding it into a fixed size representation in such away that perfect decoding is possible is challenging.", "labels": [], "entities": []}, {"text": "Instead, most of the existing sentence encoding methods focus more on learning encoding such that the encoded representations are good enough for the downstream tasks.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.719050794839859}]}, {"text": "In this work, we focus on perfectly decodable encoding of sentences which will be very useful in designing good generative models that can generate longer sentences.", "labels": [], "entities": []}, {"text": "Early efforts such as) have shown autoencoders to effectively yield compressed input representations. was the first to propose using autoencoders recursively.", "labels": [], "entities": []}, {"text": "Such models have been shown to be useful fora multitude of tasks.", "labels": [], "entities": []}, {"text": "use recursive neural networks and neural language models to better represent rare words * Corresponding author: gabriele.prato@umontreal.ca via morphemes.", "labels": [], "entities": []}, {"text": "use recursive autoencoders for paraphrase detection, learning sentence embeddings and syntactic parsing.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.8745085299015045}, {"text": "syntactic parsing", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7230353653430939}]}, {"text": "also use a recursive autoencoder to build a tree structure based on error reconstruction.", "labels": [], "entities": []}, {"text": "Additionally, use a matrix-vector RNN to learn semantic relationships present in natural language and show good performance on such task as well as sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.9440146386623383}]}, {"text": "Then,  introduced the Recursive Neural Tensor Network, trained on a their proposed Sentiment Treebank corpus to better deal with negating sub-sequences for better sentiment classification.", "labels": [], "entities": [{"text": "Recursive Neural Tensor", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6815481384595236}, {"text": "Sentiment Treebank corpus", "start_pos": 83, "end_pos": 108, "type": "DATASET", "confidence": 0.8022224307060242}, {"text": "sentiment classification", "start_pos": 163, "end_pos": 187, "type": "TASK", "confidence": 0.8886556923389435}]}, {"text": "Recently, proposed Structural Attention to build syntactic trees and improve even further performance on SST.", "labels": [], "entities": [{"text": "Structural Attention", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.9435657858848572}, {"text": "SST", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.8979824185371399}]}, {"text": "Parse trees do alleviate the burden of learning the syntactic structure of text, but these methods limit the number of generated embeddings to the number of nodes in the parse tree.", "labels": [], "entities": []}, {"text": "Our proposed method does not have such a restriction as all possible syntactic tree can be simultaneously represented by the architecture.", "labels": [], "entities": []}, {"text": "Convolutional Neural Networks ( have been used in natural language processing as well.", "labels": [], "entities": []}, {"text": "Convolutions work well for extracting low and high level text features and building sequence representations.", "labels": [], "entities": []}, {"text": "proposed to use CNNs recurrently and show good performance on various language tasks.; Dos Santos and Gatti de Bayser (2014) both train CNNs on character level for sentiment analysis, while Johnson and Zhang (2014) work on word level.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.9570858478546143}]}, {"text": "propose a Dynamic Convolutional Neural Network for semantic modelling of sentences and apply their model to sentiment prediction.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.9477704763412476}]}, {"text": "Our proposed model is very similar to 1D CNNs.", "labels": [], "entities": [{"text": "1D CNNs", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.6138349622488022}]}, {"text": "In our case though, we use a multilayer perceptron in parallel instead of a kernel to extract meaningful information out of the layer's input.", "labels": [], "entities": []}, {"text": "Much progress has been made in recent years in the field of general purpose sentence embeddings.) to produce sentence wide context embeddings for each input token and get state-of-the-art results on multiple natural language processing tasks.", "labels": [], "entities": [{"text": "general purpose sentence embeddings.", "start_pos": 60, "end_pos": 96, "type": "TASK", "confidence": 0.6858611851930618}]}, {"text": "improve the Transformer method by recursively applying it to fixed length segments of text while using a hidden state to model long dependencies.", "labels": [], "entities": []}, {"text": "One downside to these sentence embedding generation methods is that the context is always sequence wide.", "labels": [], "entities": [{"text": "sentence embedding generation", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.67755855123202}]}, {"text": "Our proposed model computes a sentence embedding as well as an embedding for all possible sub-sentences of the sequence with sub-sentence wide context only.", "labels": [], "entities": []}, {"text": "All embeddings generated throughout our architecture are constructed the same way and thus share the same properties.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first present the autoencoding results.", "labels": [], "entities": []}, {"text": "Then we present the results on sentiment analysis using our sentence encoding on the Stanford Sentiment Treebank dataset ).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9687086343765259}, {"text": "Stanford Sentiment Treebank dataset", "start_pos": 85, "end_pos": 120, "type": "DATASET", "confidence": 0.9451255947351456}]}], "tableCaptions": [{"text": " Table 1: Mean squared error loss of stacked LSTMs  and our RAE model for different embedding sizes. All  models are trained on the autoencoding task for 20  epochs and models of same embedding size have the  same capacity. MSE is computed on the BookCorpus  dev set (", "labels": [], "entities": [{"text": "Mean squared error loss", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8800667375326157}, {"text": "BookCorpus  dev set", "start_pos": 247, "end_pos": 266, "type": "DATASET", "confidence": 0.9519221385320028}]}, {"text": " Table 2: SST-5 and SST-2 performance on all and root  nodes respectively. Model results in the first section are  from the Stanford Treebank paper (2013). GenSen and  BERT BASE results are from (Subramanian et al., 2018)  and (Devlin et al., 2018) respectively.", "labels": [], "entities": [{"text": "SST-5", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.960010826587677}, {"text": "Stanford Treebank paper (2013)", "start_pos": 124, "end_pos": 154, "type": "DATASET", "confidence": 0.9378120303153992}, {"text": "BERT", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9971357583999634}, {"text": "BASE", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.6296882629394531}]}]}