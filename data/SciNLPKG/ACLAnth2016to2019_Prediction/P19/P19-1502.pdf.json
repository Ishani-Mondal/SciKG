{"title": [{"text": "Studying Summarization Evaluation Metrics in the Appropriate Scoring Range", "labels": [], "entities": [{"text": "Summarization Evaluation Metrics", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.8973285555839539}, {"text": "Appropriate", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9544795751571655}]}], "abstractContent": [{"text": "In summarization, automatic evaluation met-rics are usually compared based on their ability to correlate with human judgments.", "labels": [], "entities": []}, {"text": "Unfortunately , the few existing human judgment datasets have been created as by-products of the manual evaluations performed during the DUC/TAC shared tasks.", "labels": [], "entities": [{"text": "DUC/TAC shared tasks", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.6179366171360016}]}, {"text": "However, modern systems are typically better than the best systems submitted at the time of these shared tasks.", "labels": [], "entities": []}, {"text": "We show that, surprisingly, evaluation metrics which behave similarly on these datasets (average-scoring range) strongly disagree in the higher-scoring range in which current systems now operate.", "labels": [], "entities": []}, {"text": "It is problematic because metrics disagree yet we can't decide which one to trust.", "labels": [], "entities": []}, {"text": "This is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust.", "labels": [], "entities": []}, {"text": "This would also be greatly beneficial to further improve summa-rization systems and metrics alike.", "labels": [], "entities": []}], "introductionContent": [{"text": "The progress in summarization is tightly intertwined with the capability to quickly measure improvements.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9840102791786194}]}, {"text": "Thus, a significant body of research was dedicated to the development of automatic metrics).", "labels": [], "entities": []}, {"text": "Yet, this remains an open problem (.", "labels": [], "entities": []}, {"text": "Typically, evaluation metrics are compared based on their ability to correlate with humans (.", "labels": [], "entities": []}, {"text": "Then, the selected metrics heavily influence summarization research by guiding progress and by providing supervision for training summarization systems (.", "labels": [], "entities": [{"text": "summarization research", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.9144668579101562}]}, {"text": "Despite their central role, few human judgment datasets have been created.", "labels": [], "entities": []}, {"text": "The existing ones are the result of the manual evaluations performed * Research partly done at UKP Lab from TU Darmstadt: The blue distribution represents the score distribution of summaries available in the human judgment datasets of.", "labels": [], "entities": [{"text": "UKP Lab", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9552757441997528}]}, {"text": "The red distribution is the score distribution of summaries generated by mordern systems.", "labels": [], "entities": []}, {"text": "The green distribution corresponds to the score distribution of summaries we generated in this work as described in section 3.", "labels": [], "entities": []}, {"text": "Thus, the annotated summaries are mostly average compared to nowadays standards.", "labels": [], "entities": []}, {"text": "Indeed, the best systems submitted at the time of these sharedtasks have typically served as baselines for subsequent works.", "labels": [], "entities": []}, {"text": "This is illustrated by, which compares the score distribution of summaries inhuman judgment datasets with the score distribution of modern summarization systems.", "labels": [], "entities": [{"text": "summaries inhuman judgment", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.8360989292462667}]}, {"text": "The score distribution on which evaluation metrics are tested (blue zone) differs from the one in which they now operate (red zone).", "labels": [], "entities": []}, {"text": "Thus, there is no guarantee that evaluation metrics behave according to human judgments in the high-scoring range.", "labels": [], "entities": []}, {"text": "Yet, summarization systems explicitly target highscoring summaries (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.9852956533432007}]}, {"text": "In this work, we study several evaluation metrics in this high-scoring range based on an automatically generated dataset.", "labels": [], "entities": []}, {"text": "We show that, even though current evaluation metrics correlate well with each other in the average range, they strongly disagree for high-scoring summaries.", "labels": [], "entities": []}, {"text": "This is related to the Simpson paradox, where different conclusions are drawn depending on which slice of the population is considered.", "labels": [], "entities": []}, {"text": "This is problematic because current metrics cannot be distinguished based solely on an analysis of available human judgments.", "labels": [], "entities": []}, {"text": "Nevertheless, they will promote very different summaries and systems.", "labels": [], "entities": [{"text": "summaries", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.9720003604888916}]}, {"text": "These results call for the gathering of human judgments in the high-scoring range.", "labels": [], "entities": [{"text": "gathering of human judgments", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8376738131046295}]}, {"text": "We provide data and code to reproduce our experiments.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pairwise correlation (Kendall's \u03c4 ) between  evaluation metrics on various scoring range. (T) is the  high-scoring range, (A) is the average-scoring range  (human judgment datasets) and (W) is the whole scor- ing range", "labels": [], "entities": [{"text": "Pairwise correlation (Kendall's \u03c4 )", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.6537983247212001}]}, {"text": " Table 2: Correlation of automatic metrics with human  judgments for TAC-2008 and TAC-2009. The correla- tion is measured with Kendall's \u03c4 .", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.8089415431022644}, {"text": "TAC-2009", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.8862546682357788}]}]}