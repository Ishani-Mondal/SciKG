{"title": [{"text": "A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction", "labels": [], "entities": [{"text": "Sentiment Lexicon Induction", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7350853482882181}]}], "abstractContent": [{"text": "We present anew method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world's languages.", "labels": [], "entities": [{"text": "sentiment lexicon induction", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.9711773991584778}]}, {"text": "We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages.", "labels": [], "entities": [{"text": "Bible Corpus+ (PBC+)", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.8932575682799021}]}, {"text": "The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual em-beddings.", "labels": [], "entities": []}, {"text": "Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+.", "labels": [], "entities": []}, {"text": "Through domain adaptation, we then generalize the domain-specific lexicon to a general one.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7019460499286652}]}, {"text": "We show-across typologically diverse languages in PBC+-good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation.", "labels": [], "entities": []}, {"text": "We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexicons play an important role in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9754837155342102}]}, {"text": "Sentiment lexicons are available for highresource languages like English (), but not for many low-resource languages.", "labels": [], "entities": []}, {"text": "Researchers are trying to fill this gap by inducing lexicons monolingually () as well as multilingually, often by transfer from high-resource to low-resource languages.", "labels": [], "entities": []}, {"text": "The world's languages are heterogeneous -of particular relevance for us is heterogeneity with respect to morphology and with respect to marking token boundaries.", "labels": [], "entities": []}, {"text": "This heterogeneity poses difficulties when designing a universal approach cistern.cis.lmu.de to lexicon induction that works for all languages -implementing a high quality tokenizer and morphological analyzer for each language is not feasible short-term.", "labels": [], "entities": []}, {"text": "Given the small number of native speakers in low-resource languages), crowdsourcing cannot easily be carried out either.", "labels": [], "entities": []}, {"text": "To overcome this heterogeneity and provide sentiment resources for low-resource languages, we present anew approach to sentiment lexicon induction that is universal -that is, it is applicable to the full range of typologically different languages -and apply it to 1593 languages.", "labels": [], "entities": [{"text": "sentiment lexicon induction", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.8511572480201721}]}, {"text": "Our method first takes a parallel corpus as input and applies BPE) segmentation to it.", "labels": [], "entities": [{"text": "BPE) segmentation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7827247778574625}]}, {"text": "We then create a multilingual BPE embedding space, from which a ZS (zero-shot) lexicon for each language L is extracted by zero-shot transfer from English sentiment to L.", "labels": [], "entities": []}, {"text": "We use PBC+, an expansion of the Parallel Bible Corpus), as our parallel corpus.", "labels": [], "entities": [{"text": "Parallel Bible Corpus", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.6474609871705373}]}, {"text": "The ZS lexicons show high quality, but are specific to the domain of PBC+ (the Bible).", "labels": [], "entities": []}, {"text": "We then adapt them to the general domain.", "labels": [], "entities": []}, {"text": "For brevity, we also use generic to refer to general-domain.", "labels": [], "entities": []}, {"text": "Our method is universal and language-agnostic -it does not require language-dependent preprocessing.", "labels": [], "entities": []}, {"text": "We carryout intrinsic and extrinsic, automatic and human evaluations on 95 languages.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation shows that our approach produces word ratings that strongly correlate with gold standard lexicons and human judgments.", "labels": [], "entities": []}, {"text": "Extrinsic evaluation on Twitter sentiment classification demonstrates that our lexicons perform comparably or better than existing lexicons derived in multilingual settings.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.6789070169130961}]}, {"text": "We chose an approach to sentiment analysis based on lexicons in this paper because it is transparent and meets high standards of explainability.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9691808521747589}]}, {"text": "A classification decision can easily be traced back to the lexicon entries in the document that are responsible.", "labels": [], "entities": [{"text": "classification decision", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.8998419940471649}]}, {"text": "Many more complex methods, e.g., many deep learning approaches, do not meet this standard.", "labels": [], "entities": []}, {"text": "Transparency is of particular importance for low-resource languages because error analysis and verification are paramount when working with small and noisy resources that are typical of lowresource languages.", "labels": [], "entities": []}, {"text": "Our contributions: (i) We propose anew method for inducing sentiment lexicons fora broad range of typologically diverse languages.", "labels": [], "entities": []}, {"text": "We use BPEs as basic units and show that they work well across languages.", "labels": [], "entities": []}, {"text": "(ii) We carryout extensive evaluation to confirm correctness and high quality of the created lexicons.", "labels": [], "entities": []}, {"text": "(iii) We make our code, the 1593 ZS seed sentiment lexicons and 200 generic sentiment lexicons freely available to the community.", "labels": [], "entities": [{"text": "1593 ZS seed sentiment lexicons", "start_pos": 28, "end_pos": 59, "type": "DATASET", "confidence": 0.8225185751914978}]}, {"text": "This is the up-to-now largest sentiment resource in terms of language coverage that has been published.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the 7958 New Testament verses in PBC+ that were also used by to create the multilingual BPE embedding space.", "labels": [], "entities": []}, {"text": "To cover as many BPEs as we can, we segment each PBC+ edition three times with vocabulary sizes 2000, 4000 and 8000 using sentencepiece.", "labels": [], "entities": [{"text": "PBC+ edition", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.7957949241002401}]}, {"text": "S-ID generates a 31GB embedding training corpus including 7,414,810 BPEs in 1593 languages.", "labels": [], "entities": []}, {"text": "We employ VADER, a simple but widely used rule-based model for general sentiment analysis, to create sentiment labels for English BPEs.", "labels": [], "entities": [{"text": "VADER", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9920125603675842}, {"text": "general sentiment analysis", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7857637206713358}]}, {"text": "We consider BPEs with sentiment score +0.1 (resp. -0.1) as positive (resp. negative).", "labels": [], "entities": [{"text": "sentiment score +0.1", "start_pos": 22, "end_pos": 42, "type": "METRIC", "confidence": 0.8801237940788269}]}, {"text": "BPEs with score 0 are treated as neutral.", "labels": [], "entities": [{"text": "BPEs", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5631070733070374}]}, {"text": "As a result, we have 851 positive, 906 negative and 13,861 neutral training BPEs in English.", "labels": [], "entities": []}, {"text": "We uniformly sample 878 = floor((851 + 906)/2) neutral BPEs to speedup training.", "labels": [], "entities": []}, {"text": "The two SVMs for POS and NEG ( \u00a73.3) are trained on English training set (see above), then applied to all vectors in the multilingual BPE embedding space to create ZS lexicons for 1593 languages.", "labels": [], "entities": []}, {"text": "We only keep highconfidence BPEs -those with a predicted probability for either POS or NEG of \u2265 0.7 (Platt et al.,).", "labels": [], "entities": [{"text": "POS", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.8714767694473267}]}, {"text": "We always compute F1 on the intersection of our and gold lexicon.", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9991883635520935}]}, {"text": "Gold lexicons are also used in intrinsic evaluation of generic DA lexicons).", "labels": [], "entities": []}, {"text": "Additionally, the English WHM lexicon is also used in the evaluation of the universality of our approach).", "labels": [], "entities": [{"text": "English WHM lexicon", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.6753453512986501}]}, {"text": "For intrinsic evaluation of generic DA lexicons, we compare our results with Densifier.", "labels": [], "entities": []}, {"text": "provide embeddings and train/validation splits of gold standard lexicons in CZ, DE, ES, FR and EN -we also use them in our experiments.", "labels": [], "entities": []}, {"text": "We show (i) using GEN (the same training words as Densifier), REG ( \u00a73.4) induces generic lexicons in comparable quality; (ii) using PBC+ ZS lexicons, the induced generic DA lexicons are also in high quality. is evaluation metric.", "labels": [], "entities": []}, {"text": "As Densifier is implemented in MATLAB, we implement our model in NumPy) which is more accessible to the community.", "labels": [], "entities": []}, {"text": "For extrinsic evaluation of generic DA lexicons, we carryout Twitter sentiment classification in 13 languages.", "labels": [], "entities": [{"text": "extrinsic evaluation of generic DA lexicons", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.6519827296336492}, {"text": "Twitter sentiment classification", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.6929642756779989}]}, {"text": "For each language, we retrieve \u224812,000 tweets from the human annotated dataset devised by, and sample balanced number of positive and negative tweets (for clearer comparisons and descriptions) which are then randomly split 80/20 into train/test.", "labels": [], "entities": []}, {"text": "We compare our lexicons with Chen and Skiena (2014)'s work.", "labels": [], "entities": []}, {"text": "Two classification models are used ( \u00a75.3) -COUNT (count-based,) and ML (machine-learning-based,).", "labels": [], "entities": [{"text": "COUNT", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9067701101303101}]}, {"text": "We first evaluate the multilingual BPE space by carrying out the crosslingual verse sentiment classification experiment in.", "labels": [], "entities": [{"text": "crosslingual verse sentiment classification", "start_pos": 65, "end_pos": 108, "type": "TASK", "confidence": 0.7432661131024361}]}, {"text": "Two linear SVMs are trained on 2147 English training verses to classify the verse sentiment (positive vs. non-positive, i.e., POS, and negative vs. non-negative, i.e., NEG).", "labels": [], "entities": []}, {"text": "A verse is represented as the TF-IDF weighted sum of the embeddings of its BPEs.", "labels": [], "entities": []}, {"text": "We then conduct the crosslingual verse sentiment analysis -using the SVMs to classify 476 test verses of's 1664 editions in 1259 languages.", "labels": [], "entities": [{"text": "crosslingual verse sentiment analysis", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.783458024263382}]}, {"text": "gives results averaged over 1664 editions.", "labels": [], "entities": []}, {"text": "Word and Char are two multilingual spaces created by.", "labels": [], "entities": []}, {"text": "For Word, whitespace tokenization is used to segment all editions.", "labels": [], "entities": []}, {"text": "For Char, all editions are segmented to sequences of overlapping byte-ngrams (length n varies across languages, see).", "labels": [], "entities": []}, {"text": "Next, the S-ID method is utilized to create the two multilingual spaces.", "labels": [], "entities": []}, {"text": "The S-ID BPE space outperforms both S-ID Word and S-ID Char spaces.", "labels": [], "entities": []}, {"text": "This observation meets our expectation -the data-driven BPE   segmentation is superior to splitting on whitespace (Word) or overlapping byte-ngram segmentation (Char), for non-segmented languages like Japanese whose PBC+ editions are not tokenized.", "labels": [], "entities": [{"text": "BPE   segmentation", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7898455858230591}]}, {"text": "For the more challenging subtask POS, we find the biggest improvement of S-ID BPE over Word is for non-segmented languages like Classical Chinese (lzh), Japanese (jpn), Khmer (khm) and S'gaw Karen (ksw) as shown in (left).", "labels": [], "entities": []}, {"text": "For segmented languages, S-ID BPE delivers similar performance as S-ID Word as shown in (right).", "labels": [], "entities": [{"text": "S-ID BPE", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.7216028571128845}]}, {"text": "This observation also meets our expectation -lots of BPEs in segmented languages are essentially valid words.", "labels": [], "entities": []}, {"text": "These observations show the universality of our approach.", "labels": [], "entities": []}, {"text": "The sentiment information derived from English is successfully transferred to heterogeneous languages without language-dependent preprocessing -even for non-segmented languages.", "labels": [], "entities": []}, {"text": "Sample entries in the English ZS lexicon are shown in (left) as a qualitative evaluation.", "labels": [], "entities": [{"text": "English ZS lexicon", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.7310985326766968}]}, {"text": "two SVMs trained on English BPE embeddings perform strongly in a zero-shot crosslingual setting, and the resulting PBC+ ZS lexicons in difficult (morphologically rich, e.g., Czech; nonsegmented, e.g., Japanese) languages encode clear sentiment information.", "labels": [], "entities": []}, {"text": "(right) qualitatively shows the most sentiment-bearing words of the DA lexicon induced with English ZS lexicon and Twitter embeddings (EN Twitter).", "labels": [], "entities": [{"text": "DA lexicon", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.803711473941803}, {"text": "English ZS lexicon", "start_pos": 92, "end_pos": 110, "type": "DATASET", "confidence": 0.7952921589215597}]}, {"text": "Lots of top ranked words are strong sentiment-bearing hashtags that never occur in the ZS lexicon domain, illustrating that our approach functions well in the domain adaptation setup.", "labels": [], "entities": []}, {"text": "This observation is consistent with Densifier (.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation: ranking correlation.", "labels": [], "entities": []}, {"text": "We compute ranking correlation between our generic DA lexicons and gold standard lexicons.", "labels": [], "entities": []}, {"text": "There are overlapping words between our PBC+ ZS lexicon BPEs and the validation/test sets used by -we discard these training words fora clean comparison.", "labels": [], "entities": [{"text": "PBC+ ZS lexicon BPEs", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.6163408041000367}]}, {"text": "Columns (i) and (ii) of show that REG ( \u00a73.4) delivers results comparable to Densifier (ORTH) when using the same set of generic training words (GEN) in lexicon induction.", "labels": [], "entities": [{"text": "REG", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.7013943791389465}]}, {"text": "However, our method is more efficient -no need to compute the expensive SVD after every batch update.", "labels": [], "entities": []}, {"text": "Comparing columns (ii) and (iii), we see a marginal decrease of \u03c4 between .020 and .057 when GEN is replaced by PBC+ ZS lexicons.", "labels": [], "entities": []}, {"text": "Note that PBC+ ZS lexicons have much fewer training BPEs than GEN (e.g., 343 vs. 4298 in JA Wiki) -this may contribute to the decrease.", "labels": [], "entities": [{"text": "BPEs", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.48129594326019287}, {"text": "GEN", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.6344568729400635}, {"text": "JA Wiki", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.7486637532711029}]}, {"text": "These comparable results also reflect the correctness of PBC+ ZS lexicons.", "labels": [], "entities": []}, {"text": "We also use \u03b1 = 0.4 and \u03bb = 0.01, the optimal hyperparameter values found on the trial set of EN Twitter, to induce generic DA lexicons for the other languages.", "labels": [], "entities": [{"text": "EN Twitter", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.9175034463405609}]}, {"text": "This is the common setting    Overall, the performance differences between GEN (based on generic gold standard lexicons) and PBC+ (based on PBC+ ZS lexicons) are small and \u03c4 correlations are high.", "labels": [], "entities": [{"text": "GEN", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.7702141404151917}]}, {"text": "The high quality of generic DA lexicons in these six diverse (morphologically rich and non-segmented) languages shows the universality of our approach again -no language-dependent preprocessing is needed.", "labels": [], "entities": []}, {"text": "Extrinsic evaluation: Twitter sentiment classification.", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7940631210803986}, {"text": "Twitter sentiment classification", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.7027799586455027}]}, {"text": "Based on the subset of frequent words only, we use the top 10% most positive and most negative words for this evaluation.", "labels": [], "entities": []}, {"text": "We compare with the closest work -lexicons from.", "labels": [], "entities": []}, {"text": "Two classification models are used -wordcount-based model COUNT (Chen and Skiena, sqi bul hrv deu hun pol por rus srp slk slv spa swe \u00af x: Accuracy of Twitter sentiment classification in Albanian (sqi), Bulgarian (bul), Croatian (hrv), German (deu), Hungarian (hun), Polish (pol), Portuguese (por), Russian (rus), Serbian (srp), Slovak (slk), Slovenian (slv), Spanish (spa) and Swedish (swe).", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 151, "end_pos": 183, "type": "TASK", "confidence": 0.6469780107339224}]}, {"text": "Baseline of all experiments: 0.5.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9912609457969666}]}, {"text": "We further conduct automatic and human evaluations on 95 diverse languages to show the universality of our approach.", "labels": [], "entities": []}, {"text": "We focus on intrinsic evaluation -verifying the correctness of PBC+ ZS lexicons with F1, and assessing the quality of generic DA lexicons using \u03c4 . The extrinsic evaluation, i.e., Twitter sentiment classification, is not feasible here due to missing human annotated Twitter datasets in low-resource languages.", "labels": [], "entities": [{"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9516385197639465}, {"text": "Twitter sentiment classification", "start_pos": 180, "end_pos": 212, "type": "TASK", "confidence": 0.6570479273796082}]}, {"text": "Similar to;, we use Google Translate (GT) for automatic evaluationgiven a non-English language L, we translate its PBC+ ZS lexicon and generic DA lexicon into English.", "labels": [], "entities": []}, {"text": "Translated English lexicons are then evaluated against the gold English lexicon WHM.", "labels": [], "entities": [{"text": "WHM", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.5176211595535278}]}, {"text": "GT supports 102 non-English languages.", "labels": [], "entities": []}, {"text": "We omit ten languages that (i) are not covered by PBC+ (Corsican, Galician, Pashto, Yiddish); (ii) are covered in PBC+, but not in the alphabet used by GT (Malayalam); (iii) do not have public pretrained embeddings (Filipino, Hmong, Kyrgyz, Sesotho); or (iv) are very close to another language (we keep Croatian, but do not include Bosnian).", "labels": [], "entities": []}, {"text": "We conduct separate experiments for Bokm\u00e5l and Nynorsk, which are not distinguished by GT.", "labels": [], "entities": [{"text": "Bokm\u00e5l", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.9711828231811523}, {"text": "Nynorsk", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.8726032972335815}, {"text": "GT", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.932723879814148}]}, {"text": "Thus, we evaluate on 93 languages.", "labels": [], "entities": []}, {"text": "When translating words to English, we discard entries where GT fails (i.e., output is identical to input).", "labels": [], "entities": []}, {"text": "As GT requires the uploaded file to be small ( 1MB), we do the evaluation on uniformly sampled 600 top 1% positive and negative words that are frequent.", "labels": [], "entities": []}, {"text": "For ten languages (Chichewa, Hausa, Hawaiian, Igbo, Lao, Maori, Samoan, Shona, Xhosa, Zulu) that have very small embedding training corpora (<5MB Wikipedia pages and articles) and vocabulary sizes (e.g., 5000 for Hausa), we sample 200 words at 10%.", "labels": [], "entities": []}, {"text": "We see that PBC+ ZS lexicons show high consistency with gold labels across all 93 languages (F1 columns), including morphologically rich languages like Czech and Turkish, and non-segmented languages like Japanese and Khmer.", "labels": [], "entities": [{"text": "consistency", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9891616106033325}]}, {"text": "The generic DA lexicons show high correlation with gold labels (\u03c4 columns) -with two exceptions.", "labels": [], "entities": []}, {"text": "First, some languages have low-quality embeddings due to small embedding training corpora (e.g., Hawaiian: 998 KB; Igbo: 1014 KB) or because the training corpora apparently have low quality -e.g., the Luxembourgish embedding vocabulary contains a large amount of French and German words, suggesting that it was trained on mixed text and that the genuine Luxembourgish part is small.", "labels": [], "entities": []}, {"text": "Second, GT does not perform well for some of the languages, again this is the case for Luxembourgish and also for Frisian.", "labels": [], "entities": []}, {"text": "To give an example from Lux-  embourgish for both problems: \"vergloust\" and its first nearest neighbor \"verglousten\" are translated by GT as \"glowed\" and \"forget about it\".", "labels": [], "entities": [{"text": "Lux-  embourgish", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9275521238644918}]}, {"text": "We recommend to use the higher quality PBC+ ZS lexicon for these languages.", "labels": [], "entities": []}, {"text": "Apart from above exceptions, both F1 and \u03c4 are reasonably high, evidencing that our universal approach is applicable to abroad range of typologically diverse languages.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.999626636505127}]}, {"text": "We do human evaluation for Hiligaynon and Tibetan, languages not supported by GT.", "labels": [], "entities": []}, {"text": "There are no public pretrained embeddings for Hiligaynon.", "labels": [], "entities": []}, {"text": "We train embeddings on a concatenation of texts from project Palito () and Jehovah's Witnesses e-books (www. jw.org).", "labels": [], "entities": []}, {"text": "From the generic DA Hiligaynon and Tibetan lexicons, we uniformly sample 199 from the top 10% positive and negative frequent BPEs.", "labels": [], "entities": [{"text": "DA Hiligaynon", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.7722226083278656}]}, {"text": "Two Tibetan scholars and three Hiligaynon speakers annotated these BPEs as positive, negative, neutral, unclear where the last category refers to cases where the intended word is not apparent from the BPE.", "labels": [], "entities": []}, {"text": "We omit entries labeled as unclear and compute \u03c4 . shows \u03c4 averaged over annotators.", "labels": [], "entities": []}, {"text": "We see that our lexicons have consistent positive correlation with the human annotation in both languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F1 for verse sentiment classification. Bold:  our results. Word/Char are from Dufter et al. (2018).", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9936999082565308}, {"text": "verse sentiment classification", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.9256286223729452}]}, {"text": " Table 3: The most improved (left) editions when using  S-ID BPE (B) compared with S-ID Word (W). B and  W perform similarly on segmented languages (right)  like English (eng), French (fra), German (deu), Spanish  (spa) and Portuguese (por). Numbers are in F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 257, "end_pos": 259, "type": "METRIC", "confidence": 0.9980624318122864}]}, {"text": " Table 5: High consistency between PBC+ ZS lexicons  and generic gold lexicons in JA and five languages  used in Rothe et al. (2016). \u2229 size: intersection size.  |PBC+|: ZS lexicon size.", "labels": [], "entities": [{"text": "consistency", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9803481698036194}]}, {"text": " Table 7: Accuracy of Twitter sentiment classification in Albanian (sqi), Bulgarian (bul), Croatian (hrv), German  (deu), Hungarian (hun), Polish (pol), Portuguese (por), Russian (rus), Serbian (srp), Slovak (slk), Slovenian (slv),  Spanish (spa) and Swedish (swe). Baseline of all experiments: 0.5.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6890425781408945}, {"text": "Baseline", "start_pos": 266, "end_pos": 274, "type": "METRIC", "confidence": 0.9966172575950623}]}, {"text": " Table 8: Intrinsic evaluation of our PBC+ ZS and generic DA lexicons in 93 languages. We see high consistency  (F1) between PBC+ ZS lexicons and gold labels across languages. The generic DA lexicons are strongly correlated  (\u03c4 ) with gold labels in most languages.", "labels": [], "entities": [{"text": "consistency", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.9914178848266602}, {"text": "F1)", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9594455659389496}]}, {"text": " Table 9: Human evaluation of generic DA lexicons in  Hiligaynon and Tibetan. 2-way: positive, negative. 3- way: positive, neutral, negative.", "labels": [], "entities": []}]}