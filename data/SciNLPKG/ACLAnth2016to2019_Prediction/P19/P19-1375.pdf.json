{"title": [], "abstractContent": [{"text": "The sequential order of utterances is often meaningful in coherent dialogues, and the order changes of utterances could lead to low-quality and incoherent conversations.", "labels": [], "entities": []}, {"text": "We consider the order information as a crucial supervised signal for dialogue learning, which, however, has been neglected by many previous dialogue systems.", "labels": [], "entities": [{"text": "dialogue learning", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.9479440748691559}]}, {"text": "Therefore, in this paper , we introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the flow of conversation in dialogues.", "labels": [], "entities": [{"text": "inconsistent order detection", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6760283708572388}]}, {"text": "Given a sampled utterance pair triple, the task is to predict whether it is ordered or misordered.", "labels": [], "entities": []}, {"text": "Then we propose a sampling-based self-supervised network SSN to perform the prediction with sampled triple references from previous dialogue history.", "labels": [], "entities": []}, {"text": "Furthermore , we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training.", "labels": [], "entities": []}, {"text": "We demonstrate that the proposed methods can be applied to both open-domain and task-oriented dialogue scenarios, and achieve the new state-of-the-art performance on the Open-Subtitiles and Movie-Ticket Booking datasets.", "labels": [], "entities": [{"text": "Movie-Ticket Booking datasets", "start_pos": 190, "end_pos": 219, "type": "DATASET", "confidence": 0.752436101436615}]}], "introductionContent": [{"text": "In recent years, dialogue systems have achieved fruitful results with neural conversation models in both open-domain generation) and task-oriented completion.", "labels": [], "entities": [{"text": "task-oriented completion", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.7425236403942108}]}, {"text": "These methods empower lots of real-world dialogue applications such as Google Home and Apple Siri.", "labels": [], "entities": []}, {"text": "However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence ( . They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability ().", "labels": [], "entities": [{"text": "utterance generation", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.8620533049106598}]}, {"text": "We argue that in a meaningful and coherent dialogue, the change of utterance order will lead to a low-quality dialogue.", "labels": [], "entities": []}, {"text": "However, most existing neural-based dialogue systems either encode the full dialogue history () or only the current utterance (.", "labels": [], "entities": []}, {"text": "None of them explicitly models the sequential order and studies its criticality to the dialogue learning problem.", "labels": [], "entities": [{"text": "dialogue learning", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.8111959099769592}]}, {"text": "In this paper, we explore the sequential order within the dialogue as the self-supervised signal to guide meaningful and coherent dialogue learning.", "labels": [], "entities": []}, {"text": "We introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the order signal of the dialogue.", "labels": [], "entities": [{"text": "inconsistent order detection", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.6309952338536581}]}, {"text": "The task is defined as, given a target utterance pair triple, the model is required to predict whether the triple is correctly ordered or not.", "labels": [], "entities": []}, {"text": "For instance, the utterance pair triple (Q 1 , A 1 ), (Q 4 , A 4 ), (Q 2 , A 2 ) is misordered.", "labels": [], "entities": []}, {"text": "The key to solving this task is to model the utterance order based on the dialogue context effectively.", "labels": [], "entities": []}, {"text": "But when directly encoding the full dialogue history along the temporal order, the model actually only focuses on the ending utterances, and earlier information is largely discarded ().", "labels": [], "entities": []}, {"text": "Thus, we propose a sampling-based selfsupervised network (SSN ) to account for the forgetfulness problem and solve the inconsistent order detection task.", "labels": [], "entities": [{"text": "inconsistent order detection task", "start_pos": 119, "end_pos": 152, "type": "TASK", "confidence": 0.6953272894024849}]}, {"text": "In order to accurately predict if a target utterance triple is ordered or not, we randomly sample utterance triples from the dialogue history as the reference to incorporate the dialogue context.", "labels": [], "entities": []}, {"text": "Since for the same target utterance triple, the sampled triple references are different at different iterations during training.", "labels": [], "entities": []}, {"text": "It essentially approximates the full dialogue history without suf-fering from the forgetfulness issue.", "labels": [], "entities": []}, {"text": "To further utilize SSN in real dialogue learning, we propose to jointly learn SSN and the dialogue model via alternative training, where the output probability of SSN is treated as the order signal to evaluate the generated utterance.", "labels": [], "entities": []}, {"text": "Moreover, the proposed approach can be applied to both open-domain and task-oriented dialogue learning, which indicates that SSN is a general and scalable approach for dialogue learning.", "labels": [], "entities": [{"text": "dialogue learning", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7281557023525238}, {"text": "SSN", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9420766830444336}, {"text": "dialogue learning", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.8827766180038452}]}, {"text": "Empirical results on two widely-used benchmark datasets, OpenSubtitles and Movie-Ticket Booking, show that our self-supervised network consistently improves the state-of-the-art (SOTA) neural-based dialogue training methods.", "labels": [], "entities": []}, {"text": "In summary, our main contributions are three-fold: \u2022 We introduce the task of inconsistent order detection, and propose a self-supervised learning network SSN to solve this task and explicitly model the crucial order information in dialogue.", "labels": [], "entities": [{"text": "inconsistent order detection", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.7189362843831381}]}, {"text": "\u2022 We propose a general framework to jointly learn SSN and the dialogue models, where the sequential order in dialogues can be explicitly used to guide the utterance generation.", "labels": [], "entities": [{"text": "SSN", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9021489024162292}, {"text": "utterance generation", "start_pos": 155, "end_pos": 175, "type": "TASK", "confidence": 0.765586793422699}]}, {"text": "\u2022 Our method advances the existing state-ofthe-art dialogue systems in both open-domain and task-oriented scenarios.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before we deploy the self-supervised network into real dialogue systems, we first test the model architectures for reliability.", "labels": [], "entities": []}, {"text": "We randomly choose 40K balanced ordered and misordered utterance pair triples from the OpenSubtitles () dataset, and train the SSN to solve this 2-class classification.", "labels": [], "entities": [{"text": "OpenSubtitles () dataset", "start_pos": 87, "end_pos": 111, "type": "DATASET", "confidence": 0.7690911293029785}]}, {"text": "We sample another 1K balanced triples for testing.", "labels": [], "entities": []}, {"text": "We also consider a baseline model, where the target triple is encoded by SSN , and the previous dialogue history is encoded by a hierarchical LSTM.", "labels": [], "entities": []}, {"text": "The concatenation of two embeddings is used for the final prediction.", "labels": [], "entities": []}, {"text": "Because our SSN is a sampling-based ap-  proach, we report the average prediction accuracy of 5 runs on the 2-class classification as shown in.", "labels": [], "entities": [{"text": "prediction", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9327701330184937}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.6209445595741272}]}, {"text": "From the results, we can observe that: (1) The conventional hierarchical LSTM is not suitable for this task, and this baseline only shows a marginal improvement compared with the strategy that only considers target triple without any history.", "labels": [], "entities": []}, {"text": "The results also match previous findings (), where they suggest that only the last two proceeding utterances in the hierarchical network are semantically significant.", "labels": [], "entities": []}, {"text": "(2) As for our SSN , it is safe to tell that reference triples can be a tremendous supplement to the inconsistent order detection.", "labels": [], "entities": [{"text": "inconsistent order detection", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.6462366183598837}]}, {"text": "It is not surprising because by adding reference triples, the SSN will know more information of semantic context within the dialogue.", "labels": [], "entities": []}, {"text": "Especially when having both ordered and misordered references, the SSN has the highest classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9440554976463318}]}, {"text": "This also shows that the sampling strategy, 1*Ordered + 1*misordered references, is the most reliable structure for real dialogue systems.", "labels": [], "entities": []}, {"text": "Thus, for the rest of the experiments, we directly use the SSN with one ordered and one misordered references strategy to achieve the best performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The cross evaluation of adversarial success  rate on different generators and discriminators. Please  refer to Section 4.2 Adversarial Evaluation for expla- nations.", "labels": [], "entities": []}, {"text": " Table 3: The automatic evaluation of generated utter- ances on distinct-1 and distinct-2 metrics. Please refer  to Section 4.2 Automatic Evaluation for explanations.", "labels": [], "entities": []}, {"text": " Table 4: The human evaluation of generated utterances  in three methods. The result here is statistically signif- icant with p < 0.01 according to sign test. Please refer  to Section 4.2 Human Evaluation for explanations.", "labels": [], "entities": []}, {"text": " Table 5: The experimental results of different dialogue agents at training epoch = {100, 200, 300}. Each number  is averaged over 3 runs, and each run tested on 50 dialogues. The D3Q-SSN denotes the D3Q agent where our  proposed SSN replaces the discriminator. The \"fixed \u03b8 D /\u03b8 SSN \" indicates the discriminator/SSN is pre-trained  and fixed during the training process. Succ: Success Rate. Reward: Average Reward. Turns: Average Turns.", "labels": [], "entities": [{"text": "Succ", "start_pos": 373, "end_pos": 377, "type": "METRIC", "confidence": 0.9927372336387634}, {"text": "Success Rate", "start_pos": 379, "end_pos": 391, "type": "METRIC", "confidence": 0.9396717548370361}, {"text": "Reward", "start_pos": 393, "end_pos": 399, "type": "METRIC", "confidence": 0.977527379989624}, {"text": "Average Reward", "start_pos": 401, "end_pos": 415, "type": "METRIC", "confidence": 0.8334757387638092}, {"text": "Turns", "start_pos": 417, "end_pos": 422, "type": "METRIC", "confidence": 0.9267162680625916}]}]}