{"title": [{"text": "A Neural Graph-based Approach to Verbal MWE Identification", "labels": [], "entities": [{"text": "Verbal MWE Identification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6717914541562399}]}], "abstractContent": [{"text": "We propose to tackle the problem of verbal multiword expression (VMWE) identification using a neural graph parsing-based approach.", "labels": [], "entities": [{"text": "verbal multiword expression (VMWE) identification", "start_pos": 36, "end_pos": 85, "type": "TASK", "confidence": 0.7473301546914237}]}, {"text": "Our solution involves encoding VMWE annotations as labellings of dependency trees and, subsequently, applying a neu-ral network to model the probabilities of different labellings.", "labels": [], "entities": []}, {"text": "This strategy can be particularly effective when applied to discontinu-ous VMWEs and, thanks to dense, pre-trained word vector representations, VMWEs unseen during training.", "labels": [], "entities": []}, {"text": "Evaluation of our approach on three PARSEME datasets (German, French, and Polish) shows that it allows to achieve performance on par with the previous state-of-the-art (Al Saied et al., 2018).", "labels": [], "entities": [{"text": "PARSEME datasets", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.8695471882820129}]}], "introductionContent": [{"text": "Multiword expressions (MWEs) are defined as combinations of multiple lexemes whose overall properties are not readily predictable by those of their components.", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7149972558021546}]}, {"text": "This idiosyncrasy makes MWEs a well-known challenge for NLP and their ubiquity forces us to find ways to account for them.", "labels": [], "entities": []}, {"text": "While all types of MWEs come with their own set of issues, verbal MWEs (VMWEs) standout as a particularly challenging subclass because of properties like discontinuity, overlap, varying word order, and syntactic or semantic ambiguity.", "labels": [], "entities": []}, {"text": "These properties suggest that we have to rely on both syntactic and semantic features to successfully process VMWEs ().", "labels": [], "entities": []}, {"text": "E.g., syntactic information can help us catch long-distance dependencies, while semantic information can prove useful in disambiguating between literal and idiomatic readings.", "labels": [], "entities": []}, {"text": "One of the main tasks that constitute MWE processing is the automatic identification of MWEs in running text which can be used as a preprocessing step for parsing or machine translation.", "labels": [], "entities": [{"text": "MWE processing", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.9780315160751343}, {"text": "machine translation", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.6212656944990158}]}, {"text": "MWE identification can be seen as a sequence labeling task similar to named entity recognition (NER): A system receives sequences of tokens as input and outputs the same sequences with annotation labels added to it.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9418047964572906}, {"text": "named entity recognition (NER)", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.8203862110773722}]}, {"text": "As in NER, most parts of the sequence will belong to the negative class, that is, the majority of words is not part of an MWE.", "labels": [], "entities": [{"text": "NER", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.6088688373565674}]}, {"text": "However, certain issues that occur when dealing with NEs and MWEs are much more prevalent in case of the latter.", "labels": [], "entities": [{"text": "NEs and MWEs", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.8378411730130514}]}, {"text": "Especially with respect to discontinuity.", "labels": [], "entities": [{"text": "discontinuity", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9217647910118103}]}, {"text": "In the PARSEME 1.0 corpus (, wich comprises datasets of 18 languages, only three of them have a continuity rate of over 80% when it comes to VMWEs.", "labels": [], "entities": [{"text": "PARSEME 1.0 corpus", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.8397976160049438}, {"text": "continuity rate", "start_pos": 96, "end_pos": 111, "type": "METRIC", "confidence": 0.9876770377159119}]}, {"text": "German, the most striking example in this regard, has a continuity rate of 35.7% and 30.54% of its discontinuities are longer than three words).", "labels": [], "entities": [{"text": "continuity rate", "start_pos": 56, "end_pos": 71, "type": "METRIC", "confidence": 0.9927446842193604}]}, {"text": "In (1), the verb-particle construction teilnehmen 'take part' spans over 13 words and this is not even a particularly excessive example.", "labels": [], "entities": []}, {"text": "Much more could be inserted in between the two VMWE components nahmen and teil, e.g. a relative clause, without it sounding marked.", "labels": [], "entities": []}, {"text": "(1)  'In Paris itself roughly a thousand students took part in a rally in the Quartier Latin.", "labels": [], "entities": [{"text": "Quartier Latin", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9625399708747864}]}, {"text": "In this paper, we propose a method which identifies VMWE occurrences directly over dependency structures.", "labels": [], "entities": []}, {"text": "Relying on existing dependency trees greatly simplifies the task, since VMWEs are usually connected in such trees, even if they are discontinuous on the surface of word sequences as in.", "labels": [], "entities": []}, {"text": "In the same vein as, our method is conceptually divided into two layers.", "labels": [], "entities": []}, {"text": "The first is concerned with encoding VMWE occurrences as tree labellings, as well as the inverse process of decoding the labellings into VMWE annotations.", "labels": [], "entities": []}, {"text": "In the second layer, a probability model which allows to discriminate between different VMWE labellings is used.", "labels": [], "entities": []}, {"text": "We propose two probability models, both based on dense feature representations (i.e. pre-trained word embeddings) of input words.", "labels": [], "entities": []}, {"text": "Relying on dense features allows to easier generalize beyond training data, on the one hand, and to possibly capture helpful syntactic and semantic cues, on the other hand.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "2, we describe related work on VMWE identification.", "labels": [], "entities": [{"text": "VMWE identification", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8848294913768768}]}, {"text": "3, we give a detailed description of our methods, in particular the encoding schemata and the labelling models.", "labels": [], "entities": []}, {"text": "4, we summarize the experiments we performed to evaluate our approach.", "labels": [], "entities": []}, {"text": "Finally, we conclude and mention possible future work in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe the experiments we performed in order to evaluate the methods described in Sec.", "labels": [], "entities": []}, {"text": "3.  All the experiments were run on the German (DE), French (FR), and Polish (PL) datasets of the edition 1.1 of the PARSEME corpus (.", "labels": [], "entities": [{"text": "PARSEME corpus", "start_pos": 117, "end_pos": 131, "type": "DATASET", "confidence": 0.8980832695960999}]}, {"text": "This highly multilingual corpus was created in the context of a shared task on the automatic identification of VMWEs and consists of annotated datasets of 20 different languages.", "labels": [], "entities": [{"text": "automatic identification of VMWEs", "start_pos": 83, "end_pos": 116, "type": "TASK", "confidence": 0.6520683318376541}]}, {"text": "The individual datasets are collections of sentences which were, among other things, tokenized, part-of-speech (POS) tagged, lemmatized, and enriched with dependency information.", "labels": [], "entities": []}, {"text": "While FR contains solely manual dependency annotations, the dependencies in DE were annotated partly manually and partly automatically.", "labels": [], "entities": [{"text": "FR", "start_pos": 6, "end_pos": 8, "type": "DATASET", "confidence": 0.7108219265937805}]}, {"text": "Besides automatic annotations, PL includes dependencies that were converted from a manually annotated constituency treebank.", "labels": [], "entities": []}, {"text": "The VMWE annotation comprises the identification of the words that belong to a VMWE instance, as well as the categorization of the identified instances.", "labels": [], "entities": []}, {"text": "The categories used in the PARSEME annotation framework are lightverb constructions (LVCs), verbal idioms (VIDs), inherently reflexive verbs (IRV), verb-particle constructions (VPC), multi-verb constructions (MVC), and inherently adpositional verbs (IAV).", "labels": [], "entities": []}, {"text": "Our implementation of the global model is currently a prototype and it takes a relatively longtime to train a model.", "labels": [], "entities": []}, {"text": "We therefore focused on a few languages which come from different families and cover a large spectrum of VMWE-related phenomena.", "labels": [], "entities": []}, {"text": "This way, we hope we can test our system on a variety of problems despite the small number of languages.", "labels": [], "entities": []}, {"text": "For instance, DE contains a large amount of VPCs, a verb class very common in Germanic, but almost non-existent in Romance or Slavic languages.", "labels": [], "entities": []}, {"text": "These VPCs also account for most of the single-token VMWEs in DE which do not occur in FR or PL.", "labels": [], "entities": []}, {"text": "The Polish dataset covers a reasonable amount of IAVs, 7 which are rather challenging for our models because of their lack of connectivity in the dependency structures.", "labels": [], "entities": [{"text": "Polish dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9320170283317566}]}, {"text": "We evaluated the two implemented systems and the benchmark (ATILF) system on the development and the test parts of the German (DE), French (FR), and Polish (PL) PARSEME datasets.", "labels": [], "entities": [{"text": "ATILF)", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.8536363244056702}, {"text": "PARSEME datasets", "start_pos": 161, "end_pos": 177, "type": "DATASET", "confidence": 0.8036546409130096}]}, {"text": "We trained one local and three global models per language and per VMWE category.", "labels": [], "entities": []}, {"text": "Training separate models for different categories allows to partially handle the issue of overlapping VMWE instances, as well as to simplify the architecture (no need to encode the categories in terms of tree labellings).", "labels": [], "entities": []}, {"text": "The benchmark system predicts all the categories in one pass.", "labels": [], "entities": []}, {"text": "We used the official evaluation script 13 provided by the shared task organizers to calculate all the scores.", "labels": [], "entities": []}, {"text": "In contrast to several systems participating in the PARSEME shared task 1.1, we didn't use the development parts for training.", "labels": [], "entities": [{"text": "PARSEME shared task 1.1", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.6745799481868744}]}, {"text": "This is important in that the development set can contain VMWEs unseen in the training part.", "labels": [], "entities": []}, {"text": "Available at https://gitlab.com/parseme/ sharedtask-data/tree/master/1.1/bin/.", "labels": [], "entities": []}, {"text": "As mentioned above, we trained three global models per language and per VMWE category.", "labels": [], "entities": []}, {"text": "One model was obtained using constrained training and two models using unconstrained training (see Sec. 3.3.3).", "labels": [], "entities": []}, {"text": "We observed that the results between different training runs can differ significantly.", "labels": [], "entities": []}, {"text": "For instance, the LVC.full identification Fscores differed by almost 3% on the FR development set between the two unconstrained models.", "labels": [], "entities": [{"text": "LVC.full identification Fscores", "start_pos": 18, "end_pos": 49, "type": "METRIC", "confidence": 0.5230570634206136}, {"text": "FR development set", "start_pos": 79, "end_pos": 97, "type": "DATASET", "confidence": 0.8108030756314596}]}, {"text": "We therefore used all three models (for each language and VMWE category) to calculate ensemble node scores (see Eq. 4) and ensemble arc scores (see Eq.", "labels": [], "entities": []}, {"text": "5) by simply summing up the corresponding scores coming from the three models.", "labels": [], "entities": []}, {"text": "Such ensemble averaging should have a smoothing effect and alleviate the issue of diverging results.", "labels": [], "entities": [{"text": "ensemble averaging", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.7359119653701782}]}, {"text": "The general results of the three systems on the development and the test sets are presented in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9585487544536591}]}, {"text": "The benchmark system and the global model achieve comparable results: ATILF has better overall performance on the development sets, the global model -on the test sets.", "labels": [], "entities": [{"text": "ATILF", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.9029741287231445}]}, {"text": "The results of the local model are consistently lower than those of the global model, which shows the usefulness of extended encoding combined with global scoring.", "labels": [], "entities": []}, {"text": "Nevertheless, the local model achieves very competitive results, comparable to those obtained with the best systems participating in the PARSEME shared task 1.1.", "labels": [], "entities": [{"text": "PARSEME shared task 1.1", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.5117330774664879}]}, {"text": "More detailed evaluation results are presented in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9593059420585632}]}, {"text": "4. The former shows the performance of the systems across different VMWErelated challenges, while the latter presents their   results for the VMWE categories occurring most frequently in the three test sets.", "labels": [], "entities": []}, {"text": "These results clearly show that our approach performs particularly well for both discontinuous and unseen VMWEs.", "labels": [], "entities": []}, {"text": "Despite its relative simplicity, the local model also yields better results than the benchmark system in these two categories.", "labels": [], "entities": []}, {"text": "The global model under-performs in the identification of the identical-to-train VMWEs.", "labels": [], "entities": []}, {"text": "This also applies to the local model, which does not perform very well in the category of seen-in-train VMWEs in general.", "labels": [], "entities": []}, {"text": "Concerning the VMWE categories, VIDs proved challenging for both our models, especially in DE and PL.", "labels": [], "entities": [{"text": "VIDs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.928412675857544}]}, {"text": "This maybe due to the arc-factored nature of our approach, which maybe inadequate for handling VIDs, often composed from more than two words.", "labels": [], "entities": [{"text": "VIDs", "start_pos": 95, "end_pos": 99, "type": "TASK", "confidence": 0.9298502802848816}]}, {"text": "On the other hand, our approach proved very effective in case of LVCs (especially in PL) and, somewhat surprisingly, in case of IAVs, consistently disconnected in the PARSEME dependency structures.", "labels": [], "entities": [{"text": "PARSEME", "start_pos": 167, "end_pos": 174, "type": "DATASET", "confidence": 0.6274426579475403}]}], "tableCaptions": [{"text": " Table 1: General results per language and system on the development data.", "labels": [], "entities": []}, {"text": " Table 2: General results per language and system on the test data.", "labels": [], "entities": []}, {"text": " Table 3: Results (MWE-based F-scores) per VMWE challenge averaged over the three language test sets. The  single-token score is only calculated on German because single-token VMWEs do not occur in the other languages.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.7042874097824097}]}, {"text": " Table 4: Results (MWE-based F-scores) for the se- lected VMWE categories on the test sets. The last row  per language reports the percentage of occurrences per  category in the test data.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.643266499042511}]}]}