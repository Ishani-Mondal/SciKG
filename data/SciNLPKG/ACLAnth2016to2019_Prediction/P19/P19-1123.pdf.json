{"title": [{"text": "Dynamically Composing Domain-Data Selection with Clean-Data Selection by \"Co-Curricular Learning\" for Neural Machine Translation", "labels": [], "entities": [{"text": "Dynamically Composing Domain-Data Selection", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5615549981594086}, {"text": "Neural Machine Translation", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.6980597972869873}]}], "abstractContent": [{"text": "Noise and domain are important aspects of data quality for neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6789744794368744}]}, {"text": "Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined.", "labels": [], "entities": []}, {"text": "This paper introduces a \"co-curricular learn-ing\" method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.9016568064689636}]}, {"text": "We apply an EM-style optimization procedure to further refine the \"co-curriculum\".", "labels": [], "entities": []}, {"text": "Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.", "labels": [], "entities": []}], "introductionContent": [{"text": "Significant advancement has been witnessed in neural machine translation (NMT), thanks to better modeling and data.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.8373158772786459}]}, {"text": "As a result, NMT has found successful use cases in, for example, domain translation and helping other NLP applications, e.g.,.", "labels": [], "entities": [{"text": "domain translation", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8293408453464508}]}, {"text": "As these tasks start to scale to more domains, a challenge starts to surface: Given a source monolingual corpus, how to use it to improve an NMT model to translate same-domain sentences well?", "labels": [], "entities": []}, {"text": "Data selection plays an important role in this context.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7127812802791595}]}, {"text": "In machine translation, data selection has been a fundamental research topic.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7851578593254089}, {"text": "data selection", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8306052684783936}]}, {"text": "One idea (van der) for this problem is to use language models to select parallel data out of a background parallel corpus, seeded by the source monolingual sentences.", "labels": [], "entities": []}, {"text": "This approach, however, performs poorly on noisy data, such as large-scale, web-crawled datasets, because data noise hurts NMT performance . The lower learning curve in  NMT community has realized the harm of data noise to translation quality, leading to efforts in data denoising ( , as has been popular in computer vision ().", "labels": [], "entities": [{"text": "data denoising", "start_pos": 266, "end_pos": 280, "type": "TASK", "confidence": 0.7460890710353851}]}, {"text": "The upper curve in shows the effect of clean-data selection on the same noisy data.", "labels": [], "entities": []}, {"text": "These denoising methods, however, cannot be directly used for the problem in question as they require trusted parallel data as input.", "labels": [], "entities": []}, {"text": "We introduce a method to dynamically combine clean-data selection and domain-data selection.", "labels": [], "entities": []}, {"text": "We treat them as independent curricula, and compose them into a \"co-curriculum\".", "labels": [], "entities": []}, {"text": "We summarize our contributions as: 1.", "labels": [], "entities": []}, {"text": "\"Co-curricular learning\", for transfer learning across data quality.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.9240049719810486}]}, {"text": "It extends the single curriculum learning work in NMT and makes the existing domain-data selection method work better with noisy data.", "labels": [], "entities": [{"text": "NMT", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.8373115062713623}]}, {"text": "2. A curriculum optimization procedure to refine the co-curriculum.", "labels": [], "entities": []}, {"text": "While gaining some improvement with deep models, it surprisingly improves shallow model by 8-10 BLEU points -We find that bootstrapping seems to \"regularize\" the curriculum and make it easier fora small model to learn on.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.998262345790863}]}, {"text": "3. We wish our work contributed towards better understanding of data, such as noise, domain, or \"easy to learn\", and its interaction with NMT network.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Per-step cascading works better than mixing on", "labels": [], "entities": []}, {"text": " Table 4: Co-curriculum improves either constituent curricu-", "labels": [], "entities": []}, {"text": " Table 5: EM-style optimization further improves domain", "labels": [], "entities": [{"text": "EM-style optimization", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9093722403049469}]}, {"text": " Table 6: Retraining with a curriculum may work better than", "labels": [], "entities": [{"text": "Retraining", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9834837317466736}]}, {"text": " Table 7: Curriculum learning works slightly better than fine-", "labels": [], "entities": []}, {"text": " Table 8: Curriculum learning works better than retrain-", "labels": [], "entities": []}]}