{"title": [{"text": "Neural Architectures for Nested NER through Linearization", "labels": [], "entities": [{"text": "Nested NER", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.6131222546100616}]}], "abstractContent": [{"text": "We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label.", "labels": [], "entities": [{"text": "nested named entity recognition (NER)", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.8336148687771389}]}, {"text": "We encode the nested labels using a linearized scheme.", "labels": [], "entities": []}, {"text": "In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture.", "labels": [], "entities": []}, {"text": "In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted.", "labels": [], "entities": []}, {"text": "The proposed methods outper-form the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC.", "labels": [], "entities": [{"text": "ACE-2004", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.9340252876281738}, {"text": "ACE-2005", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.8316071033477783}, {"text": "GENIA", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.9041983485221863}, {"text": "Czech CNEC", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.92423015832901}]}, {"text": "We also enrich our architectures with the recently published contextual embed-dings: ELMo, BERT and Flair, reaching further improvements for the four nested entity corpora.", "labels": [], "entities": [{"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9955413937568665}, {"text": "Flair", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.6962153911590576}]}, {"text": "In addition, we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English.", "labels": [], "entities": [{"text": "NER", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.6677715182304382}, {"text": "CoNLL-2002 Dutch", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.864121824502945}, {"text": "CoNLL-2003 English", "start_pos": 98, "end_pos": 116, "type": "DATASET", "confidence": 0.9065106809139252}]}], "introductionContent": [{"text": "In nested named entity recognition, entities can be overlapping and labeled with more than one label such as in the example \"The Florida Supreme Court\" containing two overlapping named entities \"The Florida Supreme Court\" and \"Florida\".", "labels": [], "entities": [{"text": "nested named entity recognition", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.7733493745326996}, {"text": "Florida Supreme Court\"", "start_pos": 129, "end_pos": 151, "type": "DATASET", "confidence": 0.9127893596887589}, {"text": "Florida Supreme Court", "start_pos": 199, "end_pos": 220, "type": "DATASET", "confidence": 0.8269088268280029}, {"text": "Florida\"", "start_pos": 227, "end_pos": 235, "type": "DATASET", "confidence": 0.9406383633613586}]}, {"text": "Recent publications on nested named entity recognition involve stacked LSTM-CRF NE recognizer (, or a construction of a special structure that explicitly captures the nested entities, such as a constituency graph or various modifications of a directed hypergraph ().", "labels": [], "entities": [{"text": "nested named entity recognition", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7268486022949219}, {"text": "LSTM-CRF NE recognizer", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.6112798849741617}]}, {"text": "We propose two completely neural network architectures for nested nested named entity recognition which do not explicitly build or model any structure and infer the relationships between nested NEs implicitly: \u2022 In the first model, we concatenate the nested entity multiple labels into one multilabel, which is then predicted with a standard LSTM-CRF () model.", "labels": [], "entities": [{"text": "nested nested named entity recognition", "start_pos": 59, "end_pos": 97, "type": "TASK", "confidence": 0.8195449829101562}]}, {"text": "The advantages of this model are simplicity and effectiveness, because an already existing NE pipeline can be reused to model the nested entities.", "labels": [], "entities": []}, {"text": "The obvious disadvantage is a large growth of NE classes.", "labels": [], "entities": []}, {"text": "\u2022 In the second model, the nested entities are encoded in a sequence and then the task can be viewed as a sequence-to-sequence (seq2seq) task, in which the input sequence are the tokens (forms) and the output sequence are the labels.", "labels": [], "entities": []}, {"text": "The decoder predicts labels for each token, until a special label \"<eow>\" (end of word) is predicted and the decoder moves to the next token.", "labels": [], "entities": []}, {"text": "The expressiveness of the models depends on a non-ambiguous encoding of the nested entity structure.", "labels": [], "entities": []}, {"text": "We use an enhanced BILOU scheme described in Section 4.1.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9824334383010864}]}, {"text": "The proposed models surpass the current nested NER state of the art on four nested entity corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC.", "labels": [], "entities": [{"text": "ACE-2004", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9226599931716919}, {"text": "ACE-2005", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.8274456262588501}, {"text": "GENIA", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.8899595141410828}, {"text": "Czech CNEC", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.9210879504680634}]}, {"text": "When the recently introduced contextual embeddings -ELMo (), BERT (Devlin et al., 2018) and -are added to the architecture, we reach further improvements for the above mentioned nested entity corpora and also exceed current state of the art for 2 Related Work explicitly model the nested structure as a syntactic constituency tree.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.8479979634284973}, {"text": "BERT", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9981399774551392}]}, {"text": "run a stacked LSTM-CRF NE recognizer as long as at least one nested entity is predicted, from innermost to outermost entities.", "labels": [], "entities": [{"text": "LSTM-CRF NE recognizer", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.5435691575209299}]}, {"text": "build a hypergraph to capture all possible entity mentions in a sentence.", "labels": [], "entities": []}, {"text": "model nested entities as a directed hypergraph similar to, using RNNs to model the edge probabilities.", "labels": [], "entities": []}, {"text": "Our proposed architectures are different from these works because they do not explicitly build any structure to model the nested entities.", "labels": [], "entities": []}, {"text": "The nested entity structure is instead encoded as a sequence of labels, and the artificial neural network is supposed to model the structural relationships between the named entities implicitly.", "labels": [], "entities": []}, {"text": "A sequence-to-sequence architecture similar to one of our approaches is used by ( to predict the hierarchy of constituents in order to extract lookahead features fora shift-reduce constituency parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our results on four nested NE corpora: \u2022 English ACE-2004, (Doddington et al., 2004) 2 . We reuse the train/dev/test split used by most previous authors ().", "labels": [], "entities": [{"text": "English ACE-2004", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.8616685569286346}]}, {"text": "\u2022 English ACE-2005 3 . Again, we use the train/dev/test split by;;.", "labels": [], "entities": [{"text": "English ACE-2005", "start_pos": 2, "end_pos": 18, "type": "DATASET", "confidence": 0.8588331639766693}]}, {"text": "\u2022 English GENIA ( and).", "labels": [], "entities": [{"text": "English GENIA", "start_pos": 2, "end_pos": 15, "type": "DATASET", "confidence": 0.6682108342647552}]}, {"text": "In all cases, we use the train portion of the data for training and the development portion for hyperparameter tuning, and we report our final results on models trained on concatenated train+dev portions and evaluated on the test portion, following e.g. (. Our evaluation is a strict one: each entity mention is considered correct only when both the span and class are correct.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Nested NER results (F1) for ACE-2004, ACE-2005, GENIA and CNEC 1.0 (Czech) corpora. Bold  indicates the best result, italics results above SoTA and gray background indicates the main contribution. * uses  different data split in ACE-2005. ** non-neural model", "labels": [], "entities": [{"text": "Nested NER", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.7017977237701416}, {"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.7533077597618103}, {"text": "ACE-2004", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9333285093307495}, {"text": "ACE-2005", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.8609315752983093}, {"text": "GENIA", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9043209552764893}]}, {"text": " Table 2: Flat NER results (F1) for CoNLL-2002 and CoNLL-2003. Bold indicates best result, italics results above  SoTA.", "labels": [], "entities": [{"text": "NER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.5921645760536194}, {"text": "F1)", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9728996157646179}, {"text": "CoNLL-2002", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9722503423690796}, {"text": "CoNLL-2003", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.947274386882782}, {"text": "SoTA", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.7674633264541626}]}]}