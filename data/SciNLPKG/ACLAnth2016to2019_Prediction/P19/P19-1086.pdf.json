{"title": [{"text": "SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference", "labels": [], "entities": [{"text": "Evaluating Natural Language Inference", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.7875326573848724}]}], "abstractContent": [{"text": "We present SherLIiC, 1 a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k un-labeled InfCands, and (ii) ~190k typed tex-tual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09.", "labels": [], "entities": []}, {"text": "Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placehold-ers, each linked to one or more Freebase types.", "labels": [], "entities": []}, {"text": "Due to our candidate selection process based on strong distributional evidence, SherLIiC is much harder than existing testbeds because distributional evidence is of little utility in the classification of InfCands.", "labels": [], "entities": []}, {"text": "We also show that, due to its construction, many of SherLIiC's correct InfCands are novel and missing from existing rule bases.", "labels": [], "entities": []}, {"text": "We evaluate a number of strong baselines on SherLIiC, ranging from semantic vector space models to state of the art neural models of natural language inference (NLI).", "labels": [], "entities": []}, {"text": "We show that SherLIiC poses a tough challenge to existing NLI systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical inference (LI) can be seen as a focused variant of natural language inference (NLI), also called recognizing textual entailment (.", "labels": [], "entities": [{"text": "Lexical inference (LI)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8137705266475678}]}, {"text": "Recently, showed that annotation artifacts in current NLI testbeds distort our impression of the performance of state of the art systems, giving rise to the need for new evaluation methods for NLI.", "labels": [], "entities": []}, {"text": "investigated LI as away of evaluating NLI systems and found that even simple cases are challenging to current systems.", "labels": [], "entities": []}, {"text": "In this paper, we release SherLIiC, a testbed specifically designed for evaluating a system's ability to solve the hard problem of modeling lexical entailment in context.", "labels": [], "entities": []}, {"text": "https://github.com/mnschmit/SherLIiC (1) troponymy ORGF is granting to EMPL \u21d2 ORGF is giving to EMPL (2) synonymy + ORGF is supporter of Levy and Dagan (2016) identified contextsensitive -as opposed to \"context-free\" -entailment as an important evaluation criterion and created a dataset for LI in context (LIiC).", "labels": [], "entities": []}, {"text": "In their data, WordNet) synsets serve as context for one side of a binary relation, but the other side is still instantiated with a single concrete expression.", "labels": [], "entities": []}, {"text": "We aim to improve this setting in two ways.", "labels": [], "entities": []}, {"text": "First, we type our relations on both sides, thus making them more general.", "labels": [], "entities": []}, {"text": "Types provide a context that can help in disambiguation and at the same time allow generalization over contexts because arguments of the same type are represented abstractly.", "labels": [], "entities": []}, {"text": "An example for the need for disambiguation is the verb \"run\".", "labels": [], "entities": []}, {"text": "\"run\" entails \"lead\" in the context of PERSON / COMPANY (\"Bezos runs Amazon\").", "labels": [], "entities": []}, {"text": "But in the context of COMPUTER / SOFTWARE, \"run\" entails \"execute\"/\"use\" (\"my mac runs macOS\").", "labels": [], "entities": []}, {"text": "Here, types help find the right interpretation.", "labels": [], "entities": []}, {"text": "Second, we only consider relations between named entities (NEs).", "labels": [], "entities": []}, {"text": "Inference mining based on non-NE types such as WordNet synsets (e.g., ANI-MAL, PLANT LIFE) primarily discovers facts like \"parrotfish feed on algae\".", "labels": [], "entities": [{"text": "Inference mining", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8814996778964996}]}, {"text": "In contrast, the focus on NEs makes it more likely that we will capture events like \"Walmart closes gap with Amazon\" and thus knowledge about event entailment like [\"A is closing gap with B\" \u21d2 \"B is having lead over A\"] that is substantially different from knowledge about general facts.", "labels": [], "entities": [{"text": "NEs", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.965969979763031}]}, {"text": "In more detail, we create SherLIiC as follows.", "labels": [], "entities": []}, {"text": "First, we extract verbal relations between Freebase () entities from the entitylinked web corpus ClueWeb09 ().", "labels": [], "entities": []}, {"text": "We then divide these relations into typable subrelations based on the most frequent Freebase types found in their extensions.", "labels": [], "entities": []}, {"text": "We then create a large set of inference rule candidates (InfCands), i.e., premise-hypothesis-pairs of verbally expressed relations.", "labels": [], "entities": []}, {"text": "Finally, we use Amazon Mechanical Turk to classify each InfCand in a randomly sampled subset as entailment or non-entailment.", "labels": [], "entities": []}, {"text": "In summary, our contributions are the following: (1) We create SherLIiC, anew resource for LIiC, consisting of 3985 manually annotated InfCands.", "labels": [], "entities": []}, {"text": "Additionally, we provide ~960k unlabeled InfCands (SherLIiC-InfCands), and the typed event graph SherLIiC-TEG, containing ~190k typed textual binary relations between Freebase entities.", "labels": [], "entities": []}, {"text": "(2) SherLIiC is harder than existing testbeds because distributional evidence is of limited utility in the classification of InfCands.", "labels": [], "entities": [{"text": "SherLIiC", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.6540516018867493}]}, {"text": "Thus, SherLIiC is a promising and challenging resource for developing NLI systems that go beyond shallow semantics.", "labels": [], "entities": []}, {"text": "(3) Human-interpretable knowledge graph types serve as context for both sides of InfCands.", "labels": [], "entities": []}, {"text": "This makes InfCands more general and boosts the number of event-like relations in SherLIiC.", "labels": [], "entities": []}, {"text": "(4) SherLIiC is complementary to existing collections of inference rules as evidenced by the low recall these resources achieve (cf.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9949892163276672}]}], "datasetContent": [{"text": "summarizes the performance of our baselines on predicting the entailment class for SherLIiC-dev and -test.", "labels": [], "entities": []}, {"text": "Rule collections (lines 1-6) have recall between 0.119 and 0.308; the recall of their union (line 7) is only 0.483 on dev and 0.493 on test, showing that we found indeed new valid inferences missing from existing rule bases.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9984405636787415}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9985260963439941}]}, {"text": "The state-of-the-art neural model ESIM does not generalize well from MultiNLI (its training set) to LIiC.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.8936578631401062}]}, {"text": "In fact, it hardly improves on the baseline that always predicts entailment (Always yes).", "labels": [], "entities": []}, {"text": "Our dataset was specifically designed to only contain good InfCands based on distributional features.", "labels": [], "entities": []}, {"text": "So it poses a challenge to models that cannot make the fine semantic distinctions necessary for LIiC.", "labels": [], "entities": []}, {"text": "Turning to vector space models (lines 11-24), dense relation representations (lines 12, 13) predict entailment better than sparse models (lines  information for the task.", "labels": [], "entities": []}, {"text": "These methods were not developed to compare relations; the lack of useful information is still surprising and thus is a promising direction for future work on KG embeddings.", "labels": [], "entities": []}, {"text": "General purpose dense representations (word2vec, line 11) perform comparatively well, showing that, in principle, they cover the information necessary for LIiC.", "labels": [], "entities": []}, {"text": "Embeddings trained on our relation extensions SherLIiC-TEG (line 13), however, can already alone achieve better performance than word2vec embeddings alone.", "labels": [], "entities": []}, {"text": "In general, type-informed relation embeddings seem to have a disadvantage compared to unrestricted ones (e.g., cf. lines 12 and 13) -presumably because type-informed baselines have training sets that are smaller (due to filtering) and sparser (since relations are split up according to type signatures).", "labels": [], "entities": []}, {"text": "The combination of general word2vec and specialized relation embeddings (lines 14-16), however, consistently brings gains.", "labels": [], "entities": []}, {"text": "This indicates that distributional word properties are complementary to the relation extensions our method extracts.", "labels": [], "entities": []}, {"text": "So using both sources of information is promising for future research on modeling relational semantics.", "labels": [], "entities": []}, {"text": "rel emb is the best-performing method.", "labels": [], "entities": [{"text": "rel emb", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7595929801464081}]}, {"text": "It combines typed and untyped relation embeddings as well as general-purpose word2vec embeddings.", "labels": [], "entities": []}, {"text": "Even though one cannot rely on typed extensions only, this shows that incorporating type information is beneficial for good performance.", "labels": [], "entities": []}, {"text": "We use w2v+tsg rel emb to provide a noisy annotation for SherLIiC-InfCands.", "labels": [], "entities": [{"text": "SherLIiC-InfCands", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.8699877262115479}]}, {"text": "This is a useful resource because learning from noisy labels has been well studied and is often beneficial.", "labels": [], "entities": []}, {"text": "Although SherLIiC's creation is based on the same method that was used to create Schoenmackers, SherLIiC is fundamentally different for several reasons: (1) The rule sets are complementary (cf. the low recall of 0.139 and 0.119 in).", "labels": [], "entities": [{"text": "recall", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.9895690083503723}]}, {"text": "(2) The majority of rules in Schoenmackers has more than one premise, leaving only ~13k InfCands in Schoenmackers compared to ~960k in SherLIiC-InfCands that fit the format of NLI.", "labels": [], "entities": []}, {"text": "(3) Schoenmackers is filtered more aggressively with the goal of maximizing the number of correct rules.", "labels": [], "entities": []}, {"text": "This, however, makes it inadequate as a challenging benchmark be-  cause the performance of Always yes would be close to 100%.", "labels": [], "entities": []}, {"text": "(4) SherLIiC is focused on events.", "labels": [], "entities": []}, {"text": "When linking the relations from SherLIiC-TEG back to their surface forms in the corpus, 80% of them occur at least once in the progressive, which suggests that the large majority of our relations indeed represent events.", "labels": [], "entities": []}, {"text": "Taking a closer look at SherLIiC, we see that the data require a large variety of lexical knowledge even though their creation has been entirely automatic.", "labels": [], "entities": []}, {"text": "shows five positively labeled examples from SherLIiC-dev, each highlighting a different challenge for statistical models that is crucial for NLI.", "labels": [], "entities": []}, {"text": "(1) is an instance of troponymy: \"granting\" is a manner or kind of \"giving\".", "labels": [], "entities": []}, {"text": "This is the verbal equivalent to nominal hyponymy.", "labels": [], "entities": []}, {"text": "(2) combines synonymy (\"support\" \u21d4 \"back\") with morph. derivation.", "labels": [], "entities": []}, {"text": "(3) can only be classified correctly if one knows that it is one of the typical actions of a president to represent their country.", "labels": [], "entities": []}, {"text": "(4) requires knowledge about the typical course of events when interviewing someone.", "labels": [], "entities": []}, {"text": "A typical interview involves asking questions.", "labels": [], "entities": []}, {"text": "(5) can only be detected with commonsense knowledge that goes even beyond that: you generally only claim something if you want it.", "labels": [], "entities": []}, {"text": "An error analysis of the three best-performing baselines (lines 14-16 in) reveals that none of them was able to detect the five correct InfCands from.", "labels": [], "entities": [{"text": "InfCands", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.889316976070404}]}, {"text": "Explicit modeling of one of the phenomena described above seems a promising direction for future research to improve recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.978099524974823}]}, {"text": "shows five cases where InfCands were incorrectly labeled as entailment.", "labels": [], "entities": []}, {"text": "shows the importance of modeling directionality: every \"dictator\" is a \"ruler\" but not vice versa.", "labels": [], "entities": []}, {"text": "The other examples show other types of correlation that models relying entirely on distributional information will fall for: the outcome of events like \"coming into a country\" or \"seeking something from someone\" are in general uncertain although possible outcomes like \"remaining in said country\" (3) or \"being given the object of desire\" (4) will be highly correlated with them.", "labels": [], "entities": []}, {"text": "Finally, better models will also take into account the simultaneity constraint: \"winning a war\" and \"declaring a war\" (5) rarely happen at the same time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics for crowd-annotated InfCands. The  gold label is the majority label.", "labels": [], "entities": []}, {"text": " Table 3: Precision, recall and F1 score on SherLIiC-dev and -test. All baselines run on top of Lemma. Thresholds  (\u03b8  *  ) are F1-optimized on dev. Best result per column is set in bold.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9989879727363586}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9991727471351624}, {"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9789667129516602}, {"text": "Thresholds", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9841462969779968}, {"text": "F1-optimized", "start_pos": 128, "end_pos": 140, "type": "METRIC", "confidence": 0.9613576531410217}]}]}