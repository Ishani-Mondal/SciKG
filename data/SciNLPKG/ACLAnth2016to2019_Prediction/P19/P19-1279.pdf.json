{"title": [{"text": "Matching the Blanks: Distributional Similarity for Relation Learning", "labels": [], "entities": [{"text": "Matching the Blanks", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8726621667544047}, {"text": "Relation Learning", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.9546011090278625}]}], "abstractContent": [{"text": "General purpose relation extractors, which can model arbitrary relations, area core aspiration in information extraction.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7699586749076843}, {"text": "information extraction", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8475234508514404}]}, {"text": "Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph.", "labels": [], "entities": []}, {"text": "However , both of these approaches are limited in their ability to generalize.", "labels": [], "entities": []}, {"text": "In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specif-ically, BERT), to build task agnostic relation representations solely from entity-linked text.", "labels": [], "entities": [{"text": "BERT", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.994051992893219}]}, {"text": "We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data.", "labels": [], "entities": [{"text": "exemplar based relation extraction", "start_pos": 77, "end_pos": 111, "type": "TASK", "confidence": 0.5946061387658119}]}, {"text": "We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on Se-mEval 2010 Task 8, KBP37, and TACRED.", "labels": [], "entities": [{"text": "KBP37", "start_pos": 207, "end_pos": 212, "type": "DATASET", "confidence": 0.9249247908592224}]}], "introductionContent": [{"text": "Reading text to identify and extract relations between entities has been along standing goal in natural language processing.", "labels": [], "entities": []}, {"text": "Typically efforts in relation extraction fall into one of three groups.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.9108620584011078}]}, {"text": "Ina first group, supervised), or distantly supervised relation extractors () learn a mapping from text to relations in a limited schema.", "labels": [], "entities": []}, {"text": "Forming a second group, open information extraction removes the limitations of a predefined schema by instead representing relations using their surface forms (, which increases scope but also leads * Work done as part of the Google AI residency.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6510599354902903}]}, {"text": "to an associated lack of generality since many surface forms can express the same relation.", "labels": [], "entities": []}, {"text": "Finally, the universal schema () embraces both the diversity of text, and the concise nature of schematic relations, to build a joint representation that has been extended to arbitrary textual input (, and arbitrary entity pairs.", "labels": [], "entities": []}, {"text": "However, like distantly supervised relation extractors, universal schema rely on large knowledge graphs (typically Freebase () that can be aligned to text.", "labels": [], "entities": []}, {"text": "Building on's extension of Harris' distributional hypothesis to relations, as well as recent advances in learning word representations from observations of their contexts (, we propose anew method of learning relation representations directly from text.", "labels": [], "entities": []}, {"text": "First, we study the ability of the Transformer neural network architecture ( to encode relations between entity pairs, and we identify a method of representation that outperforms previous work in supervised relation extraction.", "labels": [], "entities": [{"text": "supervised relation extraction", "start_pos": 196, "end_pos": 226, "type": "TASK", "confidence": 0.6553047398726145}]}, {"text": "Then, we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks.", "labels": [], "entities": []}, {"text": "[BLANK], inspired by Cale's earlier cover, recorded one of the most acclaimed versions of \"\"'s rendition of \"\" has been called \"one of the great songs\" by Time, and is included on Rolling Stone's list of \"The 500 Greatest Songs of All Time\".", "labels": [], "entities": [{"text": "BLANK", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9784418940544128}]}, {"text": "Following, we assume access to a corpus of text in which entities have been linked to unique identifiers and we define a relation statement to be a block of text containing two marked entities.", "labels": [], "entities": []}, {"text": "From this, we create training data that contains relation statements in which the entities have been replaced with a special symbol, as illustrated in.", "labels": [], "entities": []}, {"text": "Our training procedure takes in pairs of blank-containing relation statements, and has an objective that encourages relation representations to be similar if they range over the same pairs of entities.", "labels": [], "entities": []}, {"text": "After training, we employ learned relation representations to the recently released FewRel task () in which specific relations, such as 'original language of work' are represented with a few exemplars, such as The Crowd (Italian: La Folla) is a 1951 Italian film.", "labels": [], "entities": []}, {"text": "presented FewRel as a supervised dataset, intended to evaluate models' ability to adapt to relations from new domains attest time.", "labels": [], "entities": [{"text": "FewRel", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.9012793302536011}]}, {"text": "We show that through training by matching the blanks, we can outperform's top performance on FewRel, without having seen any of the FewRel training data.", "labels": [], "entities": [{"text": "FewRel", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.9606893658638}, {"text": "FewRel training data", "start_pos": 132, "end_pos": 152, "type": "DATASET", "confidence": 0.9386027057965597}]}, {"text": "We also show that a model pre-trained by matching the blanks and tuned on FewRel outperforms humans on the FewRel evaluation.", "labels": [], "entities": [{"text": "FewRel", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.9716373682022095}, {"text": "FewRel evaluation", "start_pos": 107, "end_pos": 124, "type": "DATASET", "confidence": 0.9626704454421997}]}, {"text": "Similarly, by training by matching the blanks and then tuning on labeled data, we significantly improve performance on the), KBP-37 (, and TACRED () relation extraction benchmarks.", "labels": [], "entities": [{"text": "KBP-37", "start_pos": 125, "end_pos": 131, "type": "DATASET", "confidence": 0.6875266432762146}, {"text": "TACRED", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9164090752601624}, {"text": "relation extraction", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.626536414027214}]}], "datasetContent": [{"text": "In this section, we evaluate the impact of training by matching the blanks.", "labels": [], "entities": []}, {"text": "We start with the best BERT based model from Section 3.3, which we call BERT EM , and we compare this to a variant that is trained with the matching the blanks task (BERT EM +MTB We report results on all of the tasks from Section 3.1, using the same task-specific training methodology for both BERT EM and BERT EM +MTB.", "labels": [], "entities": [{"text": "BERT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9251536726951599}, {"text": "BERT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9856461882591248}, {"text": "BERT", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9487786293029785}, {"text": "BERT", "start_pos": 294, "end_pos": 298, "type": "METRIC", "confidence": 0.848158597946167}]}], "tableCaptions": [{"text": " Table 1: Results for supervised relation extraction tasks. Results on rows where the model name is marked with  a * symbol are reported as published, all other numbers have been computed by us. SemEval 2010 Task 8 does  not establish a default split for development; for this work we use a random slice of the training set with 1,500  examples.", "labels": [], "entities": [{"text": "supervised relation extraction tasks", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.6839745119214058}, {"text": "SemEval 2010 Task 8", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.6889858841896057}]}, {"text": " Table 3: Test results for FewRel few-shot relation clas- sification task. Proto Net is the best published sys- tem from", "labels": [], "entities": [{"text": "Proto Net", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.9593307971954346}]}, {"text": " Table 4: F1 scores of BERT EM +MTB and BERT EM  based relation classifiers on the respective test sets. De- tails of the SOTA systems are given in Table 1.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993916749954224}, {"text": "BERT EM +MTB", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.891375258564949}, {"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9871317148208618}]}, {"text": " Table 5: F1 scores on development sets for supervised  relation extraction tasks while varying the amount of  tuning data available to our BERT EM and BERT EM +MTB  models.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9992504715919495}, {"text": "supervised  relation extraction tasks", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.6843197420239449}, {"text": "BERT", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9581466317176819}, {"text": "BERT", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.8234160542488098}]}]}