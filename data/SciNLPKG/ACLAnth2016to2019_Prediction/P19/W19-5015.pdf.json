{"title": [{"text": "A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributed representations of text can be used as features when training a statistical classi-fier.", "labels": [], "entities": []}, {"text": "These representations maybe created as a composition of word vectors or as context-based sentence vectors.", "labels": [], "entities": []}, {"text": "We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification.", "labels": [], "entities": [{"text": "influenza infection classification", "start_pos": 101, "end_pos": 135, "type": "TASK", "confidence": 0.6403102974096934}, {"text": "drug usage classification", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.610763688882192}, {"text": "personal health mention classification", "start_pos": 167, "end_pos": 205, "type": "TASK", "confidence": 0.6610339730978012}]}, {"text": "For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontol-ogy.", "labels": [], "entities": [{"text": "FLAIR", "start_pos": 167, "end_pos": 172, "type": "METRIC", "confidence": 0.9578888416290283}, {"text": "Word2Vec", "start_pos": 189, "end_pos": 197, "type": "DATASET", "confidence": 0.9530078768730164}]}, {"text": "There is an improvement of 2-4% in the accuracy when these context-based representations are used instead of word-based representations .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9995307922363281}]}], "introductionContent": [{"text": "Distributed representations (also known as 'embeddings') are dense, real-valued vectors that capture semantics of concepts (.", "labels": [], "entities": []}, {"text": "When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words.", "labels": [], "entities": []}, {"text": "When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit.", "labels": [], "entities": []}, {"text": "This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems ( and biomedical NLP problems (.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.8231877982616425}]}, {"text": "In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and personal health mention classification.", "labels": [], "entities": [{"text": "influenza infection classification", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.6393250922362009}, {"text": "drug usage classification", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.5847420394420624}, {"text": "personal health mention classification", "start_pos": 153, "end_pos": 191, "type": "TASK", "confidence": 0.6551699042320251}]}, {"text": "We use statistical classifiers trained on tweet vectors as features.", "labels": [], "entities": []}, {"text": "To compute a tweet vector, i.e., a distributed representation for tweets, typical alternatives are: (a) tweet vector as a function of word embeddings of the content words 1 in the tweet; or, (b) a contextualised representation that computes sentence vectors using language models.", "labels": [], "entities": []}, {"text": "The former considers meanings of words in isolation, while the latter takes into account the order of these words in addition to their meaning.", "labels": [], "entities": []}, {"text": "We compare wordbased and context-based representations for the three classification problems.", "labels": [], "entities": []}, {"text": "This paper investigates the question: 'When statistical classifiers are trained on vectors of tweets for health informatics, how should the vector be computed: using word-based representations that consider words in isolation or contextbased representations that account for word order using language models?'", "labels": [], "entities": []}, {"text": "For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments on three boolean classification problems in health informatics: (A) Influenza Infection Classification (IIC): The goal is to predict if a tweet reports an influenza infection ('I have been coughing all day', for example) or describes information about influenza ('flu outbreaks are common in this month of the year', for example).", "labels": [], "entities": [{"text": "boolean classification", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6365383416414261}, {"text": "Influenza Infection Classification (IIC)", "start_pos": 95, "end_pos": 135, "type": "TASK", "confidence": 0.788428525129954}]}, {"text": "We use the dataset presented in   detect whether or not a tweet describes the usage of a medicinal drug ('I took some painkillers this morning', for example).", "labels": [], "entities": []}, {"text": "We use the dataset provided by; (C) Personal Health Mention classification (PHMC): A personal health mention is a person's report about their illness.", "labels": [], "entities": [{"text": "Personal Health Mention classification (PHMC)", "start_pos": 36, "end_pos": 81, "type": "TASK", "confidence": 0.5497489741870335}]}, {"text": "We use the dataset provided by.", "labels": [], "entities": []}, {"text": "For example 'I have been sick fora week now' is a personal health mention while 'Rollercoasters can make you sick' is not.", "labels": [], "entities": []}, {"text": "It must be noted that IIC involves influenza while the PHMC dataset covers a set of illnesses as described later.", "labels": [], "entities": [{"text": "IIC", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9869298934936523}, {"text": "PHMC dataset", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9679653346538544}]}, {"text": "The datasets for each of the classification problems consist of tweets that have been manually annotated as reported in the corresponding papers.", "labels": [], "entities": []}, {"text": "The statistics of these datasets are shown in.", "labels": [], "entities": []}, {"text": "The values in brackets indicate the number of true tweets (i.e., tweets that have been labeled as true), since these are boolean classification problems.", "labels": [], "entities": []}, {"text": "For details on inter-annotator agreement and the annotation techniques, we refer the reader to the original papers.", "labels": [], "entities": []}, {"text": "Based on sentence vectors obtained using either word-based or context-based representations, we train logistic regression with default parameters available as apart of the Liblinear package).", "labels": [], "entities": []}, {"text": "We report fivefold cross-validation results for our experiments.", "labels": [], "entities": []}, {"text": "Each fold is created using stratified k-fold sampling available in scikit-learn 4 .  We compare word-based and context-based representations for the three classification problems in.", "labels": [], "entities": []}, {"text": "Accuracy is computed as the proportion of correctly classified instances.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9954822063446045}]}, {"text": "The table contains the average accuracy values with standard deviation values shown in parentheses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9878888726234436}]}, {"text": "The table is divided into two parts.", "labels": [], "entities": []}, {"text": "Part (A) corresponds to experiments using word-based representations, while Part (B) corresponds to those using context-based representations.", "labels": [], "entities": []}, {"text": "In general, contextbased representations result in an improvement in the three classification problems as compared to word-based representations.", "labels": [], "entities": []}, {"text": "For IIC, the best word-based representation is when pre-trained Word2Vec embeddings (W ord2V ec P reT rain) of content words are averaged to generate the tweet vector.", "labels": [], "entities": [{"text": "IIC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9128556847572327}]}, {"text": "The accuracy in this case is 0.8106.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997500777244568}]}, {"text": "In contrast, the best performing context-based representation is NNLM (0.8520).", "labels": [], "entities": [{"text": "NNLM", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9005566835403442}]}, {"text": "This is an improvement of 4% points.", "labels": [], "entities": []}, {"text": "Similarly, tweet vectors created using USE result in an accuracy of 0.7790 for DUC and 0.8155 for PHMC.", "labels": [], "entities": [{"text": "USE", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.4777100086212158}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9991760849952698}, {"text": "PHMC", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8467175960540771}]}, {"text": "This is an improvement of 2-4% points each over the wordbased representations for these two classification problems as well.", "labels": [], "entities": []}, {"text": "In addition, for pre-trained embeddings (Word2Vec and GloVe) retrofitted with a medical ontology (MeSH), we observe a degrada-  tion in the accuracy for IIC and PHMC, as compared to without retrofitting.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9727510213851929}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9995296001434326}]}, {"text": "There is an improvement of 1% point in the case of DUC.", "labels": [], "entities": [{"text": "DUC", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.7945644855499268}]}, {"text": "Similarly, learning the embeddings on the specific training corpus does notwork well.", "labels": [], "entities": []}, {"text": "It leads to a degradation as compared to pre-trained embeddings.", "labels": [], "entities": []}, {"text": "This could happen because pre-trained embeddings are trained on much larger corpora than our training datasets, thereby capturing semantics more effectively than the Word2Vec SelfTrain variant.", "labels": [], "entities": [{"text": "Word2Vec SelfTrain", "start_pos": 166, "end_pos": 184, "type": "DATASET", "confidence": 0.8589340448379517}]}, {"text": "For a qualitative comparison of the two representations, we analyse 100 randomly sampled instances that are mis-classified by each classifier.", "labels": [], "entities": []}, {"text": "While these instances need not be the same for each classifier, the trends in the errors show where one kind of representation scores over the other.", "labels": [], "entities": []}, {"text": "We compared linguistic properties of these mis-classified instances, such as the person, tense and number.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparison of five word-based representations with four context-based representations; Average accuracy  with standard deviation (\u03c3) indicated in brackets.", "labels": [], "entities": [{"text": "Average", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9379101395606995}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9034292697906494}, {"text": "standard deviation (\u03c3)", "start_pos": 120, "end_pos": 142, "type": "METRIC", "confidence": 0.8974274873733521}]}, {"text": " Table 4: Average number of instances (out of 100  randomly sampled mis-classified instances) containing  first-person mentions and present participle form for  the three classification problems and two types of rep- resentations.", "labels": [], "entities": []}]}