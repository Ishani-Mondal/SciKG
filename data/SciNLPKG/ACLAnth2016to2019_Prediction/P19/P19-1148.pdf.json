{"title": [{"text": "Exact Hard Monotonic Attention for Character-Level Transduction", "labels": [], "entities": [{"text": "Character-Level Transduction", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6501204371452332}]}], "abstractContent": [{"text": "Many common character-level, string-to-string transduction tasks, e.g. grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.7469463348388672}]}, {"text": "Neural sequence-to-sequence models with soft attention, which are non-monotonic, often outperform popular monotonic models.", "labels": [], "entities": []}, {"text": "In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks?", "labels": [], "entities": []}, {"text": "We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns a latent alignment jointly while learning to transduce.", "labels": [], "entities": []}, {"text": "With the help of dynamic programming , we are able to compute the exact marginalization overall monotonic alignments.", "labels": [], "entities": []}, {"text": "Our models achieve state-of-the-art performance on morphological inflection.", "labels": [], "entities": []}, {"text": "Furthermore , we find strong performance on two other character-level transduction tasks.", "labels": [], "entities": []}, {"text": "Code is available at https://github.com/ shijie-wu/neural-transducer.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many tasks in natural language can be treated as character-level, string-to-string transduction.", "labels": [], "entities": []}, {"text": "The current dominant method is the neural sequenceto-sequence model with soft attention (.", "labels": [], "entities": []}, {"text": "This method has achieved state-of-the-art results in a plethora of tasks, for example, grapheme-to-phoneme conversion (, named-entity transliteration ( and morphological inflection generation ( . While soft attention is very similar to a traditional alignment between the source characters and target characters in some regards, it does not explicitly a distribution over alignments.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 87, "end_pos": 117, "type": "TASK", "confidence": 0.7067482769489288}, {"text": "named-entity transliteration", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.6446122974157333}, {"text": "morphological inflection generation", "start_pos": 156, "end_pos": 191, "type": "TASK", "confidence": 0.7100282112757365}]}, {"text": "On the other hand, neural sequence-to-sequence models with hard alignment ( are analogous to the latent alignment in the classic IBM models for machine translation, which do model the alignment distribution explicitly.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7433324754238129}]}, {"text": "The standard versions of both soft and hard attention are non-monotonic.", "labels": [], "entities": []}, {"text": "However, if we look at the data in grapheme-to-phoneme conversion, named-entity transliteration, and morphological inflection-examples are shown in-we see that the tasks require almost exclusively monotonic transduction.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.7399642467498779}, {"text": "named-entity transliteration", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.6672539114952087}]}, {"text": "Yet, counterintuitively, the state of the art in high resource morphological inflection is held by non-monotonic models ()!", "labels": [], "entities": []}, {"text": "Indeed, in a recent controlled experiment, found non-monotonic models (with either soft attention or hard alignment) outperform popular monotonic models) in the three above mentioned tasks.", "labels": [], "entities": []}, {"text": "However, the inductive bias of monotonicity, if correct, should help learn a better model or, at least, learn the same model.", "labels": [], "entities": []}, {"text": "In this paper, we hypothesize that the underperformance of monotonic models stems from the lack of joint training of the alignments with the transduction.", "labels": [], "entities": []}, {"text": "Generalizing the model of to enforce monotonic alignments, we show that, for all three tasks considered, monotonicity is a good inductive bias and jointly learning a monotonic alignment improves performance.", "labels": [], "entities": []}, {"text": "We provide an exact, cubic-time, dynamic-programming inference algorithm to compute the log-likelihood and an approximate greedy decoding scheme.", "labels": [], "entities": []}, {"text": "Empirically, our results indicate that, rather than the pipeline systems of and, we should jointly train monotonic alignments with the transduction model, and, indeed, we set the single model state of the art on the task of morphological inflection.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider three character-level transduction tasks: grapheme-to-phoneme conversion, named-entity transliteration ( and morphological inflection in high-esource setting ().", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.7227964997291565}, {"text": "named-entity transliteration", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6555360406637192}]}, {"text": "We compare (i) soft attention without input-feeding (SOFT) (, (ii) 0 th -order hard attention (0-HARD) (Wu et al., 2018), (iii) 0 th -order monotonic hard attention (0-MONO) and (iv) 1 st -order monotonic hard attention (1-MONO).", "labels": [], "entities": []}, {"text": "The SOFT, 0-HARD and 0-MONO models have an identical number of parameters, but the 1-MONO has more.", "labels": [], "entities": []}, {"text": "All of them have approximately 8.6M parameters.", "labels": [], "entities": []}, {"text": "Experimental details and hyperparameters maybe found in App.", "labels": [], "entities": [{"text": "App.", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9714502990245819}]}, {"text": "A.  Finding #1: Morphological Inflection.", "labels": [], "entities": [{"text": "Morphological Inflection", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.8963914811611176}]}, {"text": "The first empirical finding in our study is that we achieve single-model, state-of-the-art performance on the CoNLL-SIGMORPHON 2017 shared task dataset.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2017 shared task dataset", "start_pos": 110, "end_pos": 151, "type": "DATASET", "confidence": 0.8757586479187012}]}, {"text": "The results are shown in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9789746403694153}]}, {"text": "2. We find that the 1-MONO ties with the 0-MONO system, indicating the additional parameters do not add much.", "labels": [], "entities": []}, {"text": "Both of these monotonic systems surpass the non-monotonic system 0-HARD and SOFT.", "labels": [], "entities": []}, {"text": "We also report comparison to other top systems at the task in Tab.", "labels": [], "entities": [{"text": "Tab", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9204474091529846}]}, {"text": "1. The previous state-of-the-art model,, is a non-monotonic system that outperformed the monotonic system of.", "labels": [], "entities": []}, {"text": "However,) is a pipeline system that took alignments from an existing aligner; such a system has no manner, by which it can recover from poor initial alignment.", "labels": [], "entities": []}, {"text": "We show that jointly learning monotonic alignments lead to improved results.", "labels": [], "entities": []}, {"text": "Finding #2: Effect of Strict Monotonicity.", "labels": [], "entities": []}, {"text": "The second finding is that by comparing SOFT, 0-HARD, 0-MONO in Tab.", "labels": [], "entities": [{"text": "SOFT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.883305013179779}]}, {"text": "2, we observe 0-MONO outperforms 0-HARD and 0-HARD in turns outperforms SOFT in all three tasks.", "labels": [], "entities": [{"text": "SOFT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9938966631889343}]}, {"text": "This shows that monotonicity should be enforced strictly since strict monotonicity does not hurt the model.", "labels": [], "entities": []}, {"text": "We contrast this to the findings of, who found the nonmonotonic models outperform the monotonic ones; this suggests strict monotonicity is more helpful when the model is allowed to learn the alignment distribution jointly.", "labels": [], "entities": []}, {"text": "We ask the authors of for the split data of grapheme-to-phoneme conversion) and NEWS 2015 shared task on named-entity transliteration.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.7445977628231049}, {"text": "NEWS 2015 shared task", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.8851479440927505}]}, {"text": "In named-entity transliteration, we only run experiments on 11 language pairs.", "labels": [], "entities": []}, {"text": "4 Grapheme-to-Phoneme Conversion is evaluated byword error rate (WER) and phoneme error rate (PER) (, where PER is the edit distance divided by the length of the phonemes.", "labels": [], "entities": [{"text": "Grapheme-to-Phoneme Conversion", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.6830859333276749}, {"text": "error rate (WER)", "start_pos": 53, "end_pos": 69, "type": "METRIC", "confidence": 0.9297784328460693}, {"text": "phoneme error rate (PER)", "start_pos": 74, "end_pos": 98, "type": "METRIC", "confidence": 0.8571990976730982}]}, {"text": "Named-entity transliteration is evaluated byword accuracy (ACC) and mean F-score (MFS) (.", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.9569308757781982}, {"text": "F-score (MFS)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9007114619016647}]}, {"text": "F-score is computed by LCS(c, r) = 1 2 (|c| + |r| \u2212 ED(c, r)) where r i and c i is the i-th reference and prediction and ED(c, r) is the edit distance between c and r.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9461638331413269}, {"text": "LCS", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.92948979139328}, {"text": "ED", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.995071291923523}]}, {"text": "Morphological inflection is evaluated byword accuracy (ACC) and average edit distance (MLD) (.", "labels": [], "entities": [{"text": "Morphological inflection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7914886474609375}, {"text": "accuracy (ACC)", "start_pos": 45, "end_pos": 59, "type": "METRIC", "confidence": 0.9588970243930817}, {"text": "average edit distance (MLD)", "start_pos": 64, "end_pos": 91, "type": "METRIC", "confidence": 0.9061968227227529}]}], "tableCaptions": [{"text": " Table 1: Average dev performance on morphological in- flection of our models against single models from the  2017 shared task. All systems are single model, i.e.,  without ensembling. Why dev? No participants submit- ted single-model systems for evaluation on test and the  best systems were not open-sourced, constraining our  comparison. Note we report numbers from their paper. 3", "labels": [], "entities": []}, {"text": " Table 2: Average test performance of namded-entity  transliteration (Trans), grapheme-to-phoneme conver- sion (G2P) and morphological inflection (MorInf).  First group has exactly same number of parameter  while the second group has slightly more parameter.  , \u00d7 and  \u2020 indicate statistical significant improvement  against SOFT, 0-HARD and 0-MONO on language-level  paired permutation test (p < 0.05).", "labels": [], "entities": []}]}