{"title": [{"text": "Better Character Language Modeling Through Morphology", "labels": [], "entities": [{"text": "Character Language Modeling", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.6553473472595215}]}], "abstractContent": [{"text": "We incorporate morphological supervision into character language models (CLMs) via multitasking and show that this addition improves bits-per-character (BPC) performance across 24 languages, even when the morphology data and language modeling data are dis-joint.", "labels": [], "entities": []}, {"text": "Analyzing the CLMs shows that inflected words benefit more from explicitly modeling morphology than uninflected words, and that morphological supervision improves performance even as the amount of language modeling data grows.", "labels": [], "entities": []}, {"text": "We then transfer morphological supervision across languages to improve language modeling performance in the low-resource setting.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7172562777996063}]}], "introductionContent": [{"text": "Character language models (CLMs) are distributions over sequences of characters, in contrast to traditional language models which are distributions over sequences of words.", "labels": [], "entities": []}, {"text": "CLMs eliminate the need fora fixed word vocabulary, and modeling text at the character level gives the CLM access to subword information.", "labels": [], "entities": []}, {"text": "These attributes suggest that CLMs can model regularities that exist within words, such as morphological inflection.", "labels": [], "entities": []}, {"text": "However, even large language modeling (LM) corpora have sparse coverage of inflected forms for morphologically-rich languages, which has been shown to make word and character language modeling more difficult.", "labels": [], "entities": [{"text": "word and character language modeling", "start_pos": 156, "end_pos": 192, "type": "TASK", "confidence": 0.630436360836029}]}, {"text": "Because to this, we hypothesize that accurately modeling morphology improves language modeling, but that it is difficult for CLMs to learn this from text alone.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7442874312400818}]}, {"text": "Motivated by this hypothesis, we add morphology supervision to character language modeling and show that, across two benchmark datasets, multitasking morphology with CLMs improves bits-per-character (BPC) performance on twentyfour languages, even when the annotated morphology features and language modeling data do not overlap.", "labels": [], "entities": [{"text": "character language modeling", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.6627288063367208}]}, {"text": "We also show that models augmented by multitasking achieve better BPC improvements on inflected forms than on uninflected forms, and that increasing the amount of language modeling data does not diminish the gains from morphology.", "labels": [], "entities": []}, {"text": "Furthermore, to augment morphology annotations in low-resource languages, we also transfer morphology information between pairs of highand low-resource languages.", "labels": [], "entities": []}, {"text": "In this cross-lingual setting, we see that morphology supervision from the high-resource language improves BPC performance on the low-resource language over both the low-resource multitask model and over adding language modeling data from the high-resource language alone.", "labels": [], "entities": [{"text": "BPC", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.8250976204872131}]}], "datasetContent": [{"text": "Datasets We obtain morphological annotations for 24 languages) from Universal Dependencies (UD; v.2.3), which consists of dependency parsing treebanks with morphology annotations on a large number of languages ().", "labels": [], "entities": [{"text": "Universal Dependencies (UD; v.2.3)", "start_pos": 68, "end_pos": 102, "type": "DATASET", "confidence": 0.7384315729141235}, {"text": "dependency parsing", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7303930222988129}]}, {"text": "These languages were chosen based on the size of their treebanks (to ensure a sufficient amount of morphology annotations); we also exclude languages that do not have morphology features annotated in the treebank.", "labels": [], "entities": []}, {"text": "For language modeling supervision, we train two sets of models.", "labels": [], "entities": [{"text": "language modeling supervision", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8865902225176493}]}, {"text": "One set is trained with the text from the UD treebanks; the other set of models is trained on the Multilingual Wikipedia Corpus (MWC) (.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9402666985988617}, {"text": "Multilingual Wikipedia Corpus (MWC)", "start_pos": 98, "end_pos": 133, "type": "DATASET", "confidence": 0.8128601511319479}]}, {"text": "This language modeling dataset consists of Wikipedia data across seven languages (Czech, German, English, Spanish, Finnish, French, and Russian).", "labels": [], "entities": []}, {"text": "Model architecture Our models each consist of a stacked LSTM with 1024 hidden dimensions and a character embedding layer of 512 dimensions.", "labels": [], "entities": []}, {"text": "We include two hidden layers in the language models trained on UD, and three hidden layers in those trained on MWC.", "labels": [], "entities": [{"text": "UD", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.6952025294303894}, {"text": "MWC", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9041424989700317}]}, {"text": "The parameters that integrate multitasking into the model (the layer at which we multitask morphology and the weighting we give the morphology losses, \u03b4) are tuned individually for each language.", "labels": [], "entities": []}, {"text": "Further hyperparameter and training details are given in the supplement.", "labels": [], "entities": []}, {"text": "Modeling Inflected Words We hypothesized that morphology supervision would be most beneficial to words whose form is dependent on their morphology, e.g. inflected words.", "labels": [], "entities": []}, {"text": "To investigate this, we calculate BPC of our UD models on inflected and uninflected forms in the UD development set.", "labels": [], "entities": [{"text": "BPC", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9971727132797241}, {"text": "UD development set", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.8507963617642721}]}, {"text": "We determine whether or not a word is inflected by comparing it to the (annotated) lemma given in the UD treebank.", "labels": [], "entities": [{"text": "UD treebank", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9132975339889526}]}, {"text": "We find that on 16 of the 24 languages for which we train models on UD, the MTL model improves more on inflected words than uninflected words, and that the average delta between LM and MTL models is 31% greater for inflected words than uninflected.", "labels": [], "entities": []}, {"text": "A comparison of the improvements in six of these languages are given in We then investigate how the amount of annotated morphology data affects performance on these models(b)).", "labels": [], "entities": []}, {"text": "We find that, as expected, increasing the amount of morphological data the language model is trained on improves BPC performance.", "labels": [], "entities": [{"text": "BPC", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9352703094482422}]}, {"text": "For both Czech and Russian, the MTL models mulitasked with 25% or more of the annotated data still outperform the LM baseline, but MTL models trained on smaller subsets of the morphology data performed worse than the baseline.", "labels": [], "entities": []}, {"text": "This is inline with our findings in Section 4 that the amount of annotated morphology data is closely tied with how much multitasking helps.", "labels": [], "entities": []}, {"text": "Cross-lingual Transfer In the previous section, we showed that the amount of training data (both for LM and for morphology) the CLM sees is crucial for better performance.", "labels": [], "entities": [{"text": "Cross-lingual Transfer", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7489512860774994}]}, {"text": "Motivated by this, we extend our models to the cross-lingual setting, in which we use data from high-resource languages to improve performance on closely-related, lowresource ones.", "labels": [], "entities": []}, {"text": "We train models on the (high, low) language pairs of (Russian, Ukrainian) and (Czech, Slovak) and transfer both LM and morphological supervision(c)).", "labels": [], "entities": [{"text": "LM", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.9417914748191833}]}, {"text": "We find the best performance for each low-resource language is achieved by using both the high-resource LM data and morphology annotations to augment the low-resource data.", "labels": [], "entities": []}, {"text": "In Slovak (SK), this gives us a 0.333 BPC improvement over the MTL model on SK data alone, and in Ukranian (UK), we see a improvement of 0.032 in this setting over the MTL trained only on UK.", "labels": [], "entities": [{"text": "BPC", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9834134578704834}, {"text": "Ukranian", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9005193710327148}]}, {"text": "The languages we use from Universal Dependencies and details about their treebanks are given in.", "labels": [], "entities": [{"text": "Universal Dependencies", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.8820636868476868}]}, {"text": "Most of the treebanks we used in this paper are manually annotated (and then possibly automatically converted to their current format), except for German, English, and French, which are automatically annotated.", "labels": [], "entities": []}, {"text": "For models trained in the   fully-supervised MTL setting where UD is used for both LM and morphology supervision, we calculate the character vocabulary for each language by including any character that occurs more than 5 times in the training set of the language's UD treebank.", "labels": [], "entities": [{"text": "UD treebank", "start_pos": 265, "end_pos": 276, "type": "DATASET", "confidence": 0.6910237520933151}]}, {"text": "Dataset statistics for the Multilingual Wikipedia Corpus (MWC) are given in.", "labels": [], "entities": [{"text": "Multilingual Wikipedia Corpus (MWC)", "start_pos": 27, "end_pos": 62, "type": "DATASET", "confidence": 0.8244134386380514}]}, {"text": "When analyzing the effect of LM training dataset size on Czech and Russian, we also train models on the training portion of a larger version of the MWC corpus, MWC-large, which contains approximately twice as much training data as the standard MWC dataset.", "labels": [], "entities": [{"text": "MWC corpus", "start_pos": 148, "end_pos": 158, "type": "DATASET", "confidence": 0.8383518159389496}, {"text": "MWC dataset", "start_pos": 244, "end_pos": 255, "type": "DATASET", "confidence": 0.7609816491603851}]}, {"text": "Specifically, MWC-large contains 10.2M training characters for Czech and 18.2M for Russian.", "labels": [], "entities": []}, {"text": "There is no prior work that we know of that reports BPC on this larger dataset.", "labels": [], "entities": [{"text": "BPC", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.37175774574279785}]}, {"text": "For models trained on the disjoint supervision setting, we use the character vocabulary provided for each language in the MWC dataset (see for preprocessing details).", "labels": [], "entities": [{"text": "MWC dataset", "start_pos": 122, "end_pos": 133, "type": "DATASET", "confidence": 0.8827527463436127}]}, {"text": "In cases where we use two sources of supervision for the model -LM supervision from MWC and morphology supervision from UD -we use the MWC character vocabulary for all inputs, so that BPC results across models are comparable.", "labels": [], "entities": []}, {"text": "This only affects a small number of the character types (11 or fewer for each language) in the UD training data.", "labels": [], "entities": [{"text": "UD training data", "start_pos": 95, "end_pos": 111, "type": "DATASET", "confidence": 0.6955748498439789}]}, {"text": "The character vocabulary provided in the MWC dataset and used for the distant supervision setting differs from the vocabulary calculated by including the characters that occur more than 25 times in the MWC training set.", "labels": [], "entities": [{"text": "MWC dataset", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9119465351104736}, {"text": "MWC training set", "start_pos": 202, "end_pos": 218, "type": "DATASET", "confidence": 0.8734155893325806}]}, {"text": "Because of this, our distant supervision setting on MWC is not comparable with, which uses the second vocabulary setting.", "labels": [], "entities": [{"text": "MWC", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8812352418899536}]}, {"text": "Therefore, we re-train our character LM baselines and multitasked models in this vocabulary setting).", "labels": [], "entities": []}, {"text": "We find that our LM and MTL models generally obtain slightly better performance on this setting, and we continue to see improvement from multitasking morphology over the character LM baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Multilingual Wikipedia Corpus (MWC) in bits per character (BPC). \u2206 calculated improvement  in BPC from the baseline LM to MTL. HCLM is the best model from Kawakami et al. (2017).", "labels": [], "entities": [{"text": "BPC", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.96302330493927}]}, {"text": " Table 2: BPC results on the Universal Dependencies  (UD) test set. %Infl is the inflection rate in each lan- guage. Languages are grouped by fusional, agglutina- tive, and introflexive typologies, respectively.", "labels": [], "entities": [{"text": "BPC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5284558534622192}, {"text": "Universal Dependencies  (UD) test set", "start_pos": 29, "end_pos": 66, "type": "DATASET", "confidence": 0.6289081828934806}, {"text": "Infl", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9705790281295776}, {"text": "inflection rate", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.9217934310436249}]}, {"text": " Table 3: (a) BPC on MWC development set with varied amounts of LM training data from MWC. The last line  is from training on MWC-large dataset, (b) BPC on MWC development set with varied amounts of supervised  morphology data from UD train set (compared against the baseline LM), and (c) Cross-lingual transfer on UD,  evaluated on low-resource language's development set: from Czech (CS; 6.9M characters in training set) to Slovak  (SK; 0.4M) and from Russian (RU; 5.3M) to Ukrainian (UK; 0.5M)", "labels": [], "entities": [{"text": "MWC development set", "start_pos": 21, "end_pos": 40, "type": "DATASET", "confidence": 0.7873595356941223}, {"text": "MWC-large dataset", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.9432404637336731}, {"text": "MWC development set", "start_pos": 156, "end_pos": 175, "type": "DATASET", "confidence": 0.8402509291966757}, {"text": "UD train set", "start_pos": 232, "end_pos": 244, "type": "DATASET", "confidence": 0.8433416883150736}]}, {"text": " Table 5: Dataset statistics for Multilingual Wikipedia  Corpus (MWC). Vocabulary size is based on the char- acter vocabulary given in (Kawakami et al., 2017).", "labels": [], "entities": [{"text": "Multilingual Wikipedia  Corpus (MWC)", "start_pos": 33, "end_pos": 69, "type": "DATASET", "confidence": 0.7445740352074305}]}, {"text": " Table 6: Results on Multilingual Wikipedia Corpus (MWC) in bits per character (BPC), trained on the vocabulary  from Mielke and Eisner (2019).", "labels": [], "entities": [{"text": "Multilingual Wikipedia Corpus (MWC)", "start_pos": 21, "end_pos": 56, "type": "DATASET", "confidence": 0.712605357170105}]}]}