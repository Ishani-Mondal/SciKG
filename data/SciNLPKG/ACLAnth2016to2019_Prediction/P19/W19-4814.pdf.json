{"title": [{"text": "On the Realization of Compositionality in Neural Networks", "labels": [], "entities": [{"text": "Realization of Compositionality in Neural Networks", "start_pos": 7, "end_pos": 57, "type": "TASK", "confidence": 0.6918802956740061}]}], "abstractContent": [{"text": "We present a detailed comparison of two types of sequence to sequence models trained to conduct a compositional task.", "labels": [], "entities": []}, {"text": "The models are architecturally identical at inference time, but differ in the way that they are trained: our baseline model is trained with a task-success signal only, while the other model receives additional supervision on its attention mechanism (Attentive Guidance), which has shown to bean effective method for encouraging more compositional solutions (Hupkes et al., 2019).", "labels": [], "entities": []}, {"text": "We first confirm that the models with attentive guidance indeed infer more compo-sitional solutions than the baseline, by training them on the lookup table task presented by Li\u0161ka et al.", "labels": [], "entities": []}, {"text": "We then do an in-depth analysis of the structural differences between the two model types, focusing in particular on the organisation of the parameter space and the hidden layer activations and find noticeable differences in both these aspects.", "labels": [], "entities": []}, {"text": "Guided networks focus more on the components of the input rather than the sequence as a whole and develop small functional groups of neurons with specific purposes that use their gates more selectively.", "labels": [], "entities": []}, {"text": "Results from parameter heat maps, component swapping and graph analysis also indicate that guided networks exhibit a more modular structure with a small number of specialized , strongly connected neurons.", "labels": [], "entities": [{"text": "component swapping", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8071942031383514}]}], "introductionContent": [{"text": "Sequence to sequence models (seq2seqs), a subset of neural networks that use sequences as input and output, have enjoyed great success in many NLP tasks such as machine translation () and speech recognition (Graves et al., * Shared senior authorship 2013).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.8275449275970459}, {"text": "speech recognition", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.8142652809619904}]}, {"text": "Even though these feats indicate excellent generalization capabilities, the way seq2seqs generalize has found to be different from how humans do.", "labels": [], "entities": []}, {"text": "In particular, seq2seqs lack of compositional understanding: the ability to construct new representations by combining familiar primitive components (e.g..", "labels": [], "entities": []}, {"text": "Humans, instead, heavily rely on compositionality to learn complex functional structure efficiently (.", "labels": [], "entities": []}, {"text": "Once the primitive components are understood, a possibly infinite amount of novel combinations can be made, which allows for large scale generalization from a limited amount of examples.", "labels": [], "entities": []}, {"text": "For instance, sentences consist of words, which in turn consist of characters constructed from strokes.", "labels": [], "entities": []}, {"text": "Recently, have shown how seq2seqs can produce many different fits on the training data using stochastic gradient descent, but rarely, if ever, find a compositional solution.", "labels": [], "entities": []}, {"text": "The authors introduce anew data set called the lookup table task, which tests for out of distribution generalization.", "labels": [], "entities": [{"text": "out of distribution generalization", "start_pos": 82, "end_pos": 116, "type": "TASK", "confidence": 0.5993831306695938}]}, {"text": "This data set will be discussed in more detail in Section 2.1.", "labels": [], "entities": []}, {"text": "As a remedy,  proposed Attentive Guidance (AG), a training technique which encourages seq2seqs to encode a more compositional solution without changing their internal architecture.", "labels": [], "entities": [{"text": "Attentive Guidance (AG)", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6132341742515564}]}, {"text": "AG provides additional information about the structure of the input sequence by supervising the attention mechanism of a model.", "labels": [], "entities": []}, {"text": "As a result, the model is able to find what are the basic components of the lookup table task and how to combine them in a compositional manner.", "labels": [], "entities": []}, {"text": "Thanks to this work, we are now in the unique position of having a compositional (from now on AG) and non-compositional (from now on base-line) model that have identical architectures, but implement very different approaches to the same task.", "labels": [], "entities": []}, {"text": "In this paper, we compare those two models and aim to find structural differences between the way they organise their weights and form their representations, that could be indicative of compositional solutions.", "labels": [], "entities": []}, {"text": "In particular: \u2022 We show, through inspection of the parameter space and activations, that individual neurons in the AG model show a degree of specialization with respect to specific inputs that is unseen in baseline models.", "labels": [], "entities": []}, {"text": "\u2022 We demonstrate, by substituting parts of both models with the corresponding component of its counterpart, which model sections contribute most to the observed compositional behavior in AG models.", "labels": [], "entities": []}, {"text": "These differences confirm the findings of Hupkes et al.", "labels": [], "entities": []}, {"text": "(2019) that seq2seqs do not necessarily require big architectural adjustments to handle compositionality, since a network with identical architecture is capable of finding such a solution.", "labels": [], "entities": []}, {"text": "Furthermore, these findings could be exploited to inform architectural changes in models, such that their priors to infer compositional solutions increase even when they are not provided explicit additional feedback on the compositional structure of the data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train five baseline and AG models with the same hyperparameters and the Adam optimizer (.", "labels": [], "entities": []}, {"text": "Given the small vocabulary, we use an embedding size of 16 and a hidden size to 512.", "labels": [], "entities": []}, {"text": "All models were trained fora maximum of 100 epochs with an attention mechanism, determining attention weights by using a multi-layer perceptron.", "labels": [], "entities": []}, {"text": "Models were selected by their best accuracy on a held-out set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9995225667953491}]}, {"text": "A comprehensive list of model performances on the different sets can be found in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.8474740386009216}]}, {"text": "The model implementations themselves stem from the i-machine-think codebase.", "labels": [], "entities": []}, {"text": "In the following, we perform three different suits of experiments.", "labels": [], "entities": []}, {"text": "Firstly, we examine the parameter space of both models (Section 3).", "labels": [], "entities": []}, {"text": "Secondly, we take a closer look at the activations of single neurons and the GRU gates (Section 4).", "labels": [], "entities": []}, {"text": "Lastly, in Section 5, we perform two different ablation studies: we make components of one model interacting with the components of the other and we distill the network via strongly connected neurons.", "labels": [], "entities": []}], "tableCaptions": []}