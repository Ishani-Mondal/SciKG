{"title": [{"text": "Normalizing Non-canonical Turkish Texts Using Machine Translation Approaches", "labels": [], "entities": [{"text": "Normalizing Non-canonical Turkish Texts", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.875545471906662}, {"text": "Machine Translation Approaches", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.7657985389232635}]}], "abstractContent": [{"text": "With the growth of the social web, user-generated text data has reached unprecedented sizes.", "labels": [], "entities": []}, {"text": "Non-canonical text normalization provides away to exploit this as a practical source of training data for language processing systems.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7551168501377106}]}, {"text": "The state of the art in Turk-ish text normalization is composed of a token-level pipeline of modules, heavily dependent on external linguistic resources and manually-defined rules.", "labels": [], "entities": [{"text": "Turk-ish text normalization", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6586582660675049}]}, {"text": "Instead, we propose a fully-automated, context-aware machine translation approach with fewer stages of processing.", "labels": [], "entities": [{"text": "context-aware machine translation", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.6131167610486349}]}, {"text": "Experiments with various implementations of our approach show that we are able to surpass the current best-performing system by a large margin .", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised machine learning methods such as CRFs, SVMs, and neural networks have come to define standard solutions fora wide variety of language processing tasks.", "labels": [], "entities": []}, {"text": "These methods are typically data-driven, and require training on a substantial amount of data to reach their potential.", "labels": [], "entities": []}, {"text": "This kind of data often has to be manually annotated, which constitutes a bottleneck in development.", "labels": [], "entities": []}, {"text": "This is especially marked in some tasks, where quality or structural requirements for the data are more constraining.", "labels": [], "entities": []}, {"text": "Among the examples are text normalization and machine translation (MT), as both tasks require parallel data with limited natural availability.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8399152457714081}, {"text": "machine translation (MT)", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.8416471004486084}]}, {"text": "The success achieved by data-driven learning methods brought about an interest in usergenerated data.", "labels": [], "entities": []}, {"text": "Collaborative online platforms such as social media area great source of large amounts of text data.", "labels": [], "entities": []}, {"text": "However, these texts typically contain non-canonical usages, making them hard to leverage for systems sensitive to training data bias.", "labels": [], "entities": []}, {"text": "Non-canonical text normalization is the task of processing such texts into a canonical format.", "labels": [], "entities": [{"text": "Non-canonical text normalization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6560049951076508}]}, {"text": "As such, normalizing user-generated data has the capability of producing large amounts of serviceable data for training data-driven systems.", "labels": [], "entities": []}, {"text": "As a denoising task, text normalization can be regarded as a translation problem between closely related languages.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8013747930526733}]}, {"text": "Statistical machine translation (SMT) methods dominated the field of MT fora while, until neural machine translation (NMT) became more popular.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8161650697390238}, {"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9910571575164795}, {"text": "neural machine translation (NMT)", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.7856259942054749}]}, {"text": "The modular composition of an SMT system makes it less susceptible to data scarcity, and allows it to better exploit unaligned data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9894158840179443}]}, {"text": "In contrast, NMT is more data-hungry, with a superior capacity for learning from data, but often faring worse when data is scarce.", "labels": [], "entities": []}, {"text": "Both translation methods are very powerful in generalization.", "labels": [], "entities": [{"text": "translation", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.9623712301254272}]}, {"text": "In this study, we investigate the potential of using MT methods to normalize non-canonical texts in Turkish, a morphologically-rich, agglutinative language, allowing fora very large number of common word forms.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9869056344032288}]}, {"text": "Following in the footsteps of unsupervised MT approaches, we automatically generate synthetic parallel data from unaligned sources of \"monolingual\" canonical and non-canonical texts.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9885249733924866}]}, {"text": "Afterwards, we use these datasets to train character-based translation systems to normalize non-canonical texts . We describe our methodology in contrast with the state of the art in Section 3, outline our data and empirical results in Sections 4 and 5, and finally present our conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned earlier, our translation approach is highly data-driven.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9791040420532227}]}, {"text": "Training translation and language models for machine translation, and performing an adequate performance evaluation comparable to previous works each require datasets of different qualities.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8088827133178711}]}, {"text": "We describe all datasets that we use in this study in the following subsections.", "labels": [], "entities": []}, {"text": "The first component of our system (i.e. Orthographic Normalization) is a simple character replacement module.", "labels": [], "entities": [{"text": "Orthographic Normalization", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6270942538976669}, {"text": "character replacement module", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.7761117617289225}]}, {"text": "We gather unique characters that appear in Twitter corpus which we scrape to generate Train P araT ok . Due to non-Turkish tweets, there are some Arabic, Persian, Japanese and Hangul characters that cannot be orthographically converted to Turkish characters.", "labels": [], "entities": []}, {"text": "We filter out those characters using their unicode character name leaving only characters belonging Latin, Greek and Cyrillic alphabets.", "labels": [], "entities": []}, {"text": "Then, the remaining characters are mapped to their Turkish counterparts with the help of a library 3 . After manual review and correction of these characters mappings, we have 701 character replacement rules in this module.", "labels": [], "entities": []}, {"text": "We experiment with both SMT and NMT implementations as contrastive methods.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9786712527275085}]}, {"text": "For our SMT pipeline, we employ a fairly standard array of tools, and set their parameters similarly to and.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9953371286392212}]}, {"text": "For alignment, we use MGIZA ( with grow-diag-final-and symmetrization.", "labels": [], "entities": [{"text": "alignment", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9727239608764648}, {"text": "MGIZA", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7549024224281311}]}, {"text": "For language modeling, we use KenLM (Heafield, 2011) to train 6-gram character-level language models on OpenSubs F iltered and Huawei M onoT R . For phrase extraction and decoding, we use Moses () to train a model on Train P araT ok . Although there is a small possibility of transposition between adjacent characters, we disable distortion in translation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7681965827941895}, {"text": "OpenSubs F iltered and Huawei M onoT R", "start_pos": 104, "end_pos": 142, "type": "DATASET", "confidence": 0.8198086768388748}, {"text": "phrase extraction", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.8223554790019989}]}, {"text": "We use Val Small for minimum error rate training, optimizing our model for word error rate.", "labels": [], "entities": []}, {"text": "We train our NMT model using the OpenNMT toolkit () on Train P araT ok without any parameter tuning.", "labels": [], "entities": [{"text": "Train P araT ok", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.8905739933252335}]}, {"text": "Each model uses an attentional encoder-decoder architecture, with 2-layer LSTM encoders and decoders.", "labels": [], "entities": []}, {"text": "The input embeddings, the LSTM layers of the encoder, and the inner layer of the decoder all have a dimensionality of 500.", "labels": [], "entities": []}, {"text": "The outer layer of the decoder has a dimensionality of 1,000.", "labels": [], "entities": []}, {"text": "Both encoder and decoder LSTMs have a dropout probability of 0.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of each test datasets", "labels": [], "entities": [{"text": "Sizes", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.9833258390426636}]}, {"text": " Table 2: Case-insensitive (top) and case-sensitive  (bottom) accuracy over all tokens.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9865498542785645}]}, {"text": " Table 3: Case-insensitive (top) and case-sensitive  (bottom) accuracy scores over non-canonical tokens.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9862192273139954}]}]}