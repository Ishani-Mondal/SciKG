{"title": [{"text": "Predicting Human Activities from User-Generated Content", "labels": [], "entities": [{"text": "Predicting Human Activities from User-Generated Content", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8851428031921387}]}], "abstractContent": [{"text": "The activities we do are linked to our interests , personality, political preferences, and decisions we make about the future.", "labels": [], "entities": []}, {"text": "In this paper , we explore the task of predicting human activities from user-generated content.", "labels": [], "entities": [{"text": "predicting human activities from user-generated content", "start_pos": 39, "end_pos": 94, "type": "TASK", "confidence": 0.8716923097769419}]}, {"text": "We collect a dataset containing instances of social media users writing about a range of everyday activities.", "labels": [], "entities": []}, {"text": "We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities.", "labels": [], "entities": []}, {"text": "We train a neural network model to make predictions about which clusters contain activities that were performed by a given user based on the text of their previous posts and self-description.", "labels": [], "entities": []}, {"text": "Additionally, we explore the degree to which incorporating inferred user traits into our model helps with this prediction task.", "labels": [], "entities": []}], "introductionContent": [{"text": "What a person does says a lot about who they are.", "labels": [], "entities": []}, {"text": "Information about the types of activities that a person engages in can provide insights about their interests), personality, physical health (, the activities that they are likely to do in the future (, and other psychological phenomena like personal values.", "labels": [], "entities": []}, {"text": "For example, it has been shown that university students who exhibit traits of interpersonal affect and self-esteem are more likely to attend parties, and those that value stimulation are likely to watch movies that can be categorized as thrillers (.", "labels": [], "entities": []}, {"text": "Several studies have applied computational approaches to the understanding and modeling of human behavior at scale () and in real time (.", "labels": [], "entities": []}, {"text": "However, this previous work has mainly relied on specific devices or platforms that require structured definitions of behaviors to be measured.", "labels": [], "entities": []}, {"text": "While this leads to an accurate understanding of the types of activities being done by the involved users, these methods capture a relatively narrow set of behaviors compared to the huge range of things that people do on a day-to-day basis.", "labels": [], "entities": []}, {"text": "On the other hand, publicly available social media data provide us with information about an extremely rich and diverse set of human activities, but the data are rarely structured or categorized, and they mostly exist in the form of natural language.", "labels": [], "entities": []}, {"text": "Recently, however, natural language processing research has provided several examples of methodologies for extracting and representing human activities from text and even multimodal data (.", "labels": [], "entities": [{"text": "extracting and representing human activities from text", "start_pos": 107, "end_pos": 161, "type": "TASK", "confidence": 0.7695231522832598}]}, {"text": "In this paper, we explore the task of predicting human activities from user-generated text data, which will allow us to gain a deeper understanding of the kinds of everyday activities that people discuss online with one another.", "labels": [], "entities": [{"text": "predicting human activities", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.8783642252286276}]}, {"text": "Throughout the paper, we use the word \"activity\" to refer to what an individual user does or has done in their daily life.", "labels": [], "entities": []}, {"text": "Unlike the typical use of this term in the computer vision community (, in this paper we use it in abroad sense, to also encompass non-visual activities such as \"make vacation plans\" or \"have a dream\" We do not focus on fine-grained sequences actions such as \"pick up a camera\", \"hold a camera to one's face\", \"press the shutter release button\", and others.", "labels": [], "entities": []}, {"text": "Rather, we focus on the highlevel activity as a person would report to others: \"take a picture\".", "labels": [], "entities": []}, {"text": "Additionally, we specifically focus on everyday human activities done by the users themselves, rather than larger-scale events, which are typically characterized by the involvement or interest of many users, often at a specific time and location.", "labels": [], "entities": []}, {"text": "Given that the space of possible phrases describ-ing human activities is nearly limitless, we propose a set of human activity clusters that summarize a large set of several hundred-thousand selfreported activities.", "labels": [], "entities": []}, {"text": "We then construct predictive models that are able to estimate the likelihood that a user has reported that they have performed an activity from any cluster.", "labels": [], "entities": []}, {"text": "The paper makes the following main contributions.", "labels": [], "entities": []}, {"text": "First, starting with a set of nearly 30,000 human activity patterns, we compile a very large dataset of more than 200,000 users undertaking one of the human activities matching these patterns, along with over 500 million total tweets from these users.", "labels": [], "entities": []}, {"text": "Second, we use a state-of-theart sentence embedding framework tailored to recognize the semantics of human activities and create a set of activity clusters of variable granularity.", "labels": [], "entities": []}, {"text": "Third, we explore a neural model that can predict human activities based on natural language data, and in the process also investigate the relationships between everyday human activities and other social variables such as personal values.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our activity prediction models using a number of metrics that consider not only the most likely cluster, but also the set of k eval most likely clusters.", "labels": [], "entities": [{"text": "activity prediction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.6989450752735138}]}, {"text": "First, we evaluate the average per-class accuracy of the model's ability to rank ct , the target cluster, within the top k eval clusters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9831539988517761}]}, {"text": "These scores tell us how well the model is able to make predictions about the kinds of activities that each user is likely to do.", "labels": [], "entities": []}, {"text": "Second, we test how well the model is able to sort users by their likelihood of having reported to do an activity from a cluster.", "labels": [], "entities": []}, {"text": "This average comparison rank (ACR) score is computed as follows: for each user in the test set, we sample n other users who do not have the same activity label.", "labels": [], "entities": [{"text": "average comparison rank (ACR) score", "start_pos": 5, "end_pos": 40, "type": "METRIC", "confidence": 0.8726895877293178}]}, {"text": "Then, we use the probabilities assigned by the model to rank all n + 1 users 8 by their likelihood of being assigned ct , and the comparison rank score is the percentage of users who were ranked ahead of the target user (lower is better).", "labels": [], "entities": []}, {"text": "We then average this comparison rank across all users in the test set to get the ACR.", "labels": [], "entities": [{"text": "ACR", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8833509683609009}]}, {"text": "The ACR score tells us how well the model is able to find a rank users based on their likelihood of writing about doing a given activity, which could be useful for finding, e.g., the users who are most likely to claim that they \"purchased some pants\" or least likely to mention that they \"went to the gym\" in the future.", "labels": [], "entities": [{"text": "ACR score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.5654285699129105}]}, {"text": "We split our data at the user-level, and from our set of valid users we use 200,000 instances for training data, 10,000 as test data, and the rest as our validation set.", "labels": [], "entities": []}, {"text": "For the document encoder and profile encoder we use Bi-LSTMs with max pooling (, with dim d = 128 and dim p = 128.", "labels": [], "entities": []}, {"text": "For the history encoder, we empirically found that single mean pooling layer over the set of all document embeddings outperformed other more complicated architectures, and so that is what we use in our experiments.", "labels": [], "entities": []}, {"text": "Finally, the classifier is a 3-layer feed-forward network with and dim c = 512 for the hidden layers, followed by a softmax over the dim o -dimensional output.", "labels": [], "entities": []}, {"text": "We use Adam () as our optimizer, set the maximum number of epochs to 100, and shuffle the order of the training data at each epoch.", "labels": [], "entities": []}, {"text": "During each train- 2.00 4.00 6.00 10.00 20.00 50.00 50.00: Per-class accuracy (%) @ k eval and ACR scores for the 50-class prediction task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.8361496925354004}, {"text": "ACR", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.7947383522987366}, {"text": "50-class prediction task", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.750320573647817}]}, {"text": "Note that removing h from either full T or full A gives the same model.", "labels": [], "entities": []}, {"text": "For ACR only, lower is better.", "labels": [], "entities": []}, {"text": "ing step, we represent each user's history as anew random sample of max sample docs = 100 documents 9 if there are more than max sample docs documents available for the user, and we use a batch size of 32 users.", "labels": [], "entities": []}, {"text": "Since there is a class imbalance in our data, we use sample weighting in order to prevent the model from converging to a solution that simply predicts the most common classes present in the training data.", "labels": [], "entities": []}, {"text": "Each sample is weighted according to its class, c, using the following formula: where count(c) is the number of training instances belonging to class c.", "labels": [], "entities": [{"text": "count(c)", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.91518135368824}]}, {"text": "We evaluate our model on the development data after each epoch and save the model with the highest per-class accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9547051787376404}]}, {"text": "Finally, we compute the results on the test data using this model, and report these results.", "labels": [], "entities": []}, {"text": "We test several configurations of our model.", "labels": [], "entities": []}, {"text": "We use the complete model described in section 3.2 using either the set of additional tweets written by a user as their history (full T ), or only the set of additional activities contained in those tweets (full A ).", "labels": [], "entities": []}, {"text": "Then, to test the effect of the various model components, we systematically ablate the attributes vector input a, the profile text (and subsequently, the Profile Encoder layer) p, and the set of documents, D, comprising the history along with the Document and History Encoders, thereby removing the h vector as input to the classifier.", "labels": [], "entities": []}, {"text": "We also explore removing pairs of these inputs at the same time.", "labels": [], "entities": []}, {"text": "To contextualize the results, we also We empirically found that increasing this value beyond 100 had little effect on the development accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9892292618751526}]}, {"text": "include the theoretical scores achieved by random guessing, labeled as rand.", "labels": [], "entities": []}, {"text": "We consider two variations on our dataset: the first is a simplified, 50-class classification problem.", "labels": [], "entities": [{"text": "50-class classification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.6506356298923492}]}, {"text": "We choose the 50 most common clusters out of our full set of k act = 1024 and only make predictions about users who have reportedly performed an activity in one of these clusters.", "labels": [], "entities": []}, {"text": "The second variation uses the entire dataset, but rather than making predictions about all k act classes, we only make fine-grained predictions about those classes for which count(c) \u2265 minCount.", "labels": [], "entities": []}, {"text": "We do this under the assumption that training an adequate classifier fora given class requires at least minCount examples.", "labels": [], "entities": []}, {"text": "All classes for which count(c) < minCount are assigned an \"other\" label.", "labels": [], "entities": []}, {"text": "In this way, we still make a prediction for every instance in the dataset, but we avoid allowing the model to try to fit to a huge landscape of outputs when the training data for some of these outputs is insufficient.", "labels": [], "entities": []}, {"text": "By setting minCount to 100, we are left with 805 out of 1024 classes, and an 806th \"other\" class for our 806-class setup.", "labels": [], "entities": []}, {"text": "Note that this version includes all activities from all 1024 clusters, it is just that the smallest clusters are grouped together with the \"other\" label.", "labels": [], "entities": []}, {"text": "While our models are able to make predictions indicating that learning has taken place, it is clear that this prediction task is difficult.", "labels": [], "entities": []}, {"text": "In the 50-class setup, the full T \u2212 a, p model consistently had the strongest average per-class accuracy for all values of k eval and the lowest (best) ACR score (: Per-class accuracy (%) @ k eval and ACR scores for the 806-class prediction task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9536115527153015}, {"text": "ACR", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9438656568527222}, {"text": "ACR", "start_pos": 201, "end_pos": 204, "type": "METRIC", "confidence": 0.7833417057991028}, {"text": "806-class prediction task", "start_pos": 220, "end_pos": 245, "type": "TASK", "confidence": 0.6972737908363342}]}, {"text": "Note that removing h from either full T or full A gives the same model.", "labels": [], "entities": []}, {"text": "For ACR only, lower is better.", "labels": [], "entities": []}, {"text": "relevant content from a user's history gives similar results to using the full set of content available.", "labels": [], "entities": []}, {"text": "When including the attributes and profile fora user, the model typically overfits quickly and generalization deteriorates.", "labels": [], "entities": []}, {"text": "In the 806-class version of the task, we observe the effects of including a larger range of activities, including many that do not appear as often as others in the training data).", "labels": [], "entities": []}, {"text": "This version of the task also simulates a more realistic scenario, since predictions can be made for the \"other\" class when the model does to expect the user to claim to do an activity from any of the known clusters.", "labels": [], "entities": []}, {"text": "In this setting, we see that the full T \u2212 p model works well fork eval \u2264 25, suggesting that the use of the attribute vectors helps, especially when predicting the correct cluster within the top 25 is important.", "labels": [], "entities": []}, {"text": "For k eval \u2265 50, the same full T \u2212 a, p model that worked best in the 50-class setup again outperforms the others.", "labels": [], "entities": []}, {"text": "Here, in contrast to the 50-class setting, using the full set of tweets usually performs better than focusing only on the human activity content.", "labels": [], "entities": []}, {"text": "Interestingly, the best ACR scores are even lower in the 806-class setup, showing that it is just as easy to rank users by their likelihood of writing about an activity, even when considering many more activity clusters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect of targeted query approach on activity  frequency in tweets. \"Valid activities\" are defined as  first-person verb phrases that clearly indicate that the  author of the text has actually performed the concrete  activity being described. For each set of tweets, a ran- dom subset of 100 was chosen and manually annotated  for validity.", "labels": [], "entities": []}, {"text": " Table 2: Number of human activity queries from mul- tiple sources.", "labels": [], "entities": []}, {"text": " Table 4: Summary of additional data.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8753941655158997}]}, {"text": " Table 5: Summary valid user filtering.", "labels": [], "entities": [{"text": "Summary valid user filtering", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.5324928760528564}]}, {"text": " Table 10: Per-class accuracy (%) @ k eval and ACR scores for the 50-class prediction task. Note that removing h  from either full T or full A gives the same model. For ACR only, lower is better.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8319656252861023}, {"text": "ACR", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.900172233581543}, {"text": "50-class prediction task", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7409607768058777}]}, {"text": " Table 11: Per-class accuracy (%) @ k eval and ACR scores for the 806-class prediction task. Note that removing h  from either full T or full A gives the same model. For ACR only, lower is better.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8298021554946899}, {"text": "ACR", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9115959405899048}, {"text": "806-class prediction task", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.6903480986754099}]}]}