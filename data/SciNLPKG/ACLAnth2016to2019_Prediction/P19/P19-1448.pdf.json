{"title": [{"text": "Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing", "labels": [], "entities": [{"text": "Representing Schema Structure", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8242932558059692}]}], "abstractContent": [{"text": "Research on parsing language to SQL has largely ignored the structure of the database (DB) schema, either because the DB was very simple, or because it was observed at both training and test time.", "labels": [], "entities": []}, {"text": "In SPIDER, a recently-released text-to-SQL dataset, new and complex DBs are given attest time, and so the structure of the DB schema can inform the predicted SQL query.", "labels": [], "entities": []}, {"text": "In this paper, we present an encoder-decoder semantic parser, where the structure of the DB schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time.", "labels": [], "entities": []}, {"text": "Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8% to 39.4%, dramatically above the current state of the art, which is at 19.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9916648268699646}]}], "introductionContent": [{"text": "Semantic parsing () has recently taken increased interest in parsing questions into SQL queries, due to the popularity of SQL as a query language for relational databases.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8156844973564148}]}, {"text": "Work on parsing to SQL () has either involved simple DBs that contain just one table, or had a single DB that is observed at both training and test time.", "labels": [], "entities": []}, {"text": "Consequently, modeling the schema structure received little attention.", "labels": [], "entities": []}, {"text": "Recently, presented SPIDER, a text-to-SQL dataset, whereat test time questions are executed against unseen and complex DBs.", "labels": [], "entities": []}, {"text": "In this zero-shot setup, an informative representation of the schema structure is important.", "labels": [], "entities": []}, {"text": "Consider the questions in: while their language structure is similar, in the first query a 'join' operation is necessary because the information is distributed across three tables, while in the other query no 'join' is needed.", "labels": [], "entities": []}, {"text": "In this work, we propose a semantic parser that strongly uses the schema structure.", "labels": [], "entities": []}, {"text": "We represent the structure of the schema as a graph, and use graph neural networks (GNNs) to provide a global representation for each node (.", "labels": [], "entities": []}, {"text": "We incorporate our schema representation into the encoder-decoder parser of , which was designed to parse questions into queries against unseen semi-structured tables.", "labels": [], "entities": []}, {"text": "At encoding time we enrich each question word with a representation of the subgraph it is related to, and at decoding time we emit symbols from the schema that are related through the graph to previously decoded symbols.", "labels": [], "entities": []}, {"text": "We evaluate our parser on SPIDER, and show that encoding the schema structure improves accuracy from 33.8% to 39.4% (and from 14.6% to 26.8% on questions that involve multiple tables), well beyond 19.7%, the current stateof-the-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9991573095321655}]}, {"text": "We make our code publicly available at https://github.com/benbogin/ spider-schema-gnn.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experimental setup We evaluate on SPI-DER (), which contains 7,000/1,034/2,147 train/development/test examples.", "labels": [], "entities": []}, {"text": "We pre-process examples to remove table aliases (AS T1/T2/...) from the queries and use the explicit table name instead (i.e. we replace T1.col with table1 name.col), as in the majority of the cases (> 99% in SPIDER) these aliases are redundant.", "labels": [], "entities": []}, {"text": "In addition, we add a table reference to all columns that do not have one (i.e. we replace col with table name.col).", "labels": [], "entities": []}, {"text": "We use the official evaluation script from SPI-DER to compute accuracy, i.e., whether the predicted query is equivalent to the gold query.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9989266991615295}]}, {"text": "Results Our full model (GNN) obtains 39.4% accuracy on the test set, substantially higher than prior state-of-the-art (SYNTAXSQLNET), which is at 19.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9996829032897949}]}, {"text": "Removing the GNN from the parser (NO GNN), which results in the parser of, augmented with a grammar for SQL, obtains an accuracy of 33.8%, showing the importance of encoding the schema structure.", "labels": [], "entities": [{"text": "NO GNN)", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.815378725528717}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9992558360099792}]}, {"text": "shows results on the development set for baselines and ablations.", "labels": [], "entities": []}, {"text": "The first column describes accuracy on the entire dataset, and the next two columns show accuracy when partitioning examples to queries involving only one table (SINGLE) vs. more than one table (MULTI).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.999068558216095}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9993488192558289}, {"text": "MULTI", "start_pos": 195, "end_pos": 200, "type": "METRIC", "confidence": 0.8263795971870422}]}, {"text": "GNN dramatically outperforms previously published baselines SQLNET and SYNTAXSQLNET, and improves the performance of NO GNN from 34.9% to 40.7%.", "labels": [], "entities": []}, {"text": "Importantly, using schema structure specifically improves performance on questions with multiple tables from 14.6% to 26.8%.", "labels": [], "entities": []}, {"text": "We ablate the major novel components of our model to assess their impact.", "labels": [], "entities": []}, {"text": "First, we remove the self-attention component (NO SELF ATTEND).", "labels": [], "entities": [{"text": "NO SELF ATTEND)", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.8302083313465118}]}, {"text": "We observe that performance drops by 2 points, where SINGLE slightly improves, and MULTI drops by 6.5 points.", "labels": [], "entities": [{"text": "SINGLE", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.984977126121521}, {"text": "MULTI", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9958970546722412}]}, {"text": "Second, to verify that improvement is not only due to self-attention, we ablate all other uses of the GNN.", "labels": [], "entities": [{"text": "GNN", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9041351079940796}]}, {"text": "Namely, We use a model identical to NO GNN, except it can access the GNN representations through the self-attention (ONLY SELF ATTEND).", "labels": [], "entities": [{"text": "ONLY", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9988221526145935}, {"text": "SELF", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.6913161277770996}, {"text": "ATTEND", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.7020578384399414}]}, {"text": "We observe a large drop in performance to 35.9%, showing that all components are important.", "labels": [], "entities": []}, {"text": "Last, we ablate the relevance score by setting \u03c1 v = 1 for all schema items (NO REL.).", "labels": [], "entities": [{"text": "relevance score", "start_pos": 20, "end_pos": 35, "type": "METRIC", "confidence": 0.9411375224590302}, {"text": "NO", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9880945086479187}, {"text": "REL.", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.6742276549339294}]}, {"text": "Indeed, accuracy drops to 37.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9995286464691162}]}, {"text": "To assess the ceiling performance possible with a perfect relevance score, we run an oracle experiment, where we set \u03c1 v = 1 for all schema items that are in the gold query, and \u03c1 v = 0 for all other schema items (GNN ORACLE REL.).", "labels": [], "entities": [{"text": "GNN", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.7385932207107544}, {"text": "ORACLE", "start_pos": 218, "end_pos": 224, "type": "METRIC", "confidence": 0.7122775316238403}, {"text": "REL.", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.5465266704559326}]}, {"text": "We see that a perfect relevance score substantially improves performance to 54.3%, indicating substantial headroom for future research.", "labels": [], "entities": [{"text": "perfect relevance score", "start_pos": 14, "end_pos": 37, "type": "METRIC", "confidence": 0.730613907178243}]}, {"text": "join analysis For any model, we can examine the proportion of predicted queries with a join, where the structure of the join is \"bad\": (a) when the join condition clause uses the same table twice (ON t1.column1 = t1.column2), and (b) when the joined table are not connected through a primary-foreign key relation.", "labels": [], "entities": [{"text": "join analysis", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8027105927467346}]}, {"text": "We find that NO GNN predicts such joins in 83.4% of the cases, while GNN does so in only 15.6% of cases.", "labels": [], "entities": []}, {"text": "When automatically omitting from the beam candidates where condition (a) occurs, NO GNN predicts a \"bad\" join in 14.2% of the cases vs. 4.3% for GNN (total accuracy increases by 0.3% for both models).", "labels": [], "entities": [{"text": "NO", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.7910454273223877}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9908668994903564}]}, {"text": "As an example, in, s loc j scores the table student the highest, although it is not related to the previously decoded table semester.", "labels": [], "entities": []}, {"text": "Adding the selfattention score s att j corrects this and leads to the correct student semester, probably because the model learns to prefer connected tables.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development set accuracy for all models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8174015283584595}]}]}