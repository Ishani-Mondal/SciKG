{"title": [{"text": "Probing Neural Network Comprehension of Natural Language Arguments", "labels": [], "entities": [{"text": "Probing Neural Network Comprehension of Natural Language Arguments", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.6717565171420574}]}], "abstractContent": [{"text": "We are surprised to find that BERT's peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline.", "labels": [], "entities": [{"text": "BERT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9961180686950684}, {"text": "Argument Reasoning Comprehension Task", "start_pos": 68, "end_pos": 105, "type": "TASK", "confidence": 0.7677413672208786}]}, {"text": "However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset.", "labels": [], "entities": []}, {"text": "We analyze the nature of these cues and demonstrate that a range of models all exploit them.", "labels": [], "entities": []}, {"text": "This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.7927883863449097}]}, {"text": "Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Argumentation mining is the task of determining argumentative structure in natural language text -e.g., which text segments represent claims, and which comprise reasons that support or attack those claims (.", "labels": [], "entities": [{"text": "Argumentation mining", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8223892152309418}]}, {"text": "This is a challenging task for machine learners, as it can be hard even for humans to determine when two text segments stand in argumentative relation, as evidenced by studies on argument annotation (.", "labels": [], "entities": []}, {"text": "One approach to this problem is to focus on warrants (Toulmin, 1958) -a form of world knowledge that permit inferences.", "labels": [], "entities": []}, {"text": "Consider a simple argument: \"(1) It is raining; therefore (2) you should take an umbrella.\"", "labels": [], "entities": []}, {"text": "The warrant \"(3) it is bad to get wet\" could license this inference.", "labels": [], "entities": []}, {"text": "facilitates drawing the inferential connection between (1) and (2).", "labels": [], "entities": []}, {"text": "However it would be hard to find it stated anywhere since warrants are most often left implicit).", "labels": [], "entities": []}, {"text": "Thus, on this approach, machine learners must not only reason with warrants but also discover them.", "labels": [], "entities": []}, {"text": "This example adapted from Black and Hunter (2012)", "labels": [], "entities": []}], "datasetContent": [{"text": "If a model is exploiting distributional cues over the labels, then if trained only on the warrants (W) it should perform relatively well.", "labels": [], "entities": []}, {"text": "The same can be said for removing either just the claim, leaving the reason and warrant (R, W), or removing the reason (C, W).", "labels": [], "entities": []}, {"text": "The latter setups allow the models to additionally consider cues in the reasons and claims, as well as cues holding over their combinations with the warrants.", "labels": [], "entities": []}, {"text": "Each of these setups breaks the task since we no longer have an argument to match with a warrant.", "labels": [], "entities": []}, {"text": "Experimental results are given in.", "labels": [], "entities": []}, {"text": "On warrants alone (W) BERT achieves a maximum 71% accuracy.", "labels": [], "entities": [{"text": "BERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9886992573738098}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9977927207946777}]}, {"text": "That leaves only six percentage points to account for its peak of 77%.", "labels": [], "entities": []}, {"text": "We find again of four percentage points for (R, W) over (W), and again of two for (C, W), accounting for the missing six points.", "labels": [], "entities": []}, {"text": "Based on this evidence our major finding is that the entirety of BERT's performance can be accounted for in terms of exploiting spurious statistical cues.", "labels": [], "entities": [{"text": "BERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.7196304202079773}]}], "tableCaptions": [{"text": " Table 1: Baselines and BERT results. Our results come from 20 different random seeds (\u00b1 gives the standard  deviation). The mean for BERT Large is skewed by the 5/20 random seeds for which it failed to train, a problem  noted by", "labels": [], "entities": [{"text": "BERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9966645836830139}, {"text": "BERT", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.617263674736023}]}, {"text": " Table 2: Productivity and coverage of using the pres- ence of \"not\" in the warrant to predict the label in  ARCT. Across the whole dataset, if you pick the war- rant with \"not\" you will be right 61% of the time, which  covers 64% of all data points.", "labels": [], "entities": [{"text": "coverage", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9906013011932373}]}, {"text": " Table 3: Results of probing experiments with BERT  Large, and the BoV and BiLSTM baselines. These re- sults indicate that BERT's peak 77% performance can  be entirely accounted for by exploiting spurious cues.  By just considering warrants (W) we can get to 71%.  Adding cues over reasons (R, W) and claims (C, W)  accounts for the remaining six points.", "labels": [], "entities": [{"text": "BoV and BiLSTM baselines", "start_pos": 67, "end_pos": 91, "type": "DATASET", "confidence": 0.7777027636766434}]}]}