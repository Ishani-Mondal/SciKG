{"title": [{"text": "Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings", "labels": [], "entities": [{"text": "Unsupervised Text Classification Leveraging Experts", "start_pos": 8, "end_pos": 59, "type": "TASK", "confidence": 0.7577617704868317}]}], "abstractContent": [{"text": "Text classification aims at mapping documents into a set of predefined categories.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8122459948062897}]}, {"text": "Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9930713772773743}]}, {"text": "This is particularly true when the number of target categories is in the tensor the hundreds.", "labels": [], "entities": []}, {"text": "In this work, we explore an unsupervised approach to classify documents into categories simply described by a label.", "labels": [], "entities": []}, {"text": "The proposed method is inspired by the way a human proceeds in this situation: It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field.", "labels": [], "entities": []}, {"text": "The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific.", "labels": [], "entities": []}, {"text": "Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9996007084846497}]}, {"text": "It thus provides a practical alternative to situations where low-cost text categorization is needed, as we illustrate with our application to operational risk incidents classification.", "labels": [], "entities": [{"text": "operational risk incidents classification", "start_pos": 142, "end_pos": 183, "type": "TASK", "confidence": 0.7589020729064941}]}], "introductionContent": [{"text": "Document classification is a standard task in machine learning.", "labels": [], "entities": [{"text": "Document classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8958588242530823}]}, {"text": "Its applications span a variety of \"use cases and contexts, e.g., email filtering, news article clustering, clinical document classification, expertquestion matching\".", "labels": [], "entities": [{"text": "email filtering", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7648843228816986}, {"text": "news article clustering", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.6363270481427511}, {"text": "clinical document classification", "start_pos": 108, "end_pos": 140, "type": "TASK", "confidence": 0.6259841422239939}, {"text": "expertquestion matching", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.8226294815540314}]}, {"text": "The standard process for text categorization relies on supervised and semisupervised approaches.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8114307522773743}]}, {"text": "The motivation for the present effort comes from the banking sector, in particular the management of operational risks.", "labels": [], "entities": []}, {"text": "This category of risks corresponds to the broad set of incidents that are neither credit nor market risk and includes issues related to internal and external fraud, cybersecurity, damages on physical assets, natural disasters, etc.", "labels": [], "entities": []}, {"text": "The practical management of operational risk is partially based on the management of a dataset of historical operational risk incidents where each incident is described in details and that is shared on a regular basis with regulators.", "labels": [], "entities": [{"text": "operational risk", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7769483327865601}]}, {"text": "Historically, all incident reports have been mapped to about twenty categories of risk issued from the regulator.", "labels": [], "entities": []}, {"text": "However, from an operational perspective, a higher number of risk categories is relevant to better capture the nuances around the incidents and enable relevant comparisons.", "labels": [], "entities": []}, {"text": "This led to the creation of anew internal risk taxonomy of risk composed of 264 categories, each described by a label (a few words).", "labels": [], "entities": []}, {"text": "To make it operational, the stock of all internal and external incident reports had to be classified into categories from the new internal taxonomy.", "labels": [], "entities": []}, {"text": "However, since it had never been used before, we had no labeled samples readily available.", "labels": [], "entities": []}, {"text": "As hundreds of thousands of incidents had to be processed, text classification seemed a promising approach to assist in that mapping task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8823698461055756}]}, {"text": "Indeed, given the specificity of the domain and the lack of availability of experts, it was not conceivable to obtain many labeled examples for each category as would be required for supervised approaches.", "labels": [], "entities": []}, {"text": "This is the issue addressed in this paper where describe our work towards an unsupervised approach to classify documents into a set of categories described by a short sentence (label).", "labels": [], "entities": []}, {"text": "While the inspiration of this paper is the classification of incident reports in operational risk, our approach aims to be readily transferable to other domains.", "labels": [], "entities": [{"text": "classification of incident reports", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.8657640516757965}]}, {"text": "For that purpose, we tested it on standard text classification corpora.", "labels": [], "entities": []}, {"text": "The underlying idea is altogether simple.", "labels": [], "entities": []}, {"text": "We emulate the approach that a domain expert would follow to manually assign an input document (incident report, client review, news article, etc.) to a given category.", "labels": [], "entities": []}, {"text": "Specifically this entails developing an understanding of the categories semantic fields and then, for each document, to classify it into the closest category.", "labels": [], "entities": []}, {"text": "The novelty of our method hinges on the diversity of enrichment techniques of the categories label, including expert input that assists the semantic expansion and the use of word embeddings, both generic and domain specific.", "labels": [], "entities": [{"text": "semantic expansion", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.7231400310993195}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we provide an overview of the relevant literature.", "labels": [], "entities": []}, {"text": "Section 3 contains a detailed description of our approach.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 describe the results of its application to standard corpora and operational risks incidents respectively.", "labels": [], "entities": []}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our approach, we conduct experiments on five standard text classification corpora, described listed in  In our method, an offline process is used to extract initial keywords from category labels.", "labels": [], "entities": []}, {"text": "For the purpose of testing our approach, we had to emulate human experts ourselves.", "labels": [], "entities": []}, {"text": "For each category, one team member added a few keywords based only on label description.", "labels": [], "entities": []}, {"text": "Then, we randomly selected 2 or 3 documents for each label that were read by two team members who used them to identify 5 to 10 salient words to be added to each dictionary.", "labels": [], "entities": []}, {"text": "In average, we manually added 9 words per label for 20NewsGroup, 17 words for AGs Corpus and Google-Snippets, 11 words for YahooAnswers and 14 words for 5AbstractsGroup.", "labels": [], "entities": [{"text": "AGs Corpus", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.7996172606945038}, {"text": "YahooAnswers", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.9423415660858154}, {"text": "5AbstractsGroup", "start_pos": 153, "end_pos": 168, "type": "DATASET", "confidence": 0.940530002117157}]}, {"text": "We present in, the output of that process for the AGs Corpus dataset.", "labels": [], "entities": [{"text": "AGs Corpus dataset", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.887251615524292}]}, {"text": "Once we identify initial keywords, we make the series of enrichment steps described in section 3.2.", "labels": [], "entities": []}, {"text": "For every word in the set of initial keywords, we add all its synonym sets from WordNet as well as the 10 most similar words from Glove, CBOW and Skip-Gram.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9806569218635559}]}, {"text": "The average length of label dictionaries obtained from the full enrichment pipeline (which we refer to as all keywords) is 428 words.", "labels": [], "entities": []}, {"text": "We use the word2vec python implementation provided by gensim.", "labels": [], "entities": []}, {"text": "For Skip-gram and CBOW, a 10-word window size is used to provide the same amount of raw information.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.8163744211196899}]}, {"text": "Also words appearing 3 times or fewer are filtered out, 10 workers were used and training was performed in 100 epochs.", "labels": [], "entities": []}, {"text": "We chose 300 for the size of all word embeddings, it has been reported to perform well in classification tasks (.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.9057494103908539}]}, {"text": "Filtering word dictionaries with the Functionaware Component (FAC) allowed to keep in average 37% of all keywords per label.", "labels": [], "entities": [{"text": "Functionaware Component (FAC)", "start_pos": 37, "end_pos": 66, "type": "METRIC", "confidence": 0.6867368698120118}]}, {"text": "As described previously, once different versions of label dictionaries have been obtained, we calculate their similarity with input documents using LSA and Cosine distance.", "labels": [], "entities": [{"text": "LSA", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.8529964089393616}]}, {"text": "The optimal dimension (k) of the latent space depends on the dataset.", "labels": [], "entities": []}, {"text": "Optimal k values are typically in the range of 100-300 dimensions.", "labels": [], "entities": []}, {"text": "In this work, for each dataset, we set a range of 100-300 values, and we determine the optimal k by maximizing the topic coherence score.", "labels": [], "entities": []}, {"text": "The multi-class classification performance was evaluated in terms of precision (Prec.), recall (Rec.) and F1-score (F1).", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9997444748878479}, {"text": "Prec.", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.6996151804924011}, {"text": "recall (Rec.)", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.9058257341384888}, {"text": "F1-score", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9994921684265137}, {"text": "F1)", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.895347535610199}]}, {"text": "All measures are computed based on a weighted average of each class using the number of true instances to determine the weights.", "labels": [], "entities": []}, {"text": "summarizes the performance of each of the methods tested on the five corpora that we considered.", "labels": [], "entities": []}, {"text": "Overall, the various configurations of our method, all leveraging embeddings for semantic expansion, outperform the simple unsupervised baselines, leading to a doubling of the F1-score for all corpora, the least affected being the 5Abstracts-Group where F1 goes from 38.1 to 68.3 percent, comparing with the all keywords variant of our method.", "labels": [], "entities": [{"text": "semantic expansion", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7264831811189651}, {"text": "F1-score", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9985429048538208}, {"text": "F1", "start_pos": 254, "end_pos": 256, "type": "METRIC", "confidence": 0.9974161386489868}]}], "tableCaptions": [{"text": " Table 1: Statistics of the five mainstream datasets for  text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8587239682674408}]}, {"text": " Table 3: Performance of our methods and baseline methods on five standard text classification corpora. Bold  numbers indicate the best configurations among the unsupervised approaches. Configurations of our approach do  not contain the representative document enrichment step.", "labels": [], "entities": []}, {"text": " Table 4: Performance of our Method on the Opera- tional Risk Text Classification Task", "labels": [], "entities": [{"text": "Opera- tional Risk Text Classification", "start_pos": 43, "end_pos": 81, "type": "TASK", "confidence": 0.6857799738645554}]}]}