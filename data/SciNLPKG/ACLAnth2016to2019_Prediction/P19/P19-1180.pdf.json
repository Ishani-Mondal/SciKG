{"title": [], "abstractContent": [{"text": "We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without explicit supervision.", "labels": [], "entities": []}, {"text": "The model learns by looking at natural images and reading paired captions.", "labels": [], "entities": []}, {"text": "VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images.", "labels": [], "entities": [{"text": "VG-NSL", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.836582601070404}]}, {"text": "We define the concreteness of constituents by their matching scores with images, and use it to guide the parsing of text.", "labels": [], "entities": [{"text": "parsing of text", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.88941490650177}]}, {"text": "Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F 1 scores against gold parse trees.", "labels": [], "entities": [{"text": "MSCOCO data set", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.9720183809598287}, {"text": "F 1 scores", "start_pos": 150, "end_pos": 160, "type": "METRIC", "confidence": 0.9784950812657675}]}, {"text": "We find that VG-NSL is much more stable with respect to the choice of random initialization and the amount of training data.", "labels": [], "entities": []}, {"text": "We also find that the con-creteness acquired by VG-NSL correlates well with a similar measure defined by linguists.", "labels": [], "entities": []}, {"text": "Finally , we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior un-supervised approaches.", "labels": [], "entities": [{"text": "Multi30K data set", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.9781444867451986}]}], "introductionContent": [{"text": "We study the problem of visually grounded syntax acquisition.", "labels": [], "entities": [{"text": "visually grounded syntax acquisition", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.6931796297430992}]}, {"text": "Consider the images in, paired with the descriptive texts (captions) in English.", "labels": [], "entities": []}, {"text": "Given no prior knowledge of English, and sufficient such pairs, one can infer the correspondence between certain words and visual attributes, (e.g., recognizing that \"a cat\" refers to the objects in the blue boxes).", "labels": [], "entities": []}, {"text": "One can further extract constituents, by assuming that concrete spans of words should be processed as a whole, and thus form the constituents.", "labels": [], "entities": []}, {"text": "Similarly, the same process can be applied to verb or prepositional phrases.", "labels": [], "entities": []}, {"text": "This intuition motivates the use of image-text pairs to facilitate automated language learning, including both syntax and semantics.", "labels": [], "entities": []}, {"text": "In this paper we focus on learning syntactic structures, and propose the Visually Grounded Neural Syntax Learner (VG-NSL, shown in).", "labels": [], "entities": []}, {"text": "VG-NSL acquires syntax, in the form of constituency parsing, by looking at images and reading captions.", "labels": [], "entities": [{"text": "VG-NSL", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8362588882446289}, {"text": "constituency parsing", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8048910200595856}]}, {"text": "At a high level, VG-NSL builds latent constituency trees of word sequences and recursively composes representations for constituents.", "labels": [], "entities": []}, {"text": "Next, it matches the visual and textual representations.", "labels": [], "entities": []}, {"text": "The training procedure is built on the hypothesis that a better syntactic structure contributes to a better representation of constituents, which then leads to better alignment between vision and language.", "labels": [], "entities": []}, {"text": "We use no human-labeled constituency trees or other syntactic labeling (such as part-of-speech tags).", "labels": [], "entities": []}, {"text": "Instead, we define a concreteness score of constituents based on their matching with images, and use it to guide the parsing of sentences.", "labels": [], "entities": [{"text": "parsing of sentences", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.8733081420262655}]}, {"text": "At test time, no images paired with the text are needed.", "labels": [], "entities": []}, {"text": "We compare VG-NSL with prior approaches to unsupervised language learning, most of which do not use visual grounding.", "labels": [], "entities": []}, {"text": "Our first finding is that VG-NSL improves over the best previous approaches to unsupervised constituency parsing in terms of F 1 scores against gold parse trees.", "labels": [], "entities": [{"text": "VG-NSL", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.7794526219367981}, {"text": "constituency parsing", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.6863498985767365}, {"text": "F 1 scores", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9723511536916097}]}, {"text": "We also find that many existing approaches are quite unstable with respect to the choice of random initialization, whereas VG-NSL exhibits consistent parsing results across multiple training runs.", "labels": [], "entities": []}, {"text": "Third, we analyze the performance of different models on different types of constituents, and find that our model shows substantial improvement on noun phrases and prepositional phrases which are common in captions.", "labels": [], "entities": []}, {"text": "Fourth, VG-NSL is much more data-efficient than prior work based purely on text, achieving comparable performance to other approaches using only 20% of the training captions.", "labels": [], "entities": [{"text": "VG-NSL", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.7788876891136169}]}, {"text": "In addition, the concreteness score, which emerges during the matching between constituents and images, correlates well with a similar measure defined by linguists.", "labels": [], "entities": []}, {"text": "Finally, VG-NSL can be easily extended to multiple languages, which we evaluate on the Multi30K data set consisting of German and French image captions.", "labels": [], "entities": [{"text": "VG-NSL", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.9031929969787598}, {"text": "Multi30K data set", "start_pos": 87, "end_pos": 104, "type": "DATASET", "confidence": 0.9570910930633545}]}], "datasetContent": [{"text": "We evaluate VG-NSL for unsupervised parsing in a few ways: F 1 score with gold trees, selfconsistency across different choices of random initialization, performance on different types of constituents, and data efficiency.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9809492230415344}]}, {"text": "In addition, we find that the concreteness score acquired by VG-NSL is consistent with a similar measure defined by linguists.", "labels": [], "entities": []}, {"text": "We focus on English for the main experiments, but also extend to German and French.", "labels": [], "entities": []}, {"text": "It is important to confirm that the constituency parse trees of the MSCOCO captions produced by   A failure example by Benepar, where it fails to parse the noun phrase \"three white sinks in a bathroom under mirrors\" -according to human commonsense, it is much more common for sinks, rather than a bathroom, to be under mirrors.", "labels": [], "entities": [{"text": "MSCOCO captions", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.848814845085144}]}, {"text": "However, most of the constituents (e.g., \"three white sinks\" and \"under mirrors\") are still successfully extracted by Benepar.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Recall of specific typed phrases, and overall F 1 score, evaluated on the MSCOCO test split, averaged over  5 runs with different random initializations. We also include self-agreement F 1 score (Williams et al., 2018) across  the 5 runs. \u00b1 denotes standard deviation. * denotes models requiring extra labels and/or corpus, and  \u2020 denotes  models requiring a pre-trained visual feature extractor. We highlight the best number in each column among all  models that do not require extra data other than paired image-caption data, as well as the overall best number. The  Left, Right, PMI, and concreteness estimation-based models have no standard deviation or self F 1 (shown as N/A)  as they are deterministic given the training and/or testing data.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9861432115236918}, {"text": "MSCOCO test split", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.940054714679718}, {"text": "self-agreement F 1 score", "start_pos": 180, "end_pos": 204, "type": "METRIC", "confidence": 0.7791329324245453}, {"text": "self F 1", "start_pos": 668, "end_pos": 676, "type": "METRIC", "confidence": 0.7391930421193441}]}, {"text": " Table 2: Agreement between our concreteness esti- mates and existing models or labels, evaluated via the  Pearson correlation coefficient computed over the most  frequent 100 words in the MSCOCO test set, averaged  over 5 runs with different random initialization.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 107, "end_pos": 138, "type": "METRIC", "confidence": 0.961355984210968}, {"text": "MSCOCO test set", "start_pos": 189, "end_pos": 204, "type": "DATASET", "confidence": 0.9033955931663513}]}, {"text": " Table 3: Average F 1 scores and Self F 1 scores of VG- NSL and VG-NSL+HI with different model selection  methods. R@1 denotes using recall at 1 (Kiros et al.,  2014) as the model selection criterion. All hyperparam- eters are tuned with respect to self-agreement F 1 score.  The numbers are comparable to those in Table 1.", "labels": [], "entities": [{"text": "Average F 1 scores", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8060736656188965}, {"text": "Self F 1 scores", "start_pos": 33, "end_pos": 48, "type": "METRIC", "confidence": 0.8957809954881668}, {"text": "R@1", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9517655372619629}, {"text": "recall at 1", "start_pos": 133, "end_pos": 144, "type": "METRIC", "confidence": 0.9557802875836691}]}, {"text": " Table 4: F 1 scores on the Multi30K test split (", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9822947084903717}, {"text": "Multi30K test split", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9821731646855673}]}, {"text": " Table 5: Comparison of models for constituency parsing without explicit syntactic supervision. * denotes models  requiring extra labels, such as POS tags or manually labeled concreteness scores. All multimodal methods listed  in the table require a pretrained visual feature extractor (i.e., ResNet-101; He et al., 2016). A model is labeled  as stochastic if for fixed training data and hyperparameters the model may produce different results (e.g., due  to different choices of random initialization). To the best of our knowledge, results on concreteness estimation  (Turney et al., 2011; Brysbaert et al., 2014; Hessel et al., 2018) have not been applied to unsupervised parsing so  far.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8071583807468414}, {"text": "concreteness estimation", "start_pos": 545, "end_pos": 568, "type": "TASK", "confidence": 0.7281406819820404}]}, {"text": " Table 6: Pearson correlation coefficients between existing concreteness estimation methods, including baselines  and VG-NSL+HI. In order to make a fair comparison, the correlation coefficients are evaluated on the 100 most  frequent words on MSCOCO test set.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8744438588619232}, {"text": "MSCOCO test set", "start_pos": 243, "end_pos": 258, "type": "DATASET", "confidence": 0.9689989686012268}]}]}