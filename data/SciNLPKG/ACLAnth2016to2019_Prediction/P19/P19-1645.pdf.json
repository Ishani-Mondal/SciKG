{"title": [{"text": "Learning to Discover, Ground and Use Words with Segmental Neural Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences.", "labels": [], "entities": []}, {"text": "In contrast to previous segmentation models that treat word segmen-tation as an isolated task, our model unifies word discovery, learning how words fit together to form sentences, and, by conditioning the model on visual context, how words' meanings ground in representations of non-linguistic modalities.", "labels": [], "entities": [{"text": "word discovery", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.7260204553604126}]}, {"text": "Experiments show that the unconditional model learns predictive distributions better than character LSTM models, discovers words competitively with nonpara-metric Bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both.", "labels": [], "entities": []}], "introductionContent": [{"text": "How infants discover words that makeup their first language is a long-standing question in developmental psychology.", "labels": [], "entities": []}, {"text": "Machine learning has contributed much to this discussion by showing that predictive models of language are capable of inferring the existence of word boundaries solely based on statistical properties of the input).", "labels": [], "entities": []}, {"text": "However, there are two serious limitations of current models of word learning in the context of the broader problem of language acquisition.", "labels": [], "entities": [{"text": "word learning", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.7320844829082489}, {"text": "language acquisition", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.7234485745429993}]}, {"text": "First, language acquisition involves not only learning what words there are (\"the lexicon\"), but also how they fit together (\"the grammar\").", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.7050405591726303}]}, {"text": "Unfortunately, the best language models, measured in terms of their ability to predict language (i.e., those which seem acquire grammar best), segment quite poorly (, while the strongest models in terms of word segmentation) do not adequately account for the long-range dependencies that are manifest in language and that are easily captured by recurrent neural networks (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 206, "end_pos": 223, "type": "TASK", "confidence": 0.7187334448099136}]}, {"text": "Second, word learning involves not only discovering what words exist and how they fit together grammatically, but also determining their non-linguistic referents, that is, their grounding.", "labels": [], "entities": [{"text": "word learning", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.8327748775482178}]}, {"text": "The work that has looked at modeling acquisition of grounded language from character sequencesusually in the context of linking words to a visually experienced environment-has either explicitly avoided modeling word units) or relied on high-level representations of visual context that overly simplify the richness and ambiguity of the visual signal.", "labels": [], "entities": [{"text": "modeling acquisition of grounded language from character", "start_pos": 28, "end_pos": 84, "type": "TASK", "confidence": 0.8133083667073931}]}, {"text": "In this paper, we introduce a single model that discovers words, learns how they fit together (not just locally, but across a complete sentence), and grounds them in learned representations of naturalistic non-linguistic visual contexts.", "labels": [], "entities": []}, {"text": "We argue that such a unified model is preferable to a pipeline model of language acquisition (e.g., a model where words are learned by one character-aware model, and then a full-sentence grammar is acquired by a second language model using the words predicted by the first).", "labels": [], "entities": []}, {"text": "Our preference for the unified model maybe expressed in terms of basic notions of simplicity (we require one model rather than two), and in terms of the Continuity Hypothesis of, which argues that we should assume, absent strong evidence to the contrary, that children have the same cognitive systems as adults, and differences are due to them having set their parameters differently/immaturely.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9852988719940186}]}, {"text": "In \u00a72 we introduce a neural model of sentences that explicitly discovers and models word-like units from completely unsegmented sequences of characters.", "labels": [], "entities": []}, {"text": "Since it is a model of complete sentences (rather than just a word discovery model), and it can incorporate multimodal conditioning context (rather than just modeling language unconditionally), it avoids the two continuity problems identified above.", "labels": [], "entities": [{"text": "word discovery", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.711535781621933}]}, {"text": "Our model operates by generating text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory of multi-character units.", "labels": [], "entities": []}, {"text": "The segmentation decisions and decisions about how to generate words are not observed in the training data and marginalized during learning using a dynamic programming algorithm ( \u00a73).", "labels": [], "entities": []}, {"text": "Our model depends crucially on two components.", "labels": [], "entities": []}, {"text": "The first is, as mentioned, a lexical memory.", "labels": [], "entities": []}, {"text": "This lexicon stores pairs of a vector (key) and a string (value) the strings in the lexicon are contiguous sequences of characters encountered in the training data; and the vectors are randomly initialized and learned during training.", "labels": [], "entities": []}, {"text": "The second component is a regularizer ( \u00a74) that prevents the model from overfitting to the training data by overusing the lexicon to account for the training data.", "labels": [], "entities": []}, {"text": "Our evaluation ( \u00a75- \u00a77) looks at both language modeling performance and the quality of the induced segmentations, in both unconditional (sequence-only) contexts and when conditioning on a related image.", "labels": [], "entities": []}, {"text": "First, we look at the segmentations induced by our model.", "labels": [], "entities": []}, {"text": "We find that these correspond closely to human intuitions about word segments, competitive with the best existing models for unsupervised word discovery.", "labels": [], "entities": [{"text": "word discovery", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.7011701166629791}]}, {"text": "Importantly, these segments are obtained in models whose hyperparameters are tuned to optimize validation (held-out) likelihood, whereas tuning the hyperparameters of our benchmark models using held-out likelihood produces poor segmentations.", "labels": [], "entities": []}, {"text": "Second, we confirm findings () that show that word segmentation information leads to better language models compared to pure character models.", "labels": [], "entities": [{"text": "word segmentation information", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.8017151157061259}]}, {"text": "However, in contrast to previous work, we realize this performance improvement without having to observe the segment boundaries.", "labels": [], "entities": []}, {"text": "Thus, our model maybe applied straightforwardly to Chinese, where word boundaries are not part of the orthography.", "labels": [], "entities": []}, {"text": "Since the lexical memory stores strings that appear in the training data, each sentence could, in principle, be generated as a single lexical unit, thus the model could fit the training data perfectly while generalizing poorly.", "labels": [], "entities": []}, {"text": "The regularizer penalizes based on the expectation of the powered length of each segment, preventing this degenerate solution from being optimal.", "labels": [], "entities": []}, {"text": "Ablation studies demonstrate that both the lexicon and the regularizer are crucial for good performance, particularly in word segmentationremoving either or both significantly harms performance.", "labels": [], "entities": [{"text": "word segmentationremoving", "start_pos": 121, "end_pos": 146, "type": "TASK", "confidence": 0.7530153393745422}]}, {"text": "Ina final experiment, we learn to model language that describes images, and we find that conditioning on visual context improves segmentation performance in our model (compared to the performance when the model does not have access to the image).", "labels": [], "entities": []}, {"text": "On the other hand, in a baseline model that predicts boundaries based on entropy spikes in a character-LSTM, making the image available to the model has no impact on the quality of the induced segments, demonstrating again the value of explicitly including a word lexicon in the language model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on both English and Chinese segmentation.", "labels": [], "entities": []}, {"text": "For both languages, we used standard datasets for word segmentation and language modeling.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7717151939868927}, {"text": "language modeling", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7387793362140656}]}, {"text": "We also use MS-COCO to evaluate how the model can leverage conditioning context information.", "labels": [], "entities": []}, {"text": "For all datasets, we used train, validation and test splits.", "labels": [], "entities": []}, {"text": "Since our model assumes a closed character set, we removed validation and test samples which contain characters that do not appear in the training set.", "labels": [], "entities": []}, {"text": "In the English corpora, whitespace characters are removed.", "labels": [], "entities": []}, {"text": "In Chinese, they are not present to begin with.", "labels": [], "entities": []}, {"text": "Refer to Appendix A for dataset statistics.", "labels": [], "entities": [{"text": "Appendix A", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9266103804111481}]}, {"text": "To assess whether jointly learning about meanings of words from non-linguistic context affects segmentation performance, we use image and caption pairs from the COCO caption dataset ().", "labels": [], "entities": [{"text": "COCO caption dataset", "start_pos": 161, "end_pos": 181, "type": "DATASET", "confidence": 0.8800177971522013}]}, {"text": "We use 10,000 examples for both training and testing and we only use one reference per image.", "labels": [], "entities": []}, {"text": "The images are used to be conditional context to predict captions.", "labels": [], "entities": []}, {"text": "Refer to Appendix B for the dataset construction process.", "labels": [], "entities": [{"text": "Appendix B", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9682379066944122}]}, {"text": "We compare our model to benchmark Bayesian models, which are currently the best known unsupervised word discovery models, as well as to a simple deterministic segmentation criterion based on surprisal peaks (Elman, 1990) on language modeling and segmentation performance.", "labels": [], "entities": [{"text": "word discovery", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.7324488759040833}]}, {"text": "Although the Bayeisan models are shown to able to discover plausible word-like units, we found that a set of hyperparameters that provides best performance with such model on language modeling does not produce good structures as reported in previous works.", "labels": [], "entities": []}, {"text": "This is problematic since there is no objective criteria to find hyperparameters in fully unsupervised manner when the model is applied to completely unknown languages or domains.", "labels": [], "entities": []}, {"text": "Thus, our experiments are designed to assess how well the models infers word segmentations of unsegmented inputs when they are trained and tuned to maximize the likelihood of the held-out text.", "labels": [], "entities": [{"text": "word segmentations", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7181015163660049}]}, {"text": "DP/HDP Benchmarks Among the most effective existing word segmentation models are those based on hierarchical Dirichlet process (HDP) models () and hierarchical Pitman-Yor processes (.", "labels": [], "entities": [{"text": "DP/HDP Benchmarks", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.7106194868683815}, {"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.726218044757843}]}, {"text": "As a representative of these, we use a simple bigram HDP model: The base distribution, p 0 , is defined over strings in \u03a3 * \u222a{{/S} by deciding with a specified probability to end the utterance, a geometric length model, and a uniform probability over \u03a3 at a each position.", "labels": [], "entities": []}, {"text": "Intuitively, it captures the preference for having short words in the lexicon.", "labels": [], "entities": []}, {"text": "In addition to the HDP model, we also evaluate a simpler single Dirichlet process (DP) version of the model, in which the st 's are generated directly as draws from Categorical(\u03b8 \u00b7 ).", "labels": [], "entities": []}, {"text": "We use an empirical Bayesian approach to select hyperparameters based on the likelihood assigned by the inferred posterior to a held-out validation set.", "labels": [], "entities": []}, {"text": "Refer to Appendix D for details on inference.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9228751063346863}]}, {"text": "Deterministic Baselines Incremental word segmentation is inherently ambiguous (e.g., the letters the might be a single word, or they might be the beginning of the longer word theater).", "labels": [], "entities": [{"text": "Incremental word segmentation", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.69516521692276}]}, {"text": "Nevertheless, several deterministic functions of prefixes have been proposed in the literature as strategies for discovering rudimentary word-like units hypothesized for being useful for bootstrapping the lexical acquisition processor for improving a model's predictive accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 270, "end_pos": 278, "type": "METRIC", "confidence": 0.7672175765037537}]}, {"text": "These range from surprisal criteria to sophisticated language models that switch between models that capture intra-and inter-word dynamics based on deterministic functions of prefixes of characters ().", "labels": [], "entities": []}, {"text": "In our experiments, we also include such deterministic segmentation results using (1) the surprisal criterion of and a two-level hierarchical multiscale LSTM (, which has been shown to predict boundaries in whitespace-containing character sequences at positions corresponding to word boundaries.", "labels": [], "entities": []}, {"text": "As with all experiments in this paper, the BR-corpora for this experiment do not contain spaces.", "labels": [], "entities": []}, {"text": "LSTMs had 512 hidden units with parameters learned using the Adam update rule (.", "labels": [], "entities": []}, {"text": "We evaluated our models with bits-percharacter (bpc) and segmentation accuracy.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9437494874000549}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8096494078636169}]}, {"text": "Refer to Appendices C-F for details of model configurations and evaluation metrics.", "labels": [], "entities": []}, {"text": "For the image caption dataset, we extend the model with a standard attention mechanism in the backbone LSTM (LSTM enc ) to incorporate image context.", "labels": [], "entities": [{"text": "image caption", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.7258078455924988}]}, {"text": "For every character-input, the model calculates attentions over image features and use them to predict the next characters.", "labels": [], "entities": []}, {"text": "As for image representations, we use features from the last convolution layer of a pre-trained VGG19 model).", "labels": [], "entities": [{"text": "VGG19", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.878637433052063}]}, {"text": "We use 8000, 2000 and 10000 images for train, development and test set in order of integer ids specifying image in cocoapi and use first annotation provided for each image.", "labels": [], "entities": []}, {"text": "We will make pairs of image id and annotation id available from https://s3.eu-west-2.", "labels": [], "entities": []}, {"text": "amazonaws.com/k-kawakami/seg.zip.", "labels": [], "entities": []}, {"text": "Language Modeling We evaluated our models with bits-per-character (bpc), a standard evaluation metric for character-level language models.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.672571986913681}]}, {"text": "Following the definition in, bits-percharacter is the average value of \u2212 log 2 p(x t | x <t ) over the whole test set, where |x| is the length of the corpus in characters.", "labels": [], "entities": []}, {"text": "The bpc is reported on the test set.", "labels": [], "entities": [{"text": "bpc", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9545040130615234}]}, {"text": "Segmentation We also evaluated segmentation quality in terms of precision, recall, and F1 of word tokens).", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9995166063308716}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9993172883987427}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9994162321090698}]}, {"text": "To get credit fora word, the models must correctly identify both the left and right boundaries.", "labels": [], "entities": []}, {"text": "For example, if there is a pair of a reference segmentation and a prediction,", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of segmentation performance on  phoneme version of the Brent Corpus (BR-phono).", "labels": [], "entities": [{"text": "Brent Corpus (BR-phono)", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.9293168067932129}]}, {"text": " Table 2: Summary of segmentation performance on  other corpora.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.8964032530784607}]}, {"text": " Table 3: Examples of predicted segmentations on English and Chinese.", "labels": [], "entities": []}, {"text": " Table 4: Test language modeling performance (bpc).", "labels": [], "entities": [{"text": "Test language modeling", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6396666367848715}]}]}