{"title": [{"text": "Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data", "labels": [], "entities": [{"text": "Neural Grammatical Error Correction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8555950671434402}]}], "abstractContent": [{"text": "Considerable effort has been made to address the data sparsity problem in neural grammatical error correction.", "labels": [], "entities": [{"text": "neural grammatical error correction", "start_pos": 74, "end_pos": 109, "type": "TASK", "confidence": 0.7305441349744797}]}, {"text": "In this work, we propose a simple and surprisingly effective unsupervised synthetic error generation method based on confusion sets extracted from a spellchecker to increase the amount of training data.", "labels": [], "entities": [{"text": "synthetic error generation", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.696482390165329}]}, {"text": "Synthetic data is used to pre-train a Transformer sequence-to-sequence model, which not only improves over a strong baseline trained on authentic error-annotated data, but also enables the development of a practical GEC system in a scenario where little genuine error-annotated data is available.", "labels": [], "entities": []}, {"text": "The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F 0.5 in the restricted and low-resource tracks respectively, both on the W&I+LOCNESS test set.", "labels": [], "entities": [{"text": "BEA19", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.45728161931037903}, {"text": "F 0.5", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.968913733959198}, {"text": "W&I+LOCNESS test set", "start_pos": 161, "end_pos": 181, "type": "DATASET", "confidence": 0.9054251483508519}]}, {"text": "On the popular CoNLL 2014 test set, we report state-of-the-art results of 64.16 M 2 for the submitted system , and 61.30 M 2 for the constrained system trained on the NUCLE and Lang-8 data.", "labels": [], "entities": [{"text": "CoNLL 2014 test set", "start_pos": 15, "end_pos": 34, "type": "DATASET", "confidence": 0.9580405056476593}, {"text": "NUCLE and Lang-8 data", "start_pos": 167, "end_pos": 188, "type": "DATASET", "confidence": 0.7646918743848801}]}], "introductionContent": [{"text": "For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7608501315116882}, {"text": "automated Grammatical Error Correction (GEC)", "start_pos": 95, "end_pos": 139, "type": "TASK", "confidence": 0.7625920133931297}]}, {"text": "Work started with statistical phrase-based machine translation (SMT) methods while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently).", "labels": [], "entities": [{"text": "statistical phrase-based machine translation (SMT)", "start_pos": 18, "end_pos": 68, "type": "TASK", "confidence": 0.727772193295615}, {"text": "neural machine translation (NMT)", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.8203601936499277}]}, {"text": "These two papers established a number of techniques for neural GEC, such as transfer learning from monolingual data, strong regularization, model ensembling, and using a large-scale language model.", "labels": [], "entities": [{"text": "neural GEC", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.584229439496994}]}, {"text": "Subsequent work highlighted two challenges in neural GEC, data sparsity and multi-pass decoding: Data sparsity: parallel training data has been enlarged by generating additional parallel sentences during training (Ge et al., 2018a,b), synthesizing noisy sentences (, or pre-training a neural network on a largescale but out-of-domain parallel corpus from Wikipedia ().", "labels": [], "entities": []}, {"text": "Multi-pass decoding: the correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type, involving rightto-left models), or by pipelining SMT and NMT-based systems).", "labels": [], "entities": [{"text": "Multi-pass decoding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7452171444892883}, {"text": "SMT", "start_pos": 222, "end_pos": 225, "type": "TASK", "confidence": 0.920155942440033}]}, {"text": "Motivated by the problems identified in these papers but concerned by the complexity of their methods, we sought simpler and more effective approaches to both challenges.", "labels": [], "entities": []}, {"text": "For data sparsity, we propose an unsupervised synthetic parallel data generation method exploiting confusion sets from a spellchecker to augment training data used for pre-training sequence-to-sequence models.", "labels": [], "entities": []}, {"text": "For multi-pass decoding, we use right-to-left models in rescoring, similar to competitive neural machine translation systems.", "labels": [], "entities": []}, {"text": "In the Building Educational Application (BEA) 2019 Shared Task on Grammatical Error Correction 1, our GEC systems ranked first in the restricted and low-resource tasks.", "labels": [], "entities": [{"text": "Building Educational Application (BEA) 2019 Shared Task on Grammatical Error Correction", "start_pos": 7, "end_pos": 94, "type": "TASK", "confidence": 0.7367566182063177}]}, {"text": "This confirms the effectiveness of the proposed methods in scenarios with and without readily-available large amounts of error-annotated data.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 briefly describes the BEA19 shared task and Section 3 presents related work.", "labels": [], "entities": [{"text": "BEA19 shared task", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.45812047521273297}]}, {"text": "In Section 4 we demonstrate components of our neural GEC systems: transformer models, unsupervised synthetic data generation, ensembling and rescoring methods.", "labels": [], "entities": [{"text": "unsupervised synthetic data generation", "start_pos": 86, "end_pos": 124, "type": "TASK", "confidence": 0.7052146941423416}]}, {"text": "Section 5 provides details of the experiments.", "labels": [], "entities": []}, {"text": "The results are discussed in Sections 6 and 7, and we summarize in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Error-annotated data The restricted models are trained on data provided in the shared task: the FCE corpus), NUCLE (, W&I+LOCNESS data sets, and a preprocessed version of the Lang-8 Corpus of Learner English (.", "labels": [], "entities": [{"text": "FCE corpus", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9806762039661407}, {"text": "NUCLE", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.817126989364624}, {"text": "W&I+LOCNESS data sets", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.5983861131327493}, {"text": "Lang-8 Corpus of Learner English", "start_pos": 175, "end_pos": 207, "type": "DATASET", "confidence": 0.9163105010986328}]}, {"text": "We clean Lang-8 using regular expressions 5 to 1) filter out sentences with a low ratio of alphabetic to non-alphabetic tokens, 2) clear sentences from emoticons and sequences of repeated single nonalphanumeric characters longer than 3 elements e.g. repeated question or exclamation marks, and 3) remove trailing brackets with comments from the target sentences.", "labels": [], "entities": []}, {"text": "If a sentence has alternative corrections, we expand them to separate training examples.", "labels": [], "entities": []}, {"text": "Our final training set in the restricted setting contains 1,953,554 sentences, assembled from the cleaned Lang-8 corpus and oversampled remaining corpora: FCE and the training portion of W&I are oversampled 10 times, NUCLE 5 times.", "labels": [], "entities": [{"text": "Lang-8 corpus", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.9812224209308624}, {"text": "FCE", "start_pos": 155, "end_pos": 158, "type": "DATASET", "confidence": 0.8543607592582703}, {"text": "NUCLE", "start_pos": 217, "end_pos": 222, "type": "DATASET", "confidence": 0.8237628936767578}]}, {"text": "Table 3 summarizes all data sets used for training.", "labels": [], "entities": []}, {"text": "W&I+LOCNESS Dev is used solely as a development set in both tracks.", "labels": [], "entities": [{"text": "W&I+LOCNESS Dev", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.742021232843399}]}, {"text": "Monolingual data We use News Crawl 6 -a publicly available corpus of monolingual texts extracted from online newspapers released for the WMT series of shared tasks) -as our primary monolingual data source.", "labels": [], "entities": [{"text": "News Crawl 6 -a publicly available corpus of monolingual texts extracted from online newspapers", "start_pos": 24, "end_pos": 119, "type": "DATASET", "confidence": 0.8731179515520732}, {"text": "WMT series of shared tasks", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.542231410741806}]}, {"text": "We uniformly sampled 100 million English sentences from de-duplicated crawls in years 2007 to 2018 to produce synthetic parallel data for model pretraining.", "labels": [], "entities": []}, {"text": "Another subset of 2 million sentences was selected to augment the training data during fine-tuning.", "labels": [], "entities": []}, {"text": "The Enchant spellchecker 7 with the Aspell backend and a British English dictionary were used to generate confusion sets.", "labels": [], "entities": [{"text": "Enchant spellchecker 7", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.7765420277913412}, {"text": "British English dictionary", "start_pos": 57, "end_pos": 83, "type": "DATASET", "confidence": 0.9323249260584513}]}, {"text": "Wikipedia edits In the low-resource setting, we use a filtered subset of the WikEd corpus).", "labels": [], "entities": [{"text": "WikEd corpus", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.9583635032176971}]}, {"text": "The original corpus contains 56 million automatically extracted edited sentences from Wikipedia revisions and is quite noisy.", "labels": [], "entities": []}, {"text": "We clean the data using cross-entropy difference filtering by.", "labels": [], "entities": []}, {"text": "W&I+LOCNESS Dev is used as an in-domain seed corpus.", "labels": [], "entities": [{"text": "W&I+LOCNESS Dev", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.7441765666007996}]}, {"text": "All sentence pairs in WikEd are sorted w.r.t an average score from two language models: an n-gram probabilistic word-level language model estimated from target sentences, and a simplified operation sequence model built on edits between source and target sentences.", "labels": [], "entities": [{"text": "WikEd", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9060404300689697}]}, {"text": "We use) to build 5-gram language models.", "labels": [], "entities": []}, {"text": "The top 2 million sentence pairs with the highest scores are used as training data in place of the errorannotated ESL learner data to train models for the low-resource system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results for restricted and low-resource systems on W&I+LOCNESS Dev and FCE Test. Stars () indicate  the submitted systems.", "labels": [], "entities": [{"text": "W&I+LOCNESS Dev", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.819524864355723}, {"text": "FCE Test", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.917169839143753}]}, {"text": " Table 5: Comparison of different methods for infer- ence optimization for the final restricted system on  W&I+LOCNESS Dev.", "labels": [], "entities": [{"text": "infer- ence optimization", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6913814842700958}, {"text": "W&I+LOCNESS Dev", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.6655921737353007}]}, {"text": " Table 6: Comparison with other works on the CoNLL- 2014 and JFLEG test sets. The results for the con- strained system are reported for best systems according  to", "labels": [], "entities": [{"text": "CoNLL- 2014", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9052054087320963}, {"text": "JFLEG test sets", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.8804682890574137}]}, {"text": " Table 7: Official results for top 5 systems in the BEA19  shared task in the restricted (top) and low-resource (bot- tom) tracks. UEdin-MS is our submission.", "labels": [], "entities": [{"text": "BEA19", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.790475606918335}]}]}