{"title": [{"text": "A High-Quality Multilingual Dataset for Structured Documentation Translation", "labels": [], "entities": [{"text": "Structured Documentation Translation", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.8876913785934448}]}], "abstractContent": [{"text": "This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text.", "labels": [], "entities": []}, {"text": "Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform.", "labels": [], "entities": [{"text": "translation of plain text", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.8680177628993988}]}, {"text": "These Web pages have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair.", "labels": [], "entities": []}, {"text": "We build and evaluate translation models for seven target languages from English, with several different copy mechanisms and an XML-constrained beam search.", "labels": [], "entities": []}, {"text": "We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 \u00d7 16 translation settings.", "labels": [], "entities": []}, {"text": "Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures.", "labels": [], "entities": [{"text": "translation", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.942420482635498}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.8876414895057678}]}, {"text": "We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and named entities.", "labels": [], "entities": [{"text": "translation of numerical words and named entities", "start_pos": 71, "end_pos": 120, "type": "TASK", "confidence": 0.8298370667866298}]}, {"text": "We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications , including suitability for post-editing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation is a fundamental research area in the field of natural language processing (NLP).", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.864390105009079}, {"text": "natural language processing (NLP)", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.8164622883001963}]}, {"text": "To build a machine learning-based translation system, we usually need a large amount of bilingually-aligned text segments.", "labels": [], "entities": [{"text": "machine learning-based translation", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.6488413016001383}]}, {"text": "Examples of widely-used datasets are those included in WMT ( and LDC 1 , while new evaluation datasets are being actively created (; Bawden et al.,: English-Japanese examples in our dataset..", "labels": [], "entities": [{"text": "WMT", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.7891795635223389}]}, {"text": "These existing datasets have mainly focused on translating plain text.", "labels": [], "entities": [{"text": "translating plain text", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.8736645976702372}]}, {"text": "On the other hand, text data, especially on the Web, is not always stored as plain text, but often wrapped with markup languages to incorporate document structure and metadata such as formatting information.", "labels": [], "entities": []}, {"text": "Many companies and software platforms provide online help as Web documents, often translated into different languages to deliver useful information to people in different countries.", "labels": [], "entities": []}, {"text": "Translating such Web-structured text is a major component of the process by which companies localize their software or services for new markets, and human professionals typically perform the translation with the help of a translation memory (Silvestre to increase efficiency and maintain consistent termi-nology.", "labels": [], "entities": []}, {"text": "Explicitly handling such structured text can help bring the benefits of state-of-the-art machine translation models to additional real-world applications.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7121018469333649}]}, {"text": "For example, structure-sensitive machine translation models may help human translators accelerate the localization process.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7141458988189697}]}, {"text": "To encourage and advance research on translation of structured text, we collect parallel text segments from the public online documentation of a major enterprise software platform, while preserving the original XML structures.", "labels": [], "entities": [{"text": "translation of structured text", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8715842068195343}]}, {"text": "In experiments, we provide baseline results for seven translation pairs from English, and one nonEnglish pair.", "labels": [], "entities": []}, {"text": "We use standard neural machine translation (NMT) models, and additionally propose an XML-constrained beam search and several discrete copy mechanisms to provide solid baselines for our new dataset.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.8619837065537771}]}, {"text": "The constrained beam search contributes to accurately generating source-conditioned XML structures.", "labels": [], "entities": []}, {"text": "Besides the widely-used BLEU () scores, we also investigate more focused evaluation metrics to measure the effectiveness of our proposed methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9984884262084961}]}, {"text": "In particular, we discuss trade-offs of using the copy mechanisms by focusing on translation of named entities and numerical words.", "labels": [], "entities": [{"text": "translation of named entities and numerical words", "start_pos": 81, "end_pos": 130, "type": "TASK", "confidence": 0.8002613612583706}]}, {"text": "We further report detailed human evaluation and analysis to understand what is already achieved and what needs to be improved for the purpose of helping the human translators (a post-editing context).", "labels": [], "entities": []}, {"text": "As our dataset represents a single, well-defined domain, it can also serve as a corpus for domain adaptation research (either as a source or target domain).", "labels": [], "entities": [{"text": "domain adaptation research", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.7963084975878397}]}, {"text": "We will release our dataset publicly, and discuss potential for future expansion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Text lengths Due to the XML tag-based extraction, our dataset includes word-and phrase-level translations as well as sentence-and paragraphlevel translations, and we can see in that there are many short text segments.", "labels": [], "entities": []}, {"text": "This is, for example, different from the statistics of the widelyused News Commentary dataset.", "labels": [], "entities": [{"text": "News Commentary dataset", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.8592457373936971}]}, {"text": "The text length is defined based on the number of subword tokens, following our experimental setting described below.", "labels": [], "entities": []}, {"text": "Sentence counts Another characteristic of our dataset is that the translation pairs can consist of multiple sentences, and shows the statistics of the number of English sentences in the English-French translation pairs.", "labels": [], "entities": []}, {"text": "The number of    sentences is determined with the sentence splitter from the Stanford CoreNLP toolkit ().", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 77, "end_pos": 101, "type": "DATASET", "confidence": 0.9368330240249634}]}, {"text": "XML-tag counts As we remove the root tags from the XML elements in our dataset construction process, not all the text segments have XML tags inside them.", "labels": [], "entities": []}, {"text": "More concretely, about 25.5% of the translation pairs have at least one internal XML tag, and shows the statistics.", "labels": [], "entities": []}, {"text": "For example, Example (a) in has four XML tags, and Example (b) has three.", "labels": [], "entities": []}, {"text": "We consider multiple evaluation metrics for the new dataset.", "labels": [], "entities": []}, {"text": "For evaluation, we use the true-cased and detokenized text, because our dataset is designed for an end-user, raw-document setting.", "labels": [], "entities": []}, {"text": "BLEU without XML We include the most widely-used metric, BLEU, without XML tags.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.905462384223938}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9940590858459473}]}, {"text": "Named entities and numbers The online help frequently mentions named entities such as product names and numbers, and accurate translations of them are crucial for users.", "labels": [], "entities": []}, {"text": "Frequently, they are not translated but simply copied as English forms.", "labels": [], "entities": []}, {"text": "We evaluate corpus-level precision and recall for translation of the named entities and numerical tokens.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9927341938018799}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9992187023162842}, {"text": "translation of the named entities", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.8365408897399902}]}, {"text": "To extract the named entities and numerical words, we use a rule-based regex script, based on our manual analysis on our dataset.", "labels": [], "entities": []}, {"text": "The numerical words are extracted by The named entities are defined as appearing in a non-alphabetic language, Japanese, because in our dataset we observe that the alphabetic words in such non-alphabetic languages correspond to product names, country names, function names, etc.", "labels": [], "entities": []}, {"text": "XML accuracy, matching, and BLEU For each output text segment, we use the etree module to check if it is a valid XML structure by wrapping it with a dummy root node.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9646244645118713}, {"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9996964931488037}]}, {"text": "Then the XML accuracy score is the number of the valid outputs, divided by the number of the total evaluation examples.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.9685353338718414}]}, {"text": "We further evaluate how many translation outputs have exactly the same XML structures as their corresponding reference text (an XML matching score).", "labels": [], "entities": []}, {"text": "If a translation output matches its reference XML structure, both the translation and reference are split by the XML tags.", "labels": [], "entities": []}, {"text": "We then evaluate corpus-level BLEU by comparing each split segment one by one.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9576940536499023}]}, {"text": "If an output does not match its reference XML structure, the output is treated as empty to penalize the irrelevant outputs.", "labels": [], "entities": []}, {"text": "This section describes our experimental settings.", "labels": [], "entities": []}, {"text": "We will release the preprocessing scripts and the training code (implemented with PyTorch) upon publication.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.8974798917770386}]}, {"text": "More details are described in the supplementary material.", "labels": [], "entities": []}, {"text": "We first focus on the two evaluation metrics: BLEU without XML, and named entities and numbers (NE&NUM).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9929196238517761}]}, {"text": "In, a general observation from the comparison of OT and X is that including segment-internal XML tags tends to improve the BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9978074431419373}]}, {"text": "This is not surprising because the XML tags provide information about explicit or implicit alignments of phrases.", "labels": [], "entities": []}, {"text": "However, the BLEU score of the English-to-Finnish task significantly drops, which indicates that for some languages it is not easy to handle tags within the text.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9815524220466614}]}, {"text": "Another observation is that X rs achieves the best BLEU scores, except for English-to-French.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9987987279891968}]}, {"text": "In our experiments, we have found that the improvement of BLEU comes from the retrieval method, but it degrades the NE&NUM scores, especially the precision.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9988464117050171}, {"text": "NE&NUM scores", "start_pos": 116, "end_pos": 129, "type": "METRIC", "confidence": 0.8006056398153305}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9989880919456482}]}, {"text": "Then copying from the source tends to recover the NE&NUM scores, especially for the recall.", "labels": [], "entities": [{"text": "NE&NUM scores", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.6723258420825005}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9931714534759521}]}, {"text": "We have also observed that using beam search, which improves BLEU scores, degrades the NE&NUM scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9984922409057617}, {"text": "NE", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9190095663070679}, {"text": "NUM", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.6309869289398193}]}, {"text": "A lesson learned from these results is that work to improve BLEU scores can sometimes lead to degradation of other important metrics.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9623853266239166}]}, {"text": "Compatibility with other domains Our dataset is limited to the domain of online help, but we can use it as a seed corpus for domain adaptation if our dataset contains enough information to learn basic grammar translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7545894980430603}, {"text": "grammar translation", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.7649899423122406}]}, {"text": "We conducted a simple domain adaptation experiment in English-to-French by adding 10,000 or 20,000 training examples of the widely-used News Commentary corpus.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7458817362785339}, {"text": "News Commentary corpus", "start_pos": 136, "end_pos": 158, "type": "DATASET", "confidence": 0.7991328636805216}]}, {"text": "We used the newstest2014 dataset for evaluation in the news domain.", "labels": [], "entities": [{"text": "newstest2014 dataset", "start_pos": 12, "end_pos": 32, "type": "DATASET", "confidence": 0.9608630239963531}]}, {"text": "From, we can see that a small amount of the news-domain data significantly improves the target-domain score, and we expect that our dataset plays a good role in domain adaptation for all the covered 17 languages.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7162504345178604}]}, {"text": "shows the evaluation results with XML.", "labels": [], "entities": []}, {"text": "Again, we can see that X rs performs the best in terms of the XML-based BLEU scores, but the absolute values are lower than those in due to the more rigid segment-by-segment comparisons.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9685171842575073}]}, {"text": "This table also shows that the XML accuracy and matching scores are higher than 99% inmost of the cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9614364504814148}, {"text": "matching", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9376081824302673}]}, {"text": "Ideally, the scores could be 100%, but in reality, we set the maximum length of the translations; as a result, sometimes the model cannot find a good path within the length limitation.", "labels": [], "entities": []}, {"text": "Table 5 shows how effective our method is, based on the English-to-Japanese result, and we observed the consistent trend across the different languages.", "labels": [], "entities": []}, {"text": "These results show that our method can accurately generate the relevant XML structures.", "labels": [], "entities": []}, {"text": "How to recover XML attributes?", "labels": [], "entities": []}, {"text": "As described in Section 2.2, we removed all the attributes from the original XML elements for simplicity.", "labels": [], "entities": []}, {"text": "However, we need to recover the attributes when we use our NMT model in the real-world application.", "labels": [], "entities": []}, {"text": "We consider recovering the XML attributes by the copy mechanism from the source; that is, we can copy the attributes from the XML elements in the original source text, if the XML tags are copied from the source.", "labels": [], "entities": []}, {"text": "summarizes how our model generates the XML tags on the EnglishJapanese development set.", "labels": [], "entities": [{"text": "EnglishJapanese development set", "start_pos": 55, "end_pos": 86, "type": "DATASET", "confidence": 0.9376787344614664}]}, {"text": "We can see in the table that most of the XML tags are actually copied from the source.", "labels": [], "entities": []}, {"text": "shows an example of the output of the X rs model.", "labels": [], "entities": []}, {"text": "For this visualization, we merged all the subword tokens to form the standard words.", "labels": [], "entities": []}, {"text": "The tokens in blue are explicitly copied from the source, and we can see that the time expression \"12:57 AM\" and the XML tags are copied as expected.", "labels": [], "entities": []}, {"text": "The output also copies some relevant text segments (in red) from the retrieved translation.", "labels": [], "entities": []}, {"text": "Like this, we can explicitly know which words are copied from which parts, by using our multiple discrete copy mechanisms.", "labels": [], "entities": []}, {"text": "One surprising observation is that the underlined phrase \"for example\" is missing in the translation result, even though the BLEU scores are higher than those on other standard public datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9991528987884521}]}, {"text": "This is atypical error called under translation.", "labels": [], "entities": []}, {"text": "Therefore, no matter how large the BLEU scores are, we definitely need human corrections (or post editing) before providing the translation results to customers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9974308609962463}]}, {"text": "One important application of our NMT models is to help human translators; translating online help has to be precise, and thus any incomplete translations need post-editing.", "labels": [], "entities": []}, {"text": "We asked professional translators at a vendor to evaluate our test set results (with XML) for the English-to-{Finnish, French, German, Japanese} tasks.", "labels": [], "entities": []}, {"text": "For each language pair, we randomly selected 500 test examples, and every example is given an integer score in.", "labels": [], "entities": []}, {"text": "A translation result is rated as \"4\" if it can be used without any modifications, \"3\" if it needs simple post-edits, \"2\" if it needs more post-edits but is better than nothing, and \"1\" if using it is not better than translating from scratch.", "labels": [], "entities": [{"text": "translation", "start_pos": 2, "end_pos": 13, "type": "TASK", "confidence": 0.9522775411605835}]}, {"text": "shows the summary of the evaluation to seethe ratio of each score, and the average scores are also shown.", "labels": [], "entities": []}, {"text": "A positive observation for all the four languages is that more than 50% of the translation results are evaluated as complete or useful in post-editing.", "labels": [], "entities": []}, {"text": "However, there are still many low-quality translation results; for example, around 30% of the Finnish and German results are evaluated as useless.", "labels": [], "entities": []}, {"text": "Moreover, the German results have fewer scores of \"4\", and it took 12 hours for the translators to evaluate the German results, whereas it took 10 hours for the other three languages.", "labels": [], "entities": []}, {"text": "To further make our NMT models useful for post-editing, we have to improve the translations scored as \"1\".", "labels": [], "entities": []}, {"text": "Detailed error analysis We also asked the translators to note what kinds of errors exist for each of the evaluated examples.", "labels": [], "entities": []}, {"text": "All the errors are classified into the six types shown in, and each example can have multiple errors.", "labels": [], "entities": []}, {"text": "The \"Formatting\" type is our task-specific one to evaluate whether the XML tags are correctly inserted.", "labels": [], "entities": []}, {"text": "We can see that the Finnish results have significantly more XML-formatting errors, and this result agrees with our finding that handling the XML tags in Finnish is harder than in other languages, as discussed in Section 5.1.", "labels": [], "entities": []}, {"text": "It is worth further investigating such language-specific problems.", "labels": [], "entities": []}, {"text": "The \"Accuracy\" type covers major issues of NMT, such as adding irrelevant words, skipping important words, and mistranslating phrases.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9854651093482971}, {"text": "NMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9308702349662781}]}, {"text": "As discussed in previous work, reducing the typical errors covered by the \"Accuracy\" type is crucial.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.988319993019104}]}, {"text": "We have also noticed  that the NMT-specific errors would slowdown the human evaluation process, because the NMT errors are different from translation errors made by humans.", "labels": [], "entities": []}, {"text": "The other types of errors would be reduced by improving language models, if we have access to in-domain monolingual corpora.", "labels": [], "entities": []}, {"text": "Can MT help the localization process?", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9866535067558289}]}, {"text": "In general, it is encouraging to observe many \"4\" scores in.", "labels": [], "entities": []}, {"text": "However, one important note is that it takes significant amount of time for the translators to verify the NMT outputs are good enough.", "labels": [], "entities": []}, {"text": "That is, having better scored NMT outputs does not necessarily lead to improving the productivity of the translators; in other words, we need to take into account the time for the quality verification when we consider using our NMT system for that purpose.", "labels": [], "entities": []}, {"text": "Previous work has investigated the effectiveness of NMT models for post-editing, but it has not yet been investigated whether using NMT models can improve the translators' productivity alongside the use of a well-constructed translation memory (Silvestre.", "labels": [], "entities": [{"text": "Silvestre.", "start_pos": 245, "end_pos": 255, "type": "DATASET", "confidence": 0.8627801835536957}]}, {"text": "Therefore, our future work is investigating the effectiveness of using the NMT models in the real-world localization process where a translation memory is available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of the translation examples in the  training data used in our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Automatic evaluation results without XML on the development set, and the test set for X rs .", "labels": [], "entities": []}, {"text": " Table 3: Domain adaptation results (BLEU). The mod- els are tuned on our development set.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9614751935005188}]}, {"text": " Table 4: Automatic evaluation results with XML on the development set, and the test set for X rs .", "labels": [], "entities": []}, {"text": " Table 5: Effects of the XML-constrained beam search.", "labels": [], "entities": []}, {"text": " Table 6: Statistics of the generated XML tags.", "labels": [], "entities": []}, {"text": " Table 7: Ratio [%] of six error types.", "labels": [], "entities": []}]}