{"title": [{"text": "Towards incremental learning of word embeddings using context informativeness", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we investigate the task of learning word embeddings from very sparse data in an incremental, cognitively-plausible way.", "labels": [], "entities": []}, {"text": "We focus on the notion of informativeness, that is, the idea that some content is more valuable to the learning process than other.", "labels": [], "entities": []}, {"text": "We further highlight the challenges of online learning and argue that previous systems fall short of implementing incrementality.", "labels": [], "entities": []}, {"text": "Concretely, we incorporate informativeness in a previously proposed model of nonce learning, using it for context selection and learning rate modulation.", "labels": [], "entities": [{"text": "nonce learning", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.8438969850540161}, {"text": "context selection", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7455940842628479}]}, {"text": "We test our system on the task of learning new words from definitions, as well as on the task of learning new words from potentially unin-formative contexts.", "labels": [], "entities": []}, {"text": "We demonstrate that infor-mativeness is crucial to obtaining state-of-the-art performance in a truly incremental setup.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantics models such as word embeddings ( notoriously require exposure to a large amount of contextual data in order to generate high quality vector representations of words.", "labels": [], "entities": []}, {"text": "This poses practical challenges when the available training data is scarce, or when distributional models are intended to mimic humans' word learning abilities by constructing reasonable word representations from limited observations (.", "labels": [], "entities": []}, {"text": "In recent work, various approaches have been proposed to tackle these problems, ranging from taskspecific auto-encoders generating word embeddings from dictionary definitions only, to Bayesian models used for acquiring definitional properties of words via oneshot learning (, or recursive neural network models making use of morphological structure (.", "labels": [], "entities": []}, {"text": "Arguing that the ideal model should rely on an all-purpose architecture able to learn from any amount of data, proposed a model called Nonce2Vec (N2V), designed as a modification of Word2Vec (W2V;), refactored to allow incremental learning.", "labels": [], "entities": []}, {"text": "The model was tested on two datasets: a) the newly introduced definitional dataset, where the task is to learn a nonce word from its Wikipedia definition; and b) the chimera dataset of, where the task is to reproduce human similarity judgements related to a novel word observed in 2-6 randomly extracted sentences.", "labels": [], "entities": []}, {"text": "The N2V model performed much better than W2V on both datasets but failed to outperform a basic additive model on the chimera dataset, leading the authors to hypothesise that their system would need to perform content selection to deal with the potentially uninformative chimera sentences.", "labels": [], "entities": []}, {"text": "There are two motivations to the present work.", "labels": [], "entities": []}, {"text": "The first is to provide a formal definition of the notion of informativeness applied to both sentential context (as a whole) and context words (taken individually).", "labels": [], "entities": []}, {"text": "To do so, we rely on the intuition that an informative context is a context that is more specific to a given target, and that this notion of context specificity can be quantified by computing the entropy of the probability distribution generated by a language model over a set of vocabulary words, given the context.", "labels": [], "entities": []}, {"text": "The secondary motivation of this work lays in considerations over incrementality.", "labels": [], "entities": []}, {"text": "We show that N2V itself did not fully implement its ideal of 'online' concept learning.", "labels": [], "entities": []}, {"text": "We also point out that architectures that have outperformed N2V since its inception actually move even further from this ideal.", "labels": [], "entities": []}, {"text": "In contrast, we attempt to make our architecture as close as possible to a realistic belief update system, and we demonstrate that informativeness is an essential part of retaining acceptable performance in such a challenging setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the robustness of the results of Herbelot and Baroni (2017), we retrain a skipgram background model with the same hyperparameters but from the more recent Wikipedia snapshot of January 2019, and obtain a similar correlation ratio on the MEN similarity dataset (): \u03c1 = 0.74 vs \u03c1 = 0.75 for Herbelot and . Probability distributions used for computing CI and CWI are generated with a CBOW model trained with gensim on the same Wikipedia snapshot as our skipgram background model, and with the same hyperparameters.", "labels": [], "entities": [{"text": "Wikipedia snapshot of January 2019", "start_pos": 163, "end_pos": 197, "type": "DATASET", "confidence": 0.9558862924575806}, {"text": "MEN similarity dataset", "start_pos": 245, "end_pos": 267, "type": "DATASET", "confidence": 0.7914780179659525}]}, {"text": "For the CWI-based learning rate computation, we set \u03b1 max = 1, chosen according to \u03b1 0 in the original N2V for fair comparison; and \u03b2 = 1000, chosen given min and max CWI values output by CBOW to produce tanh(\u03b2 * x) values distributed across [\u22121, 1] and apply a learning rate \u03b1 max = 1 to maximally informative context words.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 188, "end_pos": 192, "type": "DATASET", "confidence": 0.876440167427063}]}, {"text": "We report results on the definitional and the chimera datasets (see \u00a71).", "labels": [], "entities": []}, {"text": "The definitional dataset contains first sentences from Wikipedia for 1000 words: e.g. Insulin is a peptide hormone produced by beta cells of the pancreatic islets, where the task is to learn the nonce insulin.", "labels": [], "entities": []}, {"text": "Evaluation is performed on 300 test instances in terms of Median Rank (MR) and Mean Reciprocal Rank (MRR).", "labels": [], "entities": [{"text": "Median Rank (MR)", "start_pos": 58, "end_pos": 74, "type": "METRIC", "confidence": 0.8640644788742066}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 79, "end_pos": 105, "type": "METRIC", "confidence": 0.9751842419306437}]}, {"text": "That is, for each instance, the Reciprocal Rank of the gold vector (the one that would be obtained by training standard W2V over the entire corpus) is computed over the sorted list of neighbours of the predicted representation.", "labels": [], "entities": [{"text": "Reciprocal Rank", "start_pos": 32, "end_pos": 47, "type": "METRIC", "confidence": 0.9875416159629822}]}, {"text": "The chimera dataset simulates a nonce situation where speaker encounters words for the first time in naturally-occurring (and not necessarily informative) sentences.", "labels": [], "entities": []}, {"text": "Each nonce instance in the data is associated with 2 (L2), 4 (L4) or 6 (L6) sentences showing the nonce in context, and a set of six word probes human-annotated for similarity to the nonce.", "labels": [], "entities": []}, {"text": "For instance, the nonce VALTUOR is shown in Canned sardines and VALTUOR between two slices of wholemeal bread and thinly spread Flora Original, and its similarity assessed with respect to rhubarb, onion, pear, strawberry, limousine and cushion.", "labels": [], "entities": [{"text": "VALTUOR", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9745292067527771}, {"text": "VALTUOR", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9429125189781189}]}, {"text": "Evaluation is performed on 110 test instances by computing the Spearman correlation between the similarities output by the system for each nonce-probe pair and the similarities from the human subjects.", "labels": [], "entities": []}, {"text": "We evaluate both datasets using a one-shot setup, as per the original N2V paper: each nonce word in considered individually and the background model is reloaded at each test iteration.", "labels": [], "entities": [{"text": "N2V paper", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.9544160664081573}]}, {"text": "We further propose an incremental evaluation setup where the background model is loaded only once at the beginning of testing, keeping its word vectors modifiable during subsequent learning, and where each newly learned nonce representation is added to the background model.", "labels": [], "entities": []}, {"text": "As performance in the incremental setup proved to be dependent on the order of the test items, we report average and standard deviation scores computed from 10 test runs where the test set is shuffled each time.", "labels": [], "entities": [{"text": "standard deviation scores", "start_pos": 117, "end_pos": 142, "type": "METRIC", "confidence": 0.9252481857935587}]}], "tableCaptions": [{"text": " Table 1: Performance of various additive (SUM) and neural (N2V) models on the definitional dataset, measured  in terms of Median Rank (MR) and Mean Reciprocal Rank (MRR). SOTA in one-shot evaluation setup is reported  by the Form-Context model of Schick and Sch\u00fctze (2018).", "labels": [], "entities": [{"text": "Median Rank (MR)", "start_pos": 123, "end_pos": 139, "type": "METRIC", "confidence": 0.8182623982429504}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 144, "end_pos": 170, "type": "METRIC", "confidence": 0.9669830203056335}]}, {"text": " Table 2: Performance of various additive (SUM) and neural (N2V) models on the chimera dataset, measured in  terms of Spearman correlation. SOTA in one-shot evaluation setup on the L2 and L4 test sets are reported by the A  la carte model of", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 118, "end_pos": 138, "type": "METRIC", "confidence": 0.9098486602306366}]}]}