{"title": [{"text": "Surf at MEDIQA 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model", "labels": [], "entities": [{"text": "MEDIQA 2019", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.7552220821380615}, {"text": "Improving Performance of Natural Language Inference", "start_pos": 21, "end_pos": 72, "type": "TASK", "confidence": 0.5886493722597758}]}], "abstractContent": [{"text": "While deep learning techniques have shown promising results in many natural language processing (NLP) tasks, it has not been widely applied to the clinical domain.", "labels": [], "entities": []}, {"text": "The lack of large datasets and the pervasive use of domain-specific language (i.e. abbreviations and acronyms) in the clinical domain causes slower progress in NLP tasks than that of the general NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 160, "end_pos": 169, "type": "TASK", "confidence": 0.8852126598358154}]}, {"text": "To fill this gap, we employ word/subword-level based models that adopt large-scale data-driven methods such as pre-trained language models and transfer learning in analyzing text for the clinical domain.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate the superiority of the proposed methods by achieving 90.6% accuracy in medical domain natural language inference task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9993327260017395}]}, {"text": "Furthermore, we inspect the independent strengths of the proposed approaches in quantitative and qualitative manners.", "labels": [], "entities": []}, {"text": "This analysis will help researchers to select necessary components in building models for the medical domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing (NLP) has broadened its applications rapidly in recent years such as question answering, neural machine translation, natural language inference, and other languagerelated tasks.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7909354964892069}, {"text": "question answering", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8773984313011169}, {"text": "neural machine translation", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.7315587004025778}]}, {"text": "Unlike other tasks in NLP area, the lack of large labeled datasets and restricted access in the clinical domain have discouraged active participation of NLP researchers for this domain.", "labels": [], "entities": []}, {"text": "Furthermore, the pervasive use of abbreviations and acronyms in the clinical domain causes the difficulty of text normalization and makes the related tasks more difficult.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7308308929204941}]}, {"text": "In building NLP models, a word embedding layer that transforms a sequence of tokens in text into a vector representation is considered as one of the fundamental components.", "labels": [], "entities": []}, {"text": "In recent studies, it has been shown that the pre-trained language models by using a huge diversity of corpus (i.e. BERT and) generate deep contextualized word representations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9586371183395386}]}, {"text": "These methods have shown to be very effective for improving the performance of a wide range of NLP tasks by enabling better text understanding and have become a crucial part of the tasks since they have published.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.788612574338913}]}, {"text": "To stimulate the research in the clinical domain, researchers have further investigated to transform the pre-trained language models from general purpose version into the medical domain-specific version.", "labels": [], "entities": []}, {"text": "propose BioBERT that utilizes large-scale bio-medical corpora, PubMed abstracts (PubMed) and PubMed Central full-text articles (PMC), to obtain a medical domain specific language representation through fine-tuning the BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9791861176490784}]}, {"text": "Similarity, a PubMed-ELMo , trained with medical domain corpus, is released as one of the contributed ELMo models for medical domain researchers.", "labels": [], "entities": [{"text": "PubMed-ELMo", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.9260644912719727}]}, {"text": "However, these models are not yet fully explored in medical domain tasks.", "labels": [], "entities": []}, {"text": "Besides these general efforts in building better word representations, introduce a large and publicly available natural language inference (NLI) dataset, called MedNLI, for the medical domain (see).", "labels": [], "entities": []}, {"text": "Considering the expensive annotation cost of medical text due to the sparsity of the clinical-domain experts, the medical NLI task plays an import role in boosting existing datasets for medical question answering systems by retrieving similar questions that are already answered by human experts.", "labels": [], "entities": [{"text": "question answering", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.7617664635181427}]}, {"text": "Along with this effort, ACL-BioNLP 2019 committee announced a shared task, NLI for the medical domain, motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment.", "labels": [], "entities": []}, {"text": "The newly released dataset is larger in size than that of any other previous medical domain NLI dataset, however, it is still not enough to train complicated neural network based models.", "labels": [], "entities": []}, {"text": "To fill this gap, we propose a combination approach of NLP models and machine learning methods to tackle the medical domain NLI task.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows: \u2022 We adopt the pre-trained language models (BioBERT, PubMed-ELMo) to overcome the shortage of training data which is a common problem in the clinical domain.", "labels": [], "entities": []}, {"text": "\u2022 We apply the transfer learning method with two general domain NLI datasets and show that a source task in a domain can benefit learning a target task in a different domain.", "labels": [], "entities": []}, {"text": "\u2022 We show the independent strengths of the proposed approaches in quantitative and qualitative manners.", "labels": [], "entities": []}, {"text": "This analysis will help researchers to select necessary components in building models for the clinical domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "MedNLI (, a large publicly available and expert annotated dataset, has been recently published for the MEDIQA 2019 shared task.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8796897530555725}, {"text": "MEDIQA 2019 shared task", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.6565872579813004}]}, {"text": "This dataset comprises of tuples <P, H, Y > where: P and H area clinical sentence pair, (premise and hypothesis, respectively); Y indicates whether a given hypothesis can be inferred from a given premise.", "labels": [], "entities": []}, {"text": "In particular, Y is categorized as one of three classes: \"entailment\", \"contradiction\", and \"neutral\".", "labels": [], "entities": []}, {"text": "In this research, we are interested in building a model that classifies the given sentence pair into the corresponding category.", "labels": [], "entities": []}, {"text": "First, we consider a point-wise approach that classifies each pair of data independently into one of the three classes.", "labels": [], "entities": []}, {"text": "Next, we re-organize the dataset into the set of a list that contains one of each class sentence pair.", "labels": [], "entities": []}, {"text": "Then we apply list-wise classification that classifies three sentence pair into each \"entailment\", \"contradiction\", and \"neutral\" class exclusively.", "labels": [], "entities": []}, {"text": "We explore three kinds of BioBERT that are fine-tuned from the original BERT with PMC, PubMed, and PMC+PubMed datasets.", "labels": [], "entities": [{"text": "BERT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.8459241986274719}, {"text": "PMC", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8310239911079407}, {"text": "PubMed", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.8543556332588196}, {"text": "PMC+PubMed datasets", "start_pos": 99, "end_pos": 118, "type": "DATASET", "confidence": 0.7997362464666367}]}, {"text": "As shown in table 2, BioBERT trained on PubMed+PMC performs the best.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.8577669858932495}, {"text": "PubMed+PMC", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9401604334513346}]}, {"text": "Thus we select it as abase BioBERT model for the rest of the experiments.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9852883219718933}]}, {"text": "Depends on a need for comparison or better understanding, we also include original BERT in the experiments and report the results.", "labels": [], "entities": [{"text": "BERT", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9975374937057495}]}, {"text": "The overall results of MedNLI are shown in table 3.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.7364874482154846}]}, {"text": "All experiments based on BioBERT and BERT have a fixed learning rate 2e-5.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.5901734828948975}, {"text": "BERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9885222911834717}]}, {"text": "We add early stopping to stop the models from learning if evaluation loss has not decreased for 4 steps where 1 step is defined 20% of the whole training data.", "labels": [], "entities": []}, {"text": "Other than the learning rate and early stopping, all settings are the same as they are in BioBERT and BERT.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 33, "end_pos": 47, "type": "METRIC", "confidence": 0.8071512579917908}, {"text": "BERT", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.7543107271194458}]}, {"text": "For the CompAggr model, we use a context projection weight matrix with 100 dimensions.", "labels": [], "entities": []}, {"text": "In the aggregation part, we use 1-D CNN with a total of 500 filters, which involved five types of filters K \u2208 R {1,2,3,4,5}\u00d7100 , 100 per type.", "labels": [], "entities": []}, {"text": "The weight matrices for the filters were initialized using the Xavier method.", "labels": [], "entities": []}, {"text": "We use the Adam optimizer () including gradient clipping by norm at a threshold of 5.", "labels": [], "entities": []}, {"text": "For the purpose of regularization, we applied dropout () with a ratio of 0.7.", "labels": [], "entities": [{"text": "regularization", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.9820521473884583}]}, {"text": "Transfer learning: We conduct transfer learning on four different combinations of MedNLI, SNLI, and MNLI as it shown in the table 4 (line 4 to 7) and also add the results of general domain tasks (MNLI, SNLI) for comparison.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.900964081287384}, {"text": "MedNLI", "start_pos": 82, "end_pos": 88, "type": "DATASET", "confidence": 0.9223362803459167}, {"text": "MNLI", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.8782148361206055}]}, {"text": "As expected, BERT performs better on tasks in the general domain while BioBERT performs better on MedNLI which is in the clinical domain.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9984819293022156}, {"text": "BioBERT", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.984320878982544}, {"text": "MedNLI", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.8321459889411926}]}, {"text": "In overall, positive transfer occurs on MedNLI.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.956122636795044}]}, {"text": "There are three things we can observe from the results.", "labels": [], "entities": []}, {"text": "First of all, even though BioBERT is finetuned on general domain tasks before MedNLI, transfer learning shows better results than that fine-tuned on MedNLI directly.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.9033393859863281}, {"text": "transfer learning", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.9215475916862488}]}, {"text": "It implies that the same tasks in different domains have overlapping knowledge and transfer learning between the tasks effects positively on each other as the definition of transfer learning mentions in section 4.", "labels": [], "entities": []}, {"text": "Second, the domain specific language representations from BioBERT are maintained while fine-tuning on general domain tasks by showing that the transfer learning results of MedNLI on BioBERT have better performance than the results on BERT (line 4 to 7).", "labels": [], "entities": [{"text": "BERT", "start_pos": 234, "end_pos": 238, "type": "DATASET", "confidence": 0.7752813696861267}]}, {"text": "Lastly, the accuracy of MNLI and SNLI on BioBERT is lower than the accuracy on BERT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997026324272156}, {"text": "MNLI", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.5792915225028992}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9992697834968567}, {"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.5470193028450012}]}, {"text": "The lower accuracy indicates that BioBERT captures different features such as medical terms and generate different representations than what BERT does which are helpful for the clinical domain task, MedNLI, but not for the other two tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993966817855835}, {"text": "BERT", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9851543307304382}, {"text": "MedNLI", "start_pos": 199, "end_pos": 205, "type": "DATASET", "confidence": 0.7237114310264587}]}, {"text": "The best combination is SNLI \u2192 MNLI \u2192 MedNLI on BioBERT.", "labels": [], "entities": []}, {"text": "We refer to the best result of transfer learning as BioBERT (transferred).", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9967969059944153}]}, {"text": "Results analysis for different models: There are fundamental differences between the two models we apply.", "labels": [], "entities": []}, {"text": "BioBERT tokenizes an input sentence   to sub-word level and uses the transformer model while CompAggr uses word-level embeddings and Compare&Aggregate model.", "labels": [], "entities": []}, {"text": "In light of the dissimilar nature, we expect each model captures different features and generates different language representations.", "labels": [], "entities": []}, {"text": "shows the percentage for each area takes of the test set.", "labels": [], "entities": []}, {"text": "CompAggr correctly classifies 97 examples (7% of the test set) which BioBERT classifies them incorrectly while BioBERT classifies 188 examples correctly (13% of the test set) which CompAggr does not.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.7833887934684753}]}, {"text": "It demonstrates that both models have different strength on the MedNLI task.", "labels": [], "entities": [{"text": "MedNLI task", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.6373045146465302}]}, {"text": "We manually examine all promise and hypothesis pairs of each portion of 7% and 13% of the test set with high confidence and \"element\" label.", "labels": [], "entities": []}, {"text": "For CompAggr, we pick pairs with the probability higher than 0.80 which are 6 pairs.", "labels": [], "entities": []}, {"text": "For BioBERT, we select pairs with top 10 probabilities.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.6669935584068298}]}, {"text": "Interestingly, each pair from CompAggr does not have overlapping words between premise and hypothesis.", "labels": [], "entities": []}, {"text": "It appears that CompAggr's strength is in it's ability to capture the relationship between two sentences even though there is no word overlap while BioBERT labels them \"neutral\" except one pair as you can see in table 5.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.8187212347984314}]}, {"text": "In contrast, the majority of the pairs, 7 out of 10, from BioBERT have overlapping words between them.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.8498516082763672}]}, {"text": "Biobert shows strong confidence when premise and hypothesis have overlapping words as below.", "labels": [], "entities": []}, {"text": "\u2022 (Premise) En route to the Emergency Department, she developed worsening substernal chest pain without any radiation.", "labels": [], "entities": [{"text": "Premise", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.9676928520202637}]}, {"text": "\u2022 (Hypothesis) patient has chest pain Lastly, we compute the average conditional probability of the correct results to check the confidence of each model.", "labels": [], "entities": []}, {"text": "The results are 0.87 and 0.82 for BioBERT and CompAggr showing that BioBERT predicts labels with higher confidence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples from the development set of MedNLI.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.8783473372459412}]}, {"text": " Table 2: The BioBERT performance on the MedNLI  task. Each model is trained on three different combina- tions of PMC and PubMed datasets (top score marked  as bold).", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9015953540802002}, {"text": "PubMed datasets", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.8307700157165527}]}, {"text": " Table 3: The model performance of four different meth- ods (top score marked as bold). BioBERT (transferred)  and BioBERT (expanded) refer to the best results of  transfer learning experiments and the result of MedNLI  with abbreviation expansion on BioBERT respectively.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9886060357093811}, {"text": "BioBERT", "start_pos": 115, "end_pos": 122, "type": "METRIC", "confidence": 0.9918734431266785}, {"text": "BioBERT", "start_pos": 251, "end_pos": 258, "type": "METRIC", "confidence": 0.8104318380355835}]}, {"text": " Table 4: All experiment results of transfer learning and abbreviation expansion (top-2 scores marked as bold).  MedNLI (expanded) denotes MedNLI with abbreviation expansion.", "labels": [], "entities": [{"text": "abbreviation expansion", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7859781384468079}]}, {"text": " Table 6: Performance comparison among the top-10  participants (official) of the NLI shared task. Teams [1- 4, 6-10] are from (Wu et al., 2019; Zhu et al., 2019; Xu", "labels": [], "entities": []}]}