{"title": [{"text": "Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue", "labels": [], "entities": [{"text": "Neural NLG", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.6864065676927567}]}], "abstractContent": [{"text": "Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems.", "labels": [], "entities": []}, {"text": "Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem.", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.8714353243509928}]}, {"text": "The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7210927307605743}]}, {"text": "In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 481, "end_pos": 492, "type": "DATASET", "confidence": 0.9780051708221436}]}], "introductionContent": [{"text": "Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems.", "labels": [], "entities": []}, {"text": "With their end-to-end trainability, neural approaches to natural language generation (NNLG), particularly sequence-to-sequence (Seq2Seq) models, have been promoted with great fanfare in recent years (, and avenues like the recent E2E NLG challenge) have made available large datasets to promote the development of these models.", "labels": [], "entities": [{"text": "natural language generation (NNLG)", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.8173438509305319}, {"text": "E2E NLG challenge", "start_pos": 230, "end_pos": 247, "type": "DATASET", "confidence": 0.826932946840922}]}, {"text": "Nevertheless, current NNLG models arguably remain inadequate for most real-world * Alphabetical by first name \u2020 Work done while on leave from Ohio State University task-oriented dialogue systems, given their inability to (i) reliably perform common sentence planning and discourse structuring operations (, (ii) generalize to complex inputs (, and (3) avoid generating texts with semantic errors including hallucinated content.", "labels": [], "entities": [{"text": "Ohio State University task-oriented dialogue", "start_pos": 142, "end_pos": 186, "type": "TASK", "confidence": 0.4961877942085266}, {"text": "sentence planning and discourse structuring", "start_pos": 249, "end_pos": 292, "type": "TASK", "confidence": 0.678515100479126}]}, {"text": "In this paper, we explore the extent to which these issues can be addressed by incorporating lessons from pre-neural NLG systems into a neural framework.", "labels": [], "entities": []}, {"text": "We begin by arguing in favor of enriching the input to neural generators to include discourse relations -long taken to be central in traditional NLG -and underscore the importance of exerting control over these relations when generating text, particularly when using user models to structure responses.", "labels": [], "entities": []}, {"text": "Ina closely related work,, the authors add control tokens (to indicate contrast and sentence structure) to a flat input MR, and show that these can be effectively used to control structure.", "labels": [], "entities": []}, {"text": "However, their methods are only able to control the presence or absence of these relations, without more fine-grained control over their structure.", "labels": [], "entities": []}, {"text": "We thus go beyond their approach and propose using full tree structures as inputs, and generating treestructured outputs as well.", "labels": [], "entities": []}, {"text": "This allows us to define a novel method of constrained decoding for standard sequence-to-sequence models for generation, which helps ensure that the generated text contains all and only the specified content, as in classic approaches to surface realization.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 237, "end_pos": 256, "type": "TASK", "confidence": 0.7397742569446564}]}, {"text": "On the E2E dataset, our experiments demonstrate much better control over CONTRAST relations than using Reed et al.'s method, and also show improved diversity and expressiveness over standard baselines.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9692455530166626}]}, {"text": "We also release anew dataset of responses in the weather domain, which includes the JUSTIFY, JOIN and CONTRAST rela- tions, and where discourse-level structures come into play.", "labels": [], "entities": [{"text": "JUSTIFY", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.5977262854576111}, {"text": "JOIN", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.7737516164779663}, {"text": "CONTRAST", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.8208020329475403}]}, {"text": "On both E2E and weather datasets, we show that constrained decoding over our enriched inputs results in higher semantic correctness as well as better generalizability and data efficiency.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 describes the motivation for using compositional inputs organized around discourse relations.", "labels": [], "entities": []}, {"text": "Section 3 explains our data collection approach and dataset.", "labels": [], "entities": [{"text": "data collection", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.6821401119232178}]}, {"text": "Section 4 shows how to incorporate compositional inputs into NNLG and describes our constrained decoding algorithm.", "labels": [], "entities": [{"text": "NNLG", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.880220890045166}]}, {"text": "Section 5 presents our experimental setup and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "With this representation in mind, we created an ontology of dialog acts, discourse relations, and arguments, for the weather domain.", "labels": [], "entities": []}, {"text": "Our motivation for choosing the weather domain, as explored in (, is that this domain offers significant complexity for NLG.", "labels": [], "entities": []}, {"text": "Weather forecast summaries in particular can be very long, and require reasoning over several disjoint pieces of information.", "labels": [], "entities": [{"text": "Weather forecast summaries", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6981302400430044}]}, {"text": "In this work, we focused on collecting a dataset that showcases the complexity of weather summaries over date/time ranges.", "labels": [], "entities": []}, {"text": "Our weather dataset is also unique in that it was collected in a conversational setup (see below).", "labels": [], "entities": []}, {"text": "We collected our dataset in multiple stages: 1.", "labels": [], "entities": []}, {"text": "We asked crowdworkers to come up with sample queries in the weather domain, like What's the weather like tomorrow? and Do I need an umbrella tonight?", "labels": [], "entities": []}, {"text": "We then wrote rules to automatically parse these queries, and extract key pieces of information, like the location, date, and any attributes that the user specifically requested in the question.", "labels": [], "entities": []}, {"text": "Our goal was to create MRs that are sufficiently expressive and straightforward to create automatically in a practical system.", "labels": [], "entities": [{"text": "MRs", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9780325889587402}]}, {"text": "In the weather domain, it's conceivable that the NLG system has access to a weather API that provides it with detailed weather forecasts for the range requested by the user.", "labels": [], "entities": []}, {"text": "To mimic this setting, we generated artificial weather forecasts for every user query based on the arguments (full argument set in) in the user query.", "labels": [], "entities": []}, {"text": "We then created the tree-structured MR by applying a few different types of automatic rules, like adding CONTRAST to weather conditions that are in opposition.", "labels": [], "entities": [{"text": "CONTRAST", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9946984052658081}]}, {"text": "We add more details of our response generation method and the specific rules for MR creation in Appendix A and B.", "labels": [], "entities": [{"text": "response generation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8751051425933838}, {"text": "MR creation", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.9928506910800934}]}, {"text": "4. Response generation and annotation.", "labels": [], "entities": [{"text": "Response generation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.9565272629261017}]}, {"text": "We presented these tree-structured MRs to trained annotators, and asked them to write responses that expressed the MRs.", "labels": [], "entities": []}, {"text": "They were also given the user query and asked to make their responses natural given the query.", "labels": [], "entities": []}, {"text": "They were allowed to elide in-Reference It'll be sunny throughout this weekend.", "labels": [], "entities": []}, {"text": "The high will be in the 60s, but expect temperatures to drop as low as , activity , condition , humidity precip amount, precip amount unit, precip chance precip chance summary, precip type, sunrise time, temp, temp high , temp low , temp unit wind speed , wind speed unit, sunset time, task bad arg, bad value, error reason: Ontology for the weather domain dataset that we collected.", "labels": [], "entities": [{"text": "precip amount unit", "start_pos": 120, "end_pos": 138, "type": "METRIC", "confidence": 0.8529765804608663}, {"text": "precip chance precip chance summary", "start_pos": 140, "end_pos": 175, "type": "METRIC", "confidence": 0.781714403629303}, {"text": "error reason", "start_pos": 311, "end_pos": 323, "type": "METRIC", "confidence": 0.9762411415576935}]}, {"text": "Arguments marked with * are nested arguments (see).", "labels": [], "entities": []}, {"text": "indicates arguments that have a corresponding not argument; indicates arguments that have a corresponding summary.", "labels": [], "entities": []}, {"text": "formation when arguments were repeated across dialog acts, and could choose the most appropriate surface forms for any arguments based on contextual clues (e.g. referring to a date as tomorrow, rather than April 24 th , depending on the user's date).", "labels": [], "entities": []}, {"text": "Finally, we asked them to label response spans corresponding to each argument, dialog act, and discourse relation in the MR.", "labels": [], "entities": []}, {"text": "Finally, we presented a different group of annotators with the annotated responses, and asked them to provide evaluations of fluency, correctness, naturalness, and annotation correctness.", "labels": [], "entities": []}, {"text": "Our final dataset has 33,493 examples.", "labels": [], "entities": []}, {"text": "Each example comprises a user query, the synthetic user context (datetime and location), the tree-structured MR, the response, and a complete tree-structured annotation of the response.", "labels": [], "entities": []}, {"text": "contains an example from our dataset; as shown, the response annotation structure closely mirrors that of the MR itself.", "labels": [], "entities": []}, {"text": "The MRs and responses in the dataset range from very simple (a single dialog act) to very complex (an MR with a depth and width of 4).", "labels": [], "entities": []}, {"text": "A distribution of this complexity is shown in   syntactic and semantic complexity.", "labels": [], "entities": []}, {"text": "As mentioned before, it has a rich set of referring expressions for dates and date ranges.", "labels": [], "entities": []}, {"text": "It also contains user queries on which the written response was based, thus creating the opportunity for studies on improving naturalness or relevance with respect to the user query.", "labels": [], "entities": []}, {"text": "These could be useful in particular for learning to express recommendations and justifications, as well as YES and NO dialog acts.", "labels": [], "entities": [{"text": "YES", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.5667029023170471}, {"text": "NO dialog", "start_pos": 115, "end_pos": 124, "type": "TASK", "confidence": 0.7574440836906433}]}, {"text": "Our final training set contains 25,390 examples, with 11,879 unique MRs.", "labels": [], "entities": [{"text": "MRs", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.7760727405548096}]}, {"text": "(We consider two MRs to be identical if they have the same delexicalized tree structure -see Section 4.1.)", "labels": [], "entities": []}, {"text": "The test set contains 3,121 examples, of which 1.1K (35%) have unique MRs that have never been seen in the training set.", "labels": [], "entities": [{"text": "MRs", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9803560972213745}]}, {"text": "We also used heuristic techniques to convert the E2E dataset to use tree-structured MRs.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9607348442077637}]}, {"text": "We used the output of tagger to find a character within each slot in the flat MR, and automatically adjusted these to correspond to a token boundary if they didn't already.", "labels": [], "entities": []}, {"text": "We then used the Viterbi segmentations from the model released by to get spans corresponding to each argument.", "labels": [], "entities": []}, {"text": "Finally, we used the Berkeley neural parser) to identify spans coordinated by but, and added CONTRAST relations as parents of the coordinated arguments.", "labels": [], "entities": [{"text": "CONTRAST", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9707798957824707}]}, {"text": "We added JOIN based on sentence boundaries.", "labels": [], "entities": [{"text": "JOIN", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9595581293106079}]}, {"text": "An interesting direction for future research would be  In this section, we first describe our baselines, metrics, and implementation details, followed by experimental results and analyses.", "labels": [], "entities": []}, {"text": "Baselines We consider a few Seq2Seq-based baselines in our experiments (we use the open fairseq implementation) for all our experiments).", "labels": [], "entities": []}, {"text": "All models use an LSTMbased encoder and decoder, with attention.", "labels": [], "entities": []}, {"text": "S2S-FLAT The input is a flat MR (for the E2E dataset, this is equivalent to the original form of the data; for weather, we remove all discourse relations and treat all dialog acts as a single large MR).", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9835339188575745}]}, {"text": "The output is the raw delexicalized response.", "labels": [], "entities": []}, {"text": "S2S-TOKEN Following, we add three tokens in the beginning of flat input MR (same as S2S-FLAT) to indicate the number of contrasts, joins and number of sentences (dialog acts) to be generated.", "labels": [], "entities": []}, {"text": "The output is the raw delexicalized response.", "labels": [], "entities": []}, {"text": "S2S-TREE Same architecture as S2S-FLAT, but the input and output for this model are the linearized tree-structured MR and the treestructured response respectively.", "labels": [], "entities": []}, {"text": "It has the same architecture as S2S-TREE, but decoding during beam search is constrained, as described in Section 4.2.", "labels": [], "entities": [{"text": "beam search", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.7736864387989044}]}, {"text": "Data preprocessing In the input MR, all arguments within each dialog act are ordered alphabetically, to ensure a consistent ordering across examples.", "labels": [], "entities": [{"text": "Data preprocessing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7250832319259644}]}, {"text": "We also use alignments between the reference and the MR to filter information (arguments or dialog acts/discourse relations) that are not expressed in the reference; however, we ensure that any arguments that occur multiple times in the MR, but are elided in the reference for redundancy, are still preserved in the MR.", "labels": [], "entities": []}, {"text": "This ensures that the model doesn't have to learn content selection, while still achieving our primary goal of discourse structure control.", "labels": [], "entities": []}, {"text": "The inputs to S2S-FLAT and S2S-TOKEN are prepared by removing all dialog act and discourse information in the linearized MR, and numbering arguments corresponding to the dialog act they belong in.", "labels": [], "entities": []}, {"text": "Global order of dialog acts is preserved such that arguments of the first act occur before those arguments in the following acts, but arguments within a dialog act are ordered alphabetically.", "labels": [], "entities": []}, {"text": "Metrics We consider automatic and human evaluation metrics for our model.", "labels": [], "entities": []}, {"text": "Automatic metrics are evaluated on the raw model predictions (which have delexicalized fields, like temp low): \u2022 Tree accuracy is a novel metric that we introduce for this problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9443848729133606}]}, {"text": "It measures whether the tree structure in the prediction matches that of the input MR exactly.", "labels": [], "entities": []}, {"text": "We implemented our tree accuracy metric to account for grouping and ellipsis, and will release this implementation along with our dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9606723189353943}, {"text": "grouping", "start_pos": 55, "end_pos": 63, "type": "TASK", "confidence": 0.9523811936378479}]}, {"text": "\u2022 BLEU-4 () is a wordoverlap metric commonly used for evaluating NLG systems.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9986287355422974}]}, {"text": "Due to the limitations of automatic metrics for NLG (, we also performed human evaluation studies by asking annotators to evaluate the quality of responses produced by different models.", "labels": [], "entities": []}, {"text": "Annotators provided binary ratings on the following dimensions: \u2022 Grammaticality: Measures fluency of the responses.", "labels": [], "entities": []}, {"text": "Our evaluation guidelines included considerations for proper subject-verb agreement, word order, repetition, and grammatical completeness.", "labels": [], "entities": [{"text": "repetition", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9521353840827942}]}, {"text": "\u2022 Correctness: Measures semantic correctness of the responses.", "labels": [], "entities": []}, {"text": "Our guidelines included considerations for sentence structure, contrast, hallucinations (incorrectly included attributes), and missing attributes.", "labels": [], "entities": []}, {"text": "We asked annotators to evaluate model predictions against the reference (rather than the MR -see Appendix F).", "labels": [], "entities": []}, {"text": "As mentioned in 3, we asked annotators to provide evaluations of collected responses, and used these to filter out noisy references and annotations from our final dataset.", "labels": [], "entities": []}, {"text": "The ratings were provided on a 1-5 scale and double annotated, and we filtered out 3,404 examples (out of a total 37,162) that had a scoreless than 3 on any of the four dimensions: fluency, correctness, naturalness, annotation correctness.", "labels": [], "entities": [{"text": "correctness", "start_pos": 190, "end_pos": 201, "type": "METRIC", "confidence": 0.9379459619522095}]}, {"text": "We also experimented with a reranked S2S-TREE in which the beam search candidates are reranked for tree accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9702935814857483}]}, {"text": "This yields a tree accuracy of 97.6% and 95.4% on E2E and weather.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9926087260246277}, {"text": "E2E", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.9223892092704773}]}, {"text": "We trained a Recurrent Neural Network Grammar (RNNG) to tag slots in the prediction of S2S-CONSTR in order to filter out hallucinations.", "labels": [], "entities": []}, {"text": "The correctness on filtered test sets rose from 85.89% to 87.44% for E2E, and from 91.82% to 93.84% on weather.", "labels": [], "entities": [{"text": "correctness", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9510458707809448}, {"text": "E2E", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9308106303215027}]}, {"text": "When asking annotators to rate the models on correctness, we asked them to rate the response by comparing it against the reference, rather than against the MR.", "labels": [], "entities": [{"text": "MR", "start_pos": 156, "end_pos": 158, "type": "METRIC", "confidence": 0.8400453329086304}]}, {"text": "This adds the risk that annotators are confused by noisy references, but we found that it increased annotation speed and agreement rates significantly over evaluating against the MR directly.", "labels": [], "entities": [{"text": "annotation speed", "start_pos": 100, "end_pos": 116, "type": "METRIC", "confidence": 0.7839895188808441}]}, {"text": "This is also because our MRs are treestructured and can be hard to read.", "labels": [], "entities": [{"text": "MRs", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8533796072006226}]}, {"text": "We performed double-annotation with a resolution round.", "labels": [], "entities": []}, {"text": "Automatic rejection: When analyzing evaluation results, we found that it was fairly easy to miss the absence of a contrast or a justification in our weather dataset, especially since our dataset is so large.", "labels": [], "entities": [{"text": "Automatic rejection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7697138786315918}]}, {"text": "As a result, annotators were marking several incorrect cases as correct.", "labels": [], "entities": []}, {"text": "To address this issue, we automatically marked as incorrect any examples where the MR had a CONTRAST but the response lacked any contrastive tokens, or where the MR has a JUSTIFY but the response lacked any clear markers of a justification.", "labels": [], "entities": [{"text": "CONTRAST", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9745279550552368}, {"text": "JUSTIFY", "start_pos": 171, "end_pos": 178, "type": "METRIC", "confidence": 0.7960405945777893}]}, {"text": "This eliminated noise from 2.8% of all responses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5.  The vocabulary size is 1485, and the max/aver- age/min lengths of responses are 151/40.6/8. The  dataset also poses several challenges in addition to", "labels": [], "entities": [{"text": "max/aver- age/min lengths", "start_pos": 48, "end_pos": 73, "type": "METRIC", "confidence": 0.7509320192039013}]}, {"text": " Table 4: Defined subfields for nested arguments.", "labels": [], "entities": []}, {"text": " Table 5: Frequency distribution of number of dialog  acts and discourse relations in the weather dataset.", "labels": [], "entities": []}, {"text": " Table 7: Automatic and human evaluated metrics on E2E and Weather datasets. All metrics other than BLEU are  percentages. Corr and Disc are the % of examples for which the model prediction was judged by humans as  semantically correct; Disc is measured on a challenging subset of Corr. * indicates BLEU scores that are sta- tistically significant (p < 0.01) compared to all baselines for that model.  \u2021indicates statistically significant BLEU  scores (p < 0.01 ) compared to S2S-FLAT.  \u2020indicates human-evaluated correctness scores that are statistically  significant (p < 0.05), using McNemar's chi-squared test, compared to all baselines for that model.", "labels": [], "entities": [{"text": "E2E and Weather datasets", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.7537568956613541}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9979950189590454}, {"text": "BLEU", "start_pos": 299, "end_pos": 303, "type": "METRIC", "confidence": 0.995606005191803}, {"text": "BLEU", "start_pos": 439, "end_pos": 443, "type": "METRIC", "confidence": 0.9405633211135864}]}, {"text": " Table 8: E2E dataset diversity metrics. Rows in gray correspond to metrics that we cite from Du\u0161ek et al. (2019).", "labels": [], "entities": [{"text": "E2E dataset diversity", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.8389723300933838}]}]}