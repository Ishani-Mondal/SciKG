{"title": [{"text": "Unsupervised Information Extraction: Regularizing Discriminative Approaches with Relation Distribution Losses", "labels": [], "entities": [{"text": "Unsupervised Information Extraction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5998718837896982}, {"text": "Regularizing Discriminative Approaches", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.8291697303454081}]}], "abstractContent": [{"text": "Unsupervised relation extraction aims at extracting relations between entities in text.", "labels": [], "entities": [{"text": "Unsupervised relation extraction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6473162472248077}]}, {"text": "Previous unsupervised approaches are either gen-erative or discriminative.", "labels": [], "entities": []}, {"text": "Ina supervised setting, discriminative approaches, such as deep neural network classifiers, have demonstrated substantial improvement.", "labels": [], "entities": [{"text": "deep neural network classifiers", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6091888844966888}]}, {"text": "However, these models are hard to train without supervision , and the currently proposed solutions are unstable.", "labels": [], "entities": []}, {"text": "To overcome this limitation, we introduce a skewness loss which encourages the classifier to predict a relation with confidence given a sentence, and a distribution distance loss enforcing that all relations are predicted in average.", "labels": [], "entities": []}, {"text": "These losses improve the performance of discriminative based models, and enable us to train deep neural networks satisfactorily , surpassing current state of the art on three different datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information extraction models aim at discovering the underlying semantic structure linking entities mentioned in a text.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8350386619567871}]}, {"text": "This can be used to build knowledge bases, which are widely used in several applications such as question answering, document retrieval () and logical reasoning).", "labels": [], "entities": [{"text": "question answering", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8988089263439178}, {"text": "document retrieval", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7473854720592499}, {"text": "logical reasoning", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.6993313580751419}]}, {"text": "In the relation extraction (RE) task, we are interested in discovering the semantic (binary) relation that holds between two entities mentioned in text.", "labels": [], "entities": [{"text": "relation extraction (RE)", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8700830698013305}]}, {"text": "The end goal is to extract triplets of the form (subject, relation, object).", "labels": [], "entities": []}, {"text": "A considerable amount of work has been conducted on supervised or weakly-supervised relation extraction, with recent state-of-the-art models using deep neural networks (NN).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7525127530097961}]}, {"text": "Developing unsupervised relation extraction models is interesting for three reasons: they do not necessitate labeled data except for validating the models; (2) can uncover new relation types; and (3) can be trained from large unlabeled datasets, and then fine-tuned for specific relations.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7156774699687958}]}, {"text": "The first unsupervised models used a clustering ( or generative) approach.", "labels": [], "entities": [{"text": "clustering ( or generative)", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6239473104476929}]}, {"text": "The latter, which obtained state-of-the-art performance, still makes a lot of simplifying hypotheses, such as assuming that the entities are conditionally independent between themselves given the relation.", "labels": [], "entities": []}, {"text": "To train more expressive models, a shift to discriminative approaches was necessary.", "labels": [], "entities": []}, {"text": "The open question then becomes how to provide a sufficient learning signal to the classifier.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, only Marcheggiani and Titov followed this path by leveraging representation learning for modeling knowledge bases, and proposed to use an auto-encoder model: their encoder extracts the relation from a sentence, that the decoder uses to predict a missing entity.", "labels": [], "entities": []}, {"text": "However, their encoder is still limited compared to its supervised counterpart (e.g.) and relies on hand-crafted features extracted by natural language processing tools, containing errors and unable to discover new patterns, which might hinder performances.", "labels": [], "entities": []}, {"text": "More importantly, our initial experiments showed that the above model was unstable, especially when using a deep NN relation classifier.", "labels": [], "entities": []}, {"text": "It converged to either of the two following regimes, depending on hyper-parameter settings: always predicting the same relation, or predicting a uniform distribution.", "labels": [], "entities": []}, {"text": "To overcome these limitations, we propose to use two new losses alongside a link prediction loss based on a fill-in-the-blank task, and show experimentally that this is key to learning deep neural network models.", "labels": [], "entities": []}, {"text": "Our contributions are the following: \u2022 We propose two RelDist losses: a skewness loss, which encourages the classifier to pre-dict a class with confidence fora single sentence, and a distribution distance loss, which encourages the classifier to scatter a set of sentences into different classes; \u2022 We perform extensive experiments on the usual NYT+FB dataset, as well as two new datasets; \u2022 We show that our RelDist losses allow us to train a deep PCNN classifier () as well as improve performance of feature-based models).", "labels": [], "entities": [{"text": "NYT+FB dataset", "start_pos": 345, "end_pos": 359, "type": "DATASET", "confidence": 0.8685372322797775}]}, {"text": "In the following, we first discuss related works (Section 2) before describing our model (Section 3) and presenting experimental results (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our model we use labeled datasets, the labels being used for validation 2 and evaluation.", "labels": [], "entities": []}, {"text": "The first dataset is the one of, which is similar to the one used in.", "labels": [], "entities": []}, {"text": "This dataset was built through distant supervision () by aligning sentences from the New York Times corpus (NYT, Sandhaus, 2008) with Freebase (FB, Bollacker et al., 2008) triplets.", "labels": [], "entities": [{"text": "New York Times corpus (NYT, Sandhaus, 2008)", "start_pos": 85, "end_pos": 128, "type": "DATASET", "confidence": 0.8054622709751129}, {"text": "Freebase (FB, Bollacker et al., 2008) triplets", "start_pos": 134, "end_pos": 180, "type": "DATASET", "confidence": 0.8611096468838778}]}, {"text": "Several sentences were filtered out based on features like the length of the dependency path between the two entities, resulting in 2 million sentences with only 41,000 (2%) of them labeled with one of 262 possible relations.", "labels": [], "entities": []}, {"text": "20% of the labeled sentences were set aside for validation, the remaining 80% are used to compute the final results.", "labels": [], "entities": []}, {"text": "We also extracted two datasets from T-REx (Elsahar et al., 2017) which was built as an alignment of Wikipedia with Wikidata (Vrande\u010di\u00b4Vrande\u010di\u00b4c, 2012).", "labels": [], "entities": []}, {"text": "We only consider triplets where both entities appear in the same sentence.", "labels": [], "entities": []}, {"text": "If a single sentence contains multiple triplets, it will appear multiple times in the dataset, each time with a different pair of target entities.", "labels": [], "entities": []}, {"text": "We built the first dataset DS by extracting all triplets of T-REx where the two entities are linked by a relation in Wikidata.", "labels": [], "entities": []}, {"text": "This is the usual distant supervision method.", "labels": [], "entities": []}, {"text": "It resulted in 1189 relations and nearly 12 million sentences, all of them labeled with a relation.", "labels": [], "entities": []}, {"text": "In Wikidata, each relation is annotated with a list of associated surface forms, for example \"shares border with\" can be conveyed by \"borders\", \"adjacent to\", \"next to\", etc.", "labels": [], "entities": []}, {"text": "The second dataset we built, SPO, only contains the sentences where a surface form of the relation also appears, resulting in 763,000 samples (6% of the unfiltered) and 615 relations.", "labels": [], "entities": [{"text": "SPO", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.555306077003479}]}, {"text": "This dataset still contains some misalignment, but should nevertheless be easier for models to extract the correct semantic relation.", "labels": [], "entities": []}, {"text": "We used the B 3 metric used in and, and complemented it with two more metrics commonly seen in clustering task evaluation: V-measure and ARI, allowing us to capture the characteristics of each approach more in detail.", "labels": [], "entities": [{"text": "B 3 metric", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9298218886057535}, {"text": "ARI", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9948350191116333}]}, {"text": "To clearly describe the different metrics, we propose a common probabilistic formulation of those (in practice, they are estimated on the validation and test sets), and use the following notations.", "labels": [], "entities": []}, {"text": "Let X (or Y ) be a random variable corresponding to a sentence.", "labels": [], "entities": []}, {"text": "We denote c(X) the predicted cluster of X and g(X) its conveyed gold relation.", "labels": [], "entities": []}, {"text": "The first metric we compute is a generalization of F 1 for clustering tasks called B 3 (Bagga and Baldwin, 1998).", "labels": [], "entities": []}, {"text": "The B 3 precision and recall are defined as follows: As precision and recall can be trivially maximized by putting each sample in its own cluster or by clustering all samples into a single class, the main metric B 3 F 1 is defined as the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.5676019787788391}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9984068274497986}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9990873336791992}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9978098273277283}, {"text": "B 3 F 1", "start_pos": 212, "end_pos": 219, "type": "METRIC", "confidence": 0.9013978242874146}, {"text": "precision", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.9988516569137573}, {"text": "recall", "start_pos": 269, "end_pos": 275, "type": "METRIC", "confidence": 0.9897662997245789}]}, {"text": "We also consider an entropy-based metric; this metric is defined by the homogeneity and completeness, which are akin to B 3 precision and recall, but rely on conditional entropy: As B 3 , the V-measure is summarized by the F1 value.", "labels": [], "entities": [{"text": "completeness", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.9822392463684082}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.6370735764503479}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9959718585014343}, {"text": "F1", "start_pos": 223, "end_pos": 225, "type": "METRIC", "confidence": 0.9972960352897644}]}, {"text": "Compared to B 3 , the V-measure penalizes small impurities in a relatively \"pure\" cluster more harshly than in less pure ones.", "labels": [], "entities": []}, {"text": "Symmetrically, it penalizes more a degradation of a well clustered relation than of a less well clustered one.", "labels": [], "entities": []}, {"text": "Finally, the Rand Index is defined as the probability that cluster and gold assignments are compatible: The Adjusted Rand Index) is a normalization of the Rand Index such that a random assignment has an ARI of 0, and the maximum is 1.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.8340474963188171}, {"text": "Adjusted Rand Index", "start_pos": 108, "end_pos": 127, "type": "METRIC", "confidence": 0.930358370145162}, {"text": "ARI", "start_pos": 203, "end_pos": 206, "type": "METRIC", "confidence": 0.9944242835044861}]}, {"text": "Compared to the previous metrics, ARI will be less sensitive to a discrepancy between precision/homogeneity and recall/completeness since it is not an harmonic mean of both.", "labels": [], "entities": [{"text": "ARI", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.94526207447052}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9986699819564819}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.996905505657196}]}, {"text": "e1 located in e2 (16.4%) e1 instance of e2 (15.0%) e1 in country e2 (9.6%) e2 instance of e1 (7.4%) e1 shares border e2 (4.5%) e2 shares border e1 (4.5%) e2 located in e1 (4.4%) e2 in country e1 (3.6%) e1 cast member of e2 (2.7%) e1 capital of e2 (1.6%) e1 director of e2 (1.4%) e1 has child e2 (1.2%) e2 has child e1 (1.0%) e1 member of e2 (0.9%) e2 capital of e1 (0.9%): Normalized contingency tables for the TREx SPO dataset.", "labels": [], "entities": [{"text": "TREx SPO dataset", "start_pos": 411, "end_pos": 427, "type": "DATASET", "confidence": 0.934294581413269}]}, {"text": "Each of the 10 columns corresponds to a predicted relation cluster, which were sorted to ease comparison.", "labels": [], "entities": []}, {"text": "The rows identify Wikidata relations sorted by frequency in the TREx SPO corpus.", "labels": [], "entities": [{"text": "TREx SPO corpus", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.8938529888788859}]}, {"text": "The area of each square is proportional to the number of sentences in the cell.", "labels": [], "entities": []}, {"text": "The matrix was normalized so that each row sum to 1, thus it is more akin to a B 3 per-item recall than a true contingency table.", "labels": [], "entities": [{"text": "B 3 per-item recall", "start_pos": 79, "end_pos": 98, "type": "METRIC", "confidence": 0.9240759164094925}]}], "tableCaptions": [{"text": " Table 1: Results (percentage) on our three datasets. The rel-LDA and rel-LDA1 models come from Yao et al.  (2011). The model of Marcheggiani and Titov (2016) is March \u2212L S .", "labels": [], "entities": []}]}