{"title": [{"text": "From Bilingual to Multilingual Neural Machine Translation by Incremental Training", "labels": [], "entities": [{"text": "Multilingual Neural Machine Translation", "start_pos": 18, "end_pos": 57, "type": "TASK", "confidence": 0.6343067437410355}]}], "abstractContent": [{"text": "Multilingual Neural Machine Translation approaches are based on the use of task-specific models and the addition of one more language can only be done by retraining the whole system.", "labels": [], "entities": [{"text": "Multilingual Neural Machine Translation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5889708995819092}]}, {"text": "In this work, we propose anew training schedule that allows the system to scale to more languages without modification of the previous components based on joint training and language-independent en-coder/decoder modules allowing for zero-shot translation.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 233, "end_pos": 254, "type": "TASK", "confidence": 0.6471934914588928}]}, {"text": "This work in progress shows close results to the state-of-the-art in the WMT task.", "labels": [], "entities": [{"text": "WMT task", "start_pos": 73, "end_pos": 81, "type": "TASK", "confidence": 0.9157611727714539}]}], "introductionContent": [{"text": "In recent years, neural machine translation (NMT) has had an important improvement in performance.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.8033524552981058}]}, {"text": "Among the different neural architectures, most approaches are based in an encoder-decoder structure and the use of attention-based mechanisms (.", "labels": [], "entities": []}, {"text": "The main objective is computing a representation of the source sentence that is weighted with attention-based mechanisms to compute the conditional probability of the tokens of the target sentence and the previously decoded target tokens.", "labels": [], "entities": []}, {"text": "Same principles have been successfully applied to multilingual NMT, where the system is able to translate to and from several different languages.", "labels": [], "entities": []}, {"text": "Two main approaches have been proposed for this task, language independent or shared encoder-decoders.", "labels": [], "entities": []}, {"text": "Language independent architectures() in which each language has its own encoder and some additional mechanism is added to produce shared representations, as averaging of the context vectors or sharing the attention mechanism.", "labels": [], "entities": []}, {"text": "These architectures have the flexibility that each language can be trained with its own vocabulary all languages are trained in parallel.", "labels": [], "entities": []}, {"text": "Recent work ( show how to perform many to many translations with independent encoders and decoders just by sharing additional language-specific layers that transformed the language-specific representations into a shared one without the need of a pivot language, On the other hand, architectures that share parameters between all languages) by using a single encoder and decoder trained to be able to translate from and to any of the languages of the system.", "labels": [], "entities": []}, {"text": "This approach presents the advantage that no further mechanisms are required to produced shared representation of the languages as they all share the same vocabulary and parameters, and by training all languages without distinction they allow low resources languages to take benefit of other languages in the system improving their performance.", "labels": [], "entities": []}, {"text": "Even though by sharing vocabulary between all languages the number of required tokens grows as more languages are included in the system, especially when languages employ different scripts in the system, such as Chinese or Russian.", "labels": [], "entities": []}, {"text": "Recent work proposes anew approach to add new languages to a system by adapting the vocabulary (, relying on the shared tokens between the languages to share model parameters, showing that the amount of shared tokens between the languages had an impact in the model performance.", "labels": [], "entities": []}, {"text": "This could limit the capability of the system to adapt to languages with a different script.", "labels": [], "entities": []}, {"text": "These approaches can be further explored into unsupervised machine translation where the system learns to translate between languages without parallel data just by enforcing the generation and representation of the tokens to be similar (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7179144471883774}]}, {"text": "Also related to our method, recent work has explored transfer learning for NMT ( to improve the performance of new translation directions by taking benefit of the information of a previous model.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8862863481044769}]}, {"text": "These approaches are particularly useful in low resources scenarios when a previous model trained with orders of magnitude more examples is available.", "labels": [], "entities": []}, {"text": "This paper proposes a proof of concept of anew multilingual NMT approach.", "labels": [], "entities": []}, {"text": "The current approach is based on joint training without parameter or vocabulary sharing by enforcing a compatible representation between the jointly trained languages and using multitask learning (.", "labels": [], "entities": []}, {"text": "This approach is shown to offer a scalable strategy to new languages without retraining any of the previous languages in the system and enabling zero-shot translation.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 145, "end_pos": 166, "type": "TASK", "confidence": 0.6559610813856125}]}, {"text": "Also it sets up a flexible framework to future work on the usage of pretrained compatible modules for different tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first experiment consists in comparing the performance of the jointly trained system to the standard Transformer.", "labels": [], "entities": []}, {"text": "As explained in previous sections, this joint model is trained to perform two different tasks, auto-encoding and translation in both directions.", "labels": [], "entities": []}, {"text": "In our experiments, these directions are Spanish-English and English-Spanish.", "labels": [], "entities": []}, {"text": "In auto-encoding, both languages provide good results at 98.21 and 97.44 BLEU points for English and Spanish, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9985033273696899}]}, {"text": "In translation, we observe a decrease in performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9699876308441162}]}, {"text": "shows that for both directions the new training performs more than 2 BLEU points below the baseline system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9995507597923279}]}, {"text": "This difference suggests that even though the encoders and decoders of the system are compatible they still present some differences in the internal representation.", "labels": [], "entities": []}, {"text": "Note that the languages chosen for the joint training seem relevant to the final system performance because they are used to define the representations of additional languages.", "labels": [], "entities": []}, {"text": "Further experimentation is required to understand such impact.", "labels": [], "entities": []}, {"text": "Our second experiment consists of incrementally adding different languages to the system, in this case, German and French.", "labels": [], "entities": []}, {"text": "Note that, since we freeze the weights while adding the new language, the order in which we add new languages does not have any impact on performance.", "labels": [], "entities": []}, {"text": "shows that French-English performs 0.9 BLEU points below the baseline and German-English performs 1.33 points below the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9977723956108093}]}, {"text": "French-English is closer to the baseline performance and this maybe due to its similarity to Spanish, one of the languages of the initial system languages.", "labels": [], "entities": []}, {"text": "The added languages have better performance than the jointly trained languages (SpanishEnglish from the previous section).", "labels": [], "entities": [{"text": "SpanishEnglish", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.9321010112762451}]}, {"text": "This maybe to the fact that the auto-encoding task may have a negative impact on the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.8959859311580658}]}, {"text": "Finally, another relevant aspect of the proposed architecture is enabling zero-shot translation.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7362319827079773}]}, {"text": "To evaluate it, we compare the performance of each of the added languages compared to a pivot system based on cascade.", "labels": [], "entities": []}, {"text": "Such a system consists of translating from French (German) to English and from English to Spanish with the standard Transformer.", "labels": [], "entities": []}, {"text": "Results show that the zero shot translation provides a consistent decrease in performance for both cases of zero-shot translation.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.7318691909313202}]}], "tableCaptions": [{"text": " Table 1: Experiment results measured in BLEU score.  All blank positions are not tested or not viable combi- nations with our data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9621895551681519}]}, {"text": " Table 2: Zero-shot results measured in BLEU score", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9977983832359314}]}]}