{"title": [], "abstractContent": [{"text": "Neural models have become one of the most important approaches to dialog response generation.", "labels": [], "entities": [{"text": "dialog response generation", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.8554635842641195}]}, {"text": "However, they still tend to generate the most common and generic responses in the corpus all the time.", "labels": [], "entities": []}, {"text": "To address this problem , we designed an iterative training process and ensemble method based on boosting.", "labels": [], "entities": []}, {"text": "We combined our method with different training and decoding paradigms as the base model, including mutual-information-based decoding and reward-augmented maximum likelihood learning.", "labels": [], "entities": []}, {"text": "Empirical results show that our approach can significantly improve the diversity and relevance of the responses generated by all base models, backed by objective measurements and human evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence-to-sequence models) has become one of the most popular approaches to dialog systems, for it provides a high degree of automation and flexibility.", "labels": [], "entities": [{"text": "automation", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9679065346717834}]}, {"text": "On the other hand, they are known to suffer from the \"dullresponse\" problem (.", "labels": [], "entities": [{"text": "dullresponse", "start_pos": 54, "end_pos": 66, "type": "METRIC", "confidence": 0.9955211877822876}]}, {"text": "Various research attempts have been made to improve the diversity of responses generated by sequence-tosequence models.", "labels": [], "entities": []}, {"text": "One line of research investigate alternatives to maximum likelihood learning and decoding, which is believed to be the main cause of monotonicity.", "labels": [], "entities": []}, {"text": "() employed a decoding objective based on mutual information between contexts and responses; () used reinforcement learning techniques for training the decoder to generate responses that maximize pre-defined rewards instead of perplexities; () adopted adversarial learning, in which a generator is trained to deceive a discriminator that tries to differentiate between generated responses and human responses.", "labels": [], "entities": []}, {"text": "Beside changing training and decoding objectives, () considered reweighting data points by penalizing those with overly frequent responses or by emphasizing high-quality responses.", "labels": [], "entities": []}, {"text": "() introduced stochastic latent variables into their models to capture discourse information on an inter-utterance level.", "labels": [], "entities": []}, {"text": "() experimented with a novel segment-based training and decoding paradigm to help mitigate the problem of redundancy and contradiction.", "labels": [], "entities": []}, {"text": "Yet another type of approach has not been investigated in the literature in the context of response generation -boosting and ensembling, despite having been studied for machine translation (.", "labels": [], "entities": [{"text": "response generation -boosting", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.8366938531398773}, {"text": "machine translation", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7375054359436035}]}, {"text": "Being along established machine learning method, the process typically involves iteratively training multiple models on reweighted instances according to the error of the previous models and combining these models.", "labels": [], "entities": []}, {"text": "The idea has been recently revived and extended to generative models and image generation, which also suffers from diversity problem (.", "labels": [], "entities": [{"text": "generative models", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.923958033323288}, {"text": "image generation", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7190408408641815}]}, {"text": "In computer vision, the state-of-the-art models tend to generate a few categories of objects all the time and ignore the rest, known as the problem of \"missing modes\".", "labels": [], "entities": []}, {"text": "Boosting has been shown to significantly improve the coverage of image generation models.", "labels": [], "entities": [{"text": "image generation", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7177987098693848}]}, {"text": "For language generation, given the prior success with data re-weighting and bootstrap approach (, we believe dialog response generation may benefit from boosting as well.", "labels": [], "entities": [{"text": "language generation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7660165429115295}, {"text": "dialog response generation", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.7447187701861063}]}, {"text": "In this work, we designed a principled framework of boosting response generation, based on the recently developed theory of boosting generative models.", "labels": [], "entities": [{"text": "boosting response generation", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.6749595204989115}]}, {"text": "Moreover, we combined boosting with different training and/or decoding paradigms, and empirically show that boosting can invariably improve them, in both quantitative and qualitative evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our algorithm on single-turn conversations from Persona Dataset (.", "labels": [], "entities": [{"text": "Persona Dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.8538497984409332}]}, {"text": "Participants are instructed to converse according to their given personalized background.", "labels": [], "entities": []}, {"text": "In the preparation of training data, persona descriptions are prepended to the sources, and all trailing punctuations are truncated from the responses.", "labels": [], "entities": []}, {"text": "We use a standard sequence-to-sequence architecture with attention mechanism.", "labels": [], "entities": []}, {"text": "Both encoder and decoder are LSTMs with hidden size of 512 and input size of 300.", "labels": [], "entities": []}, {"text": "Attentional contexts are weighted sums of hidden states of words in personas.", "labels": [], "entities": []}, {"text": "We use Adam optimizer to train the model with learning rate of 0.001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 46, "end_pos": 59, "type": "METRIC", "confidence": 0.9723682701587677}]}, {"text": "All model parameters including word embeddings are randomly initialized between \u22120.1 and 0.1.", "labels": [], "entities": []}, {"text": "In addition to the base models mentioned before, we investigate the combination of RAML and MMI, in which models are trained with RAML and decoded with MMI.", "labels": [], "entities": [{"text": "RAML", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9004292488098145}]}, {"text": "We employ two standard word-overlap-based metrics, BLEU () and ROUGE).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.999116837978363}, {"text": "ROUGE", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9953416585922241}]}, {"text": "We also performed embedding-based evaluation.", "labels": [], "entities": []}, {"text": "We embed the responses using the word averaging approach by, and measure the cosine similarity of the embeddings of generated responses and true responses.", "labels": [], "entities": [{"text": "word averaging", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7484160661697388}]}, {"text": "To measure the diversity of the responses, we perform k-means clustering on their embeddings with 10 clusters, and measure the inertia.", "labels": [], "entities": []}, {"text": "The larger inertia indicates more diversity.", "labels": [], "entities": []}, {"text": "We also show statistics on number of distinct n-grams.", "labels": [], "entities": []}, {"text": "As can be seen in, the general trend of boosting is that performance drastically improves up to the third model, then it slowly gets better or stays the same.", "labels": [], "entities": []}, {"text": "Boosting is far better than bootstrapping.", "labels": [], "entities": [{"text": "Boosting", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.7811003923416138}]}, {"text": "Boosting can improve lexical-level semantic similarity between generate responses and true responses, measured by cosine similarity.", "labels": [], "entities": []}, {"text": "While BLEU scores only fluctuate in a tight range, ROUGE-L suffered from boosting a little, when used on base models that can generate more diversified responses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9987751841545105}, {"text": "ROUGE-L", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9909090399742126}]}, {"text": "But we do not consider BLEU and ROUGE the most important metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9980775117874146}, {"text": "ROUGE", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9908616542816162}]}, {"text": "Diversity measures, including count of distinct n-grams and inertia of clusters, are significantly improved by boosting.", "labels": [], "entities": [{"text": "count", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9838874936103821}]}, {"text": "Combining RAML and MMI seems to give an advantage in BLEU (mainly because generated responses are longer), inertia, and number of unigrams.", "labels": [], "entities": [{"text": "RAML", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9658042788505554}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9995443224906921}]}, {"text": "To ensure the diversified responses are as relevant as before boosting, we ask 5 annotators to evaluate a randomly sampled subset of 100 examples from each base model against its boosted counterpart.", "labels": [], "entities": []}, {"text": "Each context are paired with two responsesone from the base model and one from the boosted model.", "labels": [], "entities": []}, {"text": "The annotators are asked to choose the most appropriate response, or tie if they are equal.", "labels": [], "entities": [{"text": "tie", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9919137954711914}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "On average, about 38 to 47 percent of the time the annotators showed no preferences, and boosted models beat base models for 36 to 45 percent of the trials.", "labels": [], "entities": []}, {"text": "Note that all individual tests show annotators preferred the boosted model over the base model, except for one case, where the annotator chose MMI base model over the boosted model slightly more often.", "labels": [], "entities": []}, {"text": "We also provide an example of generated responses in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Human evaluation results. \"Win\" stands for the boosted model winning.", "labels": [], "entities": [{"text": "Win", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.6151107549667358}]}]}