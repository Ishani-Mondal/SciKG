{"title": [{"text": "What does BERT learn about the structure of language?", "labels": [], "entities": [{"text": "BERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9667586088180542}]}], "abstractContent": [{"text": "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9637869596481323}]}, {"text": "This result indicates the possibility that BERT networks capture structural information about language.", "labels": [], "entities": []}, {"text": "In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 156, "end_pos": 160, "type": "DATASET", "confidence": 0.6134760975837708}]}, {"text": "We first show that BERT's phrasal representation captures phrase-level information in the lower layers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.8220032453536987}]}, {"text": "We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information , with surface features at the bottom, syntactic features in the middle and semantic features at the top.", "labels": [], "entities": [{"text": "BERT", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.7140181064605713}]}, {"text": "BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subject-verb agreement.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8476201295852661}]}, {"text": "Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.", "labels": [], "entities": [{"text": "BERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.8021286725997925}]}], "introductionContent": [{"text": "BERT (Bidirectional Encoder Representations from Transformers)) is a bidirectional variant of Transformer networks () trained to jointly predict a masked word from its context and to classify whether two sentences are consecutive or not.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.980050265789032}]}, {"text": "The trained model can be fine-tuned for downstream NLP tasks such as question answering and language inference without substantial modification.", "labels": [], "entities": [{"text": "question answering", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8684734106063843}, {"text": "language inference", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7263665944337845}]}, {"text": "BERT outperforms previous state-of-the-art models in the eleven NLP tasks in the GLUE benchmark () by a significant margin.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.988766610622406}, {"text": "GLUE benchmark", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.7026942372322083}]}, {"text": "This remarkable result suggests that BERT could \"learn\" structural information about language.", "labels": [], "entities": [{"text": "BERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9392385482788086}]}, {"text": "Can we unveil the representations learned by BERT to proto-linguistics structures?", "labels": [], "entities": [{"text": "BERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.8242064118385315}]}, {"text": "Answering this question could not only help us understand the reason behind the success of BERT but also its limitations, in turn guiding the design of improved architectures.", "labels": [], "entities": [{"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.768016517162323}]}, {"text": "This question falls under the topic of the interpretability of neural networks, a growing field in NLP ().", "labels": [], "entities": [{"text": "interpretability of neural networks", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.8947195708751678}]}, {"text": "An important step forward in this direction is, which shows that BERT captures syntactic phenomena well when evaluated on its ability to track subject-verb agreement.", "labels": [], "entities": [{"text": "BERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.8817024827003479}]}, {"text": "In this work, we perform a series of experiments to probe the nature of the representations learned by different layers of BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.8912110924720764}]}, {"text": "We first show that the lower layers capture phrase-level information, which gets diluted in the upper layers.", "labels": [], "entities": []}, {"text": "Second, we propose to use the probing tasks defined in to show that BERT captures a rich hierarchy of linguistic information, with surface features in lower layers, syntactic features in middle layers and semantic features in higher layers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.7385762929916382}]}, {"text": "Third, we test the ability of BERT representations to track subject-verb agreement and find that BERT requires deeper layers for handling harder cases involving long-distance dependencies.", "labels": [], "entities": [{"text": "BERT", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.7834632992744446}]}, {"text": "Finally, we propose to use the recently introduced Tensor Product Decomposition Network (TPDN) (  to explore different hypotheses about the compositional nature of BERT's representation and find that BERT implicitly captures classical, tree-like structures.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Clustering performance of span representations obtained from different layers of BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.48883306980133057}]}, {"text": " Table 2: Probing task performance for each BERT layer. The value within the parentheses corresponds to the  difference in performance of trained vs. untrained BERT.", "labels": [], "entities": []}, {"text": " Table 3: Subject-verb agreement scores for each BERT  layer. The last five columns correspond to the num- ber of nouns intervening between the subject and the  verb (attractors) in test instances. The average distance  between the subject and the verb is enclosed in paren- theses next to each attractor category.", "labels": [], "entities": [{"text": "BERT", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9006545543670654}]}, {"text": " Table 4: Mean squared error between TPDN and BERT representation for a given layer and role scheme on SNLI  test instances. Each number corresponds to the average across five random initializations.", "labels": [], "entities": [{"text": "Mean squared error", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9557989835739136}, {"text": "BERT", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9951926469802856}]}]}