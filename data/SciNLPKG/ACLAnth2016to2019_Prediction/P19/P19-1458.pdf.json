{"title": [{"text": "An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese", "labels": [], "entities": [{"text": "Transfer Learning-Based Sentiment Analysis", "start_pos": 20, "end_pos": 62, "type": "TASK", "confidence": 0.7842186689376831}]}], "abstractContent": [{"text": "Text classification approaches have usually required task-specific model architectures and huge labeled datasets.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.763828843832016}]}, {"text": "Recently, thanks to the rise of text-based transfer learning techniques, it is possible to pre-train a language model in an unsupervised manner and leverage them to perform effectively on downstream tasks.", "labels": [], "entities": []}, {"text": "In this work we focus on Japanese and show the potential use of transfer learning techniques in text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8649625778198242}]}, {"text": "Specifically, we perform binary and multi-class sentiment classification on the Rakuten product review and Yahoo movie review datasets.", "labels": [], "entities": [{"text": "multi-class sentiment classification", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.6706735988457998}, {"text": "Rakuten product review and Yahoo movie review datasets", "start_pos": 80, "end_pos": 134, "type": "DATASET", "confidence": 0.8701110109686852}]}, {"text": "We show that transfer learning-based approaches perform better than task-specific models trained on 3 times as much data.", "labels": [], "entities": []}, {"text": "Furthermore, these approaches perform just as well for language modeling pre-trained on 1 30 of Wikipedia.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.795549213886261}, {"text": "1 30 of Wikipedia", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.7631058543920517}]}, {"text": "We release our pre-trained models and code as open source.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis is a well-studied task in the field of natural language processing and information retrieval.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9438846409320831}, {"text": "natural language processing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6500838994979858}, {"text": "information retrieval", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.758769690990448}]}, {"text": "In the past few years, researchers have made significant progress from models that make use of deep learning techniques..", "labels": [], "entities": []}, {"text": "However, while there has been significant progress in sentiment analysis for English, not much effort has been invested in analyzing Japanese due to its sparse nature and the dependency on large datasets required by deep learning.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.9657956063747406}]}, {"text": "Japanese script contains no whitespace, and sentences maybe ambiguous such that there are multiple ways to split characters into words, each with a completely different meaning.", "labels": [], "entities": []}, {"text": "To see if existing research can make progress in Japanese, we make use of recent transfer learning models such as ELMo,, and BERT () to each pretrain a language model which can then be used to perform downstream tasks.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9290248155593872}, {"text": "BERT", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9982231259346008}]}, {"text": "We test the models on binary and multi-class classification.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.700130432844162}]}, {"text": "First, we train the LM on a large corpus.", "labels": [], "entities": []}, {"text": "Then, we finetune it on a target corpus.", "labels": [], "entities": []}, {"text": "Finally, we train the classifier using labeled examples.", "labels": [], "entities": []}, {"text": "The training process involves three stages as illustrated in.", "labels": [], "entities": []}, {"text": "The basic idea is similar to how fine-tuning ImageNet () helps many computer vision tasks.", "labels": [], "entities": []}, {"text": "However, this model does not require labeled data for pre-training.", "labels": [], "entities": []}, {"text": "Instead, we pre-train a language model in unsupervised manner and then fine-tune it on a domain-specific dataset to efficiently classify using much less data.", "labels": [], "entities": []}, {"text": "This is highly desired since there is alack of large labeled datasets in practice.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained ELMo+BCN and ULMFiT on the Rakuten datasets for 10 epochs each and selected the one that performed best.", "labels": [], "entities": [{"text": "Rakuten datasets", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.9541850388050079}]}, {"text": "Since BERT finetunes all of its layers, we only train for 3 epochs as suggested by.", "labels": [], "entities": [{"text": "BERT", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9382197856903076}]}, {"text": "All transfer learning-based methods outperform previous methods on both datasets, showing that these methods still work well without being fine-tuned on target corpora.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Yahoo test results, in error percentages.", "labels": [], "entities": [{"text": "Yahoo test results", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.8742947777112325}, {"text": "error percentages", "start_pos": 33, "end_pos": 50, "type": "METRIC", "confidence": 0.9125054776668549}]}, {"text": " Table 4: Domain adapted results. ULMFiT *  and  ELMo *  are trained for 5 epochs, while BERT *  is  trained for 10K and 50K steps.", "labels": [], "entities": [{"text": "BERT", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.997570812702179}]}, {"text": " Table 5: Low-shot learning results for the Yahoo  dataset, in error percentages. Transfer learning-based  methods are trained on 1  3 of the total dataset, while the  other models are trained on the whole dataset.", "labels": [], "entities": [{"text": "Yahoo  dataset", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9715518057346344}]}, {"text": " Table 6: Comparison of results using large and small  corpora. The small corpus is uniformly sampled from  the Japanese Wikipedia (100MB). The large corpus is  the entire Japanese Wikipedia (2.9GB).", "labels": [], "entities": [{"text": "Japanese Wikipedia", "start_pos": 112, "end_pos": 130, "type": "DATASET", "confidence": 0.8640256226062775}, {"text": "Japanese Wikipedia", "start_pos": 172, "end_pos": 190, "type": "DATASET", "confidence": 0.8333587646484375}]}, {"text": " Table 7: Results from different parameter updating  strategies. BCN+ELMo and BERT BASE are original  implementations. BCN+ELMo unfreeze shows experi- mental results of fine-tuning both BCN and ELMo lay- ers on target dataset while BERT BASE freeze is where  BERT BASE layers are frozen and only classifier layer  fine-tuned on target dataset", "labels": [], "entities": [{"text": "BERT BASE", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9209230244159698}, {"text": "BERT", "start_pos": 232, "end_pos": 236, "type": "METRIC", "confidence": 0.9860098361968994}, {"text": "BERT", "start_pos": 259, "end_pos": 263, "type": "METRIC", "confidence": 0.9097826480865479}]}]}