{"title": [{"text": "Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks", "labels": [], "entities": [{"text": "Nested Entity Mention Detection", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6378602832555771}]}], "abstractContent": [{"text": "Sequential labeling-based NER approaches restrict each word belonging to at most one entity mention, which will face a serious problem when recognizing nested entity mentions.", "labels": [], "entities": [{"text": "Sequential labeling-based NER", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6034267743428549}]}, {"text": "In this paper, we propose to resolve this problem by modeling and leveraging the head-driven phrase structures of entity mentions , i.e., although a mention can nest other mentions, they will not share the same headword.", "labels": [], "entities": []}, {"text": "Specifically, we propose Anchor-Region Networks (ARNs), a sequence-to-nuggets architecture for nested mention detection.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.6810167133808136}]}, {"text": "ARNs first identify anchor words (i.e., possible head words) of all mentions, and then recognize the mention boundaries for each anchor word by exploiting regular phrase structures.", "labels": [], "entities": []}, {"text": "Furthermore , we also design Bag Loss, an objective function which can train ARNs in an end-to-end manner without using any anchor word annotation.", "labels": [], "entities": [{"text": "Bag Loss", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.4788368493318558}]}, {"text": "Experiments show that ARNs achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks.", "labels": [], "entities": [{"text": "entity mention detection", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.6639088094234467}]}], "introductionContent": [{"text": "Named entity recognition (NER), or more generally entity mention detection 1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7787144184112549}, {"text": "entity mention detection 1", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.7236940786242485}]}, {"text": "NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction), event extraction () and machine reading comprehension (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.8115467429161072}, {"text": "relation extraction", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.8712740838527679}, {"text": "event extraction", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.7831728160381317}]}, {"text": "Previous approaches ( The minister of the department of education convened a meeting.: An example of nested entity mentions.", "labels": [], "entities": []}, {"text": "Due to the nested structure, \"the\",\"department\",\"of\" and \"education\" belong to both PER and ORG mentions.", "labels": [], "entities": [{"text": "PER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.6129094362258911}, {"text": "ORG", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9181868433952332}]}, {"text": "commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token.", "labels": [], "entities": []}, {"text": "These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately, will face a serious problem when recognizing nested entity mentions, where one token may belong to multiple mentions.", "labels": [], "entities": []}, {"text": "For example in, an Organization entity mention \"the department of education\" is nested in another Person entity mention \"the minister of the department of education\".", "labels": [], "entities": []}, {"text": "Nested entity mentions are very common.", "labels": [], "entities": [{"text": "Nested entity mentions", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9286786715189616}]}, {"text": "For instance, in the well-known ACE2005 and RichERE datasets, more than 20% of entity mentions are nested in other mentions.", "labels": [], "entities": [{"text": "ACE2005", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9324769973754883}, {"text": "RichERE datasets", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.9478447139263153}]}, {"text": "Therefore, it is critical to consider nested mentions for real-world applications and downstream tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on three standard English entity mention detection benchmarks with nested mentions: ACE2005, GENIA and TAC-KBP2017 (KBP2017) datasets.", "labels": [], "entities": [{"text": "English entity mention detection", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.5797046273946762}, {"text": "ACE2005", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9460124969482422}, {"text": "GENIA", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.9180936813354492}, {"text": "TAC-KBP2017 (KBP2017) datasets", "start_pos": 128, "end_pos": 158, "type": "DATASET", "confidence": 0.8133820652961731}]}, {"text": "For ACE2005 and GENIA, we used the same setup as previous work (, which will be further discussed in Section 5.4.", "labels": [], "entities": [{"text": "ACE2005", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8784152865409851}, {"text": "GENIA", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.9325019717216492}]}], "tableCaptions": [{"text": " Table 1: Overall experiment results on ACE2005, GENIA and KBP2017 datasets. c is the maximum length of  mention and n refers to the length of sentence. For time complexity, m denotes the number of class and k denotes  the average number of anchor words in each sentence(k << n). The time complexity of Cascaded-CRF depends  on datasets so is not listed here.", "labels": [], "entities": [{"text": "ACE2005", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9672209024429321}, {"text": "GENIA", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.957932710647583}, {"text": "KBP2017 datasets", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.9416950643062592}]}, {"text": " Table 2: The top-10 most frequent anchor words of each type on KBP2017 datasets. Line NIL shows most frequent  words that appears in a mention but are not regarded as anchor words.", "labels": [], "entities": [{"text": "KBP2017 datasets", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.9896636009216309}]}, {"text": " Table 3: F1-scores gap between the anchor detector and  the entire ARNs (anchor + region).", "labels": [], "entities": [{"text": "F1-scores gap", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9657852351665497}]}]}