{"title": [{"text": "Historical Text Normalization with Delayed Rewards", "labels": [], "entities": [{"text": "Historical Text Normalization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6214778025945028}]}], "abstractContent": [{"text": "Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normal-ization, albeit often outperformed by phrase-based models.", "labels": [], "entities": []}, {"text": "Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text nor-malization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board.", "labels": [], "entities": []}, {"text": "Policy gradient training , in particular, leads to more accurate nor-malizations for long or unseen words.", "labels": [], "entities": [{"text": "Policy gradient", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7990616858005524}]}], "introductionContent": [{"text": "Historical text normalization is a common approach to making historical documents accessible and searchable.", "labels": [], "entities": [{"text": "Historical text normalization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6735965808232626}]}, {"text": "It is a challenging problem, since most historical texts were written without fixed spelling conventions, and spelling is therefore at times idiosyncratic.", "labels": [], "entities": []}, {"text": "Traditional approaches to historical text normalization relied on hand-written rules, but recently, several authors have proposed neural models for historical text normalization ().", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6802603105703989}, {"text": "historical text normalization", "start_pos": 148, "end_pos": 177, "type": "TASK", "confidence": 0.6768962442874908}]}, {"text": "Such models are trained using characterlevel maximum-likelihood training, which is inconsistent with the objective of historical text normalization; namely, transduction into modern, searchable word forms.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 118, "end_pos": 147, "type": "TASK", "confidence": 0.6476958890755972}]}, {"text": "The discrepancy between character-level loss and our word-level objective means that model decision costs are biased.", "labels": [], "entities": []}, {"text": "Our objective, however, is reflected by the standard evaluation metric, which is computed as the fraction of benchmark words that are translated correctly.", "labels": [], "entities": []}, {"text": "In order to mitigate the discrepancy between the optimization method and the task objective, work has been carried out on using reinforcement learning to optimize directly for the evaluation metric (.", "labels": [], "entities": []}, {"text": "Reinforcement learning enables direct optimization of exact matches or other non-decomposable metrics, computing updates based on delayed rewards rather than token-level error signals.", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8945924937725067}]}, {"text": "This paper contrasts maximum likelihood training and training with delayed rewards, in the context of sequence-to-sequence historical text normalization ().", "labels": [], "entities": [{"text": "sequence-to-sequence historical text normalization", "start_pos": 102, "end_pos": 152, "type": "TASK", "confidence": 0.5615936294198036}]}, {"text": "Contributions We show that training with delayed rewards achieves better performance than maximum likelihood training across six different historical text normalization benchmarks; and that training with delayed rewards is particularly helpful for long words, words where maximum likelihood training leads to predicting long words, and for unseen words.", "labels": [], "entities": [{"text": "predicting long words", "start_pos": 309, "end_pos": 330, "type": "TASK", "confidence": 0.8656055331230164}]}, {"text": "We note that our approach differs from other applications in the NLP literature in using the mean reward as our baseline, and in comparing different reward functions; we also fine-tune relying only on rewards, rather than a mixture of cross entropy loss and rewards.", "labels": [], "entities": []}], "datasetContent": [{"text": "Historical text normalization datasets are rare and typically rather small.", "labels": [], "entities": [{"text": "Historical text normalization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5664965709050497}]}, {"text": "Most of them are based on collections of medieval documents.", "labels": [], "entities": []}, {"text": "In our experiments, we include six historical text normalization datasets: the English, Hungarian, Icelandic, and Swedish datasets from; the German dataset introduced in; and the Slovene \"Bohori\u010d\" dataset from Ljube\u0161i\u00b4.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.638057698806127}]}, {"text": "We use these datasets in the form provided by, i.e., preprocessed to remove punctuation, perform Unicode normalization, replace digits that do not require normalization with a dummy symbol, and lowercase all tokens.", "labels": [], "entities": [{"text": "Unicode normalization", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.7089226543903351}]}, {"text": "gives an overview of the datasets.", "labels": [], "entities": []}, {"text": "Note the differences in the number of words that are invariant across time, i.e., where the original input word form is the correct prediction according to the manual annotations.", "labels": [], "entities": []}, {"text": "The differences are reasons to expect performance to be higher on English, but lower on Hungarian, for example; since it is easier to learn to memorize the input than to learn abstract transduction patterns.", "labels": [], "entities": []}, {"text": "In practice, we see differences being relatively small.", "labels": [], "entities": []}, {"text": "Performance on English, however, is significantly higher than for the other languages (see).", "labels": [], "entities": []}, {"text": "Our experiments compare maximum likelihood training and policy gradient training across six historical text normalization datasets (cf.).: Comparison of maximum likelihood training (MLE) and policy gradient fine-tuning (MLE+PG), given in word-level accuracy in percent, as well as the error reduction between MLE and MLE+PG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 249, "end_pos": 257, "type": "METRIC", "confidence": 0.7718414068222046}, {"text": "error reduction", "start_pos": 285, "end_pos": 300, "type": "METRIC", "confidence": 0.961320549249649}]}, {"text": "We optimized hyper-parameters on the English development data and used the same hyperparameters across the board (see above).", "labels": [], "entities": [{"text": "English development data", "start_pos": 37, "end_pos": 61, "type": "DATASET", "confidence": 0.7218290170033773}]}, {"text": "Distance metric We also treated the distance metric used as our reward function as a hyperparameter.", "labels": [], "entities": [{"text": "Distance", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8602138757705688}]}, {"text": "shows a comparison of three reward functions on the Icelandic development data: (i) the Levenshtein distance, which is the number of character operations (substitute, insert, delete) to transform one string into another; (ii) the Hamming distance, which is the number of positions at which the corresponding characters of two strings of equal length are different (we pad the shorter of the two strings with spaces); and (iii) the Jaro-Winkler distance (, which is a distance metric designed and best suited for short strings such as person names.", "labels": [], "entities": [{"text": "Icelandic development data", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.619650254646937}, {"text": "Levenshtein distance", "start_pos": 88, "end_pos": 108, "type": "METRIC", "confidence": 0.9086675047874451}]}, {"text": "Levenshtein outperforms Hamming and Jaro-Winkler distance on the English development data, as well as on the Icelandic development data.", "labels": [], "entities": [{"text": "English development data", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.9051646788914999}, {"text": "Icelandic development data", "start_pos": 109, "end_pos": 135, "type": "DATASET", "confidence": 0.8355651100476583}]}, {"text": "We therefore use the Levenshtein distance as the reward function in our experiments.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 21, "end_pos": 41, "type": "METRIC", "confidence": 0.6789329349994659}]}], "tableCaptions": [{"text": " Table 1: Historical datasets with the time period they  represent, the size of their training sets (in tokens), and  the approximate percentage of tokens that are invariant  across time (IAT), i.e. where the historical and normal- ized forms are identical.", "labels": [], "entities": [{"text": "IAT)", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9268271625041962}]}, {"text": " Table 2: Comparison of maximum likelihood training  (MLE) and policy gradient fine-tuning (MLE+PG),  given in word-level accuracy in percent, as well as the  error reduction between MLE and MLE+PG.", "labels": [], "entities": [{"text": "policy gradient fine-tuning (MLE+PG)", "start_pos": 63, "end_pos": 99, "type": "METRIC", "confidence": 0.7388764061033726}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.7646491527557373}, {"text": "error reduction", "start_pos": 159, "end_pos": 174, "type": "METRIC", "confidence": 0.9555469453334808}]}]}