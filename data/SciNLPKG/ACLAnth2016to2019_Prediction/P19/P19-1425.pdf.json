{"title": [{"text": "Robust Neural Machine Translation with Doubly Adversarial Inputs", "labels": [], "entities": [{"text": "Robust Neural Machine Translation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6779522225260735}]}], "abstractContent": [{"text": "Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.855866014957428}]}, {"text": "We propose an approach to improving the robustness of NMT models , which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs.", "labels": [], "entities": []}, {"text": "For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs.", "labels": [], "entities": []}, {"text": "Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements (2.8 and 1.6 BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9975280165672302}]}], "introductionContent": [{"text": "In recent years, neural machine translation (NMT) has achieved tremendous success in advancing the quality of machine translation (.", "labels": [], "entities": [{"text": "neural machine translation (NMT", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.7989644408226013}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7451279163360596}]}, {"text": "As an end-to-end sequence learning framework, NMT consists of two important components, the encoder and decoder, which are usually built on similar neural networks of different types, such as recurrent neural networks, convolutional neural networks (, and more recently on transformer networks (.", "labels": [], "entities": []}, {"text": "To overcome the bottleneck of encoding the entire input sentence into a single vector, an attention mechanism was introduced, which further enhanced translation performance (.", "labels": [], "entities": []}, {"text": "Deeper neural networks with increased model capacities in NMT have also been explored and shown promising results ( .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison with baseline methods trained on different backbone models (second column). * indicates  the method trained using an extra corpus.", "labels": [], "entities": []}, {"text": " Table 3: Results on NIST Chinese-English translation.", "labels": [], "entities": [{"text": "NIST Chinese-English translation", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.7616824905077616}]}, {"text": " Table 5: Comparison of translation results of Transformer and our model for an input and its perturbed input.", "labels": [], "entities": []}, {"text": " Table 7: BLEU scores computed using the zero noise  fraction output as a reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987267851829529}]}, {"text": " Table 8: Ablation study on Chinese-English transla- tion. means that it is included in training.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9934006929397583}, {"text": "Chinese-English transla- tion", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.5328589081764221}]}, {"text": " Table 9: Effect of the ratio value \u03b3 src and \u03b3 trg on  Chinese-English Translation.", "labels": [], "entities": [{"text": "Chinese-English Translation", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.4396049231290817}]}]}