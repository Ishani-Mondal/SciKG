{"title": [{"text": "Robust Machine Translation with Domain Sensitive Pseudo-Sources: Baidu-OSU WMT19 MT Robustness Shared Task System Report", "labels": [], "entities": [{"text": "Robust Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7339344223340353}, {"text": "Baidu-OSU WMT19 MT Robustness Shared Task System Report", "start_pos": 65, "end_pos": 120, "type": "DATASET", "confidence": 0.8690365925431252}]}], "abstractContent": [{"text": "This paper describes the machine translation system developed jointly by Baidu Research and Oregon State University for WMT 2019 Machine Translation Robustness Shared Task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7958238124847412}, {"text": "WMT 2019 Machine Translation Robustness Shared Task", "start_pos": 120, "end_pos": 171, "type": "TASK", "confidence": 0.8180631995201111}]}, {"text": "Translation of social media is a very challenging problem, since its style is very different from normal parallel corpora (e.g. News) and also include various types of noises.", "labels": [], "entities": [{"text": "Translation of social media", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.901692122220993}]}, {"text": "To make it worse, the amount of social media parallel corpora is extremely limited.", "labels": [], "entities": []}, {"text": "In this paper , we use a domain sensitive training method which leverages a large amount of parallel data from popular domains together with a little amount of parallel data from social media.", "labels": [], "entities": []}, {"text": "Furthermore, we generate a parallel dataset with pseudo noisy source sentences which are back-translated from monolingual data using a model trained by a similar domain sensitive way.", "labels": [], "entities": []}, {"text": "We achieve more than 10 BLEU improvement in both En-Fr and Fr-En translation compared with the baseline methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9994101524353027}, {"text": "Fr-En translation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.5760037302970886}]}], "introductionContent": [{"text": "Translation of social media is very challenging.", "labels": [], "entities": [{"text": "Translation of social media", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8897932171821594}]}, {"text": "First, there are various types of noises, such as abbreviations, spelling errors, obfuscated profanities, inconsistent capitalization, Internet slang and emojis.", "labels": [], "entities": []}, {"text": "Second, the amount of parallel data is limited.", "labels": [], "entities": []}, {"text": "These characteristics of social media make existing neural machine translation systems extremely vulnerable.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.8015512625376383}]}, {"text": "The noise issue of social media has been investigated in some previous work (.", "labels": [], "entities": []}, {"text": "Most recently, demonstrated the vulnerability of neural machine translation system to both synthetic and natural noises.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.6707940896352133}]}, {"text": "However, the noises tested in are not real noises in social media.", "labels": [], "entities": []}, {"text": "To our best knowledge, there seems * Equal contribution to be alack of translation methods systematically targeting noises in social media.", "labels": [], "entities": [{"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9672935009002686}]}, {"text": "Existing neural machine translation systems are famous for their hungry of data.", "labels": [], "entities": []}, {"text": "However, the amount of parallel data in social media domain is very limited.", "labels": [], "entities": []}, {"text": "Just recently, a dataset collected from Reddit has been published and attracted a lot of attention (.", "labels": [], "entities": []}, {"text": "The amount of data in this dataset is still very small, compared to the large amount of data from News domain.", "labels": [], "entities": [{"text": "News domain", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.9424402713775635}]}, {"text": "Naturally, how to utilize the large amount of parallel data from the News domain become a central problem in improving the translation of social meida.", "labels": [], "entities": [{"text": "translation of social meida", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.8309204131364822}]}, {"text": "In this paper, inspired by the success of backtranslation technique), we propose to learn a model to generate \"socialmedia-style\" translation in source language from clean sentences in target language.", "labels": [], "entities": []}, {"text": "Since the amount of parallel data in social media domain is limited, we utilize the large amount of parallel data in News domain to help the training.", "labels": [], "entities": []}, {"text": "With this model, large mount of parallel data for back-translation can be generated from monolingual data in target language.", "labels": [], "entities": []}, {"text": "In the final translation model, a special \"domain\" symbol is added to indicate which domain the source sentence belonging to.", "labels": [], "entities": []}, {"text": "The contributions of this paper are multifold, and some important ones are highlighted below: 1.", "labels": [], "entities": []}, {"text": "We found that \"social-media-style\" sentences can be generated by training a translation model with different \"start-of-sentence\" symbols for sentences in different domains in the decoder side.", "labels": [], "entities": []}, {"text": "The model is trained with data from all domains, especially News domain, which has a large amount of parallel data, but also adapted to the style in the domain of social media, even the amount of parallel data in social media is limited.", "labels": [], "entities": []}, {"text": "As demonstrated by our experiments, generating \"social-media-style\" sentences is crucial in the effectiveness of back-translation for training a translation model suitable for translating social media.", "labels": [], "entities": [{"text": "translating social media", "start_pos": 176, "end_pos": 200, "type": "TASK", "confidence": 0.8734550078709921}]}, {"text": "2. We illustrated that adding a domain symbol in source sentence improves the robustness of the model.", "labels": [], "entities": []}, {"text": "This maybe because the encoder learns some domain-specific features from input sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "To investigate the empirical performances of our proposed methods, we conduct experiments on MTNT dataset (Michel and Neubig, 2018) using Transformer (.", "labels": [], "entities": [{"text": "MTNT dataset", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.8759033381938934}]}, {"text": "We first apply BPE (Sennrich et al., 2015b) on both sides in order to reduce the vocabulary for both source and target sides.", "labels": [], "entities": [{"text": "BPE", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.8843035697937012}]}, {"text": "We then exclude the sentences pairs whose length are longer than 256 words or subwords.", "labels": [], "entities": []}, {"text": "We use length reward ( ) to find the optimal target length.", "labels": [], "entities": [{"text": "length reward", "start_pos": 7, "end_pos": 20, "type": "METRIC", "confidence": 0.9639766216278076}]}, {"text": "Our implementation is adapted from PyTorchbased OpenNMT (.", "labels": [], "entities": [{"text": "PyTorchbased OpenNMT", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.8775030374526978}]}, {"text": "Our Transformer's parameters are as the same as the base model's parameter settings in the original paper (.", "labels": [], "entities": []}, {"text": "In all experiments, our evaluation uses sacre-BLEU 1 , a standardized BLEU score evaluation   tool by Post (2018).", "labels": [], "entities": [{"text": "BLEU score evaluation", "start_pos": 70, "end_pos": 91, "type": "METRIC", "confidence": 0.9363394776980082}]}, {"text": "We specify the intl tokenization option during BLEU evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9672823548316956}]}, {"text": "We also uses detokenization and normalization tools in Moses. and 2 show statistics of En2Fr and Fr2En datasets.", "labels": [], "entities": [{"text": "Fr2En datasets", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9182792901992798}]}, {"text": "For both En-Fr and Fr-En dataset, the clean parallel data is from WMT15 news translation task.", "labels": [], "entities": [{"text": "Fr-En dataset", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.8863457441329956}, {"text": "WMT15 news translation", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8609312772750854}]}, {"text": "The noisy data is from (Michel and Neubig, 2018) collected from social network.", "labels": [], "entities": []}, {"text": "Except the French and English monolingual data from WMT15 news translation task, we also make the use of English portion of parallel data from KFTT, TED and JESC used in (Michel and Neubig, 2018).", "labels": [], "entities": [{"text": "WMT15 news translation task", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7882544100284576}, {"text": "KFTT", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.9633798599243164}]}], "tableCaptions": [{"text": " Table 1: Statistics of En2Fr Dataset. Monolingual data  is French only.", "labels": [], "entities": [{"text": "En2Fr Dataset", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9588138163089752}]}, {"text": " Table 2: Statistics of Fr2En Dataset, Monolingual data  is English only.", "labels": [], "entities": [{"text": "Fr2En Dataset", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9807173013687134}]}, {"text": " Table 3: Results of noisy data generation. We re- verse the source and target direction of MTNT Fr2En  (En2Fr) dev-set to evaluate the ability of noisy data  generation for En2Fr (Fr2En).", "labels": [], "entities": [{"text": "MTNT Fr2En  (En2Fr) dev-set", "start_pos": 92, "end_pos": 119, "type": "DATASET", "confidence": 0.8555060426394144}]}, {"text": " Table 4: Results of different methods on test-set.  \u2020 (Michel and Neubig, 2018)", "labels": [], "entities": []}, {"text": " Table 5: Semi-blind test results of Fr-En.  *  Our submission.  \u2020 Unconstrained.", "labels": [], "entities": [{"text": "Fr-En", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.9283940196037292}]}, {"text": " Table 6: Semi-blind test results of En-Fr.  *  Our submission.  \u2020 Unconstrained.", "labels": [], "entities": []}, {"text": " Table 7: Human judgments over all submitted systems (the higher the better)  *  Our submission.  \u2020 Unconstrained.", "labels": [], "entities": []}]}