{"title": [{"text": "Latent Retrieval for Weakly Supervised Open Domain Question Answering", "labels": [], "entities": [{"text": "Latent Retrieval", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7417143285274506}, {"text": "Weakly Supervised Open Domain Question Answering", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.5631145040194193}]}], "abstractContent": [{"text": "Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a black-box information retrieval (IR) system to retrieve evidence candidates.", "labels": [], "entities": [{"text": "open domain question answering (QA)", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.7947518442358289}]}, {"text": "We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR.", "labels": [], "entities": []}, {"text": "We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system.", "labels": [], "entities": []}, {"text": "In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable.", "labels": [], "entities": [{"text": "evidence retrieval", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7622319459915161}]}, {"text": "Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task.", "labels": [], "entities": [{"text": "retriever", "start_pos": 66, "end_pos": 75, "type": "TASK", "confidence": 0.9421286582946777}]}, {"text": "We evaluate on open versions of five QA datasets.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.810361236333847}]}, {"text": "On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient.", "labels": [], "entities": [{"text": "BM25", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.8786500096321106}]}, {"text": "On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points inexact match.", "labels": [], "entities": [{"text": "BM25", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.7959252595901489}]}], "introductionContent": [{"text": "Due to recent advances in reading comprehension systems, there has been a revival of interest in open domain question answering (QA), where the evidence must be retrieved from an open corpus, rather than being given as input.", "labels": [], "entities": [{"text": "open domain question answering (QA)", "start_pos": 97, "end_pos": 132, "type": "TASK", "confidence": 0.7676949501037598}]}, {"text": "This presents a more realistic scenario for practical applications.", "labels": [], "entities": []}, {"text": "Current approaches require a blackbox information retrieval (IR) system to do much of the heavy lifting, even though it cannot be fine-tuned on the downstream task.", "labels": [], "entities": [{"text": "blackbox information retrieval (IR)", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.7759795387585958}, {"text": "heavy lifting", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.7074424922466278}]}, {"text": "In the strongly supervised setting popularized by, they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD ().", "labels": [], "entities": []}, {"text": "The IR system is used attest time to generate evidence candidates in place of the gold evidence.", "labels": [], "entities": [{"text": "IR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.4880237579345703}]}, {"text": "In the weakly supervised setting, proposed by), SearchQA (, and Quasar (, the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence.", "labels": [], "entities": [{"text": "SearchQA", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.8236079812049866}]}, {"text": "These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity.", "labels": [], "entities": []}, {"text": "However, QA is fundamentally different from IR.", "labels": [], "entities": [{"text": "IR", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9234732985496521}]}, {"text": "Whereas IR is concerned with lexical and semantic matching, questions are by definition under-specified and require more language understanding, since users are explicitly looking for unknown information.", "labels": [], "entities": [{"text": "IR", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9875180125236511}, {"text": "lexical and semantic matching", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.732632964849472}]}, {"text": "Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9981308579444885}]}, {"text": "In this work, we introduce the first OpenRetrieval Question Answering system (ORQA).", "labels": [], "entities": [{"text": "OpenRetrieval Question Answering", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.5516945421695709}]}, {"text": "ORQA learns to retrieve evidence from an open corpus, and is supervised only by questionanswer string pairs.", "labels": [], "entities": []}, {"text": "While recent work on improving evidence retrieval has made significant progress (, they still only rerank a closed evidence set.", "labels": [], "entities": [{"text": "evidence retrieval", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8923884928226471}]}, {"text": "The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch.", "labels": [], "entities": []}, {"text": "IR systems offer a reasonable but potentially suboptimal starting point.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9617165327072144}]}, {"text": "The key insight of this work is that end-toend learning is possible if we pre-train the retriever with an unsupervised Inverse Cloze Task (ICT).", "labels": [], "entities": []}, {"text": "In ICT, a sentence is treated as a pseudoquestion, and its context is treated as pseudoevidence.", "labels": [], "entities": [{"text": "ICT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9165182113647461}]}, {"text": "Given a pseudo-question, ICT requires selecting the corresponding pseudo-evidence out of the candidates in a batch.", "labels": [], "entities": [{"text": "ICT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9891968369483948}]}, {"text": "ICT pre-training provides a sufficiently strong initialization such that ORQA, a joint retriever and reader model, can be fine-tuned end-to-end by simply optimiz-: Comparison of assumptions made by related tasks, along with references to examples.", "labels": [], "entities": []}, {"text": "Heuristic evidence refers to the typical strategy of considering only a closed set of evidence documents from a traditional IR system, which sets a strict upper-bound on task performance.", "labels": [], "entities": []}, {"text": "In this work (ORQA), only question-answer string pairs are observed during training, and evidence retrieval is learned in a completely end-to-end manner.", "labels": [], "entities": [{"text": "evidence retrieval", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8273482322692871}]}, {"text": "ing the marginal log-likelihood of correct answers that were found.", "labels": [], "entities": []}, {"text": "We evaluate ORQA on open versions of five existing QA datasets.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.834048330783844}]}, {"text": "On datasets where the question writers already know the answer-SQuAD () and TriviaQA (-the retrieval problem resembles traditional IR, and BM25 ( provides state-of-the-art retrieval.", "labels": [], "entities": [{"text": "BM25", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.5546666383743286}]}, {"text": "On datasets where question writers do not know the answerNatural Questions (,, and CuratedTrec (Baudis and Sediv\u00b4ySediv\u00b4y, 2015)-we show that learned retrieval is crucial, providing improvements of 6 to 19 points inexact match over BM25.", "labels": [], "entities": [{"text": "BM25", "start_pos": 232, "end_pos": 236, "type": "DATASET", "confidence": 0.8316718935966492}]}], "datasetContent": [{"text": "We train and evaluate on data from 5 existing question answering or reading comprehension datasets.", "labels": [], "entities": [{"text": "question answering or reading comprehension", "start_pos": 46, "end_pos": 89, "type": "TASK", "confidence": 0.8194721937179565}]}, {"text": "Natural Questions contains question from aggregated queries to Google Search (.", "labels": [], "entities": []}, {"text": "To gather an open version of this dataset, we only keep questions with short answers and discard the given evidence document.", "labels": [], "entities": []}, {"text": "Answers with many tokens often resemble extractive snippets rather than canonical answers, so we discard answers with more than 5 tokens.: Statistics and examples for the datasets that we evaluate on.", "labels": [], "entities": []}, {"text": "There are slightly differences from the original datasets as described in Section 7.1, since not all of them were intended to be used in the open setting.", "labels": [], "entities": []}, {"text": "WebQuestions contains questions that were sampled from the Google Suggest API ().", "labels": [], "entities": []}, {"text": "The answers are annotated with respect to Freebase, but we only keep the string representation of the entities.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9764571785926819}]}, {"text": "CuratedTrec is a corpus of question-answer pairs derived from TREC QA data curated by.", "labels": [], "entities": [{"text": "TREC QA data", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.7789343992869059}]}, {"text": "The questions come from various sources of real queries, such as MSNSearch or AskJeeves logs, where the question askers do not observe any evidence documents).", "labels": [], "entities": [{"text": "MSNSearch", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9631355404853821}]}, {"text": "TriviaQA is a collection of trivia questionanswer pairs that were scraped from the web ().", "labels": [], "entities": []}, {"text": "We use their unfiltered set and discard their distantly supervised evidence.", "labels": [], "entities": []}, {"text": "SQuAD was designed to be a reading comprehension dataset rather than an open domain QA dataset ().", "labels": [], "entities": []}, {"text": "Answer spans were selected from a Wikipedia paragraph, and the questions were written by annotators who were instructed to ask questions that are answered by a given answer in a given context.", "labels": [], "entities": []}, {"text": "On datasets where a development set does not exist, we randomly holdout 10% of the training data for development.", "labels": [], "entities": []}, {"text": "On datasets where the test set is hidden, we also randomly holdout 10% of the training data for development, and use the original development set for testing (following DrQA).", "labels": [], "entities": [{"text": "DrQA", "start_pos": 169, "end_pos": 173, "type": "DATASET", "confidence": 0.938596248626709}]}, {"text": "A summary of dataset statistics and examples are shown in.", "labels": [], "entities": []}, {"text": "Evaluating on this diverse set of question-answer pairs is crucial, because all existing datasets have inherent biases that are problematic for open domain QA systems with learned retrieval.", "labels": [], "entities": []}, {"text": "These biases are summarized in.", "labels": [], "entities": []}, {"text": "In the Natural Questions, WebQuestions, and CuratedTrec, the question askers do not already know the answer.", "labels": [], "entities": []}, {"text": "This accurately reflects a distribution of genuine information-seeking questions.", "labels": [], "entities": []}, {"text": "However, annotators must separately find correct answers, which requires assistance from automatic tools and can introduce a moderate bias towards results from the tool.", "labels": [], "entities": []}, {"text": "In TriviaQA and SQuAD, automatic tools are not needed since the questions are written with known answers in mind.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.7328799366950989}]}, {"text": "However, this introduces another set of biases that are arguably more problematic.", "labels": [], "entities": []}, {"text": "Question writing is not motivated by an information need.", "labels": [], "entities": [{"text": "Question writing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7995169162750244}]}, {"text": "This often results in many hints in the question that would not be present in naturally occurring questions, as shown in the examples in.", "labels": [], "entities": []}, {"text": "This is particularly problematic for SQuAD, where the question askers are also prompted with a specific piece of evidence for the answer, leading to artificially large lexical overlap between the question and evidence.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 37, "end_pos": 42, "type": "TASK", "confidence": 0.8872928619384766}]}, {"text": "Note that these are simply properties of the datasets rather than actionable criticisms-such data collection methods are necessary to scale up, and it is unclear how one could collect a truly unbiased dataset without impractical costs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics and examples for the datasets that we evaluate on. There are slightly differences from the  original datasets as described in Section 7.1, since not all of them were intended to be used in the open setting.", "labels": [], "entities": []}, {"text": " Table 5: Main results: End-to-end exact match  for open-domain question answering from question- answer pairs only. Datasets where question askers  know the answer behave differently from datasets  where they do not.", "labels": [], "entities": [{"text": "exact", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.6649684309959412}, {"text": "open-domain question answering", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.6616171300411224}]}, {"text": " Table 7: Analysis: Example predictions on our open version of the Natural Questions dev set. We show the highest  scoring derivation, consisting of the evidence block and the predicted answer in bold. ORQA is more robust at  separating semantically distinct text that have high lexical overlap. However, the limitation of the 128-dimensional  vectors is that extremely specific concepts are less precisely represented.", "labels": [], "entities": []}]}