{"title": [{"text": "Transfer Learning for Causal Sentence Detection", "labels": [], "entities": [{"text": "Causal Sentence Detection", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6289779543876648}]}], "abstractContent": [{"text": "We consider the task of detecting sentences that express causality, as a step towards mining causal relations from texts.", "labels": [], "entities": []}, {"text": "To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO and BERT, using a bidirectional GRU with self-attention (BIGRUATT) as a base-line.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7340451180934906}, {"text": "ELMO", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9430352449417114}, {"text": "BERT", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9974339604377747}, {"text": "BIGRUATT", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9624866247177124}]}, {"text": "We experiment with both generic public relation extraction datasets and anew biomed-ical causal sentence detection dataset, a subset of which we make publicly available.", "labels": [], "entities": [{"text": "public relation extraction", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6630141337712606}, {"text": "biomed-ical causal sentence detection", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.7253098040819168}]}, {"text": "We find that transfer learning helps only in very small datasets.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.96563720703125}]}, {"text": "With larger datasets, BIGRU-ATT reaches a performance plateau, then larger datasets and transfer learning do not help.", "labels": [], "entities": [{"text": "BIGRU-ATT", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9648396372795105}]}], "introductionContent": [{"text": "A wide range of biomedical questions, from what causes a disease to what drug dosages should be recommended and which side effects might be triggered, center around detecting particular causal relationships between biomedical entities.", "labels": [], "entities": []}, {"text": "Causality, therefore, has long been a focus of biomedical research, e.g., in medical diagnostics, pharmacovigilance, and epidemiology).", "labels": [], "entities": []}, {"text": "The most common way to detect causal relationships is by carrying highly controlled randomized controlled trials, but it is also possible to mine evidence from observational studies and meta-analyses (, where information is often expressed in natural language (e.g., journal articles or clinical study reports).", "labels": [], "entities": []}, {"text": "In natural language processing (NLP), causality detection is often viewed as a type of relation extraction, where the goal is to determine which relations (e.g., part-whole, content-container, causeeffect), if any, hold between two entities in a text (, using deep learning inmost recent works (.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7830014924208323}, {"text": "causality detection", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7579380571842194}, {"text": "relation extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.748029351234436}]}, {"text": "The same view of causality detection is typically adopted in biomedical NLP.", "labels": [], "entities": [{"text": "causality detection", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8061550557613373}]}, {"text": "Existing relation extraction datasets, however, contain few causal instances, which may not allow relation extraction methods to learn to infer causality reliably.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.739526778459549}, {"text": "relation extraction", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7308362573385239}]}, {"text": "Note that causality can be expressed in many ways, from using explicit lexical markers (e.g., \"smoking causes cancer\") to markers that do not always express causality (e.g., \"heavy smoking led to cancer\" vs. \"the nurse led the patient to her room\") to no explicit markers (\"she was infected by a virus and admitted to a hospital\").", "labels": [], "entities": []}, {"text": "Also, existing relation extraction datasets contain sentences from generic, not biomedical documents.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7189849466085434}]}, {"text": "In this paper, we focus on detecting causal sentences, i.e., sentences conveying at least one causal relation.", "labels": [], "entities": []}, {"text": "This is a first step towards mining causal relations from texts.", "labels": [], "entities": []}, {"text": "Once causal sentences have been detected, computationally more intensive relation extraction methods can be used to identify the exact entities that participate in the causal relations and their roles (cause, effect).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7607466876506805}]}, {"text": "To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO () and, comparing against a bidirectional GRU with self-attention (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7276091873645782}, {"text": "ELMO", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9924087524414062}]}, {"text": "We experiment with generic public relation extraction datasets and anew larger biomedical causal sentence detection dataset, a subset of which we make publicly available.", "labels": [], "entities": [{"text": "public relation extraction", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.6997261842091879}, {"text": "biomedical causal sentence detection", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.6489282920956612}]}, {"text": "Unlike recently reported results in other NLP tasks (, we find that transfer learning helps only in datasets with hundreds of training instances.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.9414439499378204}]}, {"text": "When a few thousands of training instances are available, BIGRUATT reaches a performance plateau (both in generic and biomedical texts), then increasing the size of the dataset or employing transfer learning does not improve performance.", "labels": [], "entities": [{"text": "BIGRUATT", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9557350873947144}]}, {"text": "We believe this is the first work to (a) focus on causal sentence detection as a binary classification task, (b) consider causal sentence detection in both generic and biomedical texts, and (c) explore the effect of transfer learning in this task.", "labels": [], "entities": [{"text": "causal sentence detection", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6222552259763082}, {"text": "causal sentence detection", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.6984204252560934}]}], "datasetContent": [{"text": "SemEval-2010 (Task 8): This dataset contains 10,674 samples, of which 1,325 causal (.", "labels": [], "entities": []}, {"text": "Each sample is a sentence annotated with a pair of entities and the type of their relationship.", "labels": [], "entities": []}, {"text": "Since we are only interested in causality, we treat sentences with a Cause-Effect relationship as causal, and all the others as non-causal.", "labels": [], "entities": []}, {"text": "Causal-TimeBank (CausalTB): In this dataset (), we identified causal sentences using C-SIGNAL (causal signal) and CLINK (causal link) tags, discarding causal relationships between entities from different sentences, following.", "labels": [], "entities": []}, {"text": "The resulting dataset contains 2,470 sentences, of which 244 are causal.", "labels": [], "entities": []}, {"text": "Event StoryLine (EventSL): In this dataset, we detected causal sentences by examining the CAUSES and CAUSED BY attributes in the PLOT LINK tags, again following.", "labels": [], "entities": [{"text": "BY", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.6432920694351196}]}, {"text": "Again, we discarded causal relationships between entities from different sentences.", "labels": [], "entities": []}, {"text": "The resulting dataset contains 4,107 sentences, of which 77 are causal.", "labels": [], "entities": []}, {"text": "BioCausal: The full biomedical causal detection dataset we developed (BioCausal-Large) contains 13,342 sentences from PUBMED, of which 7,562 causal.", "labels": [], "entities": [{"text": "biomedical causal detection", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.666945735613505}]}, {"text": "Each sentence was annotated by a single annotator familiar with biomedical texts.", "labels": [], "entities": []}, {"text": "The publicly available subset (BioCausal-Small) contains 2,000 sentences, of which 1,113 causal.", "labels": [], "entities": []}, {"text": "Li and report that SemEval-2010 contains a large number of causal samples with explicit causal markers; by contrast, CausalTB and EventSL contain more complex causal relations with no explicit clues.", "labels": [], "entities": []}, {"text": "BioCausal includes causal sentences both with and without explicit clues.", "labels": [], "entities": []}, {"text": "SemEval-2010, CausalTB, and EventSL are highly imbalanced, with the vast majority of sentences being non-causal.", "labels": [], "entities": []}, {"text": "To prevent a high bias towards the non-causal class, in our experiments we randomly selected 2500, 500, 200 non-causal sentences respectively, discarding the rest.", "labels": [], "entities": []}, {"text": "The resulting causal to non-causal ratios are, thus, roughly 1:2 (SemEval, CausalTB) or 1:3 (EventSL).", "labels": [], "entities": []}, {"text": "By contrast, the BioCausal (Large and Small) datasets are roughly balanced.", "labels": [], "entities": [{"text": "BioCausal (Large and Small) datasets", "start_pos": 17, "end_pos": 53, "type": "DATASET", "confidence": 0.5567097025258201}]}, {"text": "All five datasets were then split into train (70%), validation (15%) and test (15%) subsets, maintaining the same ratio between the two classes in the three subsets.", "labels": [], "entities": []}, {"text": "Tables 1-2 report our experimental results.", "labels": [], "entities": []}, {"text": "For each neural model, we performed 10 repetitions (with different random seeds) and report averages and standard deviations.", "labels": [], "entities": []}, {"text": "For completeness, we show precision, recall, F1, and area under the precision-recall curve (AUC), though AUC scores are the main ones to consider, since they examine: Results on BioCausal-Large (7,562 : 5,780).", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9995754361152649}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9991222023963928}, {"text": "F1", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9994286894798279}, {"text": "area", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9696131348609924}, {"text": "precision-recall curve (AUC)", "start_pos": 68, "end_pos": 96, "type": "METRIC", "confidence": 0.96938157081604}, {"text": "AUC", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9163762331008911}]}, {"text": "Focusing on AUC scores, BIGRUATT outperforms the simpler LR with n-grams by a wide margin, with the exception of EventSL, which is probably too small for the capacity of BIGRU-ATT.", "labels": [], "entities": [{"text": "AUC scores", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.5883172750473022}, {"text": "BIGRUATT", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9640005230903625}, {"text": "BIGRU-ATT", "start_pos": 170, "end_pos": 179, "type": "DATASET", "confidence": 0.787988543510437}]}, {"text": "The precision of LR is perfect on CausalTB and EventSL, at the expense of very low recall, suggesting that LR learned perfectly few highprecision n-grams in those datasets.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995457530021667}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.999228835105896}]}, {"text": "Transfer learning (ELMO, BERT) improves the AUC of BI-GRUATT by a wide margin in the two smallest datasets (CausalTB, EventSL), which contain only hundreds of instances, and the AUC differences from BIGRUATT are statistically significant (stars in), except for BIGRUATT+ELMO in EventSL.", "labels": [], "entities": [{"text": "BERT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9959663152694702}, {"text": "AUC", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9792455434799194}, {"text": "BIGRUATT", "start_pos": 261, "end_pos": 269, "type": "METRIC", "confidence": 0.9430390000343323}, {"text": "ELMO", "start_pos": 270, "end_pos": 274, "type": "METRIC", "confidence": 0.7671089768409729}]}, {"text": "However, in the other three datasets which contain thousands of instances, the AUC differences between transfer learning and plain BIGRUATT are small, with no statistical significance inmost cases.", "labels": [], "entities": [{"text": "AUC", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8245865702629089}, {"text": "BIGRUATT", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9450437426567078}]}, {"text": "Also, the AUC scores of all methods on BioCausal-Large are close to those on BioCausal-Small, despite the fact that BioCausalLarge is approx. seven times larger.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9822127819061279}]}, {"text": "Similar observations can be made by looking at the F1 scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9990123510360718}]}, {"text": "Indeed BIGRUATT overfits the training set of EventSL.", "labels": [], "entities": [{"text": "BIGRUATT", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.994903564453125}]}, {"text": "We performed two-tailed Approximate Randomization tests (, p \u2264 0.05, with 10k iterations, randomly swapping in each iteration 50% of the decisions (over all tested sentences) across the two compared methods.", "labels": [], "entities": []}, {"text": "When testing statistical significance, for each method we use the repetition (among the 10) with the best validation F1 score.", "labels": [], "entities": [{"text": "repetition", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9794297218322754}, {"text": "validation F1 score", "start_pos": 106, "end_pos": 125, "type": "METRIC", "confidence": 0.7426414887110392}]}, {"text": "It seems that causal sentence detection, at least with the neural methods we considered, reaches a plateau with few thousands of training sentences both in generic and biomedical texts; then increasing the dataset size or employing transfer learning does not help.", "labels": [], "entities": [{"text": "causal sentence detection", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.7946785489718119}]}, {"text": "The latter finding is not inline with previously reported results (, where ELMO and BERT were found to improve performance inmost NLP tasks without studying, however, the effect of dataset size.", "labels": [], "entities": [{"text": "ELMO", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9408512115478516}, {"text": "BERT", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9979008436203003}]}, {"text": "Furthermore, BERT+BIGRUATT consistently performed better than BERT+LR in AUC (but not in F1), which casts doubts on the practice of adding only shallow taskspecific models to BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9969139099121094}, {"text": "BIGRUATT", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.848098635673523}, {"text": "BERT+LR", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.8784549633661906}, {"text": "AUC", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.5749719738960266}, {"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.991477906703949}, {"text": "BERT", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.8074690103530884}]}, {"text": "11 BIGRUATT+ELMO is competitive in AUC to BERT+BIGRUATT (with the exception of EventSL).", "labels": [], "entities": [{"text": "BIGRUATT", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9896690249443054}, {"text": "ELMO", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.6239748597145081}, {"text": "AUC", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.5123127102851868}, {"text": "BERT", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9977574944496155}, {"text": "BIGRUATT", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9319180250167847}]}, {"text": "Mainly comprised of sentences with simple explicit causal statements (, SemEval expectedly demonstrated the best classification performance across datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision, recall, F1, AUC on the four publicly available datasets, averaged over 10 repetitions, with  standard deviations (\u00b1). Next to each dataset name, we show in brackets the total causal and non-causal sentences  that we used. The best results are shown in bold. The best AUC results are also shown in gray background. In the  AUC columns, stars indicate statistically significant (p \u2264 0.05) differences compared to BIGRUATT.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9974741339683533}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9980676770210266}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9971804618835449}, {"text": "AUC", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9502191543579102}, {"text": "BIGRUATT", "start_pos": 432, "end_pos": 440, "type": "METRIC", "confidence": 0.9777592420578003}]}, {"text": " Table 2: Results on BioCausal-Large (7,562 : 5,780).", "labels": [], "entities": []}]}