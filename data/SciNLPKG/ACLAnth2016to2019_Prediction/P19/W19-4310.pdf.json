{"title": [{"text": "Specializing Distributional Vectors of All Words for Lexical Entailment", "labels": [], "entities": [{"text": "Lexical Entailment", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.766118198633194}]}], "abstractContent": [{"text": "Semantic specialization methods fine-tune dis-tributional word vectors using lexical knowledge from external resources (e.g., WordNet) to accentuate a particular relation between words.", "labels": [], "entities": []}, {"text": "However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources.", "labels": [], "entities": []}, {"text": "We present the first post-processing method that specializes vectors of all vocabulary words-including those unseen in the resources-for the asymmetric relation of lexical entailment (LE) (i.e., hyponymy-hypernymy relation).", "labels": [], "entities": []}, {"text": "Leveraging a partially LE-specialized distributional space, our POS-TLE (i.e., post-specialization for LE) model learns an explicit global specialization function , allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer.", "labels": [], "entities": [{"text": "POS-TLE", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.8786770701408386}]}, {"text": "We capture the function as a deep feed-forward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymy-hypernymy pairs to better reflect semantic similarity.", "labels": [], "entities": []}, {"text": "An extended model variant augments the basic architecture with an adversarial dis-criminator.", "labels": [], "entities": []}, {"text": "We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE).", "labels": [], "entities": []}, {"text": "We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word-level lexical entailment (LE), also known as the TYPE-OF or hyponymy-hypernymy relation, is a fundamental asymmetric lexico-semantic relation.", "labels": [], "entities": [{"text": "Word-level lexical entailment (LE)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.749879444638888}, {"text": "TYPE-OF", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9638230800628662}]}, {"text": "* Both authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9513880014419556}]}, {"text": "Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (), text generation (, metaphor detection (, and automatic taxonomy creation (.", "labels": [], "entities": [{"text": "Automatic reasoning about word-level LE", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5443550944328308}, {"text": "text generation", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.7940217852592468}, {"text": "metaphor detection", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8483071327209473}, {"text": "taxonomy creation", "start_pos": 163, "end_pos": 180, "type": "TASK", "confidence": 0.6959539651870728}]}, {"text": "However, standard techniques for inducing word embeddings (, inter alia) are unable to effectively capture LE.", "labels": [], "entities": []}, {"text": "Due to their crucial dependence on contextual information and the distributional hypothesis, they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (.", "labels": [], "entities": [{"text": "LE", "start_pos": 207, "end_pos": 209, "type": "METRIC", "confidence": 0.9757409691810608}]}, {"text": "To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from external resources such as WordNet or BabelNet (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 197, "end_pos": 204, "type": "DATASET", "confidence": 0.951948344707489}]}, {"text": "This process, termed retrofitting or semantic specialization, is beneficial to language understanding tasks and is extremely versatile as it can be applied on top of any input distributional space.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7684438824653625}]}, {"text": "Retrofitting methods, however, have a major weakness: they only locally update vectors of words seen in the external resources, while leaving vectors of all other unseen words unchanged, as illustrated in.", "labels": [], "entities": []}, {"text": "Recent work) has demonstrated how to specialize the full distributional space for the symmetric relation of semantic (dis)similarity.", "labels": [], "entities": []}, {"text": "The so-called post-specialization model learns a", "labels": [], "entities": []}], "datasetContent": [{"text": "We extensively evaluate the proposed POSTLE models on two fundamental LE tasks: 1) predicting graded LE and 2) LE detection (and directionality), in monolingual and cross-lingual transfer settings.", "labels": [], "entities": [{"text": "predicting graded LE", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7415439486503601}, {"text": "LE detection", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.6758017092943192}]}], "tableCaptions": [{"text": " Table 1: Accuracy of POSTLE models on *BLESS datasets, for two different sets of English distributional vectors:  Skip-Gram (SG) and GloVe (GL). LEAR reports highest scores on *BLESS datasets in the literature.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9677649736404419}, {"text": "BLESS", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9469560384750366}]}, {"text": " Table 2: Average precision (AP) of POSTLE models  in cross-lingual transfer. Results are shown for both  POSTLE models (DFFN and ADV), two target languages  (Spanish and French) and three methods for inducing  bilingual vector spaces: Ar (Artetxe et al., 2018), Co  (Conneau et al., 2018), and Sm (Smith et al., 2017).", "labels": [], "entities": [{"text": "Average precision (AP)", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8491737008094787}, {"text": "cross-lingual transfer", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7961207032203674}, {"text": "Ar", "start_pos": 236, "end_pos": 238, "type": "METRIC", "confidence": 0.9966975450515747}]}]}