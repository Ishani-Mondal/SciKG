{"title": [{"text": "Zero-Shot Entity Linking by Reading Entity Descriptions", "labels": [], "entities": [{"text": "Zero-Shot Entity Linking", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5467581947644552}]}], "abstractContent": [{"text": "We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data.", "labels": [], "entities": [{"text": "zero-shot entity linking task", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.7000181600451469}]}, {"text": "The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed.", "labels": [], "entities": []}, {"text": "In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities.", "labels": [], "entities": []}, {"text": "First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities.", "labels": [], "entities": []}, {"text": "Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in anew domain.", "labels": [], "entities": []}, {"text": "We present experiments on anew dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT.", "labels": [], "entities": [{"text": "DAP", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9029403328895569}, {"text": "BERT", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9891128540039062}]}, {"text": "The data and code are available at https: //github.com/lajanugen/zeshel.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity linking systems have achieved high performance in settings where a large set of disambiguated mentions of entities in a target entity dictionary is available for training.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7749676704406738}]}, {"text": "Such systems typically use powerful resources such as a high-coverage alias table, structured data, and linking frequency statistics.", "labels": [], "entities": []}, {"text": "For example, show that by only using the prior probability gathered from hyperlink statistics on Wikipedia training articles, one can achieve 90% accuracy on the task of predicting links in Wikipedia test articles.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9992910623550415}, {"text": "predicting links in Wikipedia test articles", "start_pos": 170, "end_pos": 213, "type": "TASK", "confidence": 0.7576252718766531}]}, {"text": "While most prior works focus on linking to general entity databases, it is often desirable to link to * Work completed while interning at Google 1 zeshel stands for zero-shot entity linking.", "labels": [], "entities": [{"text": "zero-shot entity linking", "start_pos": 165, "end_pos": 189, "type": "TASK", "confidence": 0.6832543810208639}]}], "datasetContent": [{"text": "We construct anew dataset to study the zeroshot entity linking problem using documents from Wikia.", "labels": [], "entities": [{"text": "zeroshot entity linking", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.6785480082035065}]}, {"text": "3 Wikias are community-written encyclopedias, each specializing in a particular subject or theme such as a fictional universe from a book or film series.", "labels": [], "entities": []}, {"text": "Wikias have many interesting properties suitable for our task.", "labels": [], "entities": []}, {"text": "Labeled mentions can be automatically extracted based on hyperlinks.", "labels": [], "entities": []}, {"text": "Mentions and entities have rich document context that can be exploited by reading comprehension approaches.", "labels": [], "entities": []}, {"text": "Each Wikia has a large number of unique entities relevant to a specific theme, making it a useful benchmark for evaluating domain generalization of entity linking systems.", "labels": [], "entities": []}, {"text": "We use data from 16 Wikias, and use 8 of them for training and 4 each for validation and testing.", "labels": [], "entities": []}, {"text": "To construct data for training and evaluation, we first extract a large number of mentions from the Wikias.", "labels": [], "entities": []}, {"text": "Many of these mentions can be easily linked by string matching between mention string  and the title of entity documents.", "labels": [], "entities": []}, {"text": "These mentions are downsampled during dataset construction, and occupy a small percentage (5%) of the final dataset.", "labels": [], "entities": []}, {"text": "While not completely representative of the natural distribution of mentions, this data construction method follows recent work that focuses on evaluating performance on the challenging aspects of the entity linking problem (e.g., selected mentions with multiple possible entity candidates for assessing indomain unseen entity performance).", "labels": [], "entities": []}, {"text": "Each Wikia document corresponds to an entity, represented by the title and contents of the document.", "labels": [], "entities": []}, {"text": "These entities, paired with their text descriptions, comprise the entity dictionary.", "labels": [], "entities": []}, {"text": "Since the task is already quite challenging, we assume that the target entity exists in the entity dictionary and leave NIL recognition or clustering (NIL mentions/entities refer to entities nonexistent in the knowledge-base) to future editions of the task and dataset.", "labels": [], "entities": [{"text": "NIL recognition", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7750405967235565}]}, {"text": "We categorize the mentions based on token overlap between mentions and the corresponding entity title as follows.", "labels": [], "entities": []}, {"text": "High Overlap: title is identical to mention text, Multiple Categories: title is mention text followed by a disambiguation phrase (e.g., mention string: 'Batman', title: 'Batman (Lego)'), Ambiguous substring: mention is a substring of title (e.g., mention string: 'Agent', title: 'The Agent').", "labels": [], "entities": []}, {"text": "All other mentions are categorized Zeedan Nazir is the son of the Late Kal and Jamila Nazir . .", "labels": [], "entities": []}, {"text": "Pre-training We use the BERT-Base model architecture in all our experiments.", "labels": [], "entities": [{"text": "BERT-Base", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9906308054924011}]}, {"text": "The Masked LM objective) is used for unsupervised pre-training.", "labels": [], "entities": []}, {"text": "For fine-tuning language models (in the case of multi-stage pre-training) and We use the notation Ux interchangeably to mean both the unsupervised data x and the strategy to pre-train on x.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Zero-shot entity linking dataset based on Wikia.", "labels": [], "entities": [{"text": "Wikia", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9382798075675964}]}, {"text": " Table 6: Performance on test domains with Full- Transformer. N. Acc represents the normalized accu- racy. U. Acc represents the unnormalized accuracy.  The unnormalized accuracy is upper-bounded by 68%,  the top-64 recall of the candidate generation stage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9645405411720276}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9496601223945618}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9973220229148865}]}, {"text": " Table 7: Performance on test domains categorized by  mention categories. Recall@64 indicates top-64 per- formance of candidate generation. N. Acc. and U. Acc.  are respectively the normalized and unnormalized ac- curacies.", "labels": [], "entities": [{"text": "Recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9877585768699646}, {"text": "N.", "start_pos": 140, "end_pos": 142, "type": "DATASET", "confidence": 0.5212422609329224}, {"text": "Acc.", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.43201205134391785}, {"text": "U. Acc.", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.7804318070411682}]}]}