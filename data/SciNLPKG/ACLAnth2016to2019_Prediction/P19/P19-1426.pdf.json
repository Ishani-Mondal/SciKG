{"title": [{"text": "Bridging the Gap between Training and Inference for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.7614344954490662}]}], "abstractContent": [{"text": "Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7587111294269562}]}, {"text": "At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch.", "labels": [], "entities": []}, {"text": "This discrepancy of the fed context leads to error accumulation among the way.", "labels": [], "entities": []}, {"text": "Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations.", "labels": [], "entities": []}, {"text": "In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum.", "labels": [], "entities": []}, {"text": "Experiment results on Chinese\u2192English and WMT'14 English\u2192German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets.", "labels": [], "entities": [{"text": "WMT'14 English\u2192German translation tasks", "start_pos": 42, "end_pos": 81, "type": "TASK", "confidence": 0.7794371743996938}]}], "introductionContent": [{"text": "Neural Machine Translation has shown promising results and drawn more attention recently.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8853040734926859}]}, {"text": "Most NMT models fit in the encoder-decoder framework, including the RNN-based (, the CNN-based () and the attention-based () models, which predict the next word conditioned on the previous context words, deriving a language model over target words.", "labels": [], "entities": []}, {"text": "The scenario is at training time the ground truth words are used as context * Corresponding author.", "labels": [], "entities": []}, {"text": "while at inference the entire sequence is generated by the resulting model on its own and hence the previous words generated by the model are fed as context.", "labels": [], "entities": []}, {"text": "As a result, the predicted words at training and inference are drawn from different distributions, namely, from the data distribution as opposed to the model distribution.", "labels": [], "entities": []}, {"text": "This discrepancy, called exposure bias (, leads to a gap between training and inference.", "labels": [], "entities": [{"text": "exposure bias", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.9056361019611359}]}, {"text": "As the target sequence grows, the errors accumulate among the sequence and the model has to predict under the condition it has never met at training time.", "labels": [], "entities": []}, {"text": "Intuitively, to address this problem, the model should be trained to predict under the same condition it will face at inference.", "labels": [], "entities": []}, {"text": "Inspired by DATA AS DEMONSTRATOR (DAD), feeding as context both ground truth words and the predicted words during training can be a solution.", "labels": [], "entities": [{"text": "DATA AS DEMONSTRATOR (DAD)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.4153236597776413}]}, {"text": "NMT models usually optimize the cross-entropy loss which requires a strict pairwise matching at the word level between the predicted sequence and the ground truth sequence.", "labels": [], "entities": []}, {"text": "Once the model generates a word deviating from the ground truth sequence, the cross-entropy loss will correct the error immediately and draw the remaining generation back to the ground truth sequence.", "labels": [], "entities": []}, {"text": "However, this causes anew problem.", "labels": [], "entities": []}, {"text": "A sentence usually has multiple reasonable translations and it cannot be said that the model makes a mistake even if it generates a word different from the ground truth word.", "labels": [], "entities": []}, {"text": "For example, reference: We should comply with the rule.", "labels": [], "entities": []}, {"text": "cand1: We should abide with the rule.", "labels": [], "entities": []}, {"text": "cand2: We should abide by the law.", "labels": [], "entities": []}, {"text": "cand3: We should abide by the rule.", "labels": [], "entities": []}, {"text": "once the model generates \"abide\" as the third target word, the cross-entropy loss would force the model to generate \"with\" as the fourth word (as cand1) so as to produce larger sentence-level likelihood and be inline with the reference, although \"by\" is the right choice.", "labels": [], "entities": []}, {"text": "Then, \"with\" will be fed as context to generate \"the rule\", as a result, the model is taught to generate \"abide with the rule\" which actually is wrong.", "labels": [], "entities": []}, {"text": "The translation cand1 can be treated as overcorrection phenomenon.", "labels": [], "entities": []}, {"text": "Another potential error is that even the model predicts the right word \"by\" following \"abide\", when generating subsequent translation, it may produce \"the law\" improperly by feeding \"by\" (as cand2).", "labels": [], "entities": []}, {"text": "Assume the references and the training criterion let the model memorize the pattern of the phrase \"the rule\" always following the word \"with\", to help the model recover from the two kinds of errors and create the correct translation like cand3, we should feed \"with\" as context rather than \"by\" even when the previous predicted phrase is \"abide by\".", "labels": [], "entities": []}, {"text": "We refer to this solution as Overcorrection Recovery (OR).", "labels": [], "entities": [{"text": "Overcorrection Recovery", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.729589968919754}, {"text": "OR", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.5047774910926819}]}, {"text": "In this paper, we present a method to bridge the gap between training and inference and improve the overcorrection recovery capability of NMT.", "labels": [], "entities": [{"text": "overcorrection recovery", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.6492226719856262}]}, {"text": "Our method first selects oracle words from its predicted words and then samples as context from the oracle words and ground truth words.", "labels": [], "entities": []}, {"text": "Meanwhile, the oracle words are selected not only with a wordby-word greedy search but also with a sentencelevel evaluation, e.g. BLEU, which allows greater flexibility under the pairwise matching restriction of cross-entropy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.994672954082489}]}, {"text": "At the beginning of training, the model selects as context ground truth words at a greater probability.", "labels": [], "entities": []}, {"text": "As the model converges gradually, oracle words are chosen as context more often.", "labels": [], "entities": []}, {"text": "In this way, the training process changes from a fully guided scheme towards a less guided scheme.", "labels": [], "entities": []}, {"text": "Under this mechanism, the model has the chance to learn to handle the mistakes made at inference and also has the ability to recover from overcorrection over alternative translations.", "labels": [], "entities": []}, {"text": "We verify our approach on both the RNNsearch model and the stronger Transformer model.", "labels": [], "entities": [{"text": "RNNsearch", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.8376297950744629}]}, {"text": "The results show that our approach can significantly improve the performance on both models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carryout experiments on the NIST Chinese\u2192English (Zh\u2192En) and the WMT'14 English\u2192German (En\u2192De) translation tasks.", "labels": [], "entities": [{"text": "NIST Chinese\u2192English", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.8587700575590134}, {"text": "WMT'14 English\u2192German (En\u2192De) translation", "start_pos": 68, "end_pos": 109, "type": "TASK", "confidence": 0.7694910943508149}]}], "tableCaptions": [{"text": " Table 1: Case-insensitive BLEU scores (%) on Zh\u2192En translation task. \" \u2021\", \" \u2020\", \"\" and \" * \" indicate statistically  significant difference (p<0.01) from RNNsearch, SS-NMT, MIXER and Transformer, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9452041983604431}, {"text": "Zh\u2192En translation task", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.5579349815845489}]}, {"text": " Table 2: Factor analysis on Zh\u2192En translation, the re- sults are average BLEU scores on MT03\u223c06 datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.99931800365448}, {"text": "MT03\u223c06 datasets", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.8762954473495483}]}, {"text": " Table 3: Case-sensitive BLEU scores (%) on En\u2192De  task. The \" \u2021\" indicates the results are significantly bet- ter (p<0.01) than RNNsearch and Transformer.", "labels": [], "entities": [{"text": "Case-sensitive", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8903217315673828}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8956905603408813}, {"text": "RNNsearch", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.8451371192932129}]}]}