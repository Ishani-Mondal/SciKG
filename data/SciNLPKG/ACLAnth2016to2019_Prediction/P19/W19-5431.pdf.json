{"title": [{"text": "Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation", "labels": [], "entities": [{"text": "Neural Machine Translation of Low-Resource and Similar Languages", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.7930441349744797}]}], "abstractContent": [{"text": "We present our contribution to the WMT19 Similar Language Translation shared task.", "labels": [], "entities": [{"text": "WMT19 Similar Language Translation shared task", "start_pos": 35, "end_pos": 81, "type": "TASK", "confidence": 0.7210736672083536}]}, {"text": "We investigate the utility of neural machine translation on three low-resource, similar language pairs: Spanish-Portuguese, Czech-Polish , and Hindi-Nepali.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6895387371381124}]}, {"text": "Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation.", "labels": [], "entities": []}, {"text": "In our analysis , we found Transformer models to work best on Spanish-Portuguese and Czech-Polish translation, whereas LSTMs with global attention worked best on Hindi-Nepali translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present our contribution to the WMT 2019 Similar Language Translation shared task, which focused on translation between similar language pairs in low-resource settings (.", "labels": [], "entities": [{"text": "WMT 2019 Similar Language Translation shared task", "start_pos": 35, "end_pos": 84, "type": "TASK", "confidence": 0.7678250414984567}, {"text": "translation between similar language pairs", "start_pos": 103, "end_pos": 145, "type": "TASK", "confidence": 0.8412445187568665}]}, {"text": "Similar languages have advantages that can be exploited when building machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7304569482803345}]}, {"text": "In particular, languages that come from the same language family (or that come from related language families) may have in common a multitude of information such as lexical or syntactic structures.", "labels": [], "entities": []}, {"text": "This commonality has been exploited in a number of previous works for similar language translation (.", "labels": [], "entities": [{"text": "similar language translation", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.8406352798144022}]}, {"text": "In this work, we are primarily concerned with neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.8042607009410858}]}, {"text": "NMT is a language agnostic framework where language similarities could possibly be exploited to build scalable, state-of-the-art (SOTA) machine translation systems.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8711976408958435}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.702467143535614}]}, {"text": "For example, NMT systems have been used on a number of WMT translation tasks where they enabled highly successful modeling.", "labels": [], "entities": [{"text": "WMT translation tasks", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.9578839739163717}]}, {"text": "A weakness with NMT is its dependence on large bitext corpora.", "labels": [], "entities": []}, {"text": "For this reason, researchers have considered ways to mitigate this specific issue.", "labels": [], "entities": []}, {"text": "A prominent approach meant to alleviate need for large parallel data is backtranslation.", "labels": [], "entities": []}, {"text": "This technique generates synthetic bitext by translating monolingual sentences of the target language into the source language with a pre-existing target-to-source translation system.", "labels": [], "entities": []}, {"text": "These noisy source translations are then incorporated to train anew source-to-target MT system ().", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9538594484329224}]}, {"text": "This approach is instrumental in unsupervised machine translation where authors have shown that, up to a certain amount of bitext, better translation systems can be trained with these unsupervised approaches than supervised methods (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6975254118442535}]}, {"text": "Backtranslation research has also extended to scenarios of training supervised systems with just synthetic data (.", "labels": [], "entities": []}, {"text": "Given the success of this approach, it offers a promising avenue to leverage monolingual data for improving translation between similar languages.", "labels": [], "entities": [{"text": "translation between similar languages", "start_pos": 108, "end_pos": 145, "type": "TASK", "confidence": 0.8739947378635406}]}, {"text": "Motivated by the success of backtranslation, we focus on leveraging monolingual data to improve NMT systems for similar language pairs.", "labels": [], "entities": []}, {"text": "Hence, for our submissions to the shared task, we focus on investigating the effectiveness of synthetic bitext produced with backtranslation.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: We discuss our methods in Section 2, including our NMT models and our decisions for backtranslation.", "labels": [], "entities": []}, {"text": "Section 3 is where we describe our analysis of the shared task data.", "labels": [], "entities": []}, {"text": "In Section 4, we present our experimental findings, discussing the effectiveness of backtranslation in terms of BLEU score performance.", "labels": [], "entities": [{"text": "BLEU score performance", "start_pos": 112, "end_pos": 134, "type": "METRIC", "confidence": 0.9685943325360616}]}, {"text": "We conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present an analysis of the shared task data.", "labels": [], "entities": []}, {"text": "For additional information, such as our pre-processing of the data, refer to the supplemen-tary material.", "labels": [], "entities": []}, {"text": "To get an understanding of the provided data, we collect statistics including the word and sentence counts, sentence length variation, and token overlap.", "labels": [], "entities": []}, {"text": "contains information on the approximate sentence and word counts after cleaning the data.", "labels": [], "entities": []}, {"text": "Based on the size of the datasets, we hypothesize that our most successful NMT system would be for Spanish -Portuguese (\u223c3.5M sentences), followed by, Czech -Polish (\u223c1.7M sentences), and Hindi -Nepali being the most difficult (\u223c68K sentences).", "labels": [], "entities": []}, {"text": "In addition to this, the sentence length variations in the box-plots of highlight how for Spanish -Portuguese, and Czech -Polish the sentences are generally longer in the bitext compared to Hindi -Nepali.", "labels": [], "entities": []}, {"text": "In our experimental results, we reason that part of the success for the LSTM+Attn models on Hindi -Nepali is due to the short sentence lengths.", "labels": [], "entities": []}, {"text": "A cited advantage of the Transformer () is its ability to encode longer dependencies, but also see, which on the Hindi -Nepali corpus would not be as much of a requirement due to the shorter bitext.", "labels": [], "entities": [{"text": "Hindi -Nepali corpus", "start_pos": 113, "end_pos": 133, "type": "DATASET", "confidence": 0.6357655376195908}]}, {"text": "We also wanted to understand from which perspective each of the language pairs might be considered similar, so we analyzed the overlap between tokens in each language pairs bitext.", "labels": [], "entities": []}, {"text": "We tokenized on our cleaned data with the Tok-Tok Tokenizer available through the NLTK toolkit.", "labels": [], "entities": [{"text": "NLTK toolkit", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9233463406562805}]}, {"text": "We then calculated the percentage of shared tokens compared to the total tokens at increasingly higher thresholds by token frequency.", "labels": [], "entities": []}, {"text": "shows our findings for the percentage of shared tokens at different thresholds of token frequency.", "labels": [], "entities": []}, {"text": "These plots would suggest that although Spanish -Portuguese and Czech -Polish have larger overall token overlap, the most frequent tokens are where much of the language discrepancy is.", "labels": [], "entities": []}, {"text": "Czech and Polish in particular, seem to have significantly fewer shared tokens which could suggest a smaller lexical overlap.", "labels": [], "entities": []}, {"text": "This could partially be because of differences in alphabets between Czech and Polish.", "labels": [], "entities": []}, {"text": "By contrast, Hindi and Nepali seem to share much more in common as we see an increase of overlap for more frequent tokens, but we note this could bean artefact of the small size of the Hindi and Nepali data.", "labels": [], "entities": []}, {"text": "We now present our experimental findings.", "labels": [], "entities": []}, {"text": "For all of our experiments, we use OpenNMT-py ( to handle training and build our models.", "labels": [], "entities": []}, {"text": "For our LSTM+Attn model, we used the default parameters provided in the OpenNMTpy toolkit.", "labels": [], "entities": []}, {"text": "For the Transformer, we used the recommended settings provided by the OpenNMTpy toolkit, with the exception of using 2 layers in the Transformer encoder and decoder instead of 6.", "labels": [], "entities": []}, {"text": "We changed the number of Transformer layers because we found in our preliminary results on the bitext only systems that this worked well for each language direction.", "labels": [], "entities": []}, {"text": "We did not investigate model architecture and hyperparameter tuning further, and hence we note additional work in this context could lead to better performance ().", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.6903450638055801}]}, {"text": "The exact parameters are listed in the supplemental material.", "labels": [], "entities": []}, {"text": "For our final evalua- tion, we also perform ensemble decoding by using different checkpoints in the optimization process and further details can be found in the supplement material.", "labels": [], "entities": []}, {"text": "We represented the vocabulary for each language with a joint byte-pair encoding (BPE) model) trained on all available bitext and monolingual data shared between the languages motivated by the work of.", "labels": [], "entities": []}, {"text": "Our BPE models were trained with the SentencePiece API and consisted of 20,000 merge operations.", "labels": [], "entities": [{"text": "BPE", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.733970582485199}]}, {"text": "The reader may notice that, based on our discussion in Section3, Czech and Polish may not have necessarily benefited from a joint vocabulary.", "labels": [], "entities": []}, {"text": "This indeed maybe the case, especially as our final results for Czech -Polish translation were the lowest-performing among all our final systems.", "labels": [], "entities": [{"text": "Czech -Polish translation", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.5947693362832069}]}, {"text": "We present our findings for each respective language pair on the validation data provided by task organizers.", "labels": [], "entities": []}, {"text": "We measure performance on the validation data with the BLEU score based on the BPE representations of sentences using the script that comes with the OpenNMT-py toolkit.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9994439482688904}, {"text": "BPE", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8763868808746338}]}, {"text": "Note that for our test data, BLEU score is measured on the detokenized input sequences (i.e., word tokens rather than BPE).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9791797399520874}, {"text": "BPE", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9512733221054077}]}, {"text": "mation on the size of the training data used for each model.", "labels": [], "entities": [{"text": "mation", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9640132188796997}]}, {"text": "Note that we did not evaluate the Synth 3 dataset on the LSTM+Attn model which was due to our previous findings and compute resource limitations.", "labels": [], "entities": [{"text": "Synth 3 dataset", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.6665900349617004}, {"text": "LSTM+Attn model", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.7163497656583786}]}, {"text": "Official, shared task results for our primary submissions are presented in along with a number of important choices we made as to which models to submit.", "labels": [], "entities": []}, {"text": "There area number of interesting behaviors we see in terms of performance from our validation to test sets.", "labels": [], "entities": []}, {"text": "In the SpanishPortuguese translation systems, we can see that the relative BLEU scores between the two directions are fairly stable.", "labels": [], "entities": [{"text": "SpanishPortuguese translation", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.7041354179382324}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9991151690483093}]}, {"text": "This is likely in part due to the sampling process used for backtranslation we used in comparison for the other language pairs which used greedily decoded sentences.", "labels": [], "entities": []}, {"text": "As for the other language pairs, although we originally hypothesized that Czech -Polish would produce better systems than Hindi -Nepali our results seem to suggest the opposite and that we might have overfit the Czech -Polish validation set compared to Hindi -Nepali translation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Validation BLEU scores from varying quality  and amount of backtranslated text for both directions  for Spanish -Portuguese translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9632930159568787}, {"text": "Spanish -Portuguese translation", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.5942554101347923}]}, {"text": " Table 3: Validation BLEU scores from varying quality  and amount of backtranslated text for Czech -Polish  translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9577143788337708}, {"text": "Czech -Polish  translation", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.4771410673856735}]}, {"text": " Table 4: Validation BLEU scores from varying quality  and amount of backtranslated text for both directions  of Hindi -Nepali translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9501321911811829}, {"text": "Hindi -Nepali translation", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.5721410885453224}]}, {"text": " Table 5: Final BLEU scores on the detokenized translations for the best performing models across all our experi- ments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.955303966999054}]}, {"text": " Table 6: Sentence counts for each dataset after cleaning procedure.", "labels": [], "entities": []}, {"text": " Table 7: Sentence counts for each dataset after cleaning procedure.", "labels": [], "entities": []}, {"text": " Table 8: Sentence counts for each dataset after cleaning procedure.", "labels": [], "entities": []}, {"text": " Table 9: Sentence counts for each dataset after cleaning procedure.", "labels": [], "entities": []}, {"text": " Table 10: The parameters used for the RNN Model and the Transformer model. Parameters are largely from the  OpenNMT-py toolkit suggested parameters.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9756686687469482}]}]}