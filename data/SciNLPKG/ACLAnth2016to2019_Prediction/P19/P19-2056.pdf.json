{"title": [{"text": "Using Semantic Similarity as Reward for Reinforcement Learning in Sentence Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Traditional model training for sentence generation employs cross-entropy loss as the loss function.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7676379382610321}]}, {"text": "While cross-entropy loss has convenient properties for supervised learning, it is unable to evaluate sentences as a whole, and lacks flexibility.", "labels": [], "entities": []}, {"text": "We present the approach of training the generation model using the estimated semantic similarity between the output and reference sentences to alleviate the problems faced by the training with cross-entropy loss.", "labels": [], "entities": []}, {"text": "We use the BERT-based scorer fine-tuned to the Semantic Textual Similarity (STS) task for semantic similarity estimation, and train the model with the estimated scores through reinforcement learning (RL).", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9956899285316467}, {"text": "Semantic Textual Similarity (STS) task", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.69999817439488}, {"text": "semantic similarity estimation", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.7655655741691589}]}, {"text": "Our experiments show that reinforcement learning with semantic similarity reward improves the BLEU scores from the baseline LSTM NMT model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9993184804916382}]}], "introductionContent": [{"text": "Sentence generation using neural networks has become a vital part of various natural language processing tasks including machine translation () and abstractive summarization (.", "labels": [], "entities": [{"text": "Sentence generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.954340398311615}, {"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.8078904747962952}, {"text": "abstractive summarization", "start_pos": 148, "end_pos": 173, "type": "TASK", "confidence": 0.6665522754192352}]}, {"text": "Most previous work on sentence generation employ crossentropy loss between the model outputs and the ground-truth sentence to guide the maximumlikelihood training on the token-level.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7228763103485107}]}, {"text": "Differentiability of cross-entropy loss is useful for computing gradients in supervised learning; however, it lacks flexibility and may penalize the generation model fora slight shift or change in token sequence even if the sequence retains the meaning.", "labels": [], "entities": []}, {"text": "For instance, consider the sentence pair, \"I watched a movie last night.\" and \"I saw a film last night.\".", "labels": [], "entities": []}, {"text": "As the simple cross-entropy loss lacks the ability to properly assess semantically similar tokens, these sentences are penalized for having two token mismatches.", "labels": [], "entities": []}, {"text": "As another example, the sentence pair \"He often walked to school.\" and \"He walked to school often.\" would be severely punished by the token misalignment, despite having identical meanings.", "labels": [], "entities": []}, {"text": "To tackle the inflexible nature of model evaluation during training, we propose an approach of using semantic similarity between the output sequence and the ground-truth sequence to train the generation model.", "labels": [], "entities": []}, {"text": "In the proposed framework, semantic similarity of sentence pairs is estimated by a BERT-based) regression model fine-tuned against Semantic Textual Similarity () dataset, and the resulting score is passed back to the model using reinforcement learning strategies.", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9729822278022766}]}, {"text": "Our experiment on translation datasets suggests that the proposed method is better at improving the BLEU score than the traditional cross-entropy learning.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.981848955154419}]}, {"text": "However, since the model outputs had limited paraphrastic variations, the results are also inconclusive in supporting the effectiveness of applying the proposed method to sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.8075723350048065}]}], "datasetContent": [{"text": "The dataset used for fine-tuning the STS estimator is STS-B (.", "labels": [], "entities": []}, {"text": "The tokenizer used is a wordpiece tokenizer for BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.633847713470459}]}, {"text": "For machine translation, we used De-En parallel corpora from multi30k-dataset () and WIT3 ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7752255201339722}, {"text": "WIT3", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.8650866150856018}]}, {"text": "The multi30k-dataset is comprised of textual descriptions of images while the WIT3 consists of transcribed TED talks.", "labels": [], "entities": [{"text": "WIT3", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9380123615264893}]}, {"text": "Each corpus provides a single validation set and multiple test sets.", "labels": [], "entities": []}, {"text": "We chose the best models based on their scores for the validation sets and used the two newest test sets from each corpus for testing.", "labels": [], "entities": []}, {"text": "Both corpora are tokenized using the sentencepiece BPE tokenizer with a vocabulary size of 8,000 for each language.", "labels": [], "entities": []}, {"text": "All letters are turned to lowercase and any consecutive spaces are turned into a single space before tokenization.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.9618943929672241}]}, {"text": "The source and target vocabularies are kept separate.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU and estimated STS scores for test sets in multi30k-dataset and WIT3. mscoco2017 and flickr2017  are test sets for multi30k-dataset, while TED2014 and TED2015 are test sets for WIT3. RL-GLEU and RL-STS  denote models trained with REINFORCE using GLEU reward and STS reward respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985384941101074}, {"text": "WIT3", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9187159538269043}, {"text": "flickr2017", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.8515478372573853}, {"text": "TED2014", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.7647764682769775}, {"text": "TED2015", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.7126430869102478}, {"text": "GLEU", "start_pos": 260, "end_pos": 264, "type": "METRIC", "confidence": 0.9603809714317322}]}, {"text": " Table 3: Sample outputs of the models for the training set", "labels": [], "entities": []}]}