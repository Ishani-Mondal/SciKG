{"title": [{"text": "Improving Neural Language Models by Segmenting, Attending, and Predicting the Future", "labels": [], "entities": []}], "abstractContent": [{"text": "Common language models typically predict the next word given the context.", "labels": [], "entities": []}, {"text": "In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7746776342391968}]}, {"text": "The model does not require any linguistic annotation of phrase segmentation.", "labels": [], "entities": [{"text": "phrase segmentation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7224947363138199}]}, {"text": "Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsu-pervised learning manner.", "labels": [], "entities": [{"text": "phrase segmentation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7082426249980927}]}, {"text": "Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network.", "labels": [], "entities": [{"text": "phrase induction", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.8196780383586884}, {"text": "context-phrase alignment", "start_pos": 150, "end_pos": 174, "type": "TASK", "confidence": 0.7052286267280579}]}, {"text": "Experiments have shown that our model outperformed several strong baseline models on different data sets.", "labels": [], "entities": []}, {"text": "We achieved anew state-of-the-art performance of 17.4 per-plexity on the Wikitext-103 dataset.", "labels": [], "entities": [{"text": "Wikitext-103 dataset", "start_pos": 73, "end_pos": 93, "type": "DATASET", "confidence": 0.9636317491531372}]}, {"text": "Additionally , visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.", "labels": [], "entities": [{"text": "phrase induction module", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.809920867284139}]}], "introductionContent": [{"text": "Neural language models are typically trained by predicting the next word given a past context).", "labels": [], "entities": []}, {"text": "However, natural sentences are not constructed as simple linear word sequences, as they usually contain complex syntactic information.", "labels": [], "entities": []}, {"text": "For example, a subsequence of words can constitute a phrase, and two non-neighboring words can depend on each other.", "labels": [], "entities": []}, {"text": "These properties make natural sentences more complex than simple linear sequences.", "labels": [], "entities": []}, {"text": "Most recent work on neural language modeling learns a model by encoding contexts and matching the context embeddings to the embedding of the next word (.", "labels": [], "entities": [{"text": "neural language modeling", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.7359888752301534}]}, {"text": "In this line of work, a given context is encoded with a neural network, for example along short-term memory (LSTM;) network, and is represented with a distributed vector.", "labels": [], "entities": []}, {"text": "The loglikelihood of predicting a word is computed by calculating the inner product between the word embedding and the context embedding.", "labels": [], "entities": [{"text": "predicting a word", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8705258170763651}]}, {"text": "Although most models do not explicitly consider syntax, they still achieve state-of-the-art performance on different corpora.", "labels": [], "entities": []}, {"text": "Efforts have also been made to utilize structural information to learn better language models.", "labels": [], "entities": []}, {"text": "For instance, parsing-readingpredict networks (PRPN;) explicitly learn a constituent parsing structure of a sentence and predict the next word considering the internal structure of the given context with an attention mechanism.", "labels": [], "entities": []}, {"text": "Experiments have shown that the model is able to capture some syntactic information.", "labels": [], "entities": []}, {"text": "Similar to word representation learning models that learns to match word-to-word relation matrices (, standard language models are trained to factorize context-to-word relation matrices (.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.8244173526763916}]}, {"text": "In such work, the context comprises all previous words observed by a model for predicting the next word.", "labels": [], "entities": []}, {"text": "However, we believe that contextto-word relation matrices are not sufficient for describing how natural sentences are constructed.", "labels": [], "entities": []}, {"text": "We argue that natural sentences are generated at a higher level before being decoded to words.", "labels": [], "entities": []}, {"text": "Hence a language model should be able to predict the following sequence of words given a context.", "labels": [], "entities": []}, {"text": "In this work, we propose a model that factorizes a context-to-phrase mutual information matrix to learn better language models.", "labels": [], "entities": []}, {"text": "The contextto-phrase mutual information matrix describes the relation among contexts and the probabilities of phrases following given contexts.", "labels": [], "entities": []}, {"text": "We make the following contributions in this paper: \u2022 We propose a phrase prediction model that improves the performance of state-of-the-art word-level language models.", "labels": [], "entities": [{"text": "phrase prediction", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8356472253799438}]}, {"text": "\u2022 Our model learns to predict approximate phrases and headwords without any annotation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We The PTB dataset has a vocabulary size of 10,000 unique words.", "labels": [], "entities": [{"text": "PTB dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9742994010448456}]}, {"text": "The entire corpus includes roughly 40,000 sentences in the training set, and more than 3,000 sentences in both valid and test set.", "labels": [], "entities": []}, {"text": "The WT2 data is about two times larger the the PTB dataset.", "labels": [], "entities": [{"text": "WT2 data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8848911225795746}, {"text": "PTB dataset", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9904175400733948}]}, {"text": "The dataset consists of Wikipedia articles.", "labels": [], "entities": []}, {"text": "The corpus includes 30,000 unique words in its vocabulary and is not cleaned as heavily as the PTB corpus.", "labels": [], "entities": [{"text": "PTB corpus", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.977485865354538}]}, {"text": "The WT103 corpus contains a larger vocabulary and more articles than WT2.", "labels": [], "entities": [{"text": "WT103 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9738011062145233}, {"text": "WT2", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9304171204566956}]}, {"text": "It consists of 28k articles and more than 100M words in the training set.", "labels": [], "entities": []}, {"text": "WT2 and WT103 corpora can evaluate the ability of capturing long-term dependencies.", "labels": [], "entities": [{"text": "WT2", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9605834484100342}, {"text": "WT103 corpora", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.8733988106250763}]}, {"text": "In each corpus, we apply our approach to publicly-available, state-of-the-art models.", "labels": [], "entities": []}, {"text": "This demonstrates that our approach can improve different existing architectures.", "labels": [], "entities": []}, {"text": "Our trained models will be published for downloading.", "labels": [], "entities": []}, {"text": "The implementation of our models is publicly available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results on Penn Treebank dataset. Compared with the AWD-LSTM baseline models, our  method reduced the perplexity on test set by 1.6.", "labels": [], "entities": [{"text": "Penn Treebank dataset", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.9944982528686523}, {"text": "AWD-LSTM baseline", "start_pos": 75, "end_pos": 92, "type": "DATASET", "confidence": 0.9567568302154541}]}, {"text": " Table 2: Experimental results on Wikitext-2 dataset.", "labels": [], "entities": [{"text": "Wikitext-2 dataset", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.9517568647861481}]}]}