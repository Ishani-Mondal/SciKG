{"title": [{"text": "NNE: A Dataset for Nested Named Entity Recognition in English Newswire", "labels": [], "entities": [{"text": "NNE: A Dataset", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8261166214942932}, {"text": "Nested Named Entity Recognition", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6284005865454674}, {"text": "English Newswire", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.6102558225393295}]}], "abstractContent": [{"text": "Named entity recognition (NER) is widely used in natural language processing applications and downstream tasks.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7985358436902364}]}, {"text": "However, most NER tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions.", "labels": [], "entities": []}, {"text": "We describe NNE-a fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB).", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn Treebank (PTB)", "start_pos": 74, "end_pos": 128, "type": "DATASET", "confidence": 0.9545944874936884}]}, {"text": "Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting.", "labels": [], "entities": []}, {"text": "We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER.", "labels": [], "entities": [{"text": "English newswire", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.8707515299320221}, {"text": "NER", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.832162618637085}]}], "introductionContent": [{"text": "Named entity recognition-the task of identifying and classifying entity mentions in text-plays a crucial role in understanding natural language.", "labels": [], "entities": [{"text": "Named entity recognition-the task of identifying and classifying entity mentions in text-plays", "start_pos": 0, "end_pos": 94, "type": "TASK", "confidence": 0.7708547363678614}]}, {"text": "It is used for many downstream language processing tasks, e.g., coreference resolution, question answering, summarization, entity linking, relation extraction and knowledge base population.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.9523833692073822}, {"text": "question answering", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.9030288457870483}, {"text": "summarization", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9862439036369324}, {"text": "entity linking", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.7788697183132172}, {"text": "relation extraction", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.8775914013385773}]}, {"text": "However, most NER tools are designed to capture flat mention structure over coarse entity type schemas, reflecting the available annotated datasets.", "labels": [], "entities": []}, {"text": "Focusing on flat mention structures ignores important information that can be useful for downstream tasks.", "labels": [], "entities": []}, {"text": "includes examples of nested named entities illustrating several phenomena: \u2022 Entity-entity relationships can be embedded in nested mentions.", "labels": [], "entities": []}, {"text": "For instance, the location of the 'Ontario Supreme Court' is indicated by the embedded STATE mention 'Ontario'; \u2022 Entity attribute values can be embedded in nested mentions.", "labels": [], "entities": [{"text": "Ontario Supreme Court'", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.9421102404594421}, {"text": "STATE", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.7420690059661865}]}, {"text": "For instance, the title is the embedded ROLE 'Former U.N.", "labels": [], "entities": [{"text": "ROLE", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9197896718978882}, {"text": "Former U.N.", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.7997321685155233}]}, {"text": "Ambassador', which also encodes the employment relation  between the PERSON 'Jane Kirkpatrick' and ORG 'U.N.'; \u2022 Part-whole relationships can be encoded in nested mention structure.", "labels": [], "entities": [{"text": "PERSON 'Jane Kirkpatrick' and ORG 'U.N.'", "start_pos": 69, "end_pos": 109, "type": "DATASET", "confidence": 0.7954712033271789}]}, {"text": "For instance, the REGION 'Southern California' is part of the STATE 'California'.", "labels": [], "entities": [{"text": "REGION", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9873073101043701}, {"text": "STATE 'California'", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.6848836988210678}]}, {"text": "Recent work has demonstrated increasing interest in nested entity structure, including local approaches (, hypergraph-based approaches (, cascaded approaches (, and parsing approaches (.", "labels": [], "entities": []}, {"text": "See Dai (2018) fora survey.", "labels": [], "entities": [{"text": "Dai (2018)", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8711359351873398}]}, {"text": "Yet these techniques have seen little translation from the research literature to toolsets or downstream applications.", "labels": [], "entities": []}, {"text": "To facilitate ongoing research on nested NER, we introduce NNE-a large, manuallyannotated, nested named entity dataset over English newswire.", "labels": [], "entities": []}, {"text": "This new annotation layer over the Wall Street Journal portion of the PTB includes 279,795 mentions.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the PTB", "start_pos": 35, "end_pos": 73, "type": "DATASET", "confidence": 0.9634768877710614}]}, {"text": "All mentions are annotated, including nested structures with depth as high as six layers.", "labels": [], "entities": []}, {"text": "A fine-grained entity type schema is used, extending the flat BBN) annotation from 64 to 114 entity types.", "labels": [], "entities": []}, {"text": "We are publicly releasing the standoff annotations along with detailed annotation guidelines and scripts for knitting annotations onto the underlying PTB corpus.", "labels": [], "entities": [{"text": "knitting annotations", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.8927353322505951}, {"text": "PTB corpus", "start_pos": 150, "end_pos": 160, "type": "DATASET", "confidence": 0.9702387154102325}]}, {"text": "Benchmark results using recent state-of-the-art approaches demonstrate that good accuracy is possible, but complexity and run time are open challenges.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9986273050308228}]}, {"text": "As anew layer over the already rich collection of PTB annotations, NNE provides an opportunity to explore joint modelling of nested NER and other tasks at an unprecedented scale and detail.", "labels": [], "entities": [{"text": "NNE", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8074328303337097}]}], "datasetContent": [{"text": "Annotation) is a pronoun coreference and entity type corpus, annotated with 64 types of entities, numerical and time expressions.", "labels": [], "entities": []}, {"text": "We use its flat entity schema as a starting point to design our schema.", "labels": [], "entities": []}, {"text": "We analyzed existing BBN annotations to develop and automatically apply structured preannotation for predictable entity types.", "labels": [], "entities": []}, {"text": "Additional fine-grained categories and further structural elements of entities, inspired by and, are used to augment the BBN schema.", "labels": [], "entities": [{"text": "BBN schema", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.8912375271320343}]}, {"text": "We adhere to the following general principles when annotating nested named entities in the corpus: \u2022 Annotate all named entities, all time and date (TIMEX) and numerical (NUMEX) entities, including all non-sentence initial words in title case, and instances of proper noun mentions that are not capitalized.", "labels": [], "entities": []}, {"text": "\u2022 Annotate all structural elements of entities.", "labels": [], "entities": []}, {"text": "These elements could be other entities, such as 'Ontario' (STATE) in 'Ontario Supreme Court' (GOVERNMENT), or structural components such as '40' (CARDINAL) and 'miles' (UNIT) in '40 miles' (QUANTITY:1D), as well as the internal structure induced by syntactic elements, such as coordination.", "labels": [], "entities": [{"text": "Ontario Supreme Court' (GOVERNMENT)", "start_pos": 70, "end_pos": 105, "type": "DATASET", "confidence": 0.8866082429885864}]}, {"text": "\u2022 Add consistent substructure to avoid spurious ambiguity.", "labels": [], "entities": []}, {"text": "For example, the token 'Toronto', which is a CITY, would be labeled as part https://github.com/nickyringland/nested named entities of an ORG:EDU organization span 'University of Toronto'.", "labels": [], "entities": [{"text": "ORG:EDU organization span 'University of Toronto", "start_pos": 137, "end_pos": 185, "type": "DATASET", "confidence": 0.6546203858322568}]}, {"text": "We add layers of annotations to allow each token to be annotated as consistently as possible, e.g., [University of CITY ] ORG:EDU . \u2022 Add additional categories to avoid category confusion.", "labels": [], "entities": [{"text": "University of CITY ] ORG:EDU", "start_pos": 101, "end_pos": 129, "type": "DATASET", "confidence": 0.7703490938459124}]}, {"text": "Some entities are easy to identify, but difficult to categorize consistently.", "labels": [], "entities": []}, {"text": "For instance, a hotel (or any business at a fixed location) has both organizational and locative qualities, or is at least treated metonymously as a location.", "labels": [], "entities": []}, {"text": "Rather than requiring annotators to make an ambiguous decision, we elect to add category HOTEL to simplify the individual annotation decision.", "labels": [], "entities": []}, {"text": "We also apply this principle when adding MEDIA, FUND, and BUILDING categories.", "labels": [], "entities": [{"text": "FUND", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.8202729225158691}, {"text": "BUILDING", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9915184378623962}]}, {"text": "Many annotation decisions are ambiguous and difficult, thus may require substantial research.", "labels": [], "entities": []}, {"text": "For instance, knowing that 'The Boeing Company' was named after founder 'William E. Boeing' would allow us to annotate 'Boeing' with an embedded PERSON entity.", "labels": [], "entities": [{"text": "The Boeing Company' was named after founder 'William E. Boeing'", "start_pos": 28, "end_pos": 91, "type": "DATASET", "confidence": 0.858144746376918}]}, {"text": "However, this does not apply for other companies, such as 'Sony Corporation'.", "labels": [], "entities": [{"text": "Sony Corporation'", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9202777743339539}]}, {"text": "To let annotation decisions be made without reference to external knowledge, we label all tokens that seem to be the names of people as NAME, regardless of whether they are actually a person's name.", "labels": [], "entities": []}, {"text": "Entity types and mention frequencies can be found in for annotation guidelines and extended discussion of annotation decisions.", "labels": [], "entities": []}, {"text": "Annotation Process: Although some existing annotation tools allow nested structures (e.g.,), we built a custom tool that allowed us to create a simple and fast way to add layers of entities, and suggest reusing existing structured annotations for the same span.", "labels": [], "entities": []}, {"text": "Using the annotations from BBN as underlying annotations, the annotator is shown a screen with the target sentence, as well as the previous and next sentences, if any.", "labels": [], "entities": [{"text": "BBN", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.9588591456413269}]}, {"text": "A view of the whole article is also possible to help the annotator with contextual cues.", "labels": [], "entities": []}, {"text": "When annotators select a span, they are prompted with suggestions based on their own previous annotations, and common entities.", "labels": [], "entities": []}, {"text": "Some entities are repeated frequently in an article, or over many articles in the corpus.", "labels": [], "entities": []}, {"text": "The annotation tool allows a user to add a specified annotation to all strings matching those tokens in the same article, or in all articles.", "labels": [], "entities": []}, {"text": "Four annotators, each with a background in linguistics and/or computational linguistics were selected and briefed on the annotation task and purpose.", "labels": [], "entities": []}, {"text": "The WSJ portion of the PTB consists of 25 sections (00-24).", "labels": [], "entities": [{"text": "WSJ portion of the PTB", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8079680442810059}]}, {"text": "Each annotator started with a subset of section 00 as annotation training, and was given feedback before moving onto other sections.", "labels": [], "entities": []}, {"text": "Weekly meetings were held with all annotators to discuss ambiguities in the guidelines, gaps in the annotation categories, edge cases and ambiguous entities and to resolve discrepancies.", "labels": [], "entities": []}, {"text": "Total annotation time for the corpus was 270 hours, split between the four annotators.", "labels": [], "entities": []}, {"text": "Sections 00 and 23 were doubly annotated, and section 02 was annotated by all four annotators.", "labels": [], "entities": []}, {"text": "An additional 17 hours was used for adjudicating these sections annotated by multiple annotators.", "labels": [], "entities": []}, {"text": "Dataset Analysis: The resulting NNE dataset includes a large number of entity mentions of substantial depth, with more than half of mentions occurring inside another mentions.", "labels": [], "entities": [{"text": "Dataset Analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6355655789375305}, {"text": "NNE dataset", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.8777083456516266}]}, {"text": "Of the 118,525 top-level entity mentions, 47,020 (39.6%) do not have any nested structure embedded.", "labels": [], "entities": []}, {"text": "The remaining 71,505 mentions contain 161,270 mentions, averaging 2.25 structural mentions per each of these top-layer entity mentions.", "labels": [], "entities": []}, {"text": "Note that one span can be assigned multiple entity types.", "labels": [], "entities": []}, {"text": "For example, the span '1993' can be annotated as both DATE and YEAR.", "labels": [], "entities": [{"text": "span '1993'", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.7025353312492371}, {"text": "DATE", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9812374711036682}, {"text": "YEAR", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9492231011390686}]}, {"text": "In NNE, 19,144 out of 260,386 total spans are assigned multiple types.", "labels": [], "entities": [{"text": "NNE", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.914341926574707}]}, {"text": "lists the number of spans occurring at each depth.", "labels": [], "entities": []}, {"text": "To measure how clearly the annotation guidelines delineate each category, and how reliable our annotations are, inter-annotator agreement was calculated using annotations on Section 02, which was annotated by all four annotators.", "labels": [], "entities": []}, {"text": "An adjudicated version was created by deciding a correct existing candidate label from within the four possibilities, or by adjusting one of them on a token level.", "labels": [], "entities": []}, {"text": "For the purposes of inter-annotator agreement, a tag stack is calculated for each word, essentially flattening each token's nested annotation structure into one label.", "labels": [], "entities": []}, {"text": "For example, the tag of token 'California' in the third sentence of is STATE REGION, while 'beach' is O O.", "labels": [], "entities": [{"text": "STATE", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9878166317939758}, {"text": "REGION", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.5380419492721558}]}, {"text": "Agreement using Fleiss' kappa overall tokens is 0.907.", "labels": [], "entities": [{"text": "Fleiss' kappa overall tokens", "start_pos": 16, "end_pos": 44, "type": "DATASET", "confidence": 0.6887263506650925}]}, {"text": "Considering only tokens that are part of at least one mention according to at least one annotator, Fleiss' kappa is 0.832.", "labels": [], "entities": [{"text": "Fleiss' kappa", "start_pos": 99, "end_pos": 112, "type": "METRIC", "confidence": 0.8941873908042908}]}, {"text": "Both results are above the 0.8 threshold for good reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9724245667457581}]}, {"text": "Average precision, recall and F 1 score across four annotators with respect to the adjudicated gold standard are 94.3, 91.8 and 93.0.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9997158646583557}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.999683141708374}, {"text": "F 1 score", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.983046273390452}]}], "tableCaptions": [{"text": " Table 1: Number of spans at each layer of nesting with their most frequent categories.", "labels": [], "entities": []}, {"text": " Table 2: NER results on NNE using different methods.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8948148488998413}, {"text": "NNE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.5629614591598511}]}, {"text": " Table 3: A comparison between NNE and two com- monly used corpora with nested entities.", "labels": [], "entities": []}]}