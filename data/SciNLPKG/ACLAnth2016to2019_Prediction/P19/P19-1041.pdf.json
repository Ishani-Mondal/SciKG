{"title": [{"text": "Disentangled Representation Learning for Non-Parallel Text Style Transfer", "labels": [], "entities": [{"text": "Disentangled Representation Learning", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.9147254029909769}, {"text": "Non-Parallel Text Style Transfer", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.6301712393760681}]}], "abstractContent": [{"text": "This paper tackles the problem of disentangling the latent representations of style and content in language models.", "labels": [], "entities": []}, {"text": "We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives , for style prediction and bag-of-words prediction, respectively.", "labels": [], "entities": [{"text": "style prediction", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7543114423751831}, {"text": "bag-of-words prediction", "start_pos": 138, "end_pos": 161, "type": "TASK", "confidence": 0.674077495932579}]}, {"text": "We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space.", "labels": [], "entities": []}, {"text": "This disentangled latent representation learning can be applied to style transfer on non-parallel corpora.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7593272924423218}]}, {"text": "We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9679190516471863}, {"text": "content preservation", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7944915294647217}]}], "introductionContent": [{"text": "The neural network has been a successful learning machine during the past decade due to its highly expressive modeling capability, which is a consequence of multiple layers of non-linear transformations of input features.", "labels": [], "entities": []}, {"text": "Such transformations, however, make intermediate features \"latent,\" in the sense that they do not have explicit meaning and are not interpretable.", "labels": [], "entities": []}, {"text": "Therefore, neural networks are usually treated as black-box machinery.", "labels": [], "entities": []}, {"text": "Disentangling the latent space of neural networks has become an increasingly important research topic.", "labels": [], "entities": [{"text": "Disentangling the latent space of neural networks", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7415653552327838}]}, {"text": "In the image domain, for example, use adversarial and information maximization objectives to produce interpretable latent representations that can be tweaked to adjust writing style for handwritten digits, as well as lighting and orientation for face models.", "labels": [], "entities": []}, {"text": "However, this problem is less explored in natural language processing.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of disentangling the latent space of neural networks for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.824357807636261}]}, {"text": "Our model is built on an autoencoder that encodes a sentence to the latent space (vector representation) by learning to reconstruct the sentence itself.", "labels": [], "entities": []}, {"text": "We would like the latent space to be disentangled with respect to different features, namely, style and content in our task.", "labels": [], "entities": [{"text": "style", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.9734917879104614}]}, {"text": "To accomplish this, we propose a simple yet effective approach that combines multi-task and adversarial objectives.", "labels": [], "entities": []}, {"text": "We artificially divide the latent representation into two parts: the style space and content space, where we consider the sentiment of a sentence as its style.", "labels": [], "entities": []}, {"text": "We design a systematic set of auxiliary losses, enforcing the separation of style and content latent spaces.", "labels": [], "entities": []}, {"text": "In particular, the multi-task loss operates on a latent space to ensure that the space does contain the information we wish to encode.", "labels": [], "entities": []}, {"text": "The adversarial loss, on the contrary, minimizes the predictability of information that should not be contained in a given latent space.", "labels": [], "entities": []}, {"text": "In early work, researchers typically work with the style space (), but simply ignore the content space, as it is hard to formalize what \"content\" actually refers to.", "labels": [], "entities": []}, {"text": "Cycle consistency of back-translation defines content implicitly (), but requires reinforcement learning over the discrete sentence space, which could be extremely difficult to train.", "labels": [], "entities": []}, {"text": "In our paper, we propose to approximate the content information by bag-of-words (BoW) features, where we focus on style-neutral, nonstopwords.", "labels": [], "entities": []}, {"text": "Along with traditional style-oriented auxiliary losses, our BoW multi-task loss and BoW adversarial loss enable better disentanglement of the style and content spaces.", "labels": [], "entities": []}, {"text": "The learned disentangled latent space can be directly used for text style transfer, which aims to transform a given sentence to anew sentence with the same content but a different style.", "labels": [], "entities": [{"text": "text style transfer", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.6298396786053976}]}, {"text": "We follow the setting where the model is trained on a nonparallel but style-labeled corpus (; thus, we call it non-parallel text style transfer.", "labels": [], "entities": [{"text": "non-parallel text style transfer", "start_pos": 111, "end_pos": 143, "type": "TASK", "confidence": 0.745325118303299}]}, {"text": "With our disentangled latent space, we simply use the autoencoder to encode the content vector of a sentence, but ignore its encoded style vector.", "labels": [], "entities": []}, {"text": "We then infer from the training data an empirical embedding of the style that we would like to transfer to.", "labels": [], "entities": []}, {"text": "The encoded content vector and the empirically-inferred style vector are concatenated and fed to the decoder.", "labels": [], "entities": []}, {"text": "This grafting technique enables us to obtain anew sentence similar in content to the input sentence, but with a different style.", "labels": [], "entities": []}, {"text": "We conducted experiments on two benchmark datasets.", "labels": [], "entities": []}, {"text": "Both qualitative and quantitative results show that the style and content spaces are indeed disentangled well.", "labels": [], "entities": []}, {"text": "In the style-transfer evaluation, we achieve high performance in style-transfer accuracy, content preservation, as well as language fluency, compared with previous results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9759764075279236}, {"text": "content preservation", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7753573060035706}]}, {"text": "Ablation tests also show that all our auxiliary losses can be combined well, each playing its own role in disentangling the latent space.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on two datasets, Yelp and Amazon reviews.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9537706971168518}, {"text": "Amazon reviews", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.7778679132461548}]}, {"text": "Both comprise sentences labeled by binary sentiment (positive or negative).", "labels": [], "entities": []}, {"text": "They are used to train latent space disentanglement as well as to evaluate sentiment transfer.", "labels": [], "entities": [{"text": "latent space disentanglement", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6995941003163656}, {"text": "sentiment transfer", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9489262700080872}]}, {"text": "We used the Yelp review dataset, following previous work).", "labels": [], "entities": [{"text": "Yelp review dataset", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.9431917071342468}]}, {"text": "3 It contains 444101, 63483, and 126670 labeled reviews for train, validation, and test, respectively.", "labels": [], "entities": []}, {"text": "We set the maximum length of a sentence to 15 words and the vocabulary size to \u223c9200, following.", "labels": [], "entities": []}, {"text": "We further evaluate our model with an Amazon review dataset, following some other previous papers ().", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.9298088153203329}]}, {"text": "It contains, and 2000 labeled reviews for train, validation, and test, respectively.", "labels": [], "entities": []}, {"text": "The maximum length of a sentence is set to 20 words and the vocabulary size is \u223c58k, as in.", "labels": [], "entities": []}, {"text": "Our RNN has a hidden state of 256 dimensions, linearly transformed to a style space of 8 dimensions and a content space of 128 dimensions.", "labels": [], "entities": []}, {"text": "They were chosen empirically, and we found them robust to model performance.", "labels": [], "entities": []}, {"text": "For the decoder, we fed the latent vector h = [s, c] to the hidden state at each step.", "labels": [], "entities": []}, {"text": "We used the Adam optimizer () for the autoencoder and the RMSProp optimizer (Tieleman and Hinton, 2012) for the discriminators, following stability tricks in adversarial training (.", "labels": [], "entities": []}, {"text": "Each optimizer has an initial learning rate of 10 \u22123 . Our model is trained for 20 epochs, by which time it has converged.", "labels": [], "entities": []}, {"text": "The word embedding layer was initialized by) trained on respective training sets.", "labels": [], "entities": []}, {"text": "Both the autoencoder and the discriminators are trained once per minibatch with \u03bb mul(s) = 10, \u03bb mul(c) = 3, \u03bb adv(s) = 1, and \u03bb adv(c) = 0.03.", "labels": [], "entities": []}, {"text": "These hyperparameters were tuned by a log-scale grid search within two orders of magnitude around the default value 1; we chose the values yielding the best validation results.", "labels": [], "entities": []}, {"text": "For the VAE model, the KL penalty is weighted by \u03bb kl(s) and \u03bb kl(c) for style and content, respectively.", "labels": [], "entities": [{"text": "VAE", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.7290477752685547}]}, {"text": "We set both to 0.03, tuned by the same method of log-scale grid search.", "labels": [], "entities": []}, {"text": "During training, we also used the sigmoid KL annealing schedule, following.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification accuracy on latent spaces.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9639818668365479}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.974103569984436}]}, {"text": " Table 2: Performance of text style transfer. STA: Style transfer accuracy. CS: Cosine similarity. WO: Word  overlap rate. PPL: Perplexity. GM: Geometric mean. The larger \u2191 (or lower \u2193 ), the better.  \u2020 Quoted from previous  papers (with the same data split).  \u2021 Involving custom data splits, providing a rough (but unbiased) comparison.  Others are based on our replication, and we use published code whenever possible. We achieve 0.809 and 0.835  transfer accuracy on the Yelp dataset, close to the results in Shen et al. (2017) and Zhao et al. (2018), respectively,  showing that our replication is fair. Gray numbers show that a method fails to transfer style most of the time.", "labels": [], "entities": [{"text": "text style transfer", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6366109848022461}, {"text": "STA", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9911420345306396}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.930600106716156}, {"text": "Word  overlap rate", "start_pos": 103, "end_pos": 121, "type": "METRIC", "confidence": 0.8187347054481506}, {"text": "GM", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9894636869430542}, {"text": "Geometric mean", "start_pos": 144, "end_pos": 158, "type": "METRIC", "confidence": 0.9084004163742065}, {"text": "accuracy", "start_pos": 458, "end_pos": 466, "type": "METRIC", "confidence": 0.9567912817001343}, {"text": "Yelp dataset", "start_pos": 474, "end_pos": 486, "type": "DATASET", "confidence": 0.9827147126197815}]}, {"text": " Table 3: Manual evaluation on the Yelp dataset.", "labels": [], "entities": [{"text": "Yelp dataset", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9746807217597961}]}, {"text": " Table 4: Ablation tests on Yelp. In all variants, we fol- low the same protocol of style transfer by substituting  an empirical estimate of the target style vector.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9812152981758118}, {"text": "Yelp", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.8963388204574585}, {"text": "style transfer", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.6979739665985107}]}]}