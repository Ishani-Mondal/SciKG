{"title": [{"text": "BSNLP2019 Shared Task Submission: Multisource Neural NER Transfer", "labels": [], "entities": [{"text": "Shared Task Submission", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6056105593840281}, {"text": "Multisource Neural NER Transfer", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.656862810254097}]}], "abstractContent": [{"text": "This paper describes the Cognitive Computation (CogComp) Group's submissions to the multilingual named entity recognition shared task at the Balto-Slavic Natural Language Processing (BSNLP) Workshop (Piskorski et al., 2019).", "labels": [], "entities": [{"text": "multilingual named entity recognition shared task at the Balto-Slavic Natural Language Processing (BSNLP) Workshop", "start_pos": 84, "end_pos": 198, "type": "TASK", "confidence": 0.7783630937337875}]}, {"text": "The final model submitted is a multi-source neural NER system with multilingual BERT embeddings, trained on the concate-nation of training data in various Slavic languages (as well as English).", "labels": [], "entities": [{"text": "BERT", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9899365901947021}]}, {"text": "The performance of our system on the official testing data suggests that multi-source approaches consistently outperform single-source approaches for this task, even with the noise of mismatch-ing tagsets.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the Cognitive Computation (CogComp) Group's submission to the shared task of the Balto-Slavic Natural Language Processing (BSNLP) Workshop at ACL 2019 (.", "labels": [], "entities": [{"text": "Balto-Slavic Natural Language Processing (BSNLP) Workshop at ACL 2019", "start_pos": 102, "end_pos": 171, "type": "TASK", "confidence": 0.6230039433999495}]}, {"text": "This shared task centers around multilingual named entity recognition (NER) in Slavic languages, and is composed of recognition, lemmatization, and entity linking subtasks.", "labels": [], "entities": [{"text": "multilingual named entity recognition (NER)", "start_pos": 32, "end_pos": 75, "type": "TASK", "confidence": 0.7846532421452659}]}, {"text": "The niche focus of this task on Slavic languages makes it both interesting and challenging.", "labels": [], "entities": []}, {"text": "The languages used in the shared task (Bulgarian, Czech, Polish, and Russian) belong to the same language family and share complex grammatical and morphological features which maybe understudied in an English-focused research community.", "labels": [], "entities": []}, {"text": "Further, they encompass both Latin and Cyrillic scripts, complicating the multilingual nature of the problem.", "labels": [], "entities": []}, {"text": "In addition to the language specific challenges, there are varying sizes of training data, somewhat non-standard named entity types (making finding additional data challenging), and differing domains -the training and test sets are composed of newswire documents collected around domain-specific topics, with different topics in train and test.", "labels": [], "entities": []}, {"text": "This year's shared task is the second edition of the multilingual named entity recognition task on Slavic languages organized for the BSNLP workshop.", "labels": [], "entities": [{"text": "multilingual named entity recognition task on Slavic languages organized for the BSNLP workshop", "start_pos": 53, "end_pos": 148, "type": "TASK", "confidence": 0.6967622637748718}]}, {"text": "A similar shared task was previously held in 2017 (BSNLP2017), and was composed of the same subtasks, but was evaluated on seven Slavic languages.", "labels": [], "entities": [{"text": "BSNLP2017)", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8864259123802185}]}, {"text": "It had a slightly different format, in that training data was not provided to the participants, so the majority of the submissions relied on cross-lingual or rule-based approaches.", "labels": [], "entities": []}, {"text": "Our overarching research goal for this project was to experiment with multisource neural NER transfer, leveraging recent advances in multilingual contextual embeddings.", "labels": [], "entities": [{"text": "multisource neural NER transfer", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6986014097929001}]}, {"text": "Ultimately, we aimed to maximize parametersharing by training a single model on the concatenation of training data from sources (languages).", "labels": [], "entities": []}, {"text": "Such multi-source systems have seen success in machine translation, and to some extent in non-neural NER systems, and neural systems (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8176522254943848}]}, {"text": "Given that training data is available in this iteration of the shared task, we purposefully chose to not include rule-based components into our model in order to focus on getting the most out of the given training data.", "labels": [], "entities": []}, {"text": "Our results on the official test data show that multi-source models using multilingual contextual embeddings produce strong performance, and incorporating a greater variety of languages within the same language family further boosts the results.", "labels": [], "entities": []}, {"text": "We also observe that combining training data from distinct tagsets often improves performance, and generalizes to the intended tagset better than expected.", "labels": [], "entities": []}, {"text": "Finally, our experiments using cross-lingual NER trained on English showed results inferior to monolingual experiments, but surprisingly high nonetheless.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since the shared task annotations are created on the document level, the evaluation metrics are somewhat different from standard NER.", "labels": [], "entities": []}, {"text": "They are similarly based on precision, recall, and F1 measure of retrieved entities, but are based on matching surface forms between sets of entities instead of matching spans.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9995073080062866}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9991235136985779}, {"text": "F1 measure", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.978040486574173}]}, {"text": "When matching surface forms, two types of evaluation are used.", "labels": [], "entities": []}, {"text": "These are described in the official documentation 2 as: \u2022 Relaxed evaluation: an entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one annotation of a named mention of this entity (regardless whether the extracted mention is base form); This is evaluated in two ways: -Partial match: partial matches count.", "labels": [], "entities": [{"text": "Relaxed", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9711264967918396}]}, {"text": "-Exact match: full string must match.", "labels": [], "entities": [{"text": "Exact match", "start_pos": 1, "end_pos": 12, "type": "METRIC", "confidence": 0.9418822824954987}]}, {"text": "\u2022 Strict evaluation: the system response should include exactly one annotation for each unique form of a named mention of an entity that is referred to in a given document, i.e., capturing and listing all variants of an entity is required.", "labels": [], "entities": []}, {"text": "There is no partial score given for this metric.", "labels": [], "entities": []}, {"text": "In our analysis below, we chose to report Strict evaluation as being the most similar to the spanbased F1 commonly used in NER.", "labels": [], "entities": [{"text": "Strict evaluation", "start_pos": 42, "end_pos": 59, "type": "METRIC", "confidence": 0.9382482171058655}, {"text": "spanbased F1", "start_pos": 93, "end_pos": 105, "type": "METRIC", "confidence": 0.8136895298957825}, {"text": "NER", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.8023527264595032}]}, {"text": "In our preliminary experiments, we created a development set to measure the relative improvement of each idea.", "labels": [], "entities": []}, {"text": "Given that our training set was composed of documents surrounding two distinct topics, our initial approach was to create a multitopic validation split, where the development set contained documents from both topics.", "labels": [], "entities": []}, {"text": "However, our models reached nearly perfect scores on this split due to the small variation of entities within a given topic.", "labels": [], "entities": []}, {"text": "This split was not representative of the official test set evaluation, since the testing data contains entirely new topics, and a lot more generalization would be needed.", "labels": [], "entities": []}, {"text": "To better imitate testing conditions, we split the training data by topic, using one topic for training, and the other for development.", "labels": [], "entities": []}, {"text": "Our preliminary experiments (not reported here) showed that using off-the-shelf multilingual FastText embeddings resulted in significantly worse performance than BERT, and so omitted them from our submissions.", "labels": [], "entities": [{"text": "BERT", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9726608991622925}]}], "tableCaptions": [{"text": " Table 1: Training data sizes in CoNLL and BSNLP19  datasets. Of the BSNLP19 sets, the largest (Polish) is  nearly 3 times the size of the smallest (Russian).", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.9241970181465149}, {"text": "BSNLP19  datasets", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.9184063374996185}, {"text": "BSNLP19 sets", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.8707199096679688}]}, {"text": " Table 2: Entity distribution statistics across all lan- guages in the BSNLP19 training set, where the \"Ratio\"  column refers to the proportion of the \"Total\" number  of entity type annotations to the \"Unique\" annotations.", "labels": [], "entities": [{"text": "BSNLP19 training set", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9671236077944437}]}, {"text": " Table 3: Official results on the Recognition task of BSNLP19, measured as F1 with Strict evaluation. The training  languages used are: Bulgarian (BG), Czech (CS), Polish (PL), Russian (RU), English (EN, CoNLL2003) and  the BSNLP17 languages (Croatian, Slovak, Slovene and Ukrainian). The top section of the table shows single- source experiments, in which each model is trained on a single language. The bottom section shows multi-source  experiments. The rightmost column, ALL, is a micro-average of the test results over the 4 test languages.", "labels": [], "entities": [{"text": "Recognition task", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.8240358829498291}, {"text": "BSNLP19", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.9213424921035767}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9952934384346008}, {"text": "ALL", "start_pos": 475, "end_pos": 478, "type": "METRIC", "confidence": 0.9942355751991272}]}, {"text": " Table 4: Precision (P), Recall (R), and F1 scores by  tag across all languages. AllTrain is the largest set of  training data that uses solely the target tagset, and Al- lLangsEng includes training data with the tagset with  MISC and without PRO or EVT.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9763083457946777}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9272370338439941}, {"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9995484948158264}, {"text": "AllTrain", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9173051714897156}]}]}