{"title": [{"text": "Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts", "labels": [], "entities": [{"text": "Sentence Mover's Similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9275626838207245}]}], "abstractContent": [{"text": "For evaluating machine-generated texts, automatic methods hold the promise of avoiding collection of human judgments, which can be expensive and time-consuming.", "labels": [], "entities": [{"text": "collection of human judgments", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.810192808508873}]}, {"text": "The most common automatic metrics, like BLEU and ROUGE, depend on exact word matching , an inflexible approach for measuring semantic similarity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9984742999076843}, {"text": "ROUGE", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9935387969017029}, {"text": "word matching", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.6094106584787369}]}, {"text": "We introduce methods based on sentence mover's similarity; our automatic metrics evaluate text in a continuous space using word and sentence embeddings.", "labels": [], "entities": [{"text": "sentence mover's similarity", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6952388808131218}]}, {"text": "We find that sentence-based metrics correlate with human judgments significantly better than ROUGE, both on machine-generated summaries (average length of 3.4 sentences) and human-authored essays (average length of 7.5).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9563261270523071}]}, {"text": "We also show that sentence mover's similarity can be used as a reward when learning a generation model via reinforcement learning; we present both automatic and human evaluations of summaries learned in this way, finding that our approach outperforms ROUGE.", "labels": [], "entities": [{"text": "sentence mover's similarity", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.7562059611082077}]}], "introductionContent": [{"text": "Automatic text evaluation reduces the need for human evaluations, which can be expensive and time-consuming to collect, particularly when evaluating long, multi-sentence texts.", "labels": [], "entities": [{"text": "text evaluation", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6511107534170151}]}, {"text": "Automatic metrics allow faster measures of progress when training and testing models and easier development of text generation systems.", "labels": [], "entities": []}, {"text": "However, existing automatic metrics for evaluating text are problematic.", "labels": [], "entities": []}, {"text": "Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE) for summarization,) for machine translation, and METEOR (Banerjee and) or CIDER () for image captioning.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.998497486114502}, {"text": "summarization", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.9711071252822876}, {"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.78160759806633}, {"text": "METEOR", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9843239188194275}, {"text": "image captioning", "start_pos": 184, "end_pos": 200, "type": "TASK", "confidence": 0.7285840213298798}]}, {"text": "Nevertheless, these metrics of- * Work done while author was at Microsoft Research.", "labels": [], "entities": []}, {"text": "The children eat lunch and play in the park.", "labels": [], "entities": []}, {"text": "The family is on a picnic.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the performance of the SMS and S+WMS metrics, we first examine their usefulness as evaluation metrics.", "labels": [], "entities": []}, {"text": "(In \u00a75, we evaluate their performance as cost functions for an extrinsic task, abstractive summarization.)", "labels": [], "entities": []}, {"text": "We measure the correlations between the scores assigned to texts by various automatic metrics (ROUGE-L, WMS, SMS, S+WMS) and the scores assigned by human judges.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9847608208656311}]}, {"text": "We are interested in multi-sentence texts, both machine-and humangenerated.", "labels": [], "entities": []}, {"text": "Therefore, we consider subsets of two corpora that have been judged by humans: a collection of automatically generated summaries of articles in the CNN/Daily Mail news dataset (alongside reference summaries; see Section 4.1;) and student essays from the Hewlett Foundation's Automated Student Assessment Prize (Section 4.2).", "labels": [], "entities": [{"text": "CNN/Daily Mail news dataset", "start_pos": 148, "end_pos": 175, "type": "DATASET", "confidence": 0.8751130700111389}, {"text": "Hewlett Foundation's Automated Student Assessment Prize", "start_pos": 254, "end_pos": 309, "type": "DATASET", "confidence": 0.7553913422993251}]}, {"text": "Statistics describing the datasets are in A.1.", "labels": [], "entities": [{"text": "A.1", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.5750774145126343}]}, {"text": "Because the word and sentence mover's similarity metrics are based on pretrained representations, we explore the effect of varying the word embedding method.", "labels": [], "entities": []}, {"text": "We present results for two different types of word embeddings: GloVe embeddings () and ELMo embeddings 8 ().", "labels": [], "entities": []}, {"text": "We obtain GloVe embeddings, which are type-based, 300-dimensional embeddings trained on Common Crawl, 9 using spaCy, 10 while the ELMo embeddings are character-based, 1,024-dimensional, contextual embeddings trained on the 1B Word Benchmark (.", "labels": [], "entities": [{"text": "Common Crawl", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.9211433529853821}, {"text": "1B Word Benchmark", "start_pos": 223, "end_pos": 240, "type": "DATASET", "confidence": 0.8130402167638143}]}, {"text": "We use ELMo to embed each sentence, which produces three vectors for each word, one from each layer of the model.", "labels": [], "entities": []}, {"text": "We average the vectors to get a single embedding for each word in the sentence.", "labels": [], "entities": []}, {"text": "All correlations are Spearman correlations, and significance in the improvement between two metrics' correlations with human judgment is calculated using the Williams (1959) significance test.", "labels": [], "entities": [{"text": "significance", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.9912686347961426}]}, {"text": "11  To understand how the sentence mover's similarity metrics evaluate automatically generated text, we use the subset of the CNN/Daily Mail dataset for which   from -1 to 1.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 126, "end_pos": 148, "type": "DATASET", "confidence": 0.9353245854377746}]}, {"text": "We consider the subset of summaries scored by two or more judges, taking the average to be the summary's score.", "labels": [], "entities": []}, {"text": "The automatic evaluation metrics score each generated summary's similarity to the human-authored reference summary from the CNN/Daily Mail dataset.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 124, "end_pos": 146, "type": "DATASET", "confidence": 0.9282494425773621}]}, {"text": "shows each metric's correlation with the human judgments.", "labels": [], "entities": []}, {"text": "SMS correlates best with human judgments, and both sentence-based metrics outperform ROUGE-L and WMS.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9650307297706604}, {"text": "WMS", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.8391448259353638}]}, {"text": "We find that the difference between GloVe and ELMo's scores is not significant.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9775566458702087}]}, {"text": "Discussion Two examples of generated summaries and their scores are shown in.", "labels": [], "entities": []}, {"text": "Because the scores cannot be directly compared between metrics, we distinguish scores that are in the top quartile for their metric (i.e., the highest rated) and in the bottom quartile (i.e., the lowest rated).", "labels": [], "entities": []}, {"text": "The first example in is highly rated by metrics using word and sentence embeddings, but judged to be a poor summary by ROUGE-L because information is reworded and reordered from the reference.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 119, "end_pos": 126, "type": "METRIC", "confidence": 0.8719172477722168}]}, {"text": "For example, the phrase \"asked for medical help\" is worded as \"sought medical attention\" in the hypothesis summary.", "labels": [], "entities": []}, {"text": "Nevertheless, exact word matching can be important for ensuring factual correctness.", "labels": [], "entities": [{"text": "word matching", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.6730730533599854}]}, {"text": "While the generated hypothesis summary states \"six officers have been suspended with pay\", the reference states they were actually \"suspended without pay.\"", "labels": [], "entities": []}, {"text": "The second example, which was generated with a seq2seq model, was one of the best summaries according to ROUGE-L but one of the worst according to SMS and S+WMS.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.6729365587234497}, {"text": "SMS", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.8912646770477295}]}, {"text": "It also received low human judgments, most likely due to its nonsensical repetitions.", "labels": [], "entities": []}, {"text": "While the short, repeated phrases like \"three different flavours\" match the reference summary well enough to score well with ROUGE-L, the overall sentence representations are distant from those in the reference summary, resulting in low SMS and S+WMS scores.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 125, "end_pos": 132, "type": "METRIC", "confidence": 0.9895083904266357}]}, {"text": "To test the metrics on human-authored text, we use a dataset of graded student essays that consists of responses to standardized test questions for tenth graders.", "labels": [], "entities": []}, {"text": "We use a subset of Question #3 from the exam, which asks the test-taker to synthesize information from a reading passage, where student responses contain 5-15 sentences.", "labels": [], "entities": []}, {"text": "Graders assigned the student-authored responses with scores ranging from 0 to 3.", "labels": [], "entities": []}, {"text": "For the reference essay, we use a top-scoring sample essay, which the graders had access to as a reference while assigning scores.", "labels": [], "entities": []}, {"text": "The full reference essay is in A.2.", "labels": [], "entities": [{"text": "A.2", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.954287588596344}]}, {"text": "shows the correlation of each metric with the evaluators' scores.", "labels": [], "entities": []}, {"text": "As in the summarization task, SMS outperforms both ROUGE-L and WMS.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9374560117721558}, {"text": "ROUGE-L", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9186173677444458}, {"text": "WMS", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.8448487520217896}]}, {"text": "However, in this case, having the sentence representations in the metric gives the best result, with S+WMS correlating best with human scores, significantly better than ROUGE-L.", "labels": [], "entities": [{"text": "WMS", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.5264332890510559}, {"text": "ROUGE-L", "start_pos": 169, "end_pos": 176, "type": "METRIC", "confidence": 0.9591931104660034}]}, {"text": "This is consistent across embedding type; once again, the choice of embedding does not create a significant difference between the sentence mover's metrics.", "labels": [], "entities": []}, {"text": "Discussion Aside from the length of the text, the Essays dataset presents the metrics with several challenges not found in the Summaries dataset.", "labels": [], "entities": [{"text": "Essays dataset", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.8233469724655151}, {"text": "Summaries dataset", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.7642115950584412}]}, {"text": "For example, the dataset contains a large number of spelling mistakes, due to both author misspellings and errors in the transcription process.", "labels": [], "entities": []}, {"text": "One essay begins, \"The setting of the story had effected the cycle's becuse if it was sub earbs he could have stoped anywhere and got water ...\"", "labels": [], "entities": []}, {"text": "The tone and style of the essay can also vary from the reference essay.", "labels": [], "entities": []}, {"text": "(For example, the author of Sample #3 in A.2 ends their essay by reflecting on how they would respond in the protagonist's place.)", "labels": [], "entities": []}, {"text": "Embedding-based metrics maybe more forgiving to deviations in writing style from the reference essay, such as the use of first person.", "labels": [], "entities": []}, {"text": "While indicates sentence mover's similarity metrics significantly improve correlation with human judgments over standard methods, there is still enough disagreement that we believe automatic metrics should not replace human evaluations.", "labels": [], "entities": []}, {"text": "Rather, they should complement human evaluations as an automatic proxy that can be used: Two examples from the Summaries dataset along with the scores they received (using GloVe) comparing reference (human summary) to hypothesis (model generated summary).", "labels": [], "entities": [{"text": "Summaries dataset", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.7938371002674103}]}, {"text": "Scores that are in the top quartile fora given metric are in green and bold.", "labels": [], "entities": []}, {"text": "Scores in the bottom quartile are in red and italics.", "labels": [], "entities": []}, {"text": "Human scores range from -1 to 1.", "labels": [], "entities": []}, {"text": "Please see A.2 for details.", "labels": [], "entities": [{"text": "A.2", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.9644580483436584}]}, {"text": "In addition to automatically evaluating text, we can also use sentence mover's metrics as rewards while learning text generation models.", "labels": [], "entities": []}, {"text": "To demonstrate this, we train an encoder-decoder model on the CNN/Daily Mail dataset to generate summaries using reinforcement learning (RL Model We encode the input document using 2-layered bidirectional LSTM networks and a 2-layered LSTM network for the decoder.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.9388864398002624}]}, {"text": "We use the attention mechanism ( to force the decoder model to learn to focus (i.e., attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder's LSTM.", "labels": [], "entities": []}, {"text": "We also include pointer networks (, which point to elements of the input sequence at each decoding step.", "labels": [], "entities": []}, {"text": "To train our policy-based generator, we use a mixed training objective that jointly optimizes multiple losses, which we describe below.", "labels": [], "entities": []}, {"text": "MLE Our baseline model uses maximum likelihood training for sequence generation.", "labels": [], "entities": [{"text": "MLE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.38035377860069275}, {"text": "sequence generation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7287694215774536}]}, {"text": "Given y * ={y * 1 ,y * 2 ,...,y * T } as the ground-truth summary fora given input document d, we compute the loss as: by taking the negative log-likelihood of the target word sequence.", "labels": [], "entities": []}, {"text": "Reinforcement Learning (RL) Loss The decoder generates the summary sequenc\u00ea y, which is then compared against the ground truth sequence y * to compute the reward r(\u02c6 y).", "labels": [], "entities": [{"text": "Reinforcement Learning (RL) Loss", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6508489201466242}]}, {"text": "Our model learns using a self-critical training approach (, by exploring new sequences and comparing them against the best greedily decoded sequence.", "labels": [], "entities": []}, {"text": "For each training exampled, we generate two output sequences: \u02c6 y, which is sampled from the probability distribution at each time step, p(\u02c6 y t | \u02c6 y 1 . .", "labels": [], "entities": []}, {"text": "\u02c6 y t\u22121 , d), and\u02dcyand\u02dc and\u02dcy, the baseline output, which is greedily generated by argmax decoding from p(\u02dc y t | \u02dc y 1 . .", "labels": [], "entities": []}, {"text": "\u02dc y t\u22121 , d).", "labels": [], "entities": []}, {"text": "Our mixed training objective is then to minimize: It ensures that, with better exploration, the model learns to generate sequences\u02c6ysequences\u02c6 sequences\u02c6y that receive higher rewards than the baseline\u02dcybaseline\u02dc baseline\u02dcy, increasing the overall reward expectation of the model.", "labels": [], "entities": []}, {"text": "Mixed Loss While training with only MLE loss will learn a better language model, it may not guarantee better results on discrete performance measures such as WMS and SMS.", "labels": [], "entities": []}, {"text": "Similarly, optimizing with only RL loss using SMS as a reward may increase the reward gathered at the expense of diminished readability and fluency of the generated summary.", "labels": [], "entities": []}, {"text": "A combination of the two objectives can yield improved task specific scores while maintaining a good language model: where \u03b3 is a hyperparameter balancing the two objective functions.", "labels": [], "entities": []}, {"text": "We pre-train models with MLE loss, and then continue with the mixed loss.", "labels": [], "entities": [{"text": "MLE loss", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9647774696350098}]}, {"text": "We train four different models on the CNN/Daily Mail dataset using mixed loss (MLE+RL) with ROUGE-L, WMS, SMS, and S+WMS as the reward functions.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.9319207906723023}, {"text": "ROUGE-L", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9927759170532227}]}, {"text": "Training details are in A.3 and A.4.", "labels": [], "entities": [{"text": "A.3", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.9624350666999817}, {"text": "A.4", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.9321014285087585}]}, {"text": "We evaluate the generated summaries from each model with ROUGE-L, WMS, SMS, and S+WMS in.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9764818549156189}]}, {"text": "While we include previously reported numbers, we re-trained the mixed loss models using ROUGE-L and use those as our baseline, as previously trained models should be heavily optimized and use more complex networks than ours.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9713873863220215}]}, {"text": "For fair comparison, we kept the encoder-decoder network type, structure, hyperparameters, and initialization the same for each model, changing only the reward.", "labels": [], "entities": []}, {"text": "We pre-trained an MLE model (\"MLE+Pgen (no reward) (re-trained baseline)\" in) and used it to initialize the mixed loss models with different reward functions.", "labels": [], "entities": []}, {"text": "Across all metrics, the models trained using WMS and SMS metrics as the reward outperform models trained with ROUGE-L as the reward function.", "labels": [], "entities": [{"text": "WMS", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8270494341850281}, {"text": "ROUGE-L", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.987065315246582}]}, {"text": "S+WMS models lag behind ROUGE-L.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9748759865760803}]}, {"text": "The SMS model outperforms all other models across all metrics on the abstractive summarization task, consistent with SMS's performance at evaluating summaries in \u00a74.1.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7797831892967224}]}, {"text": "shows summaries generated from each of the mixed loss models.", "labels": [], "entities": []}, {"text": "We collected human evaluations for 100 summaries generated by the mixed loss models to compare ROUGE-L as a reward to WMS, SMS, and S+WMS.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9221862554550171}]}, {"text": "Amazon Mechanical Turkers chose between two generated summaries, one from the ROUGE-L model and one from WMS, SMS, or Human Summary the 69 -year -old collaborated with nbc 's today show to launch a contest for an elvis -obsessed couple to win the ' ultimate wedding ' . the winning duo will get married in the brand new elvis presley 's graceland wedding chapel at the westgate hotel on thursday , april 23 . while she agreed to make an appearance , the woman who wed elvis in 1967 made one thing clear before unveiling the latest wedding chapel to bear his name : no impersonators .  Summaries and Essays: For the intrinsic tasks in \u00a74, we use two types of human-evaluated texts: machine-generated summaries and humanauthored essays.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.931933581829071}, {"text": "nbc 's today show", "start_pos": 168, "end_pos": 185, "type": "DATASET", "confidence": 0.9492309808731079}, {"text": "westgate", "start_pos": 369, "end_pos": 377, "type": "DATASET", "confidence": 0.9659097194671631}]}, {"text": "We follow and remove punctuation and stopwords.", "labels": [], "entities": []}, {"text": "(For contextual embeddings, these are removed after the embeddings are obtained.)", "labels": [], "entities": []}, {"text": "The details of the subsets we used are in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A comparison of scores for three different summaries for a reference passage (the first lines of a news  article). The human summary has been permuted with its clauses rearranged (Word order) and repeated (Repeti- tion). Word order changes negatively affect ROUGE-L more than repetition; the other metrics are unaffected by  word order choices but, to varying degrees, penalize repetition.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 268, "end_pos": 275, "type": "METRIC", "confidence": 0.9881439805030823}]}, {"text": " Table 2: Spearman correlation of metrics with human  evaluations. Asterisks indicate significant improve- ment over ROUGE-L, with (*) for p < 0.05 and (**)  for p < 0.01.", "labels": [], "entities": [{"text": "improve- ment", "start_pos": 98, "end_pos": 111, "type": "METRIC", "confidence": 0.9569455782572428}, {"text": "ROUGE-L", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9884033799171448}]}, {"text": " Table 3: Two examples from the Summaries dataset along with the scores they received (using GloVe) comparing  reference (human summary) to hypothesis (model generated summary). Scores that are in the top quartile for a  given metric are in green and bold. Scores in the bottom quartile are in red and italics. Human scores range from  -1 to 1. Please see A.2 for details.", "labels": [], "entities": [{"text": "Summaries dataset", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.7703942656517029}, {"text": "A.2", "start_pos": 356, "end_pos": 359, "type": "DATASET", "confidence": 0.776507556438446}]}, {"text": " Table 4: Evaluation on summarization task when various metrics are used as rewards during learning. Columns  show average score of each model's generated summaries according to various metrics. Previously reported results  (upper block): [1] MLE training with pointer networks (Pgen) (See et al., 2017) ; [2] Mixed MLE and RL training  with Pgen (Celikyilmaz et al., 2018), [3] Mixed MLE and RL training with Pgen and intra-decoder attention (Paulus  et al., 2018). The lower block reports re-trained baselines and our models with new metrics. Bold indicates best  among the lower block.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9063442349433899}]}, {"text": " Table 5: Summaries generated from the mixed MLE+RL loss models with ROUGE-L, WMS, S+WMS, and SMS  metrics as rewards, along with the corresponding human-authored reference summary.", "labels": [], "entities": [{"text": "MLE+RL loss", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.5895379334688187}, {"text": "ROUGE-L", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9764153361320496}]}, {"text": " Table 6: Human evaluations on a random subset of 100 summaries. The frequencies from the head-to-head com- parison of models trained with ROUGE-L against WMS/SMS/S+WMS are shown. Each summary is evaluated by 3  judges (300 summaries per criteria). '=' indicates no difference. All improvements are statistically significance at  p < 0.001.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9440786242485046}]}, {"text": " Table 8: Summary statistics of CNN/Daily Mail  (CNN/DM) Datasets.", "labels": [], "entities": [{"text": "CNN/Daily Mail  (CNN/DM) Datasets", "start_pos": 32, "end_pos": 65, "type": "DATASET", "confidence": 0.9398389101028443}]}]}