{"title": [{"text": "Better OOV Translation with Bilingual Terminology Mining", "labels": [], "entities": [{"text": "OOV Translation", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7278359830379486}]}], "abstractContent": [{"text": "Unseen words, also called out-of-vocabulary words (OOVs), are difficult for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7248122990131378}]}, {"text": "In neural machine translation, byte-pair encoding can be used to represent OOVs, but they are still often incorrectly translated.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6711891194184622}]}, {"text": "We improve the translation of OOVs in NMT using easy-to-obtain monolingual data.", "labels": [], "entities": []}, {"text": "We look for OOVs in the text to be translated and translate them using simple-to-construct bilingual word embeddings (BWEs).", "labels": [], "entities": []}, {"text": "In our MT experiments we take the 5\u2212best candidates, which is motivated by intrinsic mining experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.970088541507721}]}, {"text": "Using all five of the proposed target language words as queries we mine target-language sentences.", "labels": [], "entities": []}, {"text": "We then back-translate, forcing the back-translation of each of the five proposed target-language OOV-translation-candidates to be the original source-language OOV.", "labels": [], "entities": []}, {"text": "We show that by using this synthetic data to fine-tune our system the translation of OOVs can be dramatically improved.", "labels": [], "entities": []}, {"text": "In our experiments we use a system trained on Europarl and mine sentences containing medical terms from mono-lingual data.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9855929017066956}]}], "introductionContent": [{"text": "Neural machine translation (NMT) systems achieved a breakthrough in translation quality recently, by learning an end-to-end system.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7998297313849131}, {"text": "translation quality", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.9208780229091644}]}, {"text": "However, NMT systems have low quality when translating out-of-vocabulary words (OOVs), especially because they have a fixed modest sized vocabulary due to memory limitations.", "labels": [], "entities": [{"text": "translating out-of-vocabulary words (OOVs)", "start_pos": 43, "end_pos": 85, "type": "TASK", "confidence": 0.7892271280288696}]}, {"text": "By splitting words into subword units the problem of representing OOVs can be solved () but their translation is still problematic because by definition source-side OOVs were not seen in the training parallel data together with their translations.", "labels": [], "entities": []}, {"text": "In this work, we evaluate a simple approach for improving the translation of OOVs using bilingual word embeddings (BWEs), which we hope will trigger more research on this interesting problem.", "labels": [], "entities": [{"text": "translation of OOVs", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8667545318603516}]}, {"text": "In previous approaches, to include words in the target sentence for which the translation is unknown the token unk is often used which can be handled by later steps.", "labels": [], "entities": []}, {"text": "In many cases, such as named entities, it is possible to just copy the source token to the target side instead of translating it. proposed a pointer network based ( ) system which can learn when to translate and when to copy.", "labels": [], "entities": []}, {"text": "On the other hand, it is not possible to always copy when the translation is unknown.", "labels": [], "entities": []}, {"text": "If the alignment of the unk tokens to the source are known it is possible to translate source words using a large dictionary as a post-processing step.", "labels": [], "entities": []}, {"text": "Although NMT systems do not rely on word alignments explicitly, it is possible to learn and output word alignments (.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7028388530015945}]}, {"text": "It is also possible to use lexically-constrained decoders) in order to force the network to output certain words or sequences.", "labels": [], "entities": []}, {"text": "This way alignments are not needed and the system can decide the position of the constraints in the output.", "labels": [], "entities": []}, {"text": "The disadvantage of the above methods is that the translation of words needed to be decided either as a pre-or post-processing step without the context which makes the translation of some words, such as polysemous words, difficult.", "labels": [], "entities": []}, {"text": "In addition, lexically-constrained decoders require the target words to be observed in context at training time, or they will usually not be placed properly.", "labels": [], "entities": []}, {"text": "In contrast, we fine-tune NMT systems for better translation of problematic words on the sentence level and are thus able to exploit the context instead of handling the problem on the word level.", "labels": [], "entities": []}, {"text": "In our approach, we rely on bilingual word embeddings (BWEs) which can be built using large monolingual data and a cheap bilingual signal.", "labels": [], "entities": []}, {"text": "BWEs can easily cover a very large vocabulary.", "labels": [], "entities": [{"text": "BWEs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5483367443084717}]}, {"text": "Given the sentences to translate we look for source language words not included in the parallel training set of our MT system (OOVs).", "labels": [], "entities": [{"text": "translate", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.9646608829498291}, {"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.938659131526947}]}, {"text": "We translate OOVs using BWE based dictionaries taking nbest candidates as opposed to previous work (e.g.,) where only the best translation is used during post-processing.", "labels": [], "entities": []}, {"text": "In our experiments we take the 5\u2212best predictions of our BWEs, and retrieve sentences containing these target-language predictions from a monolingual corpus.", "labels": [], "entities": []}, {"text": "As was shown before, NMT systems can be quickly and effectively fine-tuned using just a few sentences.", "labels": [], "entities": []}, {"text": "Based on the 5\u2212best translations of OOVs we mine sentences from target language monolingual data and generate a synthetic parallel corpus using back-translation ().", "labels": [], "entities": []}, {"text": "We force the source-language translation of each OOV-translation-candidate to be the original OOV.", "labels": [], "entities": []}, {"text": "We show that by using this synthetic data to fine-tune our system the translation of unseen words can be dramatically improved, despite the presence of wrong translations of each OOV in the synthetic data.", "labels": [], "entities": [{"text": "translation of unseen words", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.8340000659227371}]}, {"text": "We test our system on the translation of English medical terms to German and show significant improvements using our approach.", "labels": [], "entities": [{"text": "translation of English medical terms to German", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.8682555130549839}]}, {"text": "In this paper, we study a domain adaptation task in order to show the advantages clearly, but our approach does not focus on this domain adaptation and it can also be directly applied generally with no modification (e.g., to an in-domain task).", "labels": [], "entities": []}], "datasetContent": [{"text": "We translate medical English sentences to German.", "labels": [], "entities": []}, {"text": "To train the baseline NMT system we used the Europarl v7 (EU) parallel dataset containing 1.9M sentence pairs (.", "labels": [], "entities": [{"text": "Europarl v7 (EU) parallel dataset", "start_pos": 45, "end_pos": 78, "type": "DATASET", "confidence": 0.9401597040040153}]}, {"text": "As medical data, we took 3.1M sentences from titles of medical Wikipedia articles, medical termpairs, patents and documents from the European Medicines Agency which are part of the UFAL Medical Corpus (UFAL).", "labels": [], "entities": [{"text": "UFAL Medical Corpus (UFAL)", "start_pos": 181, "end_pos": 207, "type": "DATASET", "confidence": 0.9383634328842163}]}, {"text": "Since the corpus is parallel, we split it and used even sentences for English and odd ones for German.", "labels": [], "entities": []}, {"text": "We built BWEs not only on the monolingual medical data but on  the concatenation of all Europarl data and the monolingual medical data to improve the quality of BWEs ( ).", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9923230409622192}, {"text": "BWEs", "start_pos": 161, "end_pos": 165, "type": "DATASET", "confidence": 0.6360781788825989}]}, {"text": "We only mined sentences from the monolingual medical German corpus.", "labels": [], "entities": [{"text": "monolingual medical German corpus", "start_pos": 33, "end_pos": 66, "type": "DATASET", "confidence": 0.7108249440789223}]}, {"text": "The testing of our approach was done on the medical Health In My Language (HimL) corpora ( ) containing 1.9K sentence pairs in both development and test sets.", "labels": [], "entities": [{"text": "medical Health In My Language (HimL) corpora", "start_pos": 44, "end_pos": 88, "type": "DATASET", "confidence": 0.5611508554882474}]}, {"text": "All corpora were tokenized and truecased using Moses scripts (.", "labels": [], "entities": []}, {"text": "We ran two sets of experiments.", "labels": [], "entities": []}, {"text": "First we show the translation quality of our dictionaries by looking at the OOVs and their translations using HimL development data.", "labels": [], "entities": [{"text": "HimL development data", "start_pos": 110, "end_pos": 131, "type": "DATASET", "confidence": 0.8644396861394247}]}, {"text": "Then we show translation quality improvements on the HimL test data.", "labels": [], "entities": [{"text": "HimL test data", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.8102569778760275}]}], "tableCaptions": [{"text": " Table 1: Quality of the mining procedure using different sizes of n\u2212best translations. We use only sentences from  UFAL or both EU and UFAL to build BWEs. We compare cosine only and cosine combined with orthography.", "labels": [], "entities": [{"text": "UFAL", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.9532642364501953}, {"text": "UFAL", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.9068865180015564}]}, {"text": " Table 2: Medical bilingual lexicon induction results  showing the quality of the BWE based dictionaries us- ing 1-best and 5-best translations.", "labels": [], "entities": [{"text": "Medical bilingual lexicon induction", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.5714925229549408}, {"text": "BWE based dictionaries", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.8226221402486166}]}, {"text": " Table 3: BLEU scores on the HimL test sets comparing  the baseline systems and our OOV specific fine-tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994564652442932}, {"text": "HimL test sets", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.797677089770635}]}]}