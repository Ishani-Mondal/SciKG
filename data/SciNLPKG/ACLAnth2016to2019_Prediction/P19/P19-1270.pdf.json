{"title": [{"text": "Joint Effects of Context and User History for Predicting Online Conversation Re-entries", "labels": [], "entities": [{"text": "Predicting Online Conversation Re-entries", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.9022243618965149}]}], "abstractContent": [{"text": "As the online world continues its exponential growth, interpersonal communication has come to play an increasingly central role in opinion formation and change.", "labels": [], "entities": [{"text": "opinion formation", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.8459252417087555}]}, {"text": "In order to help users better engage with each other online, we study a challenging problem of re-entry prediction foreseeing whether a user will comeback to a conversation they once participated in.", "labels": [], "entities": [{"text": "re-entry prediction", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7846141159534454}]}, {"text": "We hypothesize that both the context of the ongoing conversations and the users' previous chatting history will affect their continued interests in future engagement.", "labels": [], "entities": []}, {"text": "Specifically, we propose a neural framework with three main layers, each modeling context, user history, and interactions between them, to explore how the conversation context and user chatting history jointly result in their re-entry behavior.", "labels": [], "entities": []}, {"text": "We experiment with two large-scale datasets collected from Twitter and Reddit.", "labels": [], "entities": []}, {"text": "Results show that our proposed framework with bi-attention achieves an F1 score of 61.1 on Twit-ter conversations, outperforming the state-of-the-art methods from previous work.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9853232502937317}]}], "introductionContent": [{"text": "Interpersonal communication plays an important role in information exchange and idea sharing in our daily life.", "labels": [], "entities": [{"text": "information exchange", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7168047875165939}, {"text": "idea sharing", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.7093920111656189}]}, {"text": "We are involved in a wide variety of dialogues everyday, ranging from kitchen table conversations to online discussions, all help us make decisions, better understand important social issues, and form personal ideology.", "labels": [], "entities": []}, {"text": "However, individuals have limited attentions to engage in the massive amounts of online conversations.", "labels": [], "entities": []}, {"text": "There thus exists a pressing need to develop automatic conversation management tools to keep track of the discussions one would like to keep engaging in.", "labels": [], "entities": [{"text": "conversation management", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.6971841007471085}]}, {"text": "To meet such demand, we study the problem of predicting online conversation re-entries, * Jing Li is the corresponding author.", "labels": [], "entities": [{"text": "predicting online conversation re-entries", "start_pos": 45, "end_pos": 86, "type": "TASK", "confidence": 0.9019217938184738}]}, {"text": "where we aim to forecast whether the users will return to a discussion they once entered.", "labels": [], "entities": []}, {"text": "What will draw a user back?", "labels": [], "entities": []}, {"text": "To date, prior efforts for re-entry prediction mainly focus on modeling users engagement patterns in the ongoing conversations ( or rely on the social network structure, largely ignoring the rich information in users' previous chatting history.", "labels": [], "entities": [{"text": "re-entry prediction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.951938807964325}]}, {"text": "Here we argue that effective prediction of one's re-entry behavior requires the understanding of both the conversation context-what has been discussed in the dialogue under consideration, and user chatting history (henceforth user history)-what conversation topics the users are actively involved in.", "labels": [], "entities": []}, {"text": "In, we illustrate how the two factors together affect a user's re-entry behavior.", "labels": [], "entities": []}, {"text": "Along with two conversations that user U 1 participated in, also shown is their chatting history in previous discussions.", "labels": [], "entities": []}, {"text": "U 1 comes back to the second conversation since it involves topics on movies (e.g. mentioning Memento and Inception) and thus suits their interests according to the chatting his-tory, which also talked about movies.", "labels": [], "entities": []}, {"text": "In this work, we would like to focus on the joint effects of conversation context and user history, ignoring other information.", "labels": [], "entities": []}, {"text": "It would be a more challenging yet general task, since information like social networks maybe not available in some certain scenarios.", "labels": [], "entities": []}, {"text": "To study how conversation context and user history jointly affect user re-entries, we propose a novel neural framework that incorporates and aligns the indicative representations from the two information source.", "labels": [], "entities": []}, {"text": "To exploit the joint effects, four mechanisms are employed here: simple concatenation of the two types of representation, attention mechanism over turns in context, memory networks () -able to learn context attentions in aware of user history, and bi-attention () -further capturing interactions from two directions (context to history and history to context).", "labels": [], "entities": []}, {"text": "More importantly, our framework enables the re-entry prediction and corresponding representations to be learned in an end-to-end manner.", "labels": [], "entities": [{"text": "re-entry prediction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7599547505378723}]}, {"text": "On the contrary, previous methods for the same task rely on handcrafted features (, which often require laborintensive and time-consuming feature engineering processes.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to explore the joint effect of conversation context and user history on predicting re-entry behavior in a neural network framework.", "labels": [], "entities": []}, {"text": "We experiment with two large-scale datasets, one from Twitter (), the other from Reddit which is newly collected 1 . Our framework with bi-attention significantly outperforms all the comparing methods including the previous state of the art (.", "labels": [], "entities": []}, {"text": "For instance, our model achieves an F1 score of 61.1 on Twitter conversations, compared to an F1 score of 57.0 produced by, which is based on a rich set of handcrafted features.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9882163107395172}, {"text": "F1 score", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9881112575531006}]}, {"text": "Further experiments also show that the model with bi-attention can consistently outperform comparisons given varying lengths of conversation context.", "labels": [], "entities": []}, {"text": "It shows that bi-attention mechanism can well align users' personal interests and conversation context in varying scenarios.", "labels": [], "entities": []}, {"text": "After probing into the proposed neural framework with bi-attention, we find that meaningful representations are learned via exploring the joint effect of conversation context and user history, which explains the effectiveness of our framework in predicting re-entry behavior.", "labels": [], "entities": []}, {"text": "Finally, we carryout a human study, where we ask two humans to perform on the same task of first re-entry prediction.", "labels": [], "entities": [{"text": "first re-entry prediction", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.7105384469032288}]}, {"text": "The model with bi-attention outperforms both humans, suggesting the difficulty of the task as well as the effectiveness of our proposed framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Collection and Statistic Analysis.", "labels": [], "entities": [{"text": "Data Collection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6490446776151657}, {"text": "Statistic Analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.8735694289207458}]}, {"text": "To study re-entry behavior in online conversations, we collected two datasets: one is released by containing Twitter conversations formed by tweets from the TREC 2011 microblog track data 3 (henceforth Twitter), and the other is newly collected from Reddit (henceforth Reddit), a popular online forum.", "labels": [], "entities": [{"text": "TREC 2011 microblog track data 3", "start_pos": 157, "end_pos": 189, "type": "DATASET", "confidence": 0.9357344508171082}]}, {"text": "In our datasets, the conversations from Twitter concern diverse topics, while those from Reddit focus on the political issues.", "labels": [], "entities": []}, {"text": "Both datasets are in English.", "labels": [], "entities": []}, {"text": "To build the Reddit dataset, we first downloaded a large corpus publicly available on Reddit platform.", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9065156877040863}]}, {"text": "Then, we selected posts and comments in subreddit \"politics\" posted from Jan to Dec 2008.", "labels": [], "entities": []}, {"text": "Next, we formed Reddit posts and comments into conversations with replying relations revealed by the \"parent id\" of each comment.", "labels": [], "entities": []}, {"text": "Last, we removed conversations with only one turn.", "labels": [], "entities": []}, {"text": "In our main experiment, we focus on first reentry prediction, i.e. we predict whether a user u will comeback to a conversation c, given current turns until u's first entry inc as context and u's past chatting messages (posted before u engaging in c).", "labels": [], "entities": [{"text": "first reentry prediction", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.5813612838586172}]}, {"text": "For model training and evaluation, we randomly select 80%, 10%, and 10% conversations to form training, development, and test sets.", "labels": [], "entities": []}, {"text": "The statistics of the two datasets are shown in  average in Twitter conversations, and the number is only 1.3 on Reddit.", "labels": [], "entities": []}, {"text": "This results in the severe imbalance over instances of re-entry and non re-entry (negative samples where users do not come back) on both datasets.", "labels": [], "entities": []}, {"text": "Therefore, strategies should be adopted for alleviating the data imbalance issue, as done in Eq.", "labels": [], "entities": []}, {"text": "It indicates the sparse user activity in conversations, where most users engage in a conversation only once or twice.", "labels": [], "entities": []}, {"text": "Thus predicting user re-entries only with context will not perform well, and the complementary information underlying user history should be leveraged.", "labels": [], "entities": [{"text": "predicting user re-entries", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.8680333296457926}]}, {"text": "We further study the distributions of message number in user history and turn number in conversation context on both datasets.", "labels": [], "entities": []}, {"text": "As shown in, there exists severe sparsity in either user history or conversation context.", "labels": [], "entities": []}, {"text": "Thus combining them both might help alleviate the sparsity in one information source.", "labels": [], "entities": []}, {"text": "We also notice that Twitter and Reddit users exhibit different conversation behaviors.", "labels": [], "entities": []}, {"text": "Reddit users tend to engage in more conversations, resulting in more messages in user history (as shown in).", "labels": [], "entities": []}, {"text": "Twitter users are more likely to stay within each conversation, leading to lengthy discussions and larger re-entry frequencies on average, as shown in(b) and.", "labels": [], "entities": []}, {"text": "Data Preprocessing and Model Setting.", "labels": [], "entities": [{"text": "Data Preprocessing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6864394247531891}, {"text": "Model Setting", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.6844739615917206}]}, {"text": "For preprocessing Twitter data, we applied Glove tweet preprocessing toolkit (.", "labels": [], "entities": []}, {"text": "For the Reddit dataset, we first applied the open source natural language toolkit (NLTK)) for word tokenization.", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.86122727394104}, {"text": "word tokenization", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7397533655166626}]}, {"text": "Then, we replaced links with the generic tag \"URL\" and removed all the nonalphabetic tokens.", "labels": [], "entities": []}, {"text": "For both datasets, a vocabulary was built and maintained in experiments with all the tokens (including emoticons and punctuation) from training data.", "labels": [], "entities": []}, {"text": "For model setups, we initialize the embedding layer with 200-dimensional Glove embedding (, where Twitter version is used for our Twitter dataset and the Common Crawl version applied on Reddit dataset.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 130, "end_pos": 145, "type": "DATASET", "confidence": 0.9334081411361694}, {"text": "Reddit dataset", "start_pos": 186, "end_pos": 200, "type": "DATASET", "confidence": 0.9297742247581482}]}, {"text": "All the hyper-parameters are tuned on the development set by grid search.", "labels": [], "entities": []}, {"text": "The batch size is set to 32.", "labels": [], "entities": []}, {"text": "Adam optimizer) is adopted for parameter learning with initial learning rate selected among {10 \u22123 , 10 \u22124 , 10 \u22125 }.", "labels": [], "entities": []}, {"text": "For the BiLSTM encoders, we set the size of their hidden states to 200 (100 for each direction).", "labels": [], "entities": []}, {"text": "For the CNN encoders, we use filter windows of 2, 3, and 4, each with 50 feature maps.", "labels": [], "entities": []}, {"text": "In MemN2N interaction mechanism, we set hop numbers to 3.", "labels": [], "entities": []}, {"text": "In the learning loss, we set \u00b5 = 1 and \u03bb = 2, the weights to tackle data imbalance.", "labels": [], "entities": []}, {"text": "For re-entry prediction, a user is considered to comeback if the estimated probability for re-entry is larger than 0.5.", "labels": [], "entities": [{"text": "re-entry prediction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8897810578346252}]}, {"text": "For comparisons, we consider three baselines.", "labels": [], "entities": []}, {"text": "RANDOM baseline: randomly pickup a \"yes-or-no\" answer.", "labels": [], "entities": [{"text": "RANDOM baseline", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.5699407160282135}]}, {"text": "HISTORY baseline: predict based on users' history re-entry rate before current conversation, which will answer \"yes\" if the rate exceeds a pre-defined threshold (set on development data), and \"no\" otherwise.", "labels": [], "entities": [{"text": "HISTORY baseline", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.6159822046756744}, {"text": "predict", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9574765563011169}]}, {"text": "(For users who lack such information before current conversation, it predicts \"yes or no\" randomly.)", "labels": [], "entities": []}, {"text": "ALL-YES baseline: always answers \"yes\" in re-entry prediction.", "labels": [], "entities": [{"text": "ALL-YES baseline", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9321691691875458}, {"text": "re-entry prediction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.6316947788000107}]}, {"text": "Its assumption is that users tend to be drawn back to the conversations they once participated by the platform's auto messages inviting them to return.", "labels": [], "entities": []}, {"text": "For supervised models, we compare with CCCT, the state-of-the-art method proposed by, where the bagged decision tree with manually-crafted features (including arrival patterns, timing effects, most related terms, etc.) are employed for re-entry prediction.", "labels": [], "entities": [{"text": "re-entry prediction", "start_pos": 236, "end_pos": 255, "type": "TASK", "confidence": 0.8496142327785492}]}, {"text": "We do not compare with, since most of its features are related to social networks or Twitter group information, which is unavailable in our data.", "labels": [], "entities": []}, {"text": "In our proposed neural framework, we further compare varying encoders for turn modeling and mechanisms to model the interactions between user history and conversation context.", "labels": [], "entities": [{"text": "turn modeling", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.7179634422063828}]}, {"text": "We first compare three turn encoders -AVG-EMBED (average embedding), CNN, and BILSTM, to examine their performance in turn representation learning.", "labels": [], "entities": [{"text": "AVG-EMBED", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.8982752561569214}, {"text": "CNN", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.6278865337371826}, {"text": "BILSTM", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9974007606506348}, {"text": "turn representation learning", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.6912387808163961}]}, {"text": "Their results are compared on our variant only with context modeling layer and the best encoder (turned out to be BILSTM) is applied on the full model.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9899379014968872}]}, {"text": "For the interaction modeling layer, we also study the effectiveness of four mechanisms to combine user history and conversation context -simple concatenation (CON), attention (ATT), memory networks (MEM), and bi-attention (BIA).", "labels": [], "entities": [{"text": "bi-attention (BIA)", "start_pos": 209, "end_pos": 227, "type": "METRIC", "confidence": 0.6666299253702164}]}], "tableCaptions": [{"text": " Table 1: Statistics of two datasets.", "labels": [], "entities": []}, {"text": " Table 2: Results on first re-entry prediction. The best results in each column are in bold. Model BILSTM+BIA  yields significantly better AUC and F1 scores than all other comparisons (p < 0.05, paired t-test).", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9953984618186951}, {"text": "BIA", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.7074477672576904}, {"text": "AUC", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9936901330947876}, {"text": "F1", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.9851821064949036}]}, {"text": " Table 3: Results of our variants. SML: structure mod- eling layer. Meta: auxiliary triples a t . Our full model  BILSTM+BIA obtains the best F1.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9542210698127747}, {"text": "BIA", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.5150836706161499}, {"text": "F1", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9880650639533997}]}, {"text": " Table 4: Predicted probabilities by different models for  user U 1 's re-entry to conversations C 1 and C 2 in Fig- ure 1. CCCT can only yield binary outputs. For other  neural models, predicting threshold is 0.5.", "labels": [], "entities": []}, {"text": " Table 5: Numbers of correct predictions made by hu- mans, reading conversation context only and further  seeing users' chatting history (boldfaced numbers),  compared to the results of our best model in same set- ting. A random guess gives 25 (out of 50 pairs).", "labels": [], "entities": []}]}