{"title": [{"text": "The LAIX Systems in the BEA-2019 GEC Shared Task", "labels": [], "entities": [{"text": "BEA-2019 GEC Shared Task", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.8736422508955002}]}], "abstractContent": [{"text": "In this paper, we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task.", "labels": [], "entities": [{"text": "BEA-2019 GEC Shared Task", "start_pos": 101, "end_pos": 125, "type": "DATASET", "confidence": 0.7773519158363342}]}, {"text": "We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models.", "labels": [], "entities": [{"text": "competitive classification", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.649941623210907}]}, {"text": "For different tracks, we use ensemble systems to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems.", "labels": [], "entities": []}, {"text": "Our GEC systems ranked the first in the Unrestricted Track, and the third in both the Restricted Track and the Low Resource Track.", "labels": [], "entities": [{"text": "GEC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9114298224449158}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) is the task of automatically correcting grammatical errors in text.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8213570813337961}, {"text": "automatically correcting grammatical errors in text", "start_pos": 50, "end_pos": 101, "type": "TASK", "confidence": 0.7339339852333069}]}, {"text": "With the increasing number of language learners, GEC has gained more and more attention from educationists and researchers in the past decade.", "labels": [], "entities": [{"text": "GEC", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.621508777141571}]}, {"text": "The following is a GEC example: I [fall \u2192 fell] asleep at 11 p.m. last.", "labels": [], "entities": []}, {"text": "Here fall needs to be corrected to its past tense form and nigh is a spelling mistake.", "labels": [], "entities": [{"text": "fall", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9329399466514587}, {"text": "nigh", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9120842218399048}]}, {"text": "GEC is considered as a mapping task from incorrect sentences to correct sentences.", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5451845526695251}]}, {"text": "Incorrect sentences can be seen as being produced by adding noises to correct sentences.", "labels": [], "entities": []}, {"text": "The added noise does not happen randomly, but occurs when people learn or use the language according to a certain error distribution and language usage bias.", "labels": [], "entities": []}, {"text": "Initially, people used rule-based approaches to solve GEC problems (Naber and).", "labels": [], "entities": [{"text": "GEC", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.7917819023132324}]}, {"text": "Rules are relatively easy to make but with poor generalization.", "labels": [], "entities": []}, {"text": "Later researchers began to treat GEC as a classification task.", "labels": [], "entities": [{"text": "GEC", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.978652834892273}, {"text": "classification task", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8940469324588776}]}, {"text": "According to the grammatical information around the target word, classifiers can be constructed to predict the true grammatical role of the target word.", "labels": [], "entities": []}, {"text": "One drawback of the classification methods for GEC is that training different classifiers for different error types maybe resource-intensive and inefficient since there are many grammatical error types.", "labels": [], "entities": [{"text": "GEC", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8797673583030701}]}, {"text": "Recently, translation methods have become the focus of research, and there is a clear trend that state-of-the-art GEC systems are being shifted from traditional NLP methods to NMT based methods.", "labels": [], "entities": [{"text": "translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9741646647453308}]}, {"text": "In recent years, GEC performance has seen significant improvement in some public GEC test sets.", "labels": [], "entities": [{"text": "GEC", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9506009817123413}, {"text": "GEC test sets", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.7851534783840179}]}, {"text": "In and) GEC Shared Task, machine learning based GEC methods emerged with relatively good performance.", "labels": [], "entities": []}, {"text": "Classification methods achieved the best result in.", "labels": [], "entities": []}, {"text": "After that, statistical machine translation (SMT) methods began to show better performance in.) was the first study to obtain the state-ofthe-art result with neural networks.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.7969291259845098}]}, {"text": "Then after, machine translation methods became the mainstream in GEC solutions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8137074112892151}, {"text": "GEC", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8557882905006409}]}, {"text": "In addition, an RNNbased context model achieved better results than previous traditional classification models (.", "labels": [], "entities": []}, {"text": "Using a CNN-based sequenceto-sequence architecture, proposed the first end-to-end NMT model and reported the state-ofthe-art result.", "labels": [], "entities": []}, {"text": "As Transformer () plays an increasingly important role in sequence modeling, Transformer-based end-to-end NMT models began to lead the current GEC research (.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.743636429309845}, {"text": "GEC research", "start_pos": 143, "end_pos": 155, "type": "DATASET", "confidence": 0.9149196743965149}]}, {"text": "It is worth mentioning that () used Wikipedia ed-its history corpus, which is huge but noisy, and gained a result very close to the state-of-the-art result.", "labels": [], "entities": [{"text": "Wikipedia ed-its history corpus", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.9298425614833832}]}, {"text": "Learning a GEC translation model from noisy data is a worthy future direction as the GEC parallel corpus is expensive to obtain.", "labels": [], "entities": [{"text": "GEC translation", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.8252629935741425}, {"text": "GEC parallel corpus", "start_pos": 85, "end_pos": 104, "type": "DATASET", "confidence": 0.8049143155415853}]}, {"text": "This paper describes our two systems for the three tracks in the BEA-2019 GEC Shared Task (.", "labels": [], "entities": [{"text": "BEA-2019 GEC Shared Task", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.8327785134315491}]}, {"text": "We use two popular NMT models and two improved versions of neural classification models to train the basic models.", "labels": [], "entities": []}, {"text": "Ensemble strategies are then used to combine outcomes from different models.", "labels": [], "entities": []}, {"text": "Our two systems for the three tracks are described in next section.", "labels": [], "entities": []}, {"text": "In Section 3, we evaluate the systems on the development data and show the final results on the test data.", "labels": [], "entities": []}, {"text": "Section 4 concludes the paper and summarizes the future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus used for training in corresponding track.", "labels": [], "entities": []}, {"text": " Table 2: Results of tuned single CNN-based translation models on the development set.", "labels": [], "entities": []}, {"text": " Table 3: Results of CNN-based ensemble systems on the development set.", "labels": [], "entities": []}, {"text": " Table 4: Results of Transformer-based translation models on the development set.", "labels": [], "entities": [{"text": "Transformer-based translation", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.9260047078132629}]}, {"text": " Table 5: Parameters in the conflict solver for the  ensemble methods in Restricted and Unrestricted  Track.", "labels": [], "entities": [{"text": "conflict solver", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6778886467218399}]}, {"text": " Table 6: Results of Restricted and Unrestricted Track.", "labels": [], "entities": []}, {"text": " Table 7: Parameters for the ensemble method in Low  Resource Track.", "labels": [], "entities": []}, {"text": " Table 8: Results of Low Resource Track.", "labels": [], "entities": [{"text": "Low Resource Track", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.6458354393641154}]}]}