{"title": [{"text": "Making Fast Graph-based Algorithms with Graph Metric Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "The computation of distance measures between nodes in graphs is inefficient and does not scale to large graphs.", "labels": [], "entities": []}, {"text": "We explore dense vector representations as an effective way to approximate the same information: we introduce a simple yet efficient and effective approach for learning graph embeddings.", "labels": [], "entities": []}, {"text": "Instead of directly operating on the graph structure , our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account.", "labels": [], "entities": []}, {"text": "We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks and show evaluations on the WordNet graph and two knowledge base graphs.", "labels": [], "entities": [{"text": "predicting word similarity", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.7905359268188477}, {"text": "word sense disambiguation", "start_pos": 275, "end_pos": 300, "type": "TASK", "confidence": 0.6952038705348969}, {"text": "WordNet graph", "start_pos": 335, "end_pos": 348, "type": "DATASET", "confidence": 0.965382307767868}]}, {"text": "When operating on large graphs, such as transportation networks, social networks, or lexical resources , the need for estimating similarities between nodes arises.", "labels": [], "entities": []}, {"text": "For many domain-specific applications, custom graph node similarity measures sim : V \u00d7 V \u2192 R have been defined on pairs of nodes V of a graph G = (V, E).", "labels": [], "entities": []}, {"text": "Examples include travel time, communities, or semantic distances for knowledge-based word sense dis-ambiguation on WordNet (Miller, 1995).", "labels": [], "entities": [{"text": "word sense dis-ambiguation", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.6313616037368774}]}, {"text": "For instance , the similarity s ij between the cup.n.01 and mug.n.01 synsets in the WordNet is 1 4 according to the inverted shortest path distance as these two nodes are connected by the undirected path cup \u2192 container \u2190 vessel \u2190 drinking vessel \u2190 mug.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9547260403633118}]}, {"text": "In recent years, a large variety of such node similarity measures have been described, many of which are based on the notion of a random walk (Fouss et al., 2007; Pilehvar and Navigli, 2015; Lebichot et al., 2018).", "labels": [], "entities": []}, {"text": "As given by the structure of the problem, most such measures are defined as traversals of edges E of the graph, which makes their computation prohibitively inefficient.", "labels": [], "entities": []}, {"text": "To this end, we propose the path2vec model 1 , which solves this problem by decoupling development and use of graph-based measures, and-in contrast to purely walk-based embeddings-is trainable to reflect custom node similarity measures.", "labels": [], "entities": []}, {"text": "We represent nodes in a graph with dense embeddings that are good in approximating such custom, e.g. application-specific, pairwise node similarity measures.", "labels": [], "entities": []}, {"text": "Similarity computations in a vector space are several orders of magnitude faster than computations directly operating on the graph.", "labels": [], "entities": []}, {"text": "First, effectiveness of our model is shown in-trinsically by learning metric embeddings for three types of graphs (WordNet, FreeBase, and DBPedia), based on several similarity measures.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.9793184995651245}]}, {"text": "Second, in an extrinsic evaluation on the Word Sense Disambiguation (WSD) task (Nav-igli, 2009) we replace several original measures with their vectorized counterparts in a known graph-based WSD algorithm by Sinha and Mi-halcea (2007), reaching comparable levels of performance with the graph-based algorithms while maintaining computational gains.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD) task", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.8117295844214303}]}, {"text": "The main contribution of this paper is the demonstration of the effectiveness and efficiency of the path2vec node embedding method (Kutu-zov et al., 2019).", "labels": [], "entities": []}, {"text": "This method learns dense vector embeddings of nodes V based on a user-defined custom similarity measure sim, e.g. the shortest path distance or any other similarity measure.", "labels": [], "entities": []}, {"text": "While our method is able to closely approximate quite different similarity measures as we show 1 https://github.com/uhh-lt/path2vec", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Experimental Setting To showcase how our approach can be be used inside a graph-based algorithm, we employ word sense disambiguation (WSD) task, reproducing the approach of.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.7155674497286478}]}, {"text": "We replace graph similarities with the dot product between node embeddings and study how it influences the WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9711183905601501}]}, {"text": "The WSD algorithm starts with building a graph where the nodes are the WordNet synsets of the words in the input sentence.", "labels": [], "entities": [{"text": "WSD", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8696510791778564}]}, {"text": "The nodes are then connected by edges weighted with the similarity values between the synset pairs.", "labels": [], "entities": []}, {"text": "The final step is selecting the most likely sense for each word based on the weighted in-degree centrality score for each synset.", "labels": [], "entities": []}, {"text": "The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number.", "labels": [], "entities": []}, {"text": "To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs).", "labels": [], "entities": [{"text": "WordNet graph", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9277933835983276}]}, {"text": "More specifically, we base our experiments on two publicly available standard samples from these two resources: the FB15k-237 () dataset contains 14,951 entities/nodes and is derived from Freebase (; the DB100k () dataset contains 99,604 entities/nodes and is derived from DBPe-dia (.", "labels": [], "entities": [{"text": "FB15k-237 () dataset", "start_pos": 116, "end_pos": 136, "type": "DATASET", "confidence": 0.8329327901204427}, {"text": "Freebase", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.9739000201225281}, {"text": "DB100k () dataset", "start_pos": 204, "end_pos": 221, "type": "DATASET", "confidence": 0.8217126131057739}, {"text": "DBPe-dia", "start_pos": 273, "end_pos": 281, "type": "DATASET", "confidence": 0.9677227735519409}]}, {"text": "It is important to note that both datasets were used to evaluate approaches that learn knowledge graph embeddings, e.g. () on the task on knowledge base completion (KBC), to predict missing KB edges/relations between nodes/entities.", "labels": [], "entities": [{"text": "knowledge base completion (KBC)", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.7510250310103098}]}, {"text": "The specificity of our model is that it learns a given graph similarity metric, which is not provided in these datasets.", "labels": [], "entities": []}, {"text": "Therefore, we use only the graphs from these datasets, computing the shortest path distances between all pairs of nodes using the algorithm of.", "labels": [], "entities": []}, {"text": "Instead of the KBC task, we evaluate on the task of predicting node similarity, here using the shortest path distance.", "labels": [], "entities": [{"text": "predicting node similarity", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.8128278851509094}]}, {"text": "We generate a random sample of node pairs for testing from the set of all node pairs (these pairs are excluded from training).", "labels": [], "entities": []}, {"text": "The test set contains an equal number of paths of length 1-7 (in total 1050 pairs each, 150 pairs per path length).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlations with WordNet similar- ities (left) and human judgments (right) \u00d7100.", "labels": [], "entities": []}]}