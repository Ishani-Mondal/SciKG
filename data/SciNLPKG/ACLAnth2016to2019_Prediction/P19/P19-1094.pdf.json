{"title": [{"text": "From Surrogacy to Adoption; From Bitcoin to Cryptocurrency: Debate Topic Expansion", "labels": [], "entities": [{"text": "Surrogacy to Adoption", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.848323126633962}]}], "abstractContent": [{"text": "When debating a controversial topic, it is often desirable to expand the boundaries of discussion.", "labels": [], "entities": []}, {"text": "For example, we may consider the pros and cons of possible alternatives to the debate topic, make generalizations, or give specific examples.", "labels": [], "entities": []}, {"text": "We introduce the task of Debate Topic Expansion-finding such related topics fora given debate topic, along with a novel annotated dataset for the task.", "labels": [], "entities": [{"text": "Debate Topic Expansion-finding such related topics", "start_pos": 25, "end_pos": 75, "type": "TASK", "confidence": 0.8698577483495077}]}, {"text": "We focus on relations between Wikipedia concepts, and show that they differ from well-studied lexical-semantic relations such as hypernyms, hyponyms and antonyms.", "labels": [], "entities": []}, {"text": "We present algorithms for finding both consistent and con-trastive expansions and demonstrate their effectiveness empirically.", "labels": [], "entities": []}, {"text": "We suggest that debate topic expansion may have various use cases in argumentation mining.", "labels": [], "entities": [{"text": "debate topic expansion", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6600534816582998}, {"text": "argumentation mining", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.9097837507724762}]}], "introductionContent": [{"text": "Recent years saw substantial advancement of Debating Technologies -computational technologies developed directly to enhance, support, and engage with human debating.", "labels": [], "entities": []}, {"text": "A recent milestone in this field is IBM R 's Project Debater R 1 , the first demonstration of a live competitive debate between an AI system and a human debate champion.", "labels": [], "entities": []}, {"text": "When debating a controversial topic, it is often desirable to expand the boundaries of the discussion, and bring up arguments about related topics.", "labels": [], "entities": []}, {"text": "For example, when discussing the pros and cons of the presidential system, it is natural to contrast it with those of the parliamentary system.", "labels": [], "entities": []}, {"text": "When debating alternative medicine, we may discuss specific examples, such as homeopathy and naturopathy.", "labels": [], "entities": []}, {"text": "Conversely, when discussing bitcoins, we can speak more broadly on cryptocurrency.", "labels": [], "entities": []}, {"text": "Consider the use of debating technologies for decision support, where the pros and cons of a given proposal are extracted from a large corpus, summarized and presented to the user.", "labels": [], "entities": [{"text": "decision support", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.9212608337402344}]}, {"text": "Current methods for topic-related, corpus-wide argument mining only specify the given debate topic in their search queries (.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.686040386557579}]}, {"text": "As a result, much of the relevant argumentative content is left out of their reach.", "labels": [], "entities": []}, {"text": "Alternatively, context-independent argument mining can exhaustively extract argumentative content from a corpus (), but it cannot tell which arguments are actually relevant for the topic in question.", "labels": [], "entities": [{"text": "context-independent argument mining", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.6100970904032389}]}, {"text": "In this work we take a step towards closing this gap, by introducing the task of Debate Topic Expansion -finding related topics that can enrich our arguments and strengthen our case when debating a given topic.", "labels": [], "entities": [{"text": "Debate Topic Expansion", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.8291523257891337}]}, {"text": "Following previous work (, we focus on topics that are Wikipedia concepts (article titles in Wikipedia).", "labels": [], "entities": []}, {"text": "Two types of expansions are studied: consistent and contrastive ().", "labels": [], "entities": []}, {"text": "Arguing in favor or against a consistent expansion may support the same stance towards the original topic, whereas for contrastive expansions the stance is reversed.", "labels": [], "entities": []}, {"text": "For example, Bitcoin\u21d2Cryptocurrency and Alternative medicine\u21d2Homeopathy are consistent expansions, while Presidential system\u21d2Parliamentary system is a contrastive expansion, since we may support the presidential system by criticizing the parliamentary system.", "labels": [], "entities": []}, {"text": "While these relations may seem reminiscent of hypernyms/hyponyms, antonyms and co-hyponyms, we show that they differ from these well-studied relations.", "labels": [], "entities": []}, {"text": "We propose a three-step method for debate topic expansion.", "labels": [], "entities": [{"text": "debate topic expansion", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7969138224919637}]}, {"text": "First, expansion candidates are extracted from a large corpus using a set of prede-fined patterns.", "labels": [], "entities": []}, {"text": "Each expansion type makes use of a different type of corpus and patterns.", "labels": [], "entities": []}, {"text": "In the second step, we apply a set of filters to the extraction results.", "labels": [], "entities": []}, {"text": "Candidates that pass these filters are manually annotated as good/bad expansions, resulting in the first dataset for this task.", "labels": [], "entities": []}, {"text": "The labeled dataset is utilized in the final step, where we employ supervised classification to identify good expansions amongst the candidates.", "labels": [], "entities": []}, {"text": "We explore two approaches: (i) traditional feature-based classification, for which we introduce a novel set of features; (ii) a deep neural network, which is trained by distant supervision.", "labels": [], "entities": [{"text": "feature-based classification", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.678269624710083}]}, {"text": "Experiments with hundreds of unseen topics show promising results, and the best performance is achieved by combining both classification approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "Based on our candidate acquisition method, we created the DTE (Debate Topic Expansion) dataset, comprising about 3,000 annotated pairs of debate concepts and their expansion candidates.", "labels": [], "entities": [{"text": "candidate acquisition", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.746193915605545}, {"text": "DTE (Debate Topic Expansion) dataset", "start_pos": 58, "end_pos": 94, "type": "DATASET", "confidence": 0.5371651947498322}]}, {"text": "The dataset contains positive and negative examples of both consistent and contrastive expansions.", "labels": [], "entities": []}, {"text": "The construction of the dataset is described below.", "labels": [], "entities": []}, {"text": "We manually collected a diverse set of 632 debate concepts from a variety of sources, including the idebate website . For each debate concept, we performed candidate extraction and filtering for consistent and contrastive expansions, as described in the previous section.", "labels": [], "entities": [{"text": "idebate website", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.9344519376754761}, {"text": "candidate extraction", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.6995530873537064}]}, {"text": "Each of the resulting (DC,EC) pairs was assessed by five annotators, and was labeled as either positive (good expansion) or negative (bad expansion), based on the majority labeling.", "labels": [], "entities": []}, {"text": "One intriguing subtlety we noticed early on is that in the case of contrastive expansions, whether or not EC is a good expansion somewhat depends on our stance towards the DC.", "labels": [], "entities": []}, {"text": "If we argue against the DC (Con stance), we may choose any plausible alternative as our EC, following a line of argument such as \"EC is a better alternative to DC\".", "labels": [], "entities": []}, {"text": "However, when we argue in favor of DC (Pro stance), the typical argument changes to \"if we don't choose DC then we are left with EC\", which requires EC to be the \"default\" alternative to DC.", "labels": [], "entities": [{"text": "EC", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.7573724389076233}]}, {"text": "For example, when arguing against atheism, one may argue that Christians are happier than atheists; however, when taking a proatheism stance, it is better to argue against religion in general than specifically against Christianity.", "labels": [], "entities": []}, {"text": "The annotators were therefore asked to assess contrastive expansions for both positive and negative stances.", "labels": [], "entities": []}, {"text": "However, developing a classifier that is able to make such fine distinctions falls out of the scope of the current work.", "labels": [], "entities": []}, {"text": "Instead, we take the union of good expansions for each stance as our positive instances, while keeping per-stance annotations in the dataset for future research.", "labels": [], "entities": []}, {"text": "provides some statistics on the resulting dataset.", "labels": [], "entities": []}, {"text": "Our candidate acquisition method was found to be applicable to a significant portion of the topics: one or more good consistent expansions were found for 43% of the topics, and good contrastive expansions were found for 19% of the topics.", "labels": [], "entities": []}, {"text": "Precision, however, is low, even after applying our filters: 49% for consistent expansions, and 19% for contrastive expansions.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9909228682518005}]}, {"text": "This motivates an additional supervised classification step, to be presented in the next section.", "labels": [], "entities": []}, {"text": "These statistics suggest that identifying contrastive expansions is considerably more challenging than finding consistent expansions.", "labels": [], "entities": []}, {"text": "Fleiss' \u03ba is 0.45 for consistent expansions and 0.43 for the unified contrastive expansions, which https://idebate.org/debatabase  corresponds to \"moderate agreement\".", "labels": [], "entities": []}, {"text": "This level of agreement reflects the complexity and inherent subjectivity of the task, as discussed in Section 2, and is comparable to previous results for annotation tasks in argumentation mining.", "labels": [], "entities": [{"text": "agreement", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9629260301589966}, {"text": "argumentation mining", "start_pos": 176, "end_pos": 196, "type": "TASK", "confidence": 0.8994815647602081}]}, {"text": "For example, Aharoni et al. report \u03ba of 0.39-0.4 for claim and evidence annotation in Wikipedia articles.", "labels": [], "entities": []}, {"text": "We assess the performance of our method on the following practical task: given a debate concept DC, find one good expansion concept EC for each expansion type.", "labels": [], "entities": []}, {"text": "Recall that our dataset includes annotations for all the expansion candidates found for each DC by the candidate acquisition algorithm.", "labels": [], "entities": []}, {"text": "Here we compare different methods for choosing one good expansion from these candidates.", "labels": [], "entities": []}, {"text": "For each expansion type, we assume a scoring function f (X, Y ) over a pair of concepts, which predicts the likelihood of the target relation holding between X and Y . We further assume a threshold \u03b1 representing the minimum score fora good expansion.", "labels": [], "entities": []}, {"text": "Given a debate concept DC, we choose its highest scoring expansion, if its score exceeds the threshold.", "labels": [], "entities": []}, {"text": "If no expansion was found, or all the expansion scores are below the threshold, we make no prediction.", "labels": [], "entities": [{"text": "expansion", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9594346880912781}]}, {"text": "By modifying the threshold, we can explore the tradeoff between the number of predictions we make and their quality.", "labels": [], "entities": []}, {"text": "The following scoring functions were assessed: Due to the small number of instances in the development set, we did not use it to tune the LR classifier, but rather used both the train and the development sets to train the classifier.", "labels": [], "entities": [{"text": "LR classifier", "start_pos": 138, "end_pos": 151, "type": "DATASET", "confidence": 0.8933151960372925}]}, {"text": "Together they contain 983/626 consistent/contrastive expansion candidates, respectively.", "labels": [], "entities": []}, {"text": "Let N be the total number of debate concepts in the test set, let C be the number of correct predictions, let P be the number of predictions made, and let R be the number of debate concepts in the test set for which good expansions exist.", "labels": [], "entities": []}, {"text": "We define the following measures: (i) Precision@1= C P ; (ii) Recall@1= C R ; and (iii) Coverage= P N . compares the above candidate scoring methods for both consistent (a) and contrastive (b) expansions.", "labels": [], "entities": [{"text": "Precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9781959056854248}, {"text": "Recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9942448735237122}]}, {"text": "For each configuration, the Precision@1 vs Recall@1 graph is obtained by modifying the threshold \u03b1.", "labels": [], "entities": [{"text": "Precision@1 vs Recall@1 graph", "start_pos": 28, "end_pos": 57, "type": "METRIC", "confidence": 0.843130961060524}]}, {"text": "Only the best-performing baseline for each expansion type is shown, for readability.", "labels": [], "entities": []}, {"text": "Both the LR and the LR+DNN configurations outperform the strongest baseline by a large margin.", "labels": [], "entities": []}, {"text": "This result illustrates the importance of supervised learning for this task.", "labels": [], "entities": []}, {"text": "For consistent expansions, LR+DNN is clearly the best-preforming configuration.", "labels": [], "entities": [{"text": "LR+DNN", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.7301824490229288}]}], "tableCaptions": [{"text": " Table 2: Statistics on the DTE (Debate Topic Expan- sion) dataset", "labels": [], "entities": [{"text": "DTE (Debate Topic Expan- sion) dataset", "start_pos": 28, "end_pos": 66, "type": "DATASET", "confidence": 0.8318491246965196}]}, {"text": " Table 3: LR+DNN -Recall@1 for selected Preci- sion@1 values.", "labels": [], "entities": [{"text": "LR+DNN -Recall@1", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.7993970768792289}]}, {"text": " Table 4: LR+DNN -Coverage for selected Preci- sion@1 values.", "labels": [], "entities": [{"text": "LR+DNN -Coverage", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8389467358589172}]}]}