{"title": [{"text": "Monotonic Infinite Lookback Attention for Simultaneous Machine Translation", "labels": [], "entities": [{"text": "Simultaneous Machine Translation", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8495679299036661}]}], "abstractContent": [{"text": "Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.751657634973526}]}, {"text": "Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency.", "labels": [], "entities": []}, {"text": "We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends overall source tokens read thus far.", "labels": [], "entities": []}, {"text": "We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source.", "labels": [], "entities": []}, {"text": "We show that MILk's adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.", "labels": [], "entities": []}], "introductionContent": [{"text": "Simultaneous machine translation (MT) addresses the problem of how to begin translating a source sentence before the source speaker has finished speaking.", "labels": [], "entities": [{"text": "Simultaneous machine translation (MT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8257064819335938}]}, {"text": "This capability is crucial for live or streaming translation scenarios, such as speech-tospeech translation, where waiting for one speaker to complete their sentence before beginning the translation would introduce an intolerable delay.", "labels": [], "entities": [{"text": "speech-tospeech translation", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.7094384431838989}]}, {"text": "In these scenarios, the MT engine must balance latency against quality: if it acts before the necessary source content arrives, translation quality degrades; but waiting for too much source content can introduce unnecessary delays.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9803345203399658}]}, {"text": "We refer to the strategy an MT engine uses to balance reading source tokens against writing target tokens as its schedule.", "labels": [], "entities": [{"text": "MT engine", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.9153641164302826}]}, {"text": "Recent work in simultaneous machine translation tends to fall into one of two bins: \u2022 The schedule is learned and/or adaptive to the current context, but assumes a fixed MT system trained on complete source sentences, as typified by wait-if-* ( and reinforcement learning approaches).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6846816539764404}]}, {"text": "\u2022 The schedule is simple and fixed and can thus be easily integrated into MT training, as typified by wait-k approaches ().", "labels": [], "entities": [{"text": "MT training", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9227945804595947}]}, {"text": "A fixed schedule may introduce too much delay for some sentences, and not enough for others.", "labels": [], "entities": []}, {"text": "Meanwhile, a fixed MT system that was trained to expect complete sentences may impose a low ceiling on any adaptive schedule that uses it.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9486116766929626}]}, {"text": "Therefore, we propose to train an adaptive schedule jointly with the underlying neural machine translation (NMT) system.", "labels": [], "entities": []}, {"text": "Monotonic attention mechanisms ( are designed for integrated training in streaming scenarios and provide our starting point.", "labels": [], "entities": []}, {"text": "They encourage streaming by confining the scope of attention to the most recently read tokens.", "labels": [], "entities": [{"text": "streaming", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.9633992314338684}]}, {"text": "This restriction, however, may hamper long-distance reorderings that can occur in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9555084109306335}]}, {"text": "We develop an approach that removes this limitation while preserving the ability to stream.", "labels": [], "entities": []}, {"text": "We use their hard, monotonic attention head to determine how much of the source sentence is available.", "labels": [], "entities": []}, {"text": "Before writing each target token, our learned model advances this head zero or more times based on the current context, with each advancement revealing an additional token of the source sentence.", "labels": [], "entities": []}, {"text": "A secondary, soft attention head can then attend to any source words at or before that point, resulting in Monotonic Infinite Lookback (MILk) attention.", "labels": [], "entities": [{"text": "Monotonic Infinite Lookback (MILk) attention", "start_pos": 107, "end_pos": 151, "type": "METRIC", "confidence": 0.7428070945399148}]}, {"text": "This, however, removes the memory constraint that was encouraging the model to stream.", "labels": [], "entities": []}, {"text": "To restore streaming behaviour, we propose to jointly minimize a latency loss.", "labels": [], "entities": []}, {"text": "The entire system can efficiently be trained in expectation, as a drop-in replacement for the familiar soft attention.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We present MILk attention, which allows us to build the first simultaneous MT system to learn an adaptive schedule jointly with an NMT model that attends overall source tokens read thus far.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9760223031044006}]}, {"text": "2. We extend the recently-proposed Average Lagging latency metric (, making it differentiable and calculable in expectation, which allows it to be used as a training objective.", "labels": [], "entities": [{"text": "Average Lagging latency metric", "start_pos": 35, "end_pos": 65, "type": "METRIC", "confidence": 0.9394423216581345}]}, {"text": "3. We demonstrate favorable trade-offs to those of wait-k strategies at many latency values, and provide evidence that MILk's advantage extends from its ability to adapt based on source content.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run our experiments on the standard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) tasks.", "labels": [], "entities": [{"text": "WMT14 English-to-French (EnFr; 36.3M sentences", "start_pos": 39, "end_pos": 85, "type": "DATASET", "confidence": 0.8256013223103115}, {"text": "WMT15 German-to-English (DeEn; 4.5M sentences) tasks", "start_pos": 91, "end_pos": 143, "type": "DATASET", "confidence": 0.8841558032565646}]}, {"text": "For EnFr we use a combination of newstest 2012 and newstest 2013 for development and report results on newstest 2014.", "labels": [], "entities": [{"text": "EnFr", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9238436818122864}]}, {"text": "For DeEn we validate on newstest 2013 and then report results on newstest 2015.", "labels": [], "entities": [{"text": "DeEn", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9093353748321533}, {"text": "newstest 2013", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9312478005886078}]}, {"text": "Translation quality is measured using detokenized, cased BLEU ().", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9418610334396362}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9900124073028564}]}, {"text": "For each data set, we use BPE (Sennrich et al., 2016) on the training data to construct a 32,000-type vocabulary that is shared between the source and target languages.", "labels": [], "entities": [{"text": "BPE", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9665388464927673}]}], "tableCaptions": [{"text": " Table 3: Varying MILk's \u03bb with and without mass  preservation on the DeEn development set.", "labels": [], "entities": [{"text": "DeEn development set", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.9726386070251465}]}, {"text": " Table 4: Varying MILk's discreteness parameter n with  \u03bb fixed at 0.2 on the DeEn development set.", "labels": [], "entities": [{"text": "DeEn development set", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.9702157775561014}]}]}