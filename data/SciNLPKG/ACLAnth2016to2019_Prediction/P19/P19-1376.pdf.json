{"title": [{"text": "Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection", "labels": [], "entities": []}], "abstractContent": [{"text": "The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science.", "labels": [], "entities": []}, {"text": "Neural network models were proposed early on, but were shown to have clear flaws.", "labels": [], "entities": []}, {"text": "Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws.", "labels": [], "entities": []}, {"text": "They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task.", "labels": [], "entities": []}, {"text": "Here, we look more closely at the behaviour of their model in this task.", "labels": [], "entities": []}, {"text": "We find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong-worse than an older rule-based model.", "labels": [], "entities": []}, {"text": "These findings holdup through several alternative training regimes and evaluation measures.", "labels": [], "entities": []}, {"text": "Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets area good cognitive model for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "For over 30 years, the English past tense has served as both inspiration and testbed for models of language acquisition and processing).", "labels": [], "entities": [{"text": "language acquisition and processing", "start_pos": 99, "end_pos": 134, "type": "TASK", "confidence": 0.7793489396572113}]}, {"text": "One of the most wellknown debates centres on whether the apparently rule-governed regular past tense is indeed represented cognitively using explicit rules.", "labels": [], "entities": []}, {"text": "famously argued against this hypothesis, presenting a neural network model intended to capture both regular and irregular verbs with no explicit rules.", "labels": [], "entities": []}, {"text": "However, presented a scathing rebuttal, pointing out both theoretical and empirical failures of the model.", "labels": [], "entities": []}, {"text": "In their alternative (dual-route) view, the regular past tense is categorical and captured via explicit rules, while irregular past tenses are memorized and can (occasionally) generalize via gradient analogical processes.", "labels": [], "entities": []}, {"text": "Their arguments were so influential that although neural networks gained considerable traction in cognitive science more generally), many linguists dismissed the whole approach.", "labels": [], "entities": []}, {"text": "With the recent success of deep learning in NLP, however, there has been renewed interest in exploring the extent to which neural networks capture human behaviour in psycholinguistic tasks (e.g.,.", "labels": [], "entities": []}, {"text": "In particular, revisited the past tense debate and showed that modern sequence-based encoder-decoder (ED) models overcome many of the criticisms levelled at Rumelhart and McClelland's model.", "labels": [], "entities": []}, {"text": "Specifically, these models permit variable-length input and output that represent sequential ordering; can reach near-perfect accuracy on both regular and irregular verbs seen in training; and (using multi-task learning) can effectively generalize phonological rules across different inflections.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9974179267883301}]}, {"text": "These primary claims are undoubtedly correct (and indeed, we replicate the accuracy results below).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9995306730270386}]}, {"text": "However, we take issue with another part of K&C's work, in which they claim that their ED model also effectively models human behaviour in a nonce-word experiment (i.e., wug test, described below).", "labels": [], "entities": []}, {"text": "We explore the model's behaviour on this task in detail, and conclude that its ability to model humans is considerably weaker than K&C suggest.", "labels": [], "entities": []}, {"text": "In particular, we begin by showing that multiple simulations of the same model (with different random initializations) result in very different correlations with the human data.", "labels": [], "entities": []}, {"text": "To ensure that this instability is not just due to the evaluation measure, we introduce an alternative measure, but still find unstable results.", "labels": [], "entities": []}, {"text": "We then consider whether treating individual simulations as individual participants (rather than as a model of the average participant) captures the human data better.", "labels": [], "entities": []}, {"text": "This aggregate model does show some high-level similarities to the human participants: both model and humans tend to produce irregulars more frequently for nonce words that are similar to many real irregular verbs.", "labels": [], "entities": []}, {"text": "However, the model is still poor at capturing fine-grained distinctions at the level of individual verbs.", "labels": [], "entities": []}, {"text": "We conclude that, although deep learning approaches overcome many of the problems of earlier neural network models, there is still insufficient evidence to claim that they are good models of human morphological processing.", "labels": [], "entities": []}], "datasetContent": [{"text": "Like K&C, we use data from two experiments run by.", "labels": [], "entities": []}, {"text": "In Experiment 1, using a dialogue-based prompt, A&H presented participants auditorily with nonce \"verbs\" that are phonotactically legal in English (e.g., spling, dize), and prompted participants to produce past tense forms of these verbs, resulting in a data set of production probabilities of various past tense forms.", "labels": [], "entities": []}, {"text": "In Experiment 2, participants first produced each past tense form (as in Experiment 1) and were then asked to rate the acceptability of either two or three possible past tense forms for that verb-one regular, and one or two potential irregulars.", "labels": [], "entities": []}, {"text": "For example, for scride /skr\"aId/, participants rated scrided /skr\"aId@d/ (regular), scrode /skr\"oUd/ and scrid /skr\"Id/ (irregular).", "labels": [], "entities": []}, {"text": "This gives a data set of past tense form ratings.", "labels": [], "entities": []}, {"text": "Most of A&H's own analyses rely on the ratings data, but the ED model is a model of production, so we follow K&C and use the data from Experiment 1.", "labels": [], "entities": [{"text": "A&H", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.9046222964922587}, {"text": "K&C", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.7952412168184916}]}, {"text": "The data is coded using the same set of suggested forms that were rated in Experiment 2: for each nonce word, A&H counted how many participants produced the regular form, the irregular form (or each of the two forms, if there are two), and \"other\" (any other past tense form that was not among those rated in Experiment 2).", "labels": [], "entities": []}, {"text": "The counts are normalized to compute production probabilities for each output form.", "labels": [], "entities": []}, {"text": "The nonce words used by A&H were carefully chosen according to several criteria.", "labels": [], "entities": [{"text": "A&H", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.9336494008700053}]}, {"text": "First, they are phonologically \"bland\": i.e., not unusual-sounding as English words (as confirmed by a pre-test with participants).", "labels": [], "entities": []}, {"text": "Second, as explained in the following section, they fall into several categories designed to test A&H's hypothesis that (contra, both regular and irregular past tense forms exhibit gradient (and not categorical) effects.", "labels": [], "entities": [{"text": "A&H", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8158109982808431}]}, {"text": "We report three different evaluation measures.", "labels": [], "entities": []}, {"text": "First, we compute training set accuracy: the percentage of verbs in the training data for which the model's top-ranked output is the correct past tense form.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9561065435409546}]}, {"text": "This is largely a sanity check and test of convergence: a fully-trained model of adult performance should have near-perfect training set accuracy.", "labels": [], "entities": [{"text": "convergence", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9195294380187988}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9049323797225952}]}, {"text": "Next, as described in Section 2.3, we report Spearman's rank correlation (\u03c1) of the model's probabilities for the various nonce past tense forms with the human production probabilities.", "labels": [], "entities": [{"text": "Spearman's rank correlation (\u03c1)", "start_pos": 45, "end_pos": 76, "type": "METRIC", "confidence": 0.801764828818185}]}, {"text": "The probability for each suggested past tense form was obtained by forcing the model to output that form (e.g., providing scride as input and forcing it to output scrid).", "labels": [], "entities": []}, {"text": "This made it possible to get probabilities for forms that did not occur in the beam (the list of most likely forms output by the model).", "labels": [], "entities": []}, {"text": "Finally, we introduce a third measure, motivated further in Section 4.1, complete recall@5: where n is the total number of nonce verbs, S i: Mean training set accuracy (in %, with standard deviations in brackets), averaged over 10 runs for each training set with different random seeds.", "labels": [], "entities": [{"text": "complete", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9438713788986206}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.7810215353965759}, {"text": "Mean training set accuracy", "start_pos": 141, "end_pos": 167, "type": "METRIC", "confidence": 0.7746983766555786}]}, {"text": "Oracle accuracy is 99.85% on the K&C data and 99.55% on the A&H data, due to homophones and forms with multiple past tenses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.984167754650116}, {"text": "K&C data", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9176710844039917}, {"text": "A&H data", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9674594700336456}]}, {"text": "In order to do better on irregulars, the model would have to get more of the regulars wrong. is the set of A&H's suggested past tense forms for verb i, Bi is the set of the top five verbs in the model's beam for i, and [S i \u2286 Bi ] = 1 if all verbs from Si appear in Bi , and 0 otherwise.", "labels": [], "entities": [{"text": "A&H", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.911276638507843}]}, {"text": "For example, a model which only processed the two verbs in would have a CR@5 of 0.5, since the beam includes all suggested past tenses for murn (murned, murnt), but not for nold (nolded, nold, neld).", "labels": [], "entities": [{"text": "CR@5", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9695252180099487}]}, {"text": "Our first experiment aims to replicate K&C's results showing that (a) the model is able to produce the past tense forms of training verbs with nearperfect accuracy, and (b) its correlation with human data on the nonce verb test set is higher than that of A&H's model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9834585189819336}]}, {"text": "In K&C's paper, these results were based on a single trained model.", "labels": [], "entities": []}, {"text": "Here we trained 20 models (10 on each training set) initialized with different random seeds.", "labels": [], "entities": []}, {"text": "Accuracy lists the mean and standard deviation of training set accuracy for each of the two training sets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9849928617477417}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9579752683639526}]}, {"text": "It is not possible to get 100% accuracy because the training sets contain some homophones with different past tenses (e.g., write-wrote and right-righted), and some verbs which have two possible past tenses (e.g., spring-sprung and springsprang).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9993608593940735}, {"text": "springsprang", "start_pos": 232, "end_pos": 244, "type": "METRIC", "confidence": 0.8526052832603455}]}, {"text": "Nevertheless, the models get very close to the best possible accuracy, confirming K&C's finding that they learn both regular and irregular past tenses of previously seen    Correlation Despite having consistently high accuracy on real words, shows that models with different random initializations vary considerably in their correlation with human speakers' production probabilities on nonce words, from 0.15 to 0.56 for regulars, and from 0.23 to 0.41 for irregulars.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9978554844856262}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9851121306419373}]}, {"text": "K&C's reported results are at the high end of what we obtained, suggesting that they are likely not representative.", "labels": [], "entities": [{"text": "K&C's reported", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8902834057807922}]}, {"text": "On the other hand, we were concerned that the variability in the correlation measure might be due to an artefact: the vast majority of the beams returned by the model assign very high probability (> 98%) to the top item and little mass to anything else (as in the first example in).", "labels": [], "entities": []}, {"text": "Since the The skewedness of the beams is likely because of the training/testing scenario, where the model is effectively asked to do different tasks: at training time, it is trained to produce one correct past tense, while attest time, it's expected to produce a probability distribution over potential nonce past tenses.", "labels": [], "entities": []}, {"text": "We could surely produce better matches to the human probability distributions by training directly to do so, but that wouldn't CR@5 and second best forms The above observation motivated the CR@5 measure (Section 3.3).", "labels": [], "entities": [{"text": "CR@5 measure", "start_pos": 190, "end_pos": 202, "type": "METRIC", "confidence": 0.8650009036064148}]}, {"text": "Rather than measuring the relative probabilities of past forms across different verbs, CR@5 considers the relative rankings of different past forms for each verb.", "labels": [], "entities": []}, {"text": "However, CR@5 also yielded unstable results: 39-47% on A&H's data, and 29-44% on K&C's data, as shown in.", "labels": [], "entities": [{"text": "CR@5", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8311747709910074}, {"text": "A&H's data", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9631776452064514}, {"text": "K&C's data", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.9263014793395996}]}, {"text": "As a final exploration of the models' instability across different simulations, we looked at how often the models agree with each other on the verb occupying the first and the second position in the beam.", "labels": [], "entities": []}, {"text": "While there is very high agreement on the most likely form (top of the beam) across the simulations-usually a regular past tense-very few forms in the second position are the same across simulations (see.", "labels": [], "entities": []}, {"text": "make sense as a cognitive model, since human learners are exposed only to correct past tenses, not to distributions.", "labels": [], "entities": []}, {"text": "Summary To recap, we find similar training set accuracy to what K&C reported, but the correlation scores between the model and the human data are generally lower, and the model exhibits unstable behaviour across different simulations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9054410457611084}, {"text": "K&C reported", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.8148303329944611}]}, {"text": "However, the unstable behaviour can potentially be accounted for, if each simulation is interpreted as an individual participant rather than as a model of the average behaviour of all participants.", "labels": [], "entities": []}, {"text": "In that case, we should aggregate results from multiple simulations in order to compare them to the human results, since production probabilities from A&H's experiment were obtained by aggregating data over multiple participants.", "labels": [], "entities": [{"text": "A&H's experiment", "start_pos": 151, "end_pos": 167, "type": "DATASET", "confidence": 0.8599640011787415}]}, {"text": "The next experiment examines this alternative interpretation.", "labels": [], "entities": []}, {"text": "To simulate A&H's production experiment with each simulation as one participant, we trained 50 individual models on the A&H training data 5 using the same architecture and hyperparameters as before.", "labels": [], "entities": [{"text": "A&H", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.9064586361249288}, {"text": "A&H training data 5", "start_pos": 120, "end_pos": 139, "type": "DATASET", "confidence": 0.9460066457589468}]}, {"text": "We then sampled 100 past tense forms for each verb from each model's output probability distribution.", "labels": [], "entities": []}, {"text": "Each of the 5000 output forms (100 each from 50 simulated participants) was categorized either as (a) the verb's regular past tense form, (b-c) the first or second irregular past tense form suggested by A&H, or (d) any other possible form.", "labels": [], "entities": [{"text": "A&H", "start_pos": 203, "end_pos": 206, "type": "DATASET", "confidence": 0.9220550258954366}]}, {"text": "For the aggregate model, the correlation measure is the only evaluation that makes sense.", "labels": [], "entities": [{"text": "correlation measure", "start_pos": 29, "end_pos": 48, "type": "METRIC", "confidence": 0.972463458776474}]}, {"text": "For regulars, correlation with the human production proba-bilities was higher than in the previous experiment (0.45 vs. an average of 0.28 in Experiment 1), but for irregulars it was lower (0.19 vs. 0.22 in Experiment 1).", "labels": [], "entities": []}, {"text": "The differences between the humans and aggregate model are clear from, which shows the distribution of various past tense forms for both model and humans.", "labels": [], "entities": []}, {"text": "For example, in only one case did the humans produce an irregular more frequently than the regular (no-change past chind for present chind), whereas there are several cases where the aggregated model does so.", "labels": [], "entities": [{"text": "irregular", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9705586433410645}]}, {"text": "Moreover, for the word chind itself, the model prefers chound rather than chind.", "labels": [], "entities": []}, {"text": "In the previous experiment, we saw that individual models often rank implausible past tenses higher than plausible ones.", "labels": [], "entities": []}, {"text": "However, we see here that on aggregate nearly all the model's proposed past tenses are those suggested by A&H.", "labels": [], "entities": [{"text": "A&H", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.9209351340929667}]}, {"text": "Apparently, the unstable beam rankings washout the implausible forms, i.e., the plausible forms on average occur nearer the top of the beam than any particular implausible form.", "labels": [], "entities": []}, {"text": "In fact, the model actually produces fewer \"other\" forms than the humans.", "labels": [], "entities": []}, {"text": "We also looked at the model's average production of regular and suggested irregular forms for each of the six categories in.", "labels": [], "entities": []}, {"text": "The results, shown in, indicate that the model does capture the main trends seen in humans across these categories, but overall it is more likely to produce irregular forms.", "labels": [], "entities": []}, {"text": "Together with the low overall correlation to human results and obvious differences at the fine-grained level, these results suggest that there are serious weaknesses in the ED model, even when results are aggregated across simulations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Correlations (using Spearman's \u03c1 and Pear- son's r) between the models' output probabilities vs.  human production probabilities and rating data. The  data for the individual model is an average over 10 sim- ulations (standard deviation shown in brackets). High- est correlation in each line is shown in bold.", "labels": [], "entities": [{"text": "High- est correlation", "start_pos": 267, "end_pos": 288, "type": "METRIC", "confidence": 0.6977980509400368}]}]}