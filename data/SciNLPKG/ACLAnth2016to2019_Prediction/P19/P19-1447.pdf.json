{"title": [], "abstractContent": [{"text": "Semantic parsing considers the task of trans-ducing natural language (NL) utterances into machine executable meaning representations (MRs).", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.841097354888916}, {"text": "trans-ducing natural language (NL) utterances into machine executable meaning representations (MRs)", "start_pos": 39, "end_pos": 138, "type": "TASK", "confidence": 0.7633132557074229}]}, {"text": "While neural network-based semantic parsers have achieved impressive improvements over previous methods, results are still far from perfect, and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated MRs.", "labels": [], "entities": [{"text": "neural network-based semantic parsers", "start_pos": 6, "end_pos": 43, "type": "TASK", "confidence": 0.7262573838233948}]}, {"text": "This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using features that are designed to fix observed problems with baseline models.", "labels": [], "entities": []}, {"text": "We implement our reranker in a competitive neu-ral semantic parser and test on four semantic parsing (GEO, ATIS) and Python code generation (DJANGO, CONALA) tasks, improving the strong baseline parser by up to 5.7% absolute in BLEU (CONALA) and 2.9% inaccuracy (DJANGO), outperforming the best published neural parser results on all four datasets.", "labels": [], "entities": [{"text": "GEO", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.7198086977005005}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9975851774215698}]}], "introductionContent": [{"text": "Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code).", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8233543634414673}]}, {"text": "Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models, to more complex parsing paradigms guided by the structured topologies of target meaning representations (;;;;;;; , inter alia).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7125400453805923}]}, {"text": "Figure 1: Illustration of the reranker with areal example from the CONALA code generation task () with reconstruction (z \u2192 x) and discriminative matching (x \u2194 z) scores.", "labels": [], "entities": [{"text": "CONALA code generation task", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.7702307105064392}]}, {"text": "While neural network-based semantic parsers have achieved impressive results, there is still room for improvement.", "labels": [], "entities": [{"text": "neural network-based semantic parsers", "start_pos": 6, "end_pos": 43, "type": "TASK", "confidence": 0.714638240635395}]}, {"text": "A pilot analysis of incorrect predictions from a competitive neural semantic parser, TRANX indicates an obvious issue of incoherence.", "labels": [], "entities": [{"text": "TRANX", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.5812708735466003}]}, {"text": "In the real example in, top prediction z 1 is semantically incoherent with the intent expressed in the utterance.", "labels": [], "entities": []}, {"text": "Perhaps a more interesting issue is inadequacy -while the predicted MRs match the overall intent of the utterance, they still miss or misinterpret crucial pieces of information (e.g., missing or generating wrong arguments, as in z 2 and z 9 ).", "labels": [], "entities": [{"text": "MRs", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9278385639190674}]}, {"text": "Indeed, we observe that around 41% of the failure cases of TRANX on a popular Python code generation task (DJANGO,) are due to such inadequate predictions.", "labels": [], "entities": [{"text": "Python code generation task (DJANGO", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.6387777427832285}]}, {"text": "Although the top predictions from a semantic parser could fall short inadequacy or coherence, we found the parser still maintains high recall, covering the gold-standard MR in its n-best list of predictions most of the time . This naturally mo-tivates us to investigate whether the performance of an existing neural parser can be potentially improved by reranking the n-best list of candidate MRs.", "labels": [], "entities": [{"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.997751772403717}]}, {"text": "In this paper, we propose a simple reranker powered mainly by two quality-measuring features of a candidate MR: (1) a generative reconstruction model, which tests the coherence and adequacy of an MR via the likelihood of reconstructing the original input utterance from the MR; and (2) a discriminative matching model, which directly captures the semantic coherence between utterances and MRs.", "labels": [], "entities": [{"text": "generative reconstruction", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.9381251633167267}]}, {"text": "We implement our reranker in a strong neural semantic parser and evaluate on both tasks of parsing NL to domain-specific logical form (GEO, ATIS) and general-purpose source code (DJANGO, CONALA).", "labels": [], "entities": [{"text": "GEO", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.8579688668251038}, {"text": "ATIS", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.6208092570304871}]}, {"text": "Our reranking approach improves upon this strong parser by up to 5.7% absolute in BLEU (CONALA) and 2.9% inaccuracy (DJANGO), outperforming the best published neural parser results on all datasets.", "labels": [], "entities": [{"text": "absolute", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9883910417556763}, {"text": "BLEU (CONALA)", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.8253147155046463}, {"text": "inaccuracy (DJANGO)", "start_pos": 105, "end_pos": 124, "type": "METRIC", "confidence": 0.8651765435934067}]}], "datasetContent": [{"text": "We test on four semantic parsing and code generation benchmarks: GEO (Zelle and Mooney, 1996) and ATIS (Deborah A. Dahl and Shriber) are two closed-domain semantic parsing datasets.", "labels": [], "entities": [{"text": "semantic parsing and code generation", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.7493946313858032}, {"text": "GEO", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8734714984893799}, {"text": "ATIS", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.6949043869972229}, {"text": "semantic parsing", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.7549335360527039}]}, {"text": "The NL utterances are geographical (GEO) and flight booking (ATIS) inquiries (e.g., What is the latest flight to Boston?).", "labels": [], "entities": [{"text": "NL utterances", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7927623391151428}, {"text": "flight booking (ATIS)", "start_pos": 45, "end_pos": 66, "type": "METRIC", "confidence": 0.7737969279289245}]}, {"text": "The corresponding MRs are defined in \u03bb-calculus logical forms (e.g., argmax x (and (flight x) (to x boston)) (departure time x))).", "labels": [], "entities": [{"text": "argmax", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9903490543365479}]}, {"text": "DJANGO () is a popular Python code generation dataset consisting of NL-annotated code from the Django framework.", "labels": [], "entities": []}, {"text": "Around 70% of examples are simple cases of variable assignment (e.g., result = []), function definition/invocation or condition tests, which can be easily inferred from the verbose NL utterances (e.g., Result is an empty list).", "labels": [], "entities": [{"text": "function definition/invocation", "start_pos": 84, "end_pos": 114, "type": "TASK", "confidence": 0.783304937183857}]}], "tableCaptions": [{"text": " Table 1: Mean and standard deviation 4 over five random runs. recon., match., t.c. stand for reconstruction, matching, and token", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9959489107131958}, {"text": "match.", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9646316766738892}]}]}