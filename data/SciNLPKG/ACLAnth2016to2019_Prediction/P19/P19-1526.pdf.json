{"title": [{"text": "Enhancing Unsupervised Generative Dependency Parser with Contextual Information", "labels": [], "entities": [{"text": "Enhancing Unsupervised Generative Dependency Parser", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.58609818816185}]}], "abstractContent": [{"text": "Most of the unsupervised dependency parsers are based on probabilistic generative models that learn the joint distribution of the given sentence and its parse.", "labels": [], "entities": []}, {"text": "Probabilistic gener-ative models usually explicit decompose the desired dependency tree into factorized grammar rules, which lack the global features of the entire sentence.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel probabilistic model called dis-criminative neural dependency model with va-lence (D-NDMV) that generates a sentence and its parse from a continuous latent representation , which encodes global contextual information of the generated sentence.", "labels": [], "entities": []}, {"text": "We propose two approaches to model the latent representation: the first deterministically summarizes the representation from the sentence and the second probabilistically models the representation conditioned on the sentence.", "labels": [], "entities": []}, {"text": "Our approach can be regarded as anew type of autoencoder model to unsupervised dependency parsing that combines the benefits of both generative and discriminative techniques.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.6897814273834229}]}, {"text": "In particular, our approach breaks the context-free independence assumption in previous generative approaches and therefore becomes more expressive.", "labels": [], "entities": []}, {"text": "Our extensive experimental results on seventeen datasets from various sources show that our approach achieves competitive accuracy compared with both gen-erative and discriminative state-of-the-art un-supervised dependency parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9964904189109802}]}], "introductionContent": [{"text": "Dependency parsing is a very important task in natural language processing.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8914079070091248}, {"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.63684610525767}]}, {"text": "The dependency relations identified by dependency parsing convey syntactic information useful in subsequent applications such as semantic parsing, information extraction, and question answering.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6983259618282318}, {"text": "semantic parsing", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.7431436479091644}, {"text": "information extraction", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.8380976617336273}, {"text": "question answering", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.90841343998909}]}, {"text": "In this paper, we * Corresponding author focus on unsupervised dependency parsing, which aims to induce a dependency parser from training sentences without gold parse annotation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.66850446164608}]}, {"text": "Most previous approaches to unsupervised dependency parsing are based on probabilistic generative models, for example, the Dependency Model with Valence (DMV) () and its extensions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6828270554542542}]}, {"text": "A disadvantage of such approaches comes from the context-freeness of dependency grammars, a strong independence assumption that limits the information available in determining how likely a dependency is between two words in a sentence.", "labels": [], "entities": []}, {"text": "In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token.", "labels": [], "entities": []}, {"text": "Additional information used for computing dependency probabilities in later work is also limited to local morpho-syntactic features such as word forms, lemmas and categories), which does not break the context-free assumption.", "labels": [], "entities": []}, {"text": "More recently, researchers have started to utilize discriminative methods in unsupervised dependency parsing based on the idea of discriminative clustering (, the CRFAE framework or the neural variational transition-based parser (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.716293603181839}]}, {"text": "By conditioning dependency prediction on the whole input sentence, discriminative methods are capable of utilizing not only local information, but also global and contextual information of a dependency in determining its strength.", "labels": [], "entities": [{"text": "dependency prediction", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7167946100234985}]}, {"text": "Specifically, both and include in the feature set of a dependency the information of the tokens around the head or child token of the dependency.", "labels": [], "entities": []}, {"text": "In this way, they break the context-free independence assumption because the same dependency would have different strength in different contexts.", "labels": [], "entities": []}, {"text": "Besides, propose a variational autoencoder approach based on Recurrent Neural Network In this paper, we propose a novel approach to unsupervised dependency parsing in the middle between generative and discriminative approaches.", "labels": [], "entities": [{"text": "unsupervised dependency parsing", "start_pos": 132, "end_pos": 163, "type": "TASK", "confidence": 0.6661653319994608}]}, {"text": "Our approach is based on neural DMV (, an extension of DMV that employs a neural network to predict dependency probabilities.", "labels": [], "entities": []}, {"text": "Unlike neural DMV, however, when computing the probability of a dependency, we rely on not only local information as in DMV, but also global and contextual information from a compressed representation of the input sentence produced by neural networks.", "labels": [], "entities": []}, {"text": "In other words, instead of modeling the joint probability of the input sentence and its dependency parse as in a generative model, we model the conditional probability of the sentence and parse given global information of the sentence.", "labels": [], "entities": []}, {"text": "Therefore, our approach breaks the context-free assumption in a similar way to discriminative approaches, while it is still able to utilize many previous techniques (e.g., initialization and regularization techniques) of generative approaches.", "labels": [], "entities": []}, {"text": "Our approach can be seen as an autoencoder.", "labels": [], "entities": []}, {"text": "The decoder is a conditional generative neural DMV that generates the sentence as well as its parse from a continuous representation that captures the global features of the sentence.", "labels": [], "entities": []}, {"text": "To model such global information, we propose two types of encoders, one deterministically summarizes the sentence with a continuous vector while the other probabilistically models the continuous vector conditioned on the sentence.", "labels": [], "entities": []}, {"text": "Since the neural DMV can act as a fully-fledged unsupervised dependency parser, the encoder can be seen as a supplementary module that injects contextual information into the neural DMV for contextspecific prediction of dependency probabilities.", "labels": [], "entities": []}, {"text": "This is very different from the previous unsupervised parsing approach based on the autoencoder framework, in which the encoder is a discriminative parser and the decoder is a generative model, both of which are required for performing unsupervised parsing.", "labels": [], "entities": []}, {"text": "Our experiments verify that our approach achieves a comparable result with recent state-ofthe-art approaches on extensive datasets from various sources.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our methods on seventeen treebanks from various sources.", "labels": [], "entities": []}, {"text": "For each dataset, we compared with current state-of-the-art approaches on the specific dataset.", "labels": [], "entities": []}, {"text": "English Penn Treebank We conducted experiments on the Wall Street Journal corpus (WSJ) with section 2-21 for training, section 22 for validation and section 23 for testing.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.9059932430585226}, {"text": "Wall Street Journal corpus (WSJ)", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.9695377690451485}]}, {"text": "We trained our model with training sentences of length \u2264 10, tuned the hyer-parameters on validation sentences of length \u2264 10 the and evaluated on testing sentences of length \u2264 10 (WSJ10) and all sentences (WSJ).", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 181, "end_pos": 186, "type": "DATASET", "confidence": 0.7028595805168152}, {"text": "WSJ", "start_pos": 207, "end_pos": 210, "type": "DATASET", "confidence": 0.8165255784988403}]}, {"text": "We reported the directed dependency accuracy (DDA) of the learned grammars on the test sentences.", "labels": [], "entities": [{"text": "directed dependency accuracy (DDA)", "start_pos": 16, "end_pos": 50, "type": "METRIC", "confidence": 0.6903584649165472}]}, {"text": "Datasets from PASCAL Challenge on Grammar Induction We conducted experiments on corpora of eight languages from the PASCAL Challenge on Grammar Induction (.", "labels": [], "entities": [{"text": "PASCAL Challenge on Grammar Induction", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.7789702773094177}, {"text": "PASCAL Challenge on Grammar Induction", "start_pos": 116, "end_pos": 153, "type": "TASK", "confidence": 0.7631986260414123}]}, {"text": "We trained our model with training sentences of length \u2264 10 and evaluated on testing sentences of length \u2264 10 and all sentences.", "labels": [], "entities": []}, {"text": "We also perform experiments on the datasets from PASCAL Challenge (), which contains eight languages: Arabic, Basque, Czech, Danish, Dutch, Portuguese, Slovene and Swedish.", "labels": [], "entities": [{"text": "PASCAL Challenge", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.7520518600940704}]}, {"text": "We compare our approaches with NDMV (Jiang et al., 2016), Convex-MST () and CRFAE.", "labels": [], "entities": [{"text": "CRFAE", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.8056663870811462}]}, {"text": "NDMV and CRFAE are two state-of-the-art approaches on the PASCAL Challenge datasets.", "labels": [], "entities": [{"text": "NDMV", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8621457815170288}, {"text": "PASCAL Challenge datasets", "start_pos": 58, "end_pos": 83, "type": "DATASET", "confidence": 0.9139137069384257}]}, {"text": "We show the directed dependency accuracy on the testing sentences no longer than 10 and on all the testing sentences (   art approaches including those utilizing the universal linguistic prior.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9545668959617615}]}], "tableCaptions": [{"text": " Table 1: Comparison on WSJ.  * : approaches with lex- icalized information.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.8749978542327881}]}, {"text": " Table 3: DDA on testing sentences no longer than 10 on eight additional languages from PASCAL Challenge.", "labels": [], "entities": [{"text": "DDA", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7321075201034546}, {"text": "PASCAL Challenge", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.8447369635105133}]}, {"text": " Table 4: DDA on all the testing sentences on eight additional languages from PASCAL Challenge.", "labels": [], "entities": [{"text": "DDA", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5839083790779114}, {"text": "PASCAL Challenge", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.8493146300315857}]}, {"text": " Table 5: Comparison of the average probabilities in D- NDMV and E-DMV when the rule is used and not used  in the gold parse.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of different sentence encoders in  D-NDMV.", "labels": [], "entities": []}, {"text": " Table 7: Sentences closest to the two example sentences in terms of the L2 distance between their learned embed- dings. Both the word sequence and the POS tag sequence are shown for each sentence.", "labels": [], "entities": []}]}