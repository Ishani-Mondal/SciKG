{"title": [{"text": "Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation", "labels": [], "entities": [{"text": "Vision-and-Language Navigation", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.7442519068717957}]}], "abstractContent": [{"text": "Vision-and-Language Navigation (VLN) requires grounding instructions, such as turn right and stop at the door, to routes in a visual environment.", "labels": [], "entities": [{"text": "Vision-and-Language Navigation (VLN)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8082860052585602}]}, {"text": "The actual grounding can connect language to the environment through multiple modalities, e.g. stop at the door might ground into visual objects, while turn right might rely only on the geometric structure of a route.", "labels": [], "entities": []}, {"text": "We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models.", "labels": [], "entities": []}, {"text": "Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset.", "labels": [], "entities": []}, {"text": "To better use all the available modalities , we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Vision-and-Language Navigation (VLN) task requires an agent to navigate to a particular location in a real-world environment, following complex, context-dependent instructions written by humans (e.g. go down the second hallway on the left, enter the bedroom and stop by the mirror).", "labels": [], "entities": [{"text": "Vision-and-Language Navigation (VLN) task", "start_pos": 4, "end_pos": 45, "type": "TASK", "confidence": 0.8478261729081472}]}, {"text": "The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror).", "labels": [], "entities": []}, {"text": "Recent state-of-the-art models ( have demonstrated large gains inaccuracy on the VLN task.", "labels": [], "entities": []}, {"text": "However, it is unclear which modality these go past the couch \u2026: We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach.", "labels": [], "entities": [{"text": "object detections", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.7290384322404861}]}, {"text": "substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see, top vs. middle).", "labels": [], "entities": []}, {"text": "First, we analyze to what extent VLN models ground language into visual appearance and route structure by training versions of two state-ofthe-art models without visual features, using the benchmark Room-to-Room (R2R) dataset.", "labels": [], "entities": []}, {"text": "We find that while grounding into route structure is useful, the models with visual features fail to learn generalizable visual grounding.", "labels": [], "entities": []}, {"text": "Surprisingly, when trained without visual features, their performance on unseen environments is comparable or even better.", "labels": [], "entities": []}, {"text": "We hypothesize that the low-level, pixel-based CNN features in the visual models contribute to their failure to generalize.", "labels": [], "entities": []}, {"text": "To address this, we introduce a high-level object-based visual representation to ground language into visual context in a more generalizable way, using the symbolic output of a pretrained object detection system.", "labels": [], "entities": []}, {"text": "For Finally, inspired by the complementary errors of visual and non-visual agents, we decompose the grounding process through a mixture-of-experts approach.", "labels": [], "entities": []}, {"text": "We train separate visual and non-visual agents, encouraging each one to focus on a separate modality, and combine their predictions as an ensemble (see).", "labels": [], "entities": []}, {"text": "Our mixture-of-experts outperforms the individual agents, and is also better than the ensembles of multiple agents of the same modality (e.g. both visual or both non-visual).", "labels": [], "entities": []}, {"text": "Adding our object representation and mixtureof-experts approach to both state-of-the-art models improves their success rate by over 10% (absolute) in novel environments, obtaining a 51.9% success rate on the val-unseen split of the benchmark R2R dataset ().", "labels": [], "entities": [{"text": "R2R dataset", "start_pos": 242, "end_pos": 253, "type": "DATASET", "confidence": 0.8481177389621735}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Success rate (SR) of the vision-based full  agent (\"RN\", using ResNet) and the non-visual agent  (\"no vis.\", setting all visual features to zero) on the R2R  dataset under different model architectures (Speaker- Follower (SF) (Fried et al., 2018b) and Self-Monitoring  (SM) (Ma et al., 2019)) and training schemes.", "labels": [], "entities": [{"text": "Success rate (SR)", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9624357104301453}, {"text": "R2R  dataset", "start_pos": 163, "end_pos": 175, "type": "DATASET", "confidence": 0.7948426306247711}]}, {"text": " Table 2: Success rate (SR) of agents with different vi- sual inputs on the R2R dataset (\"RN\": ResNet CNN,  \"Obj\": objects, \"no vis.\": no visual representation).  Models: Speaker-Follower (SF) (", "labels": [], "entities": [{"text": "Success rate (SR)", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9623579382896423}, {"text": "R2R dataset", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.8599079549312592}]}]}