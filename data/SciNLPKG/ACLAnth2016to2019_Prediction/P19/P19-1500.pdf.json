{"title": [], "abstractContent": [{"text": "In this paper, we develop a neural summa-rization model which can effectively process multiple input documents and distill abstrac-tive summaries.", "labels": [], "entities": []}, {"text": "Our model augments a previously proposed Transformer architecture (Liu et al., 2018) with the ability to encode documents in a hierarchical manner.", "labels": [], "entities": []}, {"text": "We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence.", "labels": [], "entities": []}, {"text": "Our model learns latent dependencies among tex-tual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations.", "labels": [], "entities": []}, {"text": "Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.", "labels": [], "entities": [{"text": "WikiSum dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.9592399597167969}]}], "introductionContent": [{"text": "Automatic summarization has enjoyed renewed interest in recent years, thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7379574775695801}]}, {"text": "The availability of large-scale datasets) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents.", "labels": [], "entities": [{"text": "summarizing single documents", "start_pos": 155, "end_pos": 183, "type": "TASK", "confidence": 0.909319798151652}]}, {"text": "Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary (.", "labels": [], "entities": []}, {"text": "Multi-document summarization -the task of producing summaries from clusters of themati-cally related documents -has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7521340548992157}]}, {"text": "High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferences (DUC and TAC), but are relatively small (in the range of a few hundred examples) for training neural models.", "labels": [], "entities": [{"text": "Document Understanding and Text Analysis Conferences (DUC and TAC)", "start_pos": 163, "end_pos": 229, "type": "TASK", "confidence": 0.8212678242813457}]}, {"text": "In an attempt to drive research further,  tap into the potential of Wikipedia and propose a methodology for creating a large-scale dataset (WikiSum) for multidocument summarization with hundreds of thousands of instances.", "labels": [], "entities": []}, {"text": "Wikipedia articles, specifically lead sections, are viewed as summaries of various topics indicated by their title, e.g.,\"Florence\" or \"Natural Language Processing\".", "labels": [], "entities": []}, {"text": "Documents cited in the Wikipedia articles or web pages returned by Google (using the section titles as queries) are seen as the source cluster which the lead section purports to summarize.", "labels": [], "entities": []}, {"text": "Aside from the difficulties in obtaining training data, a major obstacle to the application of end-to-end models to multi-document summarization is the sheer size and number of source documents which can be very large.", "labels": [], "entities": []}, {"text": "As a result, it is practically infeasible (given memory limitations of current hardware) to train a model which encodes all of them into vectors and subsequently generates a summary from them.", "labels": [], "entities": []}, {"text": "propose a two-stage architecture, where an extractive model first selects a subset of salient passages, and subsequently an abstractive model generates the summary while conditioning on the extracted subset.", "labels": [], "entities": []}, {"text": "The selected passages are concatenated into a flat sequence and the, an architecture well-suited to language modeling overlong sequences, is used to decode the summary.", "labels": [], "entities": []}, {"text": "Although the model of  takes an important first step towards abstractive multidocument summarization, it still considers the multiple input documents as a concatenated flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents.", "labels": [], "entities": []}, {"text": "For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light).", "labels": [], "entities": []}, {"text": "The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 216, "end_pos": 244, "type": "TASK", "confidence": 0.6090913712978363}]}, {"text": "Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them.", "labels": [], "entities": []}, {"text": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries.", "labels": [], "entities": []}, {"text": "Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner.", "labels": [], "entities": []}, {"text": "We represent cross-document relationships via an attention mechanism which allows to share information across multiple documents as opposed to simply concatenating text spans and feeding them as a flat sequence to the model.", "labels": [], "entities": []}, {"text": "In this way, the model automatically learns richer structural dependencies among textual units, thus incorporating well-established insights from earlier work.", "labels": [], "entities": []}, {"text": "Advantageously, the proposed architecture can easily benefit from information external to the model, i.e., by replacing inter-document attention with a graph-matrix computed based on the basis of lexical similarity () or discourse relations.", "labels": [], "entities": []}, {"text": "We evaluate our model on the WikiSum dataset and show experimentally that the proposed architecture brings substantial improvements over several strong baselines.", "labels": [], "entities": [{"text": "WikiSum dataset", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.96987584233284}]}, {"text": "We also find that the addition of a simple ranking module which scores documents based on their usefulness for the target summary can greatly boost the performance of a multi-document summarization system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Test set results on the WikiSum dataset using ROUGE F 1 .", "labels": [], "entities": [{"text": "WikiSum dataset", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9653598368167877}, {"text": "ROUGE F 1", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9143035610516866}]}, {"text": " Table 4: System scores based on questions answered  by AMT participants and summary quality rating.", "labels": [], "entities": [{"text": "AMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.7015295624732971}, {"text": "summary quality rating", "start_pos": 77, "end_pos": 99, "type": "METRIC", "confidence": 0.9262776772181193}]}]}