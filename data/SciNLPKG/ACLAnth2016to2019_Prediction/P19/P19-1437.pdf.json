{"title": [], "abstractContent": [{"text": "Sequential recurrent neural networks have achieved superior performance on language modeling, but overlook the structure information in natural language.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7242500633001328}]}, {"text": "Recent works on structure-aware models have shown promising results on language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7316572368144989}]}, {"text": "However, how to incorporate structure knowledge on corpus without syntactic annotations remains an open problem.", "labels": [], "entities": []}, {"text": "In this work, we propose neural variational language model (NVLM), which enables the sharing of grammar knowledge among different corpora.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the effectiveness of our framework on two popular benchmark datasets.", "labels": [], "entities": []}, {"text": "With the help of shared grammar, our language model converges significantly faster to a lower perplexity on new training corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modeling has been a long-standing fundamental task in natural language processing.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7506633400917053}, {"text": "natural language processing", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.6491485436757406}]}, {"text": "In recent years, sequential recurrent neural networks (RNNs) based language models have made astonishing progress, which achieve remarkable results on various benchmark datasets ().", "labels": [], "entities": []}, {"text": "Despite the huge success, the structure information in natural language is largely overlooked due to the structural limit of sequential RNN-based language models.", "labels": [], "entities": []}, {"text": "Recently, researchers have explored to explicitly exploit the latent structures in natural language, such as recurrent neural network grammars (RNNGs;) and parsing-reading-predict networks (PRPNs;).", "labels": [], "entities": []}, {"text": "These structureaware models have shown promising results on language modeling, demonstrating that the latent nested structure in language indeed helps improve sequential language models.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7483003735542297}]}, {"text": "Models like RNNG exploit treebank data with syntactic annotations to learn grammar, which is then used to improve language model performance by a significant margin.", "labels": [], "entities": []}, {"text": "This is definitely intriguing, but we have to pay the cost: accurate syntactic annotation is very costly, and treebank data such as the Penn Treebank () is typically small-scale and not open to the public for free.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.9947917461395264}]}, {"text": "On new corpus which has no syntactic annotations, how to improve language modeling with grammar knowledge?", "labels": [], "entities": [{"text": "language modeling", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.720962256193161}]}, {"text": "This is an important and challenging open problem.", "labels": [], "entities": []}, {"text": "As a motivating example, we conduct a simple experiment by training a RNN language model on one corpus and testing it on another, and report the results in Table.", "labels": [], "entities": []}, {"text": "The RNN language model performs terribly when training and testing on different datasets, which is reasonable since the data distribution may vary dramatically on different corpora.", "labels": [], "entities": []}, {"text": "Training from scratch on every new corpus is obviously not good enough: 1) it is computationally expensive and not data-efficient; 2) the size of target corpus maybe too small to train a decent RNN-based language model; 3) the common grammar is not leveraged.", "labels": [], "entities": []}, {"text": "Some recent works on transfer learning have made attempts on language model adaptation (, however, none of them explicitly exploits the common grammar knowledge shared between corpora.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9507783353328705}, {"text": "language model adaptation", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6555454134941101}]}, {"text": "To bridge the gap of language modeling on different corpora, we believe that grammar is the key since all corpora are in the same language and should share the same grammar.", "labels": [], "entities": []}, {"text": "Motivated by that, we propose neural variational language model (NVLM).", "labels": [], "entities": []}, {"text": "Specifically, our framework consists of two probabilistic components: a constituency parser and a joint generative model of sentence and parse tree.", "labels": [], "entities": []}, {"text": "When treebank data is available, we can separately train both components.", "labels": [], "entities": []}, {"text": "On new corpus without tree annotations, we fix the pre-trained parser and train the generative model either from scratch or with warmup.", "labels": [], "entities": []}, {"text": "The pre-trained parser is armed with grammar knowledge, thus it boosts up our language model to land on new corpus.", "labels": [], "entities": []}, {"text": "Our proposed framework also supports end-to-end joint training of the two components, so that we can fine-tune the language model.", "labels": [], "entities": []}, {"text": "Experimental results show that our proposed framework is effective in all leaning schemes, which achieves good performance on two popular benchmark datasets.", "labels": [], "entities": []}, {"text": "With the help of shared grammar, our language model converges significantly faster to a lower perplexity on new corpus.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are summarized as follows: \u2022 Grammar-sharing framework: We propose a framework for grammar-sharing language modeling, which incorporates the common grammar knowledge into language modeling.", "labels": [], "entities": [{"text": "grammar-sharing language modeling", "start_pos": 115, "end_pos": 148, "type": "TASK", "confidence": 0.6591010689735413}]}, {"text": "With the shared grammar, our framework helps language model efficiently transfer to new corpus with better performance and using shorter time.", "labels": [], "entities": []}, {"text": "\u2022 End-to-end learning: Our framework can be end-to-end trained without syntactic annotations.", "labels": [], "entities": []}, {"text": "To tackle the technical challenges in end-to-end learning, we use variational methods that exploit policy gradient algorithm for joint training.", "labels": [], "entities": []}, {"text": "\u2022 Efficient software package: We provide a highly efficient implementation of our work on GPUs.", "labels": [], "entities": []}, {"text": "Our parser is capable of parsing one million sentences per hour on a single GPU.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9664141535758972}]}, {"text": "See Appendix D for details.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8836429119110107}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test perplexity of RNN language model,  which performs terribly when training and testing on  different datasets.", "labels": [], "entities": []}, {"text": " Table 3: Test perplexity on the subsampled OBWB  dataset. Models are trained on variant proportion of  training data. Models randomly initialized are marked  as \"scratch\", while models pre-trained on the PTB  dataset are marked as \"warmed\".", "labels": [], "entities": [{"text": "OBWB  dataset", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9506904184818268}, {"text": "PTB  dataset", "start_pos": 205, "end_pos": 217, "type": "DATASET", "confidence": 0.9750807583332062}]}, {"text": " Table 4: Test perplexity on the subsampled OBWB  dataset. All models are trained with 100% training  data.", "labels": [], "entities": [{"text": "OBWB  dataset", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.92278853058815}]}]}