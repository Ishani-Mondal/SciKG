{"title": [{"text": "Generating Logical Forms from Graph Representations of Text and Entities", "labels": [], "entities": [{"text": "Generating Logical Forms from Graph Representations of Text and Entities", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.8268523871898651}]}], "abstractContent": [{"text": "Structured information about entities is critical for many semantic parsing tasks.", "labels": [], "entities": [{"text": "semantic parsing tasks", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.80381178855896}]}, {"text": "We present an approach that uses a Graph Neural Network (GNN) architecture to incorporate information about relevant entities and their relations during parsing.", "labels": [], "entities": []}, {"text": "Combined with a de-coder copy mechanism, this approach provides a conceptually simple mechanism to generate logical forms with entities.", "labels": [], "entities": []}, {"text": "We demonstrate that this approach is competitive with the state-of-the-art across several tasks without pre-training, and outperforms existing approaches when combined with BERT pre-training.", "labels": [], "entities": [{"text": "BERT", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.9389861226081848}]}], "introductionContent": [{"text": "Semantic parsing maps natural language utterances into structured meaning representations.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8219030201435089}]}, {"text": "The representation languages vary between tasks, but typically provide a precise, machine interpretable logical form suitable for applications such as question answering).", "labels": [], "entities": [{"text": "question answering", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.885235995054245}]}, {"text": "The logical forms typically consist of two types of symbols: a vocabulary of operators and domain-specific predicates or functions, and entities grounded to some knowledge base or domain.", "labels": [], "entities": []}, {"text": "Recent approaches to semantic parsing have cast it as a sequence-to-sequence task, employing methods similar to those developed for neural machine translation (), with strong results.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.864448606967926}, {"text": "neural machine translation", "start_pos": 132, "end_pos": 158, "type": "TASK", "confidence": 0.7504076361656189}]}, {"text": "However, special consideration is typically given to handling of entities.", "labels": [], "entities": []}, {"text": "This is important to improve generalization and computational efficiency, as most tasks require handling entities unseen during training, and the set of unique entities can be large.", "labels": [], "entities": [{"text": "generalization", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9708192348480225}]}, {"text": "Some recent approaches have replaced surface forms of entities in the utterance with placeholders (.", "labels": [], "entities": []}, {"text": "This requires a preprocessing step to completely disambiguate entities and replace their spans in the utterance.", "labels": [], "entities": []}, {"text": "Additionally, for some tasks it maybe beneficial to leverage relations between entities, multiple entity candidates per span, or entity candidates without a corresponding span in the utterance, while generating logical forms.", "labels": [], "entities": []}, {"text": "Other approaches identify only types and surface forms of entities while constructing the logical form, using a separate post-processing step to generate the final logical form with grounded entities.", "labels": [], "entities": []}, {"text": "This ignores potentially useful knowledge about relevant entities.", "labels": [], "entities": []}, {"text": "Meanwhile, there has been considerable recent interest in Graph Neural Networks (GNNs) () for effectively learning representations for graph structures.", "labels": [], "entities": []}, {"text": "We propose a GNN architecture based on extending the self-attention mechanism of the Transformer ( to make use of relations between input elements.", "labels": [], "entities": []}, {"text": "We present an application of this GNN architecture to semantic parsing, conditioning on a graph representation of the given natural language utterance and potentially relevant entities.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7379669845104218}]}, {"text": "This approach is capable of handling ambiguous and potentially conflicting entity candidates jointly with a natural language utterance, relaxing the need for completely disambiguating a set of linked entities before parsing.", "labels": [], "entities": []}, {"text": "This graph formulation also enables us to incorporate knowledge about the relations between entities where available.", "labels": [], "entities": []}, {"text": "Combined with a copy mechanism while decoding, this approach also provides a conceptually simple method for generating logical forms with grounded entities.", "labels": [], "entities": []}, {"text": "We demonstrate the capability of the pro-", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider three semantic parsing datasets, with examples given in GEO The GeoQuery dataset consists of natural language questions about US geography along with corresponding logical forms (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7795232236385345}, {"text": "GEO", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9797366261482239}, {"text": "GeoQuery dataset", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.7984660863876343}]}, {"text": "We follow the convention of Zettlemoyer and and use 600 training examples and 280 test examples.", "labels": [], "entities": []}, {"text": "We use logical forms based on Functional Query Language (FunQL) ().", "labels": [], "entities": []}, {"text": "ATIS The Air Travel Information System (ATIS) dataset consists of natural language queries about travel planning (.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.916541337966919}, {"text": "Air Travel Information System (ATIS) dataset", "start_pos": 9, "end_pos": 53, "type": "DATASET", "confidence": 0.8412006199359894}]}, {"text": "We follow and use 4473 training examples, 448 test examples, and represent the logical forms as lambda expressions.", "labels": [], "entities": []}, {"text": "SPIDER This is a large-scale text-to-SQL dataset that consists of 10,181 questions and 5,693 unique complex SQL queries across 200 database tables spanning 138 domains ().", "labels": [], "entities": []}, {"text": "We use the standard training set of 8,659 training example and development set of 1,034 examples, split across different tables.", "labels": [], "entities": []}, {"text": "Model Configuration We configured hyperparameters based on performance on the validation set for each task, if provided, otherwise crossvalidated on the training set.", "labels": [], "entities": []}, {"text": "For the encoder and decoder, we selected the number of layers from {1, 2, 3, 4} and embedding and hidden dimensions from {64, 128, 256}, setting the feed forward layer hidden dimensions 4\u00d7 higher.", "labels": [], "entities": []}, {"text": "We employed dropout at training time with P dropout selected from {0.1, 0.2, 0.3, 0.4, 0.5, 0.6}.", "labels": [], "entities": [{"text": "P dropout", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.8744859397411346}]}, {"text": "We used 8 attention heads for each task.", "labels": [], "entities": []}, {"text": "We used a clipping distance of 8 for relative position representations (.", "labels": [], "entities": []}, {"text": "We used the Adam optimizer () with \u03b2 1 = 0.9, \u03b2 2 = 0.98, and = 10 \u22129 , and tuned the learning rate for each task.", "labels": [], "entities": []}, {"text": "We used the same warmup and decay strategy for learning rate as, selecting a number of warmup steps up to a maximum of 3000.", "labels": [], "entities": []}, {"text": "Early stopping was used to determine the total training steps for each task.", "labels": [], "entities": [{"text": "Early stopping", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6447774469852448}]}, {"text": "We used the final checkpoint for evaluation.", "labels": [], "entities": []}, {"text": "We batched training examples together, and selected batch size from {32, 64, 128, 256, 512}.", "labels": [], "entities": []}, {"text": "During training we used masked self-attention () to enable parallel decoding of output sequences.", "labels": [], "entities": []}, {"text": "For evaluation, we used greedy search.", "labels": [], "entities": []}, {"text": "We used a simple strategy of splitting each input utterance on spaces to generate a sequence of tokens.", "labels": [], "entities": []}, {"text": "We mapped any token that didn't occur at least 2 times in the training dataset to a special outof-vocabulary token.", "labels": [], "entities": []}, {"text": "For experiments that used BERT, we instead used the same wordpiece () tokenization as used for pre-training.", "labels": [], "entities": [{"text": "BERT", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9840114712715149}]}, {"text": "BERT For some of our experiments, we evaluated incorporating a pre-trained BERT) encoder by effectively using the output of the BERT encoder in place of a learned token embedding table.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8102576732635498}, {"text": "BERT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9267123937606812}]}, {"text": "We then continue to use graph encoder and decoder layers with randomly initialized parameters in addition to BERT, so there are many parameters that are not pre-trained.", "labels": [], "entities": [{"text": "BERT", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.997820258140564}]}, {"text": "The additional encoder layers are still necessary to condition on entities and relations.", "labels": [], "entities": []}, {"text": "We achieved best results by freezing the pretrained parameters for an initial number of steps, and then jointly fine-tuning all parameters, similar to existing approaches for gradual unfreezing.", "labels": [], "entities": []}, {"text": "When unfreezing the pre-trained parameters, we restart the learning rate schedule.", "labels": [], "entities": []}, {"text": "We found this to perform better than keeping pre-trained parameters either entirely frozen or entirely unfrozen during fine-tuning.", "labels": [], "entities": []}, {"text": "We used BERT LARGE, which has 24 layers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.994428277015686}, {"text": "LARGE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.4967016279697418}]}, {"text": "For fine tuning we used the same Adam optimizer with weight decay and learning rate decay as used for BERT pre-training.", "labels": [], "entities": [{"text": "fine tuning", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.868213415145874}, {"text": "weight decay", "start_pos": 53, "end_pos": 65, "type": "METRIC", "confidence": 0.9086573719978333}, {"text": "learning rate decay", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.9147488077481588}, {"text": "BERT", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.6866921186447144}]}, {"text": "We reduced batch sizes to accommodate the significantly larger model size, and tuned learning rate, warm up steps, and number of frozen steps for pre-trained parameters.", "labels": [], "entities": []}, {"text": "Entity Candidate Generator We use an entity candidate generator that, given x, can retrieve a set of potentially relevant entities, e, for the given domain.", "labels": [], "entities": []}, {"text": "Although all generators share a common interface, their implementation varies across tasks.", "labels": [], "entities": []}, {"text": "For GEO and ATIS we use a lexicon of entity aliases in the dataset and attempt to match with ngrams in the query.", "labels": [], "entities": [{"text": "GEO", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9523255228996277}, {"text": "ATIS", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8137422204017639}]}, {"text": "Each entity has a single attribute corresponding to the entity's type.", "labels": [], "entities": []}, {"text": "We used binary valued relations between entity candidates based on whether entity candidate spans overlap, but experiments did not show significant improvements from incorporating these relations.", "labels": [], "entities": []}, {"text": "For SPIDER, we generalize our notion of entities to include tables and table columns.", "labels": [], "entities": []}, {"text": "We include all relevant tables and columns as entity candidates, but make use of Levenshtein distance between query ngrams and table and column names to determine edges between tokens and entity candidates.", "labels": [], "entities": []}, {"text": "We use attributes based on the types and names of tables and columns.", "labels": [], "entities": []}, {"text": "Edges between entity candidates capture relations between columns and the table they belong to, and foreign key relations.", "labels": [], "entities": []}, {"text": "For GEO, ATIS, and SPIDER, this leads to 19.5%, 32.7%, and 74.6% of examples containing at least one span associated with multiple entity candidates, respectively, indicating some entity ambiguity.", "labels": [], "entities": [{"text": "GEO", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9182156324386597}, {"text": "ATIS", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.4211364686489105}]}, {"text": "Further details on how entity candidate generators were constructed are provided in \u00a7 A.1.", "labels": [], "entities": []}, {"text": "Output Sequences We pre-processed output sequences to identify entity argument values, and replaced those elements with references to entity candidates in the input.", "labels": [], "entities": []}, {"text": "In cases where our entity candidate generator did not retrieve an entity that was used as an argument, we dropped the example from the training data set or considered it incorrect: We report accuracies on GEO, ATIS, and SPIDER for various implementations of our GNN sub-layer.", "labels": [], "entities": [{"text": "GEO", "start_pos": 205, "end_pos": 208, "type": "DATASET", "confidence": 0.9544907808303833}]}, {"text": "For GEO and ATIS, we use \u2020 to denote neural approaches that disambiguate and replace entities in the utterance as a pre-processing step.", "labels": [], "entities": [{"text": "GEO", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8580132722854614}, {"text": "ATIS", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.6025095582008362}, {"text": "\u2020", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.9599312543869019}]}, {"text": "For SPIDER, the evaluation set consists of examples for databases unseen during training.", "labels": [], "entities": [{"text": "SPIDER", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9615877866744995}]}, {"text": "if in the test set.", "labels": [], "entities": []}, {"text": "Evaluation To evaluate accuracy, we use exact match accuracy relative to gold logical forms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9990763664245605}, {"text": "exact match accuracy", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.8220212062199911}]}, {"text": "For GEO we directly compare output symbols.", "labels": [], "entities": [{"text": "GEO", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.6658984422683716}]}, {"text": "For ATIS, we compare normalized logical forms using canonical variable naming and sorting for unordered arguments.", "labels": [], "entities": []}, {"text": "For SPI-DER we use the provided evaluation script, which decomposes each SQL query and conducts set comparison within each clause without values.", "labels": [], "entities": []}, {"text": "All accuracies are reported on the test set, except for SPIDER where we report and compare accuracies on the development set.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9660319685935974}]}, {"text": "Copying Tokens To better understand the effect of conditioning on entities and their relations, we also conducted experiments that considered an alternative method for selecting and disambiguating entities similar to.", "labels": [], "entities": []}, {"text": "In this approach we use our model's copy mechanism to copy tokens corresponding to the surface forms of entity arguments, rather than copying entities directly.", "labels": [], "entities": []}, {"text": "where W x is a learned matrix, and where i \u2208 {1, . .", "labels": [], "entities": []}, {"text": ", |x|} refers to the index of token This allows us to ablate entity information in the input while still generating logical forms.", "labels": [], "entities": []}, {"text": "When copying tokens, the decoder determines the type of the entity using an additional output symbol.", "labels": [], "entities": []}, {"text": "For GEO, the actual entity can then be identified as a post-processing step, as a type and surface form is sufficient.", "labels": [], "entities": [{"text": "GEO", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.5361438393592834}]}, {"text": "For other tasks this could require a more complicated post-processing step to disambiguate entities given a surface form and type.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: We report accuracies on GEO, ATIS, and SPIDER for various implementations of our GNN sub-layer. For  GEO and ATIS, we use  \u2020 to denote neural approaches that disambiguate and replace entities in the utterance as a  pre-processing step. For SPIDER, the evaluation set consists of examples for databases unseen during training.", "labels": [], "entities": [{"text": "GEO", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.9821788668632507}]}]}