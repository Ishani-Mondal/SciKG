{"title": [], "abstractContent": [{"text": "Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.639315277338028}]}, {"text": "As a consequence , models tend to learn to ignore this information.", "labels": [], "entities": []}, {"text": "We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder.", "labels": [], "entities": [{"text": "translate-and-refine", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.9587631225585938}]}, {"text": "This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context.", "labels": [], "entities": []}, {"text": "This approach leads to the state of the art results.", "labels": [], "entities": []}, {"text": "Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal machine translation (MMT) is an area of research that addresses the task of translating texts using context from an additional modality, generally static images.", "labels": [], "entities": [{"text": "Multimodal machine translation (MMT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.847398042678833}]}, {"text": "The assumption is that the visual context can help ground the meaning of the text and, as a consequence, generate more adequate translations.", "labels": [], "entities": []}, {"text": "Current work has focused on datasets of images paired with their descriptions, which are crowdsourced in English and then translated into different languages, namely the Multi30K dataset (.", "labels": [], "entities": [{"text": "Multi30K dataset", "start_pos": 170, "end_pos": 186, "type": "DATASET", "confidence": 0.9156462252140045}]}, {"text": "Results from the most recent evaluation campaigns in the area () have shown that visual information can be helpful, as humans generally prefer translations generated by multimodal models than by their text-only counterparts.", "labels": [], "entities": []}, {"text": "However, previous work has also shown that images are only needed in very specific cases (.", "labels": [], "entities": []}, {"text": "This is also the case for humans.", "labels": [], "entities": []}, {"text": "(see) concluded that visual information is needed by humans in the presence of the following: incorrect or ambiguous source words and gender-neutral words that need to be marked for gender in the target language.", "labels": [], "entities": []}, {"text": "In an experiment where human translators were asked to first translate descriptions based on their textual context only and then revise their translation based on a corresponding image, they report that these three cases accounted for 62-77% of the revisions in the translations in two subsets of Multi30K.", "labels": [], "entities": [{"text": "Multi30K", "start_pos": 297, "end_pos": 305, "type": "DATASET", "confidence": 0.9305340647697449}]}, {"text": "Ambiguities are very frequent in Multi30K, as inmost language corpora.", "labels": [], "entities": [{"text": "Ambiguities", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9632229804992676}]}, {"text": "shows that in its latest test set, 358 (German) and 438 (French) instances (out of 1,000) contain at least one word that has more than one translation in the training set.", "labels": [], "entities": []}, {"text": "However, these do not always represent a challenge for translation models: often the text context can easily disambiguate words (see baseline translation in(a)); additionally, the models are naturally biased to generate the most frequent translation of the word, which by definition is the correct one inmost cases.", "labels": [], "entities": []}, {"text": "The need to gender-mark words in a target language when translating from English can bethought of as a disambiguation problem, except that the text context is often less telling and the frequency bias plays ends up playing a bigger role (see baseline translation in(c)).", "labels": [], "entities": []}, {"text": "This has been shown to be a common problem in neural machine translation, as well as in areas such as image captioning and co-reference resolution (.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6523905197779337}, {"text": "image captioning", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7771921753883362}, {"text": "co-reference resolution", "start_pos": 123, "end_pos": 146, "type": "TASK", "confidence": 0.7330536842346191}]}, {"text": "Incorrect source words are common in Multi30K, as in many other crowdsourced or usergenerated dataset.", "labels": [], "entities": []}, {"text": "In this case the context may not be enough (see DE translation in(c)).", "labels": [], "entities": [{"text": "DE translation", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.685526579618454}]}, {"text": "We posit that models should be robust to such a type of noise and note that similar treatment would be  required for out of vocabulary (OOV) words, i.e. correct words that are unknown to the model.", "labels": [], "entities": []}, {"text": "We propose an approach that takes into account the strengths of a text-only baseline model and only refines its translations when needed.", "labels": [], "entities": []}, {"text": "Our approach is based on deliberation networks ( to jointly learn to generate draft translations and refine them based on left and right side target context as well as structured visual information.", "labels": [], "entities": []}, {"text": "This approach outperforms previous work.", "labels": [], "entities": []}, {"text": "In order to further probe how well our models can address the three problems mentioned above, we perform a controlled experiment where we minimise the interference of the frequency bias by masking ambiguous and gender-related words, as well as randomly selected words (to simulate noise and OOV).", "labels": [], "entities": [{"text": "OOV", "start_pos": 291, "end_pos": 294, "type": "METRIC", "confidence": 0.9575445055961609}]}, {"text": "This experiment shows that our multimodal refinement approach outperforms the textonly one in more complex linguistic setups.", "labels": [], "entities": []}, {"text": "Our main contributions are: (i) a novel approach to MMT based on deliberation networks and structured visual information which gives state of the art results (Sections 3.2 and 5.1); (ii) a frequency bias-free investigation on the need for visual context in MMT (Sections 4.2 and 5.2); and (iii) a thorough investigation on different visual representations for transformer-based architectures (Section 3.3).", "labels": [], "entities": [{"text": "MMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9805326461791992}]}], "datasetContent": [{"text": "We build and test our MMT models on the Multi30K dataset (.", "labels": [], "entities": [{"text": "MMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9437099099159241}, {"text": "Multi30K dataset", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.9732962846755981}]}, {"text": "Each image in Multi30K contains one English (EN) description taken from Flickr30K () and human translations into German (DE), French (FR) and Czech ().", "labels": [], "entities": [{"text": "Flickr30K", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9614232182502747}]}, {"text": "The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test.", "labels": [], "entities": []}, {"text": "We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present.", "labels": [], "entities": []}, {"text": "In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test).", "labels": [], "entities": [{"text": "Multi30K test set (test 2016", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.9114634493986765}, {"text": "WMT evaluation competition", "start_pos": 100, "end_pos": 126, "type": "DATASET", "confidence": 0.7972959478696188}]}], "tableCaptions": [{"text": " Table 1.  We note that RND and PERS are the same for lan- guage pairs as the degradation only depends on the  source side, while for AMB the words replaced de- pend on the target language.", "labels": [], "entities": [{"text": "RND", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9668117761611938}, {"text": "PERS", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.997445821762085}]}, {"text": " Table 1: Statistics of datasets after applying source  degradation strategies", "labels": [], "entities": []}, {"text": " Table 2: Results for the test sets 2016 and 2018. M  denotes METEOR, B -BLEU; * marks statistically sig- nificant changes for METEOR (p-value \u2264 0.05) as com- pared to base,  \u2020 -as compared to del. Bold high- lights statistically significant improvements. We report  previous state of the art results for multimodal models  from (Helcl et al., 2018).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.8333856463432312}, {"text": "B -BLEU", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.8465774456659952}, {"text": "METEOR", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.856743335723877}]}, {"text": " Table 3: Human ranking results: normalised rank  (micro-averaged). Bold highlights best results.", "labels": [], "entities": []}, {"text": " Table 4: Results for the test sets 2016 and 2018 for the three degradation configurations: RND, AMB and PERS. M  denotes METEOR, B -BLEU; * marks statistically significant changes as computed for METEOR (p-value \u2264 0.05)  as compared to base,  \u2020 -as compared to del. Bold highlights statistically significant improvements over base.", "labels": [], "entities": [{"text": "PERS", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9792879223823547}, {"text": "METEOR", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.8913552761077881}, {"text": "B -BLEU", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.8890583316485087}, {"text": "METEOR", "start_pos": 197, "end_pos": 203, "type": "DATASET", "confidence": 0.6164069771766663}]}]}