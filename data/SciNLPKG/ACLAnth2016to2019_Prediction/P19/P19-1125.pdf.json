{"title": [{"text": "Imitation Learning for Non-Autoregressive Neural Machine Translation", "labels": [], "entities": [{"text": "Imitation Learning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.908176988363266}, {"text": "Non-Autoregressive Neural Machine Translation", "start_pos": 23, "end_pos": 68, "type": "TASK", "confidence": 0.626187801361084}]}], "abstractContent": [{"text": "Non-autoregressive translation models (NAT) have achieved impressive inference speedup.", "labels": [], "entities": []}, {"text": "A potential issue of the existing NAT algorithms , however, is that the decoding is conducted in parallel, without directly considering previous context.", "labels": [], "entities": []}, {"text": "In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7690120041370392}]}, {"text": "We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets.", "labels": [], "entities": [{"text": "IWSLT16", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9574393630027771}, {"text": "WMT14", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8157832026481628}, {"text": "WMT16 datasets", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.9283010363578796}]}, {"text": "Our proposed model achieves a significant speedup over the autore-gressive models, while keeping the translation quality comparable to the autoregressive models.", "labels": [], "entities": []}, {"text": "By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro\u2192En and 30.68 BLEU on IWSLT16 En\u2192De.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9981170892715454}, {"text": "WMT16", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.8769289255142212}, {"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9975748658180237}, {"text": "IWSLT16 En\u2192De", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.8709494173526764}]}], "introductionContent": [{"text": "Neural machine translation (NMT) with encoderdecoder architectures) achieve significantly improved performance compared with traditional statistical methods(.", "labels": [], "entities": [{"text": "Neural machine translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7757968306541443}]}, {"text": "Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed.", "labels": [], "entities": []}, {"text": "Specifically, the decoder, whether based on Recurrent Neural Network (RNN)) or attention mechanism (, sequentially generates words.", "labels": [], "entities": []}, {"text": "The latter words are conditioned on previous words in a sentence.", "labels": [], "entities": []}, {"text": "Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming.", "labels": [], "entities": []}, {"text": "Recently, a line of research work ( (a) Autoregressive NMT (b) Non-Autoregressive NMT: Neural architectures for Autoregressive NMT and Non-Autoregressive NMT.", "labels": [], "entities": []}, {"text": "propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT).", "labels": [], "entities": [{"text": "neural machine translation (NAT)", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.7729617158571879}]}, {"text": "In NAT, the decoder generates all words simultaneously instead of sequentially.", "labels": [], "entities": []}, {"text": "Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation () as inputs of the decoder.", "labels": [], "entities": []}, {"text": "Thus, the generation of the NAT models does not condition on previous prediction.", "labels": [], "entities": []}, {"text": "NAT enables parallel computation of decoder, giving significantly fast translation speed with moderate accuracy (always within 5 BLEU).", "labels": [], "entities": [{"text": "NAT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.845527172088623}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.998762845993042}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.997638463973999}]}, {"text": "shows the difference between autoregressive and non-autoregressive models.", "labels": [], "entities": []}, {"text": "However, we argue that current NAT approaches suffer from delayed supervisions (or rewards) and large search space in training.", "labels": [], "entities": [{"text": "NAT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9304869771003723}]}, {"text": "NAT decoder simultaneously generates all words of the translation, the search space of which is very large.", "labels": [], "entities": [{"text": "NAT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8516159653663635}]}, {"text": "For onetime step, decoding states across layers (more than 16 layers) and time steps could be regarded as a 2-dimensional sequential decision process.", "labels": [], "entities": []}, {"text": "Every decoding state has not only to decide which part of target sentence it will focus on, but also to decide the correct target word of that part.", "labels": [], "entities": []}, {"text": "All decisions are made by interactions with other decoding states.", "labels": [], "entities": []}, {"text": "Delayed supervisions (correct target word) will be obtained by decoding states in the last layer, and intermediate decoding states will be updated by gradient propagation from the last layer.", "labels": [], "entities": []}, {"text": "Therefore, the training of NAT is non-trivial and it maybe hard for NAT to achieve a good model, which is the same case that reinforcement learning) is hard to learn with large search space.", "labels": [], "entities": []}, {"text": "The delayed supervision problem is not severe for autoregressive neural machine translation(AT) because it predicts words sequentially.", "labels": [], "entities": [{"text": "autoregressive neural machine translation(AT)", "start_pos": 50, "end_pos": 95, "type": "TASK", "confidence": 0.7841226288250515}]}, {"text": "Given the previous words, contents to be predicted at current step are relatively definite, thus the search space of AT is exponentially lower than NAT.", "labels": [], "entities": [{"text": "AT", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.8429766893386841}, {"text": "NAT", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.8252712488174438}]}, {"text": "We blame the delayed supervision and large search space for the performance gap between NAT and AT.", "labels": [], "entities": [{"text": "NAT", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.8453555107116699}, {"text": "AT", "start_pos": 96, "end_pos": 98, "type": "DATASET", "confidence": 0.6629681587219238}]}, {"text": "In this paper, we propose a novel imitation learning framework for non-autoregressive NMT (imitate-NAT ).", "labels": [], "entities": []}, {"text": "Imitation learning has been widely used to alleviate the problem of huge search space with delayed supervision in RL.", "labels": [], "entities": [{"text": "Imitation learning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9457828104496002}, {"text": "RL", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9270302653312683}]}, {"text": "It is straightforward to bring the imitation learning idea for boosting the performance of NAT.", "labels": [], "entities": []}, {"text": "Specifically, we introduce a knowledgeable AT demonstrator to supervise each decoding state of NAT model.", "labels": [], "entities": []}, {"text": "In such case, Specifically, We propose to employ a knowledgeable AT demonstrator to supervise every decoding state of NAT across different time steps and layers, which works pretty well practically.", "labels": [], "entities": []}, {"text": "Since the AT demonstrator is only used in training, our proposed imitate-NAT enjoys the high speed of NAT without suffering from its relatively lower translation performance.", "labels": [], "entities": []}, {"text": "Experiments show that our proposed imitate-NAT is fast and accurate, which effectively closes the performance gap between AT and NAT on several standard benchmarks, while maintains the speed advantages of NAT (10 times faster).", "labels": [], "entities": [{"text": "accurate", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9834817051887512}]}, {"text": "On all the benchmark datasets, our imitate-NAT with LPD achieves the best translation performance, which is even close to the results of the autoregressive model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed model on machine translation tasks and provide the analysis.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.8255697886149088}]}, {"text": "We present the experimental details in the following, including the introduction to the datasets as well as our experimental settings.", "labels": [], "entities": []}, {"text": "Datasets We evaluate the proposed method on three widely used public machine translation corpora: IWSLT16 En-De(196K pairs), WMT14 EnDe(4.5M pairs) and WMT16 En-Ro(610K pairs).", "labels": [], "entities": [{"text": "IWSLT16 En-De", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.7461926639080048}, {"text": "WMT14 EnDe", "start_pos": 125, "end_pos": 135, "type": "DATASET", "confidence": 0.6700837016105652}, {"text": "WMT16 En-Ro", "start_pos": 152, "end_pos": 163, "type": "DATASET", "confidence": 0.8222717642784119}]}, {"text": "All the datasets are tokenized by Moses and segmented into 32k\u2212subword symbols with byte pair encoding to restrict the size of the vocabulary.", "labels": [], "entities": []}, {"text": "For WMT14 En-De, we use newstest-2013 and newstest-2014 as development and test set respectively.", "labels": [], "entities": [{"text": "WMT14 En-De", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9228313565254211}]}, {"text": "For WMT16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets respectively.", "labels": [], "entities": [{"text": "WMT16 En-Ro", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9222749769687653}]}, {"text": "For IWSLT16 En-De, we use test2013 as validation for ablation experiments.", "labels": [], "entities": [{"text": "IWSLT16 En-De", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7575831115245819}]}, {"text": "Knowledge Distillation Datasets Sequencelevel knowledge distillation is applied to alleviate multimodality in the training dataset, using the AT demonstrator as the teachers.", "labels": [], "entities": [{"text": "Knowledge Distillation Datasets Sequencelevel knowledge distillation", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.709256316224734}]}, {"text": "We replace the reference target sentence of each pair of training example (X, Y ) with anew target sentence Y * , which is generated from the teacher model(AT demonstrator).", "labels": [], "entities": []}, {"text": "Then we use the new dataset (X, Y * ) to train our NAT model.", "labels": [], "entities": []}, {"text": "To avoid the redundancy of running fixed teacher models repeatedly on the same data, we decode the entire training set once using each teacher to create anew training dataset for its respective student.", "labels": [], "entities": []}, {"text": "Model Settings We first train the AT demonstrator and then freeze its parameters during the training of imitate-NAT . In order to speedup the convergence of NAT training, we also initialize imitate-NAT with the corresponding parameters of the AT expert as they have similar architecture.", "labels": [], "entities": []}, {"text": "For WMT14 En-De and WMT16 En-Ro, we use the hyperparameter settings of base Transformer model in(d model = 512, d hidden = 512, n layer = 6 and n head = 8).", "labels": [], "entities": [{"text": "WMT14 En-De", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9034055769443512}, {"text": "WMT16 En-Ro", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.9124874472618103}]}, {"text": "As in;, we use the small model (d model = 278, d hidden = 507, n layer = 5 and n head = 2) for IWSLT16 En-De.", "labels": [], "entities": [{"text": "IWSLT16 En-De", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.8465099334716797}]}, {"text": "For sequence-level distillation, we set beam size to be 4.", "labels": [], "entities": []}, {"text": "For imitate-NAT , we set the number of action category to 512 and found imitate-NAT is robust to the setting in our preliminary experiments.", "labels": [], "entities": []}, {"text": "Length Parallel Decoding For inference, we follow the common practice of noisy parallel decoding (, which generates a number of decoding candidates in parallel and selects the best translation via re-scoring using AT teacher.", "labels": [], "entities": []}, {"text": "In our scenario, we first train a module to predict the target length as\u02c6Tas\u02c6 as\u02c6T . However, due to the inherent uncertainty of the data itself, it is hard to accurately predict the target length.", "labels": [], "entities": []}, {"text": "A reasonable solution is to generate multiple translation candidates by predicting different target length \u2208 [ \u02c6 T \u2212 \u2206T, \u02c6 T + \u2206T ] , which we called LPD (length parallel decoding).", "labels": [], "entities": []}, {"text": "The model generates several outputs in parallel, then we use the pre-trained autoregressive model to identify the best overall translation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The test set performances of AT and NAT models in BLEU score. NAT-FT, NAT-IR and LT denotes the  competitor method in (Gu et al., 2017), (Lee et al., 2018) and (Kaiser et al., 2018) respectively. imitate-NAT is  our proposed NAT with imitation learning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9976369142532349}]}, {"text": " Table 2: Ablation study on the dev set of IWSLT16. w/ indicates with and w/o indicates without. LPD indicates  length parallel decoding.", "labels": [], "entities": [{"text": "IWSLT16", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8320858478546143}]}]}