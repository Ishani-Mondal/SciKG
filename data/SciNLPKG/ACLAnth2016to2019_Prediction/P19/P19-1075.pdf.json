{"title": [{"text": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test", "labels": [], "entities": [{"text": "Chinese IDiom Dataset", "start_pos": 20, "end_pos": 41, "type": "DATASET", "confidence": 0.6373774607976278}]}], "abstractContent": [{"text": "Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora.", "labels": [], "entities": []}, {"text": "In this paper we propose a large-scale Chinese cloze test dataset ChID, which studies the comprehension of idiom, a unique language phenomenon in Chinese.", "labels": [], "entities": [{"text": "Chinese cloze test dataset ChID", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.6672271668910981}]}, {"text": "In this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms.", "labels": [], "entities": []}, {"text": "We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models.", "labels": [], "entities": []}, {"text": "Results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9921541810035706}]}], "introductionContent": [{"text": "Machine reading comprehension aims to assess the ability to comprehend natural language and answer questions from a given document or passage.", "labels": [], "entities": [{"text": "Machine reading comprehension", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7925902605056763}]}, {"text": "As a classical method of assessing language proficiency, cloze test has been widely employed due to its simplicity inform.", "labels": [], "entities": [{"text": "cloze test", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.85003861784935}, {"text": "simplicity", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9806714653968811}]}, {"text": "Recently, a number of datasets for cloze test have been proposed for different languages.", "labels": [], "entities": []}, {"text": "For instance, CNN/Daily Mail () provides a benchmark for machine comprehension of English text, while the People Daily and Children's Fairy Tale dataset ( and CMRC-2017 () pioneer explorations in Chinese language.", "labels": [], "entities": [{"text": "CNN/Daily Mail", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.8411948531866074}, {"text": "People Daily and Children's Fairy Tale dataset", "start_pos": 106, "end_pos": 152, "type": "DATASET", "confidence": 0.9416264891624451}, {"text": "CMRC-2017", "start_pos": 159, "end_pos": 168, "type": "DATASET", "confidence": 0.9451191425323486}]}, {"text": "In this paper we explore idiom comprehension) in cloze test.", "labels": [], "entities": []}, {"text": "Idiom , which is called \"\u6210 \u8bed\" (chengyu) in Chinese, is an interesting linguistic phenomena in Chinese language, and this work * *Corresponding author: Minlie Huang.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following subsections, we will explain the three steps in data collection: (1) Constructing the idiom vocabulary; (2) Extracting passages within a proper length; (3) Designing candidate choices.", "labels": [], "entities": [{"text": "Extracting passages within a proper length", "start_pos": 125, "end_pos": 167, "type": "TASK", "confidence": 0.8382852772871653}]}], "tableCaptions": [{"text": " Table 4: Annotation result of near-synonyms. It shows  the number of idioms in the 200 sampled idioms that  have at least K near-synonyms, for K = 1, 2, 3, 4.  Fleiss' kappa is 0.479, indicating moderate agreement.", "labels": [], "entities": [{"text": "Fleiss' kappa", "start_pos": 161, "end_pos": 174, "type": "DATASET", "confidence": 0.5448692142963409}, {"text": "agreement", "start_pos": 205, "end_pos": 214, "type": "METRIC", "confidence": 0.9681000113487244}]}, {"text": " Table 5: Idiom frequency statistics in the whole cor- pus. The minimum and the maximum are 20 and 534  respectively.", "labels": [], "entities": [{"text": "Idiom frequency", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8938252329826355}]}, {"text": " Table 6: Annotation result of embedding similarity.  The three labels are: SYN (synonym), NEAR (near- synonym), OTHER. \u03ba is the Fleiss' kappa value.", "labels": [], "entities": [{"text": "SYN", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9409029483795166}, {"text": "NEAR", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.989237368106842}, {"text": "OTHER", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9938617944717407}]}, {"text": " Table 7: ChID dataset statistics. The out-of-domain data have longer passages (127 vs. 99) and more blanks per  passage (1.49 vs. 1.25) than the in-domain data.", "labels": [], "entities": [{"text": "ChID dataset", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.8091648519039154}]}, {"text": " Table 8: Comparison on idiom frequency distribution  between the in-domain and out-of-domain data.", "labels": [], "entities": []}, {"text": " Table 9: Performance of human and models. \u03ba indi- cates Fleiss' kappa. The overall best results are shown  in bold, and AR performs significantly better than LM  and SAR (sign test, p-value < 0.05).", "labels": [], "entities": [{"text": "AR", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9879909157752991}]}, {"text": " Table 10: Performance comparison using different id- iom representations.", "labels": [], "entities": []}]}