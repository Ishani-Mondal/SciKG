{"title": [{"text": "Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs", "labels": [], "entities": [{"text": "Relation Prediction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8729455769062042}]}], "abstractContent": [{"text": "The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information , in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction).", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 191, "end_pos": 216, "type": "TASK", "confidence": 0.6789679328600565}, {"text": "relation prediction)", "start_pos": 232, "end_pos": 252, "type": "TASK", "confidence": 0.8118880093097687}]}, {"text": "Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.9518741965293884}]}, {"text": "However, we observe that these KG embed-dings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple.", "labels": [], "entities": []}, {"text": "To this effect, our paper proposes a novel attention-based feature embedding that captures both entity and relation features in any given entity's neighborhood.", "labels": [], "entities": []}, {"text": "Additionally, we also encapsulate relation clusters and multi-hop relations in our model.", "labels": [], "entities": []}, {"text": "Our empirical study offers insights into the efficacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge graphs (KGs) represent knowledge bases (KBs) as a directed graph whose nodes and edges represent entities and relations between entities, respectively.", "labels": [], "entities": []}, {"text": "For example, in, a triple (London, capital of, United Kingdom) is represented as two entities: London and United Kingdom along with a relation (capital of) linking them.", "labels": [], "entities": [{"text": "United Kingdom", "start_pos": 106, "end_pos": 120, "type": "DATASET", "confidence": 0.8934544324874878}]}, {"text": "KGs find uses in a wide variety of applications such as semantic search), dialogue generation (, and question answering (, to name a few.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.9111930429935455}, {"text": "question answering", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.9153291881084442}]}, {"text": "However, KGs typically * Equal Contribution suffer from missing relations).", "labels": [], "entities": [{"text": "Equal Contribution", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.9553720057010651}]}, {"text": "This problem gives rise to the task of knowledge base completion (also referred to as relation prediction), which entails predicting whether a given triple is valid or not.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6218589742978414}, {"text": "relation prediction)", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.8575074474016825}]}, {"text": "State-of-the-art relation prediction methods are known to be primarily knowledge embedding based models.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8165481388568878}]}, {"text": "They are broadly classified as translational models (  and convolutional neural network (CNN) ) based models.", "labels": [], "entities": []}, {"text": "While translational models learn embeddings using simple operations and limited parameters, they produce low quality embeddings.", "labels": [], "entities": []}, {"text": "In contrast, CNN based models learn more expressive embeddings due to their parameter efficiency and consideration of complex relations.", "labels": [], "entities": []}, {"text": "However, both translational and CNN based models process each triple independently and hence fail to encapsulate the semantically rich and latent relations that are inherently present in the vicinity of a given entity in a KG.", "labels": [], "entities": []}, {"text": "Motivated by the aforementioned observations, we propose a generalized attention-based graph embedding for relation prediction.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.9546415507793427}]}, {"text": "For node classification, graph attention networks (GATs)) have been shown to focus on the most relevant portions of the graph, namely the node features in a 1-hop neighborhood.", "labels": [], "entities": [{"text": "node classification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.713896632194519}]}, {"text": "Given a KG and the task of relation prediction, our model generalizes and extends the attention mechanism by guiding attention to both entity (node) and relation (edge) features in a multi-hop neighborhood of a given entity / node.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7482473850250244}]}, {"text": "Our idea is: 1) to capture multi-hop relations () surrounding a given node, 2) to encapsulate the diversity of roles played by an entity in various relations, and 3) to consolidate the existing knowledge present in semantically similar relation clusters (Valverde-Rebaza and de Andrade.", "labels": [], "entities": []}, {"text": "Our model achieves these objectives by assigning different weight mass (attention) to nodes in a neighborhood and by propagating attention via layers in an iterative fashion.", "labels": [], "entities": []}, {"text": "However, as the model depth increases, the contribution of distant entities decreases exponentially.", "labels": [], "entities": []}, {"text": "To resolve this issue, we use relation composition as proposed by to introduce an auxiliary edge between nhop neighbors, which then readily allows the flow of knowledge between entities.", "labels": [], "entities": []}, {"text": "Our architecture is an encoder-decoder model where our generalized graph attention model and ConvKB  play the roles of an encoder and decoder, respectively.", "labels": [], "entities": []}, {"text": "Moreover, this method can be extended for learning effective embeddings for Textual Entailment Graphs (, where global learning has proven effective in the past as shown by and.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to learn new graph attention based embeddings that specifically target relation prediction on KGs.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8138224482536316}]}, {"text": "Secondly, we generalize and extend graph attention mechanisms to capture both entity and relation features in a multi-hop neighborhood of a given entity.", "labels": [], "entities": []}, {"text": "Finally, we evaluate our model on challenging relation prediction tasks fora wide variety of realworld datasets.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8458278477191925}]}, {"text": "Our experimental results indicate a clear and substantial improvement over stateof-the-art relation prediction methods.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.6922563910484314}]}, {"text": "For instance, our attention-based embedding achieves an improvement of 104% over the state-of-the-art method for the Hits@1 metric on the popular Freebase (FB15K-237) dataset.", "labels": [], "entities": [{"text": "Freebase (FB15K-237) dataset", "start_pos": 146, "end_pos": 174, "type": "DATASET", "confidence": 0.8818265438079834}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first provide a review of related work in Section 2 and then our detailed approach in Section 3.", "labels": [], "entities": []}, {"text": "Experimental results and dataset descriptions are reported in Section 4 followed by our conclusion and future research directions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the relation prediction task, the aim is to predict a triple (e i , r k , e j ) withe i ore j missing, i.e., predict e i given (r k , e j ) or predict e j given (e i , r k ).", "labels": [], "entities": [{"text": "relation prediction task", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.932127058506012}]}, {"text": "We generate a set of (N \u2212 1) corrupt triples for each entity e i by replacing it with every other entity e i \u2208 E \\ e i , then we assign a score to each such triple.", "labels": [], "entities": []}, {"text": "Subsequently, we sort these scores in ascending order and get the rank of a correct triple (e i , r k , e j ).", "labels": [], "entities": []}, {"text": "Similar to previous work (, , ), we evaluate all the models in a filtered setting, i.e, during ranking we remove corrupt triples which are already present in one of the training, validation, or test sets.", "labels": [], "entities": []}, {"text": "This whole process is repeated by replacing the tail entity e j , and averaged metrics are reported.", "labels": [], "entities": []}, {"text": "We report mean reciprocal rank (MRR), mean rank (MR) and the proportion of correct entities in the top N ranks (Hits@N) for N = 1, 3, and 10.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8903206487496694}, {"text": "mean rank (MR)", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.9684733510017395}]}, {"text": "significantly outperforms state-of-the-art results on five metrics for FB15k-237, and on two metrics for WN18RR.", "labels": [], "entities": [{"text": "FB15k-237", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.7728831768035889}, {"text": "WN18RR", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9695492386817932}]}, {"text": "We downloaded publicly available source codes to reproduce results of the state-ofthe-art methods 345678 on all the datasets.", "labels": [], "entities": []}, {"text": "Attention Values vs Epochs: We study the distribution of attention with increasing epochs fora particular node.", "labels": [], "entities": []}, {"text": "shows this distribution on FB15k-237.", "labels": [], "entities": [{"text": "FB15k-237", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.9869275689125061}]}, {"text": "In the initial stages of the learning process, the attention is distributed randomly.", "labels": [], "entities": []}, {"text": "As the training progresses and our model gathers more information from the neighborhood, it assigns more attention to direct neighbors and takes minor information from the more distant neighbors.", "labels": [], "entities": []}, {"text": "Once the model converges, it learns to gather multi-hop and clustered relation information from the n-hop neighborhood of the node.", "labels": [], "entities": []}, {"text": "We report results of our model on UMLS dataset in.", "labels": [], "entities": [{"text": "UMLS dataset", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9614666402339935}]}, {"text": "UMLS is a relatively smaller knowledge graph with only 135 entities and 46 relations.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9555134177207947}]}, {"text": "Although this dataset is comparable to Kinship in size, it is relatively much sparser than Kinship with mean in-degree being 38.63, as opposed to 82.15 in Kinship.", "labels": [], "entities": []}, {"text": "We show that despite the small size of the dataset, our model outperforms the baselines which indicates the robustness of our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results on WN18RR and FB15K-237 test sets. Hits@N values are in percentage. The best  score is in bold and second best score is underlined.", "labels": [], "entities": [{"text": "WN18RR", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.9597402811050415}, {"text": "FB15K-237 test sets", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.9636813402175903}]}, {"text": " Table 3: Experimental results on NELL-995 and Kinship test sets. Hits@N values are in percentage. The best  score is in bold and second best score is underlined.", "labels": [], "entities": [{"text": "NELL-995", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8056203126907349}, {"text": "Kinship test sets", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8222760558128357}]}, {"text": " Table 4: Mean PageRank \u00d710 \u22125 vs relative increase in  MRR wrt. DistMult.", "labels": [], "entities": [{"text": "Mean PageRank", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8401951193809509}, {"text": "MRR", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.6410149931907654}, {"text": "DistMult", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.8345394134521484}]}, {"text": " Table 5: Experimental results on UMLS test sets. Hits@N values are in percentage. The best score is in bold and  second best score is underlined.", "labels": [], "entities": [{"text": "UMLS test sets", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9018916289011637}]}, {"text": " Table 6: Optimal values of hyperparameters for attention model are reported on all datasets.", "labels": [], "entities": []}, {"text": " Table 7: Optimal values of hyperparameters for decoder(ConvKB) are reported on all datasets.", "labels": [], "entities": []}, {"text": " Table 8: This table shows number of n-hop paths in all datasets. We report unique paths for all values of n except  n = 1.", "labels": [], "entities": []}]}