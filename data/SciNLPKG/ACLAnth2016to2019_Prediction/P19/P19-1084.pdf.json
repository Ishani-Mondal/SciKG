{"title": [{"text": "Don't Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference", "labels": [], "entities": [{"text": "Mitigating Artifacts in Natural Language Inference", "start_pos": 36, "end_pos": 86, "type": "TASK", "confidence": 0.7950053016344706}]}], "abstractContent": [{"text": "Natural Language Inference (NLI) datasets often contain hypothesis-only biases-artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis.", "labels": [], "entities": []}, {"text": "We propose two prob-abilistic methods to build models that are more robust to such biases and better transfer across datasets.", "labels": [], "entities": []}, {"text": "In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise.", "labels": [], "entities": []}, {"text": "We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases.", "labels": [], "entities": []}, {"text": "Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets.", "labels": [], "entities": []}, {"text": "Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.8047981858253479}]}], "introductionContent": [{"text": "Natural Language Inference (NLI) is often used to gauge a model's ability to understand a relationship between two texts).", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7641002287467321}]}, {"text": "In NLI, a model is tasked with determining whether a hypothesis (a woman is sleeping) would likely be inferred from a premise (a woman is talking on the phone).", "labels": [], "entities": []}, {"text": "The development of new large-scale datasets has led to a flurry of various neural network architectures for solving NLI.", "labels": [], "entities": [{"text": "solving NLI", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.6839924454689026}]}, {"text": "However, recent work has found that many NLI datasets contain biases, or annotation artifacts, i.e., features present in hypotheses that enable models to perform surprisingly well using only the hypothesis, without learning the relationship between two texts (.", "labels": [], "entities": []}, {"text": "For instance, in some datasets, negation words like \"not\" and \"nobody\" are often associated with a relationship of contradiction.", "labels": [], "entities": []}, {"text": "As a ramification of such biases, models may not generalize well to other datasets that contain different or no such biases.", "labels": [], "entities": []}, {"text": "Recent studies have tried to create new NLI datasets that do not contain such artifacts, but many approaches to dealing with this issue remain unsatisfactory: constructing new datasets () is costly and may still result in other artifacts; filtering \"easy\" examples and defining a harder subset is useful for evaluation purposes, but difficult to do on a large scale that enables training; and compiling adversarial examples () is informative but again limited by scale or diversity.", "labels": [], "entities": []}, {"text": "Instead, our goal is to develop methods that overcome these biases as datasets may still contain undesired artifacts despite annotation efforts.", "labels": [], "entities": []}, {"text": "Typical NLI models learn to predict an entailment label discriminatively given a premisehypothesis pair), enabling them to learn hypothesis-only biases.", "labels": [], "entities": []}, {"text": "Instead, we predict the premise given the hypothesis and the entailment label, which by design cannot be solved using data artifacts.", "labels": [], "entities": []}, {"text": "While this objective is intractable, it motivates two approximate training methods for standard NLI classifiers that are more resistant to biases.", "labels": [], "entities": []}, {"text": "Our first method uses a hypothesis-only classifier  Figure 1: Illustration of (a) the baseline NLI architecture, and our two proposed methods to remove hypothesis only-biases from an NLI model: (b) uses a hypothesis-only classifier, and (c) samples a random premise.", "labels": [], "entities": []}, {"text": "Arrows correspond to the direction of propagation.", "labels": [], "entities": []}, {"text": "Green or red arrows respectively mean that the gradient sign is kept as is or reversed.", "labels": [], "entities": []}, {"text": "Gray arrow indicates that the gradient is not back-propagated -this only occurs in (c) when we randomly sample a premise, otherwise the gradient is back-propagated.", "labels": [], "entities": []}, {"text": "f and g represent encoders and classifiers.", "labels": [], "entities": []}, {"text": "We evaluate the ability of our methods to generalize better in synthetic and naturalistic settings.", "labels": [], "entities": []}, {"text": "First, using a controlled, synthetic dataset, we demonstrate that, unlike the baseline, our methods enable a model to ignore the artifacts and learn to correctly identify the desired relationship between the two texts.", "labels": [], "entities": []}, {"text": "Second, we train models on an NLI dataset that is known to be biased and evaluate on other datasets that may have different or no biases.", "labels": [], "entities": [{"text": "NLI dataset", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.870825469493866}]}, {"text": "We observe improved results compared to a fully discriminative baseline in 9 out of 12 target datasets, indicating that our methods generate models that are more robust to annotation artifacts.", "labels": [], "entities": []}, {"text": "An extensive analysis reveals that our methods are most effective when the target datasets have different biases from the source dataset or no noticeable biases.", "labels": [], "entities": []}, {"text": "We also observe that the more we encourage the model to ignore biases, the better it transfers, but this comes at the expense of performance on the source dataset.", "labels": [], "entities": []}, {"text": "Finally, we show that our methods can better exploit small amounts of training data in a target dataset, especially when it has different biases from the source data.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the transferability of our methods from biased datasets to ones having different or no biases.", "labels": [], "entities": []}, {"text": "Elsewhere ( , we have analyzed the effect of these methods on the learned language representations, suggesting that they may indeed be less biased.", "labels": [], "entities": []}, {"text": "However, we caution that complete removal of biases remains difficult and is dependent on the techniques used.", "labels": [], "entities": []}, {"text": "The choice of whether to remove bias also depends on the goal; in an in-domain scenario certain biases maybe helpful and should not necessarily be removed.", "labels": [], "entities": []}, {"text": "In summary, in this paper we make the following contributions: \u2022 Two novel methods to train NLI models that are more robust to dataset-specific artifacts.", "labels": [], "entities": []}, {"text": "\u2022 An empirical evaluation of the methods on a synthetic dataset and 12 naturalistic datasets.", "labels": [], "entities": []}, {"text": "\u2022 An extensive analysis of the effects of our methods on handling bias.", "labels": [], "entities": [{"text": "handling bias", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9218984544277191}]}], "datasetContent": [{"text": "To evaluate how well our methods can overcome hypothesis-only biases, we test our methods on a synthetic dataset as well as on a wide range of existing NLI datasets.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.7761321365833282}]}, {"text": "The scenario we aim to address is when training on a source dataset with biases and evaluating on a target dataset with different or no biases.", "labels": [], "entities": []}, {"text": "We first describe the data and experimental setup before discussing the results.", "labels": [], "entities": []}, {"text": "Synthetic Data We create a synthetic dataset based on the motivating example in Section 2, where P entails H if and only if their first letters are the same.", "labels": [], "entities": []}, {"text": "The training and test sets have 1K examples each, uniformly distributed among the possible entailment relations.", "labels": [], "entities": []}, {"text": "In the test set (dataset A), each premise or hypothesis is a single symbol: P, H \u2208 {a, b}, where P entails H iff P = H.", "labels": [], "entities": []}, {"text": "In the training set (dataset B), a letter c is appended to the hypothesis side in the TRUE examples, but not in the FALSE examples.", "labels": [], "entities": []}, {"text": "In order to transfer well to the test set, a model that is trained on this training set needs to learn the underlying relationship-that P entails H if and only if their first letter is identical-rather than relying on the presence of c in the hypothesis side.", "labels": [], "entities": []}, {"text": "Common NLI datasets Moving to existing NLI datasets, we train models on the Stanford Natural Language Inference dataset (SNLI;, since it is known to contain significant annotation artifacts.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference dataset (SNLI;", "start_pos": 76, "end_pos": 126, "type": "DATASET", "confidence": 0.8227755650877953}]}, {"text": "We evaluate the robustness of our methods on other, target datasets.", "labels": [], "entities": []}, {"text": "As target datasets, we use the 10 datasets investigated by in their hypothesisonly study, plus two test sets: GLUE's diagnostic test set, which was carefully constructed to not contain hypothesis-biases (, and SNLI-hard, a subset of the SNLI test set that is thought to have fewer biases).", "labels": [], "entities": [{"text": "GLUE's diagnostic test set", "start_pos": 110, "end_pos": 136, "type": "DATASET", "confidence": 0.9170980334281922}, {"text": "SNLI test set", "start_pos": 237, "end_pos": 250, "type": "DATASET", "confidence": 0.7940390110015869}]}, {"text": "The target datasets include humanjudged datasets that used automatic methods to pair premises and hypotheses, and then relied on humans to label the pairs: SCITAIL (, ADD-ONE-RTE and passed to an MLP classifier with one hidden layer.", "labels": [], "entities": []}, {"text": "Our proposed Detailed descriptions of these datasets can be found in.", "labels": [], "entities": []}, {"text": "We leave additional NLI datasets, such as the Diverse NLI Collection (), for future work.", "labels": [], "entities": [{"text": "Diverse NLI Collection", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.6252924601236979}]}, {"text": "11 Many NLI models encode P and H separately (, although some share information between the encoders via attention (.", "labels": [], "entities": []}, {"text": "Specifically, representations are concatenated, subtracted, and multiplied element-wise.", "labels": [], "entities": []}, {"text": "methods for mitigating biases use the same technique for representing and combining sentences.", "labels": [], "entities": [{"text": "mitigating biases", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.9272818267345428}]}, {"text": "Additional implementation details are provided in Appendix A.2.", "labels": [], "entities": [{"text": "Appendix A.2", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.806963324546814}]}, {"text": "For both methods, we sweep hyper-parameters \u03b1, \u03b2 over {0.05, 0.1, 0.2, 0.4, 0.8, 1.0}.", "labels": [], "entities": []}, {"text": "For each target dataset, we choose the best-performing model on its development set and report results on the test set.", "labels": [], "entities": []}, {"text": "To examine how well our methods work in a controlled setup, we train on the biased dataset (B), but evaluate on the unbiased test set (A).", "labels": [], "entities": []}, {"text": "As expected, without a method to remove hypothesisonly biases, the baseline fails to generalize to the test set.", "labels": [], "entities": []}, {"text": "Examining its predictions, we found that the baseline model learned to rely on the presence/absence of the bias term c, always predicting TRUE/FALSE respectively.", "labels": [], "entities": [{"text": "TRUE", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.982646644115448}, {"text": "FALSE", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.9352220892906189}]}, {"text": "shows the results of our two proposed methods.", "labels": [], "entities": []}, {"text": "As we increase the hyper-parameters \u03b1 and \u03b2, our methods initially behave like the baseline, learning the training set but failing on the test set.", "labels": [], "entities": []}, {"text": "However, with strong enough hyperparameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set.", "labels": [], "entities": []}, {"text": "For Method 1, stronger hyper-parameters work better.", "labels": [], "entities": []}, {"text": "Method 2, in particular, breaks down with too many random samples (increasing \u03b1), as expected.", "labels": [], "entities": []}, {"text": "We also found that Method 1 did not require as strong \u03b2 as Method 2.", "labels": [], "entities": []}, {"text": "From the synthetic experiments, it seems that Method 1 learns to ignore the bias c and learn the desired relationship between P and H across many configurations, while Method 2 requires much stronger \u03b2.", "labels": [], "entities": []}, {"text": "(left block) reports the results of our proposed methods compared to the baseline in application to the NLI datasets.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.9478845000267029}]}, {"text": "The method using the hypothesis-only classifier to remove hypothesis-only biases from the model (Method 1) outperforms the baseline in 9 out of 12 target datasets (\u2206 > 0), though most improvements are small.", "labels": [], "entities": []}, {"text": "The training method using negative sampling (Method 2) only outperforms the baseline in 5 datasets, 4 of which are cases where the other method also outperformed the baseline.", "labels": [], "entities": []}, {"text": "These gains are much larger than those of Method 1.", "labels": [], "entities": []}, {"text": "We also report results of the proposed methods on the SNLI test set (right block).", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9196982582410177}]}, {"text": "As our results improve on the target datasets, we note that Method 1's performance on SNLI does not drastically decrease (small \u2206), even when the improvement on the target dataset is large (for example, in SPR).", "labels": [], "entities": []}, {"text": "For this method, the performance on SNLI drops by just an average of 1.11 (0.65 STDV).", "labels": [], "entities": []}, {"text": "For Method 2, there is a large decrease on SNLI as results drop by an average of 11.19 (12.71 STDV).", "labels": [], "entities": [{"text": "SNLI", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.6433512568473816}]}, {"text": "For these models, when we see large improvement on a target dataset, we often see a large drop on SNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.654534637928009}]}, {"text": "For example, on ADD-ONE-RTE, Method 2 outperforms the baseline by roughly 17% but performs almost 50% lower on SNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.8448626399040222}]}, {"text": "Based on this, as well as the results on the synthetic dataset, Method 2 seems to be much more unstable and highly dependent on the right hyper-parameters.", "labels": [], "entities": []}, {"text": "Our main goal is to determine whether our methods help a model perform well across multiple datasets by ignoring dataset-specific artifacts.", "labels": [], "entities": []}, {"text": "In turn, we did not update the models' parameters on other datasets.", "labels": [], "entities": []}, {"text": "But, what if we are given different amounts of training data fora new NLI dataset?", "labels": [], "entities": [{"text": "NLI dataset", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.8575763702392578}]}, {"text": "To determine if our approach is still helpful, we updated four models on increasing sizes of training data from two target datasets (MNLI and SICK).", "labels": [], "entities": [{"text": "MNLI", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.9340970516204834}]}, {"text": "All three training approaches-the baseline, Method 1, and Method 2-are used to pretrain a model on SNLI and fine-tune on the target dataset.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 99, "end_pos": 103, "type": "TASK", "confidence": 0.7918391227722168}]}, {"text": "The fourth model is the baseline trained only on the target dataset.", "labels": [], "entities": []}, {"text": "Both MNLI and SICK have the same label spaces as SNLI, allowing us to hold that variable constant.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.9039819240570068}]}, {"text": "We use SICK because our methods resulted in good gains on it.", "labels": [], "entities": []}, {"text": "MNLI's large training set allows us to consider a wide range of training set sizes.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9724400043487549}]}, {"text": "16 shows the results on the dev sets.", "labels": [], "entities": []}, {"text": "In MNLI, pre-training is very helpful when finetuning on a small amount of new training data, although there is little to no gain from pre-training with either of our methods compared to the baseline.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.5178921222686768}]}, {"text": "This is expected, as we saw relatively small gains with the proposed methods on MNLI, and can be explained by SNLI and MNLI having similar biases.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.7157676219940186}, {"text": "MNLI", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.9077131152153015}]}, {"text": "In SICK, pre-training with either of our We holdout 10K examples from the training set for dev as gold labels for the MNLI test set are not publicly available.", "labels": [], "entities": [{"text": "MNLI test set", "start_pos": 118, "end_pos": 131, "type": "DATASET", "confidence": 0.9780961473782858}]}, {"text": "We evaluate on MNLI's matched dev set to assure consistent domains when fine-tuning.", "labels": [], "entities": [{"text": "MNLI's matched dev set", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.9001164674758911}]}, {"text": "methods is better inmost data regimes, especially with very small amounts of target training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies on the synthetic dataset, when training on the biased training set and evaluating on the unbiased  test set. Darker boxes represent higher accuracies.  *  indicates failure to learn the biased training set; all other  configurations learned the training set perfectly.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9979950189590454}]}, {"text": " Table 2: Accuracy results of transferring representations to new datasets. In all cases the models are trained on  SNLI. Left: baseline results on target test sets and differences between the proposed methods and the baseline.  Right: test results on SNLI with the models that performed best on each target dataset's dev set. \u2206 are absolute  differences between the method and the baseline on each target test set (left) or between the method and the  baseline performance (84.22) on SNLI test (right). Black rectangles show relative changes in each column.", "labels": [], "entities": [{"text": "SNLI test", "start_pos": 485, "end_pos": 494, "type": "DATASET", "confidence": 0.9306375980377197}]}, {"text": " Table 3: Results with stronger hyper-parameters for  Method 1 vs. the baseline. \u2206's are absolute differences.", "labels": [], "entities": []}]}