{"title": [{"text": "A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity", "labels": [], "entities": []}], "abstractContent": [{"text": "Cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space.", "labels": [], "entities": []}, {"text": "An important requirement for many downstream tasks is that word similarity should be independent of language-i.e., word vectors within one language should not be more similar to each other than to words in another language.", "labels": [], "entities": []}, {"text": "We measure this characteristic using modular-ity, a network measurement that measures the strength of clusters in a graph.", "labels": [], "entities": []}, {"text": "Modularity has a moderate to strong correlation with three downstream tasks, even though modularity is based only on the structure of embeddings and does not require any external resources.", "labels": [], "entities": []}, {"text": "We show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embed-dings, particularly on distant language pairs in low-resource settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The success of monolingual word embeddings in natural language processing) has motivated extensions to cross-lingual settings.", "labels": [], "entities": []}, {"text": "Cross-lingual word embeddings-where multiple languages share a single distributed representation-work well for classification) and machine translation (, even with few bilingual pairs ( or no supervision at all (.", "labels": [], "entities": [{"text": "classification", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.9714897274971008}, {"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.8300127983093262}]}, {"text": "Typically the quality of cross-lingual word embeddings is measured with respect to how well they improve a downstream task.", "labels": [], "entities": []}, {"text": "However, sometimes it is not possible to evaluate embeddings fora specific downstream task, for example a future task that does not yet have data or on a rare language that does not have resources to support traditional evaluation.", "labels": [], "entities": []}, {"text": "In such settings, it is useful to have an intrinsic evaluation metric: a metric that looks at the embedding space itself to know whether the embedding is good without resorting to an extrinsic task.", "labels": [], "entities": []}, {"text": "While extrinsic tasks are the ultimate arbiter of whether cross-lingual word embeddings work, intrinsic metrics are useful for low-resource languages where one often lacks the annotated data that would make an extrinsic evaluation possible.", "labels": [], "entities": []}, {"text": "However, few intrinsic measures exist for crosslingual word embeddings, and those that do exist require external linguistic resources (e.g., sensealigned corpora in).", "labels": [], "entities": [{"text": "crosslingual word embeddings", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6797526478767395}]}, {"text": "The requirement of language resources makes this approach limited or impossible for low-resource languages, which are the languages where intrinsic evaluations are most needed.", "labels": [], "entities": []}, {"text": "Moreover, requiring language resources can bias the evaluation toward words in the resources rather than evaluating the embedding space as a whole.", "labels": [], "entities": []}, {"text": "Our solution involves a graph-based metric that considers the characteristics of the embedding space without using linguistic resources.", "labels": [], "entities": []}, {"text": "To sketch the idea, imagine a cross-lingual word embedding space where it is possible to draw a hyperplane that separates all word vectors in one language from all vectors in another.", "labels": [], "entities": []}, {"text": "Without knowing anything about the languages, it is easy to see that this is a problematic embedding: the representations of the two languages are in distinct parts of the space rather than using a shared space.", "labels": [], "entities": []}, {"text": "While this example is exaggerated, this characteristic where vectors are clustered by language often appears within smaller neighborhoods of the embedding space, we want to discover these clusters.", "labels": [], "entities": []}, {"text": "To measure how well word embeddings are mixed across languages, we draw on concepts from network science.", "labels": [], "entities": []}, {"text": "Specifically, some cross- (b) high modularity: An example of a low modularity (languages mixed) and high modularity cross-lingual word embedding lexical graph using k-nearest neighbors of \"eat\" (left) and \"firefox\" (right) in English and Japanese.", "labels": [], "entities": []}, {"text": "lingual word embeddings are modular by language: vectors in one language are consistently closer to each other than vectors in another language ().", "labels": [], "entities": []}, {"text": "When embeddings are modular, they often fail on downstream tasks (Section 2).", "labels": [], "entities": []}, {"text": "Modularity is a concept from network theory (Section 3); because network theory is applied to graphs, we turn our word embeddings into a graph by connecting nearest-neighbors-based on vector similarity-to each other.", "labels": [], "entities": []}, {"text": "Our hypothesis is that modularity will predict how useful the embedding is in downstream tasks; low-modularity embeddings should work better.", "labels": [], "entities": []}, {"text": "We explore the relationship between modularity and three downstream tasks (Section 4) that use cross-lingual word embeddings differently: (i) cross-lingual document classification; (ii) bilingual lexical induction in Italian, Japanese, Spanish, and Danish; and (iii) low-resource document retrieval in Hungarian and Amharic, finding moderate to strong negative correlations between modularity and performance.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 142, "end_pos": 179, "type": "TASK", "confidence": 0.6399211088816324}, {"text": "bilingual lexical induction", "start_pos": 186, "end_pos": 213, "type": "TASK", "confidence": 0.6239609022935232}, {"text": "low-resource document retrieval", "start_pos": 267, "end_pos": 298, "type": "TASK", "confidence": 0.6131226122379303}]}, {"text": "Furthermore, using modularity as a validation metric (Section 5) makes MUSE (), an unsupervised model, more robust on distant language pairs.", "labels": [], "entities": []}, {"text": "Compared to other existing intrinsic evaluation metrics, modularity captures complementary properties and is more predictive of downstream performance despite needing no external resources (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "There are many approaches to training crosslingual word embeddings.", "labels": [], "entities": []}, {"text": "This section reviews the embeddings we consider in this paper, along with existing work on evaluating those embeddings.", "labels": [], "entities": []}, {"text": "We now investigate whether modularity can predict the effectiveness of cross-lingual word embeddings on three downstream tasks: (i) cross-lingual document classification, (ii) bilingual lexical induction, and (iii) document retrieval in low-resource languages.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 132, "end_pos": 169, "type": "TASK", "confidence": 0.6996186276276907}, {"text": "bilingual lexical induction", "start_pos": 176, "end_pos": 203, "type": "TASK", "confidence": 0.7406823833783468}, {"text": "document retrieval", "start_pos": 215, "end_pos": 233, "type": "TASK", "confidence": 0.7815180718898773}]}, {"text": "If modularity correlates with task performance, it can characterize embedding quality.", "labels": [], "entities": []}, {"text": "The experiments so far show that modularity captures whether an embedding is useful, which suggests that modularity could be used as an intrinsic evaluation or validation metric.", "labels": [], "entities": []}, {"text": "Here, we investigate whether modularity can capture distinct information compared to existing evaluation measures: QVEC-CCA (Ammar et al., 2016), CSLS (, and cosine similarity between translation pairs (Section 6.1).", "labels": [], "entities": [{"text": "QVEC-CCA", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.7052367925643921}]}, {"text": "We also analyze the effect of the number of nearest neighbors k (Section 6.2).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average classification accuracy on (EN \u2192  DA, ES, IT, JA) along with the average modularity of  five cross-lingual word embeddings. MUSE has the best  accuracy, captured by its low modularity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9652996063232422}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9974647760391235}]}, {"text": " Table 3: Nearest neighbors in an EN-JA embedding.  Unlike the JA word \"market\", the JA word \"closing  price\" has no EN vector nearby.", "labels": [], "entities": []}, {"text": " Table 4: Average precision@1 on (EN \u2192 DA, ES, IT,  JA) along with the average modularity of the cross- lingual word embeddings trained with different meth- ods. VECMAP scores the best P@1, which is captured  by its low modularity.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9834772944450378}]}, {"text": " Table 5: Correlation between modularity and AUC on  document retrieval. It shows a moderate correlation to  this task.", "labels": [], "entities": [{"text": "AUC", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9382086396217346}, {"text": "document retrieval", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7199333012104034}]}, {"text": " Table 6: BLI results (P@1 \u00d7100%) from EN to each  target language with different validation metrics for  MUSE: default (CSLS-10K) and modularity (Mod-10K).  We report the average (Avg.) and the best (Best) from  ten runs with ten random seeds for each validation met- ric. Bold values are mappings that are not shared be- tween the two validation metrics. Mod-10K improves  the robustness of MUSE on distant language pairs.", "labels": [], "entities": [{"text": "BLI", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9845467209815979}, {"text": "Avg.", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9650274515151978}]}]}