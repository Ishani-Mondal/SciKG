{"title": [{"text": "Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9748262763023376}, {"text": "Text Generation", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7886957824230194}]}], "abstractContent": [{"text": "We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks that transform any inputs into natural language, such as machine translation, sum-marization, dialog, content manipulation, and so forth.", "labels": [], "entities": [{"text": "Texar", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8897068500518799}, {"text": "text generation", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.732624739408493}, {"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.697742685675621}, {"text": "content manipulation", "start_pos": 202, "end_pos": 222, "type": "TASK", "confidence": 0.738829180598259}]}, {"text": "With the design goals of modularity, versatility, and extensibility in mind, Texar extracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules and functionalities, and allows arbitrary model architectures and algorithmic paradigms.", "labels": [], "entities": []}, {"text": "In Texar, model architecture , inference, and learning processes are properly decomposed.", "labels": [], "entities": [{"text": "Texar", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9365416169166565}]}, {"text": "Modules at a high concept level can be freely assembled or plugged in/swapped out.", "labels": [], "entities": []}, {"text": "Texar is thus particularly suitable for researchers and practitioners to do fast prototyping and experimentation.", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9790325164794922}]}, {"text": "The versatile toolkit also fosters technique sharing across different text generation tasks.", "labels": [], "entities": [{"text": "text generation", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7002311646938324}]}, {"text": "Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io.", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9423888325691223}]}], "introductionContent": [{"text": "Text generation spans abroad set of natural language processing tasks that aim to generate natural language from input data or machine representations.", "labels": [], "entities": [{"text": "Text generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7449801862239838}]}, {"text": "Such tasks include machine translation (), dialog systems (), text summarization), text paraphrasing and manipulation, and more.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8185723423957825}, {"text": "text summarization", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7847115993499756}, {"text": "text paraphrasing and manipulation", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.8321343213319778}]}, {"text": "Recent years have seen rapid progress of this active area, in part due to the integration of modern deep learning approaches in many of the tasks.", "labels": [], "entities": []}, {"text": "On the other hand, considerable research efforts are still needed in order to improve techniques and enable realworld applications.", "labels": [], "entities": []}, {"text": "A few remarkable open-source toolkits have been developed (section 2) which largely focus on one or a few specific tasks or algorithms.", "labels": [], "entities": []}, {"text": "Emerging new applications and approaches instead are often developed by individual teams in a more adhoc manner, which can easily result in hard-tomaintain custom code and duplicated efforts.", "labels": [], "entities": []}, {"text": "The variety of text generation tasks indeed have many common properties and share a set of key underlying techniques, such as neural encoderdecoders (), attentions, memory networks (, adversarial methods (, reinforcement learning (, structured supervision ( , as well as optimization techniques, data pre-processing and result post-processing, evaluations, etc.", "labels": [], "entities": [{"text": "text generation tasks", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7861197590827942}]}, {"text": "These techniques are often combined together in various ways to tackle different problems.", "labels": [], "entities": []}, {"text": "summarizes examples of various model architectures.", "labels": [], "entities": []}, {"text": "It is therefore highly desirable to have an opensource platform that unifies the development of the diverse yet closely-related applications, backed with clean and consistent implementations of the core algorithms.", "labels": [], "entities": []}, {"text": "Such a platform would enable reuse of common components; standardize design, implementation, and experimentation; foster reproducibility; and importantly, encourage technique sharing among tasks so that an algorithmic advance developed fora specific task can quickly be evaluated and generalized to many others.", "labels": [], "entities": []}, {"text": "We introduce Texar, a general-purpose text generation toolkit aiming to support popular and emerging applications in the field, by providing researchers and practitioners a unified and D \u00ed \u00b5\u00ed\u00b1\u00a5 \u00ed \u00b5\u00ed\u00b1\u00a6  flexible framework for building their models.", "labels": [], "entities": [{"text": "Texar", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8568382263183594}, {"text": "text generation toolkit", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7572351296742758}, {"text": "D \u00ed \u00b5\u00ed\u00b1\u00a5 \u00ed \u00b5\u00ed", "start_pos": 185, "end_pos": 198, "type": "METRIC", "confidence": 0.9008262356122335}]}, {"text": "Texar has two versions, building upon TensorFlow (tensorflow.org) and PyTorch (pytorch. org), respectively, with the same uniform design.", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9706082344055176}]}, {"text": "Underlying the core of Texar's design is principled anatomy of extensive text generation models and learning algorithms, which subsumes the diverse cases in and beyond, enabling a unified formulation and consistent implementation.", "labels": [], "entities": [{"text": "Texar", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8310686945915222}]}, {"text": "Texar emphasizes three key properties: Versatility.", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9669514298439026}, {"text": "Versatility", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.986958920955658}]}, {"text": "Texar contains a wide range of features and functionalities for 1) arbitrary model architectures as a combination of encoders, decoders, embedders, discriminators, memories, and many other modules; and 2) different modeling and learning paradigms such as sequence-tosequence, probabilistic models, adversarial methods, and reinforcement learning.", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9616285562515259}]}, {"text": "Based on these, both workhorse and cutting-edge solutions to the broad spectrum of text generation tasks are either already included or can be easily constructed.", "labels": [], "entities": [{"text": "text generation tasks", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.8331718444824219}]}, {"text": "Users can construct models at a high conceptual level just like assembling building blocks.", "labels": [], "entities": []}, {"text": "It is convenient to plugin or swap out modules, configure rich module options, or even switch between distinct modeling paradigms.", "labels": [], "entities": []}, {"text": "For example, switching from adversarial learning to reinforcement learning involves only minimal code changes (e.g.,).", "labels": [], "entities": []}, {"text": "Modularity makes Texar particularly suitable for fast prototyping and experimentation.", "labels": [], "entities": [{"text": "Texar", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.8304688334465027}]}, {"text": "The toolkit provides interfaces ranging from simple configuration files to full library APIs.", "labels": [], "entities": []}, {"text": "Users of different needs and expertise are free to choose different interfaces for appropriate programmability and internal accessibility.", "labels": [], "entities": []}, {"text": "The library APIs are fully compatible with the native TensorFlow/PyTorch interfaces, which allows seamless integration of user-customized modules, and enables the toolkit to take advantage of the vibrant open-source community by effortlessly importing any external components as needed.", "labels": [], "entities": []}, {"text": "Furthermore, Texar emphasizes on wellstructured code, clean documentation, rich tutorial examples, and distributed GPU training.", "labels": [], "entities": [{"text": "Texar", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.9738662242889404}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Top: Transformer vs LSTM for VAE LM.  Perplexity (PPL) and sentence negative log likelihood  (NLL) are evaluated (The lower the better). Bottom:", "labels": [], "entities": [{"text": "VAE LM", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.730569452047348}, {"text": "sentence negative log likelihood  (NLL)", "start_pos": 69, "end_pos": 108, "type": "METRIC", "confidence": 0.7786314700331006}]}]}