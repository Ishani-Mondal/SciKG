{"title": [{"text": "Evaluating Discourse in Structured Text Representations", "labels": [], "entities": [{"text": "Evaluating Discourse in Structured Text Representations", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.643888587752978}]}], "abstractContent": [{"text": "Discourse structure is integral to understanding a text and is helpful in many NLP tasks.", "labels": [], "entities": []}, {"text": "Learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data.", "labels": [], "entities": []}, {"text": "Liu and Lapata (2018) propose a structured attention mechanism for text classification that derives a tree over a text, akin to an RST discourse tree.", "labels": [], "entities": [{"text": "text classification", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7453465163707733}, {"text": "RST discourse tree", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.8523167173067728}]}, {"text": "We examine this model in detail, and evaluate on additional discourse-relevant tasks and datasets, in order to assess whether the struc-tured attention improves performance on the end task and whether it captures a text's discourse structure.", "labels": [], "entities": []}, {"text": "We find the learned latent trees have little to no structure and instead focus on lexical cues; even after obtaining more structured trees with proposed model modifications , the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser.", "labels": [], "entities": []}, {"text": "Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse describes how a document is organized, and how discourse units are rhetorically connected to each other.", "labels": [], "entities": []}, {"text": "Taking into account this structure has shown to help many NLP end tasks, including summarization (, machine translation (, and sentiment analysis.", "labels": [], "entities": [{"text": "summarization", "start_pos": 83, "end_pos": 96, "type": "TASK", "confidence": 0.9928727149963379}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8081161379814148}, {"text": "sentiment analysis", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.9463476836681366}]}, {"text": "However, annotating discourse requires considerable effort by trained experts and may not always yield a structure appropriate for the end task.", "labels": [], "entities": []}, {"text": "As a result, having a model induce the discourse structure of a text is an attractive option.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to evaluate such an induced structure.", "labels": [], "entities": []}, {"text": "Inducing structure has been a recent popular approach in syntax (.", "labels": [], "entities": [{"text": "Inducing structure", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8596872091293335}]}, {"text": "Evaluations of these latent trees have shown they are inconsistent, shallower than their explicitly parsed counterparts (Penn Treebank parses) and do not resemble any linguistic syntax theory (.", "labels": [], "entities": [{"text": "Penn Treebank parses", "start_pos": 121, "end_pos": 141, "type": "DATASET", "confidence": 0.9722869793574015}]}, {"text": "For discourse, Liu and Lapata (2018) (L&L) induce a document-level structure while performing text classification with a structured attention that is constrained to resolve to a non-projective dependency tree.", "labels": [], "entities": [{"text": "text classification", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7201600968837738}]}, {"text": "We evaluate the document-level structure induced by this model.", "labels": [], "entities": []}, {"text": "In order to compare the induced structure to existing linguisticallymotivated structures, we choose Rhetorical Structure Theory (RST) (, a widely-used framework for discourse structure, because it also produces tree-shaped structures.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.7362438042958578}]}, {"text": "We evaluate on some of the same tasks as L&L, but add two more tasks we theorize to be more discourse-sensitive: text classification of writing quality, and sentence order discrimination (as proposed by).", "labels": [], "entities": [{"text": "text classification", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7717584073543549}, {"text": "sentence order discrimination", "start_pos": 157, "end_pos": 186, "type": "TASK", "confidence": 0.6945379773775736}]}, {"text": "Our research uncovers multiple negative results.", "labels": [], "entities": []}, {"text": "We find that, contrary to L&L, the structured attention does not help performance inmost cases; further, the model is not learning discourse.", "labels": [], "entities": []}, {"text": "Instead, the model learns trees with little to no structure heavily influenced by lexical cues to the task.", "labels": [], "entities": []}, {"text": "In an effort to induce better trees, we propose several principled modifications to the model, some of which yield more structured trees.", "labels": [], "entities": []}, {"text": "However, even the more structured trees bear little resemblance to ground truth RST trees.", "labels": [], "entities": [{"text": "RST", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.7053670883178711}]}, {"text": "We conclude the model holds promise, but re- quires moving beyond text classification, and injecting supervision (as in).", "labels": [], "entities": [{"text": "text classification", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7722538709640503}]}, {"text": "Our contributions are (1) comprehensive performance results on existing and additional tasks and datasets showing document-level structured attention is largely unhelpful, (2) in-depth analyses of induced trees showing they do not represent discourse, and (3) several principled model changes to produce better structures but that still do not resemble the structure of discourse.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the model on four text classification tasks and one sentence order discrimination task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7812910079956055}, {"text": "sentence order discrimination", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.6826007664203644}]}, {"text": "Details and statistics are included in Yelp (in L&L, 5-way classification) comprises customer reviews from the Yelp Dataset Challenge (collected by).", "labels": [], "entities": [{"text": "Yelp", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.9599814414978027}, {"text": "Yelp Dataset Challenge", "start_pos": 111, "end_pos": 133, "type": "DATASET", "confidence": 0.963383674621582}]}, {"text": "Each review is labeled with a 1 to 5 rating (least to most positive).", "labels": [], "entities": []}, {"text": "Debates (in L&L, binary classification) are transcribed debates on Congressional bills from the U.S. House of Representatives (compiled by, preprocessed by Yogatama and Smith).", "labels": [], "entities": [{"text": "binary classification) are transcribed debates on Congressional bills from the U.S. House of Representatives", "start_pos": 17, "end_pos": 125, "type": "TASK", "confidence": 0.8002499977747599}]}, {"text": "Each speech is labeled with 1 or 0 indicating whether the speaker voted in favor of or against the bill.", "labels": [], "entities": []}, {"text": "Writing quality (WQ) (not in L&L, binary classification) contains science articles from the New York Times (extracted from).", "labels": [], "entities": []}, {"text": "Each article is labeled as either 'very good' or 'typical' to describe its writing quality.", "labels": [], "entities": []}, {"text": "While both classes contain well-written text, Louis and Nenkova (2013) find features associated with discourse including sentiment, readability, along with PDTB-style discourse relations are helpful in distinguishing between the two classes.", "labels": [], "entities": []}, {"text": "Writing quality with topic control (WQTC) (not in L&L, binary classification) is similar to WQ, but controlled for topic using a topic similarity list included with the WQ source corpus.", "labels": [], "entities": [{"text": "WQ source corpus", "start_pos": 169, "end_pos": 185, "type": "DATASET", "confidence": 0.9319067597389221}]}, {"text": "Wall Street Journal Sentence Order (WSJSO) (not in L&L, sentence order discrimination) is the WSJ portion of PTB ().", "labels": [], "entities": [{"text": "Wall Street Journal Sentence Order (WSJSO)", "start_pos": 0, "end_pos": 42, "type": "DATASET", "confidence": 0.9170453548431396}, {"text": "sentence order discrimination", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.6338039636611938}, {"text": "WSJ", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.6803523302078247}, {"text": "PTB", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.5113242268562317}]}], "tableCaptions": [{"text": " Table 1: Max | mean (standard deviation) accuracy on the test set averaged across four training runs with different  initialization weights. Bolded numbers are within 1 standard deviation of the best performing model. L&L(orig)  uses the original L&L code; L&L(ours) includes the design change and bug fix. L&L(reported) lists results re- ported by L&L on a single training run.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.7219204902648926}]}, {"text": " Table 2: Statistics for learned trees averaged across  four runs (similar results without the design change or  bug fix are in the Appendix", "labels": [], "entities": []}, {"text": " Table 4: Mean test accuracy and tree statistics on the  WQTC dev set (averaged across four runs). -biLSTM  removes the document-level biLSTM, +w uses the  weighted sum, +p performs 1 extra percolation, and  +4p does 4 levels of percolation. The last row are  ('gold') parsed RST discourse dependency trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.5365945100784302}, {"text": "WQTC dev set", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9839818278948466}]}, {"text": " Table 5: Statistics for the datasets used in the text classification and discrimination tasks (calculated after prepro- cessing). For WSJSO, the number of generated pairs are in parentheses.", "labels": [], "entities": [{"text": "text classification and discrimination tasks", "start_pos": 50, "end_pos": 94, "type": "TASK", "confidence": 0.7959216594696045}, {"text": "WSJSO", "start_pos": 135, "end_pos": 140, "type": "DATASET", "confidence": 0.8519237041473389}]}, {"text": " Table 7: Max | mean (standard deviation) test accuracy and tree statistics of the WQTC dev set (averaged across  four training runs with different initialization weights). Bolded numbers are within 1 standard deviation of the  best performing model. +w uses the weighted sum, +p adds 1 extra level of percolation, +4p adds 4 levels of  percolation. The last row are the ('gold') parsed RST discourse dependency trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9476454854011536}, {"text": "WQTC dev set", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.9447029034296671}]}]}