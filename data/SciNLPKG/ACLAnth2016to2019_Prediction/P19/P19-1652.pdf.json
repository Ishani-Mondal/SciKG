{"title": [{"text": "Bridging by Word: Image-Grounded Vocabulary Construction for Visual Captioning", "labels": [], "entities": [{"text": "Visual Captioning", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7345228493213654}]}], "abstractContent": [{"text": "Existing research for visual captioning usually employs a CNN-RNN architecture that combines a CNN for image encoding with a RNN for caption generation, where the vocabulary is constructed from the entire training dataset as the decoding space.", "labels": [], "entities": [{"text": "visual captioning", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7558637261390686}, {"text": "image encoding", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.7243358194828033}, {"text": "caption generation", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.8240983784198761}]}, {"text": "Such approaches typically suffer from the problem of generating N-grams which occur frequently in the training set but are irrelevant to the given image.", "labels": [], "entities": []}, {"text": "To tackle this problem, we propose to construct an image-grounded vocabulary that leverages image semantics for more effective caption generation.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8837014138698578}]}, {"text": "More concretely, a two-step approach is proposed to construct the vocabulary by incorporating both visual information and relationships among words.", "labels": [], "entities": []}, {"text": "Two strategies are then explored to utilize the constructed vocabulary for caption generation.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9197022616863251}]}, {"text": "One constrains the generator to select words from the image-grounded vocabulary only and the other integrates the vocabulary information into the RNN cell during the caption generation process.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.877380907535553}]}, {"text": "Experimental results on two public datasets show the effectiveness of our framework compared to state-of-the-art models.", "labels": [], "entities": []}, {"text": "Our code is available on Github 1 .", "labels": [], "entities": [{"text": "Github 1", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8895205855369568}]}], "introductionContent": [{"text": "Recent years have witnessed growing popularity of research in multimodal learning across vision and language.", "labels": [], "entities": []}, {"text": "Image captioning (, one of the most widely studied multimodal tasks, aims at constructing a short text description given an image.", "labels": [], "entities": [{"text": "Image captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7488617300987244}]}, {"text": "current Neural Network (RNN) for caption generation (.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9685051739215851}]}, {"text": "Although impressive results have been achieved, existing models suffer from the problem of generating N-grams which occurred frequently in the training set but are irrelevant to the particular given image).", "labels": [], "entities": []}, {"text": "As we can see, the N-gram \"a woman sitting at a table\" is generated mistakenly for both images.", "labels": [], "entities": []}, {"text": "This is because when generating a text sequence, the RNN-based generator tends to ignore the semantic meaning encoded in the given image and instead generate the text sequences that occurred most often in the training set.", "labels": [], "entities": []}, {"text": "Although different image grounding strategies have been proposed to address this problem, they usually consider visual information as external features for caption generation via various attention mechanisms (.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.9460951685905457}]}, {"text": "We argue that visual information should be embedded into the generation process in a more principled way.", "labels": [], "entities": []}, {"text": "In the CNN-RNN architecture, the RNN-based generator constructs image captions word byword.", "labels": [], "entities": [{"text": "RNN-based generator constructs image captions word byword", "start_pos": 33, "end_pos": 90, "type": "TASK", "confidence": 0.5802632101944515}]}, {"text": "In each step, a word is selected from the vocabulary built on the entire training set.", "labels": [], "entities": []}, {"text": "Generally, the size of the full vocabulary is on the order of 10 4 . When describing a particular image, the possible words to be used should be drawn from a much smaller word set.", "labels": [], "entities": []}, {"text": "As an illustration, we show in the statistics of the number of distinct words in human-generated captions for images from MS-COCO ().", "labels": [], "entities": []}, {"text": "We can see that the average size of the pool of words used for the description of a particular image is around 30.", "labels": [], "entities": []}, {"text": "Based on this observation, we speculate that if we can efficiently constrain the word selection space during the image caption generation process, we should be able to address the irrelevant N-gram problem.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.7958822647730509}]}, {"text": "In this paper, we propose to construct an imagegrounded vocabulary as away to leverage the image semantics for image captioning.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.708562582731247}]}, {"text": "For vocabulary construction, we propose a two-step approach which incorporates both visual semantics and the relations among words.", "labels": [], "entities": [{"text": "vocabulary construction", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8049440383911133}]}, {"text": "For text generation, we explore two strategies to utilize the constructed vocabulary.", "labels": [], "entities": [{"text": "text generation", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8208339512348175}]}, {"text": "One uses the vocabulary as a hard constraint and the other encodes the weight of each word obtained from the image-grounded vocabulary into the RNN cell as a soft constraint.", "labels": [], "entities": []}, {"text": "Experimental results on two public datasets show the effectiveness of using image-grounded vocabulary for visual captioning compared to several stateof-the-art approaches in terms of automatic evaluation metrics.", "labels": [], "entities": [{"text": "visual captioning", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7811167538166046}]}, {"text": "Further analysis reveals that our model has the advantage of generating more novel captions compared to existing approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed framework on MS-COCO () and.", "labels": [], "entities": []}, {"text": "In MS-COCO, there are 113,287 images in the training set and 5,000 images in both of the validation and test sets.", "labels": [], "entities": []}, {"text": "In Flickr30k, the number of images for the training, validation and test sets is 29,000, 1,000 and 1,000, respectively.", "labels": [], "entities": [{"text": "Flickr30k", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.9343944191932678}]}, {"text": "Each image contains 5 human annotated captions.", "labels": [], "entities": []}, {"text": "We split the dataset following the process described in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall performance of different models for image captioning, where B-4, R, M and C are short for  BLEU-4, ROUGE, METEOR and CIDEr-D scores, respectively. Numbers in bold denote the best performance in  each column.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7621709406375885}, {"text": "BLEU-4", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9980677962303162}, {"text": "ROUGE", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.9703262448310852}, {"text": "METEOR", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9859106540679932}]}, {"text": " Table 2: Performance of different models with various  vocabulary constructors on MS COCO. R, P, B-4 and  C are short for recall, precision, BLEU-4 and CIDEr-D  respectively.", "labels": [], "entities": [{"text": "MS COCO", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9015194177627563}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9991536140441895}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9989421963691711}, {"text": "BLEU-4", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9984429478645325}]}]}