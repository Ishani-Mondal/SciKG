{"title": [{"text": "Explicit Utilization of General Knowledge in Machine Reading Comprehension", "labels": [], "entities": [{"text": "Explicit Utilization of General Knowledge", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8188790202140808}, {"text": "Machine Reading Comprehension", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.7386004527409872}]}], "abstractContent": [{"text": "To bridge the gap between Machine Reading Comprehension (MRC) models and human beings, which is mainly reflected in the hunger for data and the robustness to noise, in this paper, we explore how to integrate the neu-ral networks of MRC models with the general knowledge of human beings.", "labels": [], "entities": [{"text": "Machine Reading Comprehension (MRC)", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.708381806810697}]}, {"text": "On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage-question pair.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9526798129081726}]}, {"text": "On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms.", "labels": [], "entities": [{"text": "MRC", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9668679237365723}]}, {"text": "Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them.", "labels": [], "entities": [{"text": "KAR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.5514631271362305}]}, {"text": "When only a subset (20%-80%) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.", "labels": [], "entities": [{"text": "KAR", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8733947277069092}, {"text": "MRC", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.937005877494812}]}], "introductionContent": [{"text": "Machine Reading Comprehension (MRC), as the name suggests, requires a machine to read a passage and answer its relevant questions.", "labels": [], "entities": [{"text": "Machine Reading Comprehension (MRC)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8499760429064432}]}, {"text": "Since the answer to each question is supposed to stem from the corresponding passage, a common MRC solution is to develop a neural-network-based MRC model that predicts an answer span (i.e. the answer start position and the answer end position) from the passage of each given passage-question pair.", "labels": [], "entities": [{"text": "MRC", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9507049918174744}]}, {"text": "To facilitate the explorations and innovations in this area, many MRC datasets have been established, such as SQuAD (, MS MARCO (, and TriviaQA (.", "labels": [], "entities": [{"text": "MRC", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9405713677406311}, {"text": "MS", "start_pos": 119, "end_pos": 121, "type": "DATASET", "confidence": 0.8869326114654541}, {"text": "MARCO", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.4668552279472351}]}, {"text": "Consequently, many pioneering MRC models have been proposed, such as BiDAF (, R-NET (, and QANet ().", "labels": [], "entities": [{"text": "MRC", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9837840795516968}, {"text": "BiDAF", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.8599901795387268}]}, {"text": "According to the leader board of SQuAD, the state-of-the-art MRC models have achieved the same performance as human beings.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.7330760359764099}, {"text": "MRC", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9559898376464844}]}, {"text": "However, does this imply that they have possessed the same reading comprehension ability as human beings?", "labels": [], "entities": []}, {"text": "There is a huge gap between MRC models and human beings, which is mainly reflected in the hunger for data and the robustness to noise.", "labels": [], "entities": [{"text": "MRC", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.97640061378479}]}, {"text": "On the one hand, developing MRC models requires a large amount of training examples (i.e. the passage-question pairs labeled with answer spans), while human beings can achieve good performance on evaluation examples (i.e. the passage-question pairs to address) without training examples.", "labels": [], "entities": [{"text": "MRC", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.978692352771759}]}, {"text": "On the other hand, revealed that intentionally injected noise (e.g. misleading sentences) in evaluation examples causes the performance of MRC models to drop significantly, while human beings are far less likely to suffer from this.", "labels": [], "entities": [{"text": "MRC", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.9473056793212891}]}, {"text": "The reason for these phenomena, we believe, is that MRC models can only utilize the knowledge contained in each given passagequestion pair, but in addition to this, human beings can also utilize general knowledge.", "labels": [], "entities": [{"text": "MRC", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9529359936714172}]}, {"text": "A typical category of general knowledge is inter-word semantic connections.", "labels": [], "entities": [{"text": "inter-word semantic connections", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.6972578366597494}]}, {"text": "As shown in, such general knowledge is essential to the reading comprehension ability of human beings.", "labels": [], "entities": []}, {"text": "A promising strategy to bridge the gap mentioned above is to integrate the neural networks of MRC models with the general knowledge of human beings.", "labels": [], "entities": []}, {"text": "To this end, it is necessary to solve two problems: extracting general knowledge from passagequestion pairs and utilizing the extracted general knowledge in the prediction of answer spans.", "labels": [], "entities": [{"text": "prediction of answer spans", "start_pos": 161, "end_pos": 187, "type": "TASK", "confidence": 0.8519913405179977}]}, {"text": "The first problem can be solved with knowledge bases, which store general knowledge in structured forms.", "labels": [], "entities": []}, {"text": "A broad variety of knowledge bases are available, such as WordNet  In what borough is the garment business prominent?", "labels": [], "entities": [{"text": "WordNet", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9391952157020569}]}, {"text": "Brooklyn: Two examples about the importance of inter-word semantic connections to the reading comprehension ability of human beings: in the first one, we can find the answer because we know \"facilitate\" is a synonym of \"help\"; in the second one, we can find the answer because we know \"Brooklyn\" is a hyponym of \"borough\".", "labels": [], "entities": []}, {"text": "Freebase () storing factoid knowledge.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9368354082107544}]}, {"text": "In this paper, we limit the scope of general knowledge to inter-word semantic connections, and thus use WordNet as our knowledge base.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.9337213635444641}]}, {"text": "The existing way to solve the second problem is to encode general knowledge in vector space so that the encoding results can be used to enhance the lexical or contextual representations of words (.", "labels": [], "entities": []}, {"text": "However, this is an implicit way to utilize general knowledge, since in this way we can neither understand nor control the functioning of general knowledge.", "labels": [], "entities": []}, {"text": "In this paper, we discard the existing implicit way and instead explore an explicit (i.e. understandable and controllable) way to utilize general knowledge.", "labels": [], "entities": []}, {"text": "The contribution of this paper is two-fold.", "labels": [], "entities": []}, {"text": "On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage-question pair.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9526798129081726}]}, {"text": "On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms.", "labels": [], "entities": [{"text": "MRC", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9668679237365723}]}, {"text": "Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them.", "labels": [], "entities": [{"text": "KAR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.5514631271362305}]}, {"text": "When only a subset (20%-80%) of the training examples are available, KAR outperforms the stateof-the-art MRC models by a large margin, and is still reasonably robust to noise.", "labels": [], "entities": [{"text": "KAR", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8682284355163574}]}], "datasetContent": [{"text": "The MRC dataset used in this paper is SQuAD 1.1, which contains over 100, 000 passage-question pairs and has been randomly partitioned into three parts: a training set (80%), a development set (10%), and a test set (10%).", "labels": [], "entities": [{"text": "MRC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7644295394420624}]}, {"text": "Besides, we also use two of its adversarial sets, namely AddSent and AddOneSent, to evaluate the robustness to noise of MRC models.", "labels": [], "entities": [{"text": "MRC", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9213882684707642}]}, {"text": "The passages in the adversarial sets contain misleading sentences, which are aimed at distracting MRC models.", "labels": [], "entities": [{"text": "MRC", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.8686001300811768}]}, {"text": "Specifically, each passage in AddSent contains several sentences that are similar to the question but not contradictory to the answer, while each passage in AddOneSent contains a human-approved random sentence that maybe unrelated to the passage.", "labels": [], "entities": []}, {"text": "We tokenize the MRC dataset with spaCy 2.0.13 (Honnibal and Montani, 2017), manipulate WordNet 3.0 with NLTK 3.3, and implement KAR with TensorFlow 1.11.0 (.", "labels": [], "entities": [{"text": "MRC dataset", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.7516086399555206}]}, {"text": "For the data enrichment method, we set the hyper-parameter \u03ba to 3.", "labels": [], "entities": [{"text": "data enrichment", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.6648494005203247}]}, {"text": "For the dense layers and the BiLSTMs, we set the dimensionality unit d to 600.", "labels": [], "entities": [{"text": "BiLSTMs", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.7206506729125977}]}, {"text": "For model optimization, we apply the Adam () optimizer with a learning rate of 0.0005 and a minibatch size of 32.", "labels": [], "entities": [{"text": "model optimization", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7662139534950256}, {"text": "learning rate", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9483948051929474}]}, {"text": "For model evaluation, we use Exact Match (EM) and F1 score as evaluation metrics.", "labels": [], "entities": [{"text": "model evaluation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7068621516227722}, {"text": "Exact Match (EM)", "start_pos": 29, "end_pos": 45, "type": "METRIC", "confidence": 0.9647427558898926}, {"text": "F1 score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9883993566036224}]}, {"text": "To avoid overfitting, we apply dropout) to the dense layers and the BiLSTMs with a dropout rate of 0.3.", "labels": [], "entities": [{"text": "BiLSTMs", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.7613290548324585}]}, {"text": "To boost the performance, we apply exponential moving average with a decay rate of 0.999.", "labels": [], "entities": [{"text": "exponential moving average", "start_pos": 35, "end_pos": 61, "type": "METRIC", "confidence": 0.8591790397961935}]}], "tableCaptions": [{"text": " Table 2: Model comparison based on SQuAD 1.1 and two of its adversarial sets: AddSent and AddOneSent. All  the numbers are up to date as of October 18, 2018. Note that SQuAD 2.0 (Rajpurkar et al., 2018) is not involved in  this paper, because it requires MRC models to deal with the problem of answer triggering, but this paper is aimed  at improving the hunger for data and robustness to noise of MRC models.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 295, "end_pos": 312, "type": "TASK", "confidence": 0.8610843122005463}]}, {"text": " Table 3: With \u03ba set to different values in the data en- richment method, we calculate the average number of  inter-word semantic connections per word as an estima- tion of the amount of general knowledge, and evaluate  the performance of KAR on the development set.", "labels": [], "entities": []}]}