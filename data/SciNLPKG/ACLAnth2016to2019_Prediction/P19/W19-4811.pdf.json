{"title": [{"text": "Character Eyes: Seeing Language through Character-Level Taggers", "labels": [], "entities": [{"text": "Character-Level Taggers", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.6795283555984497}]}], "abstractContent": [{"text": "Character-level models have been used extensively in recent years in NLP tasks as both supplements and replacements for closed-vocabulary token-level word representations.", "labels": [], "entities": []}, {"text": "In one popular architecture, character-level LSTMs are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech (POS) tags.", "labels": [], "entities": []}, {"text": "In this work, we examine the behavior of POS taggers across languages from the perspective of individual hidden units within the character LSTM.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.8228987157344818}]}, {"text": "We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties , and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer.", "labels": [], "entities": []}, {"text": "Ina comparative experiment , we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Subword vector representations are now a standard part of neural architectures for natural language processing (e.g.,.", "labels": [], "entities": [{"text": "Subword vector representations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7484856446584066}]}, {"text": "In particular, character representations have been shown to handle out-of-vocabulary words in supervised tagging tasks ().", "labels": [], "entities": [{"text": "supervised tagging tasks", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.6576409935951233}]}, {"text": "These advantages generalize across multiple languages, where morphological formation may differ greatly but the character composition of words remains a relatively reliable primitive (.", "labels": [], "entities": []}, {"text": "While the advantages of character-level models are readily apparent, existing evaluation methods fail to explain the mechanism by which these models encode linguistic knowledge about morphology and orthography.", "labels": [], "entities": []}, {"text": "Different languages exhibit * Work done while at Georgia Institute of Technology.", "labels": [], "entities": [{"text": "Georgia Institute of Technology", "start_pos": 49, "end_pos": 80, "type": "DATASET", "confidence": 0.8517885357141495}]}, {"text": "character-word correspondence in very different patterns, and yet the bi-directional LSTM appears to be, or is assumed to be, capable of capturing them all.", "labels": [], "entities": []}, {"text": "In large multilingual settings, it is not uncommon to tune hyperparameters on a handful of languages, and apply them to the rest (e.g.,.", "labels": [], "entities": []}, {"text": "In this work, we challenge this implicit generalization.", "labels": [], "entities": []}, {"text": "We train character-based sequence taggers on a large selection of languages exhibiting various strategies for word formation, and subject the resulting models to a novel analysis of the behavior of individual units in the characterlevel Bi-LSTM hidden layer.", "labels": [], "entities": [{"text": "character-based sequence taggers", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.6640896002451578}, {"text": "word formation", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.7235492467880249}]}, {"text": "This reveals differences in the ability of the Bi-LSTM architecture to identify parts-of-speech, based on typological properties: hidden layers trained on agglutinative languages find more regularities on the character level than in fusional languages; languages that are suffix-heavy give a stronger signal to the backward-facing hidden units, and vice versa for prefix-heavy languages.", "labels": [], "entities": []}, {"text": "In short, character-level recurrent networks function differently depending on how each language expresses morphosyntactic properties in characters.", "labels": [], "entities": []}, {"text": "These empirical results motivate a novel Bi-LSTM architecture, in which the number of hidden units is unbalanced across the forward and backward directions.", "labels": [], "entities": []}, {"text": "We find empirical correspondence between the analytical findings above and performance of such unbalanced Bi-LSTM models, allowing us to translate the typological properties of a language into concrete recommendations for model selection.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Attributes and tagging accuracy by lan- guage (Irish and Thai do not have both dev and test  sets).  \u2020 Affixation: S/s is strongly/weakly suffixing;  P/p is strongly/weakly prefixing; = is equally prefix- ing/suffixing; \u2205 is little affixation.  \u2021 Morphological syn- thesis: agglutinative, fusional, introflexive, isolating.", "labels": [], "entities": [{"text": "Attributes", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9632524251937866}, {"text": "tagging", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9325301051139832}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9826000332832336}, {"text": "Affixation", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9839283227920532}]}, {"text": " Table 2: PDI statistics for UD 2.3 models, b avg|\u00b7| metric,  sorted by the mass metric (sum of PDIs). Agglutinative  languages in bold, introflexive in italics.", "labels": [], "entities": []}, {"text": " Table 3: Imbalanced models' mean POS accuracy on  UD development data (differences between three aver- aged random runs in all models; boldfaced when sig- nificant at p < 0.05 using a paired two-tailed t-test).", "labels": [], "entities": [{"text": "POS", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.964210033416748}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.56025630235672}, {"text": "UD development data", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.7024082541465759}]}]}