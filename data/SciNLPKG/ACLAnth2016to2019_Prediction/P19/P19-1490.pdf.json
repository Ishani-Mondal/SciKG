{"title": [{"text": "Multilingual and Cross-Lingual Graded Lexical Entailment", "labels": [], "entities": [{"text": "Cross-Lingual Graded Lexical Entailment", "start_pos": 17, "end_pos": 56, "type": "TASK", "confidence": 0.5714954435825348}]}], "abstractContent": [{"text": "Grounded in cognitive linguistics, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale.", "labels": [], "entities": [{"text": "graded lexical entailment (GR-LE)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.776713823278745}]}, {"text": "In this paper, we present the first work on cross-lingual generalisation of GR-LE relation.", "labels": [], "entities": []}, {"text": "Starting from Hyper-Lex, the only available GR-LE dataset in En-glish, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX.", "labels": [], "entities": []}, {"text": "We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX).", "labels": [], "entities": [{"text": "Cross-Lingual Lexical Entailment Attract-Repel)", "start_pos": 45, "end_pos": 92, "type": "TASK", "confidence": 0.6935055732727051}]}, {"text": "Coupled with a bilingual dictionary , CLEAR leverages taxonomic LE knowledge in a resource-rich language (e.g., En-glish) and propagates it to other languages.", "labels": [], "entities": []}, {"text": "Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets.", "labels": [], "entities": [{"text": "cross-lingual LE transfer", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.5950387914975485}, {"text": "GR-LE datasets", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.7743438184261322}, {"text": "GR-LE datasets", "start_pos": 146, "end_pos": 160, "type": "DATASET", "confidence": 0.689944788813591}]}, {"text": "In addition, we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs.", "labels": [], "entities": [{"text": "cross-lingual LE detection", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.6232228676478068}]}], "introductionContent": [{"text": "Word-level lexical entailment (LE), also known as the TYPE-OF or hyponymy-hypernymy relation, is a fundamental asymmetric lexical relation.", "labels": [], "entities": [{"text": "Word-level lexical entailment (LE)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7484083424011866}, {"text": "TYPE-OF", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9648298621177673}]}, {"text": "It is a key principle behind the hierarchical structure found in semantic networks such as WordNet) or ConceptNet (.", "labels": [], "entities": []}, {"text": "As opposed to simpler discrete and binary LE detection (e.g., oregano is a TYPE-OF food), graded lexical entailment (GR-LE) measures the strength of the LE relation between two concepts on a continuous scale.", "labels": [], "entities": []}, {"text": "GR-LE is concerned with fine-grained directional assertions of hierarchical arrangements between concepts.", "labels": [], "entities": []}, {"text": "The notion of graded LE is rooted in theories of concept (proto)typicality and category vagueness from cognitive science.", "labels": [], "entities": []}, {"text": "Instead of answering the simpler (discrete) question \"Is X a type of Y?\", as in standard LE detection tasks, GR-LE aims at answering the following question: \"To what degree is X a type of Y?\"", "labels": [], "entities": [{"text": "LE detection tasks", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7616665661334991}]}, {"text": "The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex ), a GR-LE resource in English.", "labels": [], "entities": []}, {"text": "Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (.", "labels": [], "entities": [{"text": "LE detection", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.7279340624809265}, {"text": "language understanding", "start_pos": 209, "end_pos": 231, "type": "TASK", "confidence": 0.6982727646827698}]}, {"text": "The ability to reason over cross-lingual LE is pivotal fora variety of cross-lingual tasks such as recognising cross-lingual textual entailment (), constructing multilingual taxonomies (), cross-lingual event coreference (, machine translation in- , while simultaneously higher-level concepts are assigned larger norms to enforce the LE arrangement in the vector space.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.7000185698270798}]}, {"text": "An asymmetric distance that takes into account the vector direction as well as the vector magnitude can be used to grade the LE relation strength between any two concepts in the shared cross-lingual vector space.", "labels": [], "entities": [{"text": "LE relation strength", "start_pos": 125, "end_pos": 145, "type": "METRIC", "confidence": 0.9359295964241028}]}, {"text": "terpretability (, and cross-lingual lexical substitution (.", "labels": [], "entities": [{"text": "cross-lingual lexical substitution", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.6676197946071625}]}, {"text": "In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment.", "labels": [], "entities": []}, {"text": "We make several important contributions related to GR-LE in multilingual settings.", "labels": [], "entities": [{"text": "GR-LE", "start_pos": 51, "end_pos": 56, "type": "TASK", "confidence": 0.837372362613678}]}, {"text": "First, we extend the research on GR-LE beyond English and provide new human-annotated GR-LE datasets in three other languages: German, Italian, and Croatian.", "labels": [], "entities": [{"text": "GR-LE datasets", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.6887485682964325}]}, {"text": "Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-semantic relations, we automatically derive a collection of six cross-lingual GR-LE datasets: CL-HYPERLEX.", "labels": [], "entities": []}, {"text": "We analyse in detail the cross-lingual datasets (e.g., by comparing the scores to human-elicited ratings), demonstrating their robustness and reliability.", "labels": [], "entities": []}, {"text": "In order to provide a competitive baseline on new monolingual and cross-lingual datasets, we next introduce a cross-lingual specialisation/retrofitting method termed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel): starting from any two monolingual distributional spaces, CLEAR induces a bilingual cross-lingual space that reflects the asymmetric nature of the LE relation.", "labels": [], "entities": [{"text": "Cross-Lingual Lexical Entailment Attract-Repel)", "start_pos": 173, "end_pos": 220, "type": "TASK", "confidence": 0.6858807325363159}]}, {"text": "Such a crosslingual LE-specialised space is illustrated in.", "labels": [], "entities": []}, {"text": "CLEAR is an extension of the monolingual LEAR specialisation method . The key idea of CLEAR is to leverage external lexical knowledge (i.e., information on word relations from WordNet, BabelNet, or ConceptNet) to rescale vector norms which reflect the concept hierarchy, while simultaneously pushing (i.e., \"attracting\") desirable word pairs closer (by vector direction) to reflect their semantic similarity in the cross-lingual LE-specialised space.", "labels": [], "entities": []}, {"text": "Crucially, as shown later in, CLEAR relies on a curated semantic resource only in the resource-rich source language (e.g., English): coupled with a bilingual dictionary it propagates the LE knowledge to the target (resource-poor) language and constructs a shared cross-lingual LE-specialised space.", "labels": [], "entities": []}, {"text": "This cross-lingual LE-specialised space, depicted in Figure 1 and empirically validated in \u00a74, is then used to reason over GR-LE in the target language, and for making cross-lingual GR-LE assertions.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that CLEAR is a strong benchmark on all GR-LE datasets.", "labels": [], "entities": [{"text": "CLEAR", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9796064496040344}, {"text": "GR-LE datasets", "start_pos": 68, "end_pos": 82, "type": "DATASET", "confidence": 0.8755922913551331}]}, {"text": "It can effectively transfer LE knowledge to a spectrum of target languages.", "labels": [], "entities": [{"text": "LE knowledge", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.8646727502346039}]}, {"text": "What is more, through multilingual training via a resource-rich pivot language (e.g., English) CLEAR supports cross-lingual GR-LE for language pairs without any semantic resources.", "labels": [], "entities": []}, {"text": "Finally, we report state-of-the-art scores in the ungraded (i.e., binary) cross-lingual LE detection for three diverse language pairs on standard evaluation sets (.", "labels": [], "entities": [{"text": "cross-lingual LE detection", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.5955987175305685}]}, {"text": "Annotation guidelines and created datasets for all languages and language pairs are available online at: https://github.com/ivulic/ xling-grle/, and as the supplemental material.", "labels": [], "entities": []}, {"text": "We also make available the code and CLEARspecialised vector spaces.", "labels": [], "entities": []}], "datasetContent": [{"text": "Graded lexical entailment is an asymmetric relation formulated by the intuitive question \"To what degree is X a type of Y?\": it comprises two distinct phenomena studied in cognitive science.", "labels": [], "entities": [{"text": "Graded lexical entailment", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6992433766523997}]}, {"text": "First, it captures the measure of typicality in graded cognitive categorisation: some instances of a category are more central than others (e.g., basketball will often be cited as a more typical sport than biathlon).", "labels": [], "entities": []}, {"text": "Second, it covers the measure of vagueness (also referred to as graded membership): it measures the graded applicability of a concept to different instances.", "labels": [], "entities": []}, {"text": "Despite the fact that GR-LE should not be bound to any particular surface realisation of concepts (i.e., it is not tied to a particular language), a graded LE repository has so far been created only for English: it is the HyperLex dataset of Vuli\u00b4c . Starting from the established data creation protocol for HyperLex, in this work we compile similar HyperLex datasets in three other languages and introduce novel multilingual and cross-lingual GR-LE tasks.", "labels": [], "entities": [{"text": "HyperLex dataset of Vuli\u00b4c", "start_pos": 222, "end_pos": 248, "type": "DATASET", "confidence": 0.7833145409822464}]}, {"text": "HyperLex ) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the GR-LE relation.", "labels": [], "entities": [{"text": "HyperLex )", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.898603230714798}]}, {"text": "Unlike in symmetric similarity datasets (, word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings.", "labels": [], "entities": []}, {"text": "The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymyhypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9531755447387695}]}, {"text": "The ratings in the interval were then collected through crowdsourcing by posing the GR-LE \"To what degree...\" question to human subjects, with each pair rated by at least 10 raters: the score of 6 indicates strong LE relation between the concepts X and Y (in that order), and 0 indicates absence of the LE relation.", "labels": [], "entities": [{"text": "GR-LE", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.8911614418029785}]}, {"text": "The final score was averaged across individual ratings.", "labels": [], "entities": []}, {"text": "The final EN HyperLex dataset reveals that gradience effects are indeed present inhuman annotations: it contains word pairs with ratings distributed across the entire rating interval.", "labels": [], "entities": [{"text": "EN HyperLex dataset", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.921377698580424}]}, {"text": "What is more, high inter-annotator agreement scores (see), suggest that even non-expert annotators consistently reason about the degree of LE between words.", "labels": [], "entities": [{"text": "LE", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.6493542194366455}]}, {"text": "Monolingual HyperLex datasets in three target languages: German (DE), Italian (IT), and Croatian (HR) were constructed by translating word pairs from the EN HyperLex and re-scoring the translated pairs in the target language.", "labels": [], "entities": []}, {"text": "The translation approach has been selected because: 1) the original EN HyperLex pairs were already carefully selected through a controlled sampling procedure to ensure a wide coverage of diverse WordNet relations; 2) we want to ensure as comparable datasets as possible across different languages in terms of semantic coverage; 3) the approach has been extensively validated in related work on creating multilingual semantic similarity datasets ().", "labels": [], "entities": []}, {"text": "Most importantly, the translation approach allows for the automatic construction of cross-lingual GR-LE datasets.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9608303308486938}]}, {"text": "We have followed the standard word pair translation procedure (.", "labels": [], "entities": [{"text": "word pair translation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.7110830247402191}]}, {"text": "Each EN HyperLex pair was first translated independently by two native speakers of the target language.", "labels": [], "entities": []}, {"text": "The translation agreement was in the range of 85%-90% across the three target languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9749889373779297}]}, {"text": "Translation disagreements were resolved by a third annotator who selected the correct (or better) translation following discussions with both translators.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9275257587432861}]}, {"text": "To account for polysemy, each word pair was shown along with its EN HyperLex score, helping annotators to preserve word sense during translation.", "labels": [], "entities": [{"text": "EN HyperLex score", "start_pos": 65, "end_pos": 82, "type": "METRIC", "confidence": 0.8902190526326498}]}, {"text": "We allowed for multi-word translations only if there was no appropriate single word translation (e.g., typewriter \u2192 macchina da scrivere).", "labels": [], "entities": []}, {"text": "Guidelines and Concept Pair Scoring.", "labels": [], "entities": [{"text": "Concept Pair Scoring", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.633702834447225}]}, {"text": "EN HyperLex annotation guidelines were translated to all three target languages (see the supplementary).", "labels": [], "entities": [{"text": "EN HyperLex annotation", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.888037363688151}]}, {"text": "The resulting 2,616 concept pairs in each language were annotated using a procedure analogous to that for EN HyperLex: the rating interval was, and each word pair was rated by 4 native speakers.", "labels": [], "entities": []}, {"text": "The cross-lingual CL-HYPERLEX datasets were then constructed automatically, leveraging word pair translations and scores in three target languages.", "labels": [], "entities": [{"text": "word pair translations", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6168200969696045}]}, {"text": "To this end, we follow the methodology of, used previously for creating cross-lingual semantic similarity datasets.", "labels": [], "entities": []}, {"text": "In short, we first intersect aligned concept pairs (obtained through translation) in two languages: e.g., father-ancestor in English and padre-antenato in Italian are used  We verify that all score ranges are represented by a sufficient number of concept pairs.", "labels": [], "entities": []}, {"text": "The score distributions are shown in.", "labels": [], "entities": []}, {"text": "As in EN HyperLex, a large number of concept pairs is placed within the two outer sub-intervals (i.e.,   [0, 1) and): this is an artefact of having WordNet synonyms as trivial LE pairs on the one side, whereas antonyms, no-relation, and reverse hyponymy-hypernymy pairs are found on the other side of the scoring spectrum.", "labels": [], "entities": []}, {"text": "Nonetheless, the inner interval (i.e.,) covers a significant portion (\u2248 30%) of (evenly distributed) word pairs, confirming the gradience of the LE relation.", "labels": [], "entities": []}, {"text": "Inter show that humans quantify graded  Distributional Vectors.", "labels": [], "entities": []}, {"text": "Graded LE is evaluated on EN, DE, IT, and HR (see \u00a72); we also evaluate CLEAR on ungraded cross-lingual.", "labels": [], "entities": [{"text": "LE", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.5218458771705627}, {"text": "CLEAR", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9926763772964478}]}, {"text": "RU vectors are obtained by.", "labels": [], "entities": []}, {"text": "FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (, and the hrWaC corpus, also used in prior work . All word vectors are 300-dim.", "labels": [], "entities": [{"text": "FR", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8227807879447937}, {"text": "hrWaC corpus", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.7565931379795074}]}, {"text": "8 Linguistic Constraints and Dictionaries.", "labels": [], "entities": []}, {"text": "We use the same set of monolingual constraints as LEAR : synonymy and antonymy constraints from ( are extracted from WordNet and Roget's Thesaurus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9442736506462097}]}, {"text": "As in other work on LE specialisation, asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an-7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors.", "labels": [], "entities": [{"text": "LE specialisation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8949779272079468}, {"text": "WordNet", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9692257046699524}]}, {"text": "We have experimented with other standard distributional spaces in English such as fastText (, type-based ELMo embeddings (), and Glove), but the obtained results follow similar trends.", "labels": [], "entities": [{"text": "Glove", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.860236406326294}]}, {"text": "We do not report these results for brevity.", "labels": [], "entities": []}, {"text": "8 Vectors of multi-word expressions in CL-HYPERLEX are obtained by averaging over their constituent imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs.", "labels": [], "entities": []}, {"text": "Bilingual dictionaries are derived from PanLex (, which was used in prior work on cross-lingual word embeddings (.", "labels": [], "entities": [{"text": "PanLex", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.9639623761177063}]}, {"text": "PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.", "labels": [], "entities": [{"text": "PanLex", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9455652832984924}]}, {"text": "CLEAR hyperparameters are adopted from the original Attract-Repel work ): \u03b4 att = 0.6, \u03b4 rep = 0.0, \u03bb reg = \u03bb D = 10 \u22129 . All batches are of size 128 (see Eq.), and the model is trained for 5 epochs with Adagrad ().", "labels": [], "entities": [{"text": "Adagrad", "start_pos": 204, "end_pos": 211, "type": "DATASET", "confidence": 0.9016370177268982}]}, {"text": "In monolingual evaluation, we compare CLEAR to original non-specialised distributional vectors in each language.", "labels": [], "entities": []}, {"text": "Another instructive baseline is the TRANS baseline which uses exactly the same amount of information as CLEAR.", "labels": [], "entities": [{"text": "TRANS baseline", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.8180376589298248}]}, {"text": "Instead of performing joint CLEAR specialisation as described in \u00a73, TRANS is a two-step process that: 1) runs the monolingual LEAR specialisation of the English distributional space, and then 2) translates all test examples in the target language to English relying on the bilingual dictionary D.", "labels": [], "entities": [{"text": "TRANS", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.7951381206512451}]}, {"text": "10 All LE reasoning is then conducted monolingually in English.", "labels": [], "entities": [{"text": "LE reasoning", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.7198971956968307}]}, {"text": "The TRANS baseline is also used in cross-lingual graded LE evaluation.", "labels": [], "entities": [{"text": "TRANS baseline", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9150200188159943}, {"text": "cross-lingual graded LE evaluation", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.5596098825335503}]}, {"text": "For cross-lingual datasets without English (e.g., DE-IT), we again translate all words to English and use the English specialised space for graded LE assertions.", "labels": [], "entities": []}, {"text": "In addition, for each language pair we also report results of two stateof-the-art cross-lingual word embedding models (, showing the better scoring one in each run (XEMB).", "labels": [], "entities": []}, {"text": "For ungraded LE evaluation, in addition to TRANS, we compare CLEAR to two bestperforming baselines from (Upadhyay et al., 2018): they couple two methods for inducing syntactic cross-lingual vectors: 1) BI-SPARSE () and 2) CL-DEP) with an LE scorer based on the distributional inclusion hypothesis).", "labels": [], "entities": [{"text": "TRANS", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.8889920115470886}, {"text": "BI-SPARSE", "start_pos": 202, "end_pos": 211, "type": "METRIC", "confidence": 0.9909011125564575}]}, {"text": "For more details we refer the reader to (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example pairs with ratings from monolingual  and cross-lingual graded LE datasets. Note that for  cross-lingual datasets words from each language can  be placed as the first or the second word in the pair.", "labels": [], "entities": []}, {"text": " Table 2: The sizes of all monolingual (main diagonal)  and cross-lingual graded LE datasets.", "labels": [], "entities": [{"text": "LE datasets", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.6520522385835648}]}, {"text": " Table 3: Inter-annotator agreement (Spearman's \u03c1 cor- relation) for monolingual GR-LE datasets. IAA scores  for the original EN HyperLex provided for reference.", "labels": [], "entities": [{"text": "GR-LE datasets", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.8449451327323914}, {"text": "IAA", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9966071844100952}, {"text": "EN HyperLex", "start_pos": 126, "end_pos": 137, "type": "DATASET", "confidence": 0.9167585074901581}]}, {"text": " Table 4: Cross-lingual ungraded LE detection accuracy  scores on cross-lingual HYPO and COHYP evaluation  sets from Upadhyay et al. (2018).", "labels": [], "entities": [{"text": "LE detection", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.6648772358894348}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.7259888648986816}, {"text": "COHYP evaluation  sets", "start_pos": 89, "end_pos": 111, "type": "DATASET", "confidence": 0.7795566121737162}]}]}