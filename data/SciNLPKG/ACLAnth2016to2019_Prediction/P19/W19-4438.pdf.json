{"title": [{"text": "Measuring Text Complexity for Italian as a Second Language Learning Purposes", "labels": [], "entities": []}], "abstractContent": [{"text": "The selection of texts for second language learning purposes typically relies on teach-ers' and test developers' individual judgment of the observable qualitative properties of a text.", "labels": [], "entities": []}, {"text": "Little or no consideration is generally given to the quantitative dimension within an evidence-based framework of reproducibility.", "labels": [], "entities": []}, {"text": "This study aims to fill the gap by evaluating the effectiveness of an automatic tool trained to assess text complexity in the context of Italian as a second language learning.", "labels": [], "entities": []}, {"text": "A dataset of texts labeled by expert test developers was used to evaluate the performance of three classifier models (decision tree, random forest, and support vector machine), which were trained using linguistic features measured quantitatively and extracted from the texts.", "labels": [], "entities": []}, {"text": "The experimental analysis provided satisfactory results, also in relation to which kind of linguistic trait contributed the most to the final outcome.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of automatically classifying a text according to its different levels of complexity has had various applications in a number of different fields.", "labels": [], "entities": []}, {"text": "It is key in mood and sentiment analysis, in the detection of hate speech, in text simplification, and also in the assessment of text readability in relation to both native and non-native readers.", "labels": [], "entities": [{"text": "mood and sentiment analysis", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6818533316254616}, {"text": "detection of hate speech", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8887247294187546}, {"text": "text simplification", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7797360122203827}]}, {"text": "Being able to select a text and compare it with others is a central concern in the field of second language learning.", "labels": [], "entities": []}, {"text": "When choosing a text to be used in a lesson or as part of a language test, a teacher and/or language test developer will generally assess the suitability of that text on the basis of several aspects: the need to adhere to a specific syllabus and curriculum, as well as general guidelines and test specifications.", "labels": [], "entities": []}, {"text": "Other aspects that are considered include learner-related variables such as their linguistic needs, their educational background, and their age, all elements involving other aspects such as text genre, text type, tasks to be assigned to the text, and soon.", "labels": [], "entities": []}, {"text": "According to the literature, there is wide consensus on specific characteristics that can influence the difficulty of a text in the context of a reading comprehension task.", "labels": [], "entities": []}, {"text": "These characteristics have a role in terms of the cognitive demands that a text will impose on its reader).", "labels": [], "entities": []}, {"text": "These characteristics are text length, grammatical complexity, word frequency, cohesion, rhetorical organization, genre, text abstractness, subject knowledge and cultural knowledge.", "labels": [], "entities": []}, {"text": "All these aspects relate to readability, and are often evaluated intuitively and subjectively by individual experienced teachers, who will then use a given text deemed to be representative of a certain proficiency level in a lesson or as part of a test.", "labels": [], "entities": []}, {"text": "Although this kind of sensitivity to the text is extremely valuable, especially when adapting a lesson or test item to the specific needs of a group of learners, its limitations are evident for at least two reasons: the evaluation is performed by single teachers or test developers at the onetime and it is not reproducible; the evaluation is conducted solely on the basis of observable qualitative features of a text.", "labels": [], "entities": []}, {"text": "In the context of language assessment, text selection for the purposes of a reading comprehension task has considerable implications with regard to the interpretation of test scores: a text subjectively deemed suitable fora given proficiency level, which would have objectively been deemed otherwise, will inevitably hinder the validity of the overall testing process.", "labels": [], "entities": [{"text": "language assessment", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7364898920059204}, {"text": "text selection", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.704764723777771}]}, {"text": "The same can be argued for text selection aimed at lesson planning: an in-adequate text chosen fora given class will hinder the whole learning process.", "labels": [], "entities": [{"text": "text selection", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7673396468162537}, {"text": "lesson planning", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7039109766483307}]}, {"text": "As a result, an automatic system able to used extract objective and reproducible information about a text, combining qualitative and quantitative data, is highly desirable in the field of second language learning, though still largely lacking, especially for the Italian language and in relation to different proficiency levels.", "labels": [], "entities": []}, {"text": "The Common European Framework of Reference for Languages (CEFR) descriptors are unable to provide this kind of support in relation to the readability of a text.", "labels": [], "entities": [{"text": "Common European Framework of Reference for Languages (CEFR) descriptors", "start_pos": 4, "end_pos": 75, "type": "DATASET", "confidence": 0.6316694427620281}]}, {"text": "In this study, we assess the effectiveness of an automatic classification tool for the evaluation of text complexity in Italian.", "labels": [], "entities": []}, {"text": "We used a dataset of texts used at CVCL, Centro Valutazione Certificazioni Linguistiche, one of the main Italian language testing centres with sections allover the world, based at the University for Foreigners of Perugia.", "labels": [], "entities": [{"text": "CVCL", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9299890398979187}]}, {"text": "Each text in the dataset was labeled by test development experts according to the CEFR descriptors.", "labels": [], "entities": [{"text": "CEFR descriptors", "start_pos": 82, "end_pos": 98, "type": "DATASET", "confidence": 0.9754612147808075}]}, {"text": "The dataset was used to train a classification model, enabling it to automatically predict the proficiency level of any text in input.", "labels": [], "entities": []}, {"text": "The dataset was used to test three different classifiers: decision tree, random forest and support vector machine.", "labels": [], "entities": []}, {"text": "The main difference between this study and the related work in the field that will be described in the following paragraph is that a set of linguistic features is used to distinguish texts from the perspective of CEFR levels.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 213, "end_pos": 217, "type": "DATASET", "confidence": 0.84487384557724}]}, {"text": "Therefore, linguistic features measured quantitatively and extracted from the texts are used to train the classification models that, in turn, allow to predict the proficiency level of an unseen text.", "labels": [], "entities": []}, {"text": "The rest of the article is organized as follows.", "labels": [], "entities": []}, {"text": "The literature related to this work is described in Section 2.", "labels": [], "entities": []}, {"text": "The architecture of the system is introduced in Section 3, while the definitions of the linguistic features adopted in the study are provided in Section 4.", "labels": [], "entities": []}, {"text": "Experiments are discussed in Section 5, while the conclusions are drawn in Section 6 together with future lines of research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments have been held in order to: analyze the effectiveness and the robustness of the prototypical classification system here proposed, and gain useful insights about which features discriminate more the texts.", "labels": [], "entities": []}, {"text": "The rest of the section is organized as follows.", "labels": [], "entities": []}, {"text": "Section 5.1 describes the corpus of texts and the datasets used in our experimentation.", "labels": [], "entities": []}, {"text": "The experimental design is described in Section 5.2.", "labels": [], "entities": []}, {"text": "The effectiveness of our system is analyzed in Section 5.3, while its robustness is discussed in Section 5.4.", "labels": [], "entities": []}, {"text": "Finally, Section 5.5 analyzes the contribution of the different features selected for this work.", "labels": [], "entities": []}, {"text": "An important preliminary step to the experiments was the creation of a reliable corpus of labeled texts.", "labels": [], "entities": []}, {"text": "In regard to this, we collected 523 texts with CEFR levels manually marked by expert language test developers.", "labels": [], "entities": []}, {"text": "The corpus contains texts for the four CEFR levels B1, B2, C1 and C2.", "labels": [], "entities": []}, {"text": "Two different datasets, namely 4C and 2C, have been extracted from the corpus.", "labels": [], "entities": []}, {"text": "While 4C (i.e., four classes) corresponds to the whole corpus, the smaller dataset 2C (i.e., two classes) contains the subset of 226 texts belonging to the levels B2 and C2.", "labels": [], "entities": []}, {"text": "provides the distribution of the different levels for both datasets.", "labels": [], "entities": []}, {"text": "Two main reasons motivated the introduction of the smaller dataset 2C.", "labels": [], "entities": []}, {"text": "First, the distribution of the proficiency levels in 4C is unbalanced, therefore a smaller and more balanced dataset such as 2C can be more reliable in terms of representativeness.", "labels": [], "entities": []}, {"text": "Second, as the classification models do not treat the four levels as part of an ordinal scale, thus ignoring the natural ordering characterizing them, choosing two levels instead of four eliminates this issue.", "labels": [], "entities": []}, {"text": "We tested three classifier models, namely, Decision Tree (DT), Random Forest (RF) and SVM, on both the datasets 2C and 4C.", "labels": [], "entities": []}, {"text": "For each dataset, the effectiveness and robustness of each model was evaluated using the nested cross-validation scheme).", "labels": [], "entities": []}, {"text": "Two nested cross-validation loops were performed: the outer loop aims at estimating the effectiveness of the model setting which is calibrated in the inner loop.", "labels": [], "entities": []}, {"text": "Both loops use 5 stratified folds.", "labels": [], "entities": []}, {"text": "The inner loop performs an exhaustive Weights associated with classes (class weight) 1, NS NC \u00b7 n k: Parameters space for the Decision Tree and the Random Forest classifiers.", "labels": [], "entities": []}, {"text": "The original name of each parameter in the Sci-Kit documentation is in typewriter font within brackets.", "labels": [], "entities": []}, {"text": "The function used to measure the quality of a split can either be the Gini impurity (GI) or the Information entropy (IE).", "labels": [], "entities": []}, {"text": "NF is the number of features, NS and NC are the number of samples and the number of classes in the dataset, respectively; n k is the number of samples in the k-th class of the dataset.", "labels": [], "entities": []}, {"text": "grid search on the hyper-parameters space, crossvalidated on the training and validation sets obtained by the outer loop.", "labels": [], "entities": []}, {"text": "Every grid search returns the setting of hyper-parameters which maximizes the (macro-averaged) F 1 -score.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9711572676897049}]}, {"text": "Then, the generalization ability of the selected model setting is assessed on the test sets generated by the outer loop and using the classic metrics accuracy, precision, recall and F 1 -score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9985055923461914}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9993133544921875}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9994687438011169}, {"text": "F 1 -score", "start_pos": 182, "end_pos": 192, "type": "METRIC", "confidence": 0.9866384863853455}]}, {"text": "The linguistic features described in Section 4 may have different scales, hence we introduced a preprocessing step to normalize them.", "labels": [], "entities": []}, {"text": "Four normalization methods were considered: no normalization at all (NN), L 2 normalization (L2), standardization (SS), and robust standardization (RS) (which, with respect to SS, do not consider the outlier values).", "labels": [], "entities": [{"text": "standardization (SS)", "start_pos": 98, "end_pos": 118, "type": "METRIC", "confidence": 0.9412147253751755}, {"text": "robust standardization (RS)", "start_pos": 124, "end_pos": 151, "type": "METRIC", "confidence": 0.7008205056190491}]}, {"text": "Hence, the choice of the normalization method is a further hyper-parameter which is tuned by the grid search.", "labels": [], "entities": []}, {"text": "The calibrated hyper-parameters and their ranges are provided in for DT and RF, and for SVM.", "labels": [], "entities": [{"text": "SVM", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.5568037629127502}]}, {"text": "Finally, in order to reduce the computational ef-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of proficiency levels across  datasets 2C and 4C.", "labels": [], "entities": []}, {"text": " Table 3: Parameters space for the SVM classifier. The  original name of each parameter in the Sci-Kit docu- mentation is in typewriter font within brackets.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7456494867801666}]}, {"text": " Table 4: Accuracy A, precision P , recall R and F 1 - score for Decision Tree (D.T.), Random Forest (R.F.)  and Support Vector Machine (SVM) on 2C (upper ta- ble) and 4C (lower table). Such measures are first com- puted for each class, then their unweighted mean is  computed.", "labels": [], "entities": [{"text": "Accuracy A", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9186833500862122}, {"text": "precision P", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9366668164730072}, {"text": "recall R", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.963902473449707}, {"text": "F 1 - score", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9372894167900085}]}, {"text": " Table 5: Confusion Matrix for Random Forests on 4C.", "labels": [], "entities": []}]}