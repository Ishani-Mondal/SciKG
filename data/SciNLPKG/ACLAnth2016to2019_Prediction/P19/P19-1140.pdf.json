{"title": [{"text": "Multi-Channel Graph Neural Network for Entity Alignment", "labels": [], "entities": [{"text": "Entity Alignment", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7303432822227478}]}], "abstractContent": [{"text": "Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments.", "labels": [], "entities": [{"text": "Entity alignment", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.817638635635376}]}, {"text": "In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels.", "labels": [], "entities": []}, {"text": "Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques.", "labels": [], "entities": [{"text": "KG completion", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.7440434992313385}]}, {"text": "Moreover, we also infer and transfer rule knowledge for completing two KGs consistently.", "labels": [], "entities": []}, {"text": "MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments.", "labels": [], "entities": [{"text": "MuGNN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.798240602016449}]}, {"text": "Extensive experiments on five publicly available datasets demonstrate our superior performance (5% Hits@1 upon average).", "labels": [], "entities": []}, {"text": "Source code and data used in the experiments can be accessed at https:// github.com/thunlp/MuGNN.", "labels": [], "entities": [{"text": "MuGNN", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.8432700037956238}]}], "introductionContent": [{"text": "Knowledge Graphs (KGs) store the world knowledge in the form of directed graphs, where nodes denote entities and edges are their relations.", "labels": [], "entities": []}, {"text": "Since it was proposed, many KGs are constructed (e.g.,) to provide structural knowledge for different applications and languages.", "labels": [], "entities": []}, {"text": "These KGs usually contain complementary contents, attracting researchers to integrate them into a unified KG, which shall benefit many knowledge driven tasks, such as information extraction () and recommendation (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 167, "end_pos": 189, "type": "TASK", "confidence": 0.8013774454593658}]}, {"text": "It is non-trivial to align different KGs due to their distinct surface forms, which makes the symbolic based methods) not always effective.", "labels": [], "entities": []}, {"text": "Instead, recent work utilizes general KG embedding methods (e.g., TransE) and align equivalent entities into a unified vector space based on a few seed alignments ().", "labels": [], "entities": []}, {"text": "The assumption is that entities and their counterparts in different KGs should have similar structures and thus similar embeddings.", "labels": [], "entities": []}, {"text": "However, alignment performance is unsatisfactory mainly due to the following challenges: Heterogeneity of Structures Different KGs usually differ a lot, and may mislead the representation learning and the alignment information from seeds.", "labels": [], "entities": []}, {"text": "Take the entity Jilin City as an example), KG1 and KG2 present its subgraphs derived from English and Chinese Wikipedia, respectively.", "labels": [], "entities": []}, {"text": "Since it is a Chinese city, KG2 is more informative than KG1 (denoted by dashed lines and ellipse), such as the relations of Dialect and Nearby, and the entity Liu Fei through relation Mayor.", "labels": [], "entities": []}, {"text": "Clearly, the province Jilin in KG1 and Jilin City in KG2, which are incorrect alignment, are more probable close in the vector space, because they have more similar structures (e.g., Northeastern Mandarin and Changchun).", "labels": [], "entities": []}, {"text": "What's worse, this incorrect alignment shall spread further over the graph.", "labels": [], "entities": []}, {"text": "Limited Seed Alignments Recent efforts based on general embedding methods heavily rely on existing alignments as training data, while seed alignments are usually insufficient for high-quality entity embeddings.", "labels": [], "entities": [{"text": "Limited Seed Alignments", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6726360122362772}]}, {"text": "introduces Graph Convolution Network (GCN) to enhance the entity embeddings by modeling structural features, but fails to consider structural heterogeneity.", "labels": [], "entities": []}, {"text": "To address the issues, we propose to perform KG inference and alignment jointly to explicitly reconcile the structural difference between different KGs, and utilize a graph-based model to make better use of seed alignment information.", "labels": [], "entities": [{"text": "KG inference", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.8542991280555725}]}, {"text": "The basic idea of structural reconciliation is to complete missing relations and prune exclusive entities.", "labels": [], "entities": [{"text": "structural reconciliation", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.7594490647315979}]}, {"text": "As shown in, to reconcile the differences of Jilin City, it is necessary to complete the missing relations Dialect and Nearby in KG1, and filter out entity Liu Fei exclusive in KG2.", "labels": [], "entities": []}, {"text": "The asymmetric entities and relations are caused not only by the incompleteness nature of KG, but also from their different demands.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel Multi-channel Graph Neural Network model MuGNN, which can encode different KGs to learn alignmentoriented embeddings.", "labels": [], "entities": []}, {"text": "For each KG, MuGNN utilizes different channels towards KG completion and pruning, so as to reconcile two types of structural differences: missing relations and exclusive entities.", "labels": [], "entities": [{"text": "KG completion", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.7705215513706207}]}, {"text": "Different channels are combined via pooling techniques, thus entity embeddings are enhanced with reconciled structures from different perspectives, making utilization of seed alignments effectively and efficiently.", "labels": [], "entities": []}, {"text": "Between KGs, each channel transfers structure knowledge via shared parameters.", "labels": [], "entities": []}, {"text": "Specifically, for KG completion, we first employ AMIE+ () on each KG to induce rules, then transfer them between KGs towards consistent completion.", "labels": [], "entities": [{"text": "KG completion", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.8076924979686737}, {"text": "AMIE", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9033466577529907}]}, {"text": "Following Graph Attention Network (GAT) (, we utilize KG self-attention to weighting relations for GNN channels.", "labels": [], "entities": []}, {"text": "For KG pruning, we design cross-KG attention to filter out exclusive entities by assigning low weights to corresponding relations.", "labels": [], "entities": []}, {"text": "We summarize the main contributions as follows: \u2022 We propose a novel Multi-channel GNN model MuGNN that learns alignmentoriented embeddings by encoding graphs from different perspectives: completion and pruning, so as to be robust to structural differences.", "labels": [], "entities": []}, {"text": "\u2022 We propose to perform KG inference and alignment jointly, so that the heterogeneity of KGs are explicitly reconciled through completion by rule inference and transfer, and pruning via cross-KG attention.", "labels": [], "entities": [{"text": "KG inference", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8987515568733215}]}, {"text": "\u2022 We perform extensive experiments on five publicly available datasets for entity alignment tasks, and achieve significant improvements of 5% Hits@1 on average.", "labels": [], "entities": [{"text": "entity alignment tasks", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.8658604224522909}]}, {"text": "Further ablation study demonstrates the effectiveness of our key components.", "labels": [], "entities": []}, {"text": "Rectangles denote two main steps, and rounded rectangles denote the key components of the corresponding step.", "labels": [], "entities": [{"text": "Rectangles", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9736461639404297}]}, {"text": "After rule inference and transfer, we utilize rules to complete each KG, denoted by dashed lines r 3 . Through relation weighting, we obtain multiple weighted graphs for different GNN channels, in which relation r 4 is weighted to 0.0 that prunes exclusive entities.", "labels": [], "entities": []}, {"text": "These channels are combined as the input for align model for alignment-oriented KG embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In  For each dataset, we employ AMIE+ for rule mining by setting the max number of premise asp = 2 and PCA confidence not less than 0.8.", "labels": [], "entities": [{"text": "rule mining", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8750053942203522}, {"text": "max number", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9360514879226685}, {"text": "PCA confidence", "start_pos": 103, "end_pos": 117, "type": "METRIC", "confidence": 0.8613842129707336}]}, {"text": "The statistical results of rules, transferred rules (Tr.Rule for short), ground triples and ground triples based on transferred rules (Tr.ground for short) are exhibited in   To make a fair comparison, we set embedding size to 128 for MuGNN and all baselines.", "labels": [], "entities": [{"text": "MuGNN", "start_pos": 235, "end_pos": 240, "type": "DATASET", "confidence": 0.9210737347602844}]}, {"text": "All graph models stack two layers of GNN.", "labels": [], "entities": []}, {"text": "We utilize Adagrad) as the optimizer.", "labels": [], "entities": []}, {"text": "For the margins in MuGNN, we empirically set \u03b3 1 = 1.0 and \u03b3 2 = 1.0.", "labels": [], "entities": [{"text": "MuGNN", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.8894438147544861}]}, {"text": "We set \u03b3 r = 0.12 to ensure rule knowledge constraints have less impact than the alignment model.", "labels": [], "entities": []}, {"text": "Other hyperparameters are chosen by running an exhaustively search over the following possible values: learning rate in {0.1, 0.01, 0.001}, L2 in {0.01, 0.001, 0.0001}, dropout in {0.1, 0.2, 0.5}.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 103, "end_pos": 116, "type": "METRIC", "confidence": 0.9332919120788574}]}, {"text": "The optimal configuration of MuGNN for entity alignment is: learning rate= 0.001, L2= 0.01, dropout = 0.2.", "labels": [], "entities": [{"text": "entity alignment", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7601427435874939}, {"text": "learning rate", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9532578587532043}]}, {"text": "We implement MuGNN with PyTorch-1.0.", "labels": [], "entities": [{"text": "PyTorch-1.0", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.7684537172317505}]}, {"text": "The experiments are conducted on a server with two 6-core Intel Xeon E5-2620 v3@2.40ghz CPUs, two GeForce GTX TITAN X and 128 GB of memory.", "labels": [], "entities": []}, {"text": "500 epochs cost nearly one hour.", "labels": [], "entities": []}, {"text": "shows the experimental results on DBP15K and DWY100K.", "labels": [], "entities": [{"text": "DBP15K", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.9135497212409973}, {"text": "DWY100K", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9450852870941162}]}, {"text": "In general, MuGNN significantly outperforms all baselines regarding all metrics, mainly because it reconciles the structural differences by two different schemes for KG completion and pruning, which are thus well modeled in multi-channel GNN.", "labels": [], "entities": [{"text": "KG completion", "start_pos": 166, "end_pos": 179, "type": "TASK", "confidence": 0.8360016345977783}]}], "tableCaptions": [{"text": " Table 1: Statistics of DBP15K and DWY100k.", "labels": [], "entities": [{"text": "DBP15K", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.972929060459137}, {"text": "DWY100k", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9439082741737366}]}, {"text": " Table 2: Statistics of KG inference and transfer.", "labels": [], "entities": [{"text": "KG inference", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.5005884617567062}]}]}