{"title": [{"text": "Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment", "labels": [], "entities": []}], "abstractContent": [{"text": "When a speaker, Mary, asks Do you know that Florence is packed with visitors?, we take her to believe that Florence is packed with visitors, but not if she asks Do you think that Florence is packed with visitors?", "labels": [], "entities": []}, {"text": "Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering.", "labels": [], "entities": [{"text": "Inferring speaker commitment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7015433410803477}, {"text": "information extraction", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.835970014333725}, {"text": "question answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8945165276527405}]}, {"text": "Here we explore the hypothesis that linguistic deficits drive the error patterns of speaker commitment models by analyzing the linguistic correlates of model errors on a challenging naturalistic dataset.", "labels": [], "entities": []}, {"text": "We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses.", "labels": [], "entities": []}, {"text": "The Com-mitmentBank is annotated with speaker commitment towards the content of the complement (Florence is packed with visitors in our example) of clause-embedding verbs (know, think) under four entailment-canceling environments.", "labels": [], "entities": []}, {"text": "We found that a linguistically-informed model outperforms a LSTM-based one, suggesting that linguistic knowledge is needed to capture such challenging naturalis-tic data.", "labels": [], "entities": []}, {"text": "A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Prediction of speaker commitment 1 is the task of determining to what extent the speaker is committed to an event in a sentence as actual, nonactual, or uncertain.", "labels": [], "entities": [{"text": "Prediction of speaker commitment 1", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6517100095748901}]}, {"text": "This matters for downstream NLP applications, such as information extraction or question answering: for instance, we should extract from example (1) in that the speaker could wish someone dead, but from (3) that people should not be allowed to carry guns in their vehicles, even though both events are embedded under believe and negation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.8009468019008636}, {"text": "question answering", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8167164325714111}]}, {"text": "There has been work on factors leading to speaker commitment in theoretical linguistics (i.a.,;) and computational linguistics (i.a.,;;), but mostly on constructed or newswire examples, which may simplify the task by failing to reflect the lexical and syntactic diversity of naturally occurring utterances.", "labels": [], "entities": []}, {"text": "de introduced the CommitmentBank, a dataset of naturally occurring sentences annotated with speaker commitment towards the content of complements of clause-embedding verbs under canceling-entailment environments (negation, modal, question and conditional), to study the linguistic correlates of speaker commitment.", "labels": [], "entities": []}, {"text": "In this paper, we use it to evaluate two state-of-the-art (SoA) models of speaker commitment: and . The CommitmentBank, restricted to specific linguistic constructions, is a good test case.", "labels": [], "entities": []}, {"text": "It allows us to evaluate whether current speaker commitment models achieve robust language understanding, by analyzing their performance on specific challenging linguistic constructions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the models of Stanovsky et al., we trained the linear, tree, and hybrid biLSTM models using the multi-task training scheme on the four factuality datasets they used, which produced four predictions.", "labels": [], "entities": []}, {"text": "Following , we used cross-validated ridge regression to predict a final score using the four predictions.", "labels": [], "entities": []}, {"text": "We include a majority baseline \"All -2.0\" (always predicting -2.0, since -2.0 is the most frequent answer in the full and restricted CommitmentBank).", "labels": [], "entities": [{"text": "predicting -2.0", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9235682090123495}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The rule-based model outperforms the biLSTM models on the full set, but overall both SoA models do not perform very well on the CommitmentBank.", "labels": [], "entities": []}, {"text": "As shown in, the CommitmentBank is substantially more challenging for these models than the reference datasets, with lower correlation and higher absolute error rates than were obtained for any of these other datasets.", "labels": [], "entities": [{"text": "correlation", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.9923761487007141}, {"text": "absolute error rates", "start_pos": 146, "end_pos": 166, "type": "METRIC", "confidence": 0.8915481368700663}]}], "tableCaptions": [{"text": " Table 3: Performance and number of items per feature.  The scores in bold indicate the classes on which each  model has the best performance (with respect to both  metrics).  \u2020 marks statistical significance of Pearson's  correlation (p < 0.05).", "labels": [], "entities": [{"text": "Pearson's  correlation", "start_pos": 212, "end_pos": 234, "type": "METRIC", "confidence": 0.6262586116790771}]}, {"text": " Table 4: Classification performance of the models.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9057049751281738}]}]}