{"title": [{"text": "Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition", "labels": [], "entities": [{"text": "Dual Adversarial Neural Transfer", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6831787377595901}, {"text": "Low-Resource Named Entity Recognition", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.6968870609998703}]}], "abstractContent": [{"text": "We propose anew neural transfer method termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.8146520555019379}]}, {"text": "Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are investigated to explore effective feature fusion between high and low resource.", "labels": [], "entities": []}, {"text": "To address the noisy and imbal-anced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD).", "labels": [], "entities": []}, {"text": "Additionally, adversarial training is adopted to boost model generalization.", "labels": [], "entities": [{"text": "model generalization", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7274196147918701}]}, {"text": "In experiments , we examine the effects of different components in DATNet across domains and languages, and show that significant improvement can be obtained especially for low-resource data, without augmenting any additional hand-crafted features and pre-trained language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER) is an important step inmost natural language processing (NLP) applications.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8054468234380087}]}, {"text": "It detects not only the type of named entity, but also the entity boundaries, which requires deep understanding of the contextual semantics to disambiguate the different entity types of same tokens.", "labels": [], "entities": []}, {"text": "To tackle this challenging problem, most early studies were based on handcrafted rules, which suffered from limited performance in practice.", "labels": [], "entities": []}, {"text": "Current methods are devoted to developing learning based algorithms, especially neural network based methods, and have been advancing the state-of-the-art progressively.", "labels": [], "entities": []}, {"text": "These end-to-end models generalize well on new entities based on features automatically learned from the data.", "labels": [], "entities": []}, {"text": "However, when \u2020 The first two authors contributed equally.", "labels": [], "entities": []}, {"text": "the annotated corpora is small, especially in the low resource scenario (, the performance of these methods degrades significantly since the hidden feature representations cannot be learned adequately.", "labels": [], "entities": []}, {"text": "Recently, more and more approaches have been proposed to address low-resource NER.", "labels": [], "entities": []}, {"text": "Early works () primarily assumed a large parallel corpus and focused on exploiting them to project information from high-to low-resource.", "labels": [], "entities": []}, {"text": "Unfortunately, such a large parallel corpus may not be available for many low-resource languages.", "labels": [], "entities": []}, {"text": "More recently, crossresource word embedding) was proposed to bridge the low-and high-resources and enable knowledge transfer.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7846246361732483}]}, {"text": "Although the aforementioned transfer-based methods show promising performance in low-resource NER, there are two issues remain further study: 1) Representation Difference -they did not consider the representation difference across resources and enforced the feature representation to be shared across languages/domains; 2) Resource Data Imbalancethe training size of high-resource is usually much larger than that of low-resource.", "labels": [], "entities": [{"text": "Representation Difference", "start_pos": 145, "end_pos": 170, "type": "TASK", "confidence": 0.7117590606212616}]}, {"text": "The existing methods neglect such difference in their models, resulting in poor generalization.", "labels": [], "entities": []}, {"text": "In this work, we present a general neural transfer framework termed Dual Adversarial Transfer Network (DATNet) to address the above issues in a unified framework for low-resource NER.", "labels": [], "entities": []}, {"text": "Specifically, to handle the representation difference, we first investigate on two architectures of hidden layers (Bi-LSTM) for transfer.", "labels": [], "entities": []}, {"text": "The first one is that all the units in hidden layers are common units shared across languages/domains.", "labels": [], "entities": []}, {"text": "Another is composed of both private and common units, where the private part preserves the independent language/domain information.", "labels": [], "entities": []}, {"text": "Extensive experiments are conducted to show that there is not always a winner and two transfer strategies have their own advantages over each other in different situations, which is largely ignored by existing research.", "labels": [], "entities": []}, {"text": "On top of common units, the adversarial discriminator (AD) loss is introduced to encourage the resource-agnostic representation so that the knowledge from high resource can be more compatible with low resource.", "labels": [], "entities": [{"text": "adversarial discriminator (AD) loss", "start_pos": 28, "end_pos": 63, "type": "METRIC", "confidence": 0.7402646740277609}]}, {"text": "To handle the resource data imbalance issue, we further propose a variant of the AD loss, termed Generalized Resource-Adversarial Discriminator (GRAD), to impose the resource weight during training so that low-resource and hard samples can be paid more attention to.", "labels": [], "entities": [{"text": "AD loss", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.86065673828125}]}, {"text": "In addition, we create adversarial samples to conduct the Adversarial Training (AT), further improving the generalization and alleviating over-fitting problem.", "labels": [], "entities": [{"text": "Adversarial Training (AT", "start_pos": 58, "end_pos": 82, "type": "METRIC", "confidence": 0.5752452239394188}]}, {"text": "We unify two kinds of adversarial learning, i.e., GRAD and AT, into one transfer learning model, termed Dual Adversarial Transfer Network (DATNet), to achieve end-toend training and obtain significant improvements on a series of NER tasks In contrast with prior methods, we do not use additional hand-crafted features and do not use cross-lingual word embeddings as well as pre-trained language models) when addressing the crosslanguage tasks.", "labels": [], "entities": [{"text": "AT", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9818975329399109}]}], "datasetContent": [{"text": "In order to evaluate the performance of DATNet, we conduct the experiments on following widely used NER datasets:,,.", "labels": [], "entities": [{"text": "NER datasets", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.7486542761325836}]}, {"text": "The statistics of these datasets are described in   the CRF layer or introduce the orthographic feature as additional input for learning social media NER in tweets (), we do not use hand-crafted features and only words and characters are considered as the inputs.", "labels": [], "entities": []}, {"text": "Our goal is to study the effects of transferring knowledge from high-resource dataset to low-resource dataset.", "labels": [], "entities": []}, {"text": "To be noted, we used only training set for model training for all datasets except the WNUT-2016 NER dataset.", "labels": [], "entities": [{"text": "WNUT-2016 NER dataset", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.886161208152771}]}, {"text": "Since in this dataset, all the previous studies merged the training set and validation set together for training.", "labels": [], "entities": []}, {"text": "Specifically, we use CoNLL-2003 English NER dataset as high-resource (i.e., source) for all the experiments, CoNLL-2002 and WNUT datasets as low-resource (i.e., target) in cross-language and cross-domain NER settings, respectively.", "labels": [], "entities": [{"text": "CoNLL-2003 English NER dataset", "start_pos": 21, "end_pos": 51, "type": "DATASET", "confidence": 0.9005489200353622}, {"text": "CoNLL-2002", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.9466184377670288}, {"text": "WNUT datasets", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.9406696856021881}]}, {"text": "We use 50-dimensional publicly available pretrained word embeddings for English, Spanish and Dutch of CoNLL and WNUT datasets in our experiments, which are trained by word2vec on the corresponding Wikipedia articles (, and the 30-dimensional randomly initialized character embeddings are used for all the datasets.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.8939467072486877}, {"text": "WNUT datasets", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.8616252243518829}]}, {"text": "We set the filters as 20 for char-level CNN and the dimension of hidden states of the word-level LSTM as 200 for both base model and DATNet-F.", "labels": [], "entities": []}, {"text": "For DATNet-P, we set 100 for source, share, and target LSTMs, respectively.", "labels": [], "entities": []}, {"text": "Parameters optimization is performed by Adam () with gradient clipping of 5.0 and learning rate decay strategy.", "labels": [], "entities": [{"text": "Parameters optimization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7478892207145691}]}, {"text": "We set the initial learning rate of \u03b2 0 = 0.001 for all experiments.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.7200687825679779}, {"text": "\u03b2 0", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9234290421009064}]}, {"text": "At each epoch t, learning rate \u03b2 t is updated using , where \u03c1 is decay rate with 0.05.", "labels": [], "entities": []}, {"text": "To reduce over-fitting, we apply Dropout () to the embedding layer and the output of the LSTM layer, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison with State-of-the-art Results in CoNLL and WNUT datasets (F1-score).", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.9107577204704285}, {"text": "WNUT datasets", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.8528822958469391}, {"text": "F1-score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9983208775520325}]}, {"text": " Table 3: Experiments on Extremely Low Resource (F1-score).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.914760172367096}]}, {"text": " Table 4: Quantitative Performance Comparison between Models with Different Components.", "labels": [], "entities": [{"text": "Quantitative Performance Comparison", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.8503270546595255}]}, {"text": " Table 5: Analysis of Discriminator Weight \u03b1 in GRAD with Varying Data Ratio \u03c1 (F1-score).", "labels": [], "entities": [{"text": "Discriminator Weight \u03b1", "start_pos": 22, "end_pos": 44, "type": "METRIC", "confidence": 0.8635631004969279}, {"text": "GRAD", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.798831582069397}, {"text": "Varying Data Ratio \u03c1", "start_pos": 58, "end_pos": 78, "type": "METRIC", "confidence": 0.9343264251947403}, {"text": "F1-score", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9818875193595886}]}, {"text": " Table 6: Analysis of Maximum Perturbation w T in AT  with Varying Data Ratio \u03c1 (F1-score).", "labels": [], "entities": [{"text": "Maximum Perturbation w T", "start_pos": 22, "end_pos": 46, "type": "METRIC", "confidence": 0.7565213590860367}, {"text": "Varying Data Ratio \u03c1", "start_pos": 59, "end_pos": 79, "type": "METRIC", "confidence": 0.9536068141460419}, {"text": "F1-score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9824624061584473}]}, {"text": " Table 5.  From the results, it is interesting to find that \u03b1  is directly proportional to the data ratio \u03c1, basi- cally, which means that more target training data  requires larger \u03b1 (i.e., smaller 1\u2212\u03b1 to reduce train- ing emphasis on the target domain) to achieve bet- ter performance.", "labels": [], "entities": [{"text": "basi", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9535830616950989}]}]}