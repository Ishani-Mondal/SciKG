{"title": [{"text": "Classification and Clustering of Arguments with Contextualized Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We experiment with two recent contextual-ized word embedding methods (ELMo and BERT) in the context of open-domain argument search.", "labels": [], "entities": [{"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9943104982376099}, {"text": "open-domain argument search", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6002834240595499}]}, {"text": "For the first time, we show how to leverage the power of contextual-ized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets.", "labels": [], "entities": []}, {"text": "For argument classification, we improve the state-of-the-art for the UKP Senten-tial Argument Mining Corpus by 20.8 percentage points and for the IBM Debater-Evidence Sentences dataset by 7.4 percentage points.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9216691553592682}, {"text": "UKP Senten-tial Argument Mining Corpus", "start_pos": 69, "end_pos": 107, "type": "DATASET", "confidence": 0.911766517162323}, {"text": "IBM Debater-Evidence Sentences dataset", "start_pos": 146, "end_pos": 184, "type": "DATASET", "confidence": 0.7111808508634567}]}, {"text": "For the understudied task of argument clustering , we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.", "labels": [], "entities": [{"text": "argument clustering", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.747463047504425}]}], "introductionContent": [{"text": "Argument mining methods have been applied to different tasks such as identifying reasoning structures (, assessing the quality of arguments (), or linking arguments from different documents.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7483150362968445}]}, {"text": "Broadly speaking, existing methods either approach argument mining from the discourse-level perspective (aiming to analyze local argumentation structures), or from an information-seeking perspective (aiming to detect arguments relevant to a predefined topic).", "labels": [], "entities": [{"text": "argument mining", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.8121110498905182}]}, {"text": "While discourse-level approaches mostly focus on the analysis of single documents or document collections (, information-seeking approaches need to be capable of dealing with heterogeneous sources and topics and also face the problem of redundancy, as arguments might be repeated across sources.", "labels": [], "entities": []}, {"text": "As a result, this perspective naturally calls fora subsequent clustering step, which is able to identify and aggregate similar arguments for the same topic.", "labels": [], "entities": []}, {"text": "In this work, we focus on the latter perspective, referring to it as open-domain argument search, and show how contextualized word embeddings can be leveraged to overcome some of the challenges involved in topic-dependent argument classification and clustering.", "labels": [], "entities": [{"text": "topic-dependent argument classification", "start_pos": 206, "end_pos": 245, "type": "TASK", "confidence": 0.6639127631982168}]}, {"text": "Identifying arguments for unseen topics is a challenging task for machine learning systems.", "labels": [], "entities": [{"text": "Identifying arguments for unseen topics", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8914675712585449}]}, {"text": "The lexical appearance for two topics, e.g. \"net neutrality\" and \"school uniforms\", is vastly different.", "labels": [], "entities": []}, {"text": "Hence, in order to perform well, systems must develop a deep semantic understanding of both the topic as well as the sources to search for arguments.", "labels": [], "entities": []}, {"text": "Even more so, clustering similar arguments is a demanding task, as fine-grained semantic nuances may determine whether two arguments (talking about the same topic) are similar.", "labels": [], "entities": []}, {"text": "gives an example of arguments on the topic \"net neutrality\".", "labels": [], "entities": [{"text": "net neutrality", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7483772039413452}]}, {"text": "Both arguments center around the aspect of \"equal access for every Internet user\" but are differently phrased.", "labels": [], "entities": []}], "datasetContent": [{"text": "No dataset is available that allows evaluating open-domain argument search end-to-end.", "labels": [], "entities": []}, {"text": "Hence, we analyze and evaluate the involved steps (argument classification and clustering) independently.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7430972158908844}]}, {"text": "We differentiate between unsupervised and supervised methods.", "labels": [], "entities": []}, {"text": "Our unsupervised methods include no pre-training whereas the supervised methods use some data for fine-tuning the model.", "labels": [], "entities": []}, {"text": "For the UKP ASPECT corpus, we binarize the four labels to only indicate similar and dissimilar argument pairs.", "labels": [], "entities": [{"text": "UKP ASPECT corpus", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9358512361844381}]}, {"text": "Pairs labeled with some and high similarity were labeled as similar, pairs with no similarity and different topic as dissimilar.", "labels": [], "entities": []}, {"text": "We evaluate methods in a 4-fold crossvalidation setup: seven topics are used for testing and 21 topics are used for fine-tuning.", "labels": [], "entities": []}, {"text": "Final evaluation results are the average over the four folds.", "labels": [], "entities": []}, {"text": "In case of supervised clustering methods, we use 17 topics for training and four topics for tuning.", "labels": [], "entities": []}, {"text": "In  their experiments on the AFS corpus, only performed a within-topic evaluation by using 10-fold cross-validation.", "labels": [], "entities": [{"text": "AFS corpus", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.9335927069187164}]}, {"text": "As we are primarily interested in cross-topic performances, we evaluate our methods also cross-topic: we train on two topics, and evaluate on the third.", "labels": [], "entities": []}, {"text": "For the UKP ASPECT dataset we compute the marco-average F mean for the F 1 -scores for the similar-label (F sim ) and for the dissimilarlabel (F dissim ).", "labels": [], "entities": [{"text": "UKP ASPECT dataset", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.9172049562136332}, {"text": "marco-average F mean", "start_pos": 42, "end_pos": 62, "type": "METRIC", "confidence": 0.8958863814671835}, {"text": "F 1 -scores", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.8654975742101669}]}, {"text": "In the without clustering setup, we compute the similarity metric) for an argument pair directly, and assign the label similar if it exceeds a threshold, otherwise dissimilar.", "labels": [], "entities": []}, {"text": "The threshold is determined on the train set of a fold for unsupervised methods.", "labels": [], "entities": []}, {"text": "For supervised methods, we use a held-out dev set.", "labels": [], "entities": []}, {"text": "In the with clustering setup, we use the similarity metric to perform agglomerative clustering.", "labels": [], "entities": []}, {"text": "This assigns each argument exactly one cluster ID.", "labels": [], "entities": []}, {"text": "Arguments pairs in the same cluster are assigned the label similar, and argument pairs in different clusters are assigned the label dissimilar.", "labels": [], "entities": []}, {"text": "We use these labels to compute F sim and F dissim given our gold label annotations.", "labels": [], "entities": [{"text": "F dissim", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9451088309288025}]}, {"text": "For the AFS dataset, computed the correlation between the predicted similarity and the annotated similarity score.", "labels": [], "entities": [{"text": "AFS dataset", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.9693492352962494}, {"text": "annotated similarity score", "start_pos": 87, "end_pos": 113, "type": "METRIC", "confidence": 0.6989778975645701}]}, {"text": "They do not mention which correlation method they used.", "labels": [], "entities": []}, {"text": "In our evaluation, we show Pearson correlation (r) and Spearman's rank correlation coefficient (\u03c1).", "labels": [], "entities": [{"text": "Pearson correlation (r)", "start_pos": 27, "end_pos": 50, "type": "METRIC", "confidence": 0.9411554932594299}, {"text": "Spearman's rank correlation coefficient (\u03c1)", "start_pos": 55, "end_pos": 98, "type": "METRIC", "confidence": 0.876650333404541}]}], "tableCaptions": [{"text": " Table 1: Results of each model for sentence-level argument classification using cross-topic evaluation on the UKP  Sentential Argument Mining Corpus and on the IBM Debater R  -Evidence Sentences dataset. Blank fields result  from dataset-specific models. P: precision, R: recall, arg+: pro-arguments, arg-: con-arguments.", "labels": [], "entities": [{"text": "sentence-level argument classification", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.6253083248933157}, {"text": "UKP  Sentential Argument Mining Corpus", "start_pos": 111, "end_pos": 149, "type": "DATASET", "confidence": 0.9125804901123047}, {"text": "IBM Debater R  -Evidence Sentences dataset", "start_pos": 161, "end_pos": 203, "type": "DATASET", "confidence": 0.7925628508840289}, {"text": "precision", "start_pos": 259, "end_pos": 268, "type": "METRIC", "confidence": 0.9814886450767517}, {"text": "recall", "start_pos": 273, "end_pos": 279, "type": "METRIC", "confidence": 0.9264819025993347}]}, {"text": " Table 2: F 1 scores on the UKP ASPECT Corpus.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9827871024608612}, {"text": "UKP ASPECT Corpus", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.8979973395665487}]}, {"text": " Table 3: Pearson correlation r and Spearman's rank  correlation \u03c1 on the AFS dataset (Misra et al., 2016)  averaged over the three topics.", "labels": [], "entities": [{"text": "Pearson correlation r", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9788308342297872}, {"text": "Spearman's rank  correlation \u03c1", "start_pos": 36, "end_pos": 66, "type": "METRIC", "confidence": 0.7548269093036651}, {"text": "AFS dataset", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9880139827728271}]}, {"text": " Table 4: F 1 scores on the UKP ASPECT Corpus with  increasing training set sizes (BERT model).", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9790796339511871}, {"text": "UKP ASPECT Corpus", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.9046225349108378}, {"text": "BERT", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9969845414161682}]}, {"text": " Table 5: Pearson correlation r and Spearman's rank correlation \u03c1 on the AFS dataset. Within-Topic Evaluation:  10-fold cross-validation. Cross-Topic Evaluation: System trained on two topics, evaluated on the third.", "labels": [], "entities": [{"text": "Pearson correlation r", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9637319644292196}, {"text": "Spearman's rank correlation \u03c1", "start_pos": 36, "end_pos": 65, "type": "METRIC", "confidence": 0.7823473691940308}, {"text": "AFS dataset", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.9859656095504761}]}]}