{"title": [{"text": "SEMBLEU: A Robust Metric for AMR Parsing Evaluation", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6934823393821716}, {"text": "AMR Parsing", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.968202143907547}]}], "abstractContent": [{"text": "Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9023700654506683}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.8462762832641602}]}, {"text": "The major evaluation metric, SMATCH (Cai and Knight, 2013), searches for one-to-one mappings between the nodes of two AMRs with a greedy hill-climbing algorithm, which leads to search errors.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.6083272099494934}]}, {"text": "We propose SEMBLEU, a robust metric that extends BLEU (Papineni et al., 2002) to AMRs.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9805574417114258}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9986577033996582}]}, {"text": "It does not suffer from search errors and considers non-local correspondences in addition to local ones.", "labels": [], "entities": []}, {"text": "SEMBLEU is fully content-driven and punishes situations where a system's output does not preserve most information from the input.", "labels": [], "entities": []}, {"text": "Preliminary experiments on both sentence and corpus levels show that SEMBLEU has slightly higher consistency with human judgments than SMATCH.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.4630414545536041}, {"text": "consistency", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9930296540260315}]}, {"text": "Our code is available at http://github.com/ freesunshine0316/sembleu.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8844736715157827}]}, {"text": "shows two AMR graphs in which the nodes (such as \"girl\" and \"leave-11\") represent AMR concepts and the edges (such as \"ARG0\" and \"ARG1\") represent relations between the concepts.", "labels": [], "entities": []}, {"text": "The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding.", "labels": [], "entities": [{"text": "parsing sentences into AMRs", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.8729652166366577}]}, {"text": "Despite the large amount of work on AMR parsing (;, little attention has been paid to evaluating the parsing results, leaving SMATCH  (  as the only overall performance metric.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.8937225639820099}, {"text": "SMATCH", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.6695051789283752}]}, {"text": "developed a suite of fine-grained performance measures based on the node mappings of SMATCH (see below).", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.7599239945411682}]}, {"text": "SMATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between two AMRs (finding the exact best mapping is NP-complete).", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.8213187456130981}]}, {"text": "The search errors weaken its robustness as a metric.", "labels": [], "entities": []}, {"text": "To enhance robustness, the hill-climbing search is executed multiple times with random restarts.", "labels": [], "entities": []}, {"text": "This decreases efficiency and, more importantly, does not eliminate search errors.", "labels": [], "entities": [{"text": "efficiency", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9679913520812988}]}, {"text": "shows the means and error bounds of SMATCH scores as a function of the number of restarts rover 100 runs on 100 sentences.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 36, "end_pos": 42, "type": "TASK", "confidence": 0.5746690630912781}]}, {"text": "We can see that the variances are still significant when r is large.", "labels": [], "entities": []}, {"text": "Furthermore, by corresponding with other researchers, we have learned that previous papers on AMR parsing report SMATCH scores using differing values of r.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.9232653975486755}, {"text": "SMATCH", "start_pos": 113, "end_pos": 119, "type": "TASK", "confidence": 0.47696632146835327}]}, {"text": "Another problem is that SMATCH maps one node to another regardless of their actual content, and it only considers edge labels when comparing two edges.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 24, "end_pos": 30, "type": "TASK", "confidence": 0.9243682622909546}]}, {"text": "As a result, two different edges, such as \"ask-01 :ARG1 leave-11\" and \"make-01 :ARG1 pie\" in, can be considered identical by SMATCH.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 125, "end_pos": 131, "type": "TASK", "confidence": 0.7629891633987427}]}, {"text": "This can lead to a overly large score for two completely different AMRs.", "labels": [], "entities": []}, {"text": "As shown in, SMATCH gives a score of 25% for the two AMRs meaning \"The girl asked the boy to leave\" and \"The woman made two pies\", which convey obviously different meanings.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.7825035452842712}, {"text": "AMRs", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.977696418762207}]}, {"text": "The situation could be worse for two different AMRs with few types of edge labels, where the score could reach 50% if all pairs of edges between them were accidentally matched.", "labels": [], "entities": []}, {"text": "To tackle the problems above, we introduce SEMBLEU, an accurate metric for comparing AMR graphs.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9956399202346802}]}, {"text": "SEMBLEU extends BLEU (, which has been shown to be effective for evaluating a wide range of text generation tasks, such as machine translation and data-to-text generation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9985924363136292}, {"text": "text generation tasks", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7703859806060791}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8087557852268219}, {"text": "data-to-text generation", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.7301636785268784}]}, {"text": "In general, a BLEU score is a precision score calculated by comparing the n-grams (n is up to 4) of a predicted sentence to those of a reference sentence.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9826466143131256}, {"text": "precision score", "start_pos": 30, "end_pos": 45, "type": "METRIC", "confidence": 0.9807908236980438}]}, {"text": "To punish very short predictions, it is multiplied by a brevity penalty, which is less than 1.0 fora prediction shorter than its reference.", "labels": [], "entities": []}, {"text": "To adapt BLEU for comparing AMRs, we treat each AMR node (such as \"ask-01\") as a unigram, and we take each pair of directly connected AMR nodes with their relation (such as \"ask-01 :ARG0 girl\") as a bigram.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9793837666511536}, {"text": "ARG0 girl", "start_pos": 182, "end_pos": 191, "type": "DATASET", "confidence": 0.8383756279945374}]}, {"text": "Higher-order n-grams (such as \"ask-01 :ARG1 leave-11 :ARG0 boy\") are defined in a similar way.", "labels": [], "entities": [{"text": "ARG1 leave-11 :ARG0 boy", "start_pos": 39, "end_pos": 62, "type": "DATASET", "confidence": 0.7998637557029724}]}, {"text": "SEMBLEU has several advantages over SMATCH.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6160642504692078}, {"text": "SMATCH", "start_pos": 36, "end_pos": 42, "type": "TASK", "confidence": 0.6183563470840454}]}, {"text": "First, it gives an exact score for each pair of AMRs without search errors.", "labels": [], "entities": [{"text": "exact score", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9622231125831604}]}, {"text": "Second, it is very efficient to calculate.", "labels": [], "entities": []}, {"text": "On a dataset of 1368 pairs of AMRs, SEMBLEU takes 0.5 seconds, while SMATCH takes almost 2 minutes using the same machine.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9833511710166931}, {"text": "SMATCH", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8131968975067139}]}, {"text": "Third, it captures high-order relations in addition to node-to-node and edge-toedge correspondences.", "labels": [], "entities": []}, {"text": "This gives complementary judgments to the standard SMATCH metric for evaluating AMR parsing quality.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 51, "end_pos": 57, "type": "TASK", "confidence": 0.4524971842765808}, {"text": "AMR parsing", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.953179121017456}]}, {"text": "Last, it does not give overly large credit to AMRs that represent completely different meanings.", "labels": [], "entities": []}, {"text": "Our initial evaluations suggest that SEMBLEU has higher consistency with human judgments than SMATCH on both corpus-level and sentencelevel evaluations.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.5118324160575867}, {"text": "consistency", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.990826427936554}]}, {"text": "We also show that the number of n-grams extracted by SEMBLEU is roughly linear in the AMR scale.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.7134588956832886}, {"text": "AMR scale", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.5552733242511749}]}, {"text": "Evaluation on the outputs of several recent models show that SEMBLEU is mostly consistent with SMATCH for results ranking, but with occasional disagreements.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9281459450721741}, {"text": "SMATCH", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9586663842201233}]}], "datasetContent": [{"text": "We compare SEMBLEU with SMATCH on the outputs of 4 systems over 100 sentences from the testset of LDC2015E86.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.946485161781311}, {"text": "SMATCH", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9419903755187988}, {"text": "LDC2015E86", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.9597895741462708}]}, {"text": "These systems are: CAMR, 3 JAMR, 4 Gros ( and Lyu (.", "labels": [], "entities": [{"text": "CAMR", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.6562645435333252}, {"text": "JAMR", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.455678790807724}]}, {"text": "For each sentence, following, annotators decide relative orders instead of a complete order overall systems.", "labels": [], "entities": []}, {"text": "In particular, 4 system outputs are randomly grouped into 2 pairs to make 2 comparisons.", "labels": [], "entities": []}, {"text": "For each pair, we ask 3 annotators to decide which one is better and choose the majority vote as the final judgment.", "labels": [], "entities": []}, {"text": "All the annotators have several years experience on AMR-related research, and the judgments are based on their impression on how well a system-generated AMR retains the meaning of the reference AMR.", "labels": [], "entities": [{"text": "AMR-related", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9342666864395142}]}, {"text": "Out of the 200 comparisons, annotators are fully agree on 142, accounting for 71%.", "labels": [], "entities": []}, {"text": "With the judgments, we study consistencies of both metrics on sentence and corpus levels.", "labels": [], "entities": []}, {"text": "We consider all unigrams, bigrams and trigrams for SEMBLEU, and the weights (w k sin Equation 1) are equivalent (1/3 for each).", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.355787992477417}]}, {"text": "For sentencelevel evaluation, we follow previous work to use NIST geometric smoothing.", "labels": [], "entities": [{"text": "sentencelevel evaluation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7829227149486542}, {"text": "NIST geometric smoothing", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6507700681686401}]}, {"text": "Following SMATCH, inverse relations such as \"ARG0-of\", are reversed before extracting ngrams for making comparisons.", "labels": [], "entities": [{"text": "ARG0-of", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9511939287185669}]}, {"text": "For corpus-level comparison, we assign each system a human score equal to the number of times that system's output was preferred.", "labels": [], "entities": []}, {"text": "Our four systems achieved human scores of 30, 33, 63 and 74.", "labels": [], "entities": []}, {"text": "They achieved SEMBLEU scores of 28, 30, 38 and 41, respectively, and SMATCH scores of 56, 56, 63 and 67, respectively.", "labels": [], "entities": [{"text": "SEMBLEU scores", "start_pos": 14, "end_pos": 28, "type": "METRIC", "confidence": 0.9738086462020874}, {"text": "SMATCH scores", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9854949116706848}]}, {"text": "SEMBLEU is generally more consistent with the 78.0 SEMBLEU (n=4) 80.0: Sentence-level accuracies, where the highest n-gram order is set to 3 by default, unless specified.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7452879548072815}]}, {"text": "In particular, there is a tie between CAMR and JAMR for SMATCH scores, while SEMBLEU scores are more discriminating.", "labels": [], "entities": [{"text": "CAMR", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.7280662655830383}, {"text": "JAMR", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.7086588144302368}, {"text": "SMATCH scores", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.9304750561714172}, {"text": "SEMBLEU", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9405698776245117}]}, {"text": "We use the script-default 2 significant digits when calculating SMATCH scores, as their variance can be very large).", "labels": [], "entities": [{"text": "SMATCH scores", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.7028857469558716}]}, {"text": "To make fair comparison, we also use 2 significant digits for SEMBLEU.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.7563856244087219}]}, {"text": "Bootstrap tests To conduct more comprehensive comparisons, we use bootstrap resampling) to obtain 1000 new datasets, each having 100 instances.", "labels": [], "entities": []}, {"text": "Every dataset contains the references, 4 system outputs and the corresponding human scores.", "labels": [], "entities": []}, {"text": "Using the new datasets, we check how frequently SEMBLEU and SMATCH are consistent with human judgments on the corpus level as away to perform significant test.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9798470735549927}, {"text": "SMATCH", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.963943362236023}]}, {"text": "shows the accuracies of both metrics across all 6 system pairs (such as CAMR vs Lyu).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9929966330528259}]}, {"text": "Overall, SEMBLEU is equal to or slightly better than SMATCH across all system pairs.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9971871972084045}, {"text": "SMATCH", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.946135938167572}]}, {"text": "The advantages are not significant at p < .05, perhaps because of the small data size, yet human judgments on large-scale data is very time consuming.", "labels": [], "entities": []}, {"text": "Comparatively, the precisions of both metrics on CAMR vs JAMR is lower than the other system pairs.", "labels": [], "entities": [{"text": "precisions", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9994301199913025}, {"text": "JAMR", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.5650232434272766}]}, {"text": "It is likely because the gaps of this system pair on both human and metric scores are much smaller than the other system pairs.", "labels": [], "entities": []}, {"text": "Still, SEM-BLEU is better than SMATCH on this system pair, showing that it maybe more consistent with human evaluation.", "labels": [], "entities": [{"text": "SEM-BLEU", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.8276305198669434}, {"text": "SMATCH", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9980185031890869}]}, {"text": "For sentence-level comparison, we calculate the frequency with which a metric is consistent with human judgments on a pair of sentences.", "labels": [], "entities": []}, {"text": "Recall that we make two pairs out of the 4 outputs for each sentence, thus there are 200 pairs in total.", "labels": [], "entities": []}, {"text": "As shown in the upper group of, SEM-BLEU is 5.0 points better than SMATCH, meaning that it makes 10 more correct evaluations than SMATCH over the 200 instances.", "labels": [], "entities": [{"text": "SEM-BLEU", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.5666912198066711}, {"text": "SMATCH", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.41342025995254517}]}, {"text": "This indicates that SEMBLEU is more consistent with human judges than SMATCH.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.819313108921051}, {"text": "SMATCH", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.486568808555603}]}, {"text": "The lower group shows SEMBLEU accuracies with different order n.", "labels": [], "entities": [{"text": "SEMBLEU accuracies", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.89942666888237}]}, {"text": "With only unigram features (node-to-node correspondences), SEMBLEU is much worse than SMATCH.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.5591374039649963}, {"text": "SMATCH", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.4655010402202606}]}, {"text": "When incorporating bigrams and trigrams, SEM-BLEU gives consistently better numbers, demonstrating the usefulness of high-order features.", "labels": [], "entities": []}, {"text": "Further increasing n leads to a decrease of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.99915611743927}]}, {"text": "This is likely because humans care more about the whole-graph quality than occasional high-order matches.", "labels": [], "entities": []}, {"text": "shows the number of extracted n-grams as a function of the number of AMR nodes on the devset of the LDC2015E86 dataset, which has 1368 instances.", "labels": [], "entities": [{"text": "LDC2015E86 dataset", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.9717113673686981}]}, {"text": "The number of extracted unigrams is exactly the number of AMR nodes, which is expected.", "labels": [], "entities": []}, {"text": "The data points become less concentrated from bigrams to trigrams.", "labels": [], "entities": []}, {"text": "This is because the number of n-grams depends on not only the graph scale, but also how dense the graph is.", "labels": [], "entities": []}, {"text": "Overall, the amount of extracted n-grams is roughly linear in the number of nodes in the graph.", "labels": [], "entities": []}, {"text": "shows the SEMBLEU and SMATCH scores several recent models.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9730878472328186}, {"text": "SMATCH", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9233160614967346}]}, {"text": "In particular, we asked for the outputs of Lyu (, and to evaluate on our SEMBLEU.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9320036768913269}]}, {"text": "For CAMR and JAMR,  we obtain their outputs by running the released systems.", "labels": [], "entities": [{"text": "CAMR", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.505371630191803}, {"text": "JAMR", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8282844424247742}]}, {"text": "SEMBLEU is mostly consistent with SMATCH, except for the order between Guo and Gros.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9065962433815002}, {"text": "SMATCH", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.3439037501811981}]}, {"text": "It is probably because Guo has more highorder correspondences with the reference.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Corpus-level bootstrap accuracies (%) for each system pair.", "labels": [], "entities": [{"text": "Corpus-level bootstrap accuracies", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.5759923656781515}]}, {"text": " Table 4: SEMBLEU and SMATCH scores for several re- cent models.  \u2020 indicates previously reported result.", "labels": [], "entities": [{"text": "SEMBLEU", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9855367541313171}, {"text": "SMATCH", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9766073822975159}]}]}