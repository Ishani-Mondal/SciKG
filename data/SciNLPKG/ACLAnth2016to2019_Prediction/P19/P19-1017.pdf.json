{"title": [{"text": "Unsupervised Pivot Translation for Distant Languages", "labels": [], "entities": [{"text": "Unsupervised Pivot Translation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5449829200903574}]}], "abstractContent": [{"text": "Unsupervised neural machine translation (NMT) has attracted a lot of attention recently.", "labels": [], "entities": [{"text": "Unsupervised neural machine translation (NMT)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7209635504654476}]}, {"text": "While state-of-the-art methods for unsupervised translation usually perform well between similar languages (e.g., English-German translation), they perform poorly between distant languages, because unsu-pervised alignment does notwork well for distant languages.", "labels": [], "entities": []}, {"text": "In this work, we introduce unsupervised pivot translation for distant languages, which translates a language to a distant language through multiple hops, and the unsupervised translation on each hop is relatively easier than the original direct translation.", "labels": [], "entities": []}, {"text": "We propose a learning to route (LTR) method to choose the translation path between the source and target languages.", "labels": [], "entities": []}, {"text": "LTR is trained on language pairs whose best translation path is available and is applied to the unseen language pairs for path selection.", "labels": [], "entities": []}, {"text": "Experiments on 20 languages and 294 distant language pairs demonstrate the advantages of the unsupervised pivot translation for distant languages, as well as the effectiveness of the proposed LTR for path selection.", "labels": [], "entities": [{"text": "path selection", "start_pos": 200, "end_pos": 214, "type": "TASK", "confidence": 0.7754904627799988}]}, {"text": "Specifically, in the best case, LTR achieves an improvement of 5.58 BLEU points over the conventional direct unsupervised method.", "labels": [], "entities": [{"text": "LTR", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.474016010761261}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9994695782661438}]}], "introductionContent": [{"text": "Unsupervised neural machine translation (NMT) (, which uses only monolingual sentences for translation, is of great importance * The work was done when the first author was an intern at Microsoft Research Asia.", "labels": [], "entities": [{"text": "Unsupervised neural machine translation (NMT)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7138445973396301}]}, {"text": "Unsupervised cross-lingual alignment works reasonably well fora pair of similar languages, such as English-German or Portuguese-Galician, since they have similar lexica and syntax and share the same alphabets and language branch.", "labels": [], "entities": []}, {"text": "However, the alignment between a pair of distant languages, which are not in the same language branch 1 , such as Danish-Galician is challenging.", "labels": [], "entities": []}, {"text": "As a consequence, unsupervised translation between distant languages is usually of lower quality.", "labels": [], "entities": []}, {"text": "For example, the unsupervised NMT model achieves 23.43 BLEU score on PortugueseGalician translation, while just 6.56 on DanishGalician translation according to our experiments.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9800141155719757}, {"text": "PortugueseGalician translation", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.676802396774292}, {"text": "DanishGalician translation", "start_pos": 120, "end_pos": 146, "type": "TASK", "confidence": 0.6873582154512405}]}, {"text": "In this work, we focus on unsupervised translation of distant languages.", "labels": [], "entities": [{"text": "unsupervised translation of distant languages", "start_pos": 26, "end_pos": 71, "type": "TASK", "confidence": 0.7608673453330994}]}, {"text": "We observe that two distant languages can be linked through multiple intermediate hops where unsupervised translation of two languages on each hop is easier than direct translation of the two distance languages, considering that the two languages on each intermediate hop are more similar, or the size of monolingual training data is larger.", "labels": [], "entities": []}, {"text": "Therefore, we propose unsupervised pivot translation through multiple hops for distant languages, where each hop consists of unsupervised translation of a relatively easier language pair.", "labels": [], "entities": []}, {"text": "For example, the distant language pair Danish\u2192Galician can be translated by three easier hops: Danish\u2192English, English\u2192Spanish and Spanish\u2192Galician.", "labels": [], "entities": []}, {"text": "In this way, unsupervised pivot translation results in better accuracy (12.14 BLEU score) than direct unsupervised translation (6.56 BLEU score) from Danish to Galician in our experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9993997812271118}, {"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9746359884738922}, {"text": "BLEU score)", "start_pos": 133, "end_pos": 144, "type": "METRIC", "confidence": 0.9686997532844543}]}, {"text": "The challenge of unsupervised pivot translation is how to choose a good translation path.", "labels": [], "entities": []}, {"text": "Given a distant language pair X\u2192Y, there exists a large amount of paths that can translate from X to Y 2 , and different paths may yield very different translation accuracies.", "labels": [], "entities": []}, {"text": "Therefore, unsupervised pivot translation may result in lower accuracy than direct unsupervised translation if a poor path is chosen.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.99822598695755}]}, {"text": "How to choose a path with good translation accuracy is important to guarantee the performance of unsupervised pivot translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.915253221988678}]}, {"text": "A straightforward method is to calculate the translation accuracies of all possible paths on a validation set and choose the path with the best accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9973710775375366}]}, {"text": "However, it is computationally unaffordable due to the large amount of possible paths.", "labels": [], "entities": []}, {"text": "For example, suppose we consider at most 3 hops (N = 3) and 100 languages (M = 100), and assume each path takes an average of 20 minutes to translate all the sentences in the validation set using one NVIDIA P100 GPU to get the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 227, "end_pos": 237, "type": "METRIC", "confidence": 0.976159930229187}]}, {"text": "Then it will take nearly 1400000 GPU days to evaluate all candidate paths.", "labels": [], "entities": []}, {"text": "Even if we just consider 20 languages (M = 20), it will still take 2200 GPU days.", "labels": [], "entities": []}, {"text": "Therefore, an efficient method for path selection is needed.", "labels": [], "entities": [{"text": "path selection", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7614074051380157}]}, {"text": "We propose a learning to route (LTR) method that adopts a path accuracy predictor (a multi-layer LSTM) to select a good path fora distant language pair.", "labels": [], "entities": []}, {"text": "Given a translation path and the translation accuracy of each hop on the path, the path accuracy predictor can predict the overall translation accuracy following this Suppose we only consider translation paths with at most N hops.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.5932178497314453}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.7610957026481628}]}, {"text": "Given M candidate intermediate languages, there are path.", "labels": [], "entities": []}, {"text": "Such a predictor is first trained on a training set of paths with known overall accuracy, and then used to predict the accuracy of a path unseen before.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.994297206401825}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9974223375320435}]}, {"text": "We conduct experiments on a large dataset with 20 languages and a total of 294 distant language pairs to verify the effectiveness of our method.", "labels": [], "entities": []}, {"text": "Our proposed LTR achieves an improvement of more than 5 BLEU points on some language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9988960027694702}]}, {"text": "The contributions of this paper are as follows: (1) We introduce pivot translation into unsupervised NMT to improve the accuracy of distant languages.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7024884819984436}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9972656965255737}]}, {"text": "(2) We propose the learning to route (LTR) method to automatically select the good translation path.", "labels": [], "entities": []}, {"text": "(3) Large scale experiments on more than 20 languages and 294 distant language pairs demonstrate the effectiveness of our method.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments consist of two stages in general.", "labels": [], "entities": []}, {"text": "In the first stage, we need to train the unsupervised NMT model between any two languages to get the BLEU scores of each one-hop path.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9991726279258728}]}, {"text": "We also get the BLEU scores fora part of multi-hop paths through pivoting, which are used as the training and evaluation data for the second stage.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.99894779920578}]}, {"text": "In the second stage, we train the LTR model based on the training data generated in the first stage.", "labels": [], "entities": []}, {"text": "In this section, we give brief descriptions of the experiment settings for the unsupervised NMT model training (the first stage) and the LTR model training and path routing (the second stage).", "labels": [], "entities": [{"text": "path routing", "start_pos": 160, "end_pos": 172, "type": "TASK", "confidence": 0.697666272521019}]}, {"text": "Unsupervised NMT Datasets We conduct the experiments on 20 languages and a total of 20\u00d719=380 language pairs, which have no bilingual sentence pairs but just monolingual sentences for each language.", "labels": [], "entities": [{"text": "NMT Datasets", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.7770740389823914}]}, {"text": "The languages involved in the experiments can be divided into 4 language branches by the taxonomy of language family: Balto-Slavic branch, Germanic branch, Italic branch and Uralic branch 5 . The language name and its ISO 639-1 code contained in each branch can be found in the supplementary materials (Section 1 and 2).", "labels": [], "entities": []}, {"text": "We collect the monolingual corpus from Wikipedia for each language.", "labels": [], "entities": []}, {"text": "We download the language specific Wikipedia contents in XML format , and use WikiExtractor 7 to extract and clean the texts.", "labels": [], "entities": []}, {"text": "We then use the sentence tokenizer from the NLTK toolkit 8 to generate segmented sentences from Wikipedia documents.", "labels": [], "entities": []}, {"text": "To ensure we have the development and test set for the large amount of language pairs to evaluate the unsupervised translation accuracy in our experiments, we choose the languages that are covered by the common corpus of TED talks, which contains translations between more than 50 languages ( . In this circumstance, we can leverage the development and test set from TED talks for evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9298652410507202}]}, {"text": "Note that in the unsupervised setting, we just leverage monolingual sentences for unsupervised training and only use the bilingual data for developing and testing.", "labels": [], "entities": []}, {"text": "In order to alleviate the domain mismatch problem that we train on monolingual data from Wikipedia but test on the evaluation data from TED talks, we also fine-tune the unsupervised models with the small size of monolingual data from TED talks . The monolingual data from TED talks is merged with the monolingual data from Wikipedia in the The first three branches belong to Indo-European family while the last branch is actually a language family.", "labels": [], "entities": []}, {"text": "We do not further split the 3 languages in Uralic family into different branches.", "labels": [], "entities": []}, {"text": "For example, we download English Wikipedia contents from https://dumps.wikimedia.org/enwiki.", "labels": [], "entities": []}, {"text": "https://github.com/attardi/wikiextractor 8 https://www.nltk.org/ 9 https://github.com/neulab/word-embeddings-for-nmt 10 https://github.com/ajinkyakulkarni14/TEDMultilingual-Parallel-Corpus/tree/master/Monolingual data fine-tuning process, which results in better performance for the unsupervised translation.", "labels": [], "entities": []}, {"text": "The size of Wikipidia and TED talks monolingual data can be found in the supplementary materials (Section 3).", "labels": [], "entities": [{"text": "TED talks monolingual data", "start_pos": 26, "end_pos": 52, "type": "DATASET", "confidence": 0.7901173382997513}]}, {"text": "All the sentences in the bilingual and monolingual data are first tokenized with moses tokenizer and then segmented into subword symbols using Byte Pair Encoding (BPE) (.", "labels": [], "entities": []}, {"text": "When training the unsupervised model, we learn the BPE tokens with 60000 merge operations across the source and target languages for each language pair and jointly training the embedding using fastext , following the practice in.", "labels": [], "entities": []}, {"text": "Model Configurations We use transformer model as the basic NMT model structure, considering it achieves state-of-the-art accuracy and becomes a popular choice for recent NMT research.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9988675117492676}]}, {"text": "We use 4-layer encoder and 4-layer decoder with model hidden size d model and feed-forward hidden size d ff being 512, 2048 following the default configurations in.", "labels": [], "entities": []}, {"text": "We use the same model configurations for all the language pairs.", "labels": [], "entities": []}, {"text": "Model Training and Inference We train the unsupervised model with 1 NVIDIA Tesla V100 GPU.", "labels": [], "entities": []}, {"text": "One mini-batch contains roughly 4096 source tokens and 4096 target tokens, as used in.", "labels": [], "entities": []}, {"text": "We follow the default parameters of Adam optimizer and learning rate schedule in.", "labels": [], "entities": []}, {"text": "During inference, we decode with greedy search for all the languages.", "labels": [], "entities": []}, {"text": "We evaluate the translation quality by tokenized case sensitive BLEU) with multi-bleu.pl 13 .  Configurations for Routing We choose the distant language pairs from the 20 languages based on the taxonomy of language family: if two languages are not in the same language branch, then they are regarded as distant languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9805697202682495}]}, {"text": "We get 294 distant language pairs.", "labels": [], "entities": []}, {"text": "As described in Section 3.1, we choose nearly 5% and 10% of the distant language pairs as the development and test set  for routing.", "labels": [], "entities": [{"text": "routing", "start_pos": 124, "end_pos": 131, "type": "TASK", "confidence": 0.9785998463630676}]}, {"text": "Note that if the language pair X \u2192 Y is in development (test) set, then the language pair Y \u2192 X will be also in development (test) set.", "labels": [], "entities": []}, {"text": "We then enumerate all possible paths between any two language pairs in development and test set, and test the BLEU scores of the each possible path, which are regarded as the ground-truth data to evaluate the performance of the routing method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9992393255233765}]}, {"text": "For the remaining 85% distant language pairs, we just test the BLEU score for 10% of all possible paths, and use these BLEU scores as the label for LTR model training.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9712364971637726}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9970155954360962}]}, {"text": "We use 2-layer LSTM as described in Section 3.1.", "labels": [], "entities": []}, {"text": "The dimension of input feature vector is 6, which includes the embedding of the token ID with size of 5, the BLEU score with size 1 (we normalize the BLEU score into 0-1).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.977904736995697}, {"text": "BLEU score", "start_pos": 150, "end_pos": 160, "type": "METRIC", "confidence": 0.9756922125816345}]}, {"text": "We change the depth and width of LSTM, but there is no significant gain in performance.", "labels": [], "entities": [{"text": "depth", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9670079350471497}, {"text": "width", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.942613422870636}]}, {"text": "We use the mean square error as the training loss for the LTR model, and use Adam as the optimizer.", "labels": [], "entities": [{"text": "mean square error", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.8050686120986938}]}, {"text": "The initial learning rate is 0.01.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7111772994200388}]}, {"text": "When applying the LTR model on unseen pairs, we predict the BLEU scores of all the possible paths (including 1-hop (direct translation), 2-hop and 3-hop translation path) between the source and target languages, and choose the path with the highest predicted BLEU score as the routing result.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9988056421279907}, {"text": "BLEU", "start_pos": 259, "end_pos": 263, "type": "METRIC", "confidence": 0.9968153834342957}]}, {"text": "Note that when predicting the path with LTR in inference time, we do not include the pivot language which occurs less than 10 times in training set, which can improve that stability of the LTR prediction.", "labels": [], "entities": []}, {"text": "Methods for Comparison We conduct experimental comparisons on different methods described in Section 3 for path selection (routing), including Random Routing (RR), Prior Pivoting (PP), Hop Average (HA) and Learning to Route (LTR).", "labels": [], "entities": [{"text": "path selection (routing)", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.8290083408355713}, {"text": "Random Routing (RR)", "start_pos": 143, "end_pos": 162, "type": "METRIC", "confidence": 0.9009294390678406}, {"text": "Hop Average (HA)", "start_pos": 185, "end_pos": 201, "type": "METRIC", "confidence": 0.9239180326461792}]}, {"text": "We also compare these routing methods with the direct unsupervised translation, denoted as Direct Translation (DT).", "labels": [], "entities": [{"text": "Direct Translation (DT)", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7572516679763794}]}, {"text": "We list the BLEU score of the best multi-hop path (the ground truth) as a reference, which is denoted as Ground Truth (GT).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9989038705825806}]}], "tableCaptions": [{"text": " Table 1: The BLEU scores of a part of the distant language pairs in the test set (Please refer to Section 1 and 4 in  the supplementary materials for the corresponding full language name and full results). DT: direct unsupervised  translation. GT: the ground-truth best path. LTR: the routing results of LTR. (\u2206) is the BLEU gap between GT or  LTR and DT. Pivot-1 and Pivot-2 are two pivot languages in the path, which will be the same language if the path  is 2-hop and will both be empty if the path is 1-hop (direct translation).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9986020922660828}, {"text": "BLEU", "start_pos": 321, "end_pos": 325, "type": "METRIC", "confidence": 0.9956457018852234}]}, {"text": " Table 2: The length distribution of the best translation  paths. The ratio is calculated based on all language  pairs in the test set.", "labels": [], "entities": []}, {"text": " Table 3: The performance of different routing meth- ods. The BLEU score is averaged on all the distant  language pairs in the test set. The compared methods  include: DT: direct unsupervised translation, RR: ran- dom routing, PP: prior pivoting, HA: hop average, LTR:  our proposed learning to route, and GT: the best trans- lation path (the ground truth).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9993218183517456}, {"text": "HA: hop average", "start_pos": 247, "end_pos": 262, "type": "METRIC", "confidence": 0.7274001687765121}]}, {"text": " Table 5: The performance of different routing methods  when enhanced with supervised pivoting. The BLEU  score is averaged on all the distant language pairs in  the test set. The compared methods include: DT: di- rect unsupervised translation, RR: random routing, HA:  hop average, PP: prior pivoting, LTR: our proposed  learning to route, and GT: the best translation path (the  ground truth).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.999370276927948}]}]}