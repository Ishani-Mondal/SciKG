{"title": [{"text": "Crowdsourcing and Aggregating Nested Markable Annotations", "labels": [], "entities": []}], "abstractContent": [{"text": "One of the key steps in language resource creation is the identification of the text segments to be annotated, or markables-in our case, the (potentially nested) noun phrases in coreference resolution (or mentions).", "labels": [], "entities": [{"text": "language resource creation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6629816691080729}, {"text": "coreference resolution (or mentions)", "start_pos": 178, "end_pos": 214, "type": "TASK", "confidence": 0.8607033292452494}]}, {"text": "In this paper, we present a method for identifying markables for coreference annotation that combines high-performance automatic mark-able detectors with checking with a Game-With-A-Purpose (GWAP) and aggregation using a Bayesian annotation model.", "labels": [], "entities": [{"text": "coreference annotation", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9736749827861786}]}, {"text": "The method was evaluated both on news data and data from a variety of other genres and results in an improvement on F 1 of mention boundaries of over seven percentage points when compared with a state-of-the-art, domain-independent automatic mention detector, and almost three points over an in-domain mention detector.", "labels": [], "entities": [{"text": "F 1 of mention boundaries", "start_pos": 116, "end_pos": 141, "type": "METRIC", "confidence": 0.9499528884887696}]}, {"text": "One of the key contributions of our proposal is its applicability to the casein which mark-ables are nested, as is the case with corefer-ence markables; but the GWAP and several of the proposed markable detectors are task-and language-independent and are thus applicable to a variety of other annotation scenarios.", "labels": [], "entities": [{"text": "GWAP", "start_pos": 161, "end_pos": 165, "type": "DATASET", "confidence": 0.9281230568885803}]}], "introductionContent": [{"text": "Developing Natural Language Processing (NLP) systems still requires large amounts of annotated text to train models, or as a gold standard to test the effectiveness of such models.", "labels": [], "entities": []}, {"text": "The approach followed to create the most widely used data) is to separate the task of identifying the text segments to be annotated-the markables-from the annotation task proper.", "labels": [], "entities": []}, {"text": "In our specific case, the markables of interest are the mentions used in coreference resolution, to be labelled as belonging to a coreference chain or as singletons; typical examples of mentions are pronouns, named entities, and other nominal phrases ( ).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.9419798254966736}]}, {"text": "The annotation of mentions for coreference has similarities with the identification of the chunks for named entity resolution (NER), but mentions can and often are nested, as in the following example, from the Phrase Detectives corpus), where a mention of entity i is nested inside a mention of entity j.", "labels": [], "entities": [{"text": "named entity resolution (NER)", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.8285613854726156}, {"text": "Phrase Detectives", "start_pos": 210, "end_pos": 227, "type": "TASK", "confidence": 0.6779873818159103}]}, {"text": "The methods proposed in this paper are also applicable when markables are nested.", "labels": [], "entities": []}, {"text": "Mention identification for annotation is typically done semi-automatically, using first an automatic mention detector (or extractor) and then checking its output by hand.", "labels": [], "entities": [{"text": "Mention identification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8125127851963043}]}, {"text": "Automatic mention detectors developed for coreference systems are generally used in the first step.", "labels": [], "entities": [{"text": "Automatic mention detectors", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6591925919055939}, {"text": "coreference", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9682729840278625}]}, {"text": "Mention detection was recognized early on as a key step for overall coreference quality, so a number of good quality mention detectors were developed, such as the mention detector included in the Stanford CORE pipeline (, used by many of the top-performing systems in the 2012 CONLL Shared Task (.", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7750112414360046}]}, {"text": "But this performance can be improved.", "labels": [], "entities": []}, {"text": "The first contribution of this paper are new fully-trainable, language-independent mention detectors that outperform the Stanford CORE mention detector in a variety of genres.", "labels": [], "entities": [{"text": "Stanford CORE mention detector", "start_pos": 121, "end_pos": 151, "type": "DATASET", "confidence": 0.6927172392606735}]}, {"text": "But even the best automatic mention detectors do not achieve the accuracy required for high-quality corpus annotation, even when run in-domain: the difference in performance between running coreference resolvers on gold mentions and running them on system mentions can be of up to 20 percentage points, and the results are even poorer when running such systems out-of-domain, for domains like biomedicine or for under-resourced languages (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9978148937225342}, {"text": "coreference resolvers on gold mentions", "start_pos": 190, "end_pos": 228, "type": "TASK", "confidence": 0.8221304535865783}]}, {"text": "So a manual checking step is still required to obtain high-quality results.", "labels": [], "entities": []}, {"text": "Markable checking is increasingly done using crowdsourcing (.", "labels": [], "entities": [{"text": "Markable checking", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9151952564716339}]}, {"text": "But crowdsourcing, using microtask platforms such as Amazon Mechanical Turk can be too expensive for large scale annotation.", "labels": [], "entities": []}, {"text": "For these cases, gamification tends to be a cheaper alternative (, also providing more accurate results and better contributor engagement (.", "labels": [], "entities": []}, {"text": "The second contribution of this paper is an approach to mention detection for large-scale coreference annotation projects in which the output of mention detectors is corrected using a Gamewith-a-Purpose (GWAP).", "labels": [], "entities": [{"text": "mention detection", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7921109199523926}, {"text": "coreference annotation", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.857492595911026}]}, {"text": "A Game-With-A-Purpose is a game in which players label data as a by-effect of playing.", "labels": [], "entities": []}, {"text": "GWAPs have been successful in many annotation projects ().", "labels": [], "entities": [{"text": "GWAPs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7787131667137146}]}, {"text": "Examples of successful GWAPs include The ESP Game, in which players contribute image labels), and FoldIt, in which players solve protein-structure prediction problems.", "labels": [], "entities": [{"text": "FoldIt", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.7550653219223022}, {"text": "protein-structure prediction", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.7543488144874573}]}, {"text": "However, so far there have not been any truly successful GWAPs for NLP.", "labels": [], "entities": []}, {"text": "It has proven difficult to go from simple gamification of a labelling task to developing a proper game: e.g., in one of the best-known GWAPs for NLP, Phrase Detectives (, the labelling remains the core of the game dynamics.", "labels": [], "entities": [{"text": "Phrase Detectives", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.6353200525045395}]}, {"text": "Yet, games such as Puzzle Racer have shown that engaging GWAPs producing annotations for text are possible.", "labels": [], "entities": [{"text": "Puzzle Racer", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.7716115117073059}]}, {"text": "Furthermore, that the annotations thus collected are of a quality comparable to that obtainable using microtask crowdsourcing, and at 2 One difference between the mention detectors used for coreference resolvers and those used to preprocess data for coreference annotation is relevant for subsequent discussion.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.9180288910865784}, {"text": "coreference annotation", "start_pos": 250, "end_pos": 272, "type": "TASK", "confidence": 0.9178079962730408}]}, {"text": "The former usually aim for high recall and compromise on precision, placing more confidence/importance on the coreference resolution step) and being satisfied that incorrectly identified mentions will simply remain singletons which can be removed in post processing ().", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.999230146408081}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9978873133659363}, {"text": "coreference resolution", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.9449930191040039}]}, {"text": "The latter tend to go for high F.", "labels": [], "entities": [{"text": "F", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.9990455508232117}]}, {"text": "This difference played a role in our experiments, as discussed later.", "labels": [], "entities": []}, {"text": "However, such games have yet to achieve the player uptake or number of judgements comparable to GWAPs in other domains.", "labels": [], "entities": [{"text": "uptake", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.894631028175354}]}, {"text": "Furthermore, it is not clear yet whether using GWAPs can result in better performance for tasks such as mention detection, for which good-performance systems exist.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.9261310994625092}]}, {"text": "In this work, automatically extracted mentions are checked using a two-player GWAP, TileAttack.", "labels": [], "entities": []}, {"text": "Our previous analysis of the performance of TileAttack using player satisfaction metrics derived from the Free 2 Play literature suggests that we are succeeding in developing an engaging game.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that using TileAttack to check the output of our mention detector results in substantial improvement to the quality of the output.", "labels": [], "entities": []}, {"text": "The game supports any text segmentation task, whether markables are nested or non-nested, aligned or not aligned, and is therefore applicable at least in principle to a variety of text annotation tasks besides coreference, including e.g., Named Entity Resolution (NER).", "labels": [], "entities": [{"text": "text segmentation task", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7783099114894867}, {"text": "Named Entity Resolution (NER)", "start_pos": 239, "end_pos": 268, "type": "TASK", "confidence": 0.8100294768810272}]}, {"text": "Key to this result is the use of a novel aggregation method to combine the labels produced by the mention detector with the labels collected using the game.", "labels": [], "entities": []}, {"text": "A number of aggregation methods applicable to text segmentation labelling have been proposed), but they are not directly applicable when markables can be nested.", "labels": [], "entities": [{"text": "text segmentation labelling", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.852892279624939}]}, {"text": "The third contribution of this paper is a novel method to use aggregation with potentially nested markables.", "labels": [], "entities": []}, {"text": "We show that using this method to aggregate mention detector labels and TileAttack labels results in improved markable boundary quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our approach, we tested the mention boundaries obtained using the two proposed pipelines and by aggregating the judgements collected using TileAttack in several different ways over datasets in different genres.", "labels": [], "entities": []}, {"text": "As said above, our approach to human checking of mentions produced by other players or by a system is to treat existing annotations as artificial agents that human players 'play against'.", "labels": [], "entities": [{"text": "human checking of mentions produced by", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.8164005080858866}]}, {"text": "But we also pointed out that the mention detectors used for coreference resolution systems are optimised to achieve extremely high recall-the assumption being that the extra mentions will be filtered during coreference resolution proper-and that this optimisation may not be optimal when using an automatic mention detector for annotation-in our case, treating it as an agent from which the other players will derive feedback.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.9573774635791779}, {"text": "recall-the", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9982050657272339}, {"text": "coreference resolution", "start_pos": 207, "end_pos": 229, "type": "TASK", "confidence": 0.8864583671092987}]}, {"text": "In this context, a mention detector optimised for high overall F maybe preferable, as it may provide better feedback to the human players.", "labels": [], "entities": [{"text": "F", "start_pos": 63, "end_pos": 64, "type": "METRIC", "confidence": 0.9978415966033936}]}, {"text": "We tried therefore two versions of the NN pipeline in this experiment: one optimized for high recall, and one for high F 1 . The two configurations are shown in.", "labels": [], "entities": [{"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9987722039222717}, {"text": "F 1", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9642401039600372}]}, {"text": "Most coreference datasets consist primarily of news text; for this reason, our first dataset, referred to below as \"News\", consists of 102 sentences from five randomly selected documents from the Wall Street Journal section of the Penn Treebank (, annotated with coreference as part of the ARRAU corpus (.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 196, "end_pos": 244, "type": "DATASET", "confidence": 0.9389015883207321}, {"text": "ARRAU corpus", "start_pos": 290, "end_pos": 302, "type": "DATASET", "confidence": 0.9423195719718933}]}, {"text": "The second dataset, referred to below as \"Other Domains\", is 180 sentences from a collection of our own creation consisting of documents covering different genres, from simple language learning texts and student reports, to Wikipedia pages and fiction from Project Gutenberg.", "labels": [], "entities": []}, {"text": "We hand labelled the mentions in those sentences ourselves.", "labels": [], "entities": []}, {"text": "102 sentences were annotated by 131 participants.", "labels": [], "entities": []}, {"text": "Each sentence was annotated at least 8 times (max-imum of 11).", "labels": [], "entities": []}, {"text": "A boundary was considered correct iff the start and end match exactly.", "labels": [], "entities": []}, {"text": "The results in compare the results obtained using the four pipelines or application the two different aggregation approaches on the user (u), our DEP pipeline (d), NN (High F 1 and Recall configurations) and Stanford Pipeline (s).", "labels": [], "entities": [{"text": "F 1 and Recall", "start_pos": 173, "end_pos": 187, "type": "METRIC", "confidence": 0.8310966640710831}, {"text": "Stanford Pipeline", "start_pos": 208, "end_pos": 225, "type": "DATASET", "confidence": 0.9794451892375946}]}, {"text": "The presence or absence of the annotations for the users or pipelines is indicated by a preceding + or \u2212 respectively.", "labels": [], "entities": []}, {"text": "M V indicates application of the majority voting aggregation method, and P the probabilistic aggregation method.", "labels": [], "entities": []}, {"text": "The, first of all, that the domain-trained pipelines outperform the domainindependent Stanford one, as expected.", "labels": [], "entities": []}, {"text": "Second, that in this genre human judgements only match the domain-dependent pipelines when probabilistic aggregation is used.", "labels": [], "entities": []}, {"text": "Third, that by aggregating user judgements and domain-dependent pipelines we see an improvement in F1 of up to 2.5 percentage points, but only with probabilistic aggregation.", "labels": [], "entities": [{"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9997015595436096}]}], "tableCaptions": [{"text": " Table 1: Mention detectors comparison.", "labels": [], "entities": [{"text": "Mention detectors comparison", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7618623475233713}]}, {"text": " Table 2: Regular players accuracy on 'Other domains'", "labels": [], "entities": []}, {"text": " Table 3: Comparing pipeline and aggregation methods", "labels": [], "entities": []}, {"text": " Table 4: Results on the 'Other Domains' dataset (rounded to 3 dp)", "labels": [], "entities": [{"text": "Other Domains' dataset", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.7241971691449484}]}]}