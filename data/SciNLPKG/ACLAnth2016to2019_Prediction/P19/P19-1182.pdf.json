{"title": [], "abstractContent": [{"text": "Describing images with text is a fundamental problem in vision-language research.", "labels": [], "entities": [{"text": "Describing images with text", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8822431415319443}]}, {"text": "Current studies in this domain mostly focus on single image captioning.", "labels": [], "entities": [{"text": "single image captioning", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.6683356165885925}]}, {"text": "However, in various real applications (e.g., image editing, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful.", "labels": [], "entities": [{"text": "image editing", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.8918707370758057}, {"text": "difference interpretation", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.8401504456996918}]}, {"text": "This important problem has not been explored mostly due to lack of datasets and effective models.", "labels": [], "entities": []}, {"text": "To push forward the research in this direction, we first introduce anew language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions.", "labels": [], "entities": []}, {"text": "We then propose anew relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention.", "labels": [], "entities": []}, {"text": "We also extend the model with dynamic relational attention , which calculates visual alignment while decoding.", "labels": [], "entities": [{"text": "calculates visual alignment", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6159391502539316}]}, {"text": "Our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences.", "labels": [], "entities": []}, {"text": "Experimental results, based on both automatic and human evaluation, demonstrate that our model outperforms all baselines and existing methods on all the datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generating captions to describe natural images is a fundamental research problem at the intersection of computer vision and natural language processing.", "labels": [], "entities": [{"text": "Generating captions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.888128936290741}]}, {"text": "Single image captioning) has many practical applications such as text-based image search, photo curation, assisting of visuallyimpaired people, image understanding in social media, etc.", "labels": [], "entities": [{"text": "Single image captioning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6640886068344116}, {"text": "text-based image search", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.6606861849625906}, {"text": "photo curation", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7630958259105682}, {"text": "image understanding", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7783829271793365}]}, {"text": "This task has drawn significant attention in the research community with numerous studies (, and recent state of the art methods have achieved promising results on large captioning datasets, such as MS COCO ().", "labels": [], "entities": [{"text": "MS COCO", "start_pos": 199, "end_pos": 206, "type": "DATASET", "confidence": 0.8479158580303192}]}, {"text": "Besides single image captioning, the community has also explored other visual captioning problems such as video captioning, and referring expressions (.", "labels": [], "entities": [{"text": "single image captioning", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.6343331138292948}, {"text": "video captioning", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.6983938813209534}]}, {"text": "However, the problem of two-image captioning, especially the task of describing the relationships and differences between two images, is still underexplored.", "labels": [], "entities": [{"text": "two-image captioning", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.716599315404892}]}, {"text": "In this paper, we focus on advancing research in this challenging problem by introducing anew dataset and proposing novel neural relational-speaker models.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, is the only public dataset aimed at generating natural language descriptions for two real images.", "labels": [], "entities": []}, {"text": "This dataset is about 'spotting the difference', and hence focuses more on describing exhaustive differences by learning align-ments between multiple text descriptions and multiple image regions; hence the differences are intended to be explicitly identifiable by subtracting two images.", "labels": [], "entities": []}, {"text": "There are many other tasks that require more diverse, detailed and implicit relationships between two images.", "labels": [], "entities": []}, {"text": "Interpreting image editing effects with instructions is a suitable task for this purpose, because it has requirements of exploiting visual transformations and it is widely used in real life, such as explanation of complex image editing effects for laypersons or visuallyimpaired users, image editor tutorial retrieval, and language-guided image editing systems.", "labels": [], "entities": [{"text": "Interpreting image editing effects", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8592773973941803}, {"text": "explanation of complex image editing effects", "start_pos": 199, "end_pos": 243, "type": "TASK", "confidence": 0.8074996173381805}, {"text": "image editor tutorial retrieval", "start_pos": 286, "end_pos": 317, "type": "TASK", "confidence": 0.7367991954088211}, {"text": "image editing", "start_pos": 339, "end_pos": 352, "type": "TASK", "confidence": 0.690097913146019}]}, {"text": "We first build anew language-guided image editing dataset with high quality annotations by (1) crawling image pairs from real image editing request websites, (2) annotating editing instructions via Amazon Mechanical Turk, and (3) refining the annotations through experts.", "labels": [], "entities": []}, {"text": "Next, we propose anew neural speaker model for generating sentences that describe the visual relationship between a pair of images.", "labels": [], "entities": []}, {"text": "Our model is general and not dependent on any specific dataset.", "labels": [], "entities": []}, {"text": "Starting from an attentive encoderdecoder baseline, we first develop a model enhanced with two attention-based neural components, a static relational attention and a sequential multi-head attention, to address these two challenges, respectively.", "labels": [], "entities": []}, {"text": "We further extend it by designing a dynamic relational attention module to combine the advantages of these two components, which finds the relationship between two images while decoding.", "labels": [], "entities": []}, {"text": "The computation of dynamic relational attention is mathematically equivalent to attention overall visual \"relationships\".", "labels": [], "entities": []}, {"text": "Thus, our method provides a direct way to model visual relationships in language.", "labels": [], "entities": []}, {"text": "To show the effectiveness of our models, we evaluate them on three datasets: our new dataset, the \"Spot-the-Diff\" dataset (, and the two-image visual reasoning NLVR2 dataset () (adapted for our task).", "labels": [], "entities": [{"text": "NLVR2 dataset", "start_pos": 160, "end_pos": 173, "type": "DATASET", "confidence": 0.7012958228588104}]}, {"text": "We train models separately on each dataset with the same hyper-parameters and evaluate them on the same test set across all methods.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our model outperforms all the baselines and existing methods.", "labels": [], "entities": []}, {"text": "The main contributions of our paper are: (1) We create a novel human language guided image editing dataset to boost the study in describing visual relationships; (2) We design novel relationalspeaker models, including a dynamic relational attention module, to handle the problem of twoimage captioning by focusing on all their visual relationships; (3) Our method is evaluated on several datasets and achieves the state-of-the-art.", "labels": [], "entities": [{"text": "twoimage captioning", "start_pos": 282, "end_pos": 301, "type": "TASK", "confidence": 0.7653761804103851}]}], "datasetContent": [{"text": "We present the collection process and statistics of our Image Editing Request dataset and briefly introduce two public datasets (viz., Spot-the-Diff and NLVR2).", "labels": [], "entities": [{"text": "Image Editing Request dataset", "start_pos": 56, "end_pos": 85, "type": "DATASET", "confidence": 0.7346763387322426}]}, {"text": "All three datasets are used to study the task of two-image captioning and evaluating our relational-speaker models.", "labels": [], "entities": [{"text": "two-image captioning", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.7429605424404144}]}, {"text": "Examples from these three datasets are shown in.", "labels": [], "entities": []}, {"text": "Each instance in our dataset consists of an image pair (i.e., a source image and a target image) and a corresponding editing instruction which correctly and comprehensively describes the transformation from the source image to the target image.", "labels": [], "entities": []}, {"text": "Our collected Image Editing Request dataset will be publicly released along with the scripts to unify it with the other two datasets.", "labels": [], "entities": [{"text": "Image Editing Request dataset", "start_pos": 14, "end_pos": 43, "type": "DATASET", "confidence": 0.7398984581232071}]}, {"text": "The Image Editing Request dataset that we have collected and annotated currently contains 3,939 image pairs (3061 in training, 383 in validation, 495 in test) with 5,695 human-annotated instructions in total.", "labels": [], "entities": [{"text": "Image Editing Request dataset", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8263802230358124}]}, {"text": "Each image pair in the training set has one instruction, and each image pair in the validation and test sets has three instructions, written by three different annotators.", "labels": [], "entities": []}, {"text": "Instructions have an average length of 7.5 words (standard deviation: 4.8).", "labels": [], "entities": []}, {"text": "After removing the words with less than three occurrences, the dataset has a vocabulary of 786 words.", "labels": [], "entities": []}, {"text": "The human agreement of our dataset is shown in.", "labels": [], "entities": []}, {"text": "The word frequencies in our dataset are visualized in.", "labels": [], "entities": []}, {"text": "Most of the images in our dataset are realistic.", "labels": [], "entities": []}, {"text": "Since the task is image editing, target images may have some artifacts (see Image Editing Request examples in and).", "labels": [], "entities": [{"text": "image editing", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.8873924016952515}]}, {"text": "To show the generalization of our speaker model, we also train and evaluate our model on two public datasets, Spot-the-Diff () and NLVR2 (.", "labels": [], "entities": [{"text": "NLVR2", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9047578573226929}]}, {"text": "Instances in these two datasets are each composed of two natural images and a human written sentence describing the relationship between the two images.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, these are the only two public datasets with a reasonable amount of data that are suitable for our task.", "labels": [], "entities": []}, {"text": "We next briefly introduce these two datasets.", "labels": [], "entities": []}, {"text": "Spot-the-Diff This dataset is designed to help generate a set of instructions that can comprehensively describe all visual differences.", "labels": [], "entities": []}, {"text": "Thus, the dataset contains images from video-surveillance footage, in which differences can be easily found.", "labels": [], "entities": []}, {"text": "This is because all the differences could be effectively captured by subtractions between two images, as shown in.", "labels": [], "entities": []}, {"text": "The dataset contains 13,192 image pairs, and an average of 1.86 captions are collected for each image pair.", "labels": [], "entities": []}, {"text": "The dataset is split into training, validation, and test sets with a ratio of 8:1:1.", "labels": [], "entities": []}, {"text": "We use the same hyperparameters when applying our model to the three datasets.", "labels": [], "entities": []}, {"text": "Dimensions of hidden vectors are 512.", "labels": [], "entities": []}, {"text": "The model is optimized by Adam with a learning rate of 1e \u2212 4.", "labels": [], "entities": []}, {"text": "We add dropout layers of rate 0.5 everywhere to avoid over-fitting.", "labels": [], "entities": []}, {"text": "When generating instructions for evaluation, we use maximum-decoding: the word wt generated at time step t is arg max k p(w t,k ).", "labels": [], "entities": []}, {"text": "For the Spot-the-Diff dataset, we take the \"Single sentence decoding\" experiment as in.", "labels": [], "entities": [{"text": "Spot-the-Diff dataset", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.6879898756742477}]}, {"text": "We also try to mix the three datasets but we do not see any improvement.", "labels": [], "entities": []}, {"text": "We also try different ways to mix the three datasets but we do not see improvement.", "labels": [], "entities": []}, {"text": "We first train a unified model on the union of these datasets.", "labels": [], "entities": []}, {"text": "The metrics drop a lot because the tasks and language domains (e.g., the word dictionary and lengths of sentences) are different from each other.", "labels": [], "entities": []}, {"text": "We next only share the visual components to overcome the disagreement in language.", "labels": [], "entities": []}, {"text": "However, the image domain are still quite different from each other (as shown in).", "labels": [], "entities": []}, {"text": "Thus, we finally separately train three models on the three datasets with minimal cross-dataset modifications.", "labels": [], "entities": []}, {"text": "As shown in, we compare the performance of our models on all three datasets with various automated metrics.", "labels": [], "entities": []}, {"text": "Results on the test sets are reported.", "labels": [], "entities": []}, {"text": "Following the setup in, we takes CIDEr ( as the main metric in evaluating the Spot-the-Diff and NLVR2 datasets.", "labels": [], "entities": [{"text": "CIDEr", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.5931143164634705}, {"text": "NLVR2 datasets", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9504774510860443}]}, {"text": "However, CIDEr is known as its problem in up-weighting unimportant details ().", "labels": [], "entities": []}, {"text": "In our dataset, we find that instructions generated from a small set of short phrases could get a high CIDEr score.", "labels": [], "entities": [{"text": "CIDEr score", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9579344391822815}]}, {"text": "We thus change the main metric of our dataset to METEOR, which is manually verified to be aligned with human judgment on the validation set in our dataset.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9655680656433105}]}, {"text": "To avoid over-fitting, the model is  early-stopped based on the main metric on validation set.", "labels": [], "entities": []}, {"text": "We also report the BLEU-4 () and ROUGE-L (Lin, 2004) scores.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9993022680282593}, {"text": "ROUGE-L", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9981945157051086}]}, {"text": "The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images.", "labels": [], "entities": []}, {"text": "Our full model has a significant improvement in result over baseline.", "labels": [], "entities": []}, {"text": "The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions.", "labels": [], "entities": [{"text": "NLVR2 dataset", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9808351695537567}]}, {"text": "We conduct a pairwise human evaluation on our generated sentences, which is used in and.", "labels": [], "entities": []}, {"text": "Agarwala (2018) also shows that the pairwise comparison is better than scoring sentences individually.", "labels": [], "entities": []}, {"text": "We randomly select 100 examples from the test set in each dataset and generate captions via our full speaker model.", "labels": [], "entities": []}, {"text": "We ask users to choose a better instruction between the captions generated by our full model and the basic model, or alternatively indicate that the two captions are equal in quality.", "labels": [], "entities": []}, {"text": "The Image Editing Request dataset is specifically annotated by the image editing expert.", "labels": [], "entities": [{"text": "Image Editing Request dataset", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.784700021147728}]}, {"text": "The winning rate of our full model (dynamic relation attention) versus the basic model is shown in.", "labels": [], "entities": [{"text": "winning", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9573127627372742}]}, {"text": "Our full model outperforms the basic model significantly.", "labels": [], "entities": []}, {"text": "We also show positive and negative examples generated by our full model in.", "labels": [], "entities": []}, {"text": "In our Image Editing Request corpus, the model was able to detect and describe the editing actions but it failed in handling the arbitrary complex editing actions.", "labels": [], "entities": [{"text": "Image Editing Request corpus", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.6092509403824806}]}, {"text": "We keep these hard examples in our dataset to match real-world requirements and allow follow-up future works to pursue the remaining challenges in this task.", "labels": [], "entities": []}, {"text": "Our model is designed for non-localized relationships thus we do not explicitly model the pixel-level differences; however, we still find that the model could learn these differences in the Spot-the-Diff dataset.", "labels": [], "entities": [{"text": "Spot-the-Diff dataset", "start_pos": 190, "end_pos": 211, "type": "DATASET", "confidence": 0.7622845470905304}]}, {"text": "Since the descriptions in Spot-the-Diff is relatively simple, the errors mostly come from wrong entities or undetected differences as shown in.", "labels": [], "entities": []}, {"text": "Our model is also sensitive to the image contents as shown in the NLVR2 dataset.", "labels": [], "entities": [{"text": "NLVR2 dataset", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9852019548416138}]}], "tableCaptions": [{"text": " Table 1: Human agreement on our datasets, compared  with Spot-the-Diff and MS COCO (captions=3). B-1  to B-4 are BLEU-1 to BLEU-4. Our dataset has the  highest human agreement.", "labels": [], "entities": [{"text": "MS COCO", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.8052163422107697}, {"text": "BLEU-1", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9937577247619629}, {"text": "BLEU-4", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.968187153339386}]}, {"text": " Table 2: Automatic metric of test results on three datasets. Best results of the main metric are marked in bold font.  Our full model is the best on all three datasets with the main metric.", "labels": [], "entities": []}, {"text": " Table 3: Human evaluation on 100 examples. Image  pair and two captions generated by our basic model and  full model are shown to the user. The user chooses  one from 'Basic' model wins, 'Full' model wins, 'Both  Good', or 'Both Not'. Better model marked in bold  font.", "labels": [], "entities": []}]}