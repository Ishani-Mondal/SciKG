{"title": [{"text": "Unsupervised Parallel Sentence Extraction with Parallel Segment Detection Helps Machine Translation", "labels": [], "entities": [{"text": "Parallel Sentence Extraction", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.6165191133817037}, {"text": "Machine Translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7487416863441467}]}], "abstractContent": [{"text": "Mining parallel sentences from comparable corpora is important.", "labels": [], "entities": []}, {"text": "Most previous work relies on supervised systems, which are trained on parallel data, thus their applicability is problematic in low-resource scenarios.", "labels": [], "entities": []}, {"text": "Recent developments in building unsupervised bilingual word embeddings made it possible to mine parallel sentences based on cosine similarities of source and target language words.", "labels": [], "entities": []}, {"text": "We show that relying only on this information is not enough, since sentences often have similar words but different meanings.", "labels": [], "entities": []}, {"text": "We detect continuous parallel segments in sentence pair candidates and rely on them when mining parallel sentences.", "labels": [], "entities": []}, {"text": "We show better mining accuracy on three language pairs in a standard shared task on artificial data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9961563944816589}]}, {"text": "We also provide the first experiments showing that parallel sentences mined from real life sources improve unsupervised MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 120, "end_pos": 122, "type": "TASK", "confidence": 0.9773642420768738}]}, {"text": "Our code is available, we hope it will be used to support low-resource MT research.", "labels": [], "entities": [{"text": "MT research", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9548816084861755}]}], "introductionContent": [{"text": "The performance of machine translation has improved significantly recently, with some claims of even being close to human parity (, but a large amount of parallel data is required for high quality systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8061481714248657}]}, {"text": "For many language pairs the size of the available training data is not adequate.", "labels": [], "entities": []}, {"text": "Recently, developments in the field of unsupervised bilingual word embeddings (BWEs) made it possible to build MT systems without any parallel data.", "labels": [], "entities": [{"text": "unsupervised bilingual word embeddings (BWEs)", "start_pos": 39, "end_pos": 84, "type": "TASK", "confidence": 0.6808334078107562}, {"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9883614182472229}]}, {"text": "Both statistical) and neural ( MT approaches were proposed which are promising directions to overcome the data sparsity problem.", "labels": [], "entities": []}, {"text": "However, various issues of the approaches still have to be solved, e.g., better word reordering during translation or tuning system parameters.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7027406692504883}]}, {"text": "For many interesting low resource language pairs, we do not have enough parallel data, but we do have access to sources of comparable monolingual text.", "labels": [], "entities": []}, {"text": "In this paper we propose a strong unsupervised system for parallel sentence mining and show that the mined data improves the performance of unsupervised MT systems.", "labels": [], "entities": [{"text": "parallel sentence mining", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.6374850471814474}, {"text": "MT", "start_pos": 153, "end_pos": 155, "type": "TASK", "confidence": 0.9754517674446106}]}, {"text": "Previously many approaches tackled the problem of parallel sentence extraction but they were relying on different levels of bilingual signals either to build dictionaries (, parallel sentence classifiers) or bilingual sentence representations).", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6463767985502878}]}, {"text": "An unsupervised system was also proposed which only relied on unsupervised BWEs, thus no additional resources are needed ( . We use this approach as our baseline and show that relying only on word similarity information leads to false positive sentence pairs, such as in this example: \u2022 The US dollar has a considerable role in the international monetary system.", "labels": [], "entities": []}, {"text": "\u2022 Die Rolle des US Dollar im internationalen Geldsystem sollte ne\u00fc uberdacht werden.", "labels": [], "entities": [{"text": "US Dollar", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.8076552450656891}]}, {"text": "(The role of the US dollar in the international monetary system should be reconsidered.)", "labels": [], "entities": []}, {"text": "Both sentences mention the role of the US dollar in the international monetary system, but the overall claim is different.", "labels": [], "entities": []}, {"text": "One major disadvantage of the approach of (  is that, by only relying on word similarities, sentence pairs which have similar meanings but are not exactly parallel are often mined.", "labels": [], "entities": []}, {"text": "We overcome this problem by detecting continuous parallel segments in the candidate sentence pairs.", "labels": [], "entities": []}, {"text": "We align similar words in the candidate sentence pairs, instead of just averaging their similarity, and use the alignments in order to detect continuous sub-sentential segments on both sides that are aligned with each other.", "labels": [], "entities": []}, {"text": "In order to increase the precision of our system we only mine similar sentence pairs where the detected parallel segments form a large part of the full sentence pairs thus overcoming the problem of only nearly parallel sentence pairs mentioned above.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9993089437484741}]}, {"text": "We conduct two sets of experiments to show that our system mines more useful parallel sentences and that they are beneficial for MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 129, "end_pos": 131, "type": "TASK", "confidence": 0.9943554401397705}]}, {"text": "First, we evaluate the accuracy of the mining approach on the BUCC 2017 shared task data ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995729327201843}, {"text": "BUCC 2017 shared task data", "start_pos": 62, "end_pos": 88, "type": "DATASET", "confidence": 0.9685605764389038}]}, {"text": "We show that by looking for continuous parallel segments we can increase the performance significantly compared to ( , especially the precision of the system, on German-, French-and RussianEnglish language pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9990707039833069}]}, {"text": "1 Second, since the data used in previous work was artificially assembled, we use real life German and English monolingual news crawl data to mine parallel sentences, and use them to improve an unsupervised neural MT system by using the extracted data as silverstandard parallel training data.", "labels": [], "entities": [{"text": "MT", "start_pos": 214, "end_pos": 216, "type": "TASK", "confidence": 0.9038059711456299}]}, {"text": "We show for the first time that exploiting comparable monolingual text sources with an unsupervised parallel sentence mining system helps unsupervised MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.983863353729248}]}, {"text": "Furthermore, we achieve increased performance compared with the previous unsupervised mining system.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our first set of experiments on the BUCC 2017 shared task data (.", "labels": [], "entities": [{"text": "BUCC 2017 shared task data", "start_pos": 47, "end_pos": 73, "type": "DATASET", "confidence": 0.9618917465209961}]}, {"text": "The aim of this shared task is to quantitatively evaluate methods for extracting parallel sentences from comparable monolingual corpora.", "labels": [], "entities": [{"text": "extracting parallel sentences from comparable monolingual corpora", "start_pos": 70, "end_pos": 135, "type": "TASK", "confidence": 0.7815571086747306}]}, {"text": "Train, development and test datasets were built for 4 language pairs German-, French-, Russianand Chinese-English language pairs.", "labels": [], "entities": []}, {"text": "The data was built automatically by inserting parallel news commentary sentences into monolingual wikipedia dumps.", "labels": [], "entities": []}, {"text": "To make sure that the insertions are not easy to detect parallel sentences were only inserted if other strongly related sentences in terms of their topic are present in the monolingual corpus.", "labels": [], "entities": []}, {"text": "We use the system of (Hangya et al., 2018) as our baseline and run experiments on the first three language pairs (as we already mentioned, we would need to study Chinese unsupervised word segmentation to run Zh-En experiments).", "labels": [], "entities": [{"text": "Chinese unsupervised word segmentation", "start_pos": 162, "end_pos": 200, "type": "TASK", "confidence": 0.641270600259304}]}, {"text": "We consider English as the target language in all cases.", "labels": [], "entities": []}, {"text": "To mine parallel sentence pairs we use comparable monolingual data for both German and English.", "labels": [], "entities": []}, {"text": "For this we use the news crawl data between 2007 and 2015 released by the WMT 2016 translation shared task () containing about 140M and 114M German and English sentences respectively after length based filtering (see below).", "labels": [], "entities": [{"text": "WMT 2016 translation shared task", "start_pos": 74, "end_pos": 106, "type": "DATASET", "confidence": 0.758195424079895}]}, {"text": "As a first step, we build unsupervised BWEs on the same data as), i.e., newscrawl between 2007 and 2013, using the same procedure mentioned earlier.", "labels": [], "entities": []}, {"text": "The built BWEs are used to create the dictionary of word similarities for the mining and to initialize the NMT system.", "labels": [], "entities": []}, {"text": "We consider German as the source language during the mining process.", "labels": [], "entities": []}, {"text": "Before running our system on the full data to extract sentences we batch the data to decrease the number of sentence pair candidates.", "labels": [], "entities": []}, {"text": "Assuming that different news portals cover a given event in the same year we only look for parallel sentences within the same year.", "labels": [], "entities": []}, {"text": "We note that further use of batching could be possible if more fine grained date information is available.", "labels": [], "entities": []}, {"text": "Furthermore, we also batch texts based on their length assuming that sentences with very different number of tokens are not parallel.", "labels": [], "entities": []}, {"text": "We use sentences with length between 10 and 50 tokens and make batches with step size 5.", "labels": [], "entities": []}, {"text": "We also apply pre-filtering within the batches.", "labels": [], "entities": []}, {"text": "This method drastically decreased the runtime of the mining procedure which took around 1 week using 40 threads on a 2.27GHz CPU.", "labels": [], "entities": []}, {"text": "Since tuning would have been time consuming, we based our hyperparameters on the experiments in the previous section and on preliminary experiments.", "labels": [], "entities": []}, {"text": "In order to increase the precision of mined sentences we chose an aggressive setup for window size and minimum segment length, requiring long continuous segments in the sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9981024861335754}]}, {"text": "We made the following choices: threshold value for segment detection 0.3; window size of average filter 5; threshold value for deciding parallelism 0.3; minimum segment length 70%.", "labels": [], "entities": [{"text": "segment detection", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8399312198162079}]}, {"text": "At the end we extracted around 220K parallel sentence pairs from the full dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision, recall and F 1 scores for our pro- posed system and the baseline (avg) on the BUCC 2017  dataset.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9992285966873169}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9990130662918091}, {"text": "F 1 scores", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9867696364720663}, {"text": "BUCC 2017  dataset", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.9820066094398499}]}, {"text": " Table 3: NMT experiments using mined parallel sentences. We compare results using mined sentence pairs from  Hangya et al. (2018) and our approach. Texts before 2014 is used in 07-13 while all data is used in 07-15. We also  restrict the minimum sentence length to 16 tokens in case of long. We show a fully unsupervised system using no  parallel sentences, and an oracle using europarl parallel sentences.", "labels": [], "entities": []}, {"text": " Table 4: Number of parallel sentence pairs in the  datasets.", "labels": [], "entities": []}]}