{"title": [], "abstractContent": [{"text": "Scheduled sampling is a technique for avoiding one of the known problems in sequence-to-sequence generation: exposure bias.", "labels": [], "entities": [{"text": "Scheduled sampling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7337462902069092}, {"text": "sequence-to-sequence generation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6763403564691544}]}, {"text": "It consists of feeding the model a mix of the teacher forced embeddings and the model predictions from the previous step in training time.", "labels": [], "entities": []}, {"text": "The technique has been used for improving the model performance with recurrent neural networks (RNN).", "labels": [], "entities": []}, {"text": "In the Transformer model, unlike the RNN, the generation of anew word attends to the full sentence generated so far, not only to the last word, and it is not straightforward to apply the scheduled sampling technique.", "labels": [], "entities": []}, {"text": "We propose some structural changes to allow scheduled sampling to be applied to Transformer architecture, via a two-pass decoding strategy.", "labels": [], "entities": []}, {"text": "Experiments on two language pairs achieve performance close to a teacher-forcing baseline and show that this technique is promising for further exploration.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work in Neural Machine Translation (NMT) relies on a sequence-to-sequence model with global attention, trained with maximum likelihood estimation (MLE).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.8326774338881174}, {"text": "maximum likelihood estimation (MLE", "start_pos": 123, "end_pos": 157, "type": "METRIC", "confidence": 0.7327541828155517}]}, {"text": "These models are typically trained by teacher forcing, in which the model makes each decision conditioned on the gold history of the target sequence.", "labels": [], "entities": []}, {"text": "This tends to lead to quick convergence but is dissimilar to the procedure used at decoding time, when the gold target sequence is not available and decisions are conditioned on previous model predictions.", "labels": [], "entities": []}, {"text": "point out the problem that using teacher forcing means the model has never been trained on its own errors and may not be robust to them-a phenomenon called exposure bias.", "labels": [], "entities": []}, {"text": "This has the potential to cause problems at translation time, when the model is exposed to its own (likely imperfect) predictions.", "labels": [], "entities": [{"text": "translation", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9694691300392151}]}, {"text": "A common approach for addressing the problem with exposure bias is using a scheduled strategy for deciding when to use teacher forcing and when not to ().", "labels": [], "entities": []}, {"text": "For a recurrent decoder, applying scheduled sampling is trivial: for generation of each word, the model decides whether to condition on the gold embedding from the given target (teacher forcing) or the model prediction from the previous step.", "labels": [], "entities": []}, {"text": "In the Transformer model (), the decoding is still autoregressive, but unlike the RNN decoder, the generation of each word conditions on the whole prefix sequence and not only on the last word.", "labels": [], "entities": []}, {"text": "This makes it non-trivial to apply scheduled sampling directly for this model.", "labels": [], "entities": []}, {"text": "Since the Transformer achieves state-of-the-art results and has become a default choice for many natural language processing problems, it is interesting to adapt and explore the idea of scheduled sampling for it, and, to our knowledge, noway of doing this has been proposed so far.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are: \u2022 We propose anew strategy for using scheduled sampling in Transformer models by making two passes through the decoder in training time.", "labels": [], "entities": []}, {"text": "\u2022 We compare several approaches for conditioning on the model predictions when they are used instead of the gold target.", "labels": [], "entities": []}, {"text": "\u2022 We test the scheduled sampling with transformers in a machine translation task on two language pairs and achieve results close to a teacher forcing baseline (with a slight improvement of up to 1 BLEU point for some models).", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.7831081052621206}, {"text": "BLEU", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9986901879310608}]}], "datasetContent": [{"text": "We report experiments with scheduled sampling for Transformers for the task of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.8533315062522888}]}, {"text": "We run the experiments on two language pairs: \u2022 IWSLT 2017 German\u2212English (DE\u2212EN, We use byte pair encoding (BPE;) with a joint segmentation with 32,000 merges for both language pairs.", "labels": [], "entities": [{"text": "IWSLT 2017 German\u2212English", "start_pos": 48, "end_pos": 73, "type": "DATASET", "confidence": 0.8963471531867981}]}, {"text": "Hyperparameters used across experiments are shown in.", "labels": [], "entities": []}, {"text": "All models were implemented in a fork of OpenNMT-py (.", "labels": [], "entities": []}, {"text": "We compare our model to a teacher forcing baseline, i.e. a standard transformer model, without scheduled sampling, with the hyperparameters given in.", "labels": [], "entities": []}, {"text": "We did hyperparameter tuning by trying several different values for dropout and warmup steps, and choosing the best BLEU score on the validation set for the baseline model.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.8121198117733002}, {"text": "BLEU score", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9763939380645752}]}, {"text": "With the scheduled sampling method, the teacher forcing probability continuously decreases over the course of training according to a predefined function of the training steps.", "labels": [], "entities": []}, {"text": "Among the decay strategies proposed for scheduled sampling, we found that linear decay is the one that works best for our data: where 0 \u2264 < 1 is the minimum teacher forcing probability to be used in the model and k and c provide the offset and slope of the decay.", "labels": [], "entities": [{"text": "offset", "start_pos": 233, "end_pos": 239, "type": "METRIC", "confidence": 0.978879988193512}]}, {"text": "This function determines the teacher forcing ratio t for training step i, that is, the probability of doing teacher forcing at each position in the sequence.", "labels": [], "entities": []}, {"text": "The results from our experiments are shown In.", "labels": [], "entities": []}, {"text": "The scheduled sampling which uses only the highest-scored word predicted by the model does not have a very good performance.", "labels": [], "entities": []}, {"text": "The models which use mixed embeddings (the top-k, softmax, Gumbel softmax or sparsemax) and only backpropagate through the second decoder pass, perform slightly better than the baseline on the validation set, and one of them is also slightly better on the test set.", "labels": [], "entities": []}, {"text": "The differentiable scheduled sampling (when the model backpropagates through the first decoder) have much lower results.", "labels": [], "entities": []}, {"text": "The performance of these models starts degrading too early, so we expect that using more training steps with teacher forcing at the beginning of the training would lead to better performance, so this setup still needs to be examined more carefully.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameters shared across models", "labels": [], "entities": []}, {"text": " Table 2: Experiments with scheduled sampling for Transformer. The table shows BLEU score for the best check- point on BLEU, measured on the validation set. The first group of experiments do not have a backpropagation  pass through the first decoder. The results from the second group are from model runs with backpropagation pass  through the second as well as through the first decoder.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9985528588294983}, {"text": "check- point", "start_pos": 103, "end_pos": 115, "type": "METRIC", "confidence": 0.8692606886227926}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9796567559242249}]}]}