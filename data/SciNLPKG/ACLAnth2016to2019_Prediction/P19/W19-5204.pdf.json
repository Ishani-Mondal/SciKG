{"title": [{"text": "APE at Scale and its Implications on MT Evaluation Biases", "labels": [], "entities": [{"text": "APE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7912262082099915}, {"text": "MT Evaluation Biases", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.9272788166999817}]}], "abstractContent": [{"text": "In this work, we train an Automatic Post-Editing (APE) model and use it to reveal biases in standard Machine Translation (MT) evaluation procedures.", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 101, "end_pos": 136, "type": "TASK", "confidence": 0.870582381884257}]}, {"text": "The goal of our APE model is to correct typical errors introduced by the translation process, and convert the \"translationese\" output into natural text.", "labels": [], "entities": []}, {"text": "Our APE model is trained entirely on monolin-gual data that has been round-trip translated through English, to mimic errors that are similar to the ones introduced by NMT.", "labels": [], "entities": [{"text": "APE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7494615316390991}, {"text": "NMT", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.9369879961013794}]}, {"text": "We apply our model to the output of existing NMT systems, and demonstrate that, while the human-judged quality improves in all cases, BLEU scores drop with forward-translated test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9992450475692749}]}, {"text": "We verify these results for the WMT18 English\u2192German, WMT15 English\u2192French, and WMT16 English\u2192Romanian tasks.", "labels": [], "entities": [{"text": "WMT18 English\u2192German", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.707951009273529}, {"text": "WMT15 English\u2192French", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.8315616697072983}, {"text": "WMT16 English\u2192Romanian tasks", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.7997255325317383}]}, {"text": "Furthermore , we selectively apply our APE model on the output of the top submissions of the most recent WMT evaluation campaigns.", "labels": [], "entities": [{"text": "APE", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9426464438438416}, {"text": "WMT evaluation", "start_pos": 105, "end_pos": 119, "type": "TASK", "confidence": 0.8393554389476776}]}, {"text": "We see quality improvements on all tasks of up to 2.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9994226694107056}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) () is currently the most popular approach in Machine Translation leading to state-of-the-art performance for many tasks.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7853672951459885}, {"text": "Machine Translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8683832287788391}]}, {"text": "NMT relies mainly on parallel training data, which can bean expensive and scarce resource.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7106896638870239}]}, {"text": "There are several approaches to leverage monolingual data for NMT: Language model fusion for both phrase-based () and neural MT (, back-translation (), unsupervised NMT (), dual learning (), and multi-task learning.", "labels": [], "entities": [{"text": "Language model fusion", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.6357906659444174}]}, {"text": "In this paper, we present a different approach to leverage monolingual data, which can be used as a post-processor for any existing translation.", "labels": [], "entities": []}, {"text": "The idea is to train an Automatic Post-Editing (APE) system that is only trained on a large amount of synthetic data, to fix typical errors introduced by the translation process.", "labels": [], "entities": []}, {"text": "During training, our model uses a noisy version of each sentence as input and learns how to reconstruct the original sentence.", "labels": [], "entities": []}, {"text": "In this work, we model the noise with round-trip translations (RTT) through English, translating a sentence in the target language into English, then translating the result back into the original language.", "labels": [], "entities": [{"text": "round-trip translations (RTT)", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.6350997388362885}]}, {"text": "We train our APE model with a standard transformer model on the WMT18 English\u2192German, WMT15 English\u2192French and WMT16 English\u2192Romanian monolingual News Crawl data and apply this model on the output of NMT models that are either trained on all available bitext or trained on a combination of bitext and back-translated monolingual data.", "labels": [], "entities": [{"text": "APE", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.7381541132926941}, {"text": "WMT18 English\u2192German", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.9050214886665344}, {"text": "WMT15 English\u2192French", "start_pos": 86, "end_pos": 106, "type": "DATASET", "confidence": 0.8433299958705902}, {"text": "WMT16 English\u2192Romanian monolingual News Crawl data", "start_pos": 111, "end_pos": 161, "type": "DATASET", "confidence": 0.8773279339075089}]}, {"text": "Furthermore, we show that our APE model can be used as a post-processor for the best output of the recent WMT evaluation campaigns, where it improves even the output of these well engineered translation systems.", "labels": [], "entities": [{"text": "WMT evaluation", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7515223920345306}]}, {"text": "In addition to measuring quality in terms of BLEU scores on the standard WMT test sets, we split each test set into two subsets based on whether the source or target is the original sentence (each sentence is either originally written in the source or target language and human-translated into the other).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9992355108261108}, {"text": "WMT test sets", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.8291651010513306}]}, {"text": "We call these the source-language-original and target-languageoriginal halves, respectively.", "labels": [], "entities": []}, {"text": "We find that evaluating our post-edited output on the source-languageoriginal half actually decreases the BLEU scores, whereas the BLEU scores improve for the targetlanguage-original half.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.999427855014801}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9988043308258057}]}, {"text": "This is inline with results from, who demonstrate that the mere fact of being translated plays a crucial role in the makeup of a translated text, making the actual (human) translation a less natural example of the target language.", "labels": [], "entities": []}, {"text": "We hypothesize that, given these findings, the consistent decreases in BLEU scores on test sets whose source side are natural text does not mean that the actual output is of lower quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9982353448867798}]}, {"text": "To verify this hypothesis, we run human evaluations for different outputs with and without APE.", "labels": [], "entities": [{"text": "APE", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.794922411441803}]}, {"text": "The human ratings demonstrate that the output of the APE model is both consistently more accurate and consistently more fluent, regardless of whether the source or the target language is the original language, contradicting the corresponding BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 242, "end_pos": 246, "type": "METRIC", "confidence": 0.9991457462310791}]}, {"text": "To summarize the contributions of the paper: \u2022 We introduce an APE model trained only on synthetic data generated with RTT for fixing typical translation errors from NMT output and investigate its scalability.", "labels": [], "entities": [{"text": "APE", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9022649526596069}]}, {"text": "To the best of our knowledge, this paper is the first to study the effect of an APE system trained at scale and only on synthetic data.", "labels": [], "entities": [{"text": "APE", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.8773360252380371}]}, {"text": "\u2022 We improve the BLEU of top submissions of the recent WMT evaluation campaigns.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9995622038841248}, {"text": "WMT evaluation", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.8049225211143494}]}, {"text": "\u2022 We show that the BLEU scores of the APE output only correlate well with human ratings when they are calculated with target-original references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9993020296096802}, {"text": "APE output", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.6722252815961838}]}, {"text": "\u2022 We propose separately reporting scores on test sets whose source sentences are translated and whose target sentences are translated, and call for higher-quality test sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report BLEU () and human evaluations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992276430130005}]}, {"text": "All BLEU scores are calculated with sacreBLEU (Post, 2018) . Since 2014, the organizers of the WMT evaluation campaign) have created test sets with the following method: first, they crawled monolingual data in both English and the target language from news stories from online sources.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.999157190322876}, {"text": "WMT evaluation campaign", "start_pos": 95, "end_pos": 118, "type": "DATASET", "confidence": 0.6189623673756918}]}, {"text": "Thereafter they took about 1500 English sentences and translated them into the target language, and an additional 1500 sentences from the target language and translated them into English.", "labels": [], "entities": []}, {"text": "This results in test sets of about 3000 sentences for each English-X language pair.", "labels": [], "entities": []}, {"text": "The sgm files of each WMT test set include the original language for each sentence.", "labels": [], "entities": [{"text": "WMT test set", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.8128133416175842}]}, {"text": "Therefore, in addition to reporting overall BLEU scores on the different test sets, we also report results on the two subsets (based on the original language) of each newstest20XX, which we call the {German,French,Romanian}-original and English-original halves of the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9983172416687012}]}, {"text": "This is motivated by, who demonstrated that they can train a simple classifier to distinguish human-translated text from natural text with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9865972399711609}]}, {"text": "These text categorization experiments suggest that both the source language and the mere fact of being translated play a crucial role in the makeup of a translated text.", "labels": [], "entities": []}, {"text": "One of the major goals of our APE model is to rephrase the NMT output in a more natural way, aiming to remove undesirable translation artifacts that have been introduced.", "labels": [], "entities": []}, {"text": "To collect human rankings, we present each output to crowd-workers, who were asked to score each sentence on a 5-point scale for: \u2022 fluency: How do you judge the overall naturalness of the utterance in terms of its grammatical correctness and fluency?", "labels": [], "entities": []}, {"text": "Further, we included the source sentence and asked the raters to evaluate each sentence on a 2-point scale (binary decision) for: \u2022 accuracy: Does the statement factually contradict anything in the reference information?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9994964599609375}]}, {"text": "Each task was given to three different raters.", "labels": [], "entities": []}, {"text": "Consequently, each output has a separate score for each question that is the average of 3 different ratings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores for WMT18 English\u2192German. We apply the same APE model (trained on RTT with bitext  models) for both an NMT system based on pure bitext and an NMT system that uses noised back-translation (NBT)  in addition to bitext.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992247819900513}, {"text": "WMT18 English\u2192German", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.5022914111614227}, {"text": "APE", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9944542050361633}]}, {"text": " Table 2: BLEU scores for WMT18 English\u2192German. Test sets are divided by their original source language  (either German or English).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990231990814209}, {"text": "WMT18 English", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.6077284514904022}]}, {"text": " Table 3: English\u2192German human evaluation results split by original language of the test set.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores for WMT18 English\u2192German  newstest2018. We apply our APE model only on the  German-original half of the test set. BLEU scores are  calculated on the full newstest2018 set and the English- original half is just copied from the submission.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995323419570923}, {"text": "WMT18 English\u2192German  newstest2018", "start_pos": 26, "end_pos": 60, "type": "DATASET", "confidence": 0.7067277669906616}, {"text": "APE", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9784537553787231}, {"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.999049723148346}]}, {"text": " Table 5: BLEU scores for our models on WMT16  English\u2192Romanian.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995107650756836}, {"text": "WMT16  English\u2192Romanian", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.903631404042244}]}, {"text": " Table 6: BLEU scores for WMT16 English\u2192Romanian  test set. Our APE model was applied on top of the best  WMT16 submissions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994650483131409}, {"text": "WMT16 English\u2192Romanian  test set", "start_pos": 26, "end_pos": 58, "type": "DATASET", "confidence": 0.8076040347417196}, {"text": "APE", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9703297019004822}, {"text": "WMT16", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.8205018043518066}]}, {"text": " Table 7: BLEU scores for WMT15 English\u2192French.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992853999137878}, {"text": "WMT15 English\u2192French", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.624776616692543}]}, {"text": " Table 9:  Average BLEU scores for WMT18  English\u2192German newstest2014-2017. We run our  APE model a second time on the output of the already  post-editied output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9612907767295837}, {"text": "WMT18  English\u2192German newstest2014-2017", "start_pos": 35, "end_pos": 74, "type": "DATASET", "confidence": 0.8481691241264343}, {"text": "APE", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.594145655632019}]}, {"text": " Table 10: Average BLEU scores for WMT18  English\u2192German newstest2014-2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9005459547042847}, {"text": "WMT18  English\u2192German newstest2014-2017", "start_pos": 35, "end_pos": 74, "type": "DATASET", "confidence": 0.8509681105613709}]}]}