{"title": [{"text": "Label-Agnostic Sequence Labeling by Copying Nearest Neighbors", "labels": [], "entities": [{"text": "Label-Agnostic Sequence Labeling", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5782449642817179}]}], "abstractContent": [{"text": "Retrieve-and-edit based approaches to struc-tured prediction, where structures associated with retrieved neighbors are edited to form new structures, have recently attracted increased interest.", "labels": [], "entities": [{"text": "struc-tured prediction", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.8621029257774353}]}, {"text": "However, much recent work merely conditions on retrieved structures (e.g., in a sequence-to-sequence framework), rather than explicitly manipulating them.", "labels": [], "entities": []}, {"text": "We show we can perform accurate sequence labeling by explicitly (and only) copying labels from retrieved neighbors.", "labels": [], "entities": []}, {"text": "Moreover, because this copying is label-agnostic, we can achieve impressive performance in zero-shot sequence-labeling tasks.", "labels": [], "entities": [{"text": "copying", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9613344073295593}]}, {"text": "We additionally consider a dynamic programming approach to sequence labeling in the presence of retrieved neighbors, which allows for controlling the number of distinct (copied) segments used to form a prediction , and leads to both more interpretable and accurate predictions.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.6361276358366013}]}], "introductionContent": [{"text": "Retrieve-and-edit style structured prediction, where a model retrieves a set of labeled nearest neighbors from the training data and conditions on them to generate the target structure, is a promising approach that has recently received renewed interest (.", "labels": [], "entities": [{"text": "Retrieve-and-edit style structured prediction", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7013685405254364}]}, {"text": "This approach captures the intuition that while generating a highly complex structure from scratch maybe difficult, editing a sufficiently similar structure or set of structures maybe easier.", "labels": [], "entities": []}, {"text": "Recent work in this area primarily uses the nearest neighbors and their labels simply as an additional context fora sequence-to-sequence style model to condition on.", "labels": [], "entities": []}, {"text": "While effective, these models may not explicitly capture the discrete operations (like copying) that allow for the neighbors to be edited into the target structure, making interpreting the behavior of the model difficult.", "labels": [], "entities": []}, {"text": "Moreover, since many retrieve-and-edit style models condition on dataset-specific labels directly, they may not easily allow for transfer learning and in particular to porting a trained model to anew task with different labels.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.9307279884815216}]}, {"text": "We address these limitations in the context of sequence labeling by developing a simple labelagnostic model that explicitly models copying token-level labels from retrieved neighbors.", "labels": [], "entities": []}, {"text": "Since the model is not a function of the labels themselves but only of a learned notion of similarity between an input and retrieved neighbor inputs, it can be effortlessly ported (zero shot) to a task with different labels, without any retraining.", "labels": [], "entities": []}, {"text": "Such a model can also take advantage of recent advances in representation learning, such as BERT (, in defining this similarity.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.9227221310138702}, {"text": "BERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9960317015647888}]}, {"text": "We evaluate the proposed approach on standard sequence labeling tasks, and show it is competitive with label-dependent approaches when trained on the same data, but substantially outperforms strong baselines when it comes to zero-shot transfer applications, such as when training with coarse labels and testing with fine-grained labels.", "labels": [], "entities": []}, {"text": "Finally, we propose a dynamic programming based approach to sequence labeling in the presence of retrieved neighbors, which allows for trading off token-level prediction confidence with trying to minimize the number of distinct segments in the overall prediction that are taken from neighbors.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.6792219579219818}]}, {"text": "We find that such an approach allows us to both increase the interpretability of our predictions as well as their accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9977715015411377}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of fine-tuned BERT and nearest- neighbor based labeling (NN) on NER, POS tagging,  and universal POS tagging; see text. BERT numbers  are from fine-tuning the huggingface implementa- tion, and differ slightly from Devlin et al. (2018).", "labels": [], "entities": [{"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.983669102191925}, {"text": "BERT", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9505418539047241}]}, {"text": " Table 2: Zero-shot performance of models trained on  CoNLL NER and applied to fine-grained OntoNotes  NER, with universal POS tags and applied to standard  POS tagging, and on CoNLL chunking and applied to  CoNLL NER. \"NN (no FT)\" indicates BERT was not  fine tuned even on the original task.", "labels": [], "entities": [{"text": "CoNLL NER", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.9153448045253754}, {"text": "CoNLL chunking", "start_pos": 177, "end_pos": 191, "type": "TASK", "confidence": 0.6669100224971771}, {"text": "CoNLL NER", "start_pos": 208, "end_pos": 217, "type": "DATASET", "confidence": 0.8731682896614075}, {"text": "NN", "start_pos": 220, "end_pos": 222, "type": "METRIC", "confidence": 0.9377689957618713}, {"text": "FT)\"", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9456306099891663}, {"text": "BERT", "start_pos": 242, "end_pos": 246, "type": "METRIC", "confidence": 0.9965296387672424}]}]}