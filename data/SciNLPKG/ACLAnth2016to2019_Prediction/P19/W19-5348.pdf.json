{"title": [{"text": "Microsoft Research Asia's Systems for WMT19", "labels": [], "entities": [{"text": "Microsoft Research Asia's Systems", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.9309075117111206}, {"text": "WMT19", "start_pos": 38, "end_pos": 43, "type": "TASK", "confidence": 0.5401662588119507}]}], "abstractContent": [{"text": "We Microsoft Research Asia made submissions to 11 language directions in the WMT19 news translation tasks.", "labels": [], "entities": [{"text": "WMT19 news translation tasks", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.7824810743331909}]}, {"text": "We won the first place for 8 of the 11 directions and the second place for the other three.", "labels": [], "entities": []}, {"text": "Our basic systems are built on Transformer, back translation and knowledge distillation.", "labels": [], "entities": [{"text": "back translation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7021303176879883}, {"text": "knowledge distillation", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7288930714130402}]}, {"text": "We integrate several of our rececent techniques to enhance the baseline systems: multi-agent dual learning (MADL), masked sequence-to-sequence pre-training (MASS), neural architecture optimization (NAO), and soft contextual data augmentation (SCA).", "labels": [], "entities": [{"text": "neural architecture optimization (NAO)", "start_pos": 164, "end_pos": 202, "type": "TASK", "confidence": 0.8091653088728586}]}], "introductionContent": [{"text": "We participated in the WMT19 shared news translation task in 11 translation directions.", "labels": [], "entities": [{"text": "WMT19 shared news translation task", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.8104145526885986}]}, {"text": "We achieved first place for 8 directions: German\u2194English, German\u2194French, Chinese\u2194English, English\u2192Lithuanian, English\u2192Finnish, and Russian\u2192English, and three other directions were placed second (ranked by teams), which included Lithuanian\u2192English, Finnish\u2192English, and English\u2192Kazakh.", "labels": [], "entities": []}, {"text": "Our basic systems are based on Transformer, back translation and knowledge distillation.", "labels": [], "entities": [{"text": "back translation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6971153914928436}, {"text": "knowledge distillation", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7238204181194305}]}, {"text": "We experimented with several techniques we proposed recently.", "labels": [], "entities": []}, {"text": "In brief, the innovations we introduced are: Multi-agent dual learning (MADL) The core idea of dual learning is to leverage the duality between the primal task (mapping from domain X to domain Y) and dual task (mapping from domain Y to X ) to boost the performances of both tasks.", "labels": [], "entities": [{"text": "Multi-agent dual learning (MADL)", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.8180115222930908}, {"text": "dual learning", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.8938989341259003}]}, {"text": "MADL () extends the dual learning () framework by introducing multiple primal and dual models.", "labels": [], "entities": []}, {"text": "It was integrated into our submitted systems for *Corresponding author.", "labels": [], "entities": []}, {"text": "This work was conducted at Microsoft Research Asia.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 27, "end_pos": 50, "type": "DATASET", "confidence": 0.9187173446019491}]}, {"text": "German\u2194English and German\u2194French translations.", "labels": [], "entities": []}, {"text": "Masked sequence-to-sequence pretraining (MASS) Pre-training and fine-tuning have achieved great success in language understanding.", "labels": [], "entities": [{"text": "Masked sequence-to-sequence pretraining (MASS)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.5241978466510773}, {"text": "language understanding", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.8432156145572662}]}, {"text": "MASS (), a pre-training method designed for language generation, adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment.", "labels": [], "entities": [{"text": "language generation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7365223318338394}]}, {"text": "It was integrated into our submitted systems for Chinese\u2192English and English\u2192Lithuanian translations.", "labels": [], "entities": []}, {"text": "Neural architecture optimization (NAO) As well known, the evolution of neural network architecture plays a key role in advancing neural machine translation.", "labels": [], "entities": [{"text": "Neural architecture optimization (NAO)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.80460757513841}, {"text": "neural machine translation", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.6658468643824259}]}, {"text": "Neural architecture optimization (NAO), our newly proposed method (, leverages the power of a gradient-based method to conduct optimization and guide the creation of better neural architecture in a continuous and more compact space given the historically observed architectures and their performances.", "labels": [], "entities": [{"text": "Neural architecture optimization (NAO)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8262080599864324}]}, {"text": "It was applied in English\u2194Finnish translations in our submitted systems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of English\u2194German by sacreBLEU.", "labels": [], "entities": []}, {"text": " Table 2: Results of German\u2194French by sacreBLEU.", "labels": [], "entities": []}, {"text": " Table 3. We list two  baseline Transformer big systems which use 18M  bilingual data (constraint) and 56M bilingual data  (unconstraint) respectively. The pre-trained model  achieves about 1 BLEU point improvement after  fine-tuning on both 18M and 56M bilingual data.  After iterative back translation (BT) and knowl- edge distillation (KD), as well as re-ranking, our  system achieves 30.8 and 30.9 BLEU points on  newstest2017 and newstest2018 respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.998284637928009}, {"text": "BLEU", "start_pos": 402, "end_pos": 406, "type": "METRIC", "confidence": 0.9987674951553345}, {"text": "newstest2017", "start_pos": 418, "end_pos": 430, "type": "DATASET", "confidence": 0.9580129981040955}]}, {"text": " Table 3: BLEU scores on Chinese\u2192English test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987936019897461}, {"text": "Chinese\u2192English test sets", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.6344859957695007}]}, {"text": " Table 4: BLEU scores for English\u2194Lithuanian on the  newsdev19 set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990677237510681}, {"text": "newsdev19 set", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9806928336620331}]}, {"text": " Table 5: BLEU scores of L2R Transformer on  English\u2192Finnish test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991154074668884}, {"text": "Finnish test sets", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.7692222893238068}]}, {"text": " Table 6: The final BLEU scores on English\u2192Finnish  test sets, for the three models: L2R Transformer, R2L  Transformer and NAONet, after the four steps of train- ing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9962151646614075}, {"text": "Finnish  test sets", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.7257537941137949}]}, {"text": " Table 7: The final BLEU scores on Finnish\u2192English  test sets, for the three models: L2R Transformer, R2L  Transformer and NAONet, after the four steps of train- ing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9954337477684021}, {"text": "Finnish\u2192English  test sets", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.5889648497104645}, {"text": "NAONet", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.6969786286354065}]}, {"text": " Table 8: English\u2192Finnish BLEU scores of re-ranking  using the three models. \"news\" is short for \"newstest\".", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9461798071861267}]}, {"text": " Table 9: Finnish\u2192English BLEU scores of re-ranking  using the three models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9943506121635437}]}, {"text": " Table 10: Russian\u2192English BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.969813883304596}]}]}