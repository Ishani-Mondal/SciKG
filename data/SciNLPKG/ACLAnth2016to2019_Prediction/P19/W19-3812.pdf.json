{"title": [{"text": "Transfer Learning from Pre-trained BERT for Pronoun Resolution", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9905593395233154}, {"text": "Pronoun Resolution", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8645970225334167}]}], "abstractContent": [{"text": "The paper describes the submission of the team \"We used bert!\" to the shared task Gen-dered Pronoun Resolution (Pair pronouns to their correct entities).", "labels": [], "entities": [{"text": "Gen-dered Pronoun Resolution", "start_pos": 82, "end_pos": 110, "type": "TASK", "confidence": 0.6932520667711893}]}, {"text": "Our final submission model based on the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) ranks 14th among 838 teams with a multi-class logarithmic loss of 0.208.", "labels": [], "entities": [{"text": "BERT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9961581826210022}]}, {"text": "In this work, contribution of transfer learning technique to pronoun resolution systems is investigated and the gender bias contained in classification models is evaluated .", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7748108506202698}]}], "introductionContent": [{"text": "The shared task Gendered Pronoun Resolution aims to classify the pronoun resolution in the sentences, hereby to find the true name referred by a given pronoun, such as she in: In May, Fujisawa joined Mari Motohashi's rink as the team's skip, moving back from Karuizawa to Kitami where she had spent her junior days.", "labels": [], "entities": [{"text": "Gendered Pronoun Resolution", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.7574056585629781}, {"text": "classify the pronoun resolution", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.7548470348119736}, {"text": "Fujisawa", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9485053420066833}]}, {"text": "This task for pronoun resolution closely relates to the traditional coreference resolution task in natural language processing.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8136248886585236}, {"text": "coreference resolution task", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.8893032272656759}]}, {"text": "Many works) related to coreference resolution have been published recently and all of them are evaluated with CoNLL-2012 shared task dataset).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9820908010005951}, {"text": "CoNLL-2012 shared task dataset", "start_pos": 110, "end_pos": 140, "type": "DATASET", "confidence": 0.8363374471664429}]}, {"text": "However, simply pursuing the best score over the entire dataset may cause the neglect of the model performance gap between the two genders.", "labels": [], "entities": []}, {"text": "To explore the existence of gender bias in such tasks, researchers from Google built and released GAP (Gendered Ambiguous Pronouns)), a human-labeled corpus of 8908 ambiguous pronoun-name pairs derived * Both authors contributed equally in this work. from Wikipedia with balanced gender pronouns.", "labels": [], "entities": []}, {"text": "It has been shown that most of the recent representative coreference systems struggled on GAP dataset with a overall mediocre performance and a large performance gap between genders.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.7318424135446548}]}, {"text": "This maybe due to both unbalanced training dataset used by these coreference systems or the design of the systems.", "labels": [], "entities": []}, {"text": "Up to now, detecting and eliminating gender bias in such systems still remains a challenge.", "labels": [], "entities": []}, {"text": "In this paper, we explore transfer learning from pre-trained models to improve the performance of tasks with limited data.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8879779577255249}]}, {"text": "Various efficient approaches to reuse the knowledge from pre-trained BERT on this shared task are proposed and compared.", "labels": [], "entities": []}, {"text": "The final system significantly outperforms the off-the-shelf resolvers, with a balanced prediction performance for two genders.", "labels": [], "entities": []}, {"text": "Moreover, gender bias in word and sentence level embeddings is studied with a scientific statistical experiment on Caliskan dataset).", "labels": [], "entities": [{"text": "Caliskan dataset", "start_pos": 115, "end_pos": 131, "type": "DATASET", "confidence": 0.8928030729293823}]}], "datasetContent": [{"text": "In this section, we present the result of different classifiers to the shared task.", "labels": [], "entities": []}, {"text": "For SVM, C equals to 5.0 and the kernel function is the RBF function.", "labels": [], "entities": [{"text": "RBF", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9805158376693726}]}, {"text": "The SVM is trained both with the original 1024 dimension mention vectors and the 256 dimension-reduced mention vectors respectively for comparison.", "labels": [], "entities": []}, {"text": "The BIDAF network is trained for 50 epoches with a batch size of 25.", "labels": [], "entities": [{"text": "BIDAF network", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.823478490114212}]}, {"text": "We use the Adam optimizer with a learning rate of 1e-3 for training.", "labels": [], "entities": []}, {"text": "For each fully-connected layer in BIDAF, a dropout with probability 0.7 is performed.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8090339303016663}]}, {"text": "It is trained both with the original training set and the augmented training set for comparison.", "labels": [], "entities": []}, {"text": "This training process takes about 10 minutes with the GTX 1070 GPU.", "labels": [], "entities": [{"text": "GTX 1070 GPU", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9362561106681824}]}, {"text": "The fine-tuned BERT models are trained with the Adam optimizer with a learning rate of 2e-5.", "labels": [], "entities": [{"text": "BERT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9839738011360168}, {"text": "learning rate", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.95738285779953}]}, {"text": "All the dropout layers in the original BERT model are set to a dropout rate of 0.15.", "labels": [], "entities": [{"text": "BERT", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.4782540798187256}]}, {"text": "Models are trained for 1 epoch with a batch size of 16.", "labels": [], "entities": []}, {"text": "Note that it is not possible to fit 16 training sentences atone time due to the limited GPU memory.", "labels": [], "entities": []}, {"text": "Hence, gradient accumulation trick is used.", "labels": [], "entities": []}, {"text": "Every time we fit 2 training sentences and we accumulate the gradient for 8 times.", "labels": [], "entities": []}, {"text": "This fine-tuning process takes about 10 minutes with the Tesla K80 GPU.", "labels": [], "entities": [{"text": "Tesla K80 GPU", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8604361613591512}]}, {"text": "The meta classifier is the logistic regression with l 2 regularization of the regularization constant C which equals to 0.5.", "labels": [], "entities": []}, {"text": "The results are shown in table 3.", "labels": [], "entities": []}, {"text": "The masculine data loss and feminine data loss are shown respectively in order to show the gender bias.", "labels": [], "entities": [{"text": "feminine data loss", "start_pos": 28, "end_pos": 46, "type": "METRIC", "confidence": 0.6979721486568451}]}, {"text": "We compute the model loss for testing data (stage 1) and the loss caused by the masculine part and the feminine part in stage 1 testing data.", "labels": [], "entities": []}, {"text": "We also submit our base model results after the competition finishes in order to get the private testing data (stage 2) loss.", "labels": [], "entities": []}, {"text": "We derive the following conclusions: \u2022 The dimension reduction greatly enhances the result of SVM which reduces about 0.1 multi-class logarithmic loss.", "labels": [], "entities": [{"text": "SVM", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.8425305485725403}]}, {"text": "The SVM 1024 has a loss of 0.184 and 0.597 with respect to training and testing data, while the SVM 256 has a loss of 0.250 and 0.505.", "labels": [], "entities": [{"text": "SVM 1024", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9605719447135925}, {"text": "SVM", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.8819486498832703}]}, {"text": "Both SVM model overfit a lot, while the dimension reduction of BERT contextual embeddings efficiently mitigate overfitting, which bridges the performance gap between training data and testing data.", "labels": [], "entities": [{"text": "BERT", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9511474967002869}]}, {"text": "\u2022 The BIDAF model performs worse when trained with the augmented training set than the original training set, due to the distribution mismatching caused by data augmentation that, the portion of the neither data is larger in the training set than in the testing set.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.929598331451416}]}, {"text": "\u2022 Both two fine-tuned BERT models achieve much more competitive results compared to Bert as Feature Extractor models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9902614951133728}]}, {"text": "4 \u2022 The ensemble learning with logistic regression greatly enhances the overall classification result.", "labels": [], "entities": []}, {"text": "Although the data augmentation does not improve the BIDAF model directly, it still helps to make more accurate predictions of the neither class in the ensemble model.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9626978039741516}]}, {"text": "The BIDAF-aug and the BIDAF reach the loss of 0.982 and 1.095, respectively.", "labels": [], "entities": [{"text": "BIDAF-aug", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.5621686577796936}, {"text": "BIDAF", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7636699080467224}]}, {"text": "In the testing data (stage 1), the respective accuracy of A, B and neither class is 89.8%, 89.5% and 73.1%, indicating that predicting the neither class correctly is much harder than predicting A and B.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9994811415672302}]}, {"text": "We can observe that it is easier for the model to choose an answer as A or B than to predict as no reference.", "labels": [], "entities": []}, {"text": "We also evaluate our system F1 score with stage 1 testing dataset to compare to the off-the-shelf resolvers in  We apply WEAT and SEAT on Caliskan Test of male/female names with career and family, which corresponds to past social psychology studies.", "labels": [], "entities": [{"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9975181818008423}, {"text": "WEAT", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9931570887565613}, {"text": "SEAT", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9984087347984314}]}, {"text": "Method GloVe ELMo BERT F-BERT WEAT   on both word level and sentence level show significant gender bias, indicating that women are associated with family while men are associated with career.", "labels": [], "entities": [{"text": "BERT F-BERT WEAT", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.651921013991038}]}, {"text": "However, p-values of all contextual embeddings including ELMo, BERT and Fined-tuned BERT are larger than 0.05, which suggests that there is no evidence suggesting existence of gender bias in these embeddings.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.7638338804244995}, {"text": "BERT", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9938855767250061}, {"text": "BERT", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.770275354385376}]}, {"text": "One possible explanation is that, by training contextual word embeddings, a single word is usually represented differently in different sentences, resulting in more flexible word representations focusing on single context within a sentence rather than the overall word frequency distribution.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pronoun gender frequency", "labels": [], "entities": [{"text": "Pronoun gender", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8037441968917847}]}, {"text": " Table 4: Comparison to off-the-shelf resolvers, split  by Masculine and Feminine (Bias shows F/M), and  Overall. Bold indicates the best performance.", "labels": [], "entities": [{"text": "resolvers", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.9084766507148743}, {"text": "Masculine", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9696100950241089}, {"text": "Bias", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9513412117958069}, {"text": "F/M", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.8957207202911377}]}, {"text": " Table 5: Effect sizes for male/female names with ca- reer/family task with word and sentence level embed- dings.  *  : significant at 0.01. F-BERT indicates Fine- tuned BERT.", "labels": [], "entities": [{"text": "F-BERT", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9972308278083801}, {"text": "Fine- tuned", "start_pos": 158, "end_pos": 169, "type": "METRIC", "confidence": 0.8923381567001343}, {"text": "BERT", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.5672271251678467}]}]}