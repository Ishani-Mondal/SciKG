{"title": [{"text": "Learning Compressed Sentence Representations for On-Device Text Processing", "labels": [], "entities": [{"text": "Learning Compressed Sentence Representations", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.5460995733737946}, {"text": "On-Device Text Processing", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6247817873954773}]}], "abstractContent": [{"text": "Vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of NLP problems.", "labels": [], "entities": []}, {"text": "The learned representations are generally assumed to be continuous and real-valued, giving rise to a large memory footprint and slow retrieval speed, which hinders their applicability to low-resource (mem-ory and computation) platforms, such as mobile devices.", "labels": [], "entities": []}, {"text": "In this paper, we propose four different strategies to transform continuous and generic sentence embeddings into a bi-narized form, while preserving their rich semantic information.", "labels": [], "entities": []}, {"text": "The introduced methods are evaluated across a wide range of downstream tasks, where the binarized sentence em-beddings are demonstrated to degrade performance by only about 2% relative to their continuous counterparts, while reducing the storage requirement by over 98%.", "labels": [], "entities": []}, {"text": "Moreover, with the learned binary representations, the semantic relatedness of two sentences can be evaluated by simply calculating their Hamming distance , which is more computational efficient compared with the inner product operation between continuous embeddings.", "labels": [], "entities": []}, {"text": "Detailed analysis and case study further validate the effectiveness of proposed methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning general-purpose sentence representations from large training corpora has received widespread attention in recent years.", "labels": [], "entities": []}, {"text": "The learned sentence embeddings can encapsulate rich prior knowledge of natural language, which has been demonstrated to facilitate a variety of downstream tasks (without fine-tuning the encoder weights).", "labels": [], "entities": []}, {"text": "The generic sentence embeddings can be trained either in an unsupervised manner (; Gan * Equal contribution.", "labels": [], "entities": []}, {"text": "et al.,, or with supervised tasks such as paraphrase identification, natural language inference (, discourse relation classification (, machine translation (, etc.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.9020172953605652}, {"text": "discourse relation classification", "start_pos": 99, "end_pos": 132, "type": "TASK", "confidence": 0.6544378101825714}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.758473664522171}]}, {"text": "Significant effort has been devoted to designing better training objectives for learning sentence embeddings.", "labels": [], "entities": []}, {"text": "However, prior methods typically assume that the general-purpose sentence representations are continuous and real-valued.", "labels": [], "entities": []}, {"text": "However, this assumption is sub-optimal from the following perspectives: i) the sentence embeddings require large storage or memory footprint; ii) it is computationally expensive to retrieve semanticallysimilar sentences, since every sentence representation in the database needs to be compared, and the inner product operation is computationally involved.", "labels": [], "entities": []}, {"text": "These two disadvantages hinder the applicability of generic sentence representations to mobile devices, where a relatively tiny memory footprint and low computational capacity are typically available.", "labels": [], "entities": [{"text": "generic sentence representations", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.6472051839033762}]}, {"text": "In this paper, we aim to mitigate the above issues by binarizing the continuous sentence embeddings.", "labels": [], "entities": []}, {"text": "Consequently, the embeddings require much smaller footprint, and similar sentences can be obtained by simply selecting those with closest binary codes in the Hamming space.", "labels": [], "entities": []}, {"text": "One simple idea is to naively binarize the continuous vectors by setting a hard threshold.", "labels": [], "entities": []}, {"text": "However, we find that this strategy leads to significant performance drop in the empirical results.", "labels": [], "entities": []}, {"text": "Besides, the dimension of the binary sentence embeddings cannot be flexibly chosen with this strategy, further limiting the practice use of the direct binarization method.", "labels": [], "entities": []}, {"text": "In this regard, we propose three alternative strategies to parametrize the transformation from pre-trained generic continuous embeddings to their binary forms.", "labels": [], "entities": []}, {"text": "Our exploration spans from simple operations, such as a random projection, to deep neural network models, such as a regularized autoencoder.", "labels": [], "entities": []}, {"text": "Particularly, we introduce a semantic-preserving objective, which is augmented with the standard autoenoder architecture to encourage abstracting informative binary codes.", "labels": [], "entities": []}, {"text": "InferSent () is employed as the testbed sentence embeddings in our experiments, but the binarization schemes proposed here can easily be extended to other pretrained general-purpose sentence embeddings.", "labels": [], "entities": []}, {"text": "We evaluate the quality of the learned general-purpose binary representations using the SentEval toolkit (.", "labels": [], "entities": []}, {"text": "It is observed that the inferred binary codes successfully maintain the semantic features contained in the continuous embeddings, and only lead to around 2% performance drop on a set of downstream NLP tasks, while requiring merely 1.5% memory footprint of their continuous counterparts.", "labels": [], "entities": []}, {"text": "Moreover, on several sentence matching benchmarks, we demonstrate that the relatedness between a sentence pair can be evaluated by simply calculating the Hamming distance between their binary codes, which perform on par with or even superior than measuring the cosine similarity between continuous embeddings (see Table 1).", "labels": [], "entities": [{"text": "sentence matching", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7166234403848648}]}, {"text": "Note that computing the Hamming distance is much more computationally efficient than the inner product operation in a continuous space.", "labels": [], "entities": []}, {"text": "We further perform a K-nearest neighbor sentence retrieval experiment on the SNLI dataset, and show that those semanticallysimilar sentences can indeed be efficiently retrieved with off-the-shelf binary sentence representations.", "labels": [], "entities": [{"text": "K-nearest neighbor sentence retrieval", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.5849215760827065}, {"text": "SNLI dataset", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.9246343672275543}]}, {"text": "Summarizing, our contributions in this paper are as follows: i) to the best of our knowledge, we conduct the first systematic exploration on learning general-purpose binarized (memory-efficient) sentence representations, and four different strategies are proposed; ii) an autoencoder architecture with a carefullydesigned semantic-preserving loss exhibits strong empirical results on a set of downstream NLP tasks; iii) more importantly, we demonstrate, on several sentence-matching datasets, that simply evaluating the Hamming distance over binary representations performs on par or even better than calculating the cosine similarity between their continuous counterparts (which is less computationallyefficient).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our proposed model aims to produce highly informative binary sentence embeddings based upon pre-trained continuous representations.", "labels": [], "entities": []}, {"text": "In this paper, we utilize InferSent ( as the continuous embeddings (given its effectiveness and widespread use).", "labels": [], "entities": []}, {"text": "Note that all four proposed strategies can be easily extended to other pre-trained general-purpose sentence embeddings as well.", "labels": [], "entities": []}, {"text": "Specifically, a bidirectional LSTM architecture along with a max-pooling operation over the hidden units is employed as the sentence encoder, and the model parameters are optimized on the natural language inference tasks, i.e., Standford Natural Language Inference (SNLI) and Multi-Genre Natural Language Inference (MultiNLI) datasets (Williams et al., 2017).", "labels": [], "entities": []}, {"text": "To facilitate comparisons with other baseline methods, we use SentEval toolkit 1 ( to evaluate the learned binary (compact) sentence embeddings.", "labels": [], "entities": []}, {"text": "Concretely, the learned representations are tested on a series of downstream tasks to assess their transferability (with the encoder weights fixed), which can be categorized as follows: \u2022 Sentence classification, including sentiment analysis (MR, SST), product reviews (CR), subjectivity classification (SUBJ), opinion polarity detection (MPQA) and question type classification (TREC).", "labels": [], "entities": [{"text": "Sentence classification", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.8295968174934387}, {"text": "sentiment analysis (MR, SST)", "start_pos": 223, "end_pos": 251, "type": "TASK", "confidence": 0.7969707506043571}, {"text": "subjectivity classification (SUBJ)", "start_pos": 275, "end_pos": 309, "type": "TASK", "confidence": 0.7973216354846955}, {"text": "opinion polarity detection (MPQA)", "start_pos": 311, "end_pos": 344, "type": "TASK", "confidence": 0.7951626479625702}, {"text": "question type classification", "start_pos": 349, "end_pos": 377, "type": "TASK", "confidence": 0.6716325481732687}]}, {"text": "A linear classifier is trained with the generic sentence embeddings as the input features.", "labels": [], "entities": []}, {"text": "The default SentEval settings is used for all the datasets.", "labels": [], "entities": []}, {"text": "\u2022 Sentence matching, which comprises semantic relatedness (SICK-R, STS14, STSB) and paraphrase detection (MRPC).", "labels": [], "entities": [{"text": "Sentence matching", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.8997189104557037}, {"text": "paraphrase detection", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.7357461601495743}]}, {"text": "Particularly, each pair of sentences in STS14 dataset is associated with a similarity score from 0 to 5 (as the corresponding label).", "labels": [], "entities": [{"text": "STS14 dataset", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.7651628851890564}, {"text": "similarity score", "start_pos": 75, "end_pos": 91, "type": "METRIC", "confidence": 0.9420717060565948}]}, {"text": "Hamming distance between the binary representations is directly leveraged as the prediction score (without any classifier parameters).", "labels": [], "entities": [{"text": "prediction score", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.9018847048282623}]}, {"text": "For the sentence matching benchmarks, to allow fair comparison with the continuous embeddings, we do not use the same classifier architecture in SentEval.", "labels": [], "entities": [{"text": "sentence matching", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7193636298179626}]}, {"text": "Instead, we obtain the predicted relatedness by directly computing the cosine similarity between the continuous embeddings.", "labels": [], "entities": []}, {"text": "Consequently, there are no classifier parameters for both the binary and continuous representations.", "labels": [], "entities": []}, {"text": "The same valuation metrics in SentEval( are utilized for all the tasks.", "labels": [], "entities": []}, {"text": "For MRPC, the predictions are made by simply judging whether a sentence pair's score is larger or smaller than the averaged Hamming distance (or cosine similarity).", "labels": [], "entities": [{"text": "MRPC", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.9727808237075806}]}, {"text": "We experimented with five model variants to learn general-purpose binary embeddings: HTbinary (hard threshold, which is selected from {0, 0.01, 0.1} on the validation set), Rand-binary (random projection), PCA-binary (reduce the dimensionality with principal component analysis), AE-binary (autoencoder with the reconstruction objective) and AE-binary-SP (autoencoder with both the reconstruction objective and SemanticPreserving loss).", "labels": [], "entities": [{"text": "AE-binary", "start_pos": 280, "end_pos": 289, "type": "METRIC", "confidence": 0.893050491809845}, {"text": "AE-binary-SP", "start_pos": 342, "end_pos": 354, "type": "METRIC", "confidence": 0.9115478992462158}]}, {"text": "Our code will be released to encourage future research.", "labels": [], "entities": []}, {"text": "We evalaute the binary sentence representations produced by different methods with a set of transferring tasks.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The proposed autoencoder architecture generally demonstrates the best results.", "labels": [], "entities": []}, {"text": "Especially while combined with the semantic-preserving loss defined in, AE-binary-SP exhibits higher performance compared with a standard autoencoder.", "labels": [], "entities": [{"text": "AE-binary-SP", "start_pos": 72, "end_pos": 84, "type": "METRIC", "confidence": 0.9524515271186829}]}, {"text": "It is worth noting that the Rand-binary and PCAbinary model variants also show competitive performance despite their simplicity.", "labels": [], "entities": []}, {"text": "These strategies are also quite promising given that no training is required given the pre-trained continuous sentence representations.", "labels": [], "entities": []}, {"text": "Another important result is that, the AE-binary-SP achieves competitive results relative to the InferSent, leading to only about 2% loss on most datasets and even performing at par with InferSent on several datasets, such as the MPQA and STS14 datasets.", "labels": [], "entities": [{"text": "AE-binary-SP", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.8966779708862305}, {"text": "MPQA and STS14 datasets", "start_pos": 229, "end_pos": 252, "type": "DATASET", "confidence": 0.7219615504145622}]}, {"text": "On the sentence matching tasks, the yielded binary codes are evaluated by merely utilizing the hamming distance features (as mentioned above).", "labels": [], "entities": [{"text": "sentence matching", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.755455493927002}]}, {"text": "To allow fair comparison, we compare the predicted scores with the cosine similarity scores based upon the continuous representations (there are no additional parameters for the classifier).", "labels": [], "entities": []}, {"text": "The binary codes brings out promising empirical results relative to their continuous counterparts, and even slightly outperform InferSent on the STS14 dataset.", "labels": [], "entities": [{"text": "STS14 dataset", "start_pos": 145, "end_pos": 158, "type": "DATASET", "confidence": 0.9143613874912262}]}, {"text": "We also found that our AE-binary-SP model variant consistently demonstrate superior results than the InferLite baselines, which optimize the NLI objective directly over the binary representations.", "labels": [], "entities": [{"text": "InferLite baselines", "start_pos": 101, "end_pos": 120, "type": "DATASET", "confidence": 0.8555803894996643}]}, {"text": "This maybe attributed to the difficulty of backpropagating gradients through discrete/binary variables, and would bean interesting direction for future research.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Nearest neighbor retrieval results on the SNLI dataset. Given a a query sentence, the left column shows  the top-3 retrieved samples based upon the hamming distance with all sentences' binary representations, while the  right column exhibits the samples according to the cosine similarity of their continuous embeddings.", "labels": [], "entities": [{"text": "Nearest neighbor retrieval", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.5822246174017588}, {"text": "SNLI dataset", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.7922357320785522}]}, {"text": " Table 3: Ablation study for the AE-binary-SP model  with different choices of \u03bb sp (evaluated with test accu- racy on the MR dataset).", "labels": [], "entities": [{"text": "AE-binary-SP", "start_pos": 33, "end_pos": 45, "type": "METRIC", "confidence": 0.8655825257301331}, {"text": "MR dataset", "start_pos": 123, "end_pos": 133, "type": "DATASET", "confidence": 0.9320206344127655}]}]}