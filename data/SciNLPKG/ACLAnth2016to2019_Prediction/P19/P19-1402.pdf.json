{"title": [{"text": "Few-Shot Representation Learning for Out-Of-Vocabulary Words", "labels": [], "entities": []}], "abstractContent": [{"text": "Existing approaches for learning word embed-dings often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts.", "labels": [], "entities": []}, {"text": "However, in real-world scenarios, out-of-vocabulary (a.k.a.", "labels": [], "entities": []}, {"text": "OOV) words that do not appear in training corpus emerge frequently.", "labels": [], "entities": []}, {"text": "It is challenging to learn accurate representations of these words with only a few observations.", "labels": [], "entities": []}, {"text": "In this paper , we formulate the learning of OOV em-beddings as a few-shot regression problem, and address it by training a representation function to predict the oracle embedding vector (defined as embedding trained with abundant observations) based on limited observations.", "labels": [], "entities": []}, {"text": "Specifically, we propose a novel hierarchical attention-based architecture to serve as the neural regression function, with which the context information of a word is encoded and aggregated from K observations.", "labels": [], "entities": []}, {"text": "Furthermore , our approach can leverage Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed approach significantly outperforms existing methods in constructing accurate em-beddings for OOV words, and improves downstream tasks where these embeddings are utilized .", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional word embedding models aim to assign each word a low-dimensional vector representing its semantic meaning.", "labels": [], "entities": []}, {"text": "These embedding models have been used as key components in natural language processing systems.", "labels": [], "entities": []}, {"text": "To learn such embeddings, existing approaches such as skip-gram models () resort to an auxiliary task of predicting the context words (words surround the target word).", "labels": [], "entities": []}, {"text": "These embeddings have shown to be able to capture syntactic and semantic relations between words.", "labels": [], "entities": []}, {"text": "Despite the success, an essential issue arises: most existing embedding techniques assume the availability of abundant observations of each word in the training corpus.", "labels": [], "entities": []}, {"text": "When a word occurs only a few times during training (i.e., in the few-shot setting), the corresponding embedding vector is not accurate ().", "labels": [], "entities": []}, {"text": "In the extreme case, some words are not observed when training the embedding, which are known as out-ofvocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "These words are often rare and might only occurred fora few times in the testing corpus.", "labels": [], "entities": []}, {"text": "Therefore, the insufficient observations hinder the existing context-based word embedding models to infer accurate OOV embeddings.", "labels": [], "entities": []}, {"text": "This leads us to the following research problem: How can we learn accurate embedding vectors for OOV words during the inference time by observing their usages for only a few times?", "labels": [], "entities": []}, {"text": "Existing approaches for dealing with OOV words can be categorized into two groups.", "labels": [], "entities": []}, {"text": "The first group of methods derives embedding vectors of OOV words based on their morphological information (.", "labels": [], "entities": []}, {"text": "This type of approaches has a limitation when the meaning of words cannot be inferred from its subunits (e.g., names, such as Vladimir).", "labels": [], "entities": []}, {"text": "The second group of approaches attempts to learn to embed an OOV word from a few examples.", "labels": [], "entities": []}, {"text": "Ina prior study, these demonstrating examples are treated as a small corpus and are used to fine-tune OOV embeddings.", "labels": [], "entities": []}, {"text": "Unfortunately, fine-tuning with just a few examples usually leads to overfitting.", "labels": [], "entities": []}, {"text": "In another work (, a simple linear function is used to infer embedding of an OOV word by aggregating embeddings of its context words in the examples.", "labels": [], "entities": []}, {"text": "However, the simple linear averaging can fail to capture the complex semantics and relationships of an OOV word from its contexts.", "labels": [], "entities": []}, {"text": "Unlike the existing approaches mentioned above, humans have the ability to infer the meaning of a word based on a more comprehensive understanding of its contexts and morphology.", "labels": [], "entities": []}, {"text": "Given an OOV word with a few example sentences, humans are capable of understanding the semantics of each sentence, and then aggregating multiple sentences to estimate the meaning of this word.", "labels": [], "entities": []}, {"text": "In addition, humans can combine the context information with sub-word or other morphological forms to have a better estimation of the target word.", "labels": [], "entities": []}, {"text": "Inspired by this, we propose an attentionbased hierarchical context encoder (HiCE), which can leverage both sentence examples and morphological information.", "labels": [], "entities": []}, {"text": "Specifically, the proposed model adopts multi-head self-attention to integrate information extracted from multiple contexts, and the morphological information can be easily integrated through a character-level CNN encoder.", "labels": [], "entities": []}, {"text": "In order to train HiCE to effectively predict the embedding of an OOV word from just a few examples, we introduce an episode based few-shot learning framework.", "labels": [], "entities": []}, {"text": "In each episode, we suppose a word with abundant observations is actually an OOV word, and we use the embedding trained with these observations as its oracle embedding.", "labels": [], "entities": []}, {"text": "Then, the HiCE model is asked to predict the word's oracle embedding using only the word's K randomly sampled observations as well as its morphological information.", "labels": [], "entities": []}, {"text": "This training scheme can simulate the real scenarios where OOV words occur during inference, while in our case we have access to their oracle embeddings as the learning target.", "labels": [], "entities": []}, {"text": "Furthermore, OOV words may occur in anew corpus whose domain or linguistic usages are different from the main training corpus.", "labels": [], "entities": []}, {"text": "To deal with this issue, we propose to adopt Model-Agnostic Meta-Learning (MAML) () to assist the fast and robust adaptation of a pre-trained HiCE model, which allows HiCE to better infer the embeddings of OOV words in anew domain by starting from a promising initialization.", "labels": [], "entities": []}, {"text": "We conduct comprehensive experiments based on both intrinsic and extrinsic embedding evaluation.", "labels": [], "entities": []}, {"text": "Experiments of intrinsic evaluation on the Chimera benchmark dataset demonstrate that the proposed method, HiCE, can effectively utilize context information and outperform baseline algorithms.", "labels": [], "entities": [{"text": "Chimera benchmark dataset", "start_pos": 43, "end_pos": 68, "type": "DATASET", "confidence": 0.8274385730425516}]}, {"text": "For example, HiCE achieves 9.3% relative improvement in terms of Spearman correlation compared to the state-of-the-art approach, ` a la carte, regarding 6-shot learning case.", "labels": [], "entities": [{"text": "HiCE", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.8293827772140503}, {"text": "Spearman correlation", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.8000022768974304}]}, {"text": "Furthermore, with experiments on extrinsic evaluation, we show that our proposed method can benefit downstream tasks, such as named entity recognition and part-of-speech tagging, and outperform existing methods significantly.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.6189103523890177}, {"text": "part-of-speech tagging", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.7027332782745361}]}, {"text": "The contributions of this work can be summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 We formulate the OOV word embedding learning as a K-shot regression problem and propose a simulated episode-based training schema to predict oracle embeddings.", "labels": [], "entities": [{"text": "OOV word embedding learning", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.607778288424015}]}, {"text": "\u2022 We propose an attention-based hierarchical context encoder (HiCE) to encode and aggregate both context and sub-word information.", "labels": [], "entities": []}, {"text": "We further incorporate MAML for fast adapting the learned model to the new corpus by bridging the semantic gap.", "labels": [], "entities": []}, {"text": "\u2022 We conduct experiments on multiple tasks, and through quantitative and qualitative analysis, we demonstrate the effectiveness of the proposed method in fast representation learning of OOV words for down-stream tasks.", "labels": [], "entities": [{"text": "fast representation learning of OOV words", "start_pos": 154, "end_pos": 195, "type": "TASK", "confidence": 0.7369075020154318}]}], "datasetContent": [{"text": "In this section, we present two types of experiments to evaluate the effectiveness of the proposed HiCE model.", "labels": [], "entities": []}, {"text": "One is an intrinsic evaluation on a benchmark dataset, and the other is an extrinsic evaluation on two downstream tasks: (1) named entity recognition and (2) part-of-speech tagging.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.6152381797631582}, {"text": "part-of-speech tagging", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.731385737657547}]}, {"text": "As aforementioned, our approach assumes an initial embedding T trained on an existing corpus D T . As all the baseline models learn embedding from Wikipedia, we train HiCE on WikiText-103 () with the initial embedding provided by Herbelot and Baroni (2017) . WikiText-103 contains 103 million words extracted from a selected set of articles.", "labels": [], "entities": []}, {"text": "From WikiText-103, we select words with an occurrence count larger than 16 as training words.", "labels": [], "entities": []}, {"text": "Then, we collect the masked supporting contexts set St for each training word wt with its oracle embedding T wt , and split the collected words into a training set and a validation set.", "labels": [], "entities": []}, {"text": "We then train the HiCE model 2 in the previous introduced episode based K-shot learning setting, and select the best hyperparameters and model using the validation set.", "labels": [], "entities": []}, {"text": "After we obtain the trained HiCE model, we can either directly use it to infer the embedding vectors for OOV words in new corpus D N , or conduct adaptation on D N using MAML algorithm as shown in Eq.", "labels": [], "entities": []}, {"text": "First, we evaluate HiCE on Chimera (Lazaridou et al., 2017), a widely used benchmark dataset for evaluating word embedding for OOV words.", "labels": [], "entities": [{"text": "HiCE", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.9459646344184875}]}, {"text": "Dataset The Chimera dataset simulates the situation when an embedding model faces an OOV word in a real-world application.", "labels": [], "entities": []}, {"text": "For each OOV word (denoted as \"chimera\"), a few example sentences (2, 4, or 6) are provided.", "labels": [], "entities": []}, {"text": "The dataset also provides a set of probing words and the humanannotated similarity between the probing words and the OOV words.", "labels": [], "entities": []}, {"text": "To evaluate the performance of a learned embedding, Spearman correlation is used in ( to measure the agreement between the human annotations and the machine-generated results.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 52, "end_pos": 72, "type": "METRIC", "confidence": 0.649325892329216}]}, {"text": "Experimental Results lists the performance of HiCE and baselines with different numbers of context sentences.", "labels": [], "entities": [{"text": "HiCE", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.8093323111534119}]}, {"text": "In particular, our method (HiCE+Morph+MAML) 6 achieves the best performance among all the other baseline methods undermost settings.", "labels": [], "entities": []}, {"text": "Compared with the current state-of-the-art method, ` a la carte, the relative improvements (i.e., the performance difference divided by the baseline performance) of HiCE are 4.0%, 5.4% and 9.3% in terms of 2,4,6-shot learning, respectively.", "labels": [], "entities": [{"text": "HiCE", "start_pos": 165, "end_pos": 169, "type": "TASK", "confidence": 0.8803085684776306}]}, {"text": "We also compare our results with that of the oracle embedding, which is the embeddings trained from D T , and used as ground-truth to train HiCE.", "labels": [], "entities": [{"text": "HiCE", "start_pos": 140, "end_pos": 144, "type": "TASK", "confidence": 0.8421429395675659}]}, {"text": "This results can be regarded as an upper bound.", "labels": [], "entities": []}, {"text": "As is shown, when the number of context sentences (K) is relatively large (i.e., K = 6), the performance of HiCE is on a par with the upper bound (Oracle Embedding) and the relative performance difference is merely 2.7%.", "labels": [], "entities": []}, {"text": "This indicates the significance of using an advanced aggregation model.", "labels": [], "entities": []}, {"text": "Furthermore, we conduct an ablation study to analyze the effect of morphological features.", "labels": [], "entities": []}, {"text": "By comparing HiCE with and without Morph, we can see that morphological features are helpful when the number of context sentences is relatively small (i.e., 2 and 4 shot).", "labels": [], "entities": []}, {"text": "This is because morphological information does not rely on context sen-tences, and can give a good estimation when contexts are limited.", "labels": [], "entities": []}, {"text": "However, in 6-shot setting, their performance does not differ significantly.", "labels": [], "entities": []}, {"text": "In addition, we analyze the effect of MAML by comparing HiCE with and without MAML.", "labels": [], "entities": []}, {"text": "We can see that adapting with MAML can improve the performance when the number of context sentences is relatively large (i.e., 4 and 6 shot), as it can mitigate the semantic gap between source corpus D T and target corpus D N , which makes the model better capture the context semantics in the target corpus.", "labels": [], "entities": []}, {"text": "Also we evaluate the effect of MAML by comparing it with fine-tuning.", "labels": [], "entities": [{"text": "MAML", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.5305652618408203}]}, {"text": "The results show that directly fine-tuning on target corpus can lead to extremely bad performance, due to the insufficiency of data.", "labels": [], "entities": []}, {"text": "On the contrary, adapting with MAML can leverage the source corpus's information as regularization to avoid over-fitting.", "labels": [], "entities": []}, {"text": "To illustrate the effectiveness of our proposed method in dealing with OOV words, we evaluate the resulted embedding on two downstream tasks: (1) named entity recognition (NER) and (2) partof-speech (POS) tagging.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 146, "end_pos": 176, "type": "TASK", "confidence": 0.7594932268063227}, {"text": "partof-speech (POS) tagging", "start_pos": 185, "end_pos": 212, "type": "TASK", "confidence": 0.5995152831077576}]}, {"text": "Named Entity Recognition NER is a semantic task with a goal to extract named entities from a sentence.", "labels": [], "entities": [{"text": "Named Entity Recognition NER", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6859968230128288}]}, {"text": "Recent approaches for NER take word embedding as input and leverage its semantic information to annotate named entities.", "labels": [], "entities": [{"text": "NER", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9631749987602234}]}, {"text": "Therefore, a high-quality word embedding has a great impact on the NER system.", "labels": [], "entities": [{"text": "NER", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.7528066635131836}]}, {"text": "We consider the following two corpora, which contain abundant OOV words, to mimic the real situation of OOV problems.", "labels": [], "entities": []}, {"text": "\u2022 Rare-NER: This NER dataset) focus on unusual, previouslyunseen entities in the context of emerging discussions, which are mostly OOV words.", "labels": [], "entities": [{"text": "NER dataset", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.7544789910316467}]}, {"text": "\u2022 Bio-NER: The JNLPBA 2004 Bio-entity recognition dataset () focuses on technical terms in the biology domain, which also contain many OOV words.", "labels": [], "entities": [{"text": "JNLPBA 2004 Bio-entity recognition dataset", "start_pos": 15, "end_pos": 57, "type": "DATASET", "confidence": 0.8770495772361755}]}, {"text": "Both datasets use entity-level F1-score as an evaluation metric.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9685729742050171}]}, {"text": "We use the WikiText-103 as D T , and these datasets as D N . We select all the OOV words in the dataset and extract their context sentences.", "labels": [], "entities": [{"text": "WikiText-103", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.944069504737854}]}, {"text": "Then, we train different versions of OOV embeddigns based on the proposed approaches and the baseline models.", "labels": [], "entities": []}, {"text": "Finally, the inferred embedding is used in an NER system based on the  To illustrate how does HiCE extract and aggregate information from multiple context sentences, we visualize the attention weights over words and contexts.", "labels": [], "entities": []}, {"text": "We demonstrate an example in, where we choose four sentences in chimera dataset, with \"clarinet\" (a woodwind instrument) as the OOV word.", "labels": [], "entities": []}, {"text": "From the attention weight over words, we can see that the HiCE puts high attention on words that are related to instruments, such as \"horns\", \"instruments\", \"flows\", etc.", "labels": [], "entities": []}, {"text": "From the attention weight over contexts, we can see that HiCE assigns the fourth sentence the lowest context attention, in which the instrumentrelated word \"trumpet\" is distant from the target", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on the Chimera benchmark  dataset with different numbers of context sentences,  which is measured by Spearman correlation. Baseline  results are from the corresponding papers.", "labels": [], "entities": [{"text": "Chimera benchmark  dataset", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.7553064028422037}, {"text": "Spearman correlation", "start_pos": 123, "end_pos": 143, "type": "METRIC", "confidence": 0.6348584890365601}]}, {"text": " Table 2: Performance on Named Entity Recognition and Part-of-Speech Tagging tasks. All methods are evaluated  on test data containing OOV words. Results demonstrate that the proposed approach, HiCE + Morph + MAML,  improves the downstream model by learning better representations for OOV words.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.671233206987381}, {"text": "Part-of-Speech Tagging tasks", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.7947026689847311}]}]}