{"title": [{"text": "Joint Entity Extraction and Assertion Detection for Clinical Text", "labels": [], "entities": [{"text": "Joint Entity Extraction and Assertion Detection", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7209365864594778}, {"text": "Clinical Text", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.6993880271911621}]}], "abstractContent": [{"text": "Negative medical findings are prevalent in clinical reports, yet discriminating them from positive findings remains a challenging task for information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.8399319648742676}]}, {"text": "Most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (NER) and rule-based negation detection.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.7889250616232554}, {"text": "rule-based negation detection", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.6400611102581024}]}, {"text": "We consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations.", "labels": [], "entities": []}, {"text": "We extend a standard hierarchical encoder-decoder NER model and first adopt a shared encoder followed by separate decoders for the two tasks.", "labels": [], "entities": []}, {"text": "This architecture performs considerably better than the previous rule-based and machine learning-based systems.", "labels": [], "entities": []}, {"text": "To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset.", "labels": [], "entities": [{"text": "NER", "start_pos": 193, "end_pos": 196, "type": "TASK", "confidence": 0.9749050140380859}, {"text": "negation detection", "start_pos": 201, "end_pos": 219, "type": "TASK", "confidence": 0.9318976998329163}, {"text": "2010 i2b2/VA challenge dataset", "start_pos": 227, "end_pos": 257, "type": "DATASET", "confidence": 0.6928292512893677}]}], "introductionContent": [{"text": "In recent years, natural language processing (NLP) techniques have demonstrated increasing effectiveness in clinical text mining.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.7984608511130015}, {"text": "clinical text mining", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.5982366402943929}]}, {"text": "Electronic health record (EHR) narratives, e.g., discharge summaries and progress notes contain a wealth of medically relevant information such as diagnosis information and adverse drug events.", "labels": [], "entities": []}, {"text": "Automatic extraction of such information and representation of clinical knowledge in standardized formats could be employed fora variety of purposes such as clinical event surveillance, decision support (, pharmacovigilance, and drug efficacy studies.", "labels": [], "entities": [{"text": "clinical event surveillance", "start_pos": 157, "end_pos": 184, "type": "TASK", "confidence": 0.6606500645478567}, {"text": "decision support", "start_pos": 186, "end_pos": 202, "type": "TASK", "confidence": 0.857335776090622}]}, {"text": "Although many NLP applications that successfully extract findings from medical reports have Discontinue Abraxane, patient denies taking Tyleno 325 mg and is not taking calcium carbonate.", "labels": [], "entities": [{"text": "Discontinue", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.8480677008628845}, {"text": "Abraxane", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9045366048812866}]}, {"text": "Patient also stopped taking colecalciferol 1,000 units PO.", "labels": [], "entities": []}, {"text": "been developed in recent years, identifying assertions such as positive (present), negative (absent), and hypothetical remains a challenging task, especially to generalize ().", "labels": [], "entities": []}, {"text": "However, identifying assertions is critical since negative and uncertain findings are frequent in clinical notes, and information extraction algorithms that do not distinguish between them will not paint a clear picture of the patient.", "labels": [], "entities": [{"text": "identifying assertions", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.8970742225646973}, {"text": "information extraction", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.7623674869537354}]}, {"text": "In this paper, we focus on identifying the negated findings in a multi-task setting ).", "labels": [], "entities": []}, {"text": "Most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (NER) and negation detection.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.7887633889913559}, {"text": "negation detection", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.977159172296524}]}, {"text": "Previous efforts in this area include both rule-based and machine-learning approaches.", "labels": [], "entities": []}, {"text": "Rule-based systems rely on negation keywords and rules to determine the cue of negation.) is a widely used algorithm that consists of ontology lookup to index findings, and negation regular expression search in a fixed scope.", "labels": [], "entities": [{"text": "negation regular expression search", "start_pos": 173, "end_pos": 207, "type": "TASK", "confidence": 0.8826520889997482}]}, {"text": "ConText () extends NegEx to other attributes like hypothetical and make scope variable by searching fora termination term.", "labels": [], "entities": []}, {"text": "NegBio () uses a universal dependency graph for scope detection.", "labels": [], "entities": [{"text": "NegBio", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.838394045829773}, {"text": "scope detection", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.8662721514701843}]}, {"text": "Another similar work by utilizes a constituency-based parse tree to prune out the parts outside the scope.", "labels": [], "entities": []}, {"text": "However, these approaches use rules and regular expressions for cue detection which rely solely on surface text and thus are limited when attempting to capture complex syntactic constructions such as long noun phrases.", "labels": [], "entities": [{"text": "cue detection", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.7964689135551453}]}, {"text": "Kernel-based approaches are also very common, especially in the 2010 i2b2/VA task of predicting assertions.", "labels": [], "entities": [{"text": "predicting assertions", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7900396883487701}]}, {"text": "The state-of-the-art in that challenge applies support vector machines (SVM) to assertion prediction as a separate step after concept extraction).", "labels": [], "entities": [{"text": "assertion prediction", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.83452308177948}]}, {"text": "They train classifiers to predict assertions of each concept word, and a separate classifier to predict the assertion of the whole concept.", "labels": [], "entities": []}, {"text": "propose an Augmented Bag of Words Kernel (ABoW), which generates features based on NegEx rules along with bag-of-words features.", "labels": [], "entities": []}, {"text": "use CRF for classification of cues and scope detection.", "labels": [], "entities": [{"text": "scope detection", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8449968993663788}]}, {"text": "These machine learning based approaches often suffer in generalizability, the ability to perform well on unseen text.", "labels": [], "entities": []}, {"text": "Recently, neural network models by and have been proposed.", "labels": [], "entities": []}, {"text": "Most relevant to our work is that of where gated recurrent units (GRU) are used to represent the clinical events and their context, along with an attention mechanism.", "labels": [], "entities": []}, {"text": "Given a text annotated with events, it classifies the presence and period of the events.", "labels": [], "entities": []}, {"text": "However, this approach is not end-to-end as it does not predict the events.", "labels": [], "entities": []}, {"text": "Additionally, these models generally require large annotated corpus, which is necessary for good performance.", "labels": [], "entities": []}, {"text": "Unfortunately, such clinical text data is not easily available.", "labels": [], "entities": []}, {"text": "Multi-task learning (MTL) is one of the most effective solutions for knowledge transfer across tasks.", "labels": [], "entities": [{"text": "Multi-task learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8601149916648865}, {"text": "knowledge transfer", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7268453985452652}]}, {"text": "In the context of neural network architectures, we perform MTL by sharing parameters across models, such as pretraining using word embeddings (), a popular approach for most NLP tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9855314493179321}]}, {"text": "In this paper, we propose an MTL approach to negation detection that overcomes some of the limitations in the existing models such as data accessibility.", "labels": [], "entities": [{"text": "MTL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9677332639694214}, {"text": "negation detection", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.985956072807312}]}, {"text": "MTL leverages overlapping representation across sub-tasks and it is one of the most effective solutions for knowledge transfer across tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.907607913017273}, {"text": "knowledge transfer", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7056452035903931}]}, {"text": "In the context of neural network architectures, we perform MTL by sharing parameters across tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9833312630653381}]}, {"text": "To the best of our knowledge, this is the first work to jointly model named entity and negation in an end-to-end system.", "labels": [], "entities": []}, {"text": "Our main contributions are summarized below: \u2022 An end-to-end hierarchical neural model consisting of a shared encoder and different decoding schemes to jointly extract entities and negations.", "labels": [], "entities": []}, {"text": "Using our proposed model, we obtain substantial improvement over prior models for both entities and negations on the 2010 i2b2/VA challenge task as well as a proprietary de-identified clinical note dataset for medical conditions.", "labels": [], "entities": [{"text": "2010 i2b2/VA challenge task", "start_pos": 117, "end_pos": 144, "type": "DATASET", "confidence": 0.6175286173820496}]}, {"text": "\u2022 A Conditional softmax shared decoder model to overcome low resource settings (datasets with limited amounts of training data), which achieves state of art results across different corpora.", "labels": [], "entities": []}, {"text": "\u2022 A thorough empirical analysis of parameter sharings for low resource setting highlighting the significance of the shared decoder.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our model on two datasets.", "labels": [], "entities": []}, {"text": "First is the 2010 i2b2/VA challenge dataset for \"test, treatment, problem\" (TTP) entity extraction and assertion detection (i2b2 dataset).", "labels": [], "entities": [{"text": "VA challenge dataset", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.7177620430787405}, {"text": "test, treatment, problem\" (TTP) entity extraction and assertion detection", "start_pos": 49, "end_pos": 122, "type": "TASK", "confidence": 0.6026892853634698}]}, {"text": "Unfortunately, only part of this dataset was made public after the challenge, therefore we cannot directly compare with NegEx and ABoW results.", "labels": [], "entities": [{"text": "NegEx", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.8551948666572571}, {"text": "ABoW", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.710640013217926}]}, {"text": "We followed the original data split from R.", "labels": [], "entities": []}, {"text": "Chalapathy and Piccardi (2016) of 170 notes for training and 256 for testing.", "labels": [], "entities": []}, {"text": "The second dataset is proprietary and consists of 4,200 de-identified, annotated clinical notes with medical conditions (proprietary dataset).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set performance during multi-task training. The table displays precision, recall and macro averaged F1. The", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9997031092643738}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9992699027061462}, {"text": "macro averaged", "start_pos": 100, "end_pos": 114, "type": "METRIC", "confidence": 0.9119630455970764}, {"text": "F1", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.5031973123550415}]}]}