{"title": [{"text": "Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7134695649147034}]}], "abstractContent": [{"text": "This work presents our ongoing research of unsupervised pretraining in neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 71, "end_pos": 103, "type": "TASK", "confidence": 0.8221680025259653}]}, {"text": "In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation (EWC) to avoid forgetting of the original language modeling tasks.", "labels": [], "entities": []}, {"text": "We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives.", "labels": [], "entities": [{"text": "regularization", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.9708107709884644}, {"text": "EWC", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9190863966941833}]}, {"text": "The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9992955923080444}]}, {"text": "However, the model converges 2-3 times faster and does not require the original unlabeled training data during the fine-tuning stage.", "labels": [], "entities": []}, {"text": "In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related.", "labels": [], "entities": [{"text": "regularization", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9469985365867615}]}, {"text": "We show that initializing the bidirectional NMT encoder with a left-to-right language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context .", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) using sequence to sequence architectures () has become the dominant approach to automatic machine translation.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8342729210853577}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.713229775428772}]}, {"text": "While being able to approach human-level performance, it still requires a huge amount of parallel data, otherwise it can easily overfit.", "labels": [], "entities": []}, {"text": "Such data, however, might not always be available.", "labels": [], "entities": []}, {"text": "At the same time, it is generally much easier to gather large amounts of monolingual data, and therefore, it is interesting to find ways of making use of such data.", "labels": [], "entities": []}, {"text": "The simplest strategy is to use backtranslation (), but it can be rather costly since it requires training a model in the opposite translation direction and then translating the monolingual corpus.", "labels": [], "entities": []}, {"text": "It was suggested by that during the development of a general human-like AI system, one of the desired characteristics of such a system is the ability to learn in a continuous manner using previously learned tasks as building blocks for mastering new, more complex tasks.", "labels": [], "entities": []}, {"text": "Until recently, continuous learning of neural networks was problematic, among others, due to the catastrophic forgetting.", "labels": [], "entities": [{"text": "forgetting", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9554820656776428}]}, {"text": "Several methods were proposed (, however, they mainly focus only on adapting the whole network (not just its parts) to new tasks while maintaining good performance on the previously learned tasks.", "labels": [], "entities": []}, {"text": "In this work, we present an unsupervised pretraining method for NMT models using Elastic Weight Consolidation (.", "labels": [], "entities": []}, {"text": "First, we initialize both encoder and decoder with source and target language models respectively.", "labels": [], "entities": []}, {"text": "Then, we fine-tune the NMT model using the parallel data.", "labels": [], "entities": []}, {"text": "To prevent the encoder and decoder from forgetting the original language modeling (LM) task, we regularize their weights individually using Elastic Weight Consolidation based on their importance to that task.", "labels": [], "entities": []}, {"text": "Our hypothesis is the following: by forcing the network to remember the original LM tasks we can reduce overfitting of the NMT model on the limited parallel data.", "labels": [], "entities": []}, {"text": "We also provide a comparison of our approach with the method proposed by.", "labels": [], "entities": []}, {"text": "They also suggest initialization of the encoder and decoder with a language model.", "labels": [], "entities": []}, {"text": "However, during the fine-tuning phase they use the original language modeling objectives as an additional training loss in place of model regular-ization.", "labels": [], "entities": []}, {"text": "Their approach has two main drawbacks: first, during the fine-tuning phase, they still require the original monolingual data which might not be available anymore in a life-long learning scenario.", "labels": [], "entities": []}, {"text": "Second, they need to compute both machine translation and language modeling losses which increases the number of operations performed during the update slowing down the fine-tuning process.", "labels": [], "entities": [{"text": "machine translation and language modeling", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.6587881922721863}]}, {"text": "Our proposed method addresses both problems: it requires only a small held-out set to estimate the EWC regularization term and converges 2-3 times faster than the previous method.", "labels": [], "entities": [{"text": "EWC regularization", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.553302526473999}]}], "datasetContent": [{"text": "In this section, we present the results of our experiments with EWC regularization and compare them with the previously proposed regularization by language modeling objectives.", "labels": [], "entities": [{"text": "EWC regularization", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.662963330745697}, {"text": "regularization by language modeling", "start_pos": 129, "end_pos": 164, "type": "TASK", "confidence": 0.825555607676506}]}, {"text": "In our experiments, we focused on the lowresource Basque-to-English machine translation task featured in IWSLT 2018.", "labels": [], "entities": [{"text": "lowresource Basque-to-English machine translation task", "start_pos": 38, "end_pos": 92, "type": "TASK", "confidence": 0.6381595075130463}, {"text": "IWSLT 2018", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.8670097589492798}]}, {"text": "We used the parallel data provided by IWSLT organizers, consisting of 5,600 in-domain sentence pairs (TED Talks) and around 940,000 general-domain sentence pairs.", "labels": [], "entities": [{"text": "IWSLT organizers", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8113170266151428}]}, {"text": "During pretraining, we used Basque Wikipedia for source language model and News-: Comparison of the previous work (LM) with the proposed method (EWC).", "labels": [], "entities": [{"text": "Basque Wikipedia", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.8374176323413849}]}, {"text": "We compared models with only pretrained encoder (SRC), pretrained decoder (TGT) and both (ALL).", "labels": [], "entities": []}, {"text": "All pretrained language models contained 3 layers.", "labels": [], "entities": []}, {"text": "We compared both single best models and ensemble (using checkpoint averaging) of 4 best checkpoints.", "labels": [], "entities": []}, {"text": "Results where the proposed method outperformed the previous work are in bold.", "labels": [], "entities": []}, {"text": "Commentary 2015 provided by WMT 5 for target language model.", "labels": [], "entities": [{"text": "WMT 5", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.9332149922847748}]}, {"text": "Both corpora contain close to 3 million sentences.", "labels": [], "entities": []}, {"text": "We used UDPipe 6 to split the monolingual data to sentences and SentencePiece 7 to prepare the subword tokenization.", "labels": [], "entities": []}, {"text": "We used the subword models trained on the monolingual data to preprocess the parallel data.", "labels": [], "entities": []}, {"text": "During training, we used development data provided by IWSLT 2018 organizers which contains 1,140 parallel sentences.", "labels": [], "entities": [{"text": "IWSLT 2018 organizers", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.9202823638916016}]}, {"text": "To approximate the Fisher Information Matrix diagonal of the pretrained Basque and English language models, we used the respective parts of the IWSLT validation set.", "labels": [], "entities": [{"text": "IWSLT validation set", "start_pos": 144, "end_pos": 164, "type": "DATASET", "confidence": 0.8855859041213989}]}, {"text": "For final evaluation, we used the IWSLT 2018 test data consisting of 1051 sentence pairs.", "labels": [], "entities": [{"text": "IWSLT 2018 test data", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.9555609226226807}]}, {"text": "compares the performance of the models fine-tuned using the LM objective regularization and the EWC regularization.", "labels": [], "entities": [{"text": "EWC", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.8026207685470581}]}, {"text": "First, we can see that using EWC when only the decoder was pretrained slightly outperforms the previous work.", "labels": [], "entities": [{"text": "EWC", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.8532437682151794}]}, {"text": "On the other hand, our method fails when used in combination with the encoder initialization by the source language model.", "labels": [], "entities": []}, {"text": "The reason might be a difference between the original LM task that is trained in a left-to-right autoregressive fashion while the strength of the Transformer encoder is in modelling of the whole left-and-right context for each source token.", "labels": [], "entities": []}, {"text": "The learning capacity of the encoder is therefore restricted by forcing it to remember a task that is not so closely related to the sentence encoding in Transformer NMT.", "labels": [], "entities": []}, {"text": "supports our claim: the deeper the pretrained language model and therefore more layers regularized by EWC, the lower the performance of the finetuned NMT system.", "labels": [], "entities": [{"text": "EWC", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9134121537208557}]}, {"text": "We think that this behaviour can be mitigated by initializing the encoder with a language model that considers the whole bidirectional context, e.g. a recently introduced BERT encoder).", "labels": [], "entities": [{"text": "BERT encoder", "start_pos": 171, "end_pos": 183, "type": "DATASET", "confidence": 0.7372235059738159}]}, {"text": "We leave this for our future work.", "labels": [], "entities": []}, {"text": "In addition to improving model performance, EWC converges much faster than the previously introduced LM regularizer.", "labels": [], "entities": [{"text": "EWC", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.7520459294319153}]}, {"text": "shows that the model fine-tuned without LM regularization converged in about 10 hours, while it took around 20-30 hours to converge when LM regularization was in place.", "labels": [], "entities": []}, {"text": "Note, that all models converged after seeing a similar number of training examples, however, computing the LM loss for regularization introduces an additional computation overhead.", "labels": [], "entities": []}, {"text": "The main benefit of both EWC and LMbased regularization is apparent here, too.", "labels": [], "entities": [{"text": "EWC", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.806157112121582}, {"text": "LMbased regularization", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.4669058173894882}]}, {"text": "The non-regularized model quickly overfits.", "labels": [], "entities": []}, {"text": "As the comparison to the model trained on the backtranslated monolingual corpus shows, none of our regularization methods can match this simple but much more computationally demanding benchmark.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of the previous work (LM) with  the proposed method (EWC). We compared models  with only pretrained encoder (SRC), pretrained de- coder (TGT) and both (ALL). All pretrained language  models contained 3 layers. We compared both single  best models and ensemble (using checkpoint averag- ing) of 4 best checkpoints. Results where the proposed  method outperformed the previous work are in bold.", "labels": [], "entities": []}]}