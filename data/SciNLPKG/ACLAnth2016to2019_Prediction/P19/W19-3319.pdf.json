{"title": [{"text": "Semantically Constrained Multilayer Annotation: The Case of Coreference", "labels": [], "entities": [{"text": "Coreference", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9223963618278503}]}], "abstractContent": [{"text": "We propose a coreference annotation scheme as a layer on top of the Universal Conceptual Cognitive Annotation foundational layer, treating units in predicate-argument structure as a basis for entity and event mentions.", "labels": [], "entities": []}, {"text": "We argue that this allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how.", "labels": [], "entities": [{"text": "coreference annotators", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9310964643955231}]}, {"text": "The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unlike some NLP tasks, coreference resolution lacks an agreed-upon standard for annotation and evaluation (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9831960797309875}]}, {"text": "It has been approached using a multitude of different markup schemas, and the several evaluation metrics commonly used) are controversial (.", "labels": [], "entities": []}, {"text": "In particular, these schemas use divergent and often (languagespecific) syntactic criteria for defining candidate mentions in text.", "labels": [], "entities": []}, {"text": "This includes the questions of whether to annotate entity and/or event coreference, whether to include singletons, and how to identify the precise span of complex mentions.", "labels": [], "entities": []}, {"text": "Recognition of this limitation in the field has recently prompted the Universal Coreference initiative, 1 which aims to settle on a single cross-linguistically applicable annotation standard.", "labels": [], "entities": []}, {"text": "We think that many issues stem from the common practice of creating mention annotations from scratch on the raw or tokenized text, and we suggest that they could be overcome by reusing structures from existing semantic annotation, thereby ensuring compatibility between the layers.", "labels": [], "entities": []}, {"text": "We advocate * Contact: jakob@cs.georgetown.edu 1 https://sites.google.com/view/crac2019/ for the design pattern of a semantic foundational layer, which defines a basic semantic structure that additional layers can refine or make reference to.", "labels": [], "entities": []}, {"text": "Some form of predicate-argument structure involving entities and propositions should serve as a natural semantic foundation fora layer that groups coreferring entity and event mentions into clusters.", "labels": [], "entities": []}, {"text": "Here we argue that Universal Conceptual Cognitive Annotation (UCCA;) is an ideal choice, as it defines a foundational layer of predicate-argument structure whose main design principles are cross-linguistic applicability and fast annotatability by non-experts.", "labels": [], "entities": [{"text": "Universal Conceptual Cognitive Annotation (UCCA", "start_pos": 19, "end_pos": 66, "type": "TASK", "confidence": 0.6938747564951578}]}, {"text": "To that end, we develop and pilot anew layer for UCCA which adds coreference information.", "labels": [], "entities": []}, {"text": "This coreference layer is constrained by the spans already specified in the foundational predicate-argument layer.", "labels": [], "entities": []}, {"text": "We compare these manual annotations to existing gold coreference annotations in multiple frameworks, finding a healthy level of overlap.", "labels": [], "entities": []}, {"text": "Our contributions are: \u2022 A discussion of multilayer design principles informed by existing semantically annotated corpora ( \u00a72).", "labels": [], "entities": []}, {"text": "\u2022 A semantically-based framework for mention identification and coreference resolution as a layer of UCCA ( \u00a73).", "labels": [], "entities": [{"text": "mention identification", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.8849822282791138}, {"text": "coreference resolution", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.9593465030193329}, {"text": "UCCA", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.9059574604034424}]}, {"text": "Reusing UCCA units as mentions facilitates efficient and consistent multilayer annotation.", "labels": [], "entities": []}, {"text": "We call the framework Universal Conceptual Cognitive Coreference (UCoref).", "labels": [], "entities": []}, {"text": "\u2022 An in-depth comparison to three other coreference frameworks based on annotation guidelines ( \u00a74) and a pilot English dataset ( \u00a75).: A foundational UCCA analysis of three consecutive sentences from the Richer Event Description corpus, with examples of coreferent units superimposed (boxes).", "labels": [], "entities": [{"text": "English dataset", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.7072967141866684}]}, {"text": "The context is that the speaker is posting a message to a forum in which she shares her own fears and asks for advice; you is coreferent with anyone else, and them refers back to the whole first scene.", "labels": [], "entities": []}, {"text": "Circled nodes indicate semantic heads/minimal spans, as determined by following State (S) and Center (C) edges.", "labels": [], "entities": []}, {"text": "In the third sentence, Advice please!, the addressee/adviser is a salient, but implicit Participant (A) which is expressed with a remote (dashed) edge to a prior mention.", "labels": [], "entities": []}, {"text": "Remaining categories are abbreviated as: H -Parallel Scene, P -Process, E -Elaborator, D -Adverbial, F -Function.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Overview of our pilot dataset. Candidates  refers to the UCCA units that are filtered by category  for mention candidacy before manual annotation.", "labels": [], "entities": [{"text": "UCCA units", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.9119879305362701}]}, {"text": " Table 2: Distribution of mentions and referents in the datasets. Mentions: Under event, we count UCoref (UCR)  scenes, GUM mentions of the types 'event' or 'abstract', and RED EVENTs; under entity, we count UCR A's,  GUM 'person', 'object', 'place', and 'substance' mentions, and RED ENTITYs. NE stands for OntoNotes (ONT)  named entities and IMP and remote for implicit and remote UCR units. A coreference cluster (referent) is classi- fied as an event referent if there is at least one event mention of that referent, as a time referent if there is at least  one UCR T / GUM 'time' / RED TIMEX3 mention of that referent, and as an entity referent otherwise; we also  report how many of the IMP and remote units are part of non-singleton referents.", "labels": [], "entities": []}, {"text": " Table 3: Exact (=) and fuzzy (\u2248) referent matches based on exact and aligned mentions between UCoref and  GUM, OntoNotes, and RED. Precision (P) and recall (R) are measured treating gold UCoref annotation as the  prediction and gold annotation in each respective existing framework as the reference. Italics indicate minimum  UCoref spans are used. Implicit UCoref units are excluded from this evaluation, and children of remote edges are  only counted once (for their primary edge).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 132, "end_pos": 145, "type": "METRIC", "confidence": 0.9511750042438507}, {"text": "recall (R)", "start_pos": 150, "end_pos": 160, "type": "METRIC", "confidence": 0.9530833810567856}]}]}