{"title": [{"text": "Generalized Data Augmentation for Low-Resource Translation", "labels": [], "entities": [{"text": "Generalized Data Augmentation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.868843654791514}, {"text": "Low-Resource Translation", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7659198641777039}]}], "abstractContent": [{"text": "Translation to or from low-resource languages (LRLs) poses challenges for machine translation in terms of both adequacy and fluency.", "labels": [], "entities": [{"text": "Translation to or from low-resource languages (LRLs)", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7892351018057929}, {"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7987979054450989}]}, {"text": "Data augmentation utilizing large amounts of monolingual data is regarded as an effective way to alleviate these problems.", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7398000061511993}]}, {"text": "In this paper , we propose a general framework for data augmentation in low-resource machine translation that not only uses target-side monolingual data, but also pivots through a related high-resource language (HRL).", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7049402743577957}, {"text": "low-resource machine translation", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.6524208088715872}]}, {"text": "Specifically, we experiment with a two-step pivoting method to convert high-resource data to the LRL, making use of available resources to better approximate the true data distribution of the LRL.", "labels": [], "entities": []}, {"text": "First, we inject LRL words into HRL sentences through an induced bilingual dictionary.", "labels": [], "entities": []}, {"text": "Second , we further edit these modified sentences using a modified unsupervised machine translation framework.", "labels": [], "entities": []}, {"text": "Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9983199238777161}]}], "introductionContent": [{"text": "The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8655127644538879}, {"text": "Neural Machine Translation (NMT)", "start_pos": 189, "end_pos": 221, "type": "TASK", "confidence": 0.8509962757428488}]}, {"text": "A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish).", "labels": [], "entities": []}, {"text": "Both transferring from HRL to: With a low-resource language (LRL) and a related high-resource language (HRL), typical data augmentation scenarios use any available parallel data and to back-translate English monolingual data and generate parallel resources ( and).", "labels": [], "entities": []}, {"text": "We additionally propose scenarios and, where we pivot through HRL in order to generate a LRL-ENG resource.", "labels": [], "entities": []}, {"text": "joint training on HRL and LRL parallel data) have shown to be effective techniques for low-resource NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.8955197930335999}]}, {"text": "Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (.", "labels": [], "entities": []}, {"text": "Simple joint training is still not ideal, though, considering that there will still be many words and possibly even syntactic structures that will not be shared between the most highly related languages.", "labels": [], "entities": []}, {"text": "There are model-based methods that ameliorate the problem through more expressive source-side representations conducive to sharing (), but they add significant computational and implementation complexity.", "labels": [], "entities": []}, {"text": "In this paper, we examine how to better share information between related LRL and HRLs through a framework of generalized data augmentation for low-resource MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 157, "end_pos": 159, "type": "TASK", "confidence": 0.9347314238548279}]}, {"text": "In our basic setting, we have access to parallel or monolingual data of an LRL of interest, its HRL, and the target language, which we will assume is English.", "labels": [], "entities": []}, {"text": "We propose methods to create pseudo-parallel LRL data in this setting.", "labels": [], "entities": []}, {"text": "As illustrated in, we augment parallel data via two main methods: 1) back-translating from ENG to LRL or HRL; 2) converting the HRL-ENG dataset to a pseudo LRL-ENG dataset.", "labels": [], "entities": [{"text": "HRL-ENG dataset", "start_pos": 128, "end_pos": 143, "type": "DATASET", "confidence": 0.7994496524333954}]}, {"text": "In the first thread, we focus on creating new parallel sentences through back-translation.", "labels": [], "entities": []}, {"text": "Backtranslating from the target language to the source () is a common practice in data augmentation, but has also been shown to be less effective in low-resource settings where it is hard to train a good back-translation model.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7542859613895416}]}, {"text": "As away to ameliorate this problem, we examine methods to instead translate from the target language to a highly-related HRL, which remains unexplored in the context of low-resource NMT.", "labels": [], "entities": []}, {"text": "This pseudo-HRL-ENG dataset can then be used for joint training with the LRL-ENG dataset.", "labels": [], "entities": [{"text": "LRL-ENG dataset", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9209179580211639}]}, {"text": "In the second thread, we focus on converting an HRL-ENG dataset to a pseudo-LRL-to-ENG dataset that better approximates the true LRL data.", "labels": [], "entities": [{"text": "HRL-ENG dataset", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8015296459197998}]}, {"text": "Converting between HRLs and LRLs also suffers from lack of resources, but because the LRL and HRL are related, this is an easier task that we argue can be done to some extent by simple (or unsupervised) methods.", "labels": [], "entities": []}, {"text": "In our proposed method, for the first step, we substitute HRL words on the source side of HRL parallel datasets with corresponding LRL words from an induced bilingual dictionary generated by mapping word embedding spaces).", "labels": [], "entities": []}, {"text": "In the second step, we further attempt translate the pseudo-LRL sentences to be closer to LRL ones utilizing an unsupervised machine translation framework.", "labels": [], "entities": []}, {"text": "In sum, our contributions are four fold: 1.", "labels": [], "entities": []}, {"text": "We conduct a thorough empirical evaluation of data augmentation methods for lowresource translation that take advantage of all accessible data, across four language pairs.", "labels": [], "entities": [{"text": "lowresource translation", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7761772871017456}]}, {"text": "2. We explore two methods for translating between related languages: word-by-word substitution using an induced dictionary, and unsupervised machine translation that further uses this word-by-word substituted data as input.", "labels": [], "entities": [{"text": "translating between related languages", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.854656919836998}, {"text": "word-by-word substitution", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.6808272153139114}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7339889407157898}]}, {"text": "These methods improve oversimple unsupervised translation from HRL to LRL by more than 2 to 10 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.998521625995636}]}, {"text": "3. Our proposed data augmentation methods improve over standard supervised backtranslation by 1.5 to 8 BLEU points, across all datasets, and an additional improvement of up to 1.1 BLEU points by augmenting from both ENG monolingual data, as well as HRL-ENG parallel data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9959089756011963}, {"text": "BLEU", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9951902627944946}]}], "datasetContent": [{"text": "Given To clarify notation, we use Sand T to denote the source and target sides of parallel datasets, and M for monolingual data.", "labels": [], "entities": []}, {"text": "Created data will be referred to as\u02c6Sas\u02c6 as\u02c6S m A )B . The superscript m denotes a particular augmentation approach (specified in Section 3).", "labels": [], "entities": []}, {"text": "The subscripts denote the translation direction that is used to create the data, with the LRL, HRL, and ENG denoted with 'L', 'H', and 'E' respectively.", "labels": [], "entities": [{"text": "HRL", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.8553497791290283}, {"text": "ENG", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9813939332962036}]}], "tableCaptions": [{"text": " Table 2: Evaluation of translation performance over four language pairs. Rows 1 and 2 show pre-training BLEU  scores. Rows 3-13 show scores after fine tuning. Statistically significantly best scores are highlighted (p < 0.05).", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9387737512588501}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9867340326309204}]}, {"text": " Table 3: A POR-GLG pivoting example with corresponding pivot BLEU scores. Edits by word substitution or  M-UMT are highlighted.", "labels": [], "entities": [{"text": "POR-GLG", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9309045672416687}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.976252555847168}]}]}