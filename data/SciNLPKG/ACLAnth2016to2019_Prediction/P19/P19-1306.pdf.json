{"title": [{"text": "Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations", "labels": [], "entities": [{"text": "Improving Low-Resource Cross-lingual Document Retrieval", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8615689873695374}]}], "abstractContent": [{"text": "In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations.", "labels": [], "entities": [{"text": "cross-lingual document retrieval", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.6051280796527863}]}, {"text": "We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word em-beddings as input.", "labels": [], "entities": []}, {"text": "By including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs.", "labels": [], "entities": []}, {"text": "Due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label.", "labels": [], "entities": []}, {"text": "Experimental results on the MATERIAL dataset show that our model outper-forms the competitive translation-based base-lines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks.", "labels": [], "entities": [{"text": "MATERIAL dataset", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.861480176448822}, {"text": "cross-lingual information retrieval tasks", "start_pos": 179, "end_pos": 220, "type": "TASK", "confidence": 0.6843316629528999}]}], "introductionContent": [{"text": "Cross-lingual relevance ranking, or Cross-Lingual Information Retrieval (CLIR), is the task of ranking foreign documents against a user query (.", "labels": [], "entities": [{"text": "Cross-lingual relevance ranking", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.675389806429545}, {"text": "Cross-Lingual Information Retrieval (CLIR)", "start_pos": 36, "end_pos": 78, "type": "TASK", "confidence": 0.7543817758560181}]}, {"text": "As multilingual documents are more accessible, CLIR is increasingly more important whenever the relevant information is in other languages.", "labels": [], "entities": []}, {"text": "Traditional CLIR systems consist of two components: machine translation and monolingual information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7910757064819336}, {"text": "information retrieval", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.6720922291278839}]}, {"text": "Based on the translation direction, it can be further categorized into the document translation and the query translation approaches.", "labels": [], "entities": [{"text": "document translation", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.6809103637933731}, {"text": "query translation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7372783124446869}]}, {"text": "In both cases, we first solve the translation problem, and the task is transformed to the monolingual setting.", "labels": [], "entities": [{"text": "translation problem", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.9267479479312897}]}, {"text": "However, while conceptually simple, the performance of this modular approach is fundamentally limited by the quality of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.716854453086853}]}, {"text": "Recently, many deep neural IR models have shown promising results on monolingual data sets.", "labels": [], "entities": []}, {"text": "They learn a scoring function directly from the relevance label of query-document pairs.", "labels": [], "entities": []}, {"text": "However, it is not clear how to use them when documents and queries are not in the same language.", "labels": [], "entities": []}, {"text": "Furthermore, those deep neural networks need a large amount of training data.", "labels": [], "entities": []}, {"text": "This is expensive to get for lowresource language pairs in our cross-lingual case.", "labels": [], "entities": []}, {"text": "In this paper, we propose a cross-lingual deep relevance ranking architecture based on a bilingual view of queries and documents.", "labels": [], "entities": []}, {"text": "As shown in, our model first translates queries and documents and then uses four components to match them in both the source and target language.", "labels": [], "entities": []}, {"text": "Each component is implemented as a deep neural network, and the final relevance score combines all components which are jointly trained given the relevance label.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.8849822282791138}]}, {"text": "We implement this based on state-  of-the-art term interaction models because they enable us to make use of cross-lingual embeddings to explicitly encode terms of queries and documents even if they are in different languages.", "labels": [], "entities": []}, {"text": "To deal with the small amount of training data, we first perform query likelihood retrieval and include the score as an extra feature in our model.", "labels": [], "entities": []}, {"text": "In this way, the model effectively learns to rerank from a small number of relevance labels.", "labels": [], "entities": []}, {"text": "Furthermore, since the word embeddings are aligned in the same space, our model can directly transfer to another language pair with no additional training data.", "labels": [], "entities": []}, {"text": "We evaluate our model on the MATERIAL CLIR dataset with three language pairs including English to Swahili, English to Tagalog, and English to Somali.", "labels": [], "entities": [{"text": "MATERIAL CLIR dataset", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.7245153586069742}]}, {"text": "Experimental results demonstrate that our model outperforms other translation-based query likelihood retrieval and monolingual deep relevance ranking approaches.", "labels": [], "entities": [{"text": "translation-based query likelihood retrieval", "start_pos": 66, "end_pos": 110, "type": "TASK", "confidence": 0.6927777826786041}]}], "datasetContent": [{"text": "We first use the Indri 1 system which uses query likelihood with Dirichlet Smoothing () to preselect the documents from the collection.", "labels": [], "entities": []}, {"text": "To build the training dataset, for each positive example in the returned list, we randomly sample one negative example from the documents returned by Indri.", "labels": [], "entities": [{"text": "Indri", "start_pos": 150, "end_pos": 155, "type": "DATASET", "confidence": 0.8986326456069946}]}, {"text": "The model is then trained with a binary crossentropy loss.", "labels": [], "entities": []}, {"text": "On validation or testing set, we use our prediction scores to rerank the documents returned by Indri.", "labels": [], "entities": [{"text": "Indri", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.9397258162498474}]}, {"text": "Following the previous work   We use the TREC ad-hoc retrieval evaluation script to compute Precision@20, Mean Average Precision (MAP), Normalized Discounted Cumulative Gain@20 (NDCG@20).", "labels": [], "entities": [{"text": "Precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9231780767440796}, {"text": "Mean Average Precision (MAP)", "start_pos": 106, "end_pos": 134, "type": "METRIC", "confidence": 0.9714594582716624}]}, {"text": "We also report the Actual Query Weighted Value (AQWV), a set-based metric with penalty for both missing relevant and returning irrelevant documents.", "labels": [], "entities": [{"text": "Actual Query Weighted Value (AQWV)", "start_pos": 19, "end_pos": 53, "type": "METRIC", "confidence": 0.7942098762307849}]}, {"text": "We use \u03b2 = 40.0 and find the best global fixed cutoff overall queries.", "labels": [], "entities": []}, {"text": "For traditional CLIR approaches, we use query translation and document translation with the Indri system.", "labels": [], "entities": [{"text": "query translation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7635314464569092}, {"text": "document translation", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.6849639713764191}]}, {"text": "For query translation, we use Dictionary-Based Query Translation (DBQT) and Probabilistic Structured Query (PSQ).", "labels": [], "entities": [{"text": "query translation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8090144097805023}, {"text": "Dictionary-Based Query Translation", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.6135400732358297}]}, {"text": "For document translation, we use Statistical Machine Translation (SMT) and Neural Machine Translation (NMT).", "labels": [], "entities": [{"text": "document translation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7429470121860504}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.7795704404513041}, {"text": "Neural Machine Translation", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7393460472424825}]}, {"text": "For SMT, we use the moses system () with word alignments using mGiza and 5-gram KenLM language model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9945535659790039}, {"text": "word alignments", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6970777213573456}]}, {"text": "For NMT, we use sequence-to-sequence model with attention () implemented in Marian (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8185611367225647}]}, {"text": "For deep relevance ranking baselines, we investigate recent state-of-the-art models including PACRR, PACRR-DRMM, and POSIT-DRMM.", "labels": [], "entities": []}, {"text": "These models and our methods all use an SMTbased document translation as input.", "labels": [], "entities": [{"text": "SMTbased document translation", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.9119924108187357}]}, {"text": "For POSIT-DRMM and Bilingual POSIT-DRMM, we use the k-maxpooling with k = 5 and 0.3 dropout of the BiL-STM output.", "labels": [], "entities": []}, {"text": "For PACRR, PACRR-DRMM and their bilingual counterparts, we use convolutional filter sizes with, and each filter size has 32 filters.", "labels": [], "entities": []}, {"text": "We use k = 2 in the k-max-pooling.", "labels": [], "entities": []}, {"text": "The loss function is minimized using the Adam optimizer () with the training batch size as 32.", "labels": [], "entities": []}, {"text": "We monitor the MAP performance on the development set after each epoch of training to select the model which is used on the test data.", "labels": [], "entities": [{"text": "MAP", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8301230072975159}]}, {"text": "shows the result on EN->SW and EN->TL where we train and test on the same language pair.", "labels": [], "entities": []}, {"text": "For query translation, PSQ is better than DBQT because PSQ uses a weighted alternative to translate query terms and does not limit to the fixed translation from the dictionary as in DBQT.", "labels": [], "entities": [{"text": "query translation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.854585736989975}, {"text": "DBQT", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9433870315551758}, {"text": "DBQT", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.9424994587898254}]}, {"text": "For document translation, we find that both SMT and NMT have a similar performance which is close to PSQ.", "labels": [], "entities": [{"text": "document translation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7869195640087128}, {"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.954374372959137}]}, {"text": "The effectiveness of different approaches depends on the language pair (PSQ for EN->SW and SMT for EN->TL), which is a similar finding with and.", "labels": [], "entities": []}, {"text": "In our experiments with deep relevance ranking models, we all use SMT and PSQ because they have strong performances in both language pairs and it is fair to compare.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8869155645370483}]}, {"text": "Effect of Extra Features and Bilingual Representation.", "labels": [], "entities": [{"text": "Bilingual Representation", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.7964566349983215}]}, {"text": "While deep relevance ranking can achieve decent performance, the extra features are critical to achieve better results.", "labels": [], "entities": []}, {"text": "Because the extra features include the Indri score, the deep neural model essentially learns to rerank the document by effectively using a small number of training examples.", "labels": [], "entities": [{"text": "Indri score", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9626248478889465}]}, {"text": "Furthermore, our models with bilingual representations achieve better results in both language pairs, giving additional 1-3 MAP improvements over their counterparts.", "labels": [], "entities": [{"text": "MAP", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.8816255331039429}]}, {"text": "To compare language pairs, EN->TL has larger improvements over EN->SW.", "labels": [], "entities": []}, {"text": "This is because EN->TL has better query translation, document translation, and query likelihood retrieval results from the baselines, and thus it enjoys more benefits from our model.", "labels": [], "entities": [{"text": "query translation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7366339564323425}, {"text": "document translation", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7186377346515656}]}, {"text": "We also found POSIT-DRMM works better than the other two, suggesting term-gating is useful especially when the query translation can provide more alternatives.", "labels": [], "entities": []}, {"text": "We then perform ensembling of POSIT-DRMM to further improve the results.", "labels": [], "entities": []}, {"text": "shows the result fora zero-shot transfer learning setting where we train on EN->SW + EN->TL and directly test on EN->SO without using any Somali relevance labels.", "labels": [], "entities": []}, {"text": "This transfer learning delivers a 1-3 MAP improvement over PSQ and SMT.", "labels": [], "entities": [{"text": "MAP", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9864450097084045}, {"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9317830801010132}]}, {"text": "This presents a promising approach to boost performance by utilizing relevance labels from other language pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The MATERIAL dataset statistics. For SW and TL, we use the ANALYSIS document set with Q1 for  training, Q2 for dev, and Q3 for test. For transfer learning to SO, we use the DEV document set with Q1. Q1", "labels": [], "entities": [{"text": "MATERIAL dataset", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.7358465939760208}, {"text": "ANALYSIS", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9684818387031555}, {"text": "transfer learning", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.8863236606121063}, {"text": "DEV document set", "start_pos": 183, "end_pos": 199, "type": "DATASET", "confidence": 0.9479331771532694}]}, {"text": " Table 2: Test set result on English to Swahili and English to Tagalog. We report the TREC ad-hoc retrieval  evaluation metrics (MAP, P@20, NDCG@20) and the Actual Query Weighted Value (AQWV).", "labels": [], "entities": [{"text": "TREC ad-hoc retrieval  evaluation metrics", "start_pos": 86, "end_pos": 127, "type": "METRIC", "confidence": 0.6306074142456055}, {"text": "Actual Query Weighted Value (AQWV)", "start_pos": 157, "end_pos": 191, "type": "METRIC", "confidence": 0.7833573562758309}]}, {"text": " Table 3: Zero-shot transfer learning on English to So- mali test set.", "labels": [], "entities": [{"text": "So- mali test set", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.8148648142814636}]}]}