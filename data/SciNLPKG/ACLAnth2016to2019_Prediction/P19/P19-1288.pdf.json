{"title": [{"text": "Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation", "labels": [], "entities": [{"text": "Retrieving Sequential Information", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8727787534395853}, {"text": "Neural Machine Translation", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6631270547707876}]}], "abstractContent": [{"text": "Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential information.", "labels": [], "entities": []}, {"text": "Over-translation and under-translation errors often occur for the above reason, especially in the long sentence translation scenario.", "labels": [], "entities": [{"text": "long sentence translation", "start_pos": 98, "end_pos": 123, "type": "TASK", "confidence": 0.7391366759936014}]}, {"text": "In this paper, we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property.", "labels": [], "entities": []}, {"text": "Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure.", "labels": [], "entities": []}, {"text": "Secondly , we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder.", "labels": [], "entities": [{"text": "FS-decoder", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.46866220235824585}]}, {"text": "Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU without decelerating the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with considerable speedup.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9951501488685608}, {"text": "FS-decoder", "start_pos": 190, "end_pos": 200, "type": "METRIC", "confidence": 0.7202482223510742}]}], "introductionContent": [{"text": "Neural machine translation (NMT) models) solve the machine translation problem with the Encoder-Decoder framework and achieve impressive performance on translation quality.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7626160780588785}, {"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7834155559539795}]}, {"text": "Recently, the Transformer model ( further enhances the translation performance on multiple language pairs, while suffering from the slow decoding procedure, which re-: A fragment of along sentence translation.", "labels": [], "entities": [{"text": "along sentence translation", "start_pos": 182, "end_pos": 208, "type": "TASK", "confidence": 0.7424217859903971}]}, {"text": "AR stands for the translation of the autoregressive Transformer.", "labels": [], "entities": [{"text": "AR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9367576837539673}, {"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9578568935394287}]}, {"text": "The output of the NAT model contains repeated translations of word 'more' and misses the word 'tragic'.", "labels": [], "entities": []}, {"text": "The slow decoding problem of the Transformer model is caused by its autoregressive nature, which means that the target sentence is generated word byword according to the source sentence representations and the target translation history.", "labels": [], "entities": []}, {"text": "Non-autoregressive Transformer model () is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism.", "labels": [], "entities": []}, {"text": "Since the generation of target words is independent, NAT models utilize alternative information such as encoder inputs (), translation results from other systems () and latent variables as decoder inputs.", "labels": [], "entities": []}, {"text": "Without considering the target translation history, NAT models are weak to exploit the target words collocation knowledge and tend to generate repeated target words at adjacent time steps (.", "labels": [], "entities": []}, {"text": "Over-translation and undertranslation problems are aggravated and often occur due to the above reasons.", "labels": [], "entities": []}, {"text": "shows an inferior translation example generated by a NAT model.", "labels": [], "entities": []}, {"text": "Compared to the autoregressive Transformer, NAT models achieve significant speedup while suffering from a large gap in translation quality due to the lack of target sequential information.", "labels": [], "entities": []}, {"text": "In this paper, we present two approaches to retrieve the target sequential information for NAT models to enhance their translation ability and meanwhile preserve the fast-decoding property.", "labels": [], "entities": []}, {"text": "Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure.", "labels": [], "entities": []}, {"text": "We leverage the sequence-level objectives (e.g., BLEU), GLEU (), TER)) instead of the cross-entropy objective to encourage NAT model to generate high quality sentences rather than the correct token for each position.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9991534948348999}, {"text": "GLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9969037175178528}, {"text": "TER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9802778363227844}]}, {"text": "Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder.", "labels": [], "entities": [{"text": "FS-decoder", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.46866220235824585}]}, {"text": "The bottom layers of the FS-decoder run in parallel to keep the decoding speed and the top layer of the FS-decoder can exploit target sequential information to guide the target words generation procedure.", "labels": [], "entities": [{"text": "FS-decoder", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8426398634910583}, {"text": "FS-decoder", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.8908466696739197}, {"text": "words generation", "start_pos": 177, "end_pos": 193, "type": "TASK", "confidence": 0.7322588264942169}]}, {"text": "We conduct experiments on three machine translation tasks (IWSLT16 En\u2192De, WMT14 En\u2194De, WMT16 En\u2192Ro) to validate our proposed approaches.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.67723748087883}, {"text": "IWSLT16", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.7171007990837097}]}, {"text": "Experimental results show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on the translation quality without decelerating the decoding speed, and the FS-decoder achieves comparable translation capacity to the autoregressive Transformer with considerable speedup.", "labels": [], "entities": [{"text": "FS-decoder", "start_pos": 183, "end_pos": 193, "type": "METRIC", "confidence": 0.5179427862167358}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: top-k probability expection for k=1, 5, 10,  100, 1000", "labels": [], "entities": [{"text": "top-k probability expection", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.6710423429807028}]}]}