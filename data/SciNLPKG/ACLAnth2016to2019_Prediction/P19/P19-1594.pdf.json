{"title": [], "abstractContent": [{"text": "Spectral models for learning weighted non-deterministic automata have nice theoretical and algorithmic properties.", "labels": [], "entities": []}, {"text": "Despite this, it has been challenging to obtain competitive results in language modeling tasks, for two main reasons.", "labels": [], "entities": [{"text": "language modeling tasks", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.8607262372970581}]}, {"text": "First, in order to capture long-range dependencies of the data, the method must use statistics from long substrings, which results in very large matrices that are difficult to decompose.", "labels": [], "entities": []}, {"text": "The second is that the loss function behind spectral learning, based on moment matching, differs from the probabilis-tic metrics used to evaluate language models.", "labels": [], "entities": []}, {"text": "In this work we employ a technique for scaling up spectral learning, and use interpolated predictions that are optimized to maximize perplexity.", "labels": [], "entities": [{"text": "spectral learning", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7503936588764191}]}, {"text": "Our experiments in character-based language modeling show that our method matches the performance of state-of-the-art ngram models, while being very fast to train.", "labels": [], "entities": [{"text": "character-based language modeling", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.6188162366549174}]}], "introductionContent": [{"text": "In the recent years we have witnessed the development of spectral methods based on matrix decompositions to learn Probabilistic Non-deterministic Finite Automata (PNFA) and related models ().", "labels": [], "entities": []}, {"text": "Essentially, PNFA can be regarded as recurrent neural networks where the function that predicts the dynamic state representation from previous states is linear.", "labels": [], "entities": []}, {"text": "Despite the expressiveness of PNFA and the strong theoretical properties of spectral learning algorithms, it has been challenging to get competitive results on language modeling tasks.", "labels": [], "entities": [{"text": "language modeling tasks", "start_pos": 160, "end_pos": 183, "type": "TASK", "confidence": 0.7857160667578379}]}, {"text": "We argue and confirm with our experiments that there are two main reasons why using spectral methods for language modeling is challenging.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7235551178455353}]}, {"text": "The first reason is a scalability problem to handle long range dependencies.", "labels": [], "entities": []}, {"text": "The spectral method is based on computing a Hankel matrix that contains statistics of expectations over substrings generated by the target language.", "labels": [], "entities": []}, {"text": "If we want to incorporate long-range dependencies we need to consider long substrings.", "labels": [], "entities": []}, {"text": "A consequence of this is that the Hankel matrix can become too large to make it practical to perform algebraic decompositions.", "labels": [], "entities": [{"text": "algebraic decompositions", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7435373961925507}]}, {"text": "To address this problem we use the basis selection technique by to scale spectral learning and model long range dependencies.", "labels": [], "entities": []}, {"text": "Our experiments confirm that modeling long range dependencies is essential to obtain competitive language models.", "labels": [], "entities": []}, {"text": "The second limitation of classical spectral methods when applied to language modeling is that the loss function that the learning algorithm attempts to minimize is not aligned with the loss function that is used to evaluate model performance.", "labels": [], "entities": []}, {"text": "Spectral methods minimize the 2 distance on the prediction of expectations of substrings up to a certain length (see fora formulation of spectral learning in terms of loss minimization), while language models are usually evaluated using conditional perplexity.", "labels": [], "entities": []}, {"text": "There have been some proposals on generalizing the fundamental ideas of spectral learning to other loss functions ().", "labels": [], "entities": [{"text": "spectral learning", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8835900127887726}]}, {"text": "However, while these approaches are promising they have the downside that they lead to relatively expensive iterative convex optimizations and it is still a challenge to scale them to model long-range dependencies.", "labels": [], "entities": [{"text": "convex optimizations", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.7212470471858978}]}, {"text": "In this paper we propose a simpler yet effective alternative to the iterative optimization.", "labels": [], "entities": []}, {"text": "We use the classical spectral method based on low-rank matrix decomposition to learn a PNFA that computes substring expectations.", "labels": [], "entities": []}, {"text": "Then we use these expectations as features in an interpolated ngram model and we learn the weights of the interpolation so as to maximize perplexity.", "labels": [], "entities": []}, {"text": "This interpo-lation step is iterative, but it is a simple and very efficient convex optimization: the weights of the interpolation can be trained in a few seconds or minutes at most.", "labels": [], "entities": []}, {"text": "The refinement step allows us to leverage all the moments computed by the learned PNFA and to align the spectral method with the perplexity evaluation metric.", "labels": [], "entities": []}, {"text": "Our experiments on character-level language model show that: (1) modeling long range dependencies is important; and (2) with the simple interpolation step we can obtain competitive results.", "labels": [], "entities": []}, {"text": "Our perplexity results are significantly better than feed-forward NNs, as good or better than sophisticated interpolation techniques such as Kneser-Ney estimation, and close to the performance of RNNs on two datasets.", "labels": [], "entities": []}, {"text": "The main contribution of our work consists on combining two simple ideas, i.e. incorporating long-range dependencies via basis selection of long substring moments (Section 2), and refining the predictions of the PNFA with an iterative interpolation step (Section 3).", "labels": [], "entities": []}, {"text": "Our experiments show that these two simple ideas bring us one step closer to making spectral methods for PNFA reach state-of-the-art performance on language modeling tasks (Section 4).", "labels": [], "entities": []}, {"text": "The advantage of these methods over other popular approaches to language modeling is their simplicity and the fact that they rely on efficient convex optimizations for training the model parameters.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.72464419901371}]}, {"text": "Furthermore, PNFA are probabilistic models for which efficient inference methods can be easily derived for computing all sorts of expectations.", "labels": [], "entities": []}, {"text": "These expectations could then be used as features to learn predictive interpolation models.", "labels": [], "entities": []}, {"text": "In this paper we present experiments with one type of expectation and interpolation model that illustrates the potential of this approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present experiments in character-based language modeling.", "labels": [], "entities": [{"text": "character-based language modeling", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.6923117240269979}]}, {"text": "Our spectral ngram models work with a fixed context length, and we show results varying this length up to relatively large values.", "labels": [], "entities": []}, {"text": "Following the standard, the goal is to learn a language model that predicts the next symbol given a sentence prefix, including the prediction of sentence ends.", "labels": [], "entities": []}, {"text": "As datasets we use the Penn Treebank (PTB) prepared by , and \"War and Peace\" (WP) dataset prepared by . We use two probabilistic evaluation metrics that are standard in language modeling tasks: Cross Entropy and Bits per Character (BpC).", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 23, "end_pos": 42, "type": "DATASET", "confidence": 0.964401388168335}, {"text": "Bits per Character (BpC)", "start_pos": 212, "end_pos": 236, "type": "METRIC", "confidence": 0.7445004979769388}]}, {"text": "Depending on the dataset, we use one or the other such that we can directly compare to published results.", "labels": [], "entities": []}, {"text": "present results in terms of the context size (n) for the PTB and WP tests respectively.", "labels": [], "entities": [{"text": "PTB", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.7157850861549377}]}, {"text": "The column \"UB\" shows an upperbound on the performance metric using a context of size n.", "labels": [], "entities": []}, {"text": "This is computed directly using the expected counts on the test set to compute the conditional distribution.", "labels": [], "entities": []}, {"text": "If we were able to estimate these expectations perfectly, we would achieve the  reported performance.", "labels": [], "entities": []}, {"text": "As the two tables show, a context of size 10 already gives a high upperbound, suggesting that we can achieve good performance using a fixed but large horizon.", "labels": [], "entities": []}, {"text": "The tables show results of the spectral language model for different context sizes, using expectations from the \"longest\" context or \"interpolated\" expectations.", "labels": [], "entities": []}, {"text": "A clear trend is that the results improve with the context length, achieving a stable performance for n = 10.", "labels": [], "entities": []}, {"text": "It is also clear that the interpolated predictions work much better than simply using the longest context.", "labels": [], "entities": []}, {"text": "also compares to a MaxEnt model (labeled \"ME\"), which is an interpolation model of Eq.3 but uses empirical expectations f T (x) computed from training counts instead of those given by the spectral PNFA.", "labels": [], "entities": []}, {"text": "Clearly, the expectations given by the PNFA generalize better and lead to improvements.", "labels": [], "entities": []}, {"text": "The last column of the two tables shows the number of rows (and columns) of the (square) Hankel matrix we factorize for each context size.", "labels": [], "entities": []}, {"text": "This gives an idea of the cost of the estimation algorithm, which goes from a few seconds to a few hours, depending on the matrix size.", "labels": [], "entities": [{"text": "estimation", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.9642384052276611}]}, {"text": "5 Following the theory behind, this number is an upper bound on the size of the minimal PNFA that reproduces exactly the expected counts of training substrings.", "labels": [], "entities": []}, {"text": "The tables include a column \"KN\" with the results of an ngram language model estimated with Kneser-Ney interpolation).", "labels": [], "entities": []}, {"text": "Looking at the results on the PTB data in, our interpolated model performs equally well, and sometimes better, than the KN models using the same context length.", "labels": [], "entities": [{"text": "PTB data", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9108059108257294}]}, {"text": "reports the performance of other models: a feed-forward neural network 6 obtains 1.57, which our model improves with contexts of n = 6 or larger; an RNN works at 1.41, slightly better than our best result of 1.45.", "labels": [], "entities": []}, {"text": "Their best result is of 1.37 fora MaxEnt model with context length of n = 14 engineered for scalability.", "labels": [], "entities": []}, {"text": "For the WP test in, our model and the KN model perform similarly, with some slight improvements by the KN model.", "labels": [], "entities": [{"text": "WP", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9422075152397156}]}, {"text": "The table also includes the results of a feed-forward neural network (FNN) for increasing orders, by.", "labels": [], "entities": []}, {"text": "We observe that our interpolated model works better, with our best result at 1.24.", "labels": [], "entities": []}, {"text": "They also report the results of an RNN obtaining 1.24, and of LSTM and GRU which both obtain 1.08.", "labels": [], "entities": [{"text": "RNN", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.5260581970214844}, {"text": "LSTM", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.802043080329895}, {"text": "GRU", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.969692051410675}]}], "tableCaptions": [{"text": " Table 1: Bits-per-character on the PTB test set.", "labels": [], "entities": [{"text": "PTB test set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.977496882279714}]}, {"text": " Table 2: Cross-entropy on the WP test set.", "labels": [], "entities": [{"text": "WP test set", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.9012200435002645}]}]}