{"title": [], "abstractContent": [{"text": "Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8671515583992004}]}, {"text": "Recent approaches for this span-level task have been token-level models which have inherent limitations.", "labels": [], "entities": []}, {"text": "They cannot easily define and implement span-level features, cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding.", "labels": [], "entities": []}, {"text": "To address these concerns, we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction.", "labels": [], "entities": [{"text": "joint entity mention detection", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.5862502381205559}, {"text": "relation extraction", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.7792004942893982}]}, {"text": "We report anew state-of-the-art performance of 62.83 F 1 (prev best was 60.49) on the ACE2005 dataset.", "labels": [], "entities": [{"text": "F 1", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9520787596702576}, {"text": "prev best", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9587005078792572}, {"text": "ACE2005 dataset", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9934163987636566}]}], "introductionContent": [{"text": "Many NLP tasks follow the pattern of taking raw text as input and then: detecting relevant spans and classifying the relations between those spans.", "labels": [], "entities": []}, {"text": "Examples of this include Relation Extraction (), Coreference Resolution ( and Semantic Role Labeling ().", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.9632882475852966}, {"text": "Coreference Resolution", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.9500344395637512}, {"text": "Semantic Role Labeling", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7655467391014099}]}, {"text": "This class of NLP problems are inherently span-level tasks.", "labels": [], "entities": []}, {"text": "This paper focuses on Relation Extraction (RE), which is the task of entity mention detection and classifying the relations between each pair of those mentions.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8374030351638794}, {"text": "entity mention detection", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6494989593823751}]}, {"text": "We report anew state-of-the-art performance of 62.83 F 1 (prev best was 60.49) on the ACE2005 dataset.", "labels": [], "entities": [{"text": "F 1", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9520787596702576}, {"text": "prev best", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9587005078792572}, {"text": "ACE2005 dataset", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9934163987636566}]}, {"text": "Here is a simple example of Relation Extraction for the sentence, \"Washington, D.C. is the capital of the USA\".", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7677945792675018}]}, {"text": "Step 1, Entity Mention Detection will detect the spans \"Washington, D.C.\" and \"USA\" as LOCATIONS.", "labels": [], "entities": [{"text": "Entity Mention Detection", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.6427306433518728}, {"text": "USA", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.9541310667991638}, {"text": "LOCATIONS", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.8735902905464172}]}, {"text": "Step 2, Relation Extraction will classify all directed pairs of detected entity mentions.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8690772652626038}]}, {"text": "It will classify the directed pair (\"Washington, D.C.\", \"USA\") as having the relation IS CAPITAL OF.", "labels": [], "entities": [{"text": "USA", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.9553539752960205}, {"text": "IS CAPITAL OF", "start_pos": 86, "end_pos": 99, "type": "METRIC", "confidence": 0.8013354539871216}]}, {"text": "But the directed pair (\"USA\", \"Washington, D.C.\") will be classified as having no relation (NONE).", "labels": [], "entities": [{"text": "USA", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.9651897549629211}, {"text": "NONE", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9031837582588196}]}, {"text": "In more complex cases, each entity could participate in multiple different relations.", "labels": [], "entities": []}, {"text": "Since (), work on RE has revolved around end-to-end systems: single models which first perform entity mention detection and then relation extraction.", "labels": [], "entities": [{"text": "RE", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9655227065086365}, {"text": "entity mention detection", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.6394812862078348}, {"text": "relation extraction", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.8165797889232635}]}, {"text": "These recent works) have used sequential token-level methods for both the steps.", "labels": [], "entities": []}, {"text": "Token-level models are primarily constrained by the fact that each token has a single fixed representation while each token is apart of many different spans.", "labels": [], "entities": []}, {"text": "To model and extract spans, these token-level models have to resort to approximate span-level features which are increasingly indirect and expensive: Tree-LSTMs (,, Beam Search () and Pointer Networks.", "labels": [], "entities": []}, {"text": "Their usage of the BILOU) token-tagging scheme makes modelling overlapping entities impossible.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9623520374298096}]}, {"text": "In general, these tokenlevel models are sequential in nature and hence have cascading errors.", "labels": [], "entities": []}, {"text": "Another end-to-end approach for RE is to use a simple span-level model.", "labels": [], "entities": [{"text": "RE", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9957243204116821}]}, {"text": "A model which creates explicit representations for all possible spans, uses them for the entity mention detection step and then explicitly compares ordered pairs of spans for the relation extraction step.", "labels": [], "entities": [{"text": "entity mention detection", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.667092760403951}, {"text": "relation extraction", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7504224479198456}]}, {"text": "Such a model is not constrained like the token-level models because it can define direct span-specific features for each span inexpensively.", "labels": [], "entities": []}, {"text": "Since each possible span is separately considered, selecting overlapping entity mentions is possible.", "labels": [], "entities": []}, {"text": "Predicting one span as an entity no longer blocks another span from being predicted as an entity.", "labels": [], "entities": []}, {"text": "This approach models each possible span independently and in parallel i.e. it is not sequential and does not suffer from cascad-ing errors.", "labels": [], "entities": []}, {"text": "Such models have recently found success in similar NLP tasks like Coreference Resolution ( and Semantic Role Labeling (.", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.9561626613140106}, {"text": "Semantic Role Labeling", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.7544144590695699}]}, {"text": "In this paper, we present such a span-level model for Relation Extraction.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.976640522480011}]}, {"text": "We propose a simple bi-LSTM based model which generates span representations for each possible span.", "labels": [], "entities": []}, {"text": "The span representations are used to perform entity mention detection on all spans in parallel.", "labels": [], "entities": [{"text": "entity mention detection", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7535571853319804}]}, {"text": "The same span representations are then used to perform relation extraction on all pairs of detected entity mentions.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8164955973625183}]}, {"text": "We evaluated the performance of our model on the ACE2005 dataset () and report anew startof-the-art F 1 score of 62.83 for Relation Extraction.", "labels": [], "entities": [{"text": "ACE2005 dataset", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.9906958639621735}, {"text": "F 1 score", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9170917272567749}, {"text": "Relation Extraction", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.9025774002075195}]}], "datasetContent": [{"text": "Dataset We use the ACE2005 dataset).", "labels": [], "entities": [{"text": "Dataset", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7823275327682495}, {"text": "ACE2005 dataset", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.9907874763011932}]}, {"text": "It has 351 documents for train, 80 for validation and 80 for test.", "labels": [], "entities": []}, {"text": "There are seven span-level entity types and six ordered span relation types.", "labels": [], "entities": []}, {"text": "Character Embeddings The learned character embeddings are of size 8. 1-dimensional convolutions of window size 3,4,5 are applied per-token with 50 filters of each window size.", "labels": [], "entities": []}, {"text": "This is followed by ReLU activation and max-pooling over each filter.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.8274699449539185}]}, {"text": "Model Size Our stacked bi-LSTMs (Section 3.1) has 3 layers with 200-dimensional hidden states and highway connections.", "labels": [], "entities": []}, {"text": "All Multi Layer Perceptrons (MLP) has two hidden layers with 500 dimensions, each followed by ReLU activation.", "labels": [], "entities": []}, {"text": "Feature Encoding Each span gets a span width feature which is a learned 20-dimensional vector representing the number of tokens in that span.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: EMD and RE results on the ACE2005 Test dataset. Our model reports a new state-of-the-art RE perfor- mance. Sanh et al. (2018) present several results in their multi-task paper. Results marked with (*) are not fair  comparisons here because they use additional signals beyond EMD and RE. Included here for completeness.", "labels": [], "entities": [{"text": "RE", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.932873010635376}, {"text": "ACE2005 Test dataset", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.9840220212936401}]}, {"text": " Table 2: Numbers are for the Train Set (351 docs) of ACE2005, where each Relation is between exactly two  Entities. Dev and Test Sets follow the same trends.", "labels": [], "entities": [{"text": "ACE2005", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.8186344504356384}]}]}