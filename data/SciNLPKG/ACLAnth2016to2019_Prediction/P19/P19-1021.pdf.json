{"title": [{"text": "Revisiting Low-Resource Neural Machine Translation: A Case Study", "labels": [], "entities": [{"text": "Revisiting Low-Resource Neural Machine Translation", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8847434759140015}]}], "abstractContent": [{"text": "It has been shown that the performance of neu-ral machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results.", "labels": [], "entities": [{"text": "neu-ral machine translation (NMT)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.714774822195371}, {"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 134, "end_pos": 186, "type": "TASK", "confidence": 0.6936222698007312}]}, {"text": "In this paper, we reassess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings.", "labels": [], "entities": []}, {"text": "We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT.", "labels": [], "entities": []}, {"text": "In our experiments on German-English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed.", "labels": [], "entities": [{"text": "IWSLT14 training data", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.8494177460670471}]}, {"text": "We also apply these techniques to a low-resource Korean-English dataset, surpassing previously reported results by 4 BLEU.", "labels": [], "entities": [{"text": "Korean-English dataset", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.7254208475351334}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9981999397277832}]}], "introductionContent": [{"text": "While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field (, recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 6, "end_pos": 38, "type": "TASK", "confidence": 0.8303419649600983}, {"text": "phrase-based statistical machine translation (PBSMT", "start_pos": 234, "end_pos": 285, "type": "TASK", "confidence": 0.6850326458613077}]}, {"text": "In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings.", "labels": [], "entities": [{"text": "validity", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9438085556030273}]}, {"text": "Our main contributions are as follows: \u2022 we explore best practices for low-resource   How do the data needs of SMT and NMT compare?", "labels": [], "entities": [{"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9615736603736877}]}, {"text": "NMT promises both to generalize better (exploiting word similary in embeddings) and condition on larger context (entire input and all prior output words).", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7667647004127502}]}, {"text": "We built English-Spanish systems on WMT data, 7 about 385.7 million English words paired with Spanish.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9380156993865967}]}, {"text": "To obtain a learning curve, we used 1 1024 , 1 512 , ..., , and all of the data.", "labels": [], "entities": []}, {"text": "For SMT, the language model was trained on the Spanish part of each subset, respectively.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9939144253730774}]}, {"text": "In addition to a NMT and SMT system trained on each subset, we also used all additionally provided monolingual data fora big language model in contrastive SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9817302227020264}, {"text": "SMT", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.8043569922447205}]}, {"text": "NMT exhibits a much steeper learning curve, starting with abysmal results (BLEU score of Lista de una estrategi elecci\u00f3n de hojas de O", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.999262273311615}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training corpus size and subword vocabulary  size for different subsets of IWSLT14 DE\u2192EN data,  and for KO\u2192EN data.", "labels": [], "entities": [{"text": "IWSLT14 DE\u2192EN data", "start_pos": 85, "end_pos": 103, "type": "DATASET", "confidence": 0.8430845022201539}]}, {"text": " Table 2: German\u2192English IWSLT results for training corpus size of 100k words and 3.2M words (full corpus).  Mean and standard deviation of three training runs reported.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.736244797706604}, {"text": "Mean", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9974688291549683}]}, {"text": " Table 3: Results on full IWSLT14 German\u2192English data on tokenized and lowercased test set with multi-bleu.perl.", "labels": [], "entities": [{"text": "IWSLT14 German\u2192English data", "start_pos": 26, "end_pos": 53, "type": "DATASET", "confidence": 0.7541289329528809}]}, {"text": " Table 5: Configurations of NMT systems reported in Table 2. Empty fields indicate that hyperparameter was  unchanged compared to previous systems.", "labels": [], "entities": [{"text": "hyperparameter", "start_pos": 88, "end_pos": 102, "type": "METRIC", "confidence": 0.9422588348388672}]}]}