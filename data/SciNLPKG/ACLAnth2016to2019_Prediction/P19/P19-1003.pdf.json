{"title": [{"text": "Improving Multi-turn Dialogue Modelling with Utterance ReWriter", "labels": [], "entities": [{"text": "Improving Multi-turn Dialogue Modelling", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8743002563714981}]}], "abstractContent": [{"text": "Recent research has made impressive progress in single-turn dialogue modelling.", "labels": [], "entities": [{"text": "single-turn dialogue modelling", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.796184241771698}]}, {"text": "In the multi-turn setting, however, current models are still far from satisfactory.", "labels": [], "entities": []}, {"text": "One major challenge is the frequently occurred coreference and information omission in our daily conversation , making it hard for machines to understand the real intention.", "labels": [], "entities": []}, {"text": "In this paper, we propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling.", "labels": [], "entities": [{"text": "multi-turn dialgoue modelling", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.609277198712031}]}, {"text": "Each utterance is first rewritten to recover all coreferred and omitted information.", "labels": [], "entities": []}, {"text": "The next processing steps are then performed based on the rewritten utterance.", "labels": [], "entities": []}, {"text": "To properly train the utterance rewriter, we collect anew dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network.", "labels": [], "entities": []}, {"text": "We show the proposed architecture achieves remarkably good performance on the utterance rewriting task.", "labels": [], "entities": [{"text": "utterance rewriting task", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.9095558325449625}]}, {"text": "The trained utterance rewriter can be easily integrated into online chatbots and brings general improvement over different domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialogue systems have made dramatic progress in recent years, especially in single-turn chit-chat and FAQ matching.", "labels": [], "entities": [{"text": "FAQ matching", "start_pos": 102, "end_pos": 114, "type": "TASK", "confidence": 0.8622069656848907}]}, {"text": "Nonethless, multi-turn dialogue modelling still remains extremely challenging (.", "labels": [], "entities": [{"text": "multi-turn dialogue modelling", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6973861654599508}]}, {"text": "One most important difficulty is the frequently occurred coreference and information omission in our daily conversations, especially in pro-drop languages like Chinese or Japanese.", "labels": [], "entities": []}, {"text": "From our preliminary study of 2,000 Chinese multi-turn con- * Both authors contributed equally.", "labels": [], "entities": []}, {"text": "The code is available on https://github.com/ chin-gyou/dialogue-utterance-rewriter.", "labels": [], "entities": []}], "datasetContent": [{"text": "To get parallel training data for the sentence rewriting, we crawled 200k candidate multi-turn conversational data from several popular Chinese social media platforms for human annotators to work on.", "labels": [], "entities": [{"text": "sentence rewriting", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7507977187633514}]}, {"text": "Sensitive information is filtered beforehand for later processing.", "labels": [], "entities": []}, {"text": "Before starting the annotation, we randomly sample 2,000 conversational data and analyze how often coreference and omission occurs in multi-turn dialogues.", "labels": [], "entities": []}, {"text": "As can be seen, only less than 30% utterances have neither coreference nor omission and quite a few utterances have both.", "labels": [], "entities": []}, {"text": "This further validates the importance of addressing the these situations in multi-turn dialogues.", "labels": [], "entities": []}, {"text": "In the annotation process, human annotators need to identify these two situations then rewrite the utterance to coverall hidden information.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "Annotators are required to provide the rewritten utterance 3 given the original conversation [utterance 1,2 and 3].", "labels": [], "entities": []}, {"text": "To ensure the annotation quality, 10% of the annotations from each annotator are daily examined by a project manager and feedbacks are provided.", "labels": [], "entities": []}, {"text": "The annotation is considered valid only when the accuracy of examined results surpasses 95%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9994980096817017}]}, {"text": "Apart from the accuracy examination, the project manage is also required to (1) select topics that are more likely to be talked about in daily conversations, (2) try to cover broader domains and (3) balance the proportion of different coreference and omission patterns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994089603424072}]}, {"text": "The whole annotation takes 4 months to finish.", "labels": [], "entities": []}, {"text": "In the end, we get 40k highquality parallel samples.", "labels": [], "entities": []}, {"text": "Half of them are negative samples which do not need any rewriting.", "labels": [], "entities": []}, {"text": "The other half are positive samples where rewriting is needed.", "labels": [], "entities": []}, {"text": "The rewritten utterance contains 10.5 tokens in average, reducing the context length by 80%.", "labels": [], "entities": []}, {"text": "Dataset size: 40,000 Avg.", "labels": [], "entities": [{"text": "Dataset", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6984798908233643}, {"text": "size", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.5945137739181519}, {"text": "Avg", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.966813862323761}]}, {"text": "length of original conversation: 48.8 Avg.", "labels": [], "entities": [{"text": "length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9886801242828369}, {"text": "Avg", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.981130063533783}]}, {"text": "length of rewritten utterance: 10.5: Statistics of dataset.", "labels": [], "entities": [{"text": "length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.955450713634491}]}, {"text": "Length is counted in the unit of Chinese characters.", "labels": [], "entities": [{"text": "Length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9779435396194458}]}, {"text": "We train our model to perform the utterance rewriting task on our collected dataset.", "labels": [], "entities": [{"text": "utterance rewriting", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.9218344688415527}]}, {"text": "In this section, we focus on answering the following two questions: (1) How accurately our proposed model can perform coreference resolution and information completion respectively and (2) How good the trained utterance rewriter is at helping off-theshelf dialogue systems provide more appropriate responses.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.9451002776622772}, {"text": "information completion", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.8050601482391357}]}, {"text": "To answer the first question, we compare our models with several strong baselines and test them by both automatic evaluation and human judgement.", "labels": [], "entities": []}, {"text": "For the second question, we integrate our rewriting model to two online dialogue systems and analyze how it affects the humancomputer interactions.", "labels": [], "entities": []}, {"text": "The following section will first introduce the compared models and basic settings, then report our evaluation results.", "labels": [], "entities": []}, {"text": "Transformer-based models We set the hidden size as 512.", "labels": [], "entities": []}, {"text": "The attention has 8 individual heads and the encoder/decoder have 6 individual stacked layers.", "labels": [], "entities": []}, {"text": "Models are optimized with the Adam optimizer.", "labels": [], "entities": []}, {"text": "The initial learning rate is 0.0001 and batch size is 64.", "labels": [], "entities": []}, {"text": "All hyperparameters are tuned base on the performance on the validation data.", "labels": [], "entities": []}, {"text": "LSTM-based Models We encode words with a single-layer bidirectional LSTM and decode with a uni-directional LSTM.", "labels": [], "entities": []}, {"text": "We use 128-dimensional word embeddings and 256-dimensional hidden states for both the encoder and decoder.", "labels": [], "entities": []}, {"text": "The batch size is set as 128.", "labels": [], "entities": []}, {"text": "Models are trained using Adagrad with learning rate 0.15 and initial accumulator value 0.1, same as in.", "labels": [], "entities": [{"text": "learning rate 0.15", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.957292397816976}, {"text": "initial accumulator value 0.1", "start_pos": 61, "end_pos": 90, "type": "METRIC", "confidence": 0.8524175882339478}]}, {"text": "General Setup We built our vocabulary based on character-based segmentation for Chinese scripts.", "labels": [], "entities": []}, {"text": "For non-Chinese characters, like frequently mentioned entity names \"Kobe\" and \"NBA\", we split them by space and keep all unique tokens which appear more than twice.", "labels": [], "entities": []}, {"text": "characters and 816 other tokens), including the end-of-turn delimiter and a special UNK token for all unknown words.", "labels": [], "entities": []}, {"text": "In the testing stage, all models decode words by beam search with beam size set to 4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Proportion of utterances containing corefer- ence and omission in multi-turn conversation", "labels": [], "entities": []}, {"text": " Table 1. Annotators are re- quired to provide the rewritten utterance 3 given  the original conversation [utterance 1,2 and 3]. To  ensure the annotation quality, 10% of the annota- tions from each annotator are daily examined by a  project manager and feedbacks are provided. The  annotation is considered valid only when the ac- curacy of examined results surpasses 95%. Apart  from the accuracy examination, the project man- age is also required to (1) select topics that are  more likely to be talked about in daily conversa- tions, (2) try to cover broader domains and (3) bal- ance the proportion of different coreference and  omission patterns. The whole annotation takes 4  months to finish. In the end, we get 40k high-", "labels": [], "entities": [{"text": "ac- curacy", "start_pos": 328, "end_pos": 338, "type": "METRIC", "confidence": 0.9524110754330953}, {"text": "accuracy", "start_pos": 390, "end_pos": 398, "type": "METRIC", "confidence": 0.999300479888916}]}, {"text": " Table 4: BLEU, ROUGE (F 1 ), and EM scores on the test set. EM score is split into the results on the positive (left)  and negative (right) test samples. The first half is LSTM-based models and the second half is Transformer-based.  Bold denotes best results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993364214897156}, {"text": "ROUGE (F 1 )", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.8351037979125977}, {"text": "EM", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9797505736351013}, {"text": "EM score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9213214814662933}]}, {"text": " Table 5: Precision, recall and F1 score of corefer- ence resolution. First row is the current state-of-the-art  coreference resolution model", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9966235160827637}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9983018636703491}, {"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9739030003547668}, {"text": "corefer- ence resolution", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8281381726264954}, {"text": "coreference resolution", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.8853670954704285}]}, {"text": " Table 6: Examples of rewritten utterances. Highlighted utterances are exactly the same as the ground truth.", "labels": [], "entities": []}, {"text": " Table 7: Recall, Precision, F1 score on information  completion and Human evaluation results on fluency.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9963982105255127}, {"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9985036849975586}, {"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9785397052764893}, {"text": "information  completion", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.8070507645606995}]}, {"text": " Table 8: Examples of integrated test. Left column is the original system and right is the one with utterance rewriter.  Blue words denote completed information by the utterance rewriter.", "labels": [], "entities": []}, {"text": " Table 9: Results of integrated testing. Intention  precision for task-oriented and conversation-turns-per- session (CPS) for chitchat.", "labels": [], "entities": [{"text": "Intention", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9984737038612366}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.6820036172866821}]}]}