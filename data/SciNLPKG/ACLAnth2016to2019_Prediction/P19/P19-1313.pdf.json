{"title": [], "abstractContent": [], "introductionContent": [], "datasetContent": [{"text": "To evaluate the efficacy of our method, we evaluate on several commonly-used hypernymy benchmarks (as described in) as well as in a reconstruction setting (as described in).", "labels": [], "entities": []}, {"text": "Following, we compare to the following methods for unsupervised hypernymy detection: be the set of Hearst patterns in our corpus, w(x, y) be the count of how many times (x, y) occurs in E, and W = \ud97b\udf59 (x,y)\u2208E w(x, y).", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8107624650001526}]}, {"text": "We then consider the following pattern-based methods: \ud97b\udf59 SVD Count (sp) To account for missing relations, we also compare against low-rank embeddings of the Hearst corpus using Singular Value Decomposition (SVD).", "labels": [], "entities": [{"text": "SVD Count (sp)", "start_pos": 56, "end_pos": 70, "type": "METRIC", "confidence": 0.6998228549957275}, {"text": "Hearst corpus", "start_pos": 156, "end_pos": 169, "type": "DATASET", "confidence": 0.8099063336849213}]}, {"text": "Specifically, let X \u2208 R M xM , such that X ij = w(i, j)/W and U \u03a3V \u22a4 be the singular value decomposition of X, then: SVD PPMI (spmi) We also evaluate against the SVD of the PPMI matrix, which is identical to sp(i, j), with the exception that X ij = ppmi(i, j), instead of p(i, j).", "labels": [], "entities": []}, {"text": "showed that this method provides state-of-the-art results for unsupervised hypernymy detection.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7451159954071045}]}, {"text": "Hyperbolic Embeddings (HypeCones) We embed the Hearst graph into hyperbolic space as described in Section 3.2.", "labels": [], "entities": []}, {"text": "At evaluation time, we predict the likelihood using the model energy E(u, v).", "labels": [], "entities": [{"text": "likelihood", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9899877905845642}, {"text": "E", "start_pos": 69, "end_pos": 70, "type": "METRIC", "confidence": 0.779532253742218}]}, {"text": "Distributional Models The distributional models in our evaluation are based on the DIH, i.e., the idea that contexts in which a narrow term x (ex: cat) may appear should be a subset of the contexts in which a broader term y (ex: animal) may appear.", "labels": [], "entities": []}, {"text": "WeedsPrec The first distributional model we consider is WeedsPrec (), which captures the features of x which are included in the set of more general term's features, y: invCL, introduce the idea of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term.", "labels": [], "entities": []}, {"text": "The degree of inclusion is denoted as: To measure the inclusion of x and y and the noninclusion of yin x, invCL is then computed as SLQS The SLQS model is based on the informativeness hypothesis (, i.e., the idea that general words appear mostly in uninformative contexts, as measured by entropy.", "labels": [], "entities": []}, {"text": "SLQS depends on the median entropy of a term's top k contexts: where H(c i ) is the Shannon entropy of context c i across all terms.", "labels": [], "entities": []}, {"text": "SLQS is then defined as: Hypernymy Tasks We consider three distinct subtasks for evaluating the performance of these models for hypernymy prediction: \u2022 Detection: Given a pair of words (u, v), determine if v is a hypernym of u.", "labels": [], "entities": [{"text": "hypernymy prediction", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.7553305923938751}]}, {"text": "\u2022 Direction: Given a pair (u, v), determine if u is more general than v or vise versa.", "labels": [], "entities": []}, {"text": "\u2022 Graded Entailment: Given a pair of words (u, v), determine the degree to which u is av. shows the results for all tasks.", "labels": [], "entities": []}, {"text": "It can be seen that our proposed approach provides substantial gains on the detection and directionality tasks and, overall, achieves state of the art results on seven of nine benchmarks.", "labels": [], "entities": []}, {"text": "In addition, our method clearly outperforms other embeddingbased approaches on HYPERLEX, although it cannot fully match the count-based methods.", "labels": [], "entities": []}, {"text": "As noted, this might bean artifact of the evaluation metric, as count-based methods benefit from their sparse-predictions in this setting.", "labels": [], "entities": []}, {"text": "Our method achieves also strong performance when compared to Poincar\u00e9 GLOVE on the task of hypernymy prediction.", "labels": [], "entities": [{"text": "GLOVE", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.733168363571167}, {"text": "hypernymy prediction", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7529707849025726}]}, {"text": "An additional benefit is the efficiency of our embeddings.", "labels": [], "entities": []}, {"text": "For all tasks, we have used a 20-dimensional embedding for HYPECONES, while the best results for SVD-based methods have been achieved with 300 dimensions.", "labels": [], "entities": []}, {"text": "This reduction in parameters by over an order of magnitude clearly highlights the efficiency of hyperbolic embeddings for representing hierarchical structures.", "labels": [], "entities": []}, {"text": "Reconstruction In the following, we compare embedding and pattern-based methods on the task of reconstructing an entire subtree of WORDNET, i.e., the animals, plants, and vehicles taxonomies, as proposed by.", "labels": [], "entities": []}, {"text": "In addition to predicting the existence of single hypernymy relations, this allows us to evaluate the performance of these models for inferring full taxonomies and to perform an ablation for the prediction of missing and transitive relations.", "labels": [], "entities": []}, {"text": "We follow previous work and report for each observed relation (u, v) in WORDNET, its score ranked against the score of the ground-truth negative edges.", "labels": [], "entities": [{"text": "WORDNET", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.8660486936569214}]}, {"text": "In, All refers to the ranking of all edges in the subtree, Missing to edges that are not included in the Hearst graph G, Transitive to missing transitive edges in G (i.e. for all edges {(x, z) : (x, y), (y, z) \u2208 E \u2227 (x, z) / \u2208 E}).", "labels": [], "entities": []}, {"text": "It can be seen that our method clearly outperforms the SVD and count-based models with a relative improvement of typically over 40% over the best non-hyperbolic model.", "labels": [], "entities": []}, {"text": "Furthermore, our ablation shows that HYPECONES improves the consistency of the embedding due to its transitivity property.", "labels": [], "entities": [{"text": "HYPECONES", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.7620903253555298}, {"text": "consistency", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9933928847312927}]}, {"text": "For instance, in our Hearst Graph the relation (male horse, is-a, equine) is missing.", "labels": [], "entities": []}, {"text": "However, since we correctly model that (male horse, is-a, horse) and (horse, is-a, equine), by transitivity, we also infer (male horse, is-a, equine), which SVD fails to do.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results comparing distributional and pattern-based methods in all settings.", "labels": [], "entities": []}, {"text": " Table 3: Reconstruction of Animals, Plants, and Vehicles subtrees in WORDNET.", "labels": [], "entities": [{"text": "WORDNET", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.8993221521377563}]}]}