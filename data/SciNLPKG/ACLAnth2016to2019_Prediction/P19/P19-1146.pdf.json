{"title": [], "abstractContent": [{"text": "Sequence-to-sequence models area powerful workhorse of NLP.", "labels": [], "entities": []}, {"text": "Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities.", "labels": [], "entities": []}, {"text": "This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs.", "labels": [], "entities": []}, {"text": "In this paper , we propose sparse sequence-to-sequence models, rooted in anew family of \u03b1-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any \u03b1 > 1.", "labels": [], "entities": []}, {"text": "We provide fast algorithms to evaluate these transformations and their gradients , which scale well for large vocabulary sizes.", "labels": [], "entities": []}, {"text": "Our models are able to produce sparse alignments and to assign nonzero probability to a shortlist of plausible outputs, sometimes rendering beam search exact.", "labels": [], "entities": []}, {"text": "Experiments on morphological inflection and machine translation reveal consistent gains over dense models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7885815799236298}]}], "introductionContent": [{"text": "Attention-based sequence-to-sequence (seq2seq) models have proven useful fora variety of NLP applications, including machine translation (, speech recognition (, abstractive summarization (, and morphological inflection generation (, among others.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.8487550616264343}, {"text": "speech recognition", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.7909959256649017}, {"text": "abstractive summarization", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.5301241874694824}, {"text": "morphological inflection generation", "start_pos": 195, "end_pos": 230, "type": "TASK", "confidence": 0.6861502130826315}]}, {"text": "In part, their strength comes from their flexibility: many tasks can be formulated as transducing a source sequence into a target sequence of possibly different length.", "labels": [], "entities": []}, {"text": "However, conventional seq2seq models are dense: they compute both attention weights and output probabilities with the softmax function, which always returns positive values.", "labels": [], "entities": []}, {"text": "This results in dense attention alignments, in which each source position is attended to at each d r aw ed </s> n </s> </s> 66.4% 32.2%", "labels": [], "entities": []}], "datasetContent": [{"text": "The previous section establishes the computational building blocks required to train models with entmax sparse attention and loss functions.", "labels": [], "entities": []}, {"text": "We now put them to use for two important NLP tasks, morphological inflection and machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.8111661374568939}]}, {"text": "These two tasks highlight the characteristics of our innovations in different ways.", "labels": [], "entities": []}, {"text": "Morphological inflection is a character-level task with mostly monotonic alignments, but the evaluation demands exactness: the predicted sequence must match the gold standard.", "labels": [], "entities": [{"text": "Morphological inflection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.886737048625946}, {"text": "exactness", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.972711443901062}]}, {"text": "On the other hand, machine translation uses a word-level vocabulary orders of magnitude larger and forces a sparse output layer to confront more ambiguity: any sentence has several valid translations and it is not clear beforehand that entmax will manage this well.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7975305914878845}]}, {"text": "Despite the differences between the tasks, we keep the architecture and training procedure as similar as possible.", "labels": [], "entities": []}, {"text": "We use two layers for encoder and decoder LSTMs and apply dropout with probability 0.3.", "labels": [], "entities": []}, {"text": "We train with Adam (, with abase learning rate of 0.001, halved whenever the loss increases on the validation set.", "labels": [], "entities": []}, {"text": "We use a batch size of 64.", "labels": [], "entities": []}, {"text": "At test time, we select the model with the best validation accuracy and decode with abeam size of 5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.7913105487823486}]}, {"text": "We implemented all models with OpenNMT-py (.", "labels": [], "entities": []}, {"text": "In our primary experiments, we use three \u03b1 values for the attention and loss functions: \u03b1 = 1 (softmax), \u03b1 = 1.5 (to which our novel Algorithm 2 applies), and \u03b1 = 2 (sparsemax).", "labels": [], "entities": []}, {"text": "We also investigate the effect of tuning \u03b1 with increased granularity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average per-language accuracy on the test set  (CoNLL-SIGMORPHON 2018 task 1) averaged or en- sembled over three runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8103322982788086}, {"text": "CoNLL-SIGMORPHON 2018 task 1)", "start_pos": 58, "end_pos": 87, "type": "DATASET", "confidence": 0.9026868104934692}]}, {"text": " Table 2: Machine translation comparison of softmax, sparsemax, and the proposed 1.5-entmax as both attention  mapping and loss function. Reported is tokenized test BLEU averaged across three runs (higher is better).", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8460714519023895}, {"text": "BLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9975796341896057}]}]}