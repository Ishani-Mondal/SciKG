{"title": [{"text": "Simple and Effective Text Matching with Richer Alignment Features", "labels": [], "entities": [{"text": "Text Matching", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7341527491807938}]}], "abstractContent": [{"text": "In this paper, we present a fast and strong neu-ral approach for general purpose text matching applications.", "labels": [], "entities": [{"text": "general purpose text matching", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.5972687751054764}]}, {"text": "We explore what is sufficient to build a fast and well-performed text matching model and propose to keep three key features available for inter-sequence alignment: original point-wise features, previous aligned features , and contextual features while simplifying all the remaining components.", "labels": [], "entities": [{"text": "text matching", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7652117609977722}, {"text": "inter-sequence alignment", "start_pos": 138, "end_pos": 162, "type": "TASK", "confidence": 0.6766340732574463}]}, {"text": "We conduct experiments on four well-studied benchmark datasets across tasks of natural language inference , paraphrase identification and answer selection.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.8941396772861481}, {"text": "answer selection", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.8735604584217072}]}, {"text": "The performance of our model is on par with the state-of-the-art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text matching is a core research area in natural language processing with along history.", "labels": [], "entities": [{"text": "Text matching", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8408472836017609}]}, {"text": "In text matching tasks, a model takes two text sequences as input and predicts a category or a scala value indicating their relationship.", "labels": [], "entities": [{"text": "text matching tasks", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8085802594820658}]}, {"text": "A wide range of tasks, including natural language inference (also known as recognizing textual entailment), paraphrase identification ( , answer selection (, and soon, can be seen as specific forms of text matching problems.", "labels": [], "entities": [{"text": "recognizing textual entailment)", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.7436083033680916}, {"text": "paraphrase identification", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.8626909554004669}, {"text": "answer selection", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.8311952352523804}, {"text": "text matching", "start_pos": 201, "end_pos": 214, "type": "TASK", "confidence": 0.7332897186279297}]}, {"text": "Research on general purpose text matching algorithm is beneficial to a large number of relevant applications.", "labels": [], "entities": [{"text": "general purpose text matching algorithm", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.6753840565681457}]}, {"text": "Deep neural networks are the most popular choices for text matching nowadays.", "labels": [], "entities": [{"text": "text matching", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.8797198235988617}]}, {"text": "Semantic alignment and comparison of two text sequences are the keys in neural text matching.", "labels": [], "entities": [{"text": "Semantic alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8174643516540527}, {"text": "neural text matching", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.6741552253564199}]}, {"text": "Many previous deep neural networks contain a single intersequence alignment layer.", "labels": [], "entities": []}, {"text": "To make full use of this only alignment process, the model has to take rich external syntactic features or hand-designed alignment features as additional inputs of the alignment layer (, adopt a complicated alignment mechanism (, or build avast amount of post-processing layers to analyze the alignment result (.", "labels": [], "entities": []}, {"text": "More powerful models can be built with multiple inter-sequence alignment layers.", "labels": [], "entities": []}, {"text": "Instead of making a prediction based on the comparison result of a single alignment process, a stacked model with multiple alignment layers maintains its intermediate states and gradually refines its predictions.", "labels": [], "entities": []}, {"text": "However, suffering from inefficient propagation of lower-level features and vanishing gradients, these deeper architectures are harder to train.", "labels": [], "entities": []}, {"text": "Recent works have come up with ways of connecting stacked building blocks including dense connection ( and recurrent neural networks (, which strengthen the propagation of lower-level features and yield better results than those with a single alignment process.", "labels": [], "entities": []}, {"text": "This paper presents RE2, a fast and strong neural architecture with multiple alignment processes for general purpose text matching.", "labels": [], "entities": [{"text": "general purpose text matching", "start_pos": 101, "end_pos": 130, "type": "TASK", "confidence": 0.638506792485714}]}, {"text": "We question the necessity of many slow components in text matching approaches presented in previous literature, including complicated multi-way alignment mechanisms, heavy distillations of alignment results, external syntactic features, or dense connections to connect stacked blocks when the model is going deep.", "labels": [], "entities": [{"text": "text matching", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.7617268562316895}]}, {"text": "These design choices slowdown the model by a large amount and can be replaced by much more lightweight and equally effective ones.", "labels": [], "entities": []}, {"text": "Meanwhile, we highlight three key components for an efficient text matching model.", "labels": [], "entities": [{"text": "text matching", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.780664324760437}]}, {"text": "These components, which the name RE2 stands for, are previous aligned features (Residual vectors), original point-wise features (Embedding vectors), and contextual features (Encoded vectors).", "labels": [], "entities": []}, {"text": "The re-maining components can be as simple as possible to keep the model fast while still yielding strong performance.", "labels": [], "entities": []}, {"text": "The general architecture of RE2 is illustrated in.", "labels": [], "entities": [{"text": "RE2", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.7296327948570251}]}, {"text": "An embedding layer first embeds discrete tokens.", "labels": [], "entities": []}, {"text": "Several same-structured blocks consisting of encoding, alignment and fusion layers then process the sequences consecutively.", "labels": [], "entities": []}, {"text": "These blocks are connected by an augmented version of residual connections (see section 2.1).", "labels": [], "entities": []}, {"text": "A pooling layer aggregates sequential representations into vectors which are finally processed by a prediction layer to give the final prediction.", "labels": [], "entities": []}, {"text": "The implementation of each layer is kept as simple as possible, and the whole model, as a well-organized combination, is quite powerful and lightweight at the same time.", "labels": [], "entities": []}, {"text": "Our proposed method achieves the performance on par with the state-of-the-art on four benchmark datasets across three different tasks, namely SNLI and SciTail for natural language inference, Quora Question Pairs for paraphrase identification, and WikiQA for answer selection.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 216, "end_pos": 241, "type": "TASK", "confidence": 0.8905268311500549}, {"text": "answer selection", "start_pos": 258, "end_pos": 274, "type": "TASK", "confidence": 0.8717775344848633}]}, {"text": "Furthermore, our model has the least number of parameters and the fastest inference speed in all similarlyperformed models.", "labels": [], "entities": []}, {"text": "We also conduct an ablation study to compare with alternative implementations of most components, perform robustness checks to see whether the model is robust to changes of structural hyperparameters, explore what roles the three key features in RE2 play by comparing their occlusion sensitivity and show the evolution of alignment results by a case study.", "labels": [], "entities": []}, {"text": "We release the source code 1 of our experiments for reproducibility and hope to facilitate future researches.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we briefly introduce datasets used in the experiments and their evaluation metrics.", "labels": [], "entities": []}, {"text": "SNLI (Bowman et al., 2015) (Stanford Natural Language Inference) is a benchmark dataset for natural language inference.", "labels": [], "entities": [{"text": "SNLI (Bowman et al., 2015) (Stanford Natural Language Inference)", "start_pos": 0, "end_pos": 64, "type": "DATASET", "confidence": 0.8130545914173126}, {"text": "natural language inference", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.6290694276491801}]}, {"text": "In natural language inference tasks, the two input sentences are asymmetrical.", "labels": [], "entities": []}, {"text": "The first one is called \"premise\" and the second is called \"hypothesis\".", "labels": [], "entities": []}, {"text": "The dataset contains 570k human annotated sentence pairs from an image captioning corpus, with labels \"entailment\", \"neutral\", \"contradiction\" and \"-\".", "labels": [], "entities": []}, {"text": "The \"-\" label indicates that the annotators cannot reach an agreement, so we ignore text pairs with this kind of labels in training and testing following.", "labels": [], "entities": []}, {"text": "We use the same dataset split as in the original paper.", "labels": [], "entities": []}, {"text": "Accuracy is used as the evaluation metric for this dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9921755194664001}]}, {"text": "SciTail () (Science Entailment) is an entailment classification dataset constructed from science questions and answers.", "labels": [], "entities": []}, {"text": "Since scientific facts cannot contradict with each other, this dataset contains only two types of labels, entailment and neutral.", "labels": [], "entities": []}, {"text": "We use the original dataset partition.", "labels": [], "entities": []}, {"text": "This dataset contains 27k examples in total.", "labels": [], "entities": []}, {"text": "10k examples are with entailment labels and the remaining 17k are labeled as neutral.", "labels": [], "entities": []}, {"text": "Accuracy is used as the evaluation metric for this dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9921755194664001}]}, {"text": "Quora Question Pairs 2 is a dataset for paraphrase identification with two classes indicating whether one question is a paraphrase of the other.", "labels": [], "entities": [{"text": "Quora Question Pairs 2", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.7050878405570984}, {"text": "paraphrase identification", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.8807869255542755}]}, {"text": "The dataset contains more than 400k real question pairs collected from Quora.com.", "labels": [], "entities": [{"text": "Quora.com", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.9472475051879883}]}, {"text": "We use the same dataset partition as mentioned in . Accuracy is used as the evaluation metric for this dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9968492388725281}]}, {"text": "WikiQA () is a retrieval-based question answering dataset based on Wikipedia.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9312000870704651}]}, {"text": "It contains questions and their candidate answers, with binary labels indicating whether a candidate sentence is a correct answer to the question it belongs to.", "labels": [], "entities": []}, {"text": "This dataset has 20.4k training pairs, 2.7k development pairs, and 6.2k testing pairs.", "labels": [], "entities": []}, {"text": "Mean average precision (MAP) and mean reciprocal rank (MRR) are used as the evaluation metrics for this task.", "labels": [], "entities": [{"text": "Mean average precision (MAP)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.9310252666473389}, {"text": "mean reciprocal rank (MRR)", "start_pos": 33, "end_pos": 59, "type": "METRIC", "confidence": 0.8973153034845988}]}], "tableCaptions": [{"text": " Table 1: Experimental results on SNLI test set.", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8566144506136576}]}, {"text": " Table 2: Experimental results on SciTail test set.", "labels": [], "entities": [{"text": "SciTail test set", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.8373381694157919}]}, {"text": " Table 3: Experimental results on Quora test set.", "labels": [], "entities": [{"text": "Quora test set", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9466086427370707}]}, {"text": " Table 4: Experimental results on WikiQA test set.", "labels": [], "entities": [{"text": "WikiQA test set", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9645527998606364}]}, {"text": " Table 6: Ablation study on dev sets of the correspond- ing datasets.", "labels": [], "entities": []}, {"text": " Table 7: Robustness checks on dev sets of the corre- sponding datasets.", "labels": [], "entities": []}]}