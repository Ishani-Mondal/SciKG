{"title": [{"text": "Multi-Task Deep Neural Networks for Natural Language Understanding", "labels": [], "entities": [{"text": "Natural Language Understanding", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.7005236148834229}]}], "abstractContent": [{"text": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks.", "labels": [], "entities": [{"text": "learning representations across multiple natural language understanding (NLU) tasks", "start_pos": 72, "end_pos": 155, "type": "TASK", "confidence": 0.68556255644018}]}, {"text": "MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.7844910025596619}]}, {"text": "MT-DNN extends the model proposed in Liu et al.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6061121225357056}]}, {"text": "(2015) by incorporating a pre-trained bidirec-tional transformer language model, known as BERT (Devlin et al., 2018).", "labels": [], "entities": [{"text": "BERT", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9969661831855774}]}, {"text": "MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) 1.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6964929699897766}, {"text": "SNLI", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.8170207142829895}]}, {"text": "We also demonstrate using the SNLI and Sc-iTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8819162845611572}, {"text": "Sc-iTail datasets", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.7701541781425476}, {"text": "domain adaptation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7437805831432343}]}, {"text": "The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning vector-space representations of text, e.g., words and sentences, is fundamental to many natural language understanding (NLU) tasks.", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 97, "end_pos": 133, "type": "TASK", "confidence": 0.8255997598171234}]}, {"text": "Two popular approaches are multi-task learning and language model pre-training.", "labels": [], "entities": []}, {"text": "In this paper we combine the strengths of both approaches by proposing anew Multi-Task Deep Neural Network (MT-DNN).", "labels": [], "entities": []}, {"text": "Multi-Task Learning (MTL) is inspired by human learning activities where people often apply the knowledge learned from previous tasks to help learn anew task.", "labels": [], "entities": [{"text": "Multi-Task Learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8192878842353821}]}, {"text": "For example, it is easier fora person who knows how to ski to learn skating than the one who * Equal Contribution.", "labels": [], "entities": [{"text": "Equal", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9819881319999695}]}, {"text": "As of February 25, 2019 on the latest GLUE test set.", "labels": [], "entities": [{"text": "GLUE test set", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9396888812383016}]}, {"text": "Similarly, it is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks.", "labels": [], "entities": []}, {"text": "Recently, there is a growing interest in applying MTL to representation learning using deep neural networks (DNNs)) for two reasons.", "labels": [], "entities": [{"text": "MTL", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.964805006980896}, {"text": "representation learning", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.9081445634365082}]}, {"text": "First, supervised learning of DNNs requires large amounts of task-specific labeled data, which is not always available.", "labels": [], "entities": []}, {"text": "MTL provides an effective way of leveraging supervised data from many related tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.647754967212677}]}, {"text": "Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks.", "labels": [], "entities": []}, {"text": "In contrast to MTL, language model pretraining has shown to be effective for learning universal language representations by leveraging large amounts of unlabeled data.", "labels": [], "entities": [{"text": "MTL", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8894385695457458}]}, {"text": "A recent survey is included in . Some of the most prominent examples are,) and BERT).", "labels": [], "entities": [{"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.997972309589386}]}, {"text": "These are neural network language models trained on text data using unsupervised objectives.", "labels": [], "entities": []}, {"text": "For example, BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9482055902481079}, {"text": "masked word prediction", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.6358233988285065}, {"text": "next sentence prediction", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.5901473065217336}]}, {"text": "To apply a pre-trained model to specific NLU tasks, we often need to fine-tune, for each task, the model with additional task-specific layers using task-specific training data.", "labels": [], "entities": []}, {"text": "For example, shows that BERT can be fine-tuned this way to create state-of-the-art models fora range of NLU tasks, such as question answering and natural language inference.", "labels": [], "entities": [{"text": "BERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9758014678955078}, {"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.8883794546127319}, {"text": "natural language inference", "start_pos": 146, "end_pos": 172, "type": "TASK", "confidence": 0.6016444265842438}]}, {"text": "We argue that MTL and language model pretraining are complementary technologies, and can be combined to improve the learning of text rep-resentations to boost the performance of various NLU tasks.", "labels": [], "entities": []}, {"text": "To this end, we extend the MT-DNN model originally proposed in by incorporating BERT as its shared text encoding layers.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 27, "end_pos": 33, "type": "TASK", "confidence": 0.8355369567871094}, {"text": "BERT", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9805732369422913}]}, {"text": "As shown in, the lower layers (i.e., text encoding layers) are shared across all tasks, while the top layers are task-specific, combining different types of NLU tasks such as single-sentence classification, pairwise text classification, text similarity, and relevance ranking.", "labels": [], "entities": [{"text": "single-sentence classification", "start_pos": 175, "end_pos": 205, "type": "TASK", "confidence": 0.7670066654682159}, {"text": "pairwise text classification", "start_pos": 207, "end_pos": 235, "type": "TASK", "confidence": 0.5816193024317423}, {"text": "relevance ranking", "start_pos": 258, "end_pos": 275, "type": "TASK", "confidence": 0.7383115887641907}]}, {"text": "Similar to the BERT model, MT-DNN can be adapted to a specific task via fine-tuning.", "labels": [], "entities": [{"text": "BERT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9867323040962219}]}, {"text": "Unlike BERT, MT-DNN uses MTL, in addition to language model pre-training, for learning text representations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8669293522834778}]}, {"text": "MT-DNN obtains new state-of-the-art results on eight out of nine NLU tasks 2 used in the General Language Understanding Evaluation (GLUE) benchmark (, pushing the GLUE benchmark score to 82.7%, amounting to 2.2% absolute improvement over BERT.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.6427595615386963}, {"text": "BERT", "start_pos": 238, "end_pos": 242, "type": "METRIC", "confidence": 0.9946084022521973}]}, {"text": "We further extend the superiority of MT-DNN to the SNLI (Bowman et al., 2015a) and SciTail ( tasks.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 37, "end_pos": 43, "type": "TASK", "confidence": 0.7902008891105652}, {"text": "SNLI", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.7854674458503723}]}, {"text": "The representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 31, "end_pos": 37, "type": "TASK", "confidence": 0.7307804822921753}, {"text": "domain adaptation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7858922779560089}, {"text": "BERT", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9357349276542664}]}, {"text": "For example, our adapted models achieve the accuracy of 91.6% on SNLI and 95.0% on SciTail, outperforming the previous state-ofthe-art performance by 1.5% and 6.7%, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9996957778930664}, {"text": "SNLI", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9431204199790955}]}, {"text": "Even with only 0.1% or 1.0% of the original training data, the performance of MT-DNN on both SNLI and SciTail datasets is better than many existing models.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 78, "end_pos": 84, "type": "TASK", "confidence": 0.8934072256088257}, {"text": "SNLI and SciTail datasets", "start_pos": 93, "end_pos": 118, "type": "DATASET", "confidence": 0.6593374237418175}]}, {"text": "All of these clearly demonstrate MT-DNN's exceptional generalization capability via multi-task learning.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 33, "end_pos": 39, "type": "TASK", "confidence": 0.9506619572639465}]}], "datasetContent": [{"text": "We evaluate the proposed MT-DNN on three popular NLU benchmarks: GLUE (), SNLI (Bowman et al., 2015b), and SciTail ().", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 25, "end_pos": 31, "type": "TASK", "confidence": 0.9525442123413086}, {"text": "GLUE", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9364782571792603}]}, {"text": "We compare MT-DNN with existing state-of-the-art models including BERT and demonstrate the effectiveness of MTL with and without model fine-tuning using GLUE and domain adaptation using both SNLI and SciTail.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 11, "end_pos": 17, "type": "TASK", "confidence": 0.8613255620002747}, {"text": "BERT", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9869866967201233}, {"text": "GLUE", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.8610966205596924}, {"text": "domain adaptation", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.6893010586500168}, {"text": "SNLI", "start_pos": 191, "end_pos": 195, "type": "DATASET", "confidence": 0.8875924944877625}]}, {"text": "This section briefly describes the GLUE, SNLI, and SciTail datasets, as summarized in.", "labels": [], "entities": [{"text": "GLUE", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9059440493583679}, {"text": "SNLI", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.6213691234588623}, {"text": "SciTail datasets", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.9157024919986725}]}], "tableCaptions": [{"text": " Table 1: Summary of the three benchmarks: GLUE, SNLI and SciTail.", "labels": [], "entities": [{"text": "GLUE", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8382090926170349}, {"text": "SNLI", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.811261773109436}]}, {"text": " Table 2: GLUE test set results scored using the GLUE evaluation server. The number below each task denotes the  number of training examples. The state-of-the-art results are in bold, and the results on par with or pass human  performance are in bold. MT-DNN uses BERT LARGE to initialize its shared layers. All the results are obtained  from https://gluebenchmark.com/leaderboard on February 25, 2019. Model references: 1 :(Wang et al., 2018) ;  2 :(Radford et al., 2018); 3 : (Phang et al., 2018); 4 :(Devlin et al., 2018).", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 252, "end_pos": 258, "type": "TASK", "confidence": 0.7689829468727112}, {"text": "BERT LARGE", "start_pos": 264, "end_pos": 274, "type": "METRIC", "confidence": 0.9033628404140472}]}, {"text": " Table 4: Domain adaptation results on SNLI and Sci- Tail, as shown in", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.836552232503891}, {"text": "SNLI", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.6290027499198914}]}, {"text": " Table 5: Results on the SNLI and SciTail dataset.  Previous state-of-the-art results are marked by   * , obtained from the official SNLI leaderboard  (https://nlp.stanford.edu/projects/snli/)  and  the  official SciTail leaderboard maintained by AI2  (https://leaderboard.allenai.org/scitail).", "labels": [], "entities": [{"text": "SNLI and SciTail dataset", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.7251216620206833}, {"text": "SNLI leaderboard", "start_pos": 133, "end_pos": 149, "type": "DATASET", "confidence": 0.9282844066619873}]}]}