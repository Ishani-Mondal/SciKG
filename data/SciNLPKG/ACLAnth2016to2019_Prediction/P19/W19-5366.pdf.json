{"title": [{"text": "JHU 2019 Robustness Task System Description", "labels": [], "entities": [{"text": "JHU 2019 Robustness Task System Description", "start_pos": 0, "end_pos": 43, "type": "DATASET", "confidence": 0.8285799622535706}]}], "abstractContent": [{"text": "We describe the JHU submissions to the French-English, Japanese-English, and English-Japanese Robustness Task at WMT 2019.", "labels": [], "entities": [{"text": "JHU", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8213645815849304}, {"text": "English-Japanese Robustness Task at WMT 2019", "start_pos": 77, "end_pos": 121, "type": "DATASET", "confidence": 0.5897830327351888}]}, {"text": "Our goal was to evaluate the performance of baseline systems on both the official noisy test set as well as news data, in order to ensure that performance gains in the latter did not come at the expense of general-domain performance.", "labels": [], "entities": []}, {"text": "To this end, we built straightforward 6-layer Transformer models and experimented with a handful of variables including subword processing (FR-EN) and a handful of hyperparameters settings (JA\u2194EN).", "labels": [], "entities": [{"text": "FR-EN", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.9787377119064331}]}, {"text": "As expected, our systems performed reasonably.", "labels": [], "entities": []}], "introductionContent": [{"text": "The team at JHU submitted three systems to the WMT19 Robustness task: French-English, Japanese-English, and English-Japanese.", "labels": [], "entities": [{"text": "JHU", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.963797390460968}, {"text": "WMT19 Robustness task", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.769616981347402}]}, {"text": "Our goal was to evaluate the performance of reasonable state-of-the-art systems against both the robustness test set as well as more standard \"general domain\" test sets.", "labels": [], "entities": []}, {"text": "We believe this is an important component of evaluating for actual robustness.", "labels": [], "entities": []}, {"text": "In this way, we ensure that performance gains on robustness data are not purchased at the expense of this general-domain performance.", "labels": [], "entities": []}, {"text": "Our systems used no monolingual data and relatively straightforward state-of-the-art techniques, and produced systems of roughly average performance.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: French-English translation results.", "labels": [], "entities": [{"text": "French-English translation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.512206956744194}]}, {"text": " Table 3: BLEU scores with the sentencepiece models  and no other preprocessing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985962510108948}]}, {"text": " Table 4: Datasets for English-Japanese systems. Word  counts are source side only.", "labels": [], "entities": []}, {"text": " Table 6: Continued Training BLEU results on Test18- MTNT. Stage 1 results are from Table 5. Continued  Training (Stage 2) consistently improves BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9871113896369934}, {"text": "Test18- MTNT", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8108210961023966}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9987514019012451}]}]}