{"title": [{"text": "Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums", "labels": [], "entities": [{"text": "Question Analysis", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7703649699687958}]}], "abstractContent": [{"text": "Teaching machines to ask questions is an important yet challenging task.", "labels": [], "entities": []}, {"text": "Most prior work focused on generating questions with fixed answers.", "labels": [], "entities": []}, {"text": "As contents are highly limited by given answers, these questions are often not worth discussing.", "labels": [], "entities": []}, {"text": "In this paper, we take the first step on teaching machines to ask open-answered questions from real-world news for open discussion (openQG).", "labels": [], "entities": []}, {"text": "To generate high-qualified questions, effective ways for question evaluation are required.", "labels": [], "entities": [{"text": "question evaluation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7519629299640656}]}, {"text": "We take the perspective that the more answers a question receives, the better it is for open discussion, and analyze how language use affects the number of answers.", "labels": [], "entities": []}, {"text": "Compared with other factors, e.g. topic and post time, linguistic factors keep our evaluation from being domain-specific.", "labels": [], "entities": []}, {"text": "We carefully perform variable control on 11.5M questions from online forums to get a dataset, OQRanD, and further perform question analysis.", "labels": [], "entities": [{"text": "OQRanD", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.48423901200294495}]}, {"text": "Based on these conclusions, several models are built for question evaluation.", "labels": [], "entities": [{"text": "question evaluation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.809333086013794}]}, {"text": "For openQG task, we construct OQGenD, the first dataset as far as we know, and propose a model based on conditional generative adversarial networks and our question evaluation model.", "labels": [], "entities": []}, {"text": "Experiments show that our model can generate questions with higher quality compared with commonly-used text generation methods.", "labels": [], "entities": [{"text": "text generation", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.7494272589683533}]}], "introductionContent": [{"text": "Teaching machines to ask questions from given corpus, i.e. question generation (QG), is an important yet challenging task in natural language processing.", "labels": [], "entities": [{"text": "question generation (QG)", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8167397379875183}, {"text": "natural language processing", "start_pos": 125, "end_pos": 152, "type": "TASK", "confidence": 0.6389029820760092}]}, {"text": "In recent years, QG has received increasing attention from both the industrial and academic communities due to its wide applications.", "labels": [], "entities": []}, {"text": "Dialog systems can be proactive by asking users questions ( , question answering (QA) systems can benefit from the corpus produced by a QG model (, education and clinical ( systems require QG as well.", "labels": [], "entities": [{"text": "question answering", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.6983975023031235}]}, {"text": "We can divide all questions into two categories.", "labels": [], "entities": []}, {"text": "Fixed-answered questions have standard answers, e.g. \"who invented the car?", "labels": [], "entities": []}, {"text": "In contrast, different people may have distinct answers over open-answered questions like \"what do you think of the self-driving car?\".", "labels": [], "entities": []}, {"text": "Most prior work about QG (QA) aimed to generate (answer) fixedanswered questions.", "labels": [], "entities": [{"text": "QG (QA)", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.5840990915894508}]}, {"text": "As questions are targeting on answers which are certain spans of given corpus, they are always not worth discussing.", "labels": [], "entities": []}, {"text": "Nowadays, with the help of online QA forums (e.g. Quora and Zhihu ), open-answered questions can greatly arouse open discussion that helps people under different backgrounds to share knowledge and ideas (high-qualified questions can help to attract more visitors for QA forums as well).", "labels": [], "entities": [{"text": "Quora", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9245824217796326}]}, {"text": "This kind of questions are also useful for many tasks, e.g. making dialog systems more proactive.", "labels": [], "entities": []}, {"text": "In this paper, we focus on generating openanswered questions for open discussion, i.e. the openQG task.", "labels": [], "entities": []}, {"text": "To make our model useful in practice, we generate questions from real-world news which are suitable for arousing open discussion.", "labels": [], "entities": []}, {"text": "As far as we know, no research has focused on this task before due to the two difficulties: \u2022 To generate high-qualified questions (for open discussion), we need to perform question evaluation, which is rather challenging.", "labels": [], "entities": [{"text": "question evaluation", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.7189003527164459}]}, {"text": "\u2022 Questions inmost existed QG (QA) datasets, e.g. SQuAD (.", "labels": [], "entities": []}, {"text": "During the adversarial training process, we perform reinforcement learning to introduce information from the evaluation model.", "labels": [], "entities": []}, {"text": "This architecture was not used in QG before as far as we know, and experiments show that our model gets better performance compared with commonlyused text generation methods in the quality of generated questions.", "labels": [], "entities": []}, {"text": "All the experiments are performed on the \"open-answered question generation dataset (OQGenD)\" we build, which contains 20K news-question pairs.", "labels": [], "entities": []}, {"text": "It is the first dataset for openQG to the best of our knowledge.", "labels": [], "entities": []}, {"text": "Above all, the main contributions of this paper are threefold: \u2022 We propose the openQG task, and build OQGenD, OQRanD from 11.5M questions for generating and evaluating questions.", "labels": [], "entities": []}, {"text": "\u2022 We study how language use affects the number of answers a question receives, and draw some interesting conclusions for linguisticbased question evaluation.", "labels": [], "entities": [{"text": "linguisticbased question evaluation", "start_pos": 121, "end_pos": 156, "type": "TASK", "confidence": 0.6152360737323761}]}, {"text": "\u2022 We propose a model based on CGAN and our question evaluation model, which outperforms commonly-used text generation models in the quality of generated questions.", "labels": [], "entities": [{"text": "CGAN", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9419445991516113}]}, {"text": "In this paper, the two datasets OQRanD and OQGend are available at https://github.", "labels": [], "entities": [{"text": "OQRanD", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.7624599933624268}, {"text": "OQGend", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.8824259042739868}]}, {"text": "com/ChaiZ-pku/OQRanD-and-OQGenD.", "labels": [], "entities": [{"text": "OQRanD-and-OQGenD", "start_pos": 14, "end_pos": 31, "type": "DATASET", "confidence": 0.8717890381813049}]}], "datasetContent": [{"text": "Question evaluation is a rather challenging task.", "labels": [], "entities": [{"text": "Question evaluation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8840090036392212}]}, {"text": "Automatic evaluation metrics such as BLEU), ROUGE) and ME-TEOR ( were widely used to measure n-gram overlaps between generated questions and ground truth questions, however, they are far from enough since we cannot list all possible ground truth questions in openQG.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9988730549812317}, {"text": "ROUGE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9872105121612549}, {"text": "ME-TEOR", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9775351881980896}, {"text": "openQG", "start_pos": 259, "end_pos": 265, "type": "DATASET", "confidence": 0.9416602253913879}]}, {"text": "To this end, we need to develop specific evaluation metrics for questions.", "labels": [], "entities": []}, {"text": "Some researches) directly trained question ranking (QR) models via supervised learning, and used it to perform evaluation.", "labels": [], "entities": [{"text": "question ranking (QR)", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.760882568359375}]}, {"text": "However, these models are always domainspecific and not interpretable since we cannot tell what makes a question get a high (low) score.", "labels": [], "entities": []}, {"text": "Rao and Daum\u00e9 III (2018) took a step further, and pointed out that a good question is one whose expected answer will be useful.", "labels": [], "entities": []}, {"text": "By using the \"expected value of perfect information\", they proposed a useful evaluation model.", "labels": [], "entities": []}, {"text": "However, our task significantly differs from it in two aspects: first, there is no correct answer for open-answered questions thus it is hard to tell which answer is \"useful\".", "labels": [], "entities": []}, {"text": "Second, the goal of openQG is to arouse open discussions instead of \"solving a problem\".", "labels": [], "entities": []}, {"text": "Intuitively, a good question evaluation metric should be interpretable and keeps away from being domain-specific.", "labels": [], "entities": []}, {"text": "To this end, we first analyze how language use affects the number of answers, and then build evaluation models based on these conclusions.", "labels": [], "entities": []}, {"text": "There are some researches) about how language use affects the reaction that apiece of text generates, but we are the first to focus on questions as far as we know.", "labels": [], "entities": []}, {"text": "In this section, we deal with question analysis and evaluation.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8435323536396027}]}, {"text": "We first perform variable control and build OQGenD.", "labels": [], "entities": [{"text": "OQGenD", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.8685926198959351}]}, {"text": "After that, we analyze how language use affects the number of answers a question receives.", "labels": [], "entities": []}, {"text": "Based on these conclusions, we further build question evaluation models.", "labels": [], "entities": []}, {"text": "Based on OQRanD and our conclusions about how language use affects the answer that a question receives, we can train models to predict which question can receive more answers in each pair.", "labels": [], "entities": [{"text": "OQRanD", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.8129809498786926}]}, {"text": "Since questions in the same pair only differ in language use, models based on OQRanD can concentrate on linguistic facts to avoid being domain-specific.", "labels": [], "entities": []}, {"text": "Given pair (q 1 , q 2 ), we label it as \"1\" if n 1 > n 2 , otherwise we use label \"0\".", "labels": [], "entities": []}, {"text": "In this way, our task turns into a binary classification task.", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.7834871610005697}]}, {"text": "We further train a model F s which inputs a question and outputs a score.", "labels": [], "entities": []}, {"text": "The larger F s (\u00b7), the more answer is expected.", "labels": [], "entities": [{"text": "F s", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9463276565074921}, {"text": "answer", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9511827230453491}]}, {"text": "By comparing F s (q 1 ), F s (q 2 ), we can make the final prediction.", "labels": [], "entities": []}, {"text": "Although we can also use both q 1 , q 2 as inputs and train a model that directly outputs label 0 or 1, using F son q 1 , q 2 respectively is more flexible when we need to rank more than two question.", "labels": [], "entities": []}, {"text": "Besides, F scan be directly used forgetting rewards during the reinforcement QG process.", "labels": [], "entities": [{"text": "F scan", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.891730397939682}]}, {"text": "We use several models as F s , and perform training based on the hinge loss.", "labels": [], "entities": []}, {"text": "shows the accuracy of different models (hyper-parameters and training details are provided in the appendix).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993935823440552}, {"text": "hyper-parameters", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.9579125642776489}]}, {"text": "When features in Section 3.2 are not used, the CNN model gets the best performance, which is not surprised.", "labels": [], "entities": []}, {"text": "However, adding these features greatly improves the performance of all statistical models, making SVM and RF significantly surpass CNN.", "labels": [], "entities": []}, {"text": "This illustrates the importance of linguistic factors.", "labels": [], "entities": []}, {"text": "We choose several typical text-generation models as baselines.", "labels": [], "entities": []}, {"text": "We apply a Seq2seq model similar to, and use a CopyNet similar to.", "labels": [], "entities": []}, {"text": "As adversarial training has become anew trend in QG, we also adopt the Seq-GAN proposed by and SentiGAN by . For our model, the \"vanilla\": Results for openQG.", "labels": [], "entities": [{"text": "SentiGAN", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.7482938766479492}]}, {"text": "* () denotes that our vanilla (full) model differs from the baseline significantly based on one-side paired t-test with p < 0.05.", "labels": [], "entities": []}, {"text": "7 to compute rewards, and the \"full\" version uses Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.8155381083488464}]}, {"text": "10 (the SVM model which gets the best performance in are adopted as F s ).", "labels": [], "entities": []}, {"text": "More details about hyper-parameters and training process are provided in the appendix.", "labels": [], "entities": []}, {"text": "We adopt the commonly-used BLEU, ROUGE-L and METEOR for question evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9983922839164734}, {"text": "ROUGE-L", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9959438443183899}, {"text": "METEOR", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9908039569854736}, {"text": "question evaluation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7340311706066132}]}, {"text": "Besides, our score function F s based on OQRanD is also used.", "labels": [], "entities": [{"text": "score function F s", "start_pos": 13, "end_pos": 31, "type": "METRIC", "confidence": 0.9252002686262131}, {"text": "OQRanD", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.7516435980796814}]}, {"text": "Similarly, we choose the the SVM model which gets the best performance in.", "labels": [], "entities": []}, {"text": "We compute F s ( \u02c6 Y ) for each generated question\u02c6Yquestion\u02c6 question\u02c6Y , and report the average value in \"F s -SVM\" column of.", "labels": [], "entities": []}, {"text": "As mentioned above, F s shows if the generated questions are expected to receive more answers thus are more suitable for open discussion.", "labels": [], "entities": [{"text": "F s", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.986388236284256}]}, {"text": "The higher F s a model gets, the better performance it has.", "labels": [], "entities": []}, {"text": "The results of our experiments are listed in Table 5.", "labels": [], "entities": []}, {"text": "When it comes to BLEU, ROUGE-L and METEOR, our models get the best performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9984108209609985}, {"text": "ROUGE-L", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9915357828140259}, {"text": "METEOR", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9524294137954712}]}, {"text": "This shows the advantage of making both of the generator and discriminator conditioned on input news.", "labels": [], "entities": []}, {"text": "Besides, the full version of our model gets the best BLEU-3, BLEU-4 and METEOR values by introducing the linguistic-based question evaluation model during adversarial training.", "labels": [], "entities": [{"text": "BLEU-3", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9986922144889832}, {"text": "BLEU-4", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9958844780921936}, {"text": "METEOR", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9973076581954956}]}, {"text": "Of all the baselines, SentiGAN gets the best performances on BLEU-3 and BLEU-4, which is largely contributed by its penalty based objective function.", "labels": [], "entities": [{"text": "BLEU-3", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9927202463150024}, {"text": "BLEU-4", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9922541379928589}]}, {"text": "Since the same piece of news always corresponds with multiple questions (and these questions may differ a lot) in OQGenD, models based on adversarial training (SeqGAN, SentiGAN and ours) always get better results than others (Seq2seq and CopyNet).", "labels": [], "entities": [{"text": "OQGenD", "start_pos": 114, "end_pos": 120, "type": "DATASET", "confidence": 0.8096320033073425}]}, {"text": "When it comes to F s , the full version of our model gets the best performance, which illustrates that information from the SVM model is useful to generate questions with better quality.", "labels": [], "entities": []}, {"text": "Besides, we can also use the conclusions in Section 3.2 to compare different models, e.g. questions generated by our full version model are the most concise (9.68 words per question).", "labels": [], "entities": []}, {"text": "On the other hand, Senti-GAN generates the longest questions (11.54 words per question).", "labels": [], "entities": []}, {"text": "In this section, we introduce the details of our question evaluation models described in section 3.3.", "labels": [], "entities": []}, {"text": "We adopted the Ranklib toolkit 7 to train the random forest model.", "labels": [], "entities": [{"text": "Ranklib toolkit 7", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.9477458000183105}]}, {"text": "For the SVM model, we used the SVM-rank toolkit . More specifically, we set the trade-off between training error and margin of SVM to 3 and chose the linear kernel function.", "labels": [], "entities": [{"text": "training error", "start_pos": 98, "end_pos": 112, "type": "METRIC", "confidence": 0.8180287480354309}, {"text": "margin", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.949619472026825}]}, {"text": "For CNN and RNN models, the word embedding size is 128, and the size of POS embedding is 32.", "labels": [], "entities": []}, {"text": "The RNN model is a single-layer bidirectional LSTM network with 128 hidden units.", "labels": [], "entities": []}, {"text": "As for the CNN model, the convolution layer contains filters whose sizes are 160 \u00d7 1, 160 \u00d7 2, 160 \u00d7 3, 160 \u00d7 4.", "labels": [], "entities": []}, {"text": "The counts for each kind of filters are 64, 64, 64, 64, and the stride for each of them is 1.", "labels": [], "entities": []}, {"text": "After the convolution layer, there is a max-pooling layer and a fully connected layer with the sigmoid activation to get the final result.", "labels": [], "entities": []}, {"text": "As mentioned above, we controlled the effect of topic, time and author to get OQRanD.", "labels": [], "entities": [{"text": "author", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9694291353225708}, {"text": "OQRanD", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.8267194032669067}]}, {"text": "During this process, we divided all the questions into 37 subsets according to manually-tagged topics.", "labels": [], "entities": []}, {"text": "These topics are listed in.", "labels": [], "entities": []}, {"text": "The examples of OQRanD are shown in.", "labels": [], "entities": []}, {"text": "The examples of OQGenD are shown in (in case that the original news are too long, we omit the sentences that is not related to the qestions).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results for QR task. For LR (logistic re- gression), RF (random forest) and SVM (support vec- tor machine), \"traditional\" means n-gram word and  POS features. \"+ours\" means adding the 33 fea- tures that pass the significant test in", "labels": [], "entities": []}, {"text": " Table 5: Results for openQG.  *  () denotes that our vanilla (full) model differs from the baseline significantly  based on one-side paired t-test with p < 0.05.", "labels": [], "entities": []}]}