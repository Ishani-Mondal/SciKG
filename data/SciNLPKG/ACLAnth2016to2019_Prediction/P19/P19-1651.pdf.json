{"title": [{"text": "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we propose a goal-driven collab-orative task that combines language, perception , and action.", "labels": [], "entities": []}, {"text": "Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw.", "labels": [], "entities": []}, {"text": "Our game is grounded in a virtual world that contains movable clip art objects.", "labels": [], "entities": []}, {"text": "The game involves two players: a Teller and a Drawer.", "labels": [], "entities": [{"text": "Teller", "start_pos": 33, "end_pos": 39, "type": "TASK", "confidence": 0.6716557145118713}]}, {"text": "The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration , while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces.", "labels": [], "entities": [{"text": "Teller", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9477999806404114}]}, {"text": "The two players communicate with each other using natural language.", "labels": [], "entities": []}, {"text": "We collect the CoDraw dataset of \u223c10K dialogs consisting of \u223c138K messages exchanged between human players.", "labels": [], "entities": [{"text": "CoDraw dataset", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.9039891362190247}]}, {"text": "We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need fora novel crosstalk evaluation condition which pairs agents trained independently on disjoint subsets of the training data.", "labels": [], "entities": []}, {"text": "We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building agents that can interact with humans in natural language while perceiving and taking actions in their environments is one of the fundamental goals in artificial intelligence.", "labels": [], "entities": []}, {"text": "To this end, it will be necessary to ground language into perception and action), * The first two authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "\u2020 Work performed while the authors were interns at Facebook AI Research.", "labels": [], "entities": [{"text": "Facebook AI Research", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.852270265420278}]}], "datasetContent": [{"text": "In this section, we first detail our task, then present the CoDraw dataset, and finally propose a scene similarity metric which allows automatic evaluation of the reconstructed and original scene.", "labels": [], "entities": [{"text": "CoDraw dataset", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.8717253804206848}]}, {"text": "We collect 9,993 3 dialogs where pairs of people complete the CoDraw task, consisting of one dialog per scene in the Abstract Scenes dataset.", "labels": [], "entities": [{"text": "Abstract Scenes dataset", "start_pos": 117, "end_pos": 140, "type": "DATASET", "confidence": 0.7822073896725973}]}, {"text": "The dialogs contain of a total of 138K utterances and include snapshots of the intermediate state of the Drawer's canvas after each round of each conversation.", "labels": [], "entities": []}, {"text": "See Section 5 fora description of how we split the data into training, validation, and test sets.", "labels": [], "entities": []}, {"text": "shows the distribution of message lengths for both Drawers and Tellers.", "labels": [], "entities": []}, {"text": "The message length distribution for the Drawer is skewed toward 1 with passive replies like \"ok\", \"done\", etc.", "labels": [], "entities": []}, {"text": "There does exist a heavy tail, which shows that Drawers ask clarifying questions about the scene like \"where is trunk of second tree, low or high\".", "labels": [], "entities": []}, {"text": "On the other hand, Teller utterances have a median length of 16 tokens and a vocabulary size of 4,555.", "labels": [], "entities": [{"text": "Teller utterances", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.9652215242385864}, {"text": "vocabulary size", "start_pos": 77, "end_pos": 92, "type": "METRIC", "confidence": 0.8050515949726105}]}, {"text": "Due to the limited number of clip arts, the vocabulary is smaller than it would be for real images.", "labels": [], "entities": []}, {"text": "However, humans still use compositional language to describe clip art configurations and attributes, and make references to previous discourse elements in their messages.", "labels": [], "entities": []}, {"text": "shows the distribution of the numbers of conversational rounds for dialog sessions.", "labels": [], "entities": []}, {"text": "Most interactions are shorter than 20 rounds; the median number of rounds is 7.", "labels": [], "entities": []}, {"text": "In we see that the median session duration is 6 minutes.", "labels": [], "entities": [{"text": "duration", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7724202871322632}]}, {"text": "We had placed a 20-minute maximum limit on each session.", "labels": [], "entities": []}, {"text": "3 Excluding 27 empty scenes from the original dataset.", "labels": [], "entities": []}, {"text": "To evaluate our models, we pair our models with other models, as well as with a human.", "labels": [], "entities": []}, {"text": "We modified the interface used for data collection to have each trained model to play one game with a human per scene in the test set.", "labels": [], "entities": [{"text": "data collection", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.6989357620477676}]}, {"text": "We then compare the scene reconstruction quality between human-model pairs for various models and with human-human pairs.", "labels": [], "entities": [{"text": "scene reconstruction", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.6971732974052429}]}, {"text": "In addition to human evaluation, we would like to have automated evaluation protocols that can quickly estimate the quality of different models.", "labels": [], "entities": []}, {"text": "Drawer models can be evaluated against a recorded human conversation from a script (a recorded dialog from the dataset) by measuring scene similarity at the end of the dialog.", "labels": [], "entities": []}, {"text": "While this setup does not capture the full interactive nature of the task, the Drawer model still receives human descriptions of the scene and should be able to reconstruct it.", "labels": [], "entities": []}, {"text": "Our modeling assumptions include not giving Drawer models the ability to ask clarifying questions, which further suggests that script-based evaluation can reasonably measure model quality.", "labels": [], "entities": []}, {"text": "To evaluate Teller models in a goal-driven manner, a \"script\" from the dataset is not sufficient.", "labels": [], "entities": [{"text": "Teller", "start_pos": 12, "end_pos": 18, "type": "TASK", "confidence": 0.9326971173286438}]}, {"text": "We instead consider an evaluation where a Teller model and Drawer model are paired, and their joint performance is evaluated using the scene similarity metric.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for our models on the test set, using three types of evaluation: script-based (i.e. replaying Teller  utterances from the dataset), human-machine, and machine-machine pair evaluation.", "labels": [], "entities": []}]}