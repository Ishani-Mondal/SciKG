{"title": [{"text": "OpenKiwi: An Open Source Framework for Quality Estimation", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce OpenKiwi, a PyTorch-based open source framework for translation quality estimation.", "labels": [], "entities": [{"text": "translation quality estimation", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.9190974036852518}]}, {"text": "OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 2015-18 quality estimation campaigns.", "labels": [], "entities": [{"text": "sentence-level quality estimation", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.6172414819399515}, {"text": "WMT 2015-18 quality estimation", "start_pos": 140, "end_pos": 170, "type": "TASK", "confidence": 0.699445053935051}]}, {"text": "We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentence-level tasks.", "labels": [], "entities": [{"text": "WMT 2018", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9176178872585297}]}], "introductionContent": [{"text": "Quality estimation (QE) provides the missing link between machine and human translation: its goal is to evaluate a translation system's quality without access to reference translations ().", "labels": [], "entities": [{"text": "Quality estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7955739915370941}]}, {"text": "Among its potential usages are: informing an end user about the reliability of automatically translated content; deciding if a translation is ready for publishing or if it requires human post-editing; and highlighting the words that need to be post-edited.", "labels": [], "entities": []}, {"text": "While there has been tremendous progress in QE in the last years (, the ability of researchers to reproduce state-of-the-art systems has been hampered by the fact that these are either based on complex ensemble systems, complicated architectures, or require not well-documented pretraining and fine-tuning of some components.", "labels": [], "entities": [{"text": "QE", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9557514190673828}]}, {"text": "Existing open-source frameworks such as WCE-LIG (), QuEST++ (), Marmot (, or DeepQuest (, while helpful, are currently behind the recent best systems in WMT QE shared tasks.", "labels": [], "entities": [{"text": "WCE-LIG", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.8824347257614136}, {"text": "WMT QE shared tasks", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.648392528295517}]}, {"text": "To address the shortcoming * Work done during an internship at above, this paper presents OpenKiwi, anew open source framework for QE that implements the best QE systems from WMT 2015-18 shared tasks, making it easy to combine and modify their key components, while experimenting under the same framework.", "labels": [], "entities": [{"text": "WMT 2015-18 shared tasks", "start_pos": 175, "end_pos": 199, "type": "DATASET", "confidence": 0.7900493443012238}]}, {"text": "The main features of OpenKiwi are: \u2022 Implementation of four QE systems: QUETCH (), NUQE, Predictor-Estimator (, and a stacked ensemble with a linear system (); \u2022 Easy to use API: can be imported as a package in other projects or run from the command line; \u2022 Implementation in Python using PyTorch as the deep learning framework; \u2022 Ability to train new QE models on new data; \u2022 Ability to run pre-trained QE models on data from the WMT 2018 campaign; \u2022 Easy to track and reproduce experiments via YAML configuration files and (optionally) MLflow; \u2022 Open-source license (Affero GPL).", "labels": [], "entities": [{"text": "WMT 2018 campaign", "start_pos": 431, "end_pos": 448, "type": "DATASET", "confidence": 0.8884987433751425}]}, {"text": "This project is hosted at https://github.", "labels": [], "entities": []}, {"text": "com/Unbabel/OpenKiwi.", "labels": [], "entities": [{"text": "OpenKiwi", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.7051057815551758}]}, {"text": "We welcome and encourage contributions from the research community.", "labels": [], "entities": []}], "datasetContent": [{"text": "To benchmark OpenKiwi, we use the following datasets from the WMT 2018 quality estimation shared task, all English-German (En-De): \u2022 Two quality estimation datasets of sentence triplets, each consisting of a source sentence (SRC), its machine translation (MT) and a human post-edition (PE) of the machine translation: a larger dataset of 26,273 training and 1,000 development triplets, where the MT is generated by a phrase-based statistical machine translation (SMT); and a smaller dataset of 13,442 training and 1,000 development triplets, where the MT is generated by a neural machine translation system (NMT).", "labels": [], "entities": [{"text": "WMT 2018 quality estimation shared task", "start_pos": 62, "end_pos": 101, "type": "TASK", "confidence": 0.6023290107647578}, {"text": "phrase-based statistical machine translation (SMT)", "start_pos": 417, "end_pos": 467, "type": "TASK", "confidence": 0.7564870885440281}]}, {"text": "The data also contains word-level quality labels and sentencelevel scores that are obtained from the posteditions using TERCOM).", "labels": [], "entities": [{"text": "TERCOM", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.7278116941452026}]}, {"text": "\u2022 A corpus of 526,368 artificially generated sentence triplets, obtained by first cross-entropy filtering a much larger monolingual corpus for indomain sentences, then using round-trip translation and a final stratified sampling step.", "labels": [], "entities": []}, {"text": "\u2022 A parallel dataset of 3,396,364 in-domain sentences used for pre-training of the predictorestimator model.", "labels": [], "entities": []}, {"text": "In addition to the models that are part of OpenKiwi, in the experiments below, we also use Automatic Post-Editing (APE) adapted for QE (APE-QE).", "labels": [], "entities": [{"text": "Automatic Post-Editing (APE)", "start_pos": 91, "end_pos": 119, "type": "METRIC", "confidence": 0.8590787529945374}]}, {"text": "APE-QE has been used by as an intermediate step for quality estimation, where an APE system is trained on the human post-edits and its outputs are used as pseudo-post-editions to generate word-level quality labels and sentence-level scores in the same way that the original labels were created.", "labels": [], "entities": [{"text": "APE-QE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8299872279167175}, {"text": "quality estimation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7169466316699982}]}, {"text": "Since OpenKiwi's focus is not on implementing a sequence-to-sequence model, we used an external software, OpenNMT-py (, to train two separate translation models: \u2022 SRC \u2192 PE: trained first on the in-domain corpus provided, then fine-tuned on the shared task data.", "labels": [], "entities": []}, {"text": "deepQUEST is the open source system developed by, UNQE is the unpublished system from Jiangxi Normal University, described by, and QE Brain is the system from Alibaba described by.", "labels": [], "entities": [{"text": "UNQE", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.7828572392463684}, {"text": "Jiangxi Normal University", "start_pos": 86, "end_pos": 111, "type": "DATASET", "confidence": 0.9428330858548483}]}, {"text": "Reported numbers for the OpenKiwi system correspond to best models in the development set: the STACKED model for prediction of MT tags, and the ENSEMBLED model for the rest.", "labels": [], "entities": [{"text": "STACKED", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.977268636226654}, {"text": "prediction of MT tags", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.8405707329511642}]}], "tableCaptions": [{"text": " Table 1: Benchmarking of the different models implemented in OpenKiwi on the WMT 2018 development set,  along with an ensembled system (ENSEMBLED) that averages the predictions of the NUQE, APE-QE, and PRED- EST systems, as well as a stacked architecture (STACKED) which stacks their predictions into a linear feature-based  model, as described by Martins et al. (2017). For each system, we report the five official scores used in WMT  2018: word-level F mult", "labels": [], "entities": [{"text": "WMT 2018 development set", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.929562047123909}]}, {"text": " Table 2: Final results on the WMT 2018 test set. The first three systems are the official WMT18-QE winners  (underlined):", "labels": [], "entities": [{"text": "WMT 2018 test set", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9200405776500702}, {"text": "WMT18-QE", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9090210199356079}]}]}