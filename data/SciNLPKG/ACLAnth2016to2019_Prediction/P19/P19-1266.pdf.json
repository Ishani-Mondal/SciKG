{"title": [{"text": "Deep Dominance -How to Properly Compare Deep Neural Models", "labels": [], "entities": [{"text": "Properly Compare Deep Neural Models", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.7890565037727356}]}], "abstractContent": [{"text": "Comparing between Deep Neural Network (DNN) models based on their performance on unseen data is crucial for the progress of the NLP field.", "labels": [], "entities": []}, {"text": "However, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at ini-tialization and during training.", "labels": [], "entities": []}, {"text": "Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models.", "labels": [], "entities": []}, {"text": "In this paper, we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions.", "labels": [], "entities": []}, {"text": "We define the criteria fora high quality comparison method between DNNs, and show, both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so.", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.7596502304077148}]}, {"text": "We hope the test we propose here will set anew working practice in the NLP community.", "labels": [], "entities": []}], "introductionContent": [{"text": "A large portion of the research activity in Natural Language Processing (NLP) is devoted to the development of new algorithms for existing or new tasks.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7437365253766378}]}, {"text": "To evaluate the quality of anew method, its performance on unseen datasets is compared to the performance of existing methods.", "labels": [], "entities": []}, {"text": "The progress of the field hence crucially depends on our ability to draw conclusions from such comparisons.", "labels": [], "entities": []}, {"text": "In the past, most supervised NLP models have been linear (or log-linear), convex and relatively simple (e.g. ().", "labels": [], "entities": []}, {"text": "Hence, their training was deterministic and the number of configurations a model could have was rather small -decisions about model design were usually limited to feature selection and the selection of one of a few loss functions.", "labels": [], "entities": []}, {"text": "Consequently, when one model performed better than another on unseen data it was safe to argue that the winning model was generally better, especially when the results were statistically significant, and when the effect of multiple hypothesis testing was taken into account in cases of evaluation with multiple datasets.", "labels": [], "entities": []}, {"text": "With the recent emergence of Deep Neural Networks (DNNs), data-driven performance comparison has become much more complicated.", "labels": [], "entities": []}, {"text": "While models such as LSTM), Bi-LSTM () and the transformer ( improved the state-of-the-art in many NLP tasks (e.g. (), it is much more difficult to compare the performance of algorithms that are based on these models.", "labels": [], "entities": []}, {"text": "This is because the loss functions of these models are non-convex (), making the solution to which they converge (a local minimum or a saddle point) sensitive to random weight initialization and the order of training examples.", "labels": [], "entities": []}, {"text": "Moreover, as these complex models are not fully understood, their training is often enhanced by heuristics such as random dropouts () that introduces another level of non-determinism to the training process.", "labels": [], "entities": []}, {"text": "Finally, the increased model complexity results in a much larger number of configurations, governed by a large space of hyper-parameters for model properties such as the number of layers and the number of neurons in each layer.", "labels": [], "entities": []}, {"text": "With so many degrees of freedom governed by random and arbitrary values, when comparing two DNNs it is not possible to consider a single test-set evaluation score for each model.", "labels": [], "entities": []}, {"text": "If we do that, we might compare just the best models that someone happened to train rather than the methods themselves.", "labels": [], "entities": []}, {"text": "Instead, it is necessary to compare between the score distributions generated by different runs of each of the models.", "labels": [], "entities": []}, {"text": "Unfortunately, this comparison task, which is fundamental to the progress of the field, has not received a systematic treatment thus far.", "labels": [], "entities": []}, {"text": "Our goal is to close this gap and propose a simple and effective comparison tool between two DNNs based on their test set score distributions.", "labels": [], "entities": []}, {"text": "Particularly, we make four contributions: Defining a DNN comparison framework: We define three criteria that a DNN comparison tool should meet: (a) Since we observe only a sample from the population score distribution of each model, the decision should be significant under well justified statistical assumptions.", "labels": [], "entities": []}, {"text": "This assures that future runs of the superior model are likely to get higher scores than future runs of the inferior model; (b) The decision mechanism should be powerful, being able to make decisions inmost possible decision tasks; and, finally, (c) Since both models depend on random decisions, it is likely that none of them is promised to be superior over the other in all cases (e.g. with all possible random seeds).", "labels": [], "entities": []}, {"text": "A powerful comparison tool should hence augment its decision with a confidence score, reflecting the probability that the superior model will indeed produce a better output.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: NER results. (Case A).", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9239472150802612}]}, {"text": " Table 2: POS tagging results (Case B).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8010512590408325}]}, {"text": " Table 3: NER Results (Case C).", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9155275225639343}]}, {"text": " Table 4: Results summary over the 510 comparisons  of Reimers and Gurevych (2017a).", "labels": [], "entities": []}]}