{"title": [{"text": "An Effective Approach to Unsupervised Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7112105488777161}]}], "abstractContent": [{"text": "While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.8179807960987091}, {"text": "Neural Machine Translation (NMT)", "start_pos": 138, "end_pos": 170, "type": "TASK", "confidence": 0.8065235118071238}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 175, "end_pos": 212, "type": "TASK", "confidence": 0.7699916114409765}]}, {"text": "In this paper , we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9815254211425781}]}, {"text": "Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9855172038078308}]}, {"text": "Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.6965523511171341}]}, {"text": "For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system , and 0.5 points more than the (supervised) shared task winner back in 2014.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9991682767868042}, {"text": "WMT 2014", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.598861962556839}]}], "introductionContent": [{"text": "The recent advent of neural sequence-to-sequence modeling has resulted in significant progress in the field of machine translation, with large improvements in standard benchmarks () and the first solid claims of human parity in certain settings.", "labels": [], "entities": [{"text": "neural sequence-to-sequence modeling", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.6797151962916056}, {"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.807219386100769}]}, {"text": "Unfortunately, these systems rely on large amounts of parallel corpora, which are only available fora few combinations of major languages like English, German and French.", "labels": [], "entities": []}, {"text": "Aiming to remove this dependency on parallel data, a recent research line has managed to train unsupervised machine translation systems using monolingual corpora only.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7108297795057297}]}, {"text": "The first such systems were based on Neural Machine Translation (NMT), and combined denoising autoencoding and back-translation to train a dual model initialized with cross-lingual embeddings).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.8124291300773621}]}, {"text": "Nevertheless, these early systems were later superseded by Statistical Machine Translation (SMT) based approaches, which induced an initial phrase-table through cross-lingual embedding mappings, combined it with an n-gram language model, and further improved the system through iterative backtranslation (.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.8165312012036642}]}, {"text": "In this paper, we develop a more principled approach to unsupervised SMT, addressing several deficiencies of previous systems by incorporating subword information, applying a theoretically well founded unsupervised tuning method, and developing a joint refinement procedure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.8887966871261597}]}, {"text": "In addition to that, we use our improved SMT approach to initialize an unsupervised NMT system, which is further improved through on-the-fly back-translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9923916459083557}]}, {"text": "Our experiments on WMT 2014/2016 FrenchEnglish and German-English show the effectiveness of our approach, as our proposed system outperforms the previous state-of-the-art in unsupervised machine translation by 5-7 BLEU points in all these datasets and translation directions.", "labels": [], "entities": [{"text": "WMT 2014/2016 FrenchEnglish", "start_pos": 19, "end_pos": 46, "type": "DATASET", "confidence": 0.8512502431869506}, {"text": "machine translation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7173300087451935}, {"text": "BLEU", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.9978304505348206}]}, {"text": "Our system also outperforms the supervised WMT 2014 shared task winner in English-to-German, and is around 2 BLEU points behind it in the rest of translation directions, suggesting that unsupervised machine translation can be a usable alternative in practical settings.", "labels": [], "entities": [{"text": "WMT 2014 shared task winner", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.5601706981658936}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9993205070495605}, {"text": "machine translation", "start_pos": 199, "end_pos": 218, "type": "TASK", "confidence": 0.7559909522533417}]}, {"text": "The remaining of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 first discusses the related work in the topic.", "labels": [], "entities": []}, {"text": "Section 3 then describes our principled unsupervised SMT method, while Section 4 discusses our hybridization method with NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9906728863716125}, {"text": "NMT", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.8756099939346313}]}, {"text": "We then present the experiments done and the results obtained in Section 5, and Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to make our experiments comparable to previous work, we use the French-English and German-English datasets from the WMT 2014 shared task.", "labels": [], "entities": [{"text": "German-English datasets from the WMT 2014 shared task", "start_pos": 92, "end_pos": 145, "type": "DATASET", "confidence": 0.7248807661235332}]}, {"text": "More concretely, our training data consists of the concatenation of all News Crawl monolingual corpora from 2007 to 2013, which make a total of 749 million tokens in French, 1,606 millions in German, and 2,109 millions in English, from which we take a random subset of 2,000 sentences for tuning (Section 3.3).", "labels": [], "entities": [{"text": "News Crawl monolingual corpora from 2007", "start_pos": 72, "end_pos": 112, "type": "DATASET", "confidence": 0.9301876425743103}]}, {"text": "Preprocessing is done using standard Moses tools, and involves punctuation normalization, tokenization with aggressive hyphen splitting, and truecasing.", "labels": [], "entities": [{"text": "punctuation normalization", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.6822714805603027}]}, {"text": "Our SMT implementation is based on Moses 10 , and we use the tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9897940158843994}]}, {"text": "Our unsupervised tuning implementation is based on Z-MERT, and we use FastAlign () for word alignment within the joint refinement procedure.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7782851755619049}]}, {"text": "Finally, we use the big transformer implementation from fairseq 11 for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as . We use newstest2014 as our test set for 10 http://www.statmt.org/moses/ 11 https://github.com/pytorch/fairseq French-English, and both newstest2014 and newstest2016 (from WMT 2016 12 ) for GermanEnglish.", "labels": [], "entities": [{"text": "WMT 2016 12 )", "start_pos": 363, "end_pos": 376, "type": "DATASET", "confidence": 0.9417622089385986}, {"text": "GermanEnglish", "start_pos": 381, "end_pos": 394, "type": "DATASET", "confidence": 0.9371840357780457}]}, {"text": "Following common practice, we report tokenized BLEU scores as computed by the multi-bleu.perl script included in Moses.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9737839102745056}]}, {"text": "In addition to that, we also report detokenized BLEU scores as computed by SacreBLEU 13, which is equivalent to the official mteval-v13a.pl script.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9555856585502625}]}, {"text": "We next present the results of our proposed system in comparison to previous work in Section 5.1.", "labels": [], "entities": []}, {"text": "Section 5.2 then compares the obtained results to those of different supervised systems.", "labels": [], "entities": []}, {"text": "Finally, Section 5.3 presents some translation examples from our system.", "labels": [], "entities": []}, {"text": "reports the results of the proposed system in comparison to previous work.", "labels": [], "entities": []}, {"text": "As it can be seen, our full system obtains the best published results in all cases, outperforming the previous stateof-the-art by 5-7 BLEU points in all datasets and translation directions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9983402490615845}]}], "tableCaptions": [{"text": " Table 1: Results of the proposed method in comparison to previous work (BLEU). Overall best results are in bold,  the best ones in each group are underlined.   *  Detokenized BLEU equivalent to the official mteval-v13a.pl script. The rest use tokenized BLEU with  multi-bleu.perl (or similar).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9919294118881226}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.8991016149520874}, {"text": "BLEU", "start_pos": 254, "end_pos": 258, "type": "METRIC", "confidence": 0.9758731126785278}]}, {"text": " Table 2: NMT hybridization results for different unsupervised machine translation systems (BLEU).", "labels": [], "entities": [{"text": "NMT hybridization", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9113762378692627}, {"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7313222289085388}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.7799627184867859}]}, {"text": " Table 3: Results of the proposed method in comparison to different supervised systems (BLEU).   *  Detokenized BLEU equivalent to the official mteval-v13a.pl script. The rest use tokenized BLEU with  multi-bleu.perl (or similar).   \u2020 Results in the original test set from WMT 2014, which slightly differs from the full test set used in all subsequent  work. Our proposed system obtains 22.4 BLEU points (21.1 detokenized) in that same subset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9048314094543457}, {"text": "BLEU", "start_pos": 190, "end_pos": 194, "type": "METRIC", "confidence": 0.9498134255409241}, {"text": "WMT 2014", "start_pos": 273, "end_pos": 281, "type": "DATASET", "confidence": 0.809023529291153}, {"text": "BLEU", "start_pos": 392, "end_pos": 396, "type": "METRIC", "confidence": 0.9963099360466003}]}]}