{"title": [{"text": "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!", "labels": [], "entities": []}], "abstractContent": [{"text": "The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving avast store of potential training signal untapped.", "labels": [], "entities": []}, {"text": "In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in.", "labels": [], "entities": []}, {"text": "As our agent engages in conversation , it also estimates user satisfaction in its responses.", "labels": [], "entities": []}, {"text": "When the conversation appears to be going well, the user's responses become new training examples to imitate.", "labels": [], "entities": []}, {"text": "When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot's dialogue abilities further.", "labels": [], "entities": []}, {"text": "On the PERSONACHAT chitchat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance , regardless of the amount of traditional supervision.", "labels": [], "entities": [{"text": "PERSONACHAT chitchat dataset", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.7826867699623108}]}], "introductionContent": [{"text": "Training a dialogue agent to converse like a human requires extensive supervision.", "labels": [], "entities": []}, {"text": "The most common approach is to train models to imitate humans in large corpora of crowdsourced or scraped conversations ().", "labels": [], "entities": []}, {"text": "These fullysupervised conversations tend to be expensive to collect in sufficient quantity and/or occur in settings with significant differences from the deployment environment (.", "labels": [], "entities": []}, {"text": "Instead, dialogue agents would ideally learn directly from dialogue, the conversations they participate in after deployment, which are usually abundant, taskspecific, dynamic, and cheap.", "labels": [], "entities": []}, {"text": "This corresponds to the way humans learn to converse-not merely observing others engaging in \"expert-level\" conver- * *BH completed most of this work at Facebook (FAIR).", "labels": [], "entities": [{"text": "BH", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.8729430437088013}, {"text": "FAIR)", "start_pos": 163, "end_pos": 168, "type": "DATASET", "confidence": 0.6321478337049484}]}, {"text": "sations, but instead actively adjusting and correcting our speech based on feedback woven throughout our own conversations.", "labels": [], "entities": []}, {"text": "Giving a dialogue agent this ability would enable it to continuously improve and adapt over its lifetime, rather than requiring additional annotation costs for each and every improvement.", "labels": [], "entities": []}, {"text": "However, naively training a dialogue agent on its own conversations yields poor results.", "labels": [], "entities": []}, {"text": "For example, training a model on its own output can simply reinforce its existing failure modes, and mistakes by the agent can lead to absurd conversations that no longer resemble the target domain.", "labels": [], "entities": []}, {"text": "To combat this, one approach is to allow the agent to request feed-back during conversations (), e.g., when it believes it is about to make a mistake.", "labels": [], "entities": []}, {"text": "This approach, however, falls victim to the Dunning-Kruger effect), which in this case suggests that a bad model will also be bad at knowing when it is doing a bad job.", "labels": [], "entities": []}, {"text": "Regardless of when feedback is requested, existing methods typically require accompanying scalar rewards or adherence to particular templates or structure to ensure that the feedback is usable by the model ().", "labels": [], "entities": []}, {"text": "These requirements maybe acceptable for paid annotators, but they impose unnatural workflows on unpaid conversation partners in a standard dialogue environment.", "labels": [], "entities": []}, {"text": "Humans are able to request and provide feedback using only natural language; ideally, dialogue agents would be able to do the same.", "labels": [], "entities": []}, {"text": "In this work we propose the self-feeding chatbot, a dialogue agent with the ability to extract new examples from the conversations it participates in after deployment).", "labels": [], "entities": []}, {"text": "Concretely, in addition to being trained on the primary DIALOGUE task, the agent is trained to predict its speaking partner's satisfaction with its responses.", "labels": [], "entities": []}, {"text": "When the conversation seems to be going well, the user's responses (but not the bot's own utterances) become the targets in new training examples for the DIA-LOGUE task.", "labels": [], "entities": []}, {"text": "When the agent believes it has made a mistake, it instead requests feedback on what it could have said instead.", "labels": [], "entities": []}, {"text": "Predicting the feedback that will be provided in a given context becomes an auxiliary task (FEEDBACK) on which the model is also trained.", "labels": [], "entities": [{"text": "FEEDBACK", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9980270266532898}]}, {"text": "Importantly, these new examples improve the agent's dialogue abilities while using only natural responses from the user that do not require special structure, accompanying numerical feedback, or additional human intervention in order to be used.", "labels": [], "entities": []}, {"text": "With this approach, the conversations the chatbot participates in are sliced into two complementary datasets-one largely protected from the chatbot's mistakes (DIALOGUE examples), and one which directly addresses them (FEEDBACK examples).", "labels": [], "entities": [{"text": "FEEDBACK", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.8874255418777466}]}, {"text": "We validate our approach on the PER-SONACHAT () dialogue dataset, finding empirically that regardless of the number of available supervised examples, the dialogue ability of the chatbot is always improved by adding the automatically extracted examples of either type, and improves the most by adding both.", "labels": [], "entities": [{"text": "PER-SONACHAT () dialogue dataset", "start_pos": 32, "end_pos": 64, "type": "DATASET", "confidence": 0.713091179728508}]}, {"text": "The main contributions of this work thus include the following: \u2022 We propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples for itself from the conversations it participates in during deployment.", "labels": [], "entities": []}, {"text": "\u2022 We show that dialogue ability improves by imitating human responses when the human is satisfied, or by asking for feedback when they are not, predicting it as an auxiliary task.", "labels": [], "entities": []}, {"text": "\u2022 We demonstrate that classifying user satisfaction is a learnable task important for the selffeeding process, significantly outperforming an approach based on model uncertainty.", "labels": [], "entities": []}, {"text": "\u2022 We release three new datasets to further research in this direction: (1) deployment chat logs (513k messages); (2) ratings of user satisfaction (42k); (3) textual feedback on what a bot could have said in a given context (62k).", "labels": [], "entities": []}, {"text": "The datasets and models described in this paper are available via the ParlAI platform (, along with training code.", "labels": [], "entities": [{"text": "ParlAI platform", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.927608460187912}]}, {"text": "Hyperparameter values are included in Appendix G.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.5358660221099854}]}], "datasetContent": [{"text": "Throughout this section, we use the ranking metric hits@X/Y, or the fraction of the time that the correct candidate response was ranked in the top X out of Y available candidates; accuracy is another name for hits@1/Y.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9993263483047485}]}, {"text": "Statistical significance for improvement over baselines is assessed with a two-sample one-tailed T-test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of examples used in our experi- ments by task and split. Note that the HH DIALOGUE  examples come from the PERSONACHAT dataset, HB  DIALOGUE and FEEDBACK examples were collected  during deployment, and an additional 40k SATISFAC- TION training examples were collected for the analysis  in Section 5.1.", "labels": [], "entities": [{"text": "HH DIALOGUE", "start_pos": 92, "end_pos": 103, "type": "METRIC", "confidence": 0.722778707742691}, {"text": "PERSONACHAT dataset", "start_pos": 128, "end_pos": 147, "type": "DATASET", "confidence": 0.8823851048946381}, {"text": "FEEDBACK", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9829710125923157}, {"text": "SATISFAC- TION", "start_pos": 241, "end_pos": 255, "type": "METRIC", "confidence": 0.72716756661733}]}, {"text": " Table 4: The maximum F1 score (with corresponding  precision and recall) obtained on the SATISFACTION  task. For the Uncertainty methods, we also report the  maximum F1 score with the constraint that precision  must be \u2265 0.5. The Satisfaction Classifier is reported  with varying numbers of SATISFACTION training ex- amples.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9863776862621307}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9978887438774109}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9982738494873047}, {"text": "SATISFACTION", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.8040003776550293}, {"text": "F1 score", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9846554100513458}, {"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9903811812400818}, {"text": "SATISFACTION training ex- amples", "start_pos": 292, "end_pos": 324, "type": "METRIC", "confidence": 0.6905901551246643}]}, {"text": " Table 5: When the number of candidates to choose  from is increased to 10,000, adding Human-Bot (HB)  DIALOGUE and FEEDBACK (FB) examples continues  to improve performance on the DIALOGUE task at all  levels.", "labels": [], "entities": [{"text": "FEEDBACK (FB)", "start_pos": 116, "end_pos": 129, "type": "METRIC", "confidence": 0.9310949295759201}]}, {"text": " Table 6: The accuracy of various models and baselines  on the original PERSONACHAT test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.999669075012207}, {"text": "PERSONACHAT test set", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.87326051791509}]}, {"text": " Table 7: Both with few HH DIALOGUE examples  (20k) and many (131k), adding examples with bot ut- terances as the target decreased quality. We explored  using all bot responses (Bot Unfiltered, or BU) and  only those responses with estimated satisfaction scores  greater than the 0.5 (Bot Filtered, or BF).", "labels": [], "entities": [{"text": "HH", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9024709463119507}, {"text": "DIALOGUE", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.5347093939781189}]}, {"text": " Table 8: As discussed in Section 5.1 and illustrated in  Figure 3, FEEDBACK (FB) examples collected from a  more recently retrained model (set B instead of set A)  are more valuable in terms of improving performance;  see Appendix A for details on how sets A and B were  collected. We did not observe the same trend for HB  DIALOGUE examples. We include the performance of  models trained on only HH DIALOGUE examples in  italics as reference points.", "labels": [], "entities": [{"text": "FEEDBACK (FB)", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.8735320419073105}]}, {"text": " Table 9: The hyperparameters used to obtain the results in", "labels": [], "entities": []}]}