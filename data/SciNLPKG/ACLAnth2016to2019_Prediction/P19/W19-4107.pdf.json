{"title": [{"text": "DSTC7 Task 1: Noetic End-to-End Response Selection", "labels": [], "entities": [{"text": "Noetic End-to-End Response Selection", "start_pos": 14, "end_pos": 50, "type": "TASK", "confidence": 0.8948723822832108}]}], "abstractContent": [{"text": "Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets.", "labels": [], "entities": [{"text": "Goal-oriented dialogue", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7213449776172638}]}, {"text": "This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse.", "labels": [], "entities": []}, {"text": "We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases , and (3) not including a correct option in the candidate set.", "labels": [], "entities": [{"text": "utterance selection", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7899479866027832}]}, {"text": "Twenty teams participated , developing a range of neural network models, including some that successfully incorporated external data to boost performance.", "labels": [], "entities": []}, {"text": "Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic dialogue systems have great potential as anew form of user interface between people and computers.", "labels": [], "entities": []}, {"text": "Unfortunately, there are relatively few large resources of human-human dialogues, which are crucial for the development of robust statistical models.", "labels": [], "entities": []}, {"text": "Evaluation also poses a challenge, as the output of an end-to-end dialogue system could be entirely reasonable, but not match the reference, either because it is a paraphrase, or it takes the conversation in a different, but still coherent, direction.", "labels": [], "entities": []}, {"text": "In this shared task, we introduced two new datasets and explored variations in task structure for research on goal-oriented dialogue.", "labels": [], "entities": []}, {"text": "One of our datasets was carefully constructed with real people acting in a university student advising scenario.", "labels": [], "entities": []}, {"text": "The other dataset was formed by applying anew disentanglement method () to extract conversations from an IRC channel of technical help for the Ubuntu operating system.", "labels": [], "entities": []}, {"text": "We structured the dialogue problem as next utterance selection, in which participants receive partial dialogues and must select the next utterance from a set of options.", "labels": [], "entities": [{"text": "next utterance selection", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6002665460109711}]}, {"text": "Going beyond prior work, we considered larger sets of options, and variations with either additional incorrect options, paraphrases of the correct option, or no correct option at all.", "labels": [], "entities": []}, {"text": "These changes push the next utterance selection task towards real-world dialogue.", "labels": [], "entities": [{"text": "utterance selection task", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8971193035443624}]}, {"text": "This task is not a continuation of prior DSTC tasks, but it is related to tasks 1 and 2 from DSTC6 (.", "labels": [], "entities": [{"text": "DSTC6", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.9170321226119995}]}, {"text": "Like DSTC6 task 1, our task considers goal-oriented dialogue and next utterance selection, but our data is from human-human conversations, whereas theirs was simulated.", "labels": [], "entities": [{"text": "next utterance selection", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.6870881915092468}]}, {"text": "Like DSTC6 task 2, we use online resources to build a large collection of dialogues, but their dialogues were shorter (2 -2.5 utterances per conversation) and came from a more diverse set of sources (1,242 twitter customer service accounts, and a range of films).", "labels": [], "entities": []}, {"text": "This paper provides an overview of (1) the task structure, (2) the datasets, (3) the evaluation metrics, and (4) system results.", "labels": [], "entities": []}, {"text": "Twenty teams participated, with one clear winner, scoring the highest on all but one sub-task.", "labels": [], "entities": []}, {"text": "The data and other resources associated with the task have been released 1 to enable future work on this topic and to make accurate comparisons possible.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of the diversity of the underlying  datasets. Advising is smaller and has longer conversa- tions, but less diversity in utterances. Tokens are based  on splitting on whitespace.", "labels": [], "entities": []}, {"text": " Table 2: Results, ordered by the average rank of each team across the subtasks they participated in. The top result  in each column is in bold. For these results the metric is the average of MRR and Recall@10.", "labels": [], "entities": [{"text": "MRR", "start_pos": 192, "end_pos": 195, "type": "METRIC", "confidence": 0.9911871552467346}, {"text": "Recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9949347376823425}]}, {"text": " Table 3: Subtask 1 results. The left table is for Ubuntu, the middle table is for the initial Advising test set, and the  right table is for the final Advising test set. The best results are bolded.", "labels": [], "entities": [{"text": "Ubuntu", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9541620016098022}, {"text": "Advising test set", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.7371854384740194}]}, {"text": " Table 4: Subtask 5 results. The left column of tables is for Ubuntu, the middle column is for the initial Advising  test set, and the right column is for the final Advising test set. The best results are bolded.", "labels": [], "entities": [{"text": "Ubuntu", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.9507617354393005}, {"text": "Advising  test set", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.7238407035668691}]}]}