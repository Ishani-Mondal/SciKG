{"title": [{"text": "Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards", "labels": [], "entities": [{"text": "Neural Keyphrase Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7969614267349243}]}], "abstractContent": [{"text": "Generating keyphrases that summarize the main points of a document is a fundamental task in natural language processing.", "labels": [], "entities": [{"text": "Generating keyphrases that summarize the main points of a document", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8543873250484466}]}, {"text": "Although existing generative models are capable of predicting multiple keyphrases for an input document as well as determining the number of keyphrases to generate, they still suffer from the problem of generating too few keyphrases.", "labels": [], "entities": []}, {"text": "To address this problem, we propose a reinforcement learning (RL) approach for keyphrase generation, with an adaptive reward function that encourages a model to generate both sufficient and accurate keyphrases.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.7184059858322144}, {"text": "keyphrase generation", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.8377944231033325}]}, {"text": "Furthermore, we introduce anew evaluation method that incorporates name variations of the ground-truth keyphrases using the Wikipedia knowledge base.", "labels": [], "entities": [{"text": "Wikipedia knowledge base", "start_pos": 124, "end_pos": 148, "type": "DATASET", "confidence": 0.9568818410237631}]}, {"text": "Thus, our evaluation method can more robustly evaluate the quality of predicted keyphrases.", "labels": [], "entities": []}, {"text": "Extensive experiments on five real-world datasets of different scales demonstrate that our RL approach consistently and significantly improves the performance of the state-of-the-art gener-ative models with both conventional and new evaluation methods.", "labels": [], "entities": [{"text": "RL", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.8851756453514099}]}], "introductionContent": [{"text": "The task of keyphrase generation aims at predicting a set of keyphrases that convey the core ideas of a document.", "labels": [], "entities": [{"text": "keyphrase generation", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8270467221736908}]}, {"text": "shows a sample document and its keyphrase labels.", "labels": [], "entities": []}, {"text": "The keyphrases in red color are present keyphrases that appear in the document, whereas the blue ones are absent keyphrases that do not appear in the input.", "labels": [], "entities": []}, {"text": "By distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering), opinion mining, and summarization (;).", "labels": [], "entities": [{"text": "keyphrase generation", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7767979502677917}, {"text": "document clustering", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.6922192871570587}, {"text": "opinion mining", "start_pos": 191, "end_pos": 205, "type": "TASK", "confidence": 0.8740165829658508}, {"text": "summarization", "start_pos": 211, "end_pos": 224, "type": "TASK", "confidence": 0.9825233817100525}]}, {"text": "\"catSeqD\" is a keyphrase generation model from.", "labels": [], "entities": []}, {"text": "\"catSeqD-2RF 1 \" denotes the catSeqD model after being trained by our RL approach.", "labels": [], "entities": []}, {"text": "The enriched keyphrase labels are based on our new evaluation method.", "labels": [], "entities": []}, {"text": "To produce both present and absent keyphrases, generative methods (,b) are designed to apply the attentional encoder-decoder model () with copy mechanism ( to approach the keyphrase generation task.", "labels": [], "entities": [{"text": "keyphrase generation task", "start_pos": 172, "end_pos": 197, "type": "TASK", "confidence": 0.7967914938926697}]}, {"text": "However, none of the prior models can determine the appropriate number of keyphrases fora document.", "labels": [], "entities": []}, {"text": "In reality, the optimal keyphrase count varies, and is dependent on a given document's content.", "labels": [], "entities": []}, {"text": "To that end, introduced a training setup in which a generative model can learn to decide the number of keyphrases to predict fora given document and proposed two models.", "labels": [], "entities": []}, {"text": "Although they provided a more realistic setup, there still exist two drawbacks.", "labels": [], "entities": []}, {"text": "First, models trained under this setup tend to generate fewer keyphrases than the groundtruth.", "labels": [], "entities": []}, {"text": "Our experiments on the largest dataset show that their catSeqD model generates 4.3 keyphrases per document on average, while these documents have 5.3 keyphrase labels on average.", "labels": [], "entities": []}, {"text": "Ideally, a model should generate both sufficient and accurate keyphrases.", "labels": [], "entities": []}, {"text": "Second, existing evaluation methods rely only on the exact matching of word stems) to determine whether a predicted phrase matches a ground-truth phrase.", "labels": [], "entities": []}, {"text": "For example, given the document in, if a model generates \"support vector machine\", it will be treated as incorrect since it does not match the word \"svm\" given by the gold-standard labels.", "labels": [], "entities": []}, {"text": "It is therefore desirable for an evaluation method to consider name variations of a groundtruth keyphrase.", "labels": [], "entities": []}, {"text": "To address the first limitation, we design an adaptive reward function, RF 1 , that encourages a model to generate both sufficient and accurate keyphrases.", "labels": [], "entities": []}, {"text": "Concretely, if the number of generated keyphrases is less than that of the groundtruth, we use recall as the reward, which does not penalize the model for generating incorrect predictions.", "labels": [], "entities": [{"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9987223744392395}]}, {"text": "If the model generates sufficient keyphrases, we use F 1 score as the reward, to balance both recall and precision of the predictions.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9868014256159464}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9991998076438904}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9981086254119873}]}, {"text": "To optimize the model towards this nondifferentiable reward function, we formulate the task of keyphrase generation as a reinforcement learning (RL) problem and adopt the self-critical policy gradient method ( as the training procedure.", "labels": [], "entities": [{"text": "keyphrase generation", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.8113896250724792}]}, {"text": "Our RL approach is flexible and can be applied to any keyphrase generative model with an encoder-decoder structure.", "labels": [], "entities": []}, {"text": "In, we show a prediction result of the catSeqD model ( and another prediction result of the catSeqD model after being trained by our RL approach (catSeqD-2RF 1 ).", "labels": [], "entities": []}, {"text": "This example illustrates that our RL approach encourages the model to generate more correct keyphrases.", "labels": [], "entities": [{"text": "RL", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9572078585624695}]}, {"text": "Perhaps more importantly, the number of generated keyphrases also increases to five, which is closer to the ground-truth number).", "labels": [], "entities": []}, {"text": "Furthermore, we propose anew evaluation method to tackle the second limitation.", "labels": [], "entities": []}, {"text": "For each ground-truth keyphrase, we extract its name variations from various sources.", "labels": [], "entities": []}, {"text": "If the word stems of a predicted keyphrase match the word stems of any name variation of a ground-truth keyphrase, it is treated as a correct prediction.", "labels": [], "entities": []}, {"text": "For instance, in, our evaluation method enhances the \"svm\" ground-truth keyphrase with its name variation, \"support vector machine\".", "labels": [], "entities": []}, {"text": "Thus, the phrase \"support vector machine\" generated by catSeqD and catSeqD-2RF 1 will be considered correct, which demonstrates that our evaluation method is more robust than the existing one.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments to evaluate the performance of our RL approach.", "labels": [], "entities": [{"text": "RL", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9728620648384094}]}, {"text": "Experiment results on five real-world datasets show that our RL approach consistently improves the performance of the state-of-the-art models in terms of F -measures.", "labels": [], "entities": [{"text": "RL", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9428427815437317}, {"text": "F -measures", "start_pos": 154, "end_pos": 165, "type": "METRIC", "confidence": 0.962121844291687}]}, {"text": "Moreover, we analyze the sufficiency of the keyphrases generated by different models.", "labels": [], "entities": []}, {"text": "It is observed that models trained by our RL approach generate more absent keyphrases, which is closer to the number of absent groundtruth keyphrases.", "labels": [], "entities": [{"text": "RL", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9425101280212402}]}, {"text": "Finally, we deploy our new evaluation method on the largest keyphrase generation benchmark, and the new evaluation identifies at least one name variation for 14.1% of the groundtruth keyphrases.", "labels": [], "entities": []}, {"text": "We summarize our contributions as follows: (1) an RL approach with a novel adaptive reward function that explicitly encourages the model to generate both sufficient and accurate keyphrases; (2) anew evaluation method that considers name variations of the keyphrase labels; and (3) the new state-of-the-art performance on five real-world datasets in a setting where a model is able to determine the number of keyphrases to generate.", "labels": [], "entities": []}, {"text": "This is the first work to study RL approach on the keyphrase generation problem.", "labels": [], "entities": [{"text": "RL", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9845660328865051}, {"text": "keyphrase generation problem", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.8753373026847839}]}], "datasetContent": [{"text": "Our new evaluation method maintains a set of name variations\u02dcyvariations\u02dc variations\u02dcy i for each ground-truth keyphrase y i of x.", "labels": [], "entities": []}, {"text": "If a predicted keyphras\u00ea y i matches any name variation of a ground-truth keyphrase, then\u02c6ythen\u02c6 then\u02c6y i is considered a correct prediction.", "labels": [], "entities": []}, {"text": "A ground-truth keyphrase is also its own name variation.", "labels": [], "entities": []}, {"text": "If there are multiple ground-truth keyphrases in x that have the same name variations set, we will only keep one of them.", "labels": [], "entities": []}, {"text": "In our evaluation method, the name variation set of a ground-truth keyphrase may contain both present phrases and absent phrases.", "labels": [], "entities": []}, {"text": "In such a case, a ground-truth keyphrase can be matched by a present predicted keyphrase or an absent predicted keyphrase.", "labels": [], "entities": []}, {"text": "Thus, this ground-truth keyphrase should be treated as both a present ground-truth keyphrase and an absent ground-truth keyphrase, as shown in the following definition.", "labels": [], "entities": []}, {"text": "Present (Absent) ground-truth keyphrase.", "labels": [], "entities": [{"text": "Absent", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.827305018901825}]}, {"text": "If a name variation set\u02dcyset\u02dc set\u02dcy i of a groundtruth keyphrase y i only consists of present (absent) keyphrases, then y i is a present (absent) ground-truth keyphrase.", "labels": [], "entities": []}, {"text": "Otherwise, y i is both a present ground-truth keyphrase and an absent ground-truth keyphrase, i.e., y i \u2208 Y p and y i \u2208 Y a .  We first report the performance of different models using the conventional evaluation method.", "labels": [], "entities": []}, {"text": "Afterwards, we present the results based on our new evaluation method.", "labels": [], "entities": []}, {"text": "All experiments are repeated for three times using different random seeds and the averaged results are reported.", "labels": [], "entities": []}, {"text": "The source code and the enriched evaluation set are released to the public 2 . Sample output is shown in.", "labels": [], "entities": []}, {"text": "We conduct experiments on five scientific article datasets, including, Inspec (Hulth, 2003),,, and SemEval (.", "labels": [], "entities": [{"text": "Inspec (Hulth, 2003)", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.7958248655001322}]}, {"text": "Each sample from these datasets consists of the title, abstract, and keyphrases of a scientific article.", "labels": [], "entities": []}, {"text": "We concatenate the title and abstract as an input document, and use the assigned keyphrases as keyphrase labels.", "labels": [], "entities": []}, {"text": "Following the setup in (), we use the training set of the largest dataset, KP20k, for model training and the testing sets of all five datasets to evaluate the performance of a generative model.", "labels": [], "entities": [{"text": "KP20k", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.8265151977539062}]}, {"text": "From the training set of KP20k, we remove all articles that are duplicated in itself, either in the KP20k validation set, or in any of the five testing sets.", "labels": [], "entities": [{"text": "KP20k", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.881870687007904}, {"text": "KP20k validation set", "start_pos": 100, "end_pos": 120, "type": "DATASET", "confidence": 0.8754706581433614}]}, {"text": "After the cleanup, the KP20k dataset contains 509,818 training samples, 20,000 validation samples, and 20,000 testing samples.", "labels": [], "entities": [{"text": "KP20k dataset", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9262560307979584}]}, {"text": "The performance of a model is typically evaluated by comparing the top k predicted keyphrases with the ground-truth keyphrases.", "labels": [], "entities": []}, {"text": "The evaluation cutoff k can be either a fixed number or a variable.", "labels": [], "entities": []}, {"text": "Most previous work,b) adopted evaluation metrics with fixed evaluation cutoffs, e.g., F 1 @5.", "labels": [], "entities": [{"text": "F 1", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9724830985069275}]}, {"text": "Recently, proposed anew evaluation metric, F 1 @M , which has a variable evaluation cutoff.", "labels": [], "entities": [{"text": "F 1", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9516853392124176}]}, {"text": "F 1 @M compares all the keyphrases predicted by the model with the ground-truth to compute an F 1 score, i.e., k = number of predictions.", "labels": [], "entities": [{"text": "F 1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9189627766609192}, {"text": "F 1 score", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9715550144513448}]}, {"text": "It can also be interpreted as the original F 1 score with no evaluation cutoff.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9778626362482706}]}, {"text": "We evaluate the performance of a model using a metric with a variable cutoff and a metric with a fixed cutoff, namely, F 1 @M and F 1 @5. Marco average is deployed to aggregate the evaluation scores for all testing samples.", "labels": [], "entities": [{"text": "F 1 @5. Marco average", "start_pos": 130, "end_pos": 151, "type": "METRIC", "confidence": 0.8572203318277994}]}, {"text": "We apply Porter Stemmer before determining whether two phrases are matched.", "labels": [], "entities": []}, {"text": "Our implementation of F 1 @5 is different from that of.", "labels": [], "entities": []}, {"text": "Specifically, when computing F 1 @5, if a model generates less than five predictions, we append random wrong answers to the prediction until it reaches five predictions . The rationale is to avoid producing similar F 1 @5 and F 1 @M , when a model (e.g., catSeq) generates less than five keyphrases, as shown in the  We extract name variations for all keyphrase labels in the testing set of KP20k dataset, following the methodology in Section 5.", "labels": [], "entities": [{"text": "KP20k dataset", "start_pos": 391, "end_pos": 404, "type": "DATASET", "confidence": 0.9565304219722748}]}, {"text": "Our method extracts at least one additional name variation for 14.1% of the ground-truth keyphrases.", "labels": [], "entities": []}, {"text": "For these enhanced keyphrases, the average number of name variations extracted is 1.01.", "labels": [], "entities": []}, {"text": "Among all extracted name variations, 14.1% come from the acronym in the ground-truth, 28.2% from the Wikipedia disambiguation pages, and the remaining 61.6% from Wikipedia entity page titles.", "labels": [], "entities": []}, {"text": "We use our new evaluation method to evaluate the performance of different keyphrase generation models, and compare with the existing evaluation method.", "labels": [], "entities": []}, {"text": "shows that for all generative models, the evaluation scores computed by our method are higher than those computed by prior method.", "labels": [], "entities": []}, {"text": "This demonstrates that our proposed evaluation successfully captures name variations of groundtruth keyphrases generated by different models, and can therefore evaluate the quality of generated keyphrases in a more robust manner.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of present keyphrase prediction on five datasets. Suffix \"-2RF 1 \" denotes that a model is trained  by our reinforcement learning approach.", "labels": [], "entities": [{"text": "present keyphrase prediction", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6196331083774567}, {"text": "Suffix", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9881673455238342}, {"text": "2RF 1", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.8976880609989166}]}, {"text": " Table 3: Results of absent keyphrase prediction on five datasets.", "labels": [], "entities": [{"text": "absent keyphrase prediction", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6331250667572021}]}, {"text": " Table 4: The abilities of predicting the correct number  of keyphrases on the KP20k dataset. MAE denotes the  mean absolute error (the lower the better), Avg. # de- notes the average number of generated keyphrases per  document.", "labels": [], "entities": [{"text": "KP20k dataset", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.948135495185852}, {"text": "MAE", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.996711254119873}, {"text": "mean absolute error", "start_pos": 111, "end_pos": 130, "type": "METRIC", "confidence": 0.6717683672904968}, {"text": "Avg", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9885246157646179}]}, {"text": " Table 5: Ablation study on the KP20k dataset. Suf- fix \"-2RF 1 \" denotes our full RL approach. Suffix \"- 2F 1 \" denotes that we replace our adaptive RF 1 reward  function in the full approach by an F 1 reward function.  Suffix \"-RF 1 \" denotes that we replace the two separate  RF 1 reward signals in our full approach with only one  RF 1 reward signal for all the generated keyphrases.", "labels": [], "entities": [{"text": "KP20k dataset", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9700363874435425}, {"text": "Suf- fix \"-2RF 1", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.7926934560139974}]}, {"text": " Table 6: Keyphrase prediction results on the KP20k  dataset with our new evaluation method.", "labels": [], "entities": [{"text": "Keyphrase prediction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7581223547458649}, {"text": "KP20k  dataset", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.9603856801986694}]}]}