{"title": [{"text": "Multi-task Pairwise Neural Ranking for Hashtag Segmentation", "labels": [], "entities": [{"text": "Hashtag Segmentation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.9207490980625153}]}], "abstractContent": [{"text": "Hashtags are often employed on social media and beyond to add metadata to a tex-tual utterance with the goal of increasing dis-coverability, aiding search, or providing additional semantics.", "labels": [], "entities": []}, {"text": "However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings.", "labels": [], "entities": []}, {"text": "We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hash-tag segmentation by framing it as a pairwise ranking problem between candidate segmen-tations.", "labels": [], "entities": [{"text": "hash-tag segmentation", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.7561211287975311}]}, {"text": "1 Our novel neural approaches demonstrate 24.6% error reduction in hashtag seg-mentation accuracy compared to the current state-of-the-art method.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.9543468952178955}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9464021921157837}]}, {"text": "Finally, we demonstrate that a deeper understanding of hash-tag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the Se-mEval 2017 sentiment analysis dataset.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.9672252237796783}, {"text": "recall", "start_pos": 219, "end_pos": 225, "type": "METRIC", "confidence": 0.9812207221984863}, {"text": "Se-mEval 2017 sentiment analysis dataset", "start_pos": 233, "end_pos": 273, "type": "DATASET", "confidence": 0.7287283420562745}]}], "introductionContent": [{"text": "A hashtag is a keyphrase represented as a sequence of alphanumeric characters plus underscore, preceded by the # symbol.", "labels": [], "entities": []}, {"text": "Hashtags play a central role in online communication by providing a tool to categorize the millions of posts generated daily on Twitter, Instagram, etc.", "labels": [], "entities": []}, {"text": "They are useful in search, tracking content about a certain topic, or discovering emerging trends (.", "labels": [], "entities": []}, {"text": "Hashtags often carry very important information, such as emotion (Abdul-Mageed and Ungar, Our toolkit along with the code and data are publicly available at https://github.com/mounicam/ hashtag_master", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present experimental results that compare our proposed method with the other state-of-the-art approaches on hashtag segmentation datasets.", "labels": [], "entities": [{"text": "hashtag segmentation", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.8103236854076385}]}, {"text": "The next section will show experiments of applying hashtag segmentation to the popular task of sentiment analysis.", "labels": [], "entities": [{"text": "hashtag segmentation", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.716900572180748}, {"text": "sentiment analysis", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9495137631893158}]}, {"text": "We evaluate the performance by the top k (k = 1, 2) accuracy (A@1, A@2), average token-level F 1 score (F 1 @1), and mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9958255290985107}, {"text": "token-level F 1 score (F 1 @1)", "start_pos": 81, "end_pos": 111, "type": "METRIC", "confidence": 0.8171315103769302}, {"text": "mean reciprocal rank (MRR)", "start_pos": 117, "end_pos": 143, "type": "METRIC", "confidence": 0.8939202129840851}]}, {"text": "In particular, the accuracy and MRR are calculated at the segmentation-level, which means that an output segmentation is considered correct if and only if it fully matches the human segmentation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.999772846698761}, {"text": "MRR", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9983614087104797}]}, {"text": "Average token-level F 1 score accounts for partially correct segmentation in the multi-token hashtag cases.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8764294981956482}]}, {"text": "show the results on the STAN small and STAN large datasets, respectively.", "labels": [], "entities": [{"text": "STAN large datasets", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.8500267664591471}]}, {"text": "All of our pairwise neural rankers are trained on the 2,518 manually segmented hashtags in the training set of STAN large and perform favorably against other state-of-the-art approaches.", "labels": [], "entities": [{"text": "STAN large", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.5987501591444016}]}, {"text": "Our best model (MSE+multitask) that utilizes different features adaptively via a multi-task learning procedure is shown to perform better than simply combining all the features together (MR and MSE).", "labels": [], "entities": []}, {"text": "We highlight the 24.6% error reduction on STAN small and 16.5% on STAN large of our approach over the previous SOTA (C \u00b8 elebi and\u00a8Ozg\u00fcrand\u00a8 and\u00a8Ozg\u00fcr, 2017) on the Multi-token hashtags, and the importance of having a separate evaluation of multi-word cases as it is trivial to obtain 100% accuracy for Singletoken hashtags.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.9811842441558838}, {"text": "accuracy", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.9952184557914734}]}, {"text": "While our hashtag segmentation model is achieving a very high accuracy@2, to be practically useful, it remains a challenge to get the top one predication exactly correct.", "labels": [], "entities": [{"text": "hashtag segmentation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7527393698692322}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9977508187294006}]}, {"text": "Some hashtags are very difficult to interpret, e.g., #BTVSMB refers to the Social Media Breakfast (SMB) in Burlington, Vermont (BTV).", "labels": [], "entities": [{"text": "BTVSMB", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.7872296571731567}, {"text": "Social Media Breakfast (SMB) in Burlington, Vermont (BTV)", "start_pos": 75, "end_pos": 132, "type": "DATASET", "confidence": 0.7047591599134299}]}, {"text": "The improved Word Breaker with our addition of a Twitter-specific language model is a very strong   baseline, which echos the findings of the original Word Breaker paper () that having a large in-domain language model is extremely helpful for word segmentation tasks.", "labels": [], "entities": [{"text": "Word Breaker", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.7781268954277039}, {"text": "Word Breaker paper", "start_pos": 151, "end_pos": 169, "type": "DATASET", "confidence": 0.7195224165916443}, {"text": "word segmentation tasks", "start_pos": 243, "end_pos": 266, "type": "TASK", "confidence": 0.8399127920468649}]}, {"text": "It is worth noting that the other state-of-the-art system (C \u00b8 elebi and\u00a8Ozg\u00fcrand\u00a8 and\u00a8Ozg\u00fcr, 2017) also utilized a 4-gram language model trained on 476 million tweets from 2009.", "labels": [], "entities": []}, {"text": "We attempt to demonstrate the effectiveness of our hashtag segmentation system by studying its impact on the task of sentiment analysis in Twitter (.", "labels": [], "entities": [{"text": "hashtag segmentation", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7306630909442902}, {"text": "sentiment analysis", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.9304765462875366}]}, {"text": "We use our best model (MSE+multitask), under the name HashtagMaster, in the following experiments.", "labels": [], "entities": []}, {"text": "We compare the performance of the BiLSTM+Lex () sentiment analysis model under three configurations: (a) tweets with hashtags removed, (b) tweets with hashtags as single tokens excluding the # symbol, and (c) tweets with hashtags as segmented by our system, HashtagMaster.", "labels": [], "entities": [{"text": "BiLSTM+Lex () sentiment analysis", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.5891019950310389}]}, {"text": "BiLSTM+Lex is a state-of-the-art open source system for predicting tweet-level sentiment (.", "labels": [], "entities": [{"text": "BiLSTM+Lex", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.6936099330584208}, {"text": "predicting tweet-level sentiment", "start_pos": 56, "end_pos": 88, "type": "TASK", "confidence": 0.8845522403717041}]}, {"text": "It learns a context-sensitive sentiment intensity score by leveraging a Twitterbased sentiment lexicon ().", "labels": [], "entities": []}, {"text": "We use the same settings as described by to train the model.", "labels": [], "entities": []}, {"text": "We use the dataset from the Sentiment Analysis in Twitter shared task (subtask A) at SemEval 2017 (.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8559570014476776}, {"text": "Twitter shared task (subtask A) at SemEval 2017", "start_pos": 50, "end_pos": 97, "type": "DATASET", "confidence": 0.6579335421323776}]}, {"text": "Given a tweet, the goal is to predict whether it expresses POSITIVE, NEGATIVE or NEUTRAL sentiment.", "labels": [], "entities": [{"text": "POSITIVE", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9795507192611694}, {"text": "NEGATIVE", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.4517155885696411}]}, {"text": "The training and development sets consist of 49,669 tweets and we use 40,000 for training and the rest for development.", "labels": [], "entities": []}, {"text": "There area total of 4,840 tweets containing 12,128 hashtags in the SemEval 2017 test set, and our hashtag segmenter ended up splitting 6,975 of those hashtags present in 3,384 tweets.", "labels": [], "entities": [{"text": "SemEval 2017 test set", "start_pos": 67, "end_pos": 88, "type": "DATASET", "confidence": 0.7652121111750603}]}], "tableCaptions": [{"text": " Table 4: Evaluation results on the corrected version of STAN small . For reference, on the original version of  STAN small , the Microsoft Word Breaker API reported an 84.6% F 1 score and an 83.6% accuracy for the top one  output (C \u00b8 elebi and\u00a8Ozg\u00fcrand\u00a8 and\u00a8Ozg\u00fcr, 2017), while our best model (MSE+multitask) reported 89.8% F 1 and 91.0% accuracy.", "labels": [], "entities": [{"text": "Microsoft Word Breaker", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.506334106127421}, {"text": "F 1 score", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9882063865661621}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9963654279708862}, {"text": "F 1", "start_pos": 326, "end_pos": 329, "type": "METRIC", "confidence": 0.9948471784591675}, {"text": "accuracy", "start_pos": 340, "end_pos": 348, "type": "METRIC", "confidence": 0.9914968609809875}]}, {"text": " Table 5: Evaluation results on our STAN large test dataset. For single-token hashtags, the token-level F 1 @1 is  equivalent to segmentation-level A@1. For multi-token cases, A@1 and F 1 @1 for the original hashtag base- line are non-zero because 11.4% of the hashtags have more than one acceptable segmentations. Our best model  (MSE+multitask) shows a statistically significant improvement (p < 0.05) over the state-of-the-art approach  (C \u00b8 elebi and\u00a8Ozg\u00fcrand\u00a8 and\u00a8Ozg\u00fcr, 2017) based on the paired bootstrap test", "labels": [], "entities": [{"text": "STAN large test dataset", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.7924912944436073}]}, {"text": " Table 6: Evaluation of automatic hashtag segmentation  (MSE) with different features on the STAN large dev set.  A denotes accuracy@1. While Kneser-Ney features  perform well on single-token hashtags, GT+Ling fea- tures perform better on multi-token hashtags.", "labels": [], "entities": [{"text": "automatic hashtag segmentation  (MSE)", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.7726086427768072}, {"text": "STAN large dev set", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.7743472754955292}, {"text": "A", "start_pos": 114, "end_pos": 115, "type": "METRIC", "confidence": 0.9558461308479309}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9848110675811768}]}, {"text": " Table 8. With a segmentation-level  accuracy of 94.6% and average token-level F 1  score of 95.6%, our approach performs favorably  on 2019 hashtags.", "labels": [], "entities": [{"text": "segmentation-level", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9384844899177551}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9282154440879822}, {"text": "token-level F 1  score", "start_pos": 67, "end_pos": 89, "type": "METRIC", "confidence": 0.7899107933044434}]}, {"text": " Table 8: Evaluation results on 500 random hashtags  from the year 2019.", "labels": [], "entities": []}, {"text": " Table 9: Sentiment analysis evaluation on the 3384  tweets from SemEval 2017 test set using the BiL- STM+Lex method", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9681157767772675}, {"text": "SemEval 2017 test set", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.7734091430902481}]}]}