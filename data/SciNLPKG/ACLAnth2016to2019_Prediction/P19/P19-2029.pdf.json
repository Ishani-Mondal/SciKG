{"title": [{"text": "Multiple Character Embeddings for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.5564607580502828}]}], "abstractContent": [{"text": "Chinese word segmentation (CWS) is often regarded as a character-based sequence labeling task inmost current works which have achieved great success with the help of powerful neural networks.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.763429989417394}, {"text": "character-based sequence labeling task", "start_pos": 55, "end_pos": 93, "type": "TASK", "confidence": 0.7279993295669556}]}, {"text": "However, these works neglect an important clue: Chinese characters incorporate both semantic and phonetic meanings.", "labels": [], "entities": []}, {"text": "In this paper, we introduce multiple character embeddings including Pinyin Romaniza-tion and Wubi Input, both of which are easily accessible and effective in depicting semantics of characters.", "labels": [], "entities": []}, {"text": "We propose a novel shared Bi-LSTM-CRF model to fuse linguistic features efficiently by sharing the LSTM network during the training procedure.", "labels": [], "entities": []}, {"text": "Extensive experiments on five corpora show that extra embed-dings help obtain a significant improvement in labeling accuracy.", "labels": [], "entities": [{"text": "labeling", "start_pos": 107, "end_pos": 115, "type": "TASK", "confidence": 0.9482199549674988}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9408140778541565}]}, {"text": "Specifically, we achieve the state-of-the-art performance in AS and CityU corpora with F1 scores of 96.9 and 97.3, respectively without leveraging any external lexical resources.", "labels": [], "entities": [{"text": "CityU corpora", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.8731653392314911}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9993126392364502}]}], "introductionContent": [{"text": "Chinese is written without explicit word delimiters so word segmentation (CWS) is a preliminary and essential pre-processing step for most natural language processing (NLP) tasks in Chinese, such as part-of-speech tagging (POS) and named-entity recognition (NER).", "labels": [], "entities": [{"text": "word segmentation (CWS)", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7967028379440307}, {"text": "part-of-speech tagging (POS)", "start_pos": 199, "end_pos": 227, "type": "TASK", "confidence": 0.8376262068748475}, {"text": "named-entity recognition (NER)", "start_pos": 232, "end_pos": 262, "type": "TASK", "confidence": 0.8374163568019867}]}, {"text": "The representative approaches are treating CWS as a character-based sequence labeling task following and.", "labels": [], "entities": [{"text": "character-based sequence labeling task", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.6815204322338104}]}, {"text": "Although not relying on hand-crafted features, most of the neural network models rely heavily on the embeddings of characters.", "labels": [], "entities": []}, {"text": "Since proposed word2vec technique, the vector representation of words or characters has become * Equal contribution (alphabetical order).", "labels": [], "entities": []}, {"text": "a prerequisite for neural networks to solve NLP tasks in different languages.", "labels": [], "entities": []}, {"text": "However, existing approaches neglect an important fact that Chinese characters contain both semantic and phonetic meanings -there are various representations of characters designed for capturing these features.", "labels": [], "entities": []}, {"text": "The most intuitive one is Pinyin Romanization (\u62fc\u97f3) that keeps many-toone relationship with Chinese characters -for one character, different meanings in specific context may lead to different pronunciations.", "labels": [], "entities": []}, {"text": "This phenomenon called Polyphony (and Polysemy) in linguistics is very common and crucial to word segmentation task.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.8334974845250448}]}, {"text": "Apart from Pinyin Romanization, Wubi Input (\u4e94\u7b14) is another effective representation which absorbs semantic meanings of Chinese characters.", "labels": [], "entities": []}, {"text": "Compared to Radical (\u504f\u65c1) (, Wubi includes more comprehensive graphical and structural information that is highly relevant to the semantic meanings and word boundaries, due to plentiful pictographic characters in Chinese and effectiveness of Wubi in embedding the structures.", "labels": [], "entities": []}, {"text": "This paper will thoroughly study how important the extra embeddings are and what scholars can achieve by combining extra embeddings with representative models.", "labels": [], "entities": []}, {"text": "To leverage extra phonetic and semantic information efficiently, we propose a shared Bi-LSTMs-CRF model, which feeds embeddings into three stacked LSTM layers with shared parameters and finally scores with CRF layer.", "labels": [], "entities": []}, {"text": "We evaluate the proposed approach on five corpora and demonstrate that our method produces state-of-the-art results and is highly efficient as previous single-embedding scheme.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows: 1) We firstly propose to leverage both semantic and phonetic features of Chinese characters in NLP tasks by introducing Pinyin Romanization and Wubi Input embeddings, which are easily accessible and effective in representing semantic and phonetic features; 2) We put forward a shared Bi-LSTM-CRF model for efficiently integrating multiple embeddings and sharing useful linguistic features; 3) We evaluate the proposed multiembedding scheme on Bakeoff2005 and CTB6 corpora.", "labels": [], "entities": [{"text": "Bakeoff2005 and CTB6 corpora", "start_pos": 488, "end_pos": 516, "type": "DATASET", "confidence": 0.8455657958984375}]}, {"text": "Extensive experiments show that auxiliary embeddings help achieve state-of-the-art performance without external lexical resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we provide empirical results to verify the effectiveness of multiple embeddings for CWS.", "labels": [], "entities": []}, {"text": "Besides, our proposed Model-III can be: Comparison of different architectures on five corpora.", "labels": [], "entities": []}, {"text": "Bold font signifies the best performance in all given models.", "labels": [], "entities": []}, {"text": "Our proposed multiple-embedding models result in a significant improvement compared to vanilla character-embedding baseline model.", "labels": [], "entities": []}, {"text": "trained efficiently (slightly costly than baseline) and obtain the state-of-the-art performance.", "labels": [], "entities": []}, {"text": "To make the results comparable and convincing, we evaluate our models on) and Chinese Treebank 6.0 (CTB6) () datasets, which are widely used in previous works.", "labels": [], "entities": [{"text": "Chinese Treebank 6.0 (CTB6) () datasets", "start_pos": 78, "end_pos": 117, "type": "DATASET", "confidence": 0.9291538745164871}]}, {"text": "We leverage standard word2vec tool to train multiple embeddings.", "labels": [], "entities": []}, {"text": "In experiments, we tuned the embedding size following and assigned equal size (256) for three types of embedding.", "labels": [], "entities": []}, {"text": "The number of Bi-LSTM layers is set as 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of different architectures on five corpora. Bold font signifies the best performance in all  given models. Our proposed multiple-embedding models result in a significant improvement compared to vanilla  character-embedding baseline model.", "labels": [], "entities": []}, {"text": " Table 2: Comparison with previous state-of-the-art  models on all four Bakeoff2005 datasets. The second  block (*) represents allowing the use of external re- sources such as lexicon dictionary or trained embed- dings on large-scale external corpora. Note that our  WB approach does not leverage any external resources.", "labels": [], "entities": [{"text": "Bakeoff2005 datasets", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.9651234149932861}]}, {"text": " Table 3: Feature ablation on CTB6 and CityU. IO +  PY and IO + WB denote injecting Pinyin and Wubi  embeddings separately under Model-II.", "labels": [], "entities": [{"text": "CTB6", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9710816144943237}, {"text": "CityU", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8417173624038696}, {"text": "IO +  PY", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8724918166796366}]}]}