{"title": [{"text": "Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data", "labels": [], "entities": [{"text": "Episodic Memory Reader", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6542208592096964}, {"text": "Remember", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.7849535942077637}, {"text": "Question Answering from Streaming Data", "start_pos": 54, "end_pos": 92, "type": "TASK", "confidence": 0.7786934316158295}]}], "abstractContent": [{"text": "We consider a novel question answering (QA) task where the machine needs to read from large streaming data (long documents or videos) without knowing when the questions will be given, which is difficult to solve with existing QA methods due to their lack of scal-ability.", "labels": [], "entities": [{"text": "question answering (QA) task", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.8560688346624374}]}, {"text": "To tackle this problem, we propose a novel end-to-end deep network model for reading comprehension, which we refer to as Episodic Memory Reader (EMR) that sequentially reads the input contexts into an external memory, while replacing memories that are less important for answering unseen questions.", "labels": [], "entities": []}, {"text": "Specifically, we train an RL agent to replace a memory entry when the memory is full, in order to maximize its QA accuracy at a future timepoint, while encoding the external memory using either the GRU or the Transformer architecture to learn representations that considers relative importance between the memory entries.", "labels": [], "entities": [{"text": "QA", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.8706932663917542}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.5291661620140076}, {"text": "GRU", "start_pos": 198, "end_pos": 201, "type": "DATASET", "confidence": 0.9401375651359558}]}, {"text": "We validate our model on a synthetic dataset (bAbI) as well as real-world large-scale textual QA (TriviaQA) and video QA (TVQA) datasets, on which it achieves significant improvements over rule-based memory scheduling policies or an RL-based baseline that independently learns the query-specific importance of each memory.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) problem is one of the most important challenges in Natural Language Understanding (NLU).", "labels": [], "entities": [{"text": "Question answering (QA) problem", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9270078440507253}, {"text": "Natural Language Understanding (NLU)", "start_pos": 75, "end_pos": 111, "type": "TASK", "confidence": 0.775949498017629}]}, {"text": "In recent years, there has been drastic progress on the topic, owing to the success of deep learning based QA models (.", "labels": [], "entities": []}, {"text": "On certain tasks such as machine reading comprehension (MRC), where * Equal contribution the problem is to find the span of the answer within a given paragraph (, the deep-learning based QA models have even surpassed human-level performances.", "labels": [], "entities": [{"text": "machine reading comprehension (MRC)", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.7723381022612253}]}, {"text": "Despite such impressive achievements, it is still challenging to model question answering with document-level context (, where the context may include along document with a large number of paragraphs, due to problems such as difficulty in modeling long-term dependency and computational cost.", "labels": [], "entities": [{"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7242508381605148}]}, {"text": "To overcome such scalability problems, researchers have proposed pipelining or confidence based selection methods that combine paragraph-level models to obtain a document-level model ().", "labels": [], "entities": []}, {"text": "Yet, such models are applicable only when questions are given beforehand and all sentences in the document can be stored in memory.", "labels": [], "entities": []}, {"text": "However, in realistic settings, the amount of context maybe too large to fit into the system memory.", "labels": [], "entities": []}, {"text": "We may consider query-based context selection methods such as ones proposed in and , but in many cases, the question may not be given when reading in the context, and thus it would be difficult to select out the context based on the question.", "labels": [], "entities": [{"text": "query-based context selection", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6145552098751068}]}, {"text": "For example, a conversation agent may need to answer a question after numerous conversations in a long-term time period, and a video QA model may need to watch an entire movie, or a sports game, or days of streaming videos from security cameras before answering a question.", "labels": [], "entities": []}, {"text": "In such cases, existing QA models will fail to solve the problem due to memory limitation.", "labels": [], "entities": []}, {"text": "In this paper, we target a novel problem of solving question answering problem with streaming data as context, where the size of the context could be significantly larger than what the memory can accommodate (See.", "labels": [], "entities": [{"text": "question answering problem", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.8379273811976115}]}, {"text": "In such a case, the: Concept: We consider a novel problem of learning from streaming data, where the QA model may need to answer a question that is given after reading in unlimited amount of context.", "labels": [], "entities": []}, {"text": "To solve this problem, our Episodic Memory Reader (EMR) learns to retain the most important context vectors in an external memory, while replacing the memory entries in order to maximize its accuracy on an unseen question given at a future timestep.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9957108497619629}]}, {"text": "model needs to carefully manage what to remember from this streaming data such that the memory contains the most informative context instances in order to answer an unseen question in the future.", "labels": [], "entities": []}, {"text": "We pose this memory management problem as a learning problem and train both the memory representation and the scheduling agent using reinforcement learning.", "labels": [], "entities": [{"text": "memory management", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.69601009786129}]}, {"text": "Specifically, we propose to train the memory module itself using reinforcement learning to replace the most uninformative memory entry in order to maximize its reward on a given task.", "labels": [], "entities": []}, {"text": "However, this is a seemingly ill-posed problem since for most of the time, the scheduling should be performed without knowing which question will arrive next.", "labels": [], "entities": []}, {"text": "To tackle this challenge, we implement the policy network and the value network that learn not only relation between sentences and query but also relative importance among the sentences in order to maximize its question answering accuracy at a future timepoint.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.7700985670089722}]}, {"text": "We refer to this network as Episodic Memory Reader (EMR).", "labels": [], "entities": [{"text": "Episodic Memory Reader (EMR)", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6330879330635071}]}, {"text": "EMR can perform selective memorization to keep a compact set of important context that will be useful for future tasks in lifelong learning scenarios.", "labels": [], "entities": []}, {"text": "We validate our proposed memory network on a large-scale QA task (TriviaQA) and video question answering task (TVQA) where the context is too large to fit into the external memory, against rule-based and an RL-based scheduling method without consideration of relative importance between memories.", "labels": [], "entities": [{"text": "video question answering task (TVQA", "start_pos": 80, "end_pos": 115, "type": "TASK", "confidence": 0.6361177762349447}]}, {"text": "The results show that our model significantly outperforms the baselines, due to its ability to preserve the most important pieces of information from the streaming data.", "labels": [], "entities": []}, {"text": "Our contribution is threefold: \u2022 We consider a novel task of learning to remember important instances from streaming data for question answering task, where the size of the memory is significantly smaller than the length of the data stream.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 126, "end_pos": 149, "type": "TASK", "confidence": 0.8812679847081503}]}, {"text": "\u2022 We propose a novel end-to-end memoryaugmented neural architecture for solving QA from streaming data, where we train a scheduling agent via reinforcement learning to store the most important memory entries for solving future QA tasks.", "labels": [], "entities": []}, {"text": "\u2022 We validate the efficacy of our model on realworld large-scale text and video QA datasets, on which it obtains significantly improved performances over baseline methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment our EMR-biGRU and EMRTransformer against several baselines: 1) FIFO (First-In First-Out).", "labels": [], "entities": [{"text": "FIFO", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9983830451965332}]}, {"text": "A rule-based memory scheduling policy that replaces the oldest memory entry.", "labels": [], "entities": []}, {"text": "A policy that replaces all memory entries with equal probability at each time.", "labels": [], "entities": []}, {"text": "3) LIFO (Last-In First-Out).", "labels": [], "entities": [{"text": "LIFO", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9971629977226257}]}, {"text": "A policy that replaces the newest data instance.", "labels": [], "entities": []}, {"text": "That is, it first fills in the memory and then discards all following data instances.", "labels": [], "entities": []}, {"text": "A baseline EMR which learns the importance of each memory entry only relative to the new data instance.", "labels": [], "entities": []}, {"text": "An EMR implemented using a biGRU, that considers relative importance of each memory entry to its neighbors when learning the memory replacement policy.", "labels": [], "entities": [{"text": "memory replacement", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.6727065891027451}]}, {"text": "An EMR that utilizes Transformer to model the global relative importance between memory entries.", "labels": [], "entities": []}, {"text": "The codes for the baseline models and our models are available at https://github.com/ h19920918/emr.", "labels": [], "entities": []}, {"text": "In the next subsections, we present experimental results on bAbI, TriviaQA, and TVQA datasets.", "labels": [], "entities": [{"text": "TVQA datasets", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.963770180940628}]}, {"text": "For more experimental results, please see supplementary file.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Q&A accuracy on the TriviaQA dataset. model.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8144095142682394}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.8088080883026123}, {"text": "TriviaQA dataset.", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9626010954380035}]}]}