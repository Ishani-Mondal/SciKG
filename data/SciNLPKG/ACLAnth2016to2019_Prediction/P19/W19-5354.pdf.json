{"title": [{"text": "The MUCOW test suite at WMT 2019: Automatically harvested multilingual contrastive word sense disambiguation test sets for machine translation", "labels": [], "entities": [{"text": "WMT", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.8370140790939331}, {"text": "multilingual contrastive word sense disambiguation", "start_pos": 58, "end_pos": 108, "type": "TASK", "confidence": 0.6794940412044526}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7297802865505219}]}], "abstractContent": [{"text": "Supervised Neural Machine Translation (NMT) systems currently achieve impressive translation quality for many language pairs.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8517247637112936}]}, {"text": "One of the key features of a correct translation is the ability to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.7268277903397878}]}, {"text": "Existing evaluation benchmarks on WSD capabilities of translation systems rely heavily on manual work and cover only few language pairs and few word types.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9745527505874634}]}, {"text": "We present MU-COW, a multilingual contrastive test suite that covers 16 language pairs with more than 200 000 contrastive sentence pairs, automatically built from word-aligned parallel corpora and the wide-coverage multilingual sense inventory of BabelNet.", "labels": [], "entities": []}, {"text": "We evaluate the quality of the ambiguity lexicons and of the resulting test suite on all submissions from 9 language pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using pretrained NMT models.", "labels": [], "entities": [{"text": "WMT19 news shared translation task", "start_pos": 140, "end_pos": 174, "type": "TASK", "confidence": 0.8100994229316711}]}, {"text": "The MUCOW test suite is available at http://github.", "labels": [], "entities": [{"text": "MUCOW test", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7151885628700256}]}, {"text": "com/Helsinki-NLP/MuCoW.", "labels": [], "entities": [{"text": "Helsinki-NLP/MuCoW", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.845718781153361}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has provided impressive advances in translation quality, leading to a discussion whether translations produced by professional human translators can still be distinguished from the output of NMT systems, and to what extent automatic evaluation measures can reliably account for these differences (Hassan).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7843204935391744}]}, {"text": "One answer to this question lies in the development of so-called test suites or challenge sets () that focus on particular linguistic phenomena that are known to be difficult to evaluate with simple reference-based metrics such as BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 231, "end_pos": 235, "type": "METRIC", "confidence": 0.9746367931365967}]}, {"text": "Existing test suites focus e.g. on morphosyntactic and syntactic divergences between source and target language () or on discourse phenomena (.", "labels": [], "entities": []}, {"text": "Another linguistic phenomenon that is challenging for translation is lexical ambiguity (, i.e., words of the source language that have multiple translations in the target language representing different meanings.", "labels": [], "entities": [{"text": "translation", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9791977405548096}]}, {"text": "Recently, Rios introduced a lexical ambiguity benchmark called ContraWSD that is based on contrastive translation pairs: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense.", "labels": [], "entities": []}, {"text": "Contrastive evaluation makes use of the ability of NMT systems to score given translations: a contrast is considered successfully detected if the reference translation obtains a higher score than an artificially modified translation.", "labels": [], "entities": []}, {"text": "However, all these test suites require significant amounts of expert knowledge and manual work for identifying the divergences and compiling the examples, which typically limits their coverage to a small number of language pairs and directions.", "labels": [], "entities": []}, {"text": "For example, the test sets built by Rios cover only 65 ambiguous words for two language pair directions.", "labels": [], "entities": []}, {"text": "In this paper, we present a languageindependent method for automatically building ContraWSD-style test suites.", "labels": [], "entities": []}, {"text": "It involves the following steps: (1) identify ambiguous source words and their translations; (2) cluster the translations into senses; (3) select sentences with ambiguous words and create contrast pairs.", "labels": [], "entities": []}, {"text": "The setup proposed by Rios Gonzales et al.", "labels": [], "entities": []}, {"text": ".: English words aligned with the German word Eingabe and their alignment frequencies.", "labels": [], "entities": []}, {"text": "Words with frequency < 10 are discarded from further processing.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Sizes of the parallel corpora used for lexicon  extraction, the inferred and filtered ambiguity lexicons,  and the resulting test suite corpora.", "labels": [], "entities": [{"text": "lexicon  extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7183414697647095}]}, {"text": " Table 5: Comparison of MUCOW and ContraWSD ac- curacy scores and BLEU scores computed on the WMT  news2017 test set (news2016 for RO-EN).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9985294342041016}, {"text": "WMT  news2017 test set", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.968489795923233}]}, {"text": " Table 6: Sizes of the MUCOW data sets compiled for  WMT19.", "labels": [], "entities": [{"text": "MUCOW data sets compiled", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.9518359899520874}, {"text": "WMT19", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.5513772368431091}]}, {"text": " Table 7: Examples of test suite instances of the English-German WMT test suite. The ambiguous (English) source  word is highlighted in bold, and correct and incorrect (German) translations -as inferred by the MuCoW procedure  -are given. Senses classified as out-of-domain are shown in italics. Note that some example sentences may further  restrict the set of correct translations.", "labels": [], "entities": [{"text": "WMT test suite", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.7378589113553365}]}, {"text": " Table 8: Average coverage of target words among  WMT19 primary submissions.", "labels": [], "entities": [{"text": "Average coverage", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.7540570199489594}, {"text": "WMT19 primary submissions", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6852474013964335}]}, {"text": " Table 9: Results for English-Czech.", "labels": [], "entities": []}, {"text": " Table 10: Results for German-English and English-German.", "labels": [], "entities": []}]}