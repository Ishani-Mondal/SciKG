{"title": [{"text": "Learning Emphasis Selection for Written Text in Visual Media from Crowd-Sourced Label Distributions", "labels": [], "entities": []}], "abstractContent": [{"text": "In visual communication, text emphasis is used to increase the comprehension of written text and to convey the author's intent.", "labels": [], "entities": [{"text": "text emphasis", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.6938420087099075}]}, {"text": "We study the problem of emphasis selection , i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring.", "labels": [], "entities": [{"text": "emphasis selection", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.771997332572937}]}, {"text": "Without knowing the author's intent and only considering the input text, multiple emphasis selections are valid.", "labels": [], "entities": []}, {"text": "We propose a model that employs end-to-end label distribution learning (LDL) on crowd-sourced data and predicts a selection distribution, capturing the inter-subjectivity (common-sense) in the audience as well as the ambiguity of the input.", "labels": [], "entities": []}, {"text": "We compare the model with several baselines in which the problem is transformed to single-label learning by mapping label distributions to absolute labels via majority voting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Visual communication relies heavily on images and short texts.", "labels": [], "entities": [{"text": "Visual communication", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7639254927635193}]}, {"text": "Whether it is flyers, posters, ads, social media posts or motivational messages, it is usually highly designed to grab a viewer's attention and convey a message in the most efficient way.", "labels": [], "entities": []}, {"text": "For text, word emphasis is used to capture the intent better, removing the ambiguity that may exist in some plain texts.", "labels": [], "entities": []}, {"text": "Word emphasis can clarify or even change the meaning of a sentence by drawing attention to some specific information.", "labels": [], "entities": [{"text": "Word emphasis", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6488709449768066}]}, {"text": "It can be done with colors, backgrounds, or fonts, or with styles like italic and boldface.", "labels": [], "entities": []}, {"text": "Some graphic design applications such as Adobe Spark 1 perform automatic text layout using templates that include images and text with different fonts and colors.", "labels": [], "entities": [{"text": "text layout", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.7547400593757629}]}, {"text": "However, their text layout algorithms are mainly driven by visual attributes like word length, rather than the semantics of the text or the user's intent, which can lead to unintended emphasis and the wrong message.", "labels": [], "entities": [{"text": "text layout", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.7215703874826431}]}, {"text": "shows an example that is aesthetically appealing but fails to effectively communicate its intent.", "labels": [], "entities": []}, {"text": "Understanding the text would allow the system to propose a different layout that emphasizes words that contribute more to the communication of the intent, as shown in.", "labels": [], "entities": []}, {"text": "We investigate models that aim to understand the most common interpretation of a short piece of text, so the right emphasis can be achieved automatically or interactively.", "labels": [], "entities": []}, {"text": "The ultimate goal is to enable design assistance for the user during authoring.", "labels": [], "entities": []}, {"text": "The main focus is on short text instances for social media, with a variety of examples from inspirational quotes to advertising slogans.", "labels": [], "entities": []}, {"text": "We model emphasis using plain text with no additional context from the user or the rest of the design.", "labels": [], "entities": []}, {"text": "This task differs from related ones in that word emphasis patterns are person-and domainspecific, making different selections valid depending on the audience and the intent.", "labels": [], "entities": [{"text": "word emphasis", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.7103971540927887}]}, {"text": "For example, in, some users might prefer to just emphasize \"knowledge\" or \"good.\"", "labels": [], "entities": []}, {"text": "To tackle this, we model emphasis by learning label distributions (LDL) with a deep sequence labeling network and the KL-Divergence loss function.", "labels": [], "entities": []}, {"text": "LDL allows us to effectively capture the label ambiguity and inter-subjectivity within the annotators.", "labels": [], "entities": [{"text": "LDL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7961985468864441}]}, {"text": "Unlike single-or multi-label learning, LDL allows direct modeling of different importance of each label to the instance).", "labels": [], "entities": []}, {"text": "The proposed model yields good performance despite the small amount of training data and can be used as a baseline for this task for future evaluations.", "labels": [], "entities": []}, {"text": "Our main contributions are: (1) Introducing anew NLP task: emphasis selection for short text instances as used in social media, learned from anew dataset.", "labels": [], "entities": [{"text": "emphasis selection", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7418380677700043}]}, {"text": "(2) Proposing a novel end-toend sequence labeling architecture utilizing LDL to model the emphasis words in a given text.", "labels": [], "entities": []}, {"text": "(3) Defining evaluation metrics and providing comparisons with several baselines to assess the model performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We obtained 1,206 short text instances from Adobe Spark, which will be publicly available along with their annotations 2 . This collection contains a variety of subjects featured in flyers, posters, advertisement or motivational memes on social media.", "labels": [], "entities": []}, {"text": "The dataset contains 7,550 tokens and the average number of tokens per instance is 6.16, ranging from 2 to 25 tokens.", "labels": [], "entities": []}, {"text": "On average, each instance contains 2.38 emphases and the ratio of non-emphasis to emphasis tokens is 1.61.: A short text example from our collected dataset along with its nine annotations.", "labels": [], "entities": []}, {"text": "We used Amazon Mechanical Turk and asked nine annotators to label each piece of text.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 8, "end_pos": 30, "type": "DATASET", "confidence": 0.9448017279307047}]}, {"text": "To ensure high quality annotation, we included carefully-designed quality questions in 10 percent of the hits.", "labels": [], "entities": []}, {"text": "We obtained a Fleiss' kappa agreement) of 63.59, which compared to similar tasks proves the subjectivity and multi-answer nature of our problem.", "labels": [], "entities": [{"text": "Fleiss' kappa agreement)", "start_pos": 14, "end_pos": 38, "type": "METRIC", "confidence": 0.9313669353723526}]}, {"text": "We noticed higher annotation agreement in shorter length instances (2 to 5 words).", "labels": [], "entities": [{"text": "agreement", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.5166165828704834}]}, {"text": "Having many extremely short pieces of text in the dataset (\u223c60%) increased the annotation agreement.", "labels": [], "entities": [{"text": "annotation agreement", "start_pos": 79, "end_pos": 99, "type": "METRIC", "confidence": 0.7651733756065369}]}, {"text": "We split up the data randomly into training (60%), development (10%) and test (30%) sets for further analysis.", "labels": [], "entities": []}, {"text": "shows an example of text annotated with the IO annotations.", "labels": [], "entities": [{"text": "IO annotations", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.8348582684993744}]}, {"text": "Ultimately, we compute the label distribution for each instance, which corresponds to the count per label normalized by the total number of annotations.", "labels": [], "entities": []}, {"text": "To assess the performance of the model, we propose three different evaluation settings: m of m \u2208 {1 . .", "labels": [], "entities": []}, {"text": "4} words with the top m probabilities according to the ground truth.", "labels": [], "entities": []}, {"text": "Analogously, we select a prediction set\u02c6Sset\u02c6 set\u02c6S (x) m for each m, based on the predicted probabilities.", "labels": [], "entities": []}, {"text": "We define the metric Match m as follows: TopK Similarly to Match m , for each instance x, we select the top k = {1, 2, ..., 4} words with the highest probabilities from both ground truth and prediction distributions.", "labels": [], "entities": []}, {"text": "Then Precision, Recall and F1-score per each k can be computed accordingly.", "labels": [], "entities": [{"text": "Precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9994297623634338}, {"text": "Recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9979671835899353}, {"text": "F1-score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9994454979896545}]}, {"text": "MAX We map the ground truth and prediction distributions to absolute labels by selecting the class with the highest probability.", "labels": [], "entities": [{"text": "MAX", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8651584386825562}]}, {"text": "Then we compute ROC AUC.", "labels": [], "entities": [{"text": "ROC AUC", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.8349345326423645}]}, {"text": "(e.g. a token with label probability of [I = 0.75, O = 0.25] is mapped to \"I\").", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results of Label Distribution Learning and Single Label Learning models in three evaluation  settings, Match m , TopK, and MAX. F represents F1-score.", "labels": [], "entities": [{"text": "Label Distribution Learning", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7883216341336569}, {"text": "Match", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.976232647895813}, {"text": "MAX", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9518154859542847}, {"text": "F1-score", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9968920350074768}]}, {"text": " Table 3: Experimental results in SemEval setting", "labels": [], "entities": [{"text": "SemEval", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9548245072364807}]}]}