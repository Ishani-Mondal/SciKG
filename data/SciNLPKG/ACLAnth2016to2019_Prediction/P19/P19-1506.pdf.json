{"title": [{"text": "Like a Baby: Visually Situated Neural Language Acquisition", "labels": [], "entities": [{"text": "Visually Situated Neural Language Acquisition", "start_pos": 13, "end_pos": 58, "type": "TASK", "confidence": 0.6340245366096496}]}], "abstractContent": [{"text": "We examine the benefits of visual context in training neural language models to perform next-word prediction.", "labels": [], "entities": [{"text": "next-word prediction", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7366934716701508}]}, {"text": "A multi-modal neural architecture is introduced that outperform its equivalent trained on language alone with a 2% decrease in perplexity, even when no visual context is available attest.", "labels": [], "entities": []}, {"text": "Fine-tuning the embeddings of a pre-trained state-of-the-art bidirectional language model (BERT) in the language modeling framework yields a 3.5% improvement.", "labels": [], "entities": [{"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9429084658622742}]}, {"text": "The advantage for training with visual context when testing without is robust across different languages (English, German and Spanish) and different models (GRU, LSTM, \u2206-RNN, as well as those that use BERT embeddings).", "labels": [], "entities": [{"text": "GRU", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.5436519384384155}, {"text": "BERT", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.9483744502067566}]}, {"text": "Thus, language models perform better when they learn like a baby, i.e, in a multi-modal environment.", "labels": [], "entities": []}, {"text": "This finding is compatible with the theory of situated cognition: language is inseparable from its physical context.", "labels": [], "entities": []}], "introductionContent": [{"text": "The theory of situated cognition postulates that a person's knowledge is inseparable from the physical or social context in which it is learned and used.", "labels": [], "entities": []}, {"text": "Similarly, Perceptual Symbol Systems theory holds that all of cognition, thought, language, reasoning, and memory, is grounded in perceptual features ().", "labels": [], "entities": [{"text": "Perceptual Symbol Systems", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7920298377672831}]}, {"text": "Knowledge of language cannot be separated from its physical context, which allows words and sentences to be learned by grounding them in reference to objects or natural concepts on hand (see, fora review).", "labels": [], "entities": []}, {"text": "Nor can knowledge of language be separated from its social context, where language is learned interactively through communicating with others to facilitate problem-solving.", "labels": [], "entities": []}, {"text": "Simply put, language does not occur in a vacuum.", "labels": [], "entities": []}, {"text": "Yet, statistical language models, typically connectionist systems, are often trained in such a vacuum.", "labels": [], "entities": []}, {"text": "Sequences of symbols, such as sentences or phrases composed of words in any language, such as English or German, are often fed into the model independently of any real-world context they might describe.", "labels": [], "entities": []}, {"text": "In the classical language modeling framework, a model learns to predict a word based on a history of words it has seen so far.", "labels": [], "entities": []}, {"text": "While these models learn a great deal of linguistic structure from these symbol sequences alone, acquiring the essence of basic syntax, it is highly unlikely that this approach can create models that acquire much in terms of semantics or pragmatics, which are integral to the human experience of language.", "labels": [], "entities": []}, {"text": "How might one build neural language models that \"understand\" the semantic content held within the symbol sequences, of any language, presented to it?", "labels": [], "entities": []}, {"text": "In this paper, we take a small step towards a model that understands language as a human does by training a neural model jointly on corresponding linguistic and visual data.", "labels": [], "entities": []}, {"text": "From an imagecaptioning dataset, we create a multi-lingual corpus where sentences are mapped to the real-world images they describe.", "labels": [], "entities": []}, {"text": "We ask how adding such real-world context at training can improve language model performance.", "labels": [], "entities": []}, {"text": "We create a unified multi-modal connectionist architecture that incorporates visual context and uses either \u2206-RNN, Long Short Term Memory (LSTM; or Gated Recurrent Unit (GRU;) units.", "labels": [], "entities": []}, {"text": "We find that the models acquire more knowledge of language than if they were trained without corresponding, real-world visual context.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments in this paper were conducted using the MS-COCO image-captioning dataset.", "labels": [], "entities": [{"text": "MS-COCO image-captioning dataset", "start_pos": 55, "end_pos": 87, "type": "DATASET", "confidence": 0.9322309295336405}]}, {"text": "Each image in the dataset has five captions provided by human annotators.", "labels": [], "entities": []}, {"text": "We use the captions to create five different ground truth splits.", "labels": [], "entities": []}, {"text": "We translated each ground truth split into German and Spanish using the Google Translation API, which was chosen as a state-of-the-art, independently evaluated MT tool that produces, according to our inspection of the results, idiomatic, and syntactically and semantically faithful translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.9482194185256958}]}, {"text": "To our knowledge, this represents the first Multi-lingual MSCOCO dataset on situated learning.", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.7555801272392273}]}, {"text": "We tokenize the corpus and obtain a 16.6K vocabulary for English, 33.2K for German and 18.2k for Spanish.", "labels": [], "entities": []}, {"text": "https://competitions.codalab.org/competitions/3221 As our primary concern is the next-step prediction of words/tokens, we use negative log likelihood and perplexity to evaluate the models.", "labels": [], "entities": []}, {"text": "This is different from the goals of machine translation or image captioning, which, inmost cases, is concerned with a ranking of possible captions where one measures how similar the model's generated sequences are to ground-truth target phrases.", "labels": [], "entities": [{"text": "machine translation or image captioning", "start_pos": 36, "end_pos": 75, "type": "TASK", "confidence": 0.6796474933624268}]}, {"text": "Baseline results were obtained with neural language models trained on text alone.", "labels": [], "entities": []}, {"text": "For the \u2206-RNN, this meant implementing a model using only Equations 1-7.", "labels": [], "entities": [{"text": "\u2206-RNN", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.6398558914661407}]}, {"text": "The best results were achieved using the BERT Large model (bidirectional Transformer, 24 layers, 1024dims, 16 attention heads:).", "labels": [], "entities": [{"text": "BERT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9194749593734741}]}, {"text": "We used the large pretrained model and then trained with visual context.", "labels": [], "entities": []}, {"text": "All models were trained to minimize the sequence loss of the sentences in the training split.", "labels": [], "entities": []}, {"text": "The weight matrices of all models were initialized from uniform distribution, U (\u22120.1, 0.1), biases were initialized from zero, and the \u2206-RNNspecific biases {\u03b1, \u03b2 1 , \u03b2 2 } were all initialized to one.", "labels": [], "entities": []}, {"text": "Parameter updates calculated through backpropagation through time required unrolling the model over 49 steps in time (this length was determined based on validation set likelihood).", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9675796627998352}]}, {"text": "All symbol sequences were zero-padded and appropriately masked to ensure efficient mini-batching.", "labels": [], "entities": []}, {"text": "Gradients were hard-clipped at a magnitude bound of l = 2.0.", "labels": [], "entities": []}, {"text": "Over mini-batches of 32 samples, model parameters were optimized using simple stochastic gradient descent (learning rate \u03bb = 1.0 which was halved if the perplexity, measured at the end of each epoch, goes up three or more times).", "labels": [], "entities": [{"text": "learning rate \u03bb", "start_pos": 107, "end_pos": 122, "type": "METRIC", "confidence": 0.9149645765622457}]}, {"text": "To determine if our multi-modal language models capture knowledge that is different from a textonly language model, we evaluate each model twice.", "labels": [], "entities": []}, {"text": "First, we compute the model perplexity on the test set using the sentences' visual context vectors.", "labels": [], "entities": []}, {"text": "Next, we compute model perplexity on test sentences by feeding in a null-vector to the multimodal model as the visual context.", "labels": [], "entities": []}, {"text": "If the model did truly pickup some semantic knowledge that is not exclusively dependent on the context vector, its perplexity in the second setting, while naturally worse than the first setting, should still outperform text-only baselines.", "labels": [], "entities": []}, {"text": "In, we report each model's negative log likelihood (NLL) and per-word perplexity (PPL).", "labels": [], "entities": [{"text": "negative log likelihood (NLL)", "start_pos": 27, "end_pos": 56, "type": "METRIC", "confidence": 0.8430616855621338}, {"text": "per-word perplexity (PPL)", "start_pos": 61, "end_pos": 86, "type": "METRIC", "confidence": 0.8084684669971466}]}, {"text": "PPL is calculated as: We observe that in all cases the multi-modal models outperform their respective text-only baselines.", "labels": [], "entities": []}, {"text": "More importantly, the multi-modal models, when evaluated without the Inception-v3 representations on holdout samples, still perform better than the text-only baselines.", "labels": [], "entities": []}, {"text": "The improvement in language generalization can be attributed to the visual context information provided during training, enriching its representations over word sequences with knowledge of actual objects and actions.", "labels": [], "entities": [{"text": "language generalization", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.740600973367691}]}, {"text": "shows the validation perplexity of the \u2206-RNN on each language as a function of the first 15 epochs of learning.", "labels": [], "entities": []}, {"text": "We observe that throughout learning, the improvement in generalization afforded by the visual context c is persistent.", "labels": [], "entities": []}, {"text": "Validation performance was also tracked for the various GRU and LSTM models, where the same trend was also observed (see supplementary material).", "labels": [], "entities": [{"text": "GRU", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8525859117507935}]}], "tableCaptions": [{"text": " Table 1: Generalization performance as measured by negative log likelihood (NLL) and perplexity (PPL). Lower  values indicate better performance. Baseline model (L-L) trained and evaluated on linguistic data only. Full model  (LV-LV) trained and evaluated on both linguistic and visual data. Blind model (LV-L) trained on both but evaluated  on language only. The difference between L-L and LV-L illustrates the performance improvement. German  and Spanish data are machine-translated (MT) and provide additional, but correlated, evidence. For comparison,  Devlin et al. (2018) report a perplexity of 3.23 for their (broad) English test data, using the same base model we  use here to define input representations.", "labels": [], "entities": [{"text": "negative log likelihood (NLL)", "start_pos": 52, "end_pos": 81, "type": "METRIC", "confidence": 0.8060508469740549}, {"text": "English test data", "start_pos": 625, "end_pos": 642, "type": "DATASET", "confidence": 0.6961827178796133}]}]}