{"title": [{"text": "Improved Sentiment Detection via Label Transfer from Monolingual to Synthetic Code-Switched Text", "labels": [], "entities": [{"text": "Improved Sentiment Detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7333184679349264}, {"text": "Label Transfer", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.6623711585998535}]}], "abstractContent": [{"text": "Multilingual writers and speakers often alternate between two languages in a single discourse , a practice called \"code-switching\".", "labels": [], "entities": []}, {"text": "Existing sentiment detection methods are usually trained on sentiment-labeled monolingual text.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8809290826320648}]}, {"text": "Manually labeled code-switched text, especially involving minority languages, is extremely rare.", "labels": [], "entities": []}, {"text": "Consequently, the best mono-lingual methods perform relatively poorly on code-switched text.", "labels": [], "entities": []}, {"text": "We present an effective technique for synthesizing labeled code-switched text from labeled monolingual text, which is more readily available.", "labels": [], "entities": []}, {"text": "The idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language.", "labels": [], "entities": []}, {"text": "By augmenting scarce human-labeled code-switched text with plentiful synthetic code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5%, 5.11%, 7.20%) for three different language pairs (English-Hindi, English-Spanish and English-Bengali).", "labels": [], "entities": [{"text": "sentiment labeling", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7833118140697479}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9224227070808411}]}, {"text": "We also get significant gains for hate speech detection: 4% improvement using only synthetic text and 6% if augmented with real text.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8088796933492025}]}], "introductionContent": [{"text": "Sentiment analysis on social media is critical for commerce and governance.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9425590634346008}]}, {"text": "Multilingual social media users often use code-switching, particularly to express emotion ( . However, a basic requirement to train any sentiment analysis (SA) system is the availability of large sentimentlabeled corpora.", "labels": [], "entities": [{"text": "sentiment analysis (SA)", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.7791149139404296}]}, {"text": "These are extremely challenging to obtain), requiring volunteers fluent in multiple languages.", "labels": [], "entities": []}, {"text": "We present CSGen, a system which provides supervised SA algorithms with synthesized unlimited sentiment-tagged code-switched text, without involving human labelers of code-switched text, or any linguistic theory or grammar for codeswitching.", "labels": [], "entities": []}, {"text": "These texts can then train state-ofthe-art SA algorithms which, until now, primarily worked with monolingual text.", "labels": [], "entities": []}, {"text": "A common scenario in code-switching is that a resource-rich source language is mixed with a resource-poor target language.", "labels": [], "entities": []}, {"text": "Given a sentimentlabeled source corpus, we first create a parallel corpus by translating to the target language, using a standard translator.", "labels": [], "entities": []}, {"text": "Although existing neural machine translators (NMTs) can translate a complete source sentence to a target sentence with good quality, it is difficult to translate only designated source segments in isolation because of missing context and lack of coherent semantics.", "labels": [], "entities": []}, {"text": "Among our key contributions is a suite of approaches to automatic segment conversion.", "labels": [], "entities": [{"text": "segment conversion", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.80276158452034}]}, {"text": "Broadly, given a source segment selected for codeswitching, we propose intuitive ways to select a corresponding segment from the target sentence, based on maximum similarity or minimum dissimilarity with the source segment, so that the segment blends naturally in the outer source context.", "labels": [], "entities": []}, {"text": "Finally, the generated synthetic sentence is tagged with the same sentiment label as the source sentence.", "labels": [], "entities": []}, {"text": "The source segment to replace is carefully chosen based on an observation that, apart from natural switching points dictated by syntax, there is a propensity to code-switch between highly opinionated segments.", "labels": [], "entities": []}, {"text": "Extensive experiments show that augmenting scarce natural labeled code-switched text with plentiful synthetic text associated with 'borrowed' source labels enriches the feature space, enhances its coverage, and improves sentiment detection accuracy, compared to using only natural text.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.9661678969860077}, {"text": "accuracy", "start_pos": 240, "end_pos": 248, "type": "METRIC", "confidence": 0.8714012503623962}]}, {"text": "On four natural corpora having gold sentiment tags, we demonstrate that adding synthetic text can improve accuracy by 5.11% in English-Spanish, 7.20% in English-Bengali and (1.5%, 0.97%) in English-Hindi.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9989557266235352}]}, {"text": "The synthetic code-switch text, even when used by itself to train SA, performs almost as well as natural text in several cases.", "labels": [], "entities": []}, {"text": "Hate speech is an extreme emotion expressed often on social media.", "labels": [], "entities": [{"text": "Hate speech is an extreme emotion expressed", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8313910024506705}]}, {"text": "On an EnglishHindi gold-tagged hate speech benchmark, we achieve 6% absolute F1 improvement with data augmentation, partly because synthetic text mitigates label imbalance present in scarce real text.", "labels": [], "entities": [{"text": "EnglishHindi gold-tagged hate speech benchmark", "start_pos": 6, "end_pos": 52, "type": "DATASET", "confidence": 0.9372455596923828}, {"text": "F1", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9964663982391357}]}], "datasetContent": [{"text": "We demonstrate the effectiveness of augmenting gold code-switched text with synthetic codeswitched text.", "labels": [], "entities": []}, {"text": "We also measure the usefulness of synthetic text without gold text.", "labels": [], "entities": []}, {"text": "In this section, we will first describe the data sets used to generate the synthetic text and then the resource-poor labeled code-switched text used for evaluation.", "labels": [], "entities": []}, {"text": "Next, we will present the method used for sentiment detection, baseline performance, and finally our performance, along with a detailed comparative analysis.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.9738683104515076}]}, {"text": "To evaluate the usefulness of the generated synthetic tagged sentences as a training set for sentiment analysis, we have used three different codeswitched language pair data sets.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.9768917858600616}]}, {"text": "Each data set below was divided into 70% training, 10% validation and 20% testing folds.", "labels": [], "entities": []}, {"text": "The training fold was (or was not) augmented with synthetic labeled text to train sentiment classifiers, which were then applied on the test fold judge the quality of synthesis.", "labels": [], "entities": []}, {"text": "We also found a significant number of abusive tweets marked hate speech.", "labels": [], "entities": []}, {"text": "For uniformity, we merged hate speech tweets and abusive tweets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average pairwise Euclidean distance between  training data and test data features. Rows correspond to  standalone (respectively, augmented) text for training.  Gray: reference distance of gold test from gold train.  Red: largest distance observed.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy (%) on 20% test data after training with augmented and only gold text. Rows correspond to  sources of augmentation. In most cells we show (A) no thresholding or stratification and (B) with thresholding  and stratification (within brackets). Gray: reference accuracy with only gold training. Blue: A or B or MSR  outperforms gold. Green: B performs best. Row 'MSR' uses text synthesized by", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988866448402405}, {"text": "accuracy", "start_pos": 276, "end_pos": 284, "type": "METRIC", "confidence": 0.9115073084831238}]}, {"text": " Table 3: F1 score for each class prediction. Blue: CS- Gen is better than Gold.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9826790690422058}]}, {"text": " Table 4: Percent accuracy on 20% test data after train- ing on only synthetic and only gold text. Each row  corresponds to a source. Grey: Accuracy achieved  with only gold training. Blue: The closest accuracy  achieved to best.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9253852367401123}, {"text": "Accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9981004595756531}, {"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9963991641998291}]}, {"text": " Table 6: Hate speech results (3-fold cross val.). In  most cells we show performance without thresholding  and stratification (within bracket with thresholding and  stratification). Green: Best performance in each col- umn.", "labels": [], "entities": []}]}