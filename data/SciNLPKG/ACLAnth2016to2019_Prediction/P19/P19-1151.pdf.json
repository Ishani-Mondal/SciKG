{"title": [{"text": "Soft Representation Learning for Sparse Transfer", "labels": [], "entities": [{"text": "Sparse Transfer", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.8965876400470734}]}], "abstractContent": [{"text": "Transfer learning is effective for improving the performance of tasks that are related, and Multi-task learning (MTL) and Cross-lingual learning (CLL) are important instances.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9255711436271667}]}, {"text": "This paper argues that hard-parameter sharing, of hard-coding layers shared across different tasks or languages, cannot generalize well, when sharing with a loosely related task.", "labels": [], "entities": [{"text": "hard-parameter sharing", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.6894966661930084}]}, {"text": "Such case, which we call sparse transfer, might actually hurt performance, a phenomenon known as negative transfer.", "labels": [], "entities": []}, {"text": "Our contribution is using adversarial training across tasks, to \"soft-code\" shared and private spaces, to avoid the shared space gets too sparse.", "labels": [], "entities": []}, {"text": "In CLL, our proposed architecture considers another challenge of dealing with low-quality input.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transfer learning in neural networks has been applied in recent years to improving the performance of related tasks, for example, 1) multi-task learning (MTL) with different tasks (labeled data available all tasks) and 2) cross-lingual learning (CLL) with different language (but the same task) though labeled data available only in source language.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8892858624458313}, {"text": "multi-task learning (MTL)", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.6878775775432586}]}, {"text": "For both settings, one of their most common strategies is hard-parameter sharing, as shown in, which shares the hidden layers across tasks, which we will call shared layer.", "labels": [], "entities": []}, {"text": "This approach works well when tasks are closely related, when most features are invariant across tasks.", "labels": [], "entities": []}, {"text": "Otherwise, which we call sparse transfer, transferring between loosely related tasks often hurt performance, known as negative transfer.", "labels": [], "entities": []}, {"text": "We elaborate this problem in MTL and CLL scenarios.", "labels": [], "entities": [{"text": "MTL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6164843440055847}]}, {"text": "First, for MTL, the shared space is reported to be sparse, in an architecture with one shared encoder, when shared by K (e.g., K > 2 tasks) loosely-related tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9528374075889587}]}, {"text": "To address this problem, as shown in, recent models ( divide the features of different tasks into task-invariant and task-dependent latent spaces, which we will call shared and private spaces from this point on.", "labels": [], "entities": []}, {"text": "However, since such approach still hard-codes shared and private features, deciding which subsets of tasks should share encoders in many-task settings, among all possible combinations of tasks, is a non-trivial design problem.", "labels": [], "entities": []}, {"text": "Second, for CLL, the given task in source language (with rich resources) transfers to that for target languages without training resources.", "labels": [], "entities": []}, {"text": "For the latter, machine-translated resources are fed instead, to the shared encoder (.", "labels": [], "entities": []}, {"text": "When translation is perfect, the shared space would be dense: For example, English training pair with entailment relationship, \"Because it looked so formidable\" and \"It really did look wonderful\" can be translated to Chinese sentences of the same meaning, to preserve labels.", "labels": [], "entities": []}, {"text": "Meanwhile, its translation into \"\u56e0\u4e3a \u5b83\u770b\u8d77\u6765\u90a3\u4e48\u53ef\u6015\" (Because it looks so scary) and \"\u5b83\u771f\u7684\u770b\u8d77\u6765\u5f88\u68d2\" (It really looks great), fails to preserve the entailment relationship, and makes the shared space sparse.", "labels": [], "entities": []}, {"text": "As a unified solution for both problems, we propose soft-coding approaches that can adapt in the following novel ways.", "labels": [], "entities": []}, {"text": "First, for MTL, we propose Task-Adaptive Representation learning using Soft-coding, namely TARS, wherein shared and private features are both mixtures of features.", "labels": [], "entities": [{"text": "MTL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.944678783416748}, {"text": "Task-Adaptive Representation learning", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.7662803033987681}, {"text": "TARS", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.8990785479545593}]}, {"text": "Specifically, as shown in, TARS begins as a generic sharing framework using one common shared encoder, but also adopts its paired task-specific layers to feed a Mixture-of-Experts (MoE) module () which captures soft-private features with a weighted combination of all task-dependent features, where  a gating network G in, decides on output weights for each task.", "labels": [], "entities": []}, {"text": "Based on this basic architecture, TARS softly-shares features balanced by two conflicting auxiliary losses: one is used to eliminate private features from the shared space, which decreases the generalization across task, while the other is used to keep shared space \"dense\" with soft-private features, which is a form of adversarial training.", "labels": [], "entities": []}, {"text": "Such balancing efforts prevent the shared space from being too sparse to be generalized for every task, even when K > 2.", "labels": [], "entities": []}, {"text": "Second, for CLL, we propose a Cross-lingual AdverSarial Example, namely CASE.", "labels": [], "entities": []}, {"text": "Compared to, task-specific private layers no longer exist in, because CLL deals with a single task for multiple languages.", "labels": [], "entities": []}, {"text": "Instead, for an additional challenge of refining low-quality input, we add Refiner.", "labels": [], "entities": [{"text": "Refiner", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9161654710769653}]}, {"text": "Specifically, once the source language is translated into the target language, CASE moves the noisy representation on the target side towards a direction of space on the source side back in a form of adversarial example, and uses this as an additional training data to task classifier.", "labels": [], "entities": [{"text": "CASE", "start_pos": 79, "end_pos": 83, "type": "TASK", "confidence": 0.9630613327026367}]}, {"text": "However, this refinement may have adverse effects (, for which a policy network P in decides whether to refine or not.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness and flexibility of our soft-coding approaches, we evaluate TARS on five different datasets covering diverse scenarios and CASE on cross-lingual natural language inference (XNLI) datasets with 15 languages (including low-resource language such as Swahili and Urdu), and show that TARS and CASE outperform existing hard-coding approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "To show the effectiveness of our proposed approaches, we conduct experiments on both multitask and cross-lingual settings.: Accuracy over MTL with two-source tasks measure the relatedness of a given premise and hypothesis.", "labels": [], "entities": []}, {"text": "The hyperparameter \u03bb is empirically set to 0.005.", "labels": [], "entities": []}, {"text": "All our implementation is available at github.com/haejupark/soft.", "labels": [], "entities": []}, {"text": "Using () as hard-code baselines, we apply Adversarial training (and so-called orthogonality constraints) to FS and SP models, namely AFS and ASP.", "labels": [], "entities": []}, {"text": "Such techniques enhance the distinct nature of shared and private features.", "labels": [], "entities": []}, {"text": "Two-source MTL shows the performance on three text classification tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.4812908172607422}, {"text": "text classification tasks", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.8069549202919006}]}, {"text": "The first row shows the results of \"single task\", and other rows show the results of \"multiple tasks\" by corresponding MTL models trained with two source tasks.", "labels": [], "entities": []}, {"text": "More concretely, (SNLI+MNLI) and (*NLI+QQP) are for cross-domain and cross-task classification respectively.", "labels": [], "entities": [{"text": "cross-task classification", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7344879508018494}]}, {"text": "In this table, we can see that TARS achieves higher accuracy than all sharing scheme baselines in all scenarios, surpassing multi-task learning (i.e., ASP) as well as single task learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9988507032394409}]}, {"text": "These results show that our softcode approach also works well in typical MTL settings with two source tasks, though they are not our targeted sparse scenario.", "labels": [], "entities": []}, {"text": "Three-source MTL In, MTL models use three source tasks (SNLI+MNLI+QQP), where the first row shows the results of \"single task\".", "labels": [], "entities": []}, {"text": "We first test SNLI, MNLI, and QQP as a supervised target task.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.8085063099861145}, {"text": "QQP", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8930843472480774}]}, {"text": "From the results, we can see that TARS outperforms all baselines including MoE, which is a variant of TARS excluding the two auxiliary losses.", "labels": [], "entities": [{"text": "TARS", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9752408266067505}, {"text": "MoE", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.5709172487258911}, {"text": "TARS", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.897838294506073}]}, {"text": "We also include the recent work,  MMoE (, which explicitly learns to model task relationship by modeling an expert for each task (which is not desirable fora new task).", "labels": [], "entities": []}, {"text": "This suggests that the synergetic effect of soft-private and -shared modules in TARS is critical to outperform other baselines.", "labels": [], "entities": []}, {"text": "Specifically, AFS and ASP show a \"negative transfer\", which is an inherent challenge of MTL.", "labels": [], "entities": [{"text": "MTL", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9352045655250549}]}, {"text": "For example, ASP with three-source tasks achieves 82.23% and 66.92% accuracy, respectively, in SNLI and MNLI, which are lower than 82.28% and 67.39% accuracy with its best performance with two-source tasks.", "labels": [], "entities": [{"text": "ASP", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9801446199417114}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9993300437927246}, {"text": "SNLI", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.8137629628181458}, {"text": "MNLI", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9288284182548523}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9982927441596985}]}, {"text": "In contrast, TARS overcomes such challenges, for example, 83.12% > 82.67% and 68.24% > 67.79% in SNLI and MNLI, except for QQP, which can be further improved by asymmetric MTL techniques (.", "labels": [], "entities": [{"text": "TARS", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.7613599300384521}, {"text": "MNLI", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.8600191473960876}]}, {"text": "To investigate how TARS helps transfer knowledge across tasks, and 2b contrast the feature representation of shared space in ASP and TARS, in two-and three-source settings respectively.", "labels": [], "entities": []}, {"text": "First, for two-sources, ASP and TARS are comparable, capturing the distribution of two tasks that are nearly identical, which is desirable for transfer learning.", "labels": [], "entities": [{"text": "TARS", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9635191559791565}, {"text": "transfer learning", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.9472337067127228}]}, {"text": "Second, for three sources, the shared space of ASP shows two quite distinct distributions (task-dependent), while TARS keeps two distributions comparable (and task-invariant).", "labels": [], "entities": [{"text": "TARS", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.6026588082313538}]}, {"text": "Zero-shot Learning Lastly, in, we test zero-shot learning with two target tasks, CNLI and LCQMC, excluding their own training data (except for the first row single task).", "labels": [], "entities": [{"text": "CNLI", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9732972979545593}, {"text": "LCQMC", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.7874515056610107}]}, {"text": "As ASP requires target task labels to train its private encoders, we compare TARS only with AFS and MoE, where TARS shows the best performance in MTL.", "labels": [], "entities": [{"text": "TARS", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9626531004905701}, {"text": "TARS", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9562795162200928}]}, {"text": "As shown in, we observe that when TARS covers sentences in CNLI and LCQMC, using its gating network that identifies that the unknown target tasks are the most similar to SNLI and QQP, respectively: Specifically, highest weights are assigned to these two, but other source tasks also contribute, with non-zero weights.", "labels": [], "entities": [{"text": "CNLI", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9628171324729919}, {"text": "LCQMC", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.7330902218818665}]}, {"text": "shows our results on 14 XNLI languages.", "labels": [], "entities": [{"text": "XNLI languages", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.8486631214618683}]}, {"text": "Following (, we divide the models into following three categories: 1) Translate train, where the English NLI training set is translated into each XNLI language and train a language-specific NLI classifier for each language; 2) Translate test, where all dev and test set of XNLI is translated to English and apply English NLI classifier; and 3) Zero-shot Learning, where English classifier is directly applied to the target language without any translation.", "labels": [], "entities": []}, {"text": "We also report the results of XNLI baselines (Conneau et al., 2018), a supervised cross-lingual MTL model that combines the L adv loss using pseudoparallel data (, the multilingual BERT (, and the recent work of.", "labels": [], "entities": [{"text": "BERT", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9765432476997375}]}, {"text": "First, in  to perform consistently better than Translate train for all languages, which means a single English model works better than training each target model with translated data.", "labels": [], "entities": []}, {"text": "In contrast, Multilingual BERT () achieves best results on Translate train, outperforming most languages, suggesting the generalization of BERT across languages significantly better than BiLSTM model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.8562337160110474}, {"text": "BERT", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9771809577941895}]}, {"text": "Meanwhile, CASE, significantly outperforms the BiLSTM and BiLSTM+MTL models in Translate train for all languages, and even outperforms BiLSTM in Translate test.", "labels": [], "entities": []}, {"text": "Compared to the best performing MTL baseline, CASE achieves an improvement of 1.7% and 9.5% in Bulgarian (bg) and Urdu (ur) languages respectively.", "labels": [], "entities": [{"text": "CASE", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.572892963886261}]}, {"text": "From these results, we observe that: 1) the improvements on low-resource language (e.g., Swahili and Urdu) are more substantial than those on other languages; 2) the selective refinement strategy consistently contributes to the performance improvement.", "labels": [], "entities": []}, {"text": "These results show that CASE, by incorporating pseudo-adversarial example as an additional resource, contributes to the robustness and the generalization of the model.", "labels": [], "entities": [{"text": "CASE", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.9575567841529846}]}, {"text": "Lastly, we show that CASE with multilingual BERT model achieves the state-of-the-art, and even significantly outperforms the supervised approach of enjoying an unfair advantage of extremely large amounts of parallel sentences.", "labels": [], "entities": [{"text": "CASE", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.9739358425140381}, {"text": "BERT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9718924760818481}]}, {"text": "These results show that CASE, with the help of strong baselines, gets a significant boost in performance, particularly for Swahili and Urdu that are low-resource languages, achieving the improvement of 9.4% and 10.3% respectively.", "labels": [], "entities": [{"text": "CASE", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.8919965028762817}]}, {"text": "Robustness Analysis In order to verify whether CASE is robust, inspired by (, we test if models keep its prediction, even after changes to the sentence, as long as the meaning remains unchanged.", "labels": [], "entities": [{"text": "Robustness Analysis", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8470458388328552}, {"text": "CASE", "start_pos": 47, "end_pos": 51, "type": "TASK", "confidence": 0.9003927111625671}]}, {"text": "For example, the given sentence can be paraphrased by changing some words with their synonyms, and the models should give the same answer to the paraphrase.", "labels": [], "entities": []}, {"text": "Meanwhile, existing models, especially those overfitted to surface forms, are sensitive to such \"semantic-preserving\" perturbations.", "labels": [], "entities": []}, {"text": "As human annotation for such perturbations is expensive, an automated approach ( was studied for English, to generate semanticpreserving adversaries that fool well-trained sentiment analysis and NLI models with success rates of 97% and 70%, respectively.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.8614423871040344}]}, {"text": "In our problem setting of XNLI, we need such a generator (or generated resources) for each language.", "labels": [], "entities": []}, {"text": "For which, we identify three research questions: \u2022 (RQ1) How hard is it to build a generator fora new language?", "labels": [], "entities": []}, {"text": "\u2022 (RQ2) Are the observations consistent?", "labels": [], "entities": []}, {"text": "\u2022 (RQ3) Does our model improve robustness?", "labels": [], "entities": []}, {"text": "Specifically, in this paper we focus on Chinese, as we could hire native speaking volunteers to validate whether automatically generated perturbations indeed preserve semantics.", "labels": [], "entities": []}, {"text": "First, for RQ1, we leverage Chinese synonyms and antonyms to build counter fitting vectors as) to ensure the selected words are synonyms.", "labels": [], "entities": [{"text": "RQ1", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.6799865365028381}]}, {"text": "Then, we slightly change, for NLI problem, we only add perturbation to the hypothesis, excluding premise, and aim to divert the prediction result from entailment to contradiction, and vice versa. is an example of generated adversarial example.", "labels": [], "entities": []}, {"text": "For RQ2, we validate the automatically generated perturbations by native speaking volunteers.", "labels": [], "entities": [{"text": "RQ2", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6453254222869873}]}, {"text": "We show volunteers 500 samples to label whether it is contradiction, neutral or entailment.", "labels": [], "entities": []}, {"text": "84 percent of the responses matched the original ground truth.", "labels": [], "entities": []}, {"text": "Second, we sample 500 samples, with each sample including the original sentence and the corresponding adversarial example.", "labels": [], "entities": []}, {"text": "Volunteers were asked to judge the similarity of each pair on a scale from 1 (very different) to 4 (very similar).", "labels": [], "entities": [{"text": "similarity", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9931527376174927}]}, {"text": "The average rating is 2.12, which shows the performance of our implementation for Chinese perturbation is also competitive.", "labels": [], "entities": []}, {"text": "Lastly, for RQ3, we show the attack success rates over generated adversarial example in Table 5.", "labels": [], "entities": [{"text": "RQ3", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.514504611492157}]}, {"text": "For comparison, we include the single task and MTL baselines.", "labels": [], "entities": []}, {"text": "As shown in the, CASEs are able to achieve higher defense rate (or lower success rate) in performance of 36.6%, while baselines obtained 15.7% and 21.4% respectively, which demonstrates incorporating pseudoadversarial example is indeed helpful to the robustness of the model.", "labels": [], "entities": [{"text": "CASEs", "start_pos": 17, "end_pos": 22, "type": "TASK", "confidence": 0.9024665355682373}, {"text": "defense rate", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.987892359495163}]}], "tableCaptions": [{"text": " Table 2: Accuracy of MTL with three-source tasks", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9931681156158447}, {"text": "MTL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9555369019508362}]}, {"text": " Table 3: Accuracy over 14 XNLI languages (test set accuracy). We report results for translation baselines, multi- task learning baselines and zero-shot baselines. Overall best results are in bold, and the best in each group is un- derlined. All results * from its Github project https://github.com/google-research/bert/blob/  master/multilingual.md.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.982297420501709}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9517931342124939}]}, {"text": " Table 5: Attack success rates over Chinese adversarial  example for the text classification task.", "labels": [], "entities": [{"text": "Attack success", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9095306098461151}, {"text": "text classification task", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.8913419842720032}]}]}