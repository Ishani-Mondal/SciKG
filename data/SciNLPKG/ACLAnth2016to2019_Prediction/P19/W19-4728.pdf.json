{"title": [{"text": "Conceptual Change and Distributional Semantic Models: an Exploratory Study on Pitfalls and Possibilities", "labels": [], "entities": []}], "abstractContent": [{"text": "Studying conceptual change using embedding models has become increasingly popular in the Digital Humanities community, while critical observations about them have received less attention.", "labels": [], "entities": []}, {"text": "This paper investigates what the impact of known pitfalls can be on the conclusions drawn in a digital humanities study through the use case of \"Racism\" in the 20th century.", "labels": [], "entities": []}, {"text": "In addition, we suggest an approach for modeling a complex concept in terms of words and relations representative of the conceptual system.", "labels": [], "entities": []}, {"text": "Our results show that different models created from the same data yield different results, but also indicate that (i) using different model architectures, (ii) comparing different corpora and (iii) comparing to control words and relations can help to identify which results are solid and which maybe due to artefacts.", "labels": [], "entities": []}, {"text": "We propose guidelines to conduct similar studies, but also note that more work is needed to fully understand how we can distinguish artefacts from actual conceptual changes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional models have been used to detect shifts in meaning with various degrees of success (, e.g.).", "labels": [], "entities": []}, {"text": "Based on promising examples such as the shift of the word gay from meaning 'carefree' to 'homosexual', researchers in digital humanities have been inspired to explore the use of distributional semantic models for studying the more complex phenomenon of concept drift, e.g.).", "labels": [], "entities": []}, {"text": "In most cases, standard methods with high results on identifying known examples of semantic shift are adopted and applied to specific data and use-cases.", "labels": [], "entities": []}, {"text": "Literature that raises critical questions concerning the reliability of these methods (e.g.), however, seems to have received less attention in the digital humanities community.", "labels": [], "entities": []}, {"text": "It is, in fact, far from trivial to apply distributional semantic models to study a complex phenomenon such as concept drift in a methodologically sound manner.", "labels": [], "entities": []}, {"text": "We distinguish three main challenges: First, distributional semantic models reflect the way words are used and not directly how concepts are perceived.", "labels": [], "entities": []}, {"text": "This leads to the question of which words should be studied and how shifts in their meaning relate to the underlying concept.", "labels": [], "entities": []}, {"text": "Second, the relation between data, frequency and information emphasized by different model types is not fully understood (.", "labels": [], "entities": []}, {"text": "Third, the semantic models resulting from neural network-inspired architectures as provided by (e.g.) () depend on random factors such as initialization and the order in which data is presented.", "labels": [], "entities": []}, {"text": "If these challenges are not taken into account, researchers may end up publishing insights and results that are the result of artefacts in the data or models rather than valid observations on change.", "labels": [], "entities": []}, {"text": "Existing research has shown that these variations exist, but we are not aware of previous work that explored their consequences in atypical digital humanities set-up, which does not just consider the most extreme changes or words in commonly used evaluation sets, but considers words of a specific topic under consideration.", "labels": [], "entities": []}, {"text": "In order to enable digital humanities research that makes use of distributional semantic models, it is essential to establish how these models can be used in a methodologically sound manner and to communicate this to potential users.", "labels": [], "entities": []}, {"text": "In this paper, we illustrate this importance and propose methods that take these risks into account when investigating conceptual change using word embeddings.", "labels": [], "entities": []}, {"text": "We illustrate this through a use case of a concept known to have changed radically during the 20th century, namely \"Racism\".", "labels": [], "entities": []}, {"text": "We define a set of words that represent this complex conceptual system and test various hypotheses concerning how relations between these words changed.", "labels": [], "entities": []}, {"text": "We investigate the impact of artefacts by (1) using two datasets, (2) testing the impact on control words and (3) creating different models.", "labels": [], "entities": []}, {"text": "In particular, we compare predict models both to count models and to other predict models created with different random initializations.", "labels": [], "entities": []}, {"text": "The results show that not all conclusions drawn in a naive methodological set-up can withstand a more critical investigation.", "labels": [], "entities": []}, {"text": "The main contributions of this work are the following: \u2022 We propsose ways of critically investigating apparent changes with respect to artefacts of the data and/or model.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we outline the outcome of a 'naive' approach to testing our hypotheses, using the methods with best results in.", "labels": [], "entities": []}, {"text": "We use two corpora: COHA with the advantage of being well-balanced and disadvantage of being relatively small (on average 24,5 million words per decade) and the larger but unbalanced English Google Ngram corpus (hencforth ngram).", "labels": [], "entities": [{"text": "English Google Ngram corpus (hencforth ngram", "start_pos": 183, "end_pos": 227, "type": "DATASET", "confidence": 0.7341398809637342}]}, {"text": "com/cltl/semantic_space_navigation/tree/ master/projects/neighbor_stability change direction ngrams both coha: Hypotheses about changes in relations between words confirmed in the n-grams corpus, the COHA or both.", "labels": [], "entities": [{"text": "COHA", "start_pos": 200, "end_pos": 204, "type": "DATASET", "confidence": 0.9177424907684326}]}, {"text": "The changes significantly correlate with time (either over the entire century or over the second half only).", "labels": [], "entities": []}, {"text": "Embeddings are created by with the skip-gram with negative sampling model (SGNS) of the Word2vec toolkit.", "labels": [], "entities": [{"text": "Word2vec toolkit", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.9472261667251587}]}, {"text": "We first explore whether cosine distances between vectors changed according to our hypotheses.", "labels": [], "entities": []}, {"text": "Because we are ultimately interested in the reliability of positive results, we limit our presentation to the statistically significant confirmations presented in.", "labels": [], "entities": []}, {"text": "We observe three hypotheses confirmed in both corpora, four only in the COHA corpus and three just in the Google Ngram corpus.", "labels": [], "entities": [{"text": "COHA corpus", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9705630838871002}, {"text": "Google Ngram corpus", "start_pos": 106, "end_pos": 125, "type": "DATASET", "confidence": 0.9317696293195089}]}, {"text": "We furthermore explore changes in nearest neighbors of cultural and racial illustrated in.", "labels": [], "entities": []}, {"text": "The shifts observed in nearest neighbors indicate that biologically connotated term racial is increasingly avoided in contexts in which racially constructed groups are described or compared.", "labels": [], "entities": []}, {"text": "The results indicate that it is used to name social problems partly rooted in racist ideologies.", "labels": [], "entities": []}, {"text": "This naive approach seems to confirm that the shift in \"Racism\" established by scholars is indeed reflected in language use to a certain extent.", "labels": [], "entities": []}, {"text": "We observed stastically significant shifts between ten word pairs in the direction that was expected.", "labels": [], "entities": []}, {"text": "(a) Nearest neighbors of racial in 1900 (small darker), 1950 (small lighter) and both decades (big).", "labels": [], "entities": []}, {"text": "(b) Nearest neighbors of racial in the 1950s (small darker), 1990 (small lighter) and both decades (big).", "labels": [], "entities": []}, {"text": "We furthermore found changes in the environment of the nearest neigbors of racial and cultural that confirm the change of discourse from a biological racial vision of difference between people to a more cultural one.", "labels": [], "entities": []}, {"text": "In the next section, we test whether the conclusions hold when being tested through alternative means.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Nearest neighbors of cultural shared across all  three models in the n-gram corpus.", "labels": [], "entities": []}, {"text": " Table 8: Average differences in rank between the top  25 nearest neighbors of racial in the models created  with three different initializations on the same decades  of COHA.", "labels": [], "entities": [{"text": "COHA", "start_pos": 170, "end_pos": 174, "type": "DATASET", "confidence": 0.8334600329399109}]}]}