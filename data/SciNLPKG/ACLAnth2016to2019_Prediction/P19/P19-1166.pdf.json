{"title": [], "abstractContent": [{"text": "Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes.", "labels": [], "entities": []}, {"text": "However, methods for measuring and removing such biases remain poorly understood.", "labels": [], "entities": []}, {"text": "We show that for any embedding model that implicitly does matrix fac-torization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus.", "labels": [], "entities": []}, {"text": "We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias.", "labels": [], "entities": [{"text": "WEAT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.8374047875404358}]}, {"text": "Given that the subspace projection method is provably effective, we use it to derive anew measure of association called the relational inner product association (RIPA).", "labels": [], "entities": [{"text": "relational inner product association (RIPA)", "start_pos": 124, "end_pos": 167, "type": "METRIC", "confidence": 0.6695042422839573}]}, {"text": "Experiments with RIPA reveal that, on average , skipgram with negative sampling (SGNS) does not make most words anymore gendered than they are in the training corpus.", "labels": [], "entities": []}, {"text": "However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.9307318329811096}]}], "introductionContent": [{"text": "A common criticism of word embeddings is that they capture undesirable associations in vector space.", "labels": [], "entities": []}, {"text": "In addition to gender-appropriate analogies such as king:queen::man:woman, stereotypical analogies such as doctor:nurse::man:woman also hold in SGNS embedding spaces (.", "labels": [], "entities": []}, {"text": "created an association test for word vectors called WEAT, which uses cosine similarity to measure how associated words are with respect to two sets of attribute words (e.g., 'male' vs. 'female').", "labels": [], "entities": [{"text": "WEAT", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.5113537907600403}]}, {"text": "For example, they claimed that science-related words were significantly more associated with male attributes and art-related words with female ones.", "labels": [], "entities": []}, {"text": "Since these associations are socially undesirable, they were described as gender bias.", "labels": [], "entities": []}, {"text": "Despite these remarkable findings, such undesirable word associations remain poorly understood.", "labels": [], "entities": []}, {"text": "For one, what causes them -is it biased training data, the embedding model itself, or just noise?", "labels": [], "entities": []}, {"text": "Why should WEAT be the test of choice for measuring associations in word embeddings?", "labels": [], "entities": []}, {"text": "found that word vectors could be debiased by defining a \"bias subspace\" in the embedding space and then subtracting from each vector its projection on this subspace.", "labels": [], "entities": []}, {"text": "But what theoretical guarantee is there that this method actually debiases vectors?", "labels": [], "entities": []}, {"text": "In this paper, we answer several of these open questions.", "labels": [], "entities": []}, {"text": "We begin by proving that for any embedding model that implicitly does matrix factorization (e.g., GloVe, SGNS), debiasing vectors post hoc via subspace projection is, under certain conditions, equivalent to training on an unbiased corpus without reconstruction error.", "labels": [], "entities": []}, {"text": "We find that contrary to what suggested, word embeddings should not be normalized before debiasing, as vector length can contain important information (.", "labels": [], "entities": []}, {"text": "To guarantee unbiasedness, the bias subspace should also be the span -rather than a principal component -of the vectors used to define it.", "labels": [], "entities": []}, {"text": "If applied this way, the subspace projection method can be used to provably debias SGNS and GloVe embeddings with respect to the word pairs that define the bias subspace.", "labels": [], "entities": []}, {"text": "Using this notion of a \"bias subspace\", we then prove that WEAT, the most common association test for word embeddings, has theoretical flaws that cause it to systematically overestimate bias.", "labels": [], "entities": [{"text": "WEAT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.5872464179992676}]}, {"text": "At least for SGNS and GloVe, it implicitly requires the two sets of attribute words (e.g., 'male' vs. 'female') to occur with equal frequency in the training corpus; when they do not, even gender-neutral words can be classified as gender-biased, for example.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8828623294830322}]}, {"text": "The outcome of a WEAT test can also be easily manipulated by contriving the attribute word sets, allowing virtually any word -even a gender-neutral one such as 'door' -to be classified as male-or female-biased relative to another gender-neutral word.", "labels": [], "entities": [{"text": "WEAT", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.7778266072273254}]}, {"text": "Given that subspace projection removal provably debiases embeddings, we use it to derive anew measure of association in word embeddings called the relational inner product association (RIPA).", "labels": [], "entities": [{"text": "subspace projection removal", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.7201940814654032}, {"text": "relational inner product association (RIPA)", "start_pos": 147, "end_pos": 190, "type": "METRIC", "confidence": 0.6161672132355827}]}, {"text": "Given a set of ordered word pairs (e.g., {('man', 'woman'), ('male', 'female')}), we take the first principal component of all the difference vectors, which we call the relation vector b.", "labels": [], "entities": []}, {"text": "In Bolukbasi et al.'s terminology, b would be a one-dimensional bias subspace.", "labels": [], "entities": []}, {"text": "Then, fora word vector w, the relational inner product is simply w, b.", "labels": [], "entities": []}, {"text": "Because RIPA is intended for embedding models that implicitly do matrix factorization, it has an information theoretic interpretation.", "labels": [], "entities": []}, {"text": "This allows us to directly compare the actual word association in embedding space with what we would expect the word association to be, given the training corpus.", "labels": [], "entities": []}, {"text": "Making such comparisons yields several novel insights: 1.", "labels": [], "entities": []}, {"text": "SGNS does not, on average, make the vast majority of words anymore gendered in the vector space than they are in the training corpus; individual words maybe slightly more or less gendered due to reconstruction error.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7561291456222534}]}, {"text": "However, for words that are genderstereotyped (e.g., 'nurse') or gender-specific by definition (e.g., 'queen'), SGNS amplifies the gender association in the training corpus.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 112, "end_pos": 116, "type": "TASK", "confidence": 0.9414562582969666}]}, {"text": "2. To use the subspace projection method, one must have prior knowledge of which words are gender-specific by definition, so that they are not also debiased.", "labels": [], "entities": [{"text": "subspace projection", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.8810983002185822}]}, {"text": "Debiasing all vectors can preclude gender-appropriate analogies such as king:queen::man:woman from holding in the embedding space.", "labels": [], "entities": []}, {"text": "In contrast to the supervised method proposed by for identifying these gender-specific words, we introduce an unsupervised method.", "labels": [], "entities": []}, {"text": "Ours is much more effective at preserving gender-appropriate analogies and precluding gender-biased ones.", "labels": [], "entities": []}, {"text": "To allow a fair comparison with prior work, our experiments in this paper focus on gender association.", "labels": [], "entities": []}, {"text": "However, our claims extend to other types of word associations as well, which we leave as future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "With our experiments, we address two open questions.", "labels": [], "entities": []}, {"text": "For one, how much of the gender association in an embedding space is due to the embedding model itself, how much is due to the training corpus, and how much is just noise?", "labels": [], "entities": []}, {"text": "Secondly, how can we debias gender-biased words (e.g., 'doctor', 'nurse') but not gender-appropriate ones (e.g., 'king', 'queen') without a priori knowledge of which words belong in which category?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: By contriving the male and female attribute words, we can easily manipulate WEAT to claim that a given  target word is more female-biased or male-biased than another. For example, in the top row,  door is more male- associated than  curtain when the attribute words are 'masculine' and 'feminine', but it is more female-associated  when the attribute words are 'woman' and 'man'. In both cases, the associations are highly statistically significant.", "labels": [], "entities": [{"text": "WEAT", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.39129409193992615}]}, {"text": " Table 2: On average, SGNS makes gender-appropriate words (e.g., 'queen') and gender-biased words (e.g., 'nurse')  more gendered in the embedding space than they are in the training corpus. As seen in the last column (in bold), the  average change in absolute genderedness is 0.291 and 0.197 respectively (p < 0.001 for both). For gender-neutral  words, the average change is only \u22120.006 (p = 0.84): SGNS does not make them any more gendered.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.934425950050354}]}]}