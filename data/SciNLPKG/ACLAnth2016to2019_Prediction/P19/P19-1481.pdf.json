{"title": [{"text": "Cross-Lingual Training for Automatic Question Generation", "labels": [], "entities": [{"text": "Automatic Question Generation", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.7574156721433004}]}], "abstractContent": [{"text": "Automatic question generation (QG) is a challenging problem in natural language understanding.", "labels": [], "entities": [{"text": "Automatic question generation (QG)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7594200819730759}, {"text": "natural language understanding", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.663254847129186}]}, {"text": "QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer.", "labels": [], "entities": []}, {"text": "For anew language, such training instances are hard to obtain making the QG problem even more challenging.", "labels": [], "entities": []}, {"text": "Using this as our motivation, we study the reuse of an available large QG dataset in a secondary language (e.g. English) to learn a QG model fora primary language (e.g. Hindi) of interest.", "labels": [], "entities": [{"text": "QG dataset", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.7197352051734924}]}, {"text": "For the primary language, we assume access to a large amount of monolingual text but only a small QG dataset.", "labels": [], "entities": [{"text": "QG dataset", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.8563730418682098}]}, {"text": "We propose a cross-lingual QG model which uses the following training regime: (i) Unsupervised pre-training of language models in both primary and secondary languages and (ii) joint supervised training for QG in both languages.", "labels": [], "entities": []}, {"text": "We demonstrate the efficacy of our proposed approach using two different primary languages, Hindi and Chinese.", "labels": [], "entities": []}, {"text": "We also create and release anew question answering dataset for Hindi consisting of 6555 sentences.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7165793627500534}]}], "introductionContent": [{"text": "Automatic question generation from text is an important yet challenging problem especially when there is limited training data (i.e., pairs of sentences and corresponding questions).", "labels": [], "entities": [{"text": "Automatic question generation from text", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7231055080890656}]}, {"text": "Standard sequence to sequence models for automatic question generation have been shown to perform reasonably well for languages like English, for which hundreds of thousands of training instances are available.", "labels": [], "entities": [{"text": "automatic question generation", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6560406982898712}]}, {"text": "However, training sets of this size are not available for most languages.", "labels": [], "entities": []}, {"text": "Manually curating a dataset of comparable size fora new language will be tedious and expensive.", "labels": [], "entities": []}, {"text": "Thus, it would be desirable to leverage existing question answering datasets to help build QG models fora  new language.", "labels": [], "entities": []}, {"text": "This is the overarching idea that motivates this work.", "labels": [], "entities": []}, {"text": "In this paper, we present a cross-lingual model for leveraging a large question answering dataset in a secondary language (such as English) to train models for QG in a primary language (such as Hindi) with a significantly smaller question answering dataset.", "labels": [], "entities": [{"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7449596524238586}, {"text": "question answering", "start_pos": 230, "end_pos": 248, "type": "TASK", "confidence": 0.7437124252319336}]}, {"text": "We chose Hindi to be one of our primary languages.", "labels": [], "entities": []}, {"text": "There is no established dataset available for Hindi that can be used to build question answering or question generation systems, making it an appropriate choice as a primary language.", "labels": [], "entities": [{"text": "question answering or question generation", "start_pos": 78, "end_pos": 119, "type": "TASK", "confidence": 0.7165922224521637}]}, {"text": "We create anew question answering dataset for Hindi (named HiQuAD): https://www.cse.iitb.ac.in/ \u02dc ganesh/HiQuAD/clqg/.", "labels": [], "entities": [{"text": "question answering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7071643024682999}]}, {"text": "shows two examples of sentence-question pairs from HiQuAD along with the questions predicted by our best model.", "labels": [], "entities": [{"text": "HiQuAD", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9794139266014099}]}, {"text": "We also experimented with Chinese as a primary language.", "labels": [], "entities": []}, {"text": "This choice was informed by our desire to use a language that was very different from Hindi.", "labels": [], "entities": []}, {"text": "We use the same secondary language -English -with both choices of our primary language.", "labels": [], "entities": []}, {"text": "Drawing inspiration from recent work on unsupervised neural machine translation (, we propose a crosslingual model to leverage resources available in a secondary language while learning to automatically generate questions from a primary language.", "labels": [], "entities": [{"text": "unsupervised neural machine translation", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.7077298909425735}]}, {"text": "We first train models for alignment between the primary and secondary languages in an unsupervised manner using monolingual text in both languages.", "labels": [], "entities": []}, {"text": "We then use the relatively larger QG dataset in a secondary language to improve QG on the primary language.", "labels": [], "entities": [{"text": "QG dataset", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.7782001495361328}, {"text": "QG", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.6359637975692749}]}, {"text": "Our main contributions can be summarized as follows: \u2022 We present a cross-lingual model that effectively exploits resources in a secondary language to improve QG fora primary language.", "labels": [], "entities": []}, {"text": "\u2022 We demonstrate the value of cross-lingual training for QG using two primary languages, Hindi and Chinese.", "labels": [], "entities": [{"text": "QG", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.8712962865829468}]}, {"text": "\u2022 We create anew question answering dataset for Hindi, HiQuAD.", "labels": [], "entities": [{"text": "question answering", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7226428687572479}, {"text": "HiQuAD", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8929812908172607}]}], "datasetContent": [{"text": "We first describe all the datasets we used in our experiments, starting with a detailed description of our new Hindi question answering dataset, \"HiQuAD\".", "labels": [], "entities": []}, {"text": "We will then describe various implementation-specific details relevant to training our models.", "labels": [], "entities": []}, {"text": "We conclude this section with a description of our evaluation methods..", "labels": [], "entities": []}, {"text": "All model hyperparameters are optimized using the development set and all results are reported on the test set.", "labels": [], "entities": []}, {"text": "We briefly describe all the remaining datasets used in our experiments.", "labels": [], "entities": []}, {"text": "(The relevant primary or secondary language is mentioned in parenthesis, alongside the name of the datasets.)", "labels": [], "entities": []}, {"text": "IITB Hindi Monolingual Corpus (Primary language: Hindi) We extracted 93,000 sentences from the IITB Hindi monolingual corpus , where each sentence has between 4 and 25 tokens.", "labels": [], "entities": [{"text": "IITB Hindi Monolingual Corpus", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.9440789520740509}, {"text": "IITB Hindi monolingual corpus", "start_pos": 95, "end_pos": 124, "type": "DATASET", "confidence": 0.7557486742734909}]}, {"text": "These sentences were used for unsupervised pretraining.", "labels": [], "entities": []}, {"text": "(Primary language: Chinese) This dataset consists of question-answer pairs along with the question type.", "labels": [], "entities": []}, {"text": "We preprocessed and used \"DESCRIP-TION\" type questions for our experiments, resulting in a total of 8000 instances.", "labels": [], "entities": []}, {"text": "From this subset, we created a 6000/1000/1000 split to construct train, development and test sets for our experiments.", "labels": [], "entities": []}, {"text": "We also preprocessed and randomly extracted 100,000 descriptions to be used as a Chinese monolingual corpus for the unsupervised pretraining stage.", "labels": [], "entities": []}, {"text": "News Commentary Dataset: (Primary language: Chinese) This is a parallel corpus of news commentaries provided by WMT.", "labels": [], "entities": [{"text": "News Commentary Dataset", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.8505707780520121}, {"text": "WMT", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.9671661257743835}]}, {"text": "5 It contains roughly 91000 English sentences along with their Chinese translations.", "labels": [], "entities": []}, {"text": "We preprocessed this dataset and used this parallel data for fine-tuning the weights of the encoder and decoder layers after unsupervised pretraining.", "labels": [], "entities": []}, {"text": "SQuAD Dataset: (Secondary language: English) This is a very popular English question answering dataset.", "labels": [], "entities": [{"text": "SQuAD Dataset", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9166797995567322}]}, {"text": "We used the train split of the pre-processed QG data released by for supervised QG training.", "labels": [], "entities": [{"text": "QG data released", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.841629167397817}]}, {"text": "This dataset consists of 70,484 sentencequestion pairs in English.", "labels": [], "entities": []}, {"text": "We evaluate our systems and report results on widely used BLEU (), ROUGE-L and METEOR metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9990611672401428}, {"text": "ROUGE-L", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9927957653999329}, {"text": "METEOR", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9735324382781982}]}, {"text": "We also performed a human evaluation study to evaluate the quality of the questions generated.", "labels": [], "entities": []}, {"text": "We conduct a human evaluation study comparing the questions generated by the Transformer and CLQG+parallel models.", "labels": [], "entities": []}, {"text": "We randomly selected a subset of 100 sentences from the Hindi test set and generated questions using both models.", "labels": [], "entities": [{"text": "Hindi test set", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.8066039780775706}]}, {"text": "We presented these sentence-question pairs for each model to three language experts and asked fora binary response on three quality parameters namely syntactic correctness, semantic correctness and relevance.", "labels": [], "entities": []}, {"text": "The responses from all the experts for each parameter was averaged for 8 shows two examples of correctly generated Chinese questions.", "labels": [], "entities": []}, {"text": "each model to get the final numbers shown in Table 3.", "labels": [], "entities": []}, {"text": "Although we perform comparably to the baseline model on syntactic correctness scores, we obtain significantly higher agreement across annotators using our cross-lingual model.", "labels": [], "entities": [{"text": "agreement", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9586041569709778}]}, {"text": "Our crosslingual model performs significantly better than the Transformer model on \"Relevance\" at the cost of agreement.", "labels": [], "entities": []}, {"text": "On semantic correctness, we perform signficantly better both in terms of the score and agreement statistics.", "labels": [], "entities": [{"text": "semantic correctness", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7503992915153503}]}], "tableCaptions": [{"text": " Table 1: HiQuAD dataset details", "labels": [], "entities": [{"text": "HiQuAD dataset", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9869026839733124}]}, {"text": " Table 1. All  model hyperparameters are optimized using the  development set and all results are reported on the  test set.", "labels": [], "entities": []}, {"text": " Table 2: BLEU, METEOR and ROUGE-L scores on the test set for Hindi and Chinese question generation. Best  results for each metric (column) are highlighted in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995792508125305}, {"text": "METEOR", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9875715970993042}, {"text": "ROUGE-L", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9981226325035095}, {"text": "Hindi and Chinese question generation", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.5011269211769104}]}, {"text": " Table 3: Human evaluation results as well as inter-rater  agreement (column \"Kappa\") for each model on the  Hindi test set. The scores are between 0-100, 0 be- ing the worst and 100 being the best. Best results for  each metric (column) are in bold. The three evalua- tion criteria are: (1) syntactic correctness (Syntax), (2)  semantic correctness (Semantics), and (3) relevance to  the paragraph (Relevance).", "labels": [], "entities": [{"text": "Hindi test set", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.7968835135300955}, {"text": "relevance", "start_pos": 371, "end_pos": 380, "type": "METRIC", "confidence": 0.974969208240509}]}, {"text": " Table 4: Ablation study showing the importance of both unsupervised and unsupervised pretraining for Hindi", "labels": [], "entities": [{"text": "Hindi", "start_pos": 102, "end_pos": 107, "type": "TASK", "confidence": 0.8340387940406799}]}, {"text": " Table 5: Ablation study showing the importance of using English QG data for Hindi QG", "labels": [], "entities": [{"text": "English QG data", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.6258137424786886}]}]}