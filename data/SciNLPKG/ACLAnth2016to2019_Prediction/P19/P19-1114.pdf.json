{"title": [{"text": "Context-specific language modeling for human trafficking detection from online advertisements", "labels": [], "entities": [{"text": "Context-specific language modeling", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7730177640914917}, {"text": "human trafficking detection", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.7135287721951803}]}], "abstractContent": [{"text": "Human trafficking is a worldwide crisis.", "labels": [], "entities": [{"text": "Human trafficking", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6292487680912018}]}, {"text": "Traf-fickers exploit their victims by anonymously offering sexual services through online advertisements.", "labels": [], "entities": []}, {"text": "These ads often contain clues that law enforcement can use to separate out potential trafficking cases from volunteer sex advertisements.", "labels": [], "entities": []}, {"text": "The problem is that the sheer volume of ads is too overwhelming for manual processing.", "labels": [], "entities": []}, {"text": "Ideally, a centralized semi-automated tool can be used to assist law enforcement agencies with this task.", "labels": [], "entities": []}, {"text": "Here, we present an approach using natural language processing to identify trafficking ads on these websites.", "labels": [], "entities": []}, {"text": "We propose a classifier by integrating multiple text feature sets, including the publicly available pre-trained textual language model Bi-directional Encoder Representation from transformers (BERT).", "labels": [], "entities": [{"text": "Bi-directional Encoder Representation from transformers (BERT", "start_pos": 135, "end_pos": 196, "type": "TASK", "confidence": 0.6735461354255676}]}, {"text": "In this paper, we demonstrate that a classifier using this composite feature set has significantly better performance compared to any single feature set alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "In 2013, the Global Slavery Index reported that 30 million individuals were living in involuntary servitude.", "labels": [], "entities": [{"text": "Global Slavery Index", "start_pos": 13, "end_pos": 33, "type": "DATASET", "confidence": 0.8511869112650553}]}, {"text": "Another estimation found that 600,000 women are trafficked in the sex industry per year with the United States being the second most popular destination for these individuals;).", "labels": [], "entities": []}, {"text": "In the last decade, it has become more difficult for law enforcement (LE) to trace traffickers as they have begun to take increasing advantage of online advertisement platforms for sexual services to solicit clients and become less visible.", "labels": [], "entities": []}, {"text": "LE is capable of tracking the posted ads and mining such data to detect trafficking victims.", "labels": [], "entities": []}, {"text": "However, the large volume of online unstructured data, the high degree of similarity of ads), and the lack of an automated approach in detecting suspicious activities through advertisements present obstacles for LE to independently develop methods for surveying these criminal activities.", "labels": [], "entities": [{"text": "LE", "start_pos": 212, "end_pos": 214, "type": "TASK", "confidence": 0.8793351650238037}]}, {"text": "Sex trafficking advertisements are unique texts.", "labels": [], "entities": []}, {"text": "They have incorrect grammatical structures and misspellings, and are enriched with unconventional words, abbreviations, and emojis.", "labels": [], "entities": []}, {"text": "Oftentimes the author uses emojis and emoticons to convey messages to a potential customer.", "labels": [], "entities": []}, {"text": "In particular these types of advertisements may also contain equivocal words, e.g., roses as a substitute for dollars.", "labels": [], "entities": []}, {"text": "Additionally, dominant keywords from these online ads continuously evolve as traffickers and consenting sex workers alike seek to evade prosecution.", "labels": [], "entities": []}, {"text": "While previous researchers have tried to develop automated systems to detect trafficking advertisements, this has proved an enormous challenge for natural language processing and machine learning.", "labels": [], "entities": []}, {"text": "In (), Whitney and colleagues propose to track the use of emojis and their significance in online sex ads as a potential indicator of trafficking.", "labels": [], "entities": []}, {"text": "This team processed emojis to determine the meaning of them used Close your eyes and imagine sliding into a warm flowing river of relaxation as I slowly pull and push your worries away.", "labels": [], "entities": []}, {"text": "I want you herewith me.", "labels": [], "entities": []}, {"text": "Satisfy my need to please you now.", "labels": [], "entities": []}, {"text": "in a sample of online ads, as indicated by interviews with law enforcement officials and individuals combating human trafficking.", "labels": [], "entities": []}, {"text": "Taking a different approach, Tong, Zadeh, and colleagues) collaborated with LE officials and annotated 10,000 ads.", "labels": [], "entities": []}, {"text": "With these annotated texts, they proposed the use of deep multimodal models to reach the accuracy of LE officials in identifying suspicious ads.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9991850256919861}]}, {"text": "Szekely and colleagues) created a large generic knowledge graph from a large database of online sexual ads that allows for visualization and querying data.", "labels": [], "entities": []}, {"text": "In this paper, we present part of an ongoing project.", "labels": [], "entities": []}, {"text": "Unlike previous studies, we tested our method on a relatively large number of ads labeled based on the corresponding phone number rather than human interpretation of the text itself.", "labels": [], "entities": []}, {"text": "In the following sections, we propose a method relying on extracting feature sets from ads to quantify their context.", "labels": [], "entities": []}, {"text": "We later use these feature sets in several predictive models to flag suspicious ads.", "labels": [], "entities": []}, {"text": "We also investigate the performance of a newly released pre-trained language model called the Bidirectional Encoder Representation from Transformers (BERT) to assess its power in analyzing this type of unstructured data.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representation from Transformers (BERT)", "start_pos": 94, "end_pos": 155, "type": "TASK", "confidence": 0.654724195599556}]}], "datasetContent": [{"text": "In our study, we employ the feature models described above and compare the results of the binary classification corresponding to them.", "labels": [], "entities": []}, {"text": "We use logistic regression and compute the precision and recall curve (PRC) to evaluate the performance of different models.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9992350339889526}, {"text": "recall curve (PRC)", "start_pos": 57, "end_pos": 75, "type": "METRIC", "confidence": 0.9580312848091126}]}, {"text": "Moreover, in this application, it is important to have a model with good recall while keeping high precision, i.e., a high positive predictive value (PPV) to avoid unnecessary actions.", "labels": [], "entities": [{"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9985671043395996}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9963591694831848}, {"text": "predictive value (PPV)", "start_pos": 132, "end_pos": 154, "type": "METRIC", "confidence": 0.8242871046066285}]}, {"text": "To do so, we investigate the sensitivity of models in different high PPVs.", "labels": [], "entities": []}, {"text": "We choose to not remove stop words or not use any stemming or lemmatization techniques as we are faced with different writing structures which could be informative for our model.", "labels": [], "entities": []}, {"text": "We test the impact of emojis and punctuation by training and testing our model by creating two text sets.", "labels": [], "entities": []}, {"text": "In the first text set, we keep the emojis and punctuation and remove them in the second set.", "labels": [], "entities": []}, {"text": "In the second set, we convert the emojis to words.", "labels": [], "entities": []}, {"text": "Numbers in the texts are removed, because: 1) we have made the labels based on phone numbers and 2), the ads are likely to have the same age or same price throughout the texts.", "labels": [], "entities": []}, {"text": "We then divide the data into an 80/20% training/testing set.", "labels": [], "entities": []}, {"text": "In the following sections, we describe how each set of features is processed while using logistic regression as our fixed classification model.", "labels": [], "entities": []}, {"text": "We begin with features coming from LDA topic modeling scores where we assign it to 12 topics.", "labels": [], "entities": [{"text": "LDA topic modeling", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7214158574740092}]}, {"text": "Gensim LDA is implemented by making a bag of words dictionary of our training set.", "labels": [], "entities": [{"text": "Gensim LDA", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8396826088428497}]}, {"text": "We find this optimal topic number where we examined the explained LDA feature set via crossvalidation on January 2017 alone.", "labels": [], "entities": [{"text": "LDA feature set", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.7857690652211508}]}, {"text": "Our FastText model is trained on a set including a minimum count of 2 words and a window size of 3 to give us a vector of dimension 100.", "labels": [], "entities": []}, {"text": "After training the FastText model, the average word vector of the training set is computed.", "labels": [], "entities": []}, {"text": "Using this saved language model from the training set, we compute the feature test vectors.", "labels": [], "entities": []}, {"text": "For encoding our texts using BERT, we make a list of all documents and use the BERT service client.", "labels": [], "entities": [{"text": "BERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.8107165098190308}, {"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.7855271100997925}]}, {"text": "We use the weights of the words that BERT BASE learned in its pretraining to encode each document to a vector of size 768 for both the training and testing sets.", "labels": [], "entities": [{"text": "BERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9135844111442566}, {"text": "BASE", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.40386104583740234}]}, {"text": "We examine encoding texts with both Cased BERT (C-BASED) and Uncased BERT (U-BERT).", "labels": [], "entities": [{"text": "Cased BERT", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.7146395742893219}, {"text": "Uncased BERT (U-BERT)", "start_pos": 61, "end_pos": 82, "type": "METRIC", "confidence": 0.7961361289024353}]}, {"text": "In the U-BERT, the text has been lower cased, whereas in C-BERT, the true case and accent markers are preserved.", "labels": [], "entities": []}, {"text": "In this final step in featurization towards our composite model, we combine all three types of features to build a unified feature set, i.e. combining LDA, AWV and BERT.", "labels": [], "entities": [{"text": "AWV", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.815644383430481}, {"text": "BERT", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9916941523551941}]}, {"text": "depicts the results of the classifications of the different feature sets.", "labels": [], "entities": []}, {"text": "It can be seen that both classification approaches based on LDA and the average word vector features achieve similarly average precision scores (APS).", "labels": [], "entities": [{"text": "precision scores (APS)", "start_pos": 127, "end_pos": 149, "type": "METRIC", "confidence": 0.8973765730857849}]}, {"text": "Based on our analysis, keeping the entire text or removing emojis and punctuation do not significantly impact the results.", "labels": [], "entities": []}, {"text": "From, it can be seen that, despite small improvements, different featurizations provide similar APS values.", "labels": [], "entities": [{"text": "APS", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.7262322902679443}]}, {"text": "However, focusing more on the early parts of the PRC, i.e., high precision, we can see that there is a significant improvement of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9976174235343933}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9988915324211121}]}, {"text": "For example, as summarized in, at 85% precision, our proposed full model (with U-BERT) achieves 69% and 67% sensitivity on pure text and text without emojis and punctuation, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9990878105163574}, {"text": "sensitivity", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9836249351501465}]}, {"text": "However, in the composite model with C-BERT, there is an opposite effect where recalls become 65% and 69% for the two scenarios, respectively.", "labels": [], "entities": [{"text": "recalls", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9988967180252075}]}], "tableCaptions": []}