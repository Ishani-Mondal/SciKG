{"title": [{"text": "RankQA: Neural Question Answering with Answer Re-Ranking", "labels": [], "entities": [{"text": "RankQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8407324552536011}, {"text": "Neural Question Answering with Answer Re-Ranking", "start_pos": 8, "end_pos": 56, "type": "TASK", "confidence": 0.7615616023540497}]}], "abstractContent": [{"text": "The conventional paradigm in neural question answering (QA) for narrative content is limited to a two-stage process: first, relevant text passages are retrieved and, subsequently, a neural network for machine comprehension extracts the likeliest answer.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.8856052398681641}]}, {"text": "However, both stages are largely isolated in the status quo and, hence, information from the two phases is never properly fused.", "labels": [], "entities": []}, {"text": "In contrast, this work proposes RankQA 1 : RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re-ranking.", "labels": [], "entities": []}, {"text": "The re-ranking leverages different features that are directly extracted from the QA pipeline, i. e., a combination of retrieval and comprehension features.", "labels": [], "entities": []}, {"text": "While our intentionally simple design allows for an efficient, data-sparse estimation, it nevertheless outper-forms more complex QA systems by a significant margin: in fact, RankQA achieves state-of-the-art performance on 3 out of 4 benchmark datasets.", "labels": [], "entities": [{"text": "RankQA", "start_pos": 174, "end_pos": 180, "type": "DATASET", "confidence": 0.8388949632644653}]}, {"text": "Furthermore, its performance is especially superior in settings where the size of the corpus is dynamic.", "labels": [], "entities": []}, {"text": "Here the answer re-ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size.", "labels": [], "entities": []}, {"text": "As a consequence, RankQA represents a novel, powerful, and thus challenging baseline for future research in content-based QA.", "labels": [], "entities": [{"text": "RankQA", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.933243453502655}]}], "introductionContent": [{"text": "Question answering (QA) has recently experienced considerable success in variety of benchmarks due to the development of neural QA).", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9456744194030762}]}, {"text": "These systems largely follow a two-stage process.", "labels": [], "entities": []}, {"text": "First, a module for information retrieval selects text passages which appear relevant to the query from the cor-pus.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.8031732738018036}]}, {"text": "Second, a module for machine comprehension extracts the final answer, which is then returned to the user.", "labels": [], "entities": []}, {"text": "This two-stage process is necessary for condensing the original corpus to passages and eventually answers; however, the dependence limits the extent to which information is passed on from one stage to the other.", "labels": [], "entities": []}, {"text": "Extensive efforts have been made to facilitate better information flow between the two stages.", "labels": [], "entities": []}, {"text": "These works primarily address the interface between the stages (, i. e., which passages and how many of them are forwarded from information retrieval to machine comprehension.", "labels": [], "entities": []}, {"text": "For instance, the QA performance is dependent on the corpus size and the number of top-n passages that are fed into the module for machine comprehension.", "labels": [], "entities": []}, {"text": "Nevertheless, machine comprehension in this approach makes use of only limited information (e. g., it ignores the confidence or similarity information computed during retrieval).", "labels": [], "entities": []}, {"text": "State-of-the-art approaches for selecting better answers engineer additional features within the machine comprehension model with the implicit goal of considering information retrieval.", "labels": [], "entities": []}, {"text": "For instance, the DrQA architecture of includes features pertaining to the match between question words and words in the paragraph.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.9081925749778748}]}, {"text": "Certain other works also incorporate a linear combination of paragraph and answer score (.", "labels": [], "entities": []}, {"text": "Despite that, the use is limited to simplistic features and the potential gains of re-ranking remain untapped.", "labels": [], "entities": []}, {"text": "Prior literature has recently hinted at potential benefits from answer re-ranking, albeit in a different setting (: the authors studied multi-paragraph machine comprehension at sentence level, instead of a complete QA pipeline involving an actual information retrieval module Figure 1: The RankQA system consisting of three modules for information retrieval, machine comprehension, and our novel answer re-ranking.", "labels": [], "entities": [{"text": "answer re-ranking", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8142076730728149}, {"text": "information retrieval", "start_pos": 336, "end_pos": 357, "type": "TASK", "confidence": 0.7677240371704102}]}, {"text": "RankQA fuses information from the information retrieval and machine comprehension phase to re-rank answer candidates within a full neural QA pipeline.", "labels": [], "entities": []}, {"text": "over a full corpus of documents.", "labels": [], "entities": []}, {"text": "However, when adapting it from a multi-paragraph setting to a complete corpus, this type of approach is known to become computationally infeasible (cf. discussion in.", "labels": [], "entities": []}, {"text": "In contrast, answer reranking as part of an actual QA pipeline not been previously studied.", "labels": [], "entities": [{"text": "answer reranking", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.8770558834075928}]}, {"text": "Proposed RankQA: This paper proposes a novel paradigm for neural QA.", "labels": [], "entities": []}, {"text": "That is, we augment the conventional two-staged process with an additional third stage for efficient answer reranking.", "labels": [], "entities": [{"text": "answer reranking", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.9120714366436005}]}, {"text": "This approach, named \"RankQA\", overcomes the limitations of a two-stage process in the status quo whereby both stages operate largely in isolation and where information from the two is never properly fused.", "labels": [], "entities": []}, {"text": "In contrast, our module for answer re-ranking fuses features that stem from both retrieval and comprehension.", "labels": [], "entities": [{"text": "answer re-ranking", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8733668029308319}]}, {"text": "Our approach is intentionally light-weight, which contributes to an efficient estimation, even when directly integrated into the full QA pipeline.", "labels": [], "entities": []}, {"text": "We show the robustness of our approach by demonstrating significant performance improvements over different QA pipelines.", "labels": [], "entities": []}, {"text": "Contributions: To the best of our knowledge, RankQA represents the first neural QA pipeline with an additional third stage for answer re-ranking.", "labels": [], "entities": [{"text": "RankQA", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.8672757744789124}, {"text": "answer re-ranking", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.8709748387336731}]}, {"text": "Despite the light-weight architecture, RankQA achieves state-of-the-art performance across 3 established benchmark datasets.", "labels": [], "entities": [{"text": "RankQA", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.873199999332428}]}, {"text": "In fact, it even outperforms more complex approaches by a considerable margin.", "labels": [], "entities": []}, {"text": "This particularly holds true when the corpus size is variable and where the resulting noise-information trade-off requires an effective remedy.", "labels": [], "entities": []}, {"text": "Altogether, RankQA yields a strong new baseline for contentbased question answering.", "labels": [], "entities": [{"text": "RankQA", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.8712763786315918}, {"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6959216445684433}]}], "datasetContent": [{"text": "Following earlier research, our content base comprises documents from the English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.9120565056800842}]}, {"text": "For comparison purposes, we use the same dump as in prior work (e. g.,.", "labels": [], "entities": []}, {"text": "We do not use pre-selected documents or other textual content in order to answer questions.", "labels": [], "entities": []}, {"text": "We base our experiments on four wellestablished datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Exact matches of RankQA compared to DrQA as natural baseline without re-ranking and state-of-the-art  systems for neural QA. We use a general model that is trained on all datasets, and a task-specific model that is  trained individually for every dataset. The two best results for every dataset are marked in bold.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.937031626701355}]}, {"text": " Table 3: Exact matches of RankQA based on the BERT-QA pipeline. We show results of the the pipline without  re-ranking, the results obtained by our re-ranking model, and an upper bound (i. e., perfect re-ranking).", "labels": [], "entities": [{"text": "BERT-QA", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9834058284759521}]}, {"text": " Table 4: Feature importance (i. e., averaged performance of exact matches on a hold-out sample). We train the  general model using the same data, but blind one group of features every time. We underline results that undershoot  the baseline and mark results in bold that surpass the general model trained on all features.", "labels": [], "entities": []}]}