{"title": [{"text": "Argument Generation with Retrieval, Planning, and Realization", "labels": [], "entities": [{"text": "Argument Generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8178811073303223}, {"text": "Realization", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.7846318483352661}]}], "abstractContent": [{"text": "Automatic argument generation is an appealing but challenging task.", "labels": [], "entities": [{"text": "Automatic argument generation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7122122049331665}]}, {"text": "In this paper, we study the specific problem of counter-argument generation, and present a novel framework, CANDELA.", "labels": [], "entities": [{"text": "counter-argument generation", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.828698068857193}, {"text": "CANDELA", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.8098238110542297}]}, {"text": "It consists of a powerful retrieval system and a novel two-step generation model, where a text planning de-coder first decides on the main talking points and a proper language style for each sentence, then a content realization decoder reflects the decisions and constructs an informative paragraph-level argument.", "labels": [], "entities": []}, {"text": "Furthermore, our generation model is empowered by a retrieval system indexed with 12 million articles collected from Wikipedia and popular English news media, which provides access to high-quality content with diversity.", "labels": [], "entities": []}, {"text": "Automatic evaluation on a large-scale dataset collected from Reddit shows that our model yields significantly higher BLEU, ROUGE, and METEOR scores than the state-of-the-art and non-trivial comparisons.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9994381070137024}, {"text": "ROUGE", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.995364785194397}, {"text": "METEOR", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9979352951049805}]}, {"text": "Human evaluation further indicates that our system arguments are more appropriate for refutation and richer in content.", "labels": [], "entities": []}], "introductionContent": [{"text": "Counter-argument generation aims to produce arguments of a different stance, in order to refute the given proposition on a controversial issue.", "labels": [], "entities": [{"text": "Counter-argument generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.902629941701889}]}, {"text": "A system that automatically constructs counter-arguments can effectively present alternative perspectives along with associated evidence and reasoning, and thus facilitate a more comprehensive understanding of complicated problems when controversy arises.", "labels": [], "entities": []}, {"text": "Nevertheless, constructing persuasive arguments is a challenging task, as it requires an appropriate combination of credible evidence, rigorous logical reasoning, and sometimes emotional appeal (;.", "labels": [], "entities": []}, {"text": "A sample counter-argument fora pro-death penalty post is shown in.", "labels": [], "entities": []}, {"text": "As can be seen, a sequence of talking points on the \"imperfect justice system\" are presented: it starts with the fundamental concept, then follows up with more specific evaluative claim and supporting fact.", "labels": [], "entities": []}, {"text": "Although retrieval-based methods have been investigated to construct counter-arguments ( , they typically produce a collection of sentences from disparate sources, thus fall short of coherence and conciseness.", "labels": [], "entities": []}, {"text": "Moreover, human always deploy stylistic languages with specific argumentative functions to promote persuasiveness, such as making a concessive move (e.g., \"In theory I agree with you\").", "labels": [], "entities": []}, {"text": "This further requires the generation system to have better control of the languages style.", "labels": [], "entities": []}, {"text": "Our goal is to design a counter-argument generation system to address the above challenges and produce paragraph-level arguments with rich-yetcoherent content.", "labels": [], "entities": [{"text": "counter-argument generation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.7535316050052643}]}, {"text": "To this end, we present CAN-DELA-a novel framework to generate CounterArguments with two-step Neural Decoders and ExternaL knowledge Augmentation.", "labels": [], "entities": [{"text": "ExternaL knowledge Augmentation", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.5919172664483389}]}, {"text": "Concretely, CANDELA has three major distinct features: First, it is equipped with two decoders: one for text planning-selecting talking points to cover for each sentence to be generated, the other for content realization-producing a fluent argument to reflect decisions made by the text planner.", "labels": [], "entities": [{"text": "text planning-selecting talking points", "start_pos": 104, "end_pos": 142, "type": "TASK", "confidence": 0.8141122087836266}, {"text": "content realization-producing a fluent argument", "start_pos": 201, "end_pos": 248, "type": "TASK", "confidence": 0.8208240628242492}]}, {"text": "This enables our model to produce longer arguments with richer information.", "labels": [], "entities": []}, {"text": "Furthermore, multiple objectives are designed for our text planning decoder to both handle content selection and ordering, and select a proper argumentative discourse function of a desired language style for each sentence generation.", "labels": [], "entities": []}, {"text": "Lastly, the input to our argument generation model is augmented with keyphrases and passages retrieved from a large-scale search engine, which indexes 12 million articles from Wikipedia and four popular English news media of varying ideological leanings.", "labels": [], "entities": [{"text": "argument generation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7115913331508636}]}, {"text": "This ensures access to reliable evidence, high-quality reasoning, and diverse opinions from different sources, as opposed to recent work that mostly considers a single origin, such as Wikipedia () or online debate portals ().", "labels": [], "entities": []}, {"text": "We experiment with argument and counterargument pairs collected from the Reddit /r/ChangeMyView group.", "labels": [], "entities": [{"text": "Reddit /r/ChangeMyView group", "start_pos": 73, "end_pos": 101, "type": "DATASET", "confidence": 0.7177828053633372}]}, {"text": "Automatic evaluation shows that the proposed model significantly outperforms our prior argument generation system) and other non-trivial comparisons.", "labels": [], "entities": []}, {"text": "Human evaluation further suggests that our model produces more appropriate counter-arguments with richer content than other automatic systems, while maintaining a fluency level comparable to human-constructed arguments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employ ROUGE), a recall-oriented metric, BLEU (), based on n-gram precision, and METEOR), measuring unigram precision and recall by considering synonyms, paraphrases, and stemming.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9929343461990356}, {"text": "recall-oriented", "start_pos": 20, "end_pos": 35, "type": "METRIC", "confidence": 0.9824460744857788}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9977248311042786}, {"text": "METEOR", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9975468516349792}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.735248327255249}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9955654740333557}]}, {"text": "BLEU-2, BLEU-4, ROUGE-2 recall, and METEOR are reported in for both setups.", "labels": [], "entities": [{"text": "BLEU-2", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9872903227806091}, {"text": "BLEU-4", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9975870847702026}, {"text": "ROUGE-2", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9970779418945312}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.7687935829162598}, {"text": "METEOR", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9962360262870789}]}, {"text": "Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test  0.0005).", "labels": [], "entities": [{"text": "CANDELA", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.8772746324539185}]}, {"text": "Furthermore, our model generates longer sentences whose lengths are comparable with human arguments, both with about 22 words per sentence.", "labels": [], "entities": []}, {"text": "This also results in longer arguments.", "labels": [], "entities": []}, {"text": "Under oracle setup, all models are notably improved due to the higher quality of reranked passages, and our model achieves statistically significantly better BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9991543292999268}]}, {"text": "Interestingly, we observe a decrease of ROUGE and METEOR, but a marginal increase of BLEU-2 by removing passages from our model input.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9978945851325989}, {"text": "METEOR", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9962150454521179}, {"text": "BLEU-2", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9989032745361328}]}, {"text": "This could be because the passages introduce divergent content, albeit probably on-topic, that cannot be captured by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.912224531173706}]}, {"text": "We further measure whether our model is able to generate diverse content.", "labels": [], "entities": []}, {"text": "First, borrowing the diversity measurement from dialogue generation research (, we report the average number of distinct n-grams per argument under system setup in.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7666899263858795}]}, {"text": "Our system generates more unique unigrams and bigrams than other automatic systems, underscoring its capability of generating diverse content.", "labels": [], "entities": []}, {"text": "Our model also maintains a comparable typetoken ratio (TTR) compared to systems that generate shorter arguments, e.g., a 0.79 for bigram TTR of our model versus 0.83 and 0.84 for SEQ2SEQAUG and SEQ2SEQ.", "labels": [], "entities": [{"text": "typetoken ratio (TTR)", "start_pos": 38, "end_pos": 59, "type": "METRIC", "confidence": 0.9313219666481019}]}, {"text": "RETRIEVAL, con-: Main results on argument generation.", "labels": [], "entities": [{"text": "RETRIEVAL", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9516821503639221}, {"text": "argument generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7418038100004196}]}, {"text": "We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence.", "labels": [], "entities": [{"text": "BLEU-2", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9982459545135498}, {"text": "BLEU-4", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9980704188346863}, {"text": "ROUGE-2 (R-2)", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.8470510542392731}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.5116883516311646}, {"text": "METEOR (MTR)", "start_pos": 60, "end_pos": 72, "type": "METRIC", "confidence": 0.9302481561899185}]}, {"text": "Best scores are in bold.", "labels": [], "entities": []}, {"text": "* : statistically significantly better than all comparisons (randomization approximation test, p < 0.0005).", "labels": [], "entities": []}, {"text": "Input is the same for SEQ2SEQ for both system and oracle setups.", "labels": [], "entities": [{"text": "Input", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.982353687286377}]}, {"text": "taining top ranked passages of human-edited content, produces the most distinct words.", "labels": [], "entities": []}, {"text": "Next, we compare how each system generates content beyond the common words.", "labels": [], "entities": []}, {"text": "As shown in, human-edited text, including goldstandard arguments (HUMAN) and retrieved passages, tends to have higher usage of uncommon words than automatic systems, suggesting the gap between human vs. system arguments.", "labels": [], "entities": []}, {"text": "Among the four automatic systems, our prior model) generates a significantly higher portion of uncommon words, yet further inspection shows that the output often includes more offtopic information.", "labels": [], "entities": []}, {"text": "Human judges are asked to rate arguments on a Likert scale of 1 (worst) to 5 (best) on the following three aspects: grammaticality-denotes language fluency; appropriateness-indicates if the output is on-topic and on the opposing stance; content richness-measures the amount of distinct talking points.", "labels": [], "entities": []}, {"text": "In order to promote consistency of annotation, we provide descriptions and sample arguments for each scale.", "labels": [], "entities": [{"text": "consistency", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9556851983070374}]}, {"text": "For example, an appropriateness score of 3 means the counterargument contains relevant words and is likely to be on a different stance.", "labels": [], "entities": []}, {"text": "The judges are then asked to rank all arguments for the same input based on their overall quality.", "labels": [], "entities": []}, {"text": "We randomly sampled 43 threads from the test set, and hired three native or proficient English speakers to evaluate arguments generated by SEQ2SEQAUG, our prior argument generation: Human evaluation on grammaticality (Gram), appropriateness (Appr), and content richness (Cont.), on a scale of 1 to 5 (best).", "labels": [], "entities": [{"text": "Appr", "start_pos": 242, "end_pos": 246, "type": "METRIC", "confidence": 0.9021490812301636}]}, {"text": "The best result among automatic systems is highlighted in bold, with statistical significance marked with * (approximation randomization test, p < 0.0005).", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 69, "end_pos": 93, "type": "METRIC", "confidence": 0.89943927526474}]}, {"text": "The highest standard deviation among all is 1.0.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 12, "end_pos": 30, "type": "METRIC", "confidence": 0.92887282371521}]}, {"text": "Top-1/2: % of evaluations a system being ranked in top 1 or 2 for overall quality.", "labels": [], "entities": []}, {"text": "model, and the new model CANDELA, along with gold-standard HUMAN arguments and the top passage by RETRIEVAL.", "labels": [], "entities": [{"text": "CANDELA", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.8835347890853882}, {"text": "RETRIEVAL", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9368337392807007}]}, {"text": "The first 3 examples are used only for calibration, and the remaining 40 are used to report results in.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement scores (Krippendorff's \u03b1) of 0.44, 0.58, 0.49 are achieved for the three aspects, implying general consensus to intermediate agreement.", "labels": [], "entities": [{"text": "Inter-annotator agreement scores (Krippendorff's \u03b1)", "start_pos": 0, "end_pos": 51, "type": "METRIC", "confidence": 0.7213674299418926}]}, {"text": "Our system obtains the highest appropriateness and content richness among all automatic systems.", "labels": [], "entities": []}, {"text": "This confirms the previous observation that our model produces more informative argument than other neural models.", "labels": [], "entities": []}, {"text": "SEQ2SEQAUG has a marginally better grammaticality score, likely due to the fact that our arguments are longer, and tend to contain less fluent generation towards the end.", "labels": [], "entities": [{"text": "SEQ2SEQAUG", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7542831301689148}]}, {"text": "Furthermore, we see that human arguments are ranked as the best in about 76% of the evaluation, followed by RETRIEVAL.", "labels": [], "entities": [{"text": "RETRIEVAL", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9869183897972107}]}, {"text": "Our model is more likely to be ranked top than any other automatic models.", "labels": [], "entities": []}, {"text": "Especially, our model is rated better than either HUMAN or RETRIEVAL, i.e., human-edited text, in 39.2% of the evaluations, compared to 34.2% for SEQ2SEQAUG and 13.3% for our prior model.", "labels": [], "entities": [{"text": "RETRIEVAL", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9406653642654419}]}], "tableCaptions": [{"text": " Table 1: Statistics on information sources for argu- ment retrieval. News media are sorted by ideologi- cal leanings from left to right, according to https:  //www.adfontesmedia.com/.", "labels": [], "entities": [{"text": "argu- ment retrieval", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.5847363620996475}]}, {"text": " Table 1.  We segment articles into passages with a slid- ing window of three sentences, with a step size of  two. We further constraint the passages to have  at least 50 words. For shorter passages, we keep  adding subsequent sentences until reaching the  length limit. Per", "labels": [], "entities": []}, {"text": " Table 2: Statistics on the datasets for experiments.", "labels": [], "entities": []}, {"text": " Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall,  METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold.  * : statis- tically significantly better than all comparisons (randomization approximation test", "labels": [], "entities": [{"text": "argument generation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7296925038099289}, {"text": "BLEU-2", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.998012900352478}, {"text": "BLEU-4", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9980705380439758}, {"text": "ROUGE-2 (R-2) recall", "start_pos": 85, "end_pos": 105, "type": "METRIC", "confidence": 0.7913403272628784}, {"text": "METEOR (MTR)", "start_pos": 108, "end_pos": 120, "type": "METRIC", "confidence": 0.937263548374176}]}, {"text": " Table 4: Human evaluation on grammaticality (Gram),  appropriateness (Appr), and content richness (Cont.),  on a scale of 1 to 5 (best). The best result among au- tomatic systems is highlighted in bold, with statistical  significance marked with  *  (approximation randomiza- tion test, p < 0.0005). The highest standard deviation  among all is 1.0. Top-1/2: % of evaluations a system  being ranked in top 1 or 2 for overall quality.", "labels": [], "entities": [{"text": "Appr", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9299618005752563}]}]}