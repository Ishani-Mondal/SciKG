{"title": [{"text": "CodeForTheChange at SemEval-2019 Task 8: Skip-Thoughts for Fact Checking in Community Question Answering", "labels": [], "entities": [{"text": "Fact Checking in Community Question Answering", "start_pos": 59, "end_pos": 104, "type": "TASK", "confidence": 0.7780115803082784}]}], "abstractContent": [{"text": "Community Question Answering (cQA) is one of the popular Natural Language Processing (NLP) problems being targeted by researchers across the globe.", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7690049558877945}]}, {"text": "Couple of the unanswered questions in the domain of cQA are 'can we label the questions/answers as factual or not?' and 'Is the given answer by the user to a particular factual question is correct and if it is correct, can we measure the correctness and factuality of the given answer?'.", "labels": [], "entities": []}, {"text": "We have participated in SemEval-2019 Task 8 which deals with these questions.", "labels": [], "entities": [{"text": "SemEval-2019 Task 8", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8782188495000204}]}, {"text": "In this paper, we present the features used, approaches followed for feature engineering, models experimented with and finally the results.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8463129103183746}]}, {"text": "Our primary submission with accuracy (official metric for Se-mEval Task 8) of 0.65 in Subtask B (An-swer Classification) and 0.63 in Subtask A (Question Classification) stood at 6 th and 16 th places respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9994975328445435}]}], "introductionContent": [{"text": "Community Question Answering (cQA) forums such as Quora, StackOverflow, Yahoo!", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.732711523771286}]}, {"text": "Answers, Qatar Living etc., now-a-days are fast and effective means of getting answers for any question.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5220115184783936}, {"text": "Qatar Living", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.9589596390724182}]}, {"text": "But the answers mayor may not be correct and factual always.", "labels": [], "entities": []}, {"text": "The focus of cQA research, for the last few couple of years, is revolving around determining the model which predicts the best answer for the question, given a question and a number of answers (might be hundreds or even thousands in number).", "labels": [], "entities": []}, {"text": "cQA is one of the popular problems being constantly in focus of SemEval organizers since 2015.", "labels": [], "entities": [{"text": "cQA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8971951603889465}]}, {"text": "The subtasks that were targeted earlier include (i) classifying the answer to a particular question as good or potentially good or bad in 2015 1 , (ii) three reranking subtasks i.e., QuestionComment Similarity, Question-Question Similarity and Question-External Comment Similarity in http://alt.qcri.org/semeval2015/task3/ 2016 2 and (iii) Question Similarity (QS) to detect duplicate questions and Relevance Classification (RC) in 2017 3 . Contrary to earlier tasks of SemEval focusing mainly on classification and similarity of questions and/or answers and/or comments, SemEval-2019 targets the factuality of the questions (whether the question is factual or not) and the factuality of the answers (whether the answers provided to the factual questions are factual or not).", "labels": [], "entities": [{"text": "Question-External Comment Similarity", "start_pos": 244, "end_pos": 280, "type": "TASK", "confidence": 0.596796711285909}, {"text": "Question Similarity (QS)", "start_pos": 340, "end_pos": 364, "type": "METRIC", "confidence": 0.5896856844425201}, {"text": "Relevance Classification (RC)", "start_pos": 399, "end_pos": 428, "type": "METRIC", "confidence": 0.9271350502967834}]}, {"text": "The tasks become more challenging as data have noisy (like !!!), and unstructured (like Oh..) words.", "labels": [], "entities": []}, {"text": "SemEval-2019 Task 8 features the following two subtasks: whether a question asks fora factual information, an opinion/advice or is just socializing.", "labels": [], "entities": []}, {"text": "Example from the \"Qatar Living\" forum given in competition page 4 for this subtask is as follows: Q: I have heard its not possible to extend visit visa more than 6 months?", "labels": [], "entities": [{"text": "Qatar Living\" forum", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.9372581988573074}, {"text": "Q", "start_pos": 98, "end_pos": 99, "type": "METRIC", "confidence": 0.9732670187950134}]}, {"text": "Can U please answer me..", "labels": [], "entities": []}, {"text": "answer 1: Maximum period is 9 Months....", "labels": [], "entities": []}, {"text": "answer 2: 6 months maximum answer 3: This has been answered in QL so many times.", "labels": [], "entities": [{"text": "QL", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.7786899209022522}]}, {"text": "Please do search for information regarding this.", "labels": [], "entities": []}, {"text": "BTW answer is 6 months.", "labels": [], "entities": [{"text": "BTW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7844644784927368}, {"text": "answer", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.4986359179019928}]}, {"text": "This subtask aims at building models to detect true factual information in cQA forums.", "labels": [], "entities": []}, {"text": "Subtask B (Answer Classification) -determine whether an answer to a factual question is true, false, or does not constitute a proper answer.", "labels": [], "entities": [{"text": "Answer Classification", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6738380193710327}]}, {"text": "This subtask aims at building models that classify the answers into the following three categories, given a factual question: a) Fac-tual -True b) Factual -False and c) Non-Factual.", "labels": [], "entities": []}, {"text": "The examples for each of them are as follows: \u2022 We participated in both the subtasks of SemEval-2019 Task 8.", "labels": [], "entities": [{"text": "SemEval-2019 Task 8", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7552209297815958}]}, {"text": "For detailed description of the task, different approaches used by other participants and results obtained by all the participants, please refer the task description paper (.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 describes the related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the data used for this SemEval task.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.9336533844470978}]}, {"text": "Sections 4 and 5 elucidate the system architecture (feature extraction and model building) and experimentation details (along with the results) respectively.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7338649928569794}]}, {"text": "Section 6 concludes the paper with focus on future research on this task.", "labels": [], "entities": []}, {"text": "Another related task to cQA is Fact Checking in Community Forums (.", "labels": [], "entities": [{"text": "Fact Checking in Community Forums", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.9301825761795044}]}, {"text": "This work doesn't involve classification of questions/answers based on factuality but it determines the veracity of the answer given a particular question.", "labels": [], "entities": [{"text": "classification of questions/answers", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.8499542474746704}, {"text": "veracity", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9620586633682251}]}, {"text": "This work is related to our task in away that the data being used in our task is annotated and released to the research community by Tsvetomila Mihaylova and her team.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Evaluation scores for Subtask A   *  -marks the scores of our primary submission   *  *  -marks the scores of our contrastive submission  Row in bold -post evaluation accuracy score (improved  over actual submission)", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 177, "end_pos": 191, "type": "METRIC", "confidence": 0.918735533952713}]}, {"text": " Table 4: Evaluation scores for Subtask B   *  -marks the scores of our primary submission   *  *  -marks the scores of our contrastive submission  Row in bold -post evaluation accuracy score (improved  over actual submission)", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 177, "end_pos": 191, "type": "METRIC", "confidence": 0.9199005961418152}]}]}