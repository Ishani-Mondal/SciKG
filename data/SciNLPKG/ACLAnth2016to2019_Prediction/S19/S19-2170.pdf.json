{"title": [{"text": "Spider-Jerusalem at SemEval-2019 Task 4: Hyperpartisan News Detection", "labels": [], "entities": [{"text": "Hyperpartisan News Detection", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.7188604672749838}]}], "abstractContent": [{"text": "This paper describes our system for detecting hyperpartisan news articles, which was submitted for the shared task in SemEval 2019 on Hyperpartisan News Detection.", "labels": [], "entities": [{"text": "detecting hyperpartisan news articles", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.801168903708458}, {"text": "SemEval 2019 on Hyperpartisan News Detection", "start_pos": 118, "end_pos": 162, "type": "TASK", "confidence": 0.6756631930669149}]}, {"text": "We developed a Support Vector Machine (SVM) model that uses TF-IDF of tokens, Language Inquiry and Word Count (LIWC) features, and structural features such as number of paragraphs and hyperlink count in an article.", "labels": [], "entities": [{"text": "Word Count (LIWC)", "start_pos": 99, "end_pos": 116, "type": "METRIC", "confidence": 0.7950405478477478}]}, {"text": "The model was trained on 645 articles from two classes: mainstream and hyperpartisan.", "labels": [], "entities": []}, {"text": "Our system was ranked seventeenth out of forty two participating teams in the binary classification task with an accuracy score of 0.742 on the blind test set (the accuracy of the top ranked system was 0.822).", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.7818931837876638}, {"text": "accuracy score", "start_pos": 113, "end_pos": 127, "type": "METRIC", "confidence": 0.9821900725364685}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9993288516998291}]}, {"text": "We provide a detailed description of our preprocessing steps, discussion of our experiments using different combinations of features, and analysis of our results and prediction errors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Fake news on various online media outlets misinform the public and threaten the integrity of journalism.", "labels": [], "entities": []}, {"text": "This has serious effects on shaping public opinions on controversial topics such as climate change, and swaying voters in political elections.", "labels": [], "entities": []}, {"text": "Yellow press existed long before the digital age but had limited reach when compared to mainstream press.", "labels": [], "entities": []}, {"text": "However, with the introduction of social media, news that are extremely biased (hyperpartisan) tend to spread more quickly than the ones that are not (.", "labels": [], "entities": []}, {"text": "Therefore, there is a need for automatic detection methods of hyperpartisan news.", "labels": [], "entities": []}, {"text": "Computational methods for fighting fake news mainly focus on automatic fact-checking rather than looking at writing styles of news articles . SemEval-2019 Hyperpartisan News Detection shared task aims to study the ability of a system to detect if a given article exhibits a hyperpartisan argumentation writing style to convince readers of a certain position.", "labels": [], "entities": [{"text": "SemEval-2019 Hyperpartisan News Detection shared task", "start_pos": 142, "end_pos": 195, "type": "TASK", "confidence": 0.679679716626803}]}, {"text": "The shared task introduces a binary classification task of classifying an article into one of two possibilities: mainstream or hyperpartisan.", "labels": [], "entities": []}, {"text": "The data for the shared task was introduced by ) and consists of 645 of articles from mainstream, left-wing, and right-wing publishers.", "labels": [], "entities": []}, {"text": "The articles from both left-wing and right-wing publishers were labeled as hyperpartisan.", "labels": [], "entities": []}, {"text": "The baseline system to detect hyperpartisan developed by ) uses Unmasking ( and was trained on 1,627 of articles.", "labels": [], "entities": []}, {"text": "The articles are from nine publishers in the US: three mainstream (ABC News, CNN, and Politico), three left-wing (Addicting Info, Occupy Democrats, and The Other 98%), and three right-wing (Eagle Rising, Freedom Daily, and Right Wing News).", "labels": [], "entities": [{"text": "Eagle Rising, Freedom Daily", "start_pos": 190, "end_pos": 217, "type": "DATASET", "confidence": 0.8770681977272033}, {"text": "Right Wing News", "start_pos": 223, "end_pos": 238, "type": "DATASET", "confidence": 0.9125649730364481}]}, {"text": "Their model had a best accuracy of 75% by using stylistic features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9996128678321838}]}, {"text": "However, their model is not directly comparable with ours since the dataset for the shared task is different.", "labels": [], "entities": []}, {"text": "In the following sections, we describe our system for identifying hyperpartisan news articles as part of our participation in the Hyperpartisan New Detection shared task in SemEval 2019 ( ).", "labels": [], "entities": [{"text": "Hyperpartisan New Detection shared task in SemEval 2019", "start_pos": 130, "end_pos": 185, "type": "TASK", "confidence": 0.6427473053336143}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Different feature combinations (W: Words  vector, L: LIWC features, P: Punctuation Marks, E:  Emotions and S: Articles Structure) and their weighted  F1 score on the local validation set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9723110496997833}]}, {"text": " Table 2: Main task classification results of the local  validation and test datasets.", "labels": [], "entities": [{"text": "Main task classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6544943551222483}]}, {"text": " Table 3: Classification results of various models.  Ensemble-RNN model was tested using cross- validation so it is not directly comparable with the  other two models in the validation scores.", "labels": [], "entities": []}]}