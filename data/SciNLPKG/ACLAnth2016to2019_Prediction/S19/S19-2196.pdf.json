{"title": [{"text": "SINAI-DL at SemEval-2019 Task 7: Data Augmentation and Temporal Expressions", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7886844277381897}]}], "abstractContent": [{"text": "This paper describes the participation of the SINAI-DL team at RumourEval (Task 7 in Se-mEval 2019, subtask A: SDQC).", "labels": [], "entities": [{"text": "RumourEval", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.6015825271606445}]}, {"text": "SDQC addresses the challenge of rumour stance classification as an indirect way of identifying potential rumours.", "labels": [], "entities": [{"text": "SDQC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8583310842514038}, {"text": "rumour stance classification", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.9327313899993896}, {"text": "identifying potential rumours", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.8455156882603964}]}, {"text": "Given a tweet with several replies , our system classifies each reply into either supporting, denying, questioning or commenting on the underlying rumours.", "labels": [], "entities": []}, {"text": "We have applied data augmentation, temporal expressions labeling and transfer learning with a four-layer neural classifier.", "labels": [], "entities": [{"text": "temporal expressions labeling", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6266010800997416}]}, {"text": "We achieve an accuracy of 0.715 with the official run over reply tweets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9993780851364136}]}], "introductionContent": [{"text": "Fake news has been identified as \"news stories that have no factual basis but are presented as news\".", "labels": [], "entities": [{"text": "Fake news", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8730548620223999}]}, {"text": "On a conceptual level, we can define hoaxes or rumours as false information spread across social media with the intention to be picked up by traditional news websites (.", "labels": [], "entities": []}, {"text": "Rumours have been around for millennia, as attested by the ancient (and modern) Greek word 'pheme', which means rumour or inaccurate information.", "labels": [], "entities": []}, {"text": "We have participated in RumourEval (SemEval 2019 Task 7, Subtask A), named SDQC: Determining support for rumours, with the complementary objective of tracking how other sources orient to the accuracy of the rumourous story by looking at the replies to the tweet that presented the rumourous statement (.", "labels": [], "entities": []}, {"text": "These replies are extracted from Twitter and Reddit, but we have only processed the tweet replies, obtaining a good score in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9982023239135742}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 is a brief overview of the task.", "labels": [], "entities": []}, {"text": "Data analysis is shown in section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes the neural network architecture.", "labels": [], "entities": []}, {"text": "The experiments and results are analyzed in section 5.", "labels": [], "entities": []}, {"text": "Finally, conclusions and proposals for further experimentation are provided in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed an evaluation of the proposed neural network on the development set, but training a model on two different official training sets: the official ones and those augmented by paraphrasing the given tweets.", "labels": [], "entities": []}, {"text": "The results were discouraging when paraphrased tweets are added to the training set, as shows.", "labels": [], "entities": []}, {"text": "After checking the tweets generated by the paraphrasing tool, we noticed that the quality was low, with non-sense texts in some cases and few structural variations from the original tweet.", "labels": [], "entities": []}, {"text": "Thus, we believe that the network was not even less robust, but worse as a nonrealistic model was learned.", "labels": [], "entities": []}, {"text": "The detection of temporal expressions and the inclusion of the generated tags into the model didn't report any improvement either.", "labels": [], "entities": []}, {"text": "We believe that the related embeddings (randomly initialized) needed afar larger set to fit in the transferred learned embedding model for GloVe vectors.", "labels": [], "entities": []}, {"text": "Thus, our submission relies only on official training data which was, as we know, not enough data to ensure a good learning process.", "labels": [], "entities": []}, {"text": "Anyhow, our system performed in 9th position out from 21 in train data accuracy on dev data official 0.690499 official + paraphrased 0.675808 official with temporal tags 0.684622: Development experiments subtask A, with an SDQC value of 0.3927 (F1-score).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9814833998680115}, {"text": "dev data official 0.690499 official", "start_pos": 83, "end_pos": 118, "type": "DATASET", "confidence": 0.8365814328193665}, {"text": "F1-score", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9993409514427185}]}, {"text": "shows the results obtained with the official run over test set (only with the tweet replies).", "labels": [], "entities": []}, {"text": "In the first analysis of results we can verify that the neural network system, on the base case, has worked correctly (almost perfect) for the comment class that have a sufficient number of examples of train and dev, and much worse for those with very few examples (classes support, deny and query).", "labels": [], "entities": []}, {"text": "We have to finish a more exhaustive analysis of these results, especially of the mislabelled samples.", "labels": [], "entities": []}, {"text": "For instance, in the analysis of the truth label support, our system tags the most of the cases with the comment label.", "labels": [], "entities": []}, {"text": "In this case, we can conclude that the comment label has been overtrained because of the greater number of examples (high bias).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets distribution (only tweets).", "labels": [], "entities": []}, {"text": " Table 2: Tweets replies distribution.", "labels": [], "entities": []}, {"text": " Table 3: Length of tweet replies covering 80 % and  90 % of cases.", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9985517859458923}]}, {"text": " Table 6: Test experiments: official run", "labels": [], "entities": []}]}