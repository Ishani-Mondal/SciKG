{"title": [{"text": "UBC-NLP at SemEval-2019 Task 6: Ensemble Learning of Offensive Content With Enhanced Training Data", "labels": [], "entities": [{"text": "UBC-NLP at SemEval-2019 Task 6", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.7073674976825715}]}], "abstractContent": [{"text": "We examine learning offensive content on Twitter with limited, imbalanced data.", "labels": [], "entities": []}, {"text": "For the purpose, we investigate the utility of using various data enhancement methods with a host of classical ensemble classifiers.", "labels": [], "entities": []}, {"text": "Among the 75 participating teams in SemEval-2019 sub-task B, our system ranks 6th (with 0.706 macro F 1-score).", "labels": [], "entities": [{"text": "macro F 1-score", "start_pos": 94, "end_pos": 109, "type": "METRIC", "confidence": 0.6600707570711771}]}, {"text": "For sub-task C, among the 65 participating teams, our system ranks 9th (with 0.587 macro F 1-score).", "labels": [], "entities": [{"text": "macro F 1-score", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.6867352922757467}]}], "introductionContent": [{"text": "With the proliferation of social media, millions of people currently express their opinions freely online.", "labels": [], "entities": []}, {"text": "Unfortunately, this is not without costs as some users fail to maintain the thin line between freedom of expression and hate speech, defamation, ad hominem attacks, etc.", "labels": [], "entities": []}, {"text": "Manually detecting these types of negative content is not feasible, due to the sheer volume of online communication.", "labels": [], "entities": []}, {"text": "In addition, individuals tasked with inspecting such types of content may suffer from depression and burnout.", "labels": [], "entities": []}, {"text": "For these reasons, it is desirable to build machine learning systems that can flag offensive online content.", "labels": [], "entities": []}, {"text": "Several works have investigated detecting undesirable () and offensive language online using traditional machine learning methods.", "labels": [], "entities": []}, {"text": "For example, employ statistical topic modelling and feature engineering to detect offensive tweets.", "labels": [], "entities": [{"text": "statistical topic modelling", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6533632576465607}]}, {"text": "Similarly, train multiple classifiers (e.g., logistic regression, decision trees, and support vector machines) to detect hate speech from general offensive tweets.", "labels": [], "entities": []}, {"text": "More recently, deep artificial neural networks (i.e., deep learning) has been used for several text classification tasks, including detecting offensive and hateful language.", "labels": [], "entities": [{"text": "text classification", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7902219593524933}, {"text": "detecting offensive and hateful language", "start_pos": 132, "end_pos": 172, "type": "TASK", "confidence": 0.8192035317420959}]}, {"text": "For example, use recurrent neural networks (RNN) to detect offensive language in tweets.", "labels": [], "entities": []}, {"text": "use transfer learning with convolutional neural networks (CNN) for offensive tweet classification on Twitter data.", "labels": [], "entities": [{"text": "offensive tweet classification", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6280615230401357}]}, {"text": "Most of these works, however, either assume relatively balanced data (traditional classifiers) and/or large amounts of labeled data (deep learning).", "labels": [], "entities": []}, {"text": "In scenarios where only highly imbalanced data are available, it becomes challenging to learn good generalizations.", "labels": [], "entities": []}, {"text": "In these cases, it is useful to employ methods with good predictive power for especially minority classes.", "labels": [], "entities": []}, {"text": "For example, methods capable of enhancing training data (e.g., by augmenting minority categories) are desirable in such scenarios.", "labels": [], "entities": []}, {"text": "In the literature, some works have been undertaken to address issues of data imbalance in language tasks.", "labels": [], "entities": []}, {"text": "For example, propose different undersampling techniques that yield better performance than common random undersampling on sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.92200767993927}]}, {"text": "Along similar lines Gopalakrishnan and Ramaswamy (2014) propose a modified ensemble based bagging algorithm and sampling techniques that improve sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.9471806585788727}]}, {"text": "Further, present a novel oversampling technique that generates synthetic texts from word spaces.", "labels": [], "entities": []}, {"text": "In addition to data enhancement, combining various classifiers in an ensemble fashion can be useful since different classifiers have different learning biases.", "labels": [], "entities": []}, {"text": "Past research has shown the effectiveness of ensembling classifiers for text classification., for example, study the performance of ensemble models for sentiment analysis of Arabic reviews.", "labels": [], "entities": [{"text": "text classification.", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7236449122428894}, {"text": "sentiment analysis of Arabic reviews", "start_pos": 152, "end_pos": 188, "type": "TASK", "confidence": 0.9211121916770935}]}, {"text": "Da exploit ensembles to boost the accuracy on twitter sentiment analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9991421699523926}, {"text": "twitter sentiment analysis", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6825781563917795}]}, {"text": "demonstrate the utility of combining sampling techniques with ensemble models for solving the data imbalance problem.", "labels": [], "entities": []}, {"text": "In this paper, we describe our submissions to SemEval-2019 task 6 (OffenseEval) ().", "labels": [], "entities": [{"text": "SemEval-2019 task", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.866206020116806}]}, {"text": "We focus on sub-tasks B and C.", "labels": [], "entities": []}, {"text": "The Offensive Language Identification Dataset (), the data released by the organizers for each of these sub-tasks, is extremely imbalanced (see Section 2).", "labels": [], "entities": [{"text": "Offensive Language Identification", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.6089555223782858}]}, {"text": "We propose effective methods for developing models exploiting the data.", "labels": [], "entities": []}, {"text": "Our main contributions are: (1) we experiment with a number of simple data augmentation methods to alleviate class imbalance, and we apply a number of classical machine learning methods in the context of ensembling to develop highly successful models for each of the competition sub-tasks.", "labels": [], "entities": []}, {"text": "Our work shows the utility of the proposed methods for detecting offensive language in absence of budget for performing feature engineering and/or small, imbalanced data.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: We describe the datasets in Section 2.", "labels": [], "entities": []}, {"text": "We introduce our methods in Section 3.", "labels": [], "entities": []}, {"text": "Next, we detail our models for each sub-task (Sections 4 and 5).", "labels": [], "entities": []}, {"text": "We then offer an analysis of the performance of our models in Section 6, and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Examples of each class in sub-tasks B and C", "labels": [], "entities": []}, {"text": " Table 2: Distribution of classes over our data splits", "labels": [], "entities": []}, {"text": " Table 3: Sub-Task B: XGBoost performance with  sampling methods. Baseline 1 is our majority class in  training data. Baseline 2 is a logistic regression model  with no data sampling.", "labels": [], "entities": []}, {"text": " Table 4: Sub-Task B: Best ensemble model results. We reproduce XGBoost results from Table 3 for comparison.", "labels": [], "entities": []}, {"text": " Table 5: Sub-Task C: Best results with various sam- pling methods.", "labels": [], "entities": []}, {"text": " Table 6: Sub-Task C: Results of our 3 final submitted models", "labels": [], "entities": []}]}