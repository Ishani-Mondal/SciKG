{"title": [], "abstractContent": [{"text": "This paper describes our submission to SemEval-2019 Task 7: RumourEval: Determining Rumor Veracity and Support for Rumors.", "labels": [], "entities": [{"text": "SemEval-2019 Task 7", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7934754888216654}, {"text": "Determining Rumor Veracity", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.7003066341082255}]}, {"text": "We participated in both subtasks.", "labels": [], "entities": []}, {"text": "The goal of subtask A is to classify the type of interaction between a rumorous social media post and a reply post as support, query, deny, or comment.", "labels": [], "entities": [{"text": "classify the type of interaction between a rumorous social media post and a reply post as support, query, deny, or comment", "start_pos": 28, "end_pos": 150, "type": "Description", "confidence": 0.66218884040912}]}, {"text": "The goal of subtask B is to predict the veracity of a given rumor.", "labels": [], "entities": [{"text": "predict the veracity of a given rumor", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.7177829657282148}]}, {"text": "For subtask A, we implement a CNN-based neural architecture using ELMo embeddings of post text combined with auxiliary features and achieve a F 1-score of 44.6%.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9895934164524078}]}, {"text": "For subtask B, we employ a MLP neural network leveraging our estimates for subtask A and achieve a F 1-score of 30.1% (second place in the competition).", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9850950837135315}]}, {"text": "We provide results and analysis of our system performance and present ablation experiments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online social media has changed the way of communicating and disseminating media content and opinions, but also paved the way for spreading false or unverified rumors.) provides a dataset of labelled threads from Twitter and Reddit where each source post mentions a rumor.", "labels": [], "entities": []}, {"text": "Subtask A (SDQC) consists of deciding for each post in a thread whether it is in a support, deny, query, or comment relation to the rumor.", "labels": [], "entities": [{"text": "Subtask A (SDQC) consists of deciding for each post in a thread whether it is in a support, deny, query, or comment relation to the rumor", "start_pos": 0, "end_pos": 137, "type": "Description", "confidence": 0.7170007036578271}]}, {"text": "The goal of subtask B (Verification) it to classify the veracity of the rumor as true, false, or unverified.", "labels": [], "entities": []}, {"text": "Automated rumor classification is a challenging task as there is no definite evidence (e.g., authorized confirmation).", "labels": [], "entities": [{"text": "Automated rumor classification", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6798487901687622}]}, {"text": "In its absence, stance analysis is a useful approach.", "labels": [], "entities": [{"text": "stance analysis", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.9843242168426514}]}, {"text": "Systems that employ neural network architectures showed promising results in, with * The first two authors contributed equally.: An example Twitter thread from the training dataset with SDQC labels for each post and a veracity label for the thread's source post.", "labels": [], "entities": []}, {"text": "Any post that does not reply to another is a source post.", "labels": [], "entities": []}, {"text": "Reply posts can be direct replies (replies to a source post) or nested replies (replies that reply to another reply post).", "labels": [], "entities": []}, {"text": "A thread is the set containing a source post and all its reply posts.", "labels": [], "entities": []}, {"text": "the LSTM-based sequential model of In this paper, we describe our approach CLEARumor (ConvoLving ELMo Against Rumors) for solving both subtasks and provide empirical results and ablation experiments of our architecture.", "labels": [], "entities": [{"text": "CLEARumor", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9132047891616821}]}, {"text": "We make our PyTorch-based implementation and trained models publicly available 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "The reported results differ from our official submission, because we continued to tune hyperparameters afterwards.", "labels": [], "entities": []}, {"text": "We report results as trained on the training dataset and then evaluated on the development and test datasets, as provided by the RumourEval organizers.", "labels": [], "entities": [{"text": "RumourEval organizers", "start_pos": 129, "end_pos": 150, "type": "DATASET", "confidence": 0.868704617023468}]}, {"text": "Because neural network experiments are naturally nondeterministic and we did indeed notice huge variances when retraining models, we report the mean and standard deviation over 10 runs for each experiment.", "labels": [], "entities": []}, {"text": "Additionally, we report scores from a 10-fold cross validation over the whole dataset.", "labels": [], "entities": []}, {"text": "Simple cross-validation would be inappropriate in our setting, because for example a split could result in the case where the same rumors occur in both the training and the test dataset which would allow a model to just memorize which posts are rumorous.", "labels": [], "entities": []}, {"text": "We ensure that this does not happen in our case, by keeping all posts belonging to the same rumor 6 in the same cross For Twitter posts, the dataset contains rumor-topic labels validation fold.", "labels": [], "entities": []}, {"text": "Note that scores on the organizer split and the cross validation are not directly comparable as different fractions of the whole dataset are used for training (~60-70% for the organizer split and ~90% for 10-fold cross validation).", "labels": [], "entities": [{"text": "cross validation", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.6340508162975311}]}], "tableCaptions": [{"text": " Table 1: Number of labelled instances for both sub- tasks of the RumourEval 2019 dataset broken down  into (1) class frequencies, per (2) social media plat- form, and (3) training, development, and test dataset.", "labels": [], "entities": [{"text": "RumourEval 2019 dataset", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.8419649600982666}]}, {"text": " Table 2: Evaluation results for subtask A. All reported scores are multiplied by 100. We provide the macro- averaged F 1 -score for the development (Dev), the test (Test) datasets and for 10-fold cross validation (CV). For  the test dataset, we further provide the individual F 1 -scores per class. \"Always Comment\" is a baseline predict- ing always the most common class. \"Submitted\" are the results we officially submitted to RumourEval 2019. For  our CLEARumor architecture we provide multiple ablation experiments. CLEAR aux  CNN+MLP is our full system,  CLEAR CNN+MLP the same but without the auxiliary features, CLEAR aux  MLP instead uses no convolutional layers,  and CLEAR aux just concatenates averages ELMo embeddings with auxiliary features uses a single linear layer.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9169241487979889}]}, {"text": " Table 3: Evaluation results for subtask B. We report F 1 (multiplied by 100) and RMSE (root mean squared error)  scores for the development (Dev), the test (Test) datasets and for 10-fold cross validation (CV). CLEAR Subtask-B  is our subtask B architecture using the subtask A estimates from CLEAR aux  CNN+MLP . CLEAR NileTMRG uses the  same estimates but computes task B results using the NileTMRG system", "labels": [], "entities": [{"text": "F 1", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9879896938800812}, {"text": "RMSE (root mean squared error)", "start_pos": 82, "end_pos": 112, "type": "METRIC", "confidence": 0.921494151864733}, {"text": "CLEAR NileTMRG", "start_pos": 315, "end_pos": 329, "type": "DATASET", "confidence": 0.6768986880779266}, {"text": "NileTMRG", "start_pos": 393, "end_pos": 401, "type": "DATASET", "confidence": 0.9258744716644287}]}]}