{"title": [{"text": "AUTOHOME-ORCA at SemEval-2019 Task 8: Application of BERT for Fact-Checking in Community Forums", "labels": [], "entities": [{"text": "AUTOHOME-ORCA", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8075584769248962}, {"text": "BERT", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9982699155807495}]}], "abstractContent": [{"text": "Fact checking is an important task for maintaining high quality posts and improving user experience in Community Question Answering forums.", "labels": [], "entities": [{"text": "Fact checking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7668159902095795}, {"text": "Community Question Answering forums", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.6827632114291191}]}, {"text": "Therefore, the SemEval-2019 task 8 is aimed to identify factual question (subtask A) and detect true factual information from corresponding answers (subtask B).", "labels": [], "entities": [{"text": "SemEval-2019 task 8", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8538516362508138}]}, {"text": "In order to address this task, we propose a system based on the BERT model with meta information of questions.", "labels": [], "entities": [{"text": "BERT", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9534024596214294}]}, {"text": "For the subtask A, the outputs of fine-tuned BERT classification model are combined with the feature of length of questions to boost the performance.", "labels": [], "entities": [{"text": "BERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9301883578300476}]}, {"text": "For the subtask B, the predictions of several variants of BERT model encoding the meta information are combined to create an ensemble model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9455316066741943}]}, {"text": "Our system achieved competitive results with an accuracy of 0.82 in the subtask A and 0.83 in the subtask B.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9995197057723999}]}, {"text": "The experimental results validate the effectiveness of our system.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Community Question Answering (CQA) forums are gaining more and more popularity because they can offer great opportunity for users to get appropriate answers to their questions from other users.", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7481898218393326}]}, {"text": "Meanwhile, the accumulated massive questions and answers in CQA forums present anew challenge to provide valuable information for users more effectively.", "labels": [], "entities": [{"text": "CQA forums", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.9003166258335114}]}, {"text": "Therefore, researchers have shown an increased interest in CQA systems (, aiming to facilitate efficient knowledge acquisition and circulation.", "labels": [], "entities": [{"text": "knowledge acquisition and circulation", "start_pos": 105, "end_pos": 142, "type": "TASK", "confidence": 0.6799150034785271}]}, {"text": "Specifically, a large portion of researches mainly focus on the two tasks: find relevant questions to anew question to reuse corresponding answers (Question Retrieval), and search for relevant answers among existing answers to other questions (Answer Selection).", "labels": [], "entities": [{"text": "Answer Selection)", "start_pos": 244, "end_pos": 261, "type": "TASK", "confidence": 0.7435872654120127}]}, {"text": "Despite a great deal of research on CQA, there are relatively few studies focusing on the quality of questions and answers.", "labels": [], "entities": [{"text": "CQA", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.7795575857162476}]}, {"text": "Actually, the credibility of answers is an important aspect, which can directly affect the user experience for CQA forums.", "labels": [], "entities": []}, {"text": "In order to check the veracity of answers automatically, some recent works () attempt to utilize external sources and extract appropriate features for classification.", "labels": [], "entities": []}, {"text": "Considering the importance of information veracity in CQA forums, the fact checking of answers is still an issue that is worth investigating further.", "labels": [], "entities": [{"text": "fact checking of answers", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.8750800788402557}]}, {"text": "Therefore, the SemEval-2019 task 8 aims to conduct fact checking in CQA forums.", "labels": [], "entities": [{"text": "SemEval-2019 task 8", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8445634047190348}, {"text": "fact checking in CQA forums", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6888707995414733}]}, {"text": "In order to detect the veracity of answers, it is necessary to identify whether the questions are factual firstly.", "labels": [], "entities": []}, {"text": "The task is comprised of two subtasks: the subtask A is targeted to identify whether a question is asking for factual information, an opinion/advice or socializing.", "labels": [], "entities": []}, {"text": "Given factual questions, the subtask B is aimed to determine whether the corresponding answers are true, false or not factual.", "labels": [], "entities": []}, {"text": "In order to address the SemEval-2019 task 8, we propose a system based on the BERT model).", "labels": [], "entities": [{"text": "SemEval-2019 task 8", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7751118143399557}, {"text": "BERT", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9701669812202454}]}, {"text": "In our system, we extend BERT for integrating some meta information of questions into the BERT encoder, and generate an ensemble model from some potential classification models to achieve very competitive results.", "labels": [], "entities": [{"text": "BERT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9903900623321533}, {"text": "BERT", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.7777622938156128}]}, {"text": "To be specific, in subtask A, two outputs of fine-tuned BERT classifiers are obtained from subjects and bodies of questions respectively.", "labels": [], "entities": [{"text": "BERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9789091348648071}]}, {"text": "Then by combining both outputs with the length of questions as features, the AdaBoost method) is utilized to boost the performance of question classification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.7377936244010925}]}, {"text": "As for subtask B, while encoding additional meta information (category and subject of questions) into BERT model, we adopt the bagging method for some variants of BERT model produced by adding additional layers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.784144401550293}]}, {"text": "The experimental results in both subtasks demonstrate the effectiveness of our system.", "labels": [], "entities": []}, {"text": "The rest of our paper is organized in the following way.", "labels": [], "entities": []}, {"text": "The related work about CQA is summarized in Section 2.", "labels": [], "entities": [{"text": "CQA", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8353509902954102}]}, {"text": "Section 3 gives a more detailed description of our system.", "labels": [], "entities": []}, {"text": "The results and analysis of experiments are demonstrated in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 presents the main conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset is organized in question-answer threads from the Qatar Living forum.", "labels": [], "entities": [{"text": "Qatar Living forum", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.9750872850418091}]}, {"text": "Each question, which is annotated by labels: Opinion, Factual and Socializing, has a subject, a body and meta information including question ID, category, posting time, user's ID and name.", "labels": [], "entities": []}, {"text": "And each answer, which is classified as Factual-True, FactualFalse and Non-Factual, has a body and meta information (answer ID, posting time, user's ID and name).", "labels": [], "entities": []}, {"text": "The detailed statistics of the dataset in this task are illustrated in the task description paper (Mihaylova et al., 2019).", "labels": [], "entities": []}, {"text": "As for pre-training the BERT model, it is trained based on the BERT-Base-Cased model by the forum corpus provided by organizer 6 . The training batch size is 32, the number of train steps is 1e+5 and the learning rate is 2e-5.", "labels": [], "entities": [{"text": "BERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.7139865159988403}, {"text": "BERT-Base-Cased", "start_pos": 63, "end_pos": 78, "type": "METRIC", "confidence": 0.9375916123390198}]}, {"text": "The detailed experimental results for both subtasks are described as following.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of different models in the  subtask A.", "labels": [], "entities": []}, {"text": " Table 3: Performance of different models in the  subtask B.", "labels": [], "entities": []}]}