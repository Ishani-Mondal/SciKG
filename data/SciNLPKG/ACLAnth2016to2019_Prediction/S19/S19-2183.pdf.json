{"title": [{"text": "Team Xenophilius Lovegood at SemEval-2019 Task 4: Hyperpartisanship Classification using Convolutional Neural Networks", "labels": [], "entities": [{"text": "SemEval-2019 Task", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7653194963932037}, {"text": "Hyperpartisanship Classification", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.8184995055198669}]}], "abstractContent": [{"text": "This paper describes our system for the Sem-Eval 2019 Task 4 on hyperpartisan news detection.", "labels": [], "entities": [{"text": "Sem-Eval 2019 Task 4", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.8489160388708115}, {"text": "hyperpartisan news detection", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6324065128962199}]}, {"text": "We build on an existing deep learning approach for sentence classification based on a Convolutional Neural Network.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7336564362049103}]}, {"text": "Modifying the original model with additional layers to increase its expressiveness and finally building an ensemble of multiple versions of the model, we obtain an accuracy of 67.52 % and an F1 score of 73.78 % on the main test dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9996998310089111}, {"text": "F1 score", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9885454773902893}]}, {"text": "We also report on additional experiments incorporating handcrafted features into the CNN and using it as a feature extractor fora linear SVM.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of is to determine whether a news article blindly follows apolitical argumentation or not, which is referred to as \"hyperpartisan news\" (.", "labels": [], "entities": []}, {"text": "Instead of predicting the exact political orientation, it focuses on whether an article is hyperpartisan in anyway.", "labels": [], "entities": []}, {"text": "This is a very topical issue since news are easily able to reach millions of people over the internet, and in recent years have been excessively used to influence the population, for example regarding elections.", "labels": [], "entities": []}, {"text": "Specifically, one sided media coverage influences a lot of readers without their knowledge, demonstrating the necessity of automated detection of hyperpartisan news.", "labels": [], "entities": []}, {"text": "Approach In this work, we make use of deep learning models to address this task.", "labels": [], "entities": []}, {"text": "We decided to adapt the sentence CNN proposed by, as it has been shown to be a strong baseline for text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.8562891880671183}]}, {"text": "Since the model was originally designed for the classification of sentences, we had to make some modifications in order to deal with the longer texts provided in this task.", "labels": [], "entities": [{"text": "classification of sentences", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.8931928078333536}]}, {"text": "While the very shallow model originally proposed by is enough to adequately represent sentences, we found that it is not expressive enough to model entire news articles.", "labels": [], "entities": []}, {"text": "Thus, we added a second convolutional layer and a batch normalization layer to the model.", "labels": [], "entities": []}, {"text": "Additionally, we specified a maximum length for articles, after which they are cutoff.", "labels": [], "entities": []}, {"text": "These modifications will be described in more detail in Section 3.", "labels": [], "entities": []}, {"text": "We also experiment with including some hand-crafted features into the model in an attempt to improve the performance.", "labels": [], "entities": []}, {"text": "Finally, we build an ensemble of multiple models to obtain our final results.", "labels": [], "entities": []}], "datasetContent": [{"text": "After defining our models, we now shortly describe the datasets (for a more detailed description, see) before presenting the results of training and evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Best hyperparameters obtained via random search. Article CNN HC has two dense layers, hence two  numbers for dropout and units. We evaluated multiple filter sizes for Article CNN only, due to time constraints.", "labels": [], "entities": [{"text": "Article CNN HC", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.9191446304321289}, {"text": "Article CNN", "start_pos": 177, "end_pos": 188, "type": "DATASET", "confidence": 0.8279609084129333}]}, {"text": " Table 2: Best results achieved on the validation dataset.", "labels": [], "entities": [{"text": "validation dataset", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.892829954624176}]}, {"text": " Table 3. As on the  validation dataset, the ensembles outperform the", "labels": [], "entities": []}, {"text": " Table 3: Results on the hidden test datasets.", "labels": [], "entities": []}]}