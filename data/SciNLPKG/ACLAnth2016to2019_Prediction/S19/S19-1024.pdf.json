{"title": [{"text": "Automatic Accuracy Prediction for AMR Parsing", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8800243139266968}, {"text": "AMR Parsing", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9433053135871887}]}], "abstractContent": [{"text": "Meaning Representation (AMR) represents sentences as directed, acyclic and rooted graphs, aiming at capturing their meaning in a machine readable format.", "labels": [], "entities": [{"text": "Meaning Representation (AMR)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8211230516433716}]}, {"text": "AMR parsing converts natural language sentences into such graphs.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.807394802570343}]}, {"text": "However, evaluating a parser on new data by means of comparison to manually created AMR graphs is very costly.", "labels": [], "entities": []}, {"text": "Also, we would like to be able to detect parses of questionable quality, or preferring results of alternative systems by selecting the ones for which we can assess good quality.", "labels": [], "entities": []}, {"text": "We propose AMR accuracy prediction as the task of predicting several metrics of correctness for an automatically generated AMR parse-in absence of the corresponding gold parse.", "labels": [], "entities": [{"text": "AMR accuracy prediction", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.9158026774724325}]}, {"text": "We develop a neural end-to-end multi-output regression model and perform three case studies: firstly, we evaluate the model's capacity of predicting AMR parse accuracies and test whether it can reliably assign high scores to gold parses.", "labels": [], "entities": [{"text": "predicting AMR parse accuracies", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.8185903280973434}]}, {"text": "Secondly , we perform parse selection based on predicted parse accuracies of candidate parses from alternative systems, with the aim of improving overall results.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.9827812314033508}]}, {"text": "Finally, we predict system ranks for submissions from two AMR shared tasks on the basis of their predicted parse accuracy averages.", "labels": [], "entities": [{"text": "AMR shared tasks", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.6022053360939026}, {"text": "parse accuracy averages", "start_pos": 107, "end_pos": 130, "type": "METRIC", "confidence": 0.6715890963872274}]}, {"text": "All experiments are carried out across two different domains and show that our method is effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () represents the semantic structure of a sentence, including concepts, semantic operators and relations, sense-disambiguated predicates and their arguments.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8175961722930273}]}, {"text": "As a machine readable representation of the meaning of a sentence, AMR is potentially useful for many NLP tasks.", "labels": [], "entities": []}, {"text": "Among other applications it has been used in machine translation (), text (a / asbestos :polarity -:time (n / now) :location (t / thing :ARG1-of (p / produce-01 :ARG0 (w / we)))) Figure 1: Humanly produced AMR for: There is no asbestos in our products now.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.795511394739151}, {"text": "ARG1-of", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.8239952921867371}]}, {"text": "Numbered predicates refer to PropBank senses ().", "labels": [], "entities": [{"text": "PropBank senses", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9183924496173859}]}, {"text": "summarization () and question answering.", "labels": [], "entities": [{"text": "summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6839376091957092}, {"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8710957467556}]}, {"text": "Since the introduction of AMR, many approaches to AMR parsing have been proposed: graph-based pipeline systems which rely on an alignment step ( or transition-based parsers relying on dependency annotation ().", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9307777285575867}]}, {"text": "In the following we will denote the former by JAMR and the latter by CAMR.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.5113832354545593}, {"text": "CAMR", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8522503972053528}]}, {"text": "More recently, endto-end neural systems have been proposed which produce linearized AMR graphs within characterbased) or word-based () encoding models.", "labels": [], "entities": []}, {"text": "Both approaches greatly profit from large amounts of silver training data.", "labels": [], "entities": []}, {"text": "The silver data is obtained with self-training ( or the aid of additional parsers, where only parses with considerable agreement are chosen to extend the training data.", "labels": [], "entities": []}, {"text": "formulate a neural model that jointly predicts alignments, concepts and relations.", "labels": [], "entities": []}, {"text": "Their system -henceforth called GPLA (Graph Prediction with Latent Alignments) -defines the current state-of-the-art in AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.9469059109687805}]}, {"text": "A system that can perform accuracy prediction for AMR parsing can be used in a variety of ways: (i) estimating the quality of downstream tasks that deploy AMR parses.", "labels": [], "entities": [{"text": "accuracy prediction", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.9034313857555389}, {"text": "AMR parsing", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9555587470531464}, {"text": "AMR parses", "start_pos": 155, "end_pos": 165, "type": "TASK", "confidence": 0.8004575371742249}]}, {"text": "E.g., in a document summarization scenario, we might expect lower qual-ity of a summary if the estimated quality of AMR parses used as a basis for the summary is low; (ii) AMR parsing accuracy estimation can be used to produce high-quality automatically parsed data: by filtering the outputs of single parsing systems in self-training, by selecting high-quality outputs from different parsing systems in a tri-parsing setting, or else by predicting overall rankings over alternative parsing systems applied to in-or outof-domain data; (iii) finally, AMR parse accuracy prediction could be used in the context of a parsersupported treebank construction process.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.6492517292499542}, {"text": "AMR parsing accuracy estimation", "start_pos": 172, "end_pos": 203, "type": "TASK", "confidence": 0.6993255913257599}, {"text": "AMR parse accuracy prediction", "start_pos": 550, "end_pos": 579, "type": "TASK", "confidence": 0.7782919853925705}]}, {"text": "E.g., in an active learning scenario, we can select useful targets for manual annotation based on their expected efficiency for parser improvement -the fine-grained evaluation measures predicted by our system can be used for targeted improvements.", "labels": [], "entities": []}, {"text": "In the simplest case, we can provide the human annotator with automatic parses where only few flaws have to be mended.", "labels": [], "entities": []}, {"text": "Hence, AMR accuracy prediction systems have the potential to tremendously reduce manual annotation cost and time.", "labels": [], "entities": [{"text": "AMR accuracy prediction", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.8880897363026937}]}, {"text": "Contributions We define AMR accuracy prediction as the task of predicting a rich suite of metrics to assess various subtasks covered by AMR parsing (e.g. negation detection or semantic role labeling).", "labels": [], "entities": [{"text": "AMR accuracy prediction", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.8549536069234213}, {"text": "AMR parsing", "start_pos": 136, "end_pos": 147, "type": "TASK", "confidence": 0.928570568561554}, {"text": "negation detection", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.9529218375682831}, {"text": "semantic role labeling", "start_pos": 176, "end_pos": 198, "type": "TASK", "confidence": 0.5829601486523946}]}, {"text": "To approach this task, we use the AMR evaluation suite suggested by and develop a hierarchical multi-output regression model for automatically performing evaluation of 12 different tasks involved in AMR parsing (Sections \u00a73 and \u00a74; our code is publicly accessible 1 ).", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 199, "end_pos": 210, "type": "TASK", "confidence": 0.8983592689037323}]}, {"text": "We perform experiments in three different scenarios on unseen in-domain and out-of-domain data and show that our model (i) is able to predict scores with significant correlation to gold scores and (ii) can be used to rank parses on a sentencelevel or to rank parsers on a corpus-level ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Since our goal is to predict the accuracy of an automatic parse, we need a data set containing automatically produced AMR parses and their scores, as they would emerge from comparison to gold parses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9985352754592896}, {"text": "AMR parses", "start_pos": 123, "end_pos": 133, "type": "TASK", "confidence": 0.5820009410381317}]}, {"text": "Our largest data set, LDC2015E86, comprises 19,572 sentences and comes in a predefined training, development and test split.", "labels": [], "entities": [{"text": "LDC2015E86", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.8514261245727539}]}, {"text": "We parse this data set with three parsers, JAMR), CAMR (Wang et al., 2015b,a, 2016a) and GPLA (.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8094262480735779}, {"text": "CAMR", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.5416245460510254}, {"text": "GPLA", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.7628077864646912}]}, {"text": "Since the three parsers have been trained on the training data partition, we naturally obtain more accurate parses for the training partition than for development and test data., however, indicates that we still obtain a considerable amount of deficient parses for training.", "labels": [], "entities": [{"text": "training data partition", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.7613304654757181}]}, {"text": "Based on the parser outputs we compute evaluations comparing the automatic parses with the gold parses by using amrevaluation-tool-enhanced 5 , a bug-fixed version of the script that computes the metrics of.", "labels": [], "entities": []}, {"text": "This allows us to create full-fledged training, development and test instances for our accuracy prediction task.", "labels": [], "entities": [{"text": "accuracy prediction task", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.8952025373776754}]}, {"text": "Each instance consists of a sentence and an AMR parse as input and a vector of metric scores as target.", "labels": [], "entities": [{"text": "AMR parse", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.5162948668003082}]}, {"text": "Our second data set, LDC2015R36, comprises submissions to the.", "labels": [], "entities": [{"text": "LDC2015R36", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8603008985519409}]}, {"text": "We have 1053 parses from each of the 11 team submissions (and 2 baseline systems).", "labels": [], "entities": []}, {"text": "third dataset, BioAMRTest is used as the test set in the SemEval-2017 Task 9 (May and Priyadarshi, 2017) and consists of 500 parses from each of the 6 teams.", "labels": [], "entities": [{"text": "BioAMRTest", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.7537901997566223}, {"text": "SemEval-2017 Task 9 (May and Priyadarshi, 2017)", "start_pos": 57, "end_pos": 104, "type": "TASK", "confidence": 0.6291721016168594}]}, {"text": "The shared task organizers kindly made this data available for our experiments.", "labels": [], "entities": []}, {"text": "Preprocessing For dependency annotation, we parse all sentences with spacyV2.0 8 . For sequentializing the AMR and dependency graph representations we take intuitions from van Noord and Bos (2017b) & and output tokens by performing a depth-first-search over the graph.", "labels": [], "entities": []}, {"text": "We replace the AMR negation token '-' and strings representing numbers with special tokens.", "labels": [], "entities": []}, {"text": "The vocabularies (tokens, senses and pointers) are computed from our training partition of LDC2015E86 and comprise all tokens with a frequency \u2265 5 (tokens with lesser frequency are replaced by an OOV-token).", "labels": [], "entities": [{"text": "LDC2015E86", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.8454839587211609}]}, {"text": "PropBank senses of predicates are removed and collected in an extra list that is parallel to the tokens in the linearized AMR sequence.", "labels": [], "entities": []}, {"text": "For each linearized AMR and dependency tree we generate a sequence with index pointers to tokens in the original sentence (-1 for tokens which do not explicitly refer to any token in the sentence, e.g. brackets, 'subj' or 'arg0' relations).", "labels": [], "entities": []}, {"text": "Extraction of token-pointers from the dependency graph is trivial.", "labels": [], "entities": []}, {"text": "For every concept in the linearized AMR we execute a search for the corresponding token in the sentence, looking for exact matches with surface tokens and lemmas.", "labels": [], "entities": []}, {"text": "Training For the optimization of the accuracy prediction model we use only the development and training sections of LDC2015E86 and the corresponding automatic parses together with the gold scores.", "labels": [], "entities": [{"text": "accuracy prediction", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.8569513857364655}, {"text": "LDC2015E86", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.9405550360679626}]}, {"text": "Details on the training cycle can be found in the Supplemental Material \u00a7A (the loss is de-   scribed in \u00a74).", "labels": [], "entities": [{"text": "Supplemental Material \u00a7A", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.8363811075687408}]}, {"text": "We use the same single (hierarchical) model for all three evaluation studies, proving its applicability across different scenarios (a nonhierarchical model is only instantiated for the ablation experiments in Section \u00a75.4).", "labels": [], "entities": []}, {"text": "We finally perform ablation experiments to evaluate the impact of individual model components.", "labels": [], "entities": []}, {"text": "We experiment with five different setups.", "labels": [], "entities": []}, {"text": "(i) instead of stacking two Bi-LSTMs, we use only one Bi-LSTM (one-lstm,).", "labels": [], "entities": []}, {"text": "(ii) instead of the dependency tree, we feed the words in the order as they occur in the sentence (no-dep).", "labels": [], "entities": []}, {"text": "(iii) nopointers: we remove the token-pointers from our model.", "labels": [], "entities": []}, {"text": "(iv), instead of using the hierarchical setup, we predict all metrics on the same level (green in, no-HL in) and (v), no-HMTL: we optimize the non-hierarchical model only with respect to Smatch, disregarding the AMR subtasks.", "labels": [], "entities": []}, {"text": "Remarkably, the dependency tree greatly helps the model on in-domain data overall measures (-37 total \u2206 without dependencies) but hurts the model on out-of-domain data (+27 total \u2206).", "labels": [], "entities": []}, {"text": "A possible explanation is the degradation of the dependency parse quality: bio-medical data not only poses a challenge for our model, but also for the dependency parser.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.8097023963928223}, {"text": "dependency parser", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7542131543159485}]}, {"text": "With special regard to the main AMR evaluation measure, Smatch F1, the learned pointer embeddings provide useful input on the indomain test data (-4 \u2206 without pointers).", "labels": [], "entities": [{"text": "AMR evaluation", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7696461379528046}, {"text": "Smatch F1", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.6848400831222534}]}], "tableCaptions": [{"text": " Table 1: Parser output evaluation on training and devel- opment partitions of LDC2015E86. Smatch F1: avg.  over Smatch F1 per sentence, % def.: percentage of  deficient parses (i.e., parses with Smatch F1 < 1).", "labels": [], "entities": [{"text": "LDC2015E86", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9230871200561523}]}, {"text": " Table 2: Statistics of data sets used in this work.", "labels": [], "entities": []}, {"text": " Table 3: Pearson correlation coefficient (\u03c1) over vari- ous metrics and across domains. Explanations of the  metrics and AMR subtasks are in Section  \u00a73 and fn. 3", "labels": [], "entities": [{"text": "Pearson correlation coefficient (\u03c1)", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.9576235810915629}]}, {"text": " Table 3. Over all metrics, in-domain and out-of- domain, we achieve significant correlations with  the gold scores (p < 0.005 for every metric).", "labels": [], "entities": []}, {"text": " Table 4: Various percentiles of Smatch F1 predictions  for gold graphs.", "labels": [], "entities": [{"text": "Smatch F1 predictions", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.6078145404656728}]}, {"text": " Table 5: Results (sentence averages) of different AMR  parsing (bottom part) and ranking (top part) systems on  two test sets. Upper part: results when selecting from  alternative parses: lower-bound (upper-bound): oracle  selecting the worst (best) AMR parse; ours: results  when selecting the best parse according to our models'  accuracy prediction (hierarchical model).", "labels": [], "entities": [{"text": "AMR  parsing", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9546954929828644}, {"text": "accuracy", "start_pos": 333, "end_pos": 341, "type": "METRIC", "confidence": 0.9913296699523926}]}, {"text": " Table 6: Results of different parse-ranking systems  with respect to sentence-level parse rankings. \u00af  \u03c1: av- erage Pearson-r on a sentence level. %pos: ratio of  predicted rankings with positive \u03c1 to gold ranking.", "labels": [], "entities": []}, {"text": " Table 7: True rank r (given corpus-Smatch) and pre- dicted rank\u02c6rrank\u02c6 rank\u02c6r (based on sentence average Smatch com- puted using our model). p 1 : probability of non- correlation. p 2 : probability that a randomly produced  ranking achieves equal or greater \u03c1 (estimated over 10 6  random rankings). For team names, see fn.", "labels": [], "entities": []}, {"text": " Table 8: \u03c1 correlation (F1) differences over different  setups (columns), test sets (out-of-domain, in-domain)  and subtasks (rows). \u00b1x: plus and minus x pp.\u03c1.", "labels": [], "entities": [{"text": "\u03c1 correlation (F1)", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8345996141433716}]}]}