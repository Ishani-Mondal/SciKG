{"title": [{"text": "DAG Semantic Parsing with Attention-based Decoder", "labels": [], "entities": [{"text": "DAG Semantic Parsing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5289649466673533}]}], "abstractContent": [{"text": "We present a simple and accurate model for semantic parsing with UCCA as our submission for SemEval 2019 Task 1.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.8373640775680542}, {"text": "UCCA", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9718405604362488}, {"text": "SemEval 2019 Task 1", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.9163233935832977}]}, {"text": "We propose an encoder-decoder model that maps strings to directed acyclic graphs.", "labels": [], "entities": []}, {"text": "Unlike many transition-based approaches, our approach does not use a state representation, and unlike graph-based parsers, it does not score graphs directly.", "labels": [], "entities": []}, {"text": "Instead, we encode input sentences with a bidirectional-LSTM, and decode with self-attention to build a graph structure.", "labels": [], "entities": []}, {"text": "Results show that our parser is simple and effective for semantic parsing with reentrancy and discon-tinuous structures.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.814505934715271}]}], "introductionContent": [{"text": "Semantic parsing aims to capture structural relationships between input strings and graph representations of sentence meaning, going beyond concerns of surface word order, phrases and relationships.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8237413167953491}]}, {"text": "The focus on meaning rather than surface relations often requires the use of reentrant nodes and discontinuous structures.", "labels": [], "entities": []}, {"text": "Universal Conceptual Cognitive Annotation (UCCA)) is designed to support semantic parsing with mappings between sentences and their corresponding meanings in a framework intended to be applicable across languages.) focuses on semantic parsing of texts into graphs consisting of terminal nodes that represent words, non-terminal nodes that represent internal structure, and labeled edges representing relationships between nodes (e.g. participant, center, linker, adverbial, elaborator), according to the UCCA scheme.", "labels": [], "entities": [{"text": "Universal Conceptual Cognitive Annotation (UCCA))", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7349707611969539}, {"text": "semantic parsing", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7448816299438477}, {"text": "semantic parsing of texts into graphs", "start_pos": 226, "end_pos": 263, "type": "TASK", "confidence": 0.8435759842395782}, {"text": "UCCA scheme", "start_pos": 504, "end_pos": 515, "type": "DATASET", "confidence": 0.9441991448402405}]}, {"text": "Annotated datasets are provided, and participants are evaluated in four settings: English with domain-specific data, English with out-of-domain data, German with domainspecific data, and French with only development and test data, but no training data.", "labels": [], "entities": []}, {"text": "Additionally, there are open and closed tracks, where the use of additional resources is and is not allowed, respectively.", "labels": [], "entities": []}, {"text": "Our entry in the task is limited to the closed track and the first setting, domain-specific English using the Wiki corpus, where the relatively small dataset (4113 sentences for training, 514 for development, and 515 for testing) consists of annotated sentences from English Wikipedia.", "labels": [], "entities": [{"text": "Wiki corpus", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.9376990795135498}]}, {"text": "Our model follows the encoder-decoder architecture commonly used in state-of-the-art neural parsing models).", "labels": [], "entities": []}, {"text": "However, we propose a very simple decoder architecture that relies only on a recursive attention mechanism of the encoded latent representation.", "labels": [], "entities": []}, {"text": "In other words, the decoder does not require state encoding and model-optimal inference whatsoever.", "labels": [], "entities": []}, {"text": "Our novel model achieved a macro-averaged F1-score of 0.753 in labeled primary edges and 0.864 in unlabeled primary edge prediction on the test set.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9800961017608643}]}, {"text": "The results confirm the suitability of our proposed model to the semantic parsing task.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.8427738746007284}]}], "datasetContent": [{"text": "For the encoder, we use a 2-layer, 500 dimensional BiLSTM with 0.2 dropout.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.8226685523986816}]}, {"text": "The word embedding size is 300 with feature embedding size of 20 each (pos tagging, entity type, and case information).", "labels": [], "entities": [{"text": "pos tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.6572219878435135}]}, {"text": "We use Adam optimizer) with \u03b2 2 set to 0.9 as suggested by.", "labels": [], "entities": []}, {"text": "Development set is used for early stopping.", "labels": [], "entities": [{"text": "Development set", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8705098927021027}, {"text": "early stopping", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.9347731471061707}]}, {"text": "Because of the small dataset (4113 training sentences), the model overfits after 4 epochs.", "labels": [], "entities": []}, {"text": "provides the results on the development set and shows the results on the test set.", "labels": [], "entities": []}, {"text": "official shows results of the model we submitted to the competition with a maximum recursion number of 5 and a \u03b2 2 = 0.99.", "labels": [], "entities": []}, {"text": "We obtained higher scores by increasing the recursion limit as in section 3.3 (+ max revur = 7), using current span only  as explained in section 3.4 (+ child pred), changing \u03b2 2 as shown in section 5 (+ \u03b2 2 = 0.9) and fixing minor bugs (+ bug fix) incrementally.", "labels": [], "entities": []}, {"text": "baseline shows the results of the baseline model (TUPA) from.", "labels": [], "entities": []}, {"text": "final shows the results of the model fine-tuned on the development set mentioned in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 score on primary edges evaluated on the  development set", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9753912687301636}]}, {"text": " Table 2: F1 score on primary and remote edges re- ported on the test set", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9768109917640686}]}]}