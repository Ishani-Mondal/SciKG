{"title": [{"text": "Learning Graph Embeddings from WordNet-based Similarity Measures", "labels": [], "entities": [{"text": "Similarity Measures", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.6777038276195526}]}], "abstractContent": [{"text": "We present path2vec, anew approach for learning graph embeddings that relies on structural measures of pairwise node similarities.", "labels": [], "entities": []}, {"text": "The model learns representations for nodes in a dense space that approximate a given user-defined graph distance measure, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account.", "labels": [], "entities": []}, {"text": "Evaluation of the proposed model on semantic similarity and word sense disambiguation tasks, using various WordNet-based similarity measures, show that our approach yields competitive results, outperform-ing strong graph embedding baselines.", "labels": [], "entities": [{"text": "word sense disambiguation tasks", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.7573770731687546}]}, {"text": "The model is computationally efficient, being orders of magnitude faster than the direct computation of graph-based distances.", "labels": [], "entities": []}], "introductionContent": [{"text": "Developing applications making use of large graphs, such as networks of roads, social media users, or word senses, often involves the design of a domain-specific graph node similarity measure sim : V \u00d7 V \u2192 R defined on a set of nodes V of a graph G = (V, E).", "labels": [], "entities": []}, {"text": "For instance, it can represent the shortest distance from home to work, a community of interest in asocial network fora user, or a semantically related sense to a given synset in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 179, "end_pos": 186, "type": "DATASET", "confidence": 0.9667662978172302}]}, {"text": "There exist a wide variety of such measures greatly ranging in their complexity and design from simple deterministic ones, e.g. based on shortest paths in a network) to more complex ones, e.g. based on random walks (.", "labels": [], "entities": []}, {"text": "Naturally, the majority of such measures rely on walks along edges E of the graph, often resulting in effective, but prohibitively inefficient measures requiring complex and computationally expensive graph traversals.", "labels": [], "entities": []}, {"text": "Also, there are measures that in addition take e.g. corpus information into account beyond what is directly given in the graph, see e.g. ().", "labels": [], "entities": []}, {"text": "We propose a solution to this problem by decoupling development and use of graph-based measures.", "labels": [], "entities": []}, {"text": "Namely, once anode similarity measure is defined, we learn vector representations of nodes that enable efficient computation of this measure.", "labels": [], "entities": []}, {"text": "We represent nodes in a graph with dense embeddings that are good in approximating such custom, e.g. applicationspecific, pairwise node similarity measures.", "labels": [], "entities": []}, {"text": "Similarity computations in a vector space are several orders of magnitude faster than computations directly using the graph.", "labels": [], "entities": []}, {"text": "Additionally, graph embeddings can be of importance in privacy-sensitive network datasets, since in this setup, explicitly storing edges is not required anymore.", "labels": [], "entities": []}, {"text": "The main advantage over other graph embeddings is that our model can learn a custom user-defined application or domain specific similarity measure.", "labels": [], "entities": []}, {"text": "We show the effectiveness of the proposed approach intrinsically on a word similarity task, by learning synset vectors of the WordNet graph based on several similarity measures.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8046613335609436}, {"text": "WordNet graph", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.9464424252510071}]}, {"text": "Our model is not only able to closely approximate various measures, but also to improve the results of the original measures in terms of (1) correlation with human judgments and (2) computational efficiency, with gains up to 4 orders of magnitude.", "labels": [], "entities": []}, {"text": "Our method outperforms other state-of-theart graph embeddings models.", "labels": [], "entities": []}, {"text": "Besides, we evaluate it extrinsically in a WSD task by replacing the original structural measures with their vectorized counterparts in a graph-based WSD algorithm by, reaching comparable performance.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 43, "end_pos": 51, "type": "TASK", "confidence": 0.9228835999965668}]}, {"text": "Because of being inspired by the word2vec architecture, we dub our model 'path2vec' 1 mean-ing it encodes paths (or other similarities) between graph nodes into dense vectors.", "labels": [], "entities": []}, {"text": "Our first contribution is an effective and efficient approach to learn graph embeddings based on a user-defined custom similarity measure sim on a set of nodes V , e.g. the shortest path distance.", "labels": [], "entities": []}, {"text": "The second contribution is an application of state-of-the-art graph embeddings to word sense disambiguation task.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.8143919259309769}]}], "datasetContent": [{"text": "Experimental Setting It is possible to evaluate the models by calculating the rank correlation of their cosine similarities with the corresponding similarities for all the unique pairs from the training dataset, or at least a large part of them.", "labels": [], "entities": []}, {"text": "(2015) evaluated their approach on LCH similarities for all unique noun synset pairs from WordNet Core (about 5 million similarities total); their model achieves Spearman rank correlation of 0.732 on this task.", "labels": [], "entities": [{"text": "WordNet Core", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9469031989574432}, {"text": "Spearman rank correlation", "start_pos": 162, "end_pos": 187, "type": "METRIC", "confidence": 0.9112275640169779}]}, {"text": "However, this kind of evaluation does not measure the ability of the model to produce meaningful predictions, at least for language data: the overwhelming part of these unique pairs are synsets not related to each other at all.", "labels": [], "entities": []}, {"text": "For most tasks, it is useless to 'know' that, e.g., 'ambulance' and 'general' are less similar than 'ambulance' and 'president'.", "labels": [], "entities": []}, {"text": "While the distances between these node pairs are indeed different on the WordNet graph, we find it much more important for the model to be able to robustly tell really similar pairs from the unrelated ones so that they could benefit applications.", "labels": [], "entities": [{"text": "WordNet graph", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.9740688502788544}]}, {"text": "As a more balanced and relevant test set, we use noun pairs (666 total) from the SimLex999 semantic similarity dataset ().", "labels": [], "entities": [{"text": "SimLex999 semantic similarity dataset", "start_pos": 81, "end_pos": 118, "type": "DATASET", "confidence": 0.764405369758606}]}, {"text": "SimLex999 contains lemmas; as some lemmas may map to several WordNet synsets, for each word pair we choose the synset pair maximizing the WordNet similarity, following).", "labels": [], "entities": []}, {"text": "Then, we measure the Spearman rank correlation between these 'gold' scores and the similarities produced by the graph embedding models trained on the WordNet.", "labels": [], "entities": [{"text": "Spearman rank correlation", "start_pos": 21, "end_pos": 46, "type": "METRIC", "confidence": 0.6480661431948344}, {"text": "WordNet", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.9763271808624268}]}, {"text": "Further on, we call this evaluation score the 'correlation with WordNet similarities'.", "labels": [], "entities": []}, {"text": "This evaluation method directly measures how well the model fits the training objective . We also would like to check whether our models generalize to extrinsic tasks.", "labels": [], "entities": []}, {"text": "Thus, we additionally used human-annotated semantic similarities from the same SimLex999.", "labels": [], "entities": []}, {"text": "This additional evaluation strategy directly tests the models' correspondence to human judgments independently of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9729030132293701}]}, {"text": "These correlations were tested in two synset selection setups, important to distinguish: 1.", "labels": [], "entities": []}, {"text": "WordNet-based synset selection (static synsets): this setup uses the same lemma-to-synset mappings, based on maximizing WordNet similarity for each SimLex999 word pair with the corresponding similarity function.", "labels": [], "entities": [{"text": "WordNet-based synset selection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.747312088807424}]}, {"text": "It means that all the models are tested on exactly the same set of synset pairs (but the similarities themselves are taken from SimLex999, not from the WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 152, "end_pos": 159, "type": "DATASET", "confidence": 0.9599075317382812}]}, {"text": "2. Model-based synset selection (dynamic synsets): in this setup, lemmas are converted to synsets dynamically as apart of the evaluation workflow.", "labels": [], "entities": []}, {"text": "We choose the synsets that maximize word pair similarity using the vectors from the model itself, not similarity functions on the WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.9768792986869812}]}, {"text": "Then the resulting ranking is evaluated against the original SimLex999 ranking.", "labels": [], "entities": [{"text": "SimLex999 ranking", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.8552826941013336}]}, {"text": "The second (dynamic) setup in principle allows the models to find better lemma-to-synset mappings than those provided by the WordNet similarity functions.", "labels": [], "entities": []}, {"text": "This setup essentially evaluates two abilities of the model: 1) to find the best pair of synsets fora given pair of lemmas (sort of a disambiguation task), and 2) to produce the similarity score for the chosen synsets.", "labels": [], "entities": []}, {"text": "We are not aware of any 'gold' lemma-to-synset mapping for SimLex999, thus we directly evaluate only the second part, but implicitly the first one still influences the resulting scores.", "labels": [], "entities": []}, {"text": "Models often choose different synsets.", "labels": [], "entities": []}, {"text": "For example, for the word pair 'atom, carbon', the synset pair maximizing the JCN-S similarity calculated on the 'raw' WordNet would be 'atom.n.02 ('a tiny piece of anything'), carbon.n.01 ('an abundant nonmetallic tetravalent element')' with the similarity 0.11.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9836840629577637}]}, {"text": "However, in a path2vec model trained on the same gold similarities, the synset pair with the highest similarity 0.14 has a different first element: 'atom.n.01 ('the smallest component of an element having the chemical properties of the element')', which seems to beat least as good a decision as the one from the raw WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 317, "end_pos": 324, "type": "DATASET", "confidence": 0.97124183177948}]}, {"text": "an edge list as input, we had to add a selfconnection for all nodes (synsets) that lack edges in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9783310890197754}]}, {"text": "During the training of DeepWalk and node2vec models, we tested different values for the number of random walks (in the range from 10 to 100), and the vector size (100 to 600).", "labels": [], "entities": []}, {"text": "For DeepWalk, we additionally experimented with the window size (5 to 100).", "labels": [], "entities": []}, {"text": "All other hyperparameters were left at their default values.", "labels": [], "entities": []}, {"text": "FSE embeddings of the WordNet noun synsets were provided to us by the authors, and consist of 128-bit vectors.", "labels": [], "entities": [{"text": "WordNet noun synsets", "start_pos": 22, "end_pos": 42, "type": "DATASET", "confidence": 0.9028922319412231}]}, {"text": "presents the comparison of path2vec and the baselines with regards to how well they approximate the WordNet similarity functions output (the raw WordNet similarities always get the perfect correlation in this evaluation setup).", "labels": [], "entities": []}, {"text": "All the reported rank correlation value differences in this and other tables are statistically significant based on the standard two-sided p-value.", "labels": [], "entities": [{"text": "rank correlation value", "start_pos": 17, "end_pos": 39, "type": "METRIC", "confidence": 0.8373968402544657}]}, {"text": "We report the results for the best models for each method, all of them (except FSE) using vector size 300 for comparability.", "labels": [], "entities": [{"text": "FSE", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.5221163630485535}]}, {"text": "Path2vec outperform other baseline embeddings, achieving high correlation with the raw WordNet similarities.", "labels": [], "entities": []}, {"text": "This shows that our simple model can approximate different graph measures.", "labels": [], "entities": []}, {"text": "shows the similarities' distributions in the resulting models, reflecting the original measures' distributions in.", "labels": [], "entities": []}, {"text": "Experimental Setting As an additional extrinsic evaluation, we turned to word sense disambiguation task, reproducing the WSD approach from (Sinha and Mihalcea, 2007).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7537815570831299}, {"text": "WSD", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9006485342979431}]}, {"text": "The original algorithm uses WordNet similarities; we tested how using dot products and the learned embeddings instead will influence the WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9157131314277649}]}, {"text": "The employed WSD algorithm starts with building a graph where the nodes are the WordNet synsets of the words in the input sentence.", "labels": [], "entities": [{"text": "WSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8950831890106201}]}, {"text": "The nodes are then connected by edges weighted with the similarity values between the synset pairs (only if the similarity exceeds a threshold, which is a hyperparameter; we set it to 0.95).", "labels": [], "entities": []}, {"text": "The final step is selecting the most likely sense for each word based on the weighted in-degree centrality score for each synset (in case of ties, the first synset is chosen).", "labels": [], "entities": []}, {"text": "shows a graph generated for the sentence 'More often than not, ringers think of the church as something stuck on the bottom of the belfry'.", "labels": [], "entities": []}, {"text": "Note that we disambiguate nouns only.", "labels": [], "entities": []}, {"text": "presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, Deepwalk and node2vec models, and the 128D FSE model.", "labels": [], "entities": []}, {"text": "We evaluate on the following all-words English WSD test sets: Senseval-2 (),), and SemEval-15 Task 13.", "labels": [], "entities": [{"text": "English WSD test sets", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.7486968636512756}]}, {"text": "Raw WordNet similarities are still the best, but the path2vec models are consistently the second after them (and orders of magnitude faster), outperforming other graph embedding baselines.", "labels": [], "entities": []}, {"text": "The largest drop between the original and vector-based measures is for JCN-S, which is also the only one which relies not only on graph but also on external information from a corpus, making it more difficult to approximate (see also, where this measure distribution seems to be the most difficult to reproduce).", "labels": [], "entities": [{"text": "JCN-S", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.8552505373954773}]}, {"text": "Note that both the original graph-based measures and graph embeddings do not outperform the most frequent sense (MFS) baseline, which is inline with the original algorithm.", "labels": [], "entities": []}, {"text": "Pairwise Similarity Computation One of the reasons to use path2vec embeddings is computational efficiency.", "labels": [], "entities": []}, {"text": "Directly employing the WordNet graph to find semantic similarities between synsets is expensive.", "labels": [], "entities": [{"text": "WordNet graph", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9392023086547852}]}, {"text": "The dot product computation is much faster as compared to shortest path computation (and other complex walks) on a large graph.", "labels": [], "entities": []}, {"text": "Also, dense low-dimensional vector representations of nodes take much less space than the pairwise similarities between all the nodes.", "labels": [], "entities": []}, {"text": "The time complexity of calculating the shortest path between graph nodes (as in ShP or LCH) is in the best case linear in the number of nodes and edges.", "labels": [], "entities": []}, {"text": "JCN-S compares favorably since it is linear in the height of the taxonomy tree (Jiang and Conrath, 1997); however, it still cannot leverage highly-optimized routines and hardware capabilities, which makes the use of vectorized representations so efficient.", "labels": [], "entities": [{"text": "JCN-S", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8676098585128784}]}, {"text": "Calculating Hamming distance between binary strings (as in the FSE algorithm) is linear in the sum of string lengths, which are equivalent of vector sizes.", "labels": [], "entities": [{"text": "Hamming distance between binary strings", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.7553092002868652}]}, {"text": "At the same time, the complexity of calculating dot product between float vectors (as in path2vec) is linear in the vector size by the definition of the dot product and is easily and routinely parallelized.", "labels": [], "entities": []}, {"text": "As an example, let us consider the popular problem of ranking the graph nodes by their similarity to one particular node of interest (finding the 'nearest neighbors').", "labels": [], "entities": []}, {"text": "shows the time for computing similarities of one node to all other WordNet noun nodes, using either standard graph similarity functions from NLTK, Hamming distance between 128D binary embeddings, or dot product between a 300D float vector (representing this node) and all rows of a 82115 \u00d7 300 matrix.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9270598888397217}, {"text": "NLTK", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.9021166563034058}]}, {"text": "Using float vectors is 4 orders of magnitude faster than LCH, 3 orders faster than JCN, and 2 orders faster than Hamming distance.", "labels": [], "entities": [{"text": "JCN", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.7112717032432556}]}, {"text": "ing a training dataset for path2vec (following the workflow described in Section 4) requires calculating pairwise similarities between all possible pairs of graph nodes.", "labels": [], "entities": []}, {"text": "This leads to a number of similarity calculations quadratic in the number of nodes, which can be prohibitive in case of very large graphs.", "labels": [], "entities": []}, {"text": "However, instead of this, the training datasets for path2vec can be constructed much faster by taking the graph structure into account.", "labels": [], "entities": []}, {"text": "In essence, this implies finding for each node v the set of other nodes directly connected to it or to its direct graph neighbors (set of second order graph neighbors, V 2 ).", "labels": [], "entities": []}, {"text": "Then, graph similarity is calculated only for the pairs consisting of each v and the nodes in their respective V 2 ; these pairs constitute the training dataset (the same thresholds and normalization procedures apply).", "labels": [], "entities": []}, {"text": "The amount of pairwise similarity calculations is then linear in the number of nodes times the average number of neighbors in V 2 , which is much better.", "labels": [], "entities": []}, {"text": "Particularly, in the case of WordNet, each node (synset) has 36 synsets in its V 2 on average, and half of the nodes do not have any neighbors at all.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9498758316040039}]}, {"text": "Thus, only 2,935,829 pairwise similarity calculations are needed, 1,000 times less than when calculating similarities between all synset pairs.", "labels": [], "entities": []}, {"text": "Following that, e.g., the training dataset for JCN-S can be constructed in 3 minutes, instead of 5 hours, with similar speedups for other graph distance measures.", "labels": [], "entities": [{"text": "JCN-S", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8916541934013367}]}, {"text": "The training datasets constructed in this 'fast' way showed negligible performance decrease compared to the 'full' datasets (0.07...0.03 drop in the semantic similarity experiments, and < 0.03 drop in the WSD experiments).", "labels": [], "entities": [{"text": "WSD experiments", "start_pos": 205, "end_pos": 220, "type": "DATASET", "confidence": 0.8046940267086029}]}, {"text": "It means that when using path2vec in practical tasks, one can construct the training dataset very quickly, preserving embeddings performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlation scores with WordNet  similarities on the 666 noun pairs in SimLex999.", "labels": [], "entities": [{"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.6178574562072754}]}, {"text": " Table 2: Spearman correlations with human Sim- Lex999 noun similarities (WordNet synset selection).", "labels": [], "entities": [{"text": "Sim- Lex999 noun similarities", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.5709526658058166}]}, {"text": " Table 4: F1 scores on all-words WSD tasks.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9995415210723877}, {"text": "WSD tasks", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.8845695555210114}]}]}