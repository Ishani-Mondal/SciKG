{"title": [{"text": "SURel: A Gold Standard for Incorporating Meaning Shifts into Term Extraction", "labels": [], "entities": [{"text": "SURel", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.8488919138908386}, {"text": "Incorporating Meaning Shifts into Term Extraction", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.7669264127810796}]}], "abstractContent": [{"text": "We introduce SURel, a novel dataset for Ger-man with human-annotated meaning shifts between general-language and domain-specific contexts.", "labels": [], "entities": []}, {"text": "We show that meaning shifts of term candidates cause errors in term extraction, and demonstrate that the SURel annotation reflects these errors.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7519617080688477}]}, {"text": "Furthermore, we illustrate that SURel enables us to assess optimisations of term extraction techniques when incorporating meaning shifts.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7042424529790878}]}], "introductionContent": [{"text": "Domain-specific terms often undergo meaning shifts from general-language use to their respective domain-specific language use.", "labels": [], "entities": []}, {"text": "For example, the German noun Schnee predominantly means 'snow' in its general-language usage, and 'beaten egg whites' in the cooking domain.", "labels": [], "entities": []}, {"text": "Terms with these characteristics are referred to as subtechnical terms and pose a problem for term extraction: Their hybrid character makes it hard for humans to rank them along with unambiguous terms, and hard for computational models to classify them as terms, because of the strong bias towards their general-language meanings.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.7388601154088974}]}, {"text": "In this study, we present SURel (Synchronic Usage Relatedness), a novel dataset for meaning shifts from general to domain-specific language, based on human annotations on the degrees of semantic relatedness between contexts of term candidates.", "labels": [], "entities": [{"text": "Synchronic Usage Relatedness)", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7569665163755417}]}, {"text": "We illustrate that SURel reflects the error that is commonly made by term extraction measures for sub-technical terms when relying on a general-language reference corpus.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7122105956077576}]}, {"text": "Ina first experiment, we predict the meaning shift automatically and use SURel for evaluation.", "labels": [], "entities": []}, {"text": "We then incorporate the model's prediction as a factor into an established term extraction measure, to correct the error in termhood prediction caused by meaning shifts.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.6900319755077362}, {"text": "termhood prediction", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7547695338726044}]}], "datasetContent": [{"text": "Dataset Creation SURel was created analogously to), a dataset for meaning shifts across time.", "labels": [], "entities": []}, {"text": "Our novel dataset comprises a manual annotation of meaning relatedness between uses of target words in a general-language and a domain-specific corpus.", "labels": [], "entities": []}, {"text": "The strength of relatedness between uses defines whether the meanings of a word are related or differ, thus indicating if a meaning shift took place.", "labels": [], "entities": []}, {"text": "As general-language corpus (GEN) we subsampled SdeWaC), a cleaned version of the web corpus DEWAC ().", "labels": [], "entities": []}, {"text": "As domain-specific corpus (SPEC), we crawled cooking-related texts from several categories (recipes, ingredients, cookware, cooking techniques) from the German cooking recipe websites kochwiki.de and Wikibooks Kochbuch 2 . The reduced SdeWaC contains \u2248126 million words, SPEC contains \u22481.3 million words.", "labels": [], "entities": []}, {"text": "We selected 22 target words which occurred in both GEN and SPEC, and which we expected to exhibit different degrees of domain-specific meaning shift.", "labels": [], "entities": [{"text": "GEN", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.7752223610877991}]}, {"text": "For each target word we randomly sampled 20 use pairs (i.e., combinations of two contexts) from GEN, SPEC and across both, a total of 60 use pairs per word and 1,320 use pairs overall.", "labels": [], "entities": [{"text": "GEN", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9314095377922058}]}, {"text": "Four native speakers annotated the use pairs on a scale from 1 (unrelated meanings) to 4 (identical meanings), reaching a strong mean pairwise agreement of \u03c1 =0.88.", "labels": [], "entities": []}, {"text": "The ranking of the 22 target words by their average strength of relatedness between general-language and domainspecific uses is shown in.", "labels": [], "entities": []}, {"text": "On the left are target words with highly related meanings in GEN and SPEC; on the right are words with strongly different meanings.", "labels": [], "entities": [{"text": "GEN", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.7523236870765686}]}, {"text": "3 Dataset Analysis In the following, we analyse the meaning relatedness of use pairs within and across GEN and SPEC.", "labels": [], "entities": []}, {"text": "shows examples of annotations that nicely correspond to cases of meaning stability, reduction and change, respectively.", "labels": [], "entities": []}, {"text": "The y-axes show how often the use pairs were rated as 1-4.", "labels": [], "entities": []}, {"text": "In top left we find Schnittlauch 'chive' with strongly related meanings within and across GEN and SPEC, thus indicating meaning stability.", "labels": [], "entities": [{"text": "GEN", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.849571704864502}]}, {"text": "Top right, we find The dataset is available at www.ims.uni-stuttgart.de/data/surel.", "labels": [], "entities": []}, {"text": "Messer 'knife' with more related meanings in SPEC than in GEN, and even less strongly related meanings across GEN and SPEC, thus indicating meaning reduction.", "labels": [], "entities": [{"text": "SPEC", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7290035486221313}, {"text": "GEN", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9392167925834656}, {"text": "GEN", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.9421522617340088}, {"text": "meaning reduction", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.7559079229831696}]}, {"text": "In at the bottom we find Schnee 'snow'/'beaten egg whites' with strongly related meanings within GEN and also within SPEC but very different meanings when comparing GEN and SPEC uses, thus indicating a meaning shift.", "labels": [], "entities": [{"text": "GEN", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9418648481369019}, {"text": "GEN", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.8883658647537231}]}, {"text": "The three examples are taken from the two extremes and amid position in.  where f spec and f gen correspond to the frequencies of a term candidate x in a general and a domain-specific corpus, and s spec and s gen are the respective sizes of the corpora.", "labels": [], "entities": []}, {"text": "The left panel in shows the ranking of the SURel target words after computing their WEIRD scores, with decreasing termhood scores for targets from left to right.", "labels": [], "entities": [{"text": "WEIRD", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9369269609451294}]}, {"text": "The figure clearly illustrates that WEIRD ranks the targets words with strongest meaning shifts in SURel lowest, independently of their termhood: targets with high SURel scores are ranked as most terminological by WEIRD and occupy the first ranks (Messerspitze, Eiwei\u00df, . .", "labels": [], "entities": [{"text": "WEIRD", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.6317309141159058}, {"text": "WEIRD", "start_pos": 214, "end_pos": 219, "type": "DATASET", "confidence": 0.9248255491256714}]}, {"text": "), and targets with low SURel We use versions of our corpora which are limited to content words to be consistent with following experiments.", "labels": [], "entities": [{"text": "SURel", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9934462904930115}]}, {"text": "scores are ranked as the least terminological ones and occupy the last ranks (.", "labels": [], "entities": []}, {"text": ", Form, schlagen).", "labels": [], "entities": []}, {"text": "To further investigate this bias, we looked up the SURel targets in (a) Wiktionary and Wikipedia, (b) the German dictionary Duden and (c) popular German translation dictionaries (Langenscheidt and PONS).", "labels": [], "entities": [{"text": "SURel", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.5205223560333252}, {"text": "Wikipedia", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.8607461452484131}, {"text": "German dictionary Duden", "start_pos": 106, "end_pos": 129, "type": "DATASET", "confidence": 0.5604577461878458}]}, {"text": "If a word was assigned a cooking or gastronomy tag in any of these resources, we categorised it as a domain term.", "labels": [], "entities": []}, {"text": "In this way, ten of our targets 5 were categorised as terms; seven of them are among the ten most non-terminologically ranked targets by WEIRD.", "labels": [], "entities": [{"text": "WEIRD", "start_pos": 137, "end_pos": 142, "type": "DATASET", "confidence": 0.9393072128295898}]}, {"text": "This confirms that termhood predictions by WEIRD as a representative of contrastive termhood measures are strongly influenced by terminological meaning shifts.", "labels": [], "entities": [{"text": "WEIRD", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8440982699394226}]}, {"text": "Although the influence of meaning shifts might not be equally evident in other term extraction measures as in our simple example measure WEIRD, any other measure heavily relying on a general-language word frequency distribution will to some extent be negatively influenced by terminological meaning shifts.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.7381876707077026}]}, {"text": "Consequently, we need to correct the bias caused by meaning shifts.", "labels": [], "entities": []}, {"text": "In the following, we show that we can use SURel to assess factors that potentially reduce the bias.", "labels": [], "entities": []}, {"text": "Correcting the Meaning Shift For automatically predicting meaning shifts we rely on a stateof-the-art model for diachronic meaning change ().", "labels": [], "entities": [{"text": "automatically predicting meaning shifts", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.6903165504336357}]}, {"text": "We learn two separate word2vec SGNS vector spaces for GEN and SPEC.", "labels": [], "entities": [{"text": "GEN", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.8781778812408447}]}, {"text": "In order to compare the target vectors across spaces the spaces are aligned, i.e., the best rotation of one vector space onto the other is computed.", "labels": [], "entities": []}, {"text": "This corresponds to the solution of the orthogonal Procrustes problem.", "labels": [], "entities": []}, {"text": "If G and S are the matrices for the general and the specific vector spaces, then we rotate G by GW where W = UV T , with U and V retrieved from the singular value decomposition ST G = U \u03a3V T . Following standard practice we then length-normalize and mean-center G and S in a pre-processing step (.", "labels": [], "entities": []}, {"text": "After the alignment, cosine similarity between the vectors of the same word in both spaces is computed.", "labels": [], "entities": []}, {"text": "The cosine score of the two vectors of a word w predicts the strength of meaning change of w between GEN and SPEC, ranging from 0 (complete change) to 1 (stability).", "labels": [], "entities": [{"text": "GEN", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.8203978538513184}]}, {"text": "As input for the model, we use POS-tagged versions of our corpora, keeping only content words.", "labels": [], "entities": []}, {"text": "Evaluating the output of the model on the SURel dataset, we reach a Spearman's rank-order correlation coefficient of \u03c1=0.866 between the model's change predictions and SURel meaningshift ranks.", "labels": [], "entities": [{"text": "SURel dataset", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.850786417722702}, {"text": "Spearman's rank-order correlation coefficient", "start_pos": 68, "end_pos": 113, "type": "METRIC", "confidence": 0.7175874054431916}]}, {"text": "Inspecting the nearest neighbors (NNs) of our target words in confirms the ability of the model to predict strengths of meaning shifts.", "labels": [], "entities": []}, {"text": "For example, the NNs for Schnee change completely (from mud, leaves, foggy in the GEN space to egg whites, foamy, beat in the SPEC space), while for Schnittlauch all nearest neighbors in both spaces are cooking-related.", "labels": [], "entities": [{"text": "GEN space", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.8958883285522461}]}, {"text": "Finally, to correct WEIRD for the meaning-shift error, we incorporate the model's predictions of meaning change into the WEIRD formula, where \u03b1(x) corresponds to the model's predicted strength of meaning change for word x: The right panel in shows the ranking of the SURel target words based on their WEIRD M OD scores, again with decreasing termhood scores for targets from left to right.", "labels": [], "entities": [{"text": "WEIRD", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.5138691663742065}, {"text": "WEIRD M OD scores", "start_pos": 301, "end_pos": 318, "type": "METRIC", "confidence": 0.6954728290438652}]}, {"text": "The plot clearly shows that WEIRD M OD improves over WEIRD regarding the negative bias for meaning-shifted targets: now shifted target words do not gather in one part of the plot but occur across ranks.", "labels": [], "entities": [{"text": "WEIRD", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.6988321542739868}, {"text": "M", "start_pos": 34, "end_pos": 35, "type": "METRIC", "confidence": 0.660748302936554}, {"text": "OD", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.7009646892547607}, {"text": "WEIRD", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.7338495850563049}]}, {"text": "While WEIRD only reaches an average precision of 0.45, WEIRD M OD reaches an average precision of 0.59.", "labels": [], "entities": [{"text": "WEIRD", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.7510951161384583}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9928062558174133}, {"text": "WEIRD M OD", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.6477832198143005}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9912647604942322}]}, {"text": "In the same way as we incorporated the Hamilton measure of semantic change into WEIRD, we could rely on other contrastive term extraction techniques and incorporate further measures of semantic change.", "labels": [], "entities": [{"text": "WEIRD", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.7466750144958496}, {"text": "contrastive term extraction", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.6598424514134725}]}, {"text": "SURel can be utilised to evaluate modifications and thus to optimise termhood prediction techniques regarding the sub-technical terminological meaning shift bias.", "labels": [], "entities": [{"text": "termhood prediction", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.9097602069377899}]}], "tableCaptions": []}