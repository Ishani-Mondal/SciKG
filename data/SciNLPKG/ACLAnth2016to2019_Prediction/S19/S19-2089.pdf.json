{"title": [], "abstractContent": [{"text": "The detection of hate speech, especially in on-line platforms and forums, is quickly becoming a hot topic as anti-hate speech legislation begins to be applied to public discourse on-line.", "labels": [], "entities": [{"text": "detection of hate speech", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8231628388166428}]}, {"text": "The HatEval shared task was created with this in mind; participants were expected to develop a model capable of determining whether or not input (in this case, Twitter posts in En-glish and Spanish) could be considered hate speech (designated as Subtask A), if they were aggressive, and whether the tweet was targeting an individual, or speaking generally (Sub-task B).", "labels": [], "entities": []}, {"text": "We approached this Subtask by creating a LSTM model with an embedding layer.", "labels": [], "entities": []}, {"text": "We found that our model performed considerably better on English language input when compared to Spanish language input.", "labels": [], "entities": []}, {"text": "In En-glish, we achieved an F1-Score of 0.466 for Subtask A and 0.462 for Subtask B; In Span-ish, we achieved scores of 0.617 and 0.612 on Subtask A and Subtask B, respectively.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996776580810547}]}], "introductionContent": [{"text": "Social media plays an important role nowadays and dominates everyday life.", "labels": [], "entities": []}, {"text": "Social networks like Facebook, Twitter and Instagram are platforms where people express thoughts, feelings and emotions regarding themselves or others.", "labels": [], "entities": []}, {"text": "This can lead to different opinions colliding and creating conflicts.", "labels": [], "entities": []}, {"text": "Often, feelings are not expressed objectively and can be offensive to other users.", "labels": [], "entities": []}, {"text": "In order to make social media more comfortable, so called hate speech needs to be detected and removed.", "labels": [], "entities": []}, {"text": "Hate speech is here defined as: Any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics (.", "labels": [], "entities": [{"text": "Any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other", "start_pos": 32, "end_pos": 213, "type": "Description", "confidence": 0.8450318864163231}]}, {"text": "To assure there is no spread of illegal hate speech, the EU has created a code of conduct for social media platforms that needs to be followed.", "labels": [], "entities": []}, {"text": "According to these EU regulations, social media platforms must regulate hateful speech.", "labels": [], "entities": []}, {"text": "In addition, social media occupies an increasingly larger portion of public discourse; even without these EU regulations, it seems that these platforms should have some methods for controlling violent discourse.", "labels": [], "entities": []}, {"text": "For these reasons, the HatEval shared task () was created.", "labels": [], "entities": []}, {"text": "The task is divided into two subtasks; Subtask A is hate speech detection against immigrants and women, a binary classification problem where a tweet is classified as either hateful or not hateful.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.6751911441485087}]}, {"text": "Subtask B is determining whether a given tweet is aggressive, and whether it is targeting an individual, or not referring to any particular person.", "labels": [], "entities": []}, {"text": "Further, each of these Subtasks is evaluated on English tweets and using Spanish tweets.", "labels": [], "entities": []}, {"text": "We were provided a 9000-tweet English training set, and a 5000 tweet Spanish training set.", "labels": [], "entities": []}, {"text": "The training sets were manually tagged as hateful or not hateful, aggressive or not aggressive, and targeted or not targeted -examples of tweets marked as hateful can be seen in below.", "labels": [], "entities": []}, {"text": "In this paper, we detail our methods for approaching these problems.", "labels": [], "entities": []}, {"text": "We will first cover related works before detailing our specific solutions for Subtask A and Subtask B; we will then cover our model (and previously attempted models) and present our results.", "labels": [], "entities": []}, {"text": "Hate speech detection is naturally afar reaching topic, and in conclusion we will discuss the implications of our work for the field in general.", "labels": [], "entities": [{"text": "Hate speech detection", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9031452139218649}]}], "datasetContent": [{"text": "We first evaluated our preliminary models using the development data set, specifically using results of Subtask A in English to determine which of our beginning approaches was most successful.", "labels": [], "entities": [{"text": "development data set", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.7813558578491211}]}, {"text": "After this determination, we expanded upon our best working model (the simple LSTM model with embedding layer), and proceeded to use this approach to handle all tasks in both languages.", "labels": [], "entities": []}, {"text": "We calculated the F1-score for each of our models, and used this for our evaluations.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9994938373565674}]}, {"text": "As shown in Table 1, the 1-Layer LSTM model with Embedding outperformed our other two models significantly and achieved an F1-Score of 0.69 on the development set.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9995428323745728}]}, {"text": "The average results for each metric are shown in.", "labels": [], "entities": []}, {"text": "The final ranking for Subtask A for English, as well as Spanish, was based on the F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9404311180114746}]}, {"text": "Our F1-score was 0.466 for English, which ranked us 27 th out of 69 teams that submitted a result for this Task.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9984478950500488}, {"text": "English", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.6681547164916992}]}, {"text": "Since we had some problems with the Spanish data set, we could only submit one solution for Subtask A, which placed us 36 th out of 39 teams.", "labels": [], "entities": [{"text": "Spanish data set", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.9245729843775431}, {"text": "Subtask A", "start_pos": 92, "end_pos": 101, "type": "TASK", "confidence": 0.7338794767856598}]}, {"text": "Evaluation for Subtask B was based on two criteria -partial match and exact match.", "labels": [], "entities": [{"text": "Subtask B", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.8633004128932953}, {"text": "exact match", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.966960608959198}]}, {"text": "For partial match, each dimension that needs to be predicted, is being looked at independently and therefore the usual evaluation metrics are being used (Precision, Accuracy, Recall and F1-Score).", "labels": [], "entities": [{"text": "Precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9919575452804565}, {"text": "Accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9824344515800476}, {"text": "Recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.9884338974952698}, {"text": "F1-Score", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9979871511459351}]}, {"text": "For the exact match all the dimensions to be predicted are jointly considered.", "labels": [], "entities": []}, {"text": "Ranking was solely based on the score of the Exact Match Ratio (EMR).", "labels": [], "entities": [{"text": "Exact Match Ratio (EMR)", "start_pos": 45, "end_pos": 68, "type": "METRIC", "confidence": 0.9099471867084503}]}, {"text": "For English we achieved an EMR score of 0.173, which ranked us second to last, even if our average F1-score was higher than other systems'.", "labels": [], "entities": [{"text": "EMR score", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9838388562202454}, {"text": "F1-score", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9990942478179932}]}, {"text": "Since we had the same problems as in Subtask A, we again were only able to submit one file in Subtask B for Spanish, where we achieved an EMR of 0.428 and an average F1-Score of 0.612.", "labels": [], "entities": [{"text": "EMR", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9985107779502869}, {"text": "F1-Score", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9995356798171997}]}, {"text": "The significant difference for the F1-Score between English and Spanish comes from the fact that there was less training data for Spanish compared to English.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9989812970161438}]}], "tableCaptions": [{"text": " Table 1: Development set F1-scores for prelimi- nary testing of models.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.8660616278648376}]}, {"text": " Table 2: Results for Tasks A and B in English and  Spanish.", "labels": [], "entities": []}]}