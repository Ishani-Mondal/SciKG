{"title": [{"text": "A Corpus of Negations and their Underlying Positive Interpretations", "labels": [], "entities": []}], "abstractContent": [{"text": "Negation often conveys implicit positive meaning.", "labels": [], "entities": []}, {"text": "In this paper, we present a corpus of negations and their underlying positive interpretations.", "labels": [], "entities": []}, {"text": "We work with negations from Simple Wikipedia, automatically generate potential positive interpretations, and then collect manual annotations that effectively rewrite the negation in positive terms.", "labels": [], "entities": [{"text": "Simple Wikipedia", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.7672129273414612}]}, {"text": "This procedure yields positive interpretations for approximately 77% of negations, and the final corpus includes over 5,700 negations and over 5,900 positive interpretations.", "labels": [], "entities": []}, {"text": "We also present base-line results using seq2seq neural models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Negation is present in every human language.", "labels": [], "entities": []}, {"text": "It is in the first place a phenomenon of semantical opposition.", "labels": [], "entities": [{"text": "semantical opposition", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8149055242538452}]}, {"text": "As such, negation relates an expression e to another expression with a meaning that is in someway opposed to the meaning of e (.", "labels": [], "entities": [{"text": "negation", "start_pos": 9, "end_pos": 17, "type": "TASK", "confidence": 0.9769457578659058}]}, {"text": "Sentences containing negation are generally (a) less informative than affirmative ones (e.g., Milan is not the capital of Italy vs. Rome is the capital of Italy), (b) morphosyntactically more marked-all languages have negative markers while only a few have affirmative markers, and (c) psychologically more complex and harder to process.", "labels": [], "entities": []}, {"text": "Negation often conveys implicit positive meanings.", "labels": [], "entities": []}, {"text": "This meaning ranges from implicatures to entailments, and we refer to it as positive interpretations.", "labels": [], "entities": []}, {"text": "Consider the following text from Simple Wikipedia: An abjad is an alphabet in which all its letters are consonants.", "labels": [], "entities": [{"text": "Simple Wikipedia", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.77161505818367}]}, {"text": "Though vowels can be added in some abjads, they are not needed to write a word correctly.", "labels": [], "entities": []}, {"text": "Some examples of abjads are the Arabic alphabet and the 1 Mr. Smith apologized for :: not getting involved.", "labels": [], "entities": []}, {"text": "Mr. Smith apologized for staying passive.", "labels": [], "entities": []}, {"text": "2 I :::: never heard of this guy before they started doing these commercials on television and radio.", "labels": [], "entities": []}, {"text": "I heard of this guy after they started doing these commercials on on television and radio.", "labels": [], "entities": []}, {"text": "3 In Hinduism, beef is :: not allowed to be eaten.", "labels": [], "entities": []}, {"text": "In Hinduism, chicken is allowed to be eaten.", "labels": [], "entities": []}, {"text": "In other religions, beef is allowed to be eaten.", "labels": [], "entities": []}, {"text": "Humans intuitively understand that the negation (second sentence) implies the following positive interpretation: Though vowels can be added in some abjads, only consonants are needed to write a word correctly.", "labels": [], "entities": []}, {"text": "shows three sentences containing negation and their underlying positive interpretations.", "labels": [], "entities": []}, {"text": "Positive interpretations do not have any negation cues (e.g., not, never) and Example 3 shows that some negations may have more than one underlying positive interpretation depending on the context.", "labels": [], "entities": []}, {"text": "Revealing the underlying positive interpretation of negation is challenging.", "labels": [], "entities": [{"text": "negation", "start_pos": 52, "end_pos": 60, "type": "TASK", "confidence": 0.9250901341438293}]}, {"text": "First, we need to identify which tokens are intended to be negated (e.g., getting involved and before in Examples 1 and 2 from).", "labels": [], "entities": []}, {"text": "Second, we need to rewrite those tokens to generate an actual positive interpretation (e.g., getting involved: staying passive).", "labels": [], "entities": []}, {"text": "This paper presents a corpus of negations and their underlying positive interpretations.", "labels": [], "entities": []}, {"text": "The main contributions are: (a) deterministic procedure to generate potential positive interpretations from negations, (b) corpus of negations and their positive interpretations manually annotated, (c) detailed analysis including which subtrees in the dependency tree are more likely to be rewritten and qualitative analysis of positive interpreta-tions.", "labels": [], "entities": []}, {"text": "Additionally, we establish baseline results with sequence-to-sequence neural models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task of generating positive interpretations from a sentence containing negation can be approached with sequence-to-sequence (seq2seq) models (input: sentence containing negation, output: positive interpretation).", "labels": [], "entities": []}, {"text": "In this section, we present baseline results with existing seq2seq models.", "labels": [], "entities": []}, {"text": "Specifically, we experiment with a basic seq2seq model ( ), two seq2seq models with attention (, and Google's neural machine translation (NMT) system (, which is also seq2seq model with attention and arguably the most complex.", "labels": [], "entities": []}, {"text": "We acknowledge that these systems are usually trained with orders of magnitude more examples, and comparing them when trained with our fairly small corpus maybe unfair because they were designed for other tasks.", "labels": [], "entities": []}, {"text": "Our goal is not to obtain the best results possible, but rather provide baseline results for our task and corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Basic counts for sentences containing negation  in Simple Wikipedia.", "labels": [], "entities": [{"text": "Simple Wikipedia", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8473472893238068}]}, {"text": " Table 3: Distribution of negation type in Simple  Wikipedia.", "labels": [], "entities": [{"text": "negation type", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.8590572774410248}, {"text": "Simple  Wikipedia", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.7996203005313873}]}, {"text": " Table 4: Initial number of negations in Simple  Wikipedia, and how many remain after each filter.", "labels": [], "entities": [{"text": "Simple  Wikipedia", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.7038127183914185}]}, {"text": " Table 5: Distribution of negations by number of poten- tial positive interpretations generated.", "labels": [], "entities": []}, {"text": " Table 7: Categories and subcategories discovered in a sample of 100 negations and all their positive interpretations.", "labels": [], "entities": []}, {"text": " Table 9: Results (BLEU-4, grammaticality and correctness) obtained with the test set.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9989684820175171}, {"text": "correctness", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9559016823768616}]}]}