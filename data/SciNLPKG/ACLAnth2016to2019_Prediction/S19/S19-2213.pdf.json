{"title": [], "abstractContent": [{"text": "In this paper we present our approach and the system description for Sub Task A of SemEval 2019 Task 9: Suggestion Mining from On-line Reviews and Forums.", "labels": [], "entities": [{"text": "SemEval 2019 Task 9", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8640436977148056}, {"text": "Suggestion Mining", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.9706766307353973}]}, {"text": "Given a sentence, the task asks to predict whether the sentence consists of a suggestion or not.", "labels": [], "entities": []}, {"text": "Our model is based on Universal Language Model Fine-tuning for Text Classification.", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7293100506067276}]}, {"text": "We apply various pre-processing techniques before training the language and the classification model.", "labels": [], "entities": []}, {"text": "We further provide detailed analysis of the results obtained using the trained model.", "labels": [], "entities": []}, {"text": "Our team ranked 10th out of 34 participants, achieving an F1 score of 0.7011.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9897659420967102}]}, {"text": "We publicly share our implementation 1 .", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The dataset used in all our experiments was provided by the organizers of the task and consists of sentences from a suggestion forum annotated by humans to be a suggestion or a non-suggestion.", "labels": [], "entities": []}, {"text": "Suggestion forums are dedicated forums used for providing suggestions on a specific product, service, processor an entity of interest.", "labels": [], "entities": [{"text": "Suggestion forums are dedicated forums used for providing suggestions on a specific product, service, processor an entity of interest", "start_pos": 0, "end_pos": 133, "type": "Description", "confidence": 0.7869868604909807}]}, {"text": "The provided dataset is collected from uservoice.com 2 , and consists of feedback posts on Universal Windows Platform.", "labels": [], "entities": [{"text": "Universal Windows Platform", "start_pos": 91, "end_pos": 117, "type": "DATASET", "confidence": 0.9487207531929016}]}, {"text": "Only those sentences are present in the dataset that explicitly expresses suggestions, for example -Do try the cupcakes from the bakery next door, instead of those that contain implicit suggestions such as -I loved the cup cakes from the bakery next door (.", "labels": [], "entities": []}, {"text": "Before using the provided dataset for training a prediction model, we take steps to prepare it as an input to our machine learning models.", "labels": [], "entities": []}, {"text": "We primarily use Ekphrasis 3 for implementing our preprocessing steps.", "labels": [], "entities": []}, {"text": "Some of the steps that we take are presented in this section.", "labels": [], "entities": []}, {"text": "We show the effectiveness of transfer learning for the task of suggestion mining by training Universal Language Model Fine-tuning for Text Classification (ULMFiT).", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7420688569545746}, {"text": "Text Classification", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7115684151649475}]}, {"text": "One of the main advantages of training ULMFiT is that it works very well fora small dataset as provided in the Sub Task A and also avoids the process of training a classification model from scratch.", "labels": [], "entities": []}, {"text": "We use the fast.ai 4 implementation of this model.", "labels": [], "entities": []}, {"text": "The ULMFiT model has mainly two parts, the language model and the classification model.", "labels": [], "entities": []}, {"text": "The language model is trained on a Wiki Text corpus to capture general features of the language in different layers.", "labels": [], "entities": [{"text": "Wiki Text corpus", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.8113375107447306}]}, {"text": "We fine tune the language model on the training, validation and the evaluation data.", "labels": [], "entities": []}, {"text": "Also, we additionally scrap around two thousand reviews from the Universal Windows Platform for training our language model.", "labels": [], "entities": [{"text": "Universal Windows Platform", "start_pos": 65, "end_pos": 91, "type": "DATASET", "confidence": 0.8666846553484598}]}, {"text": "After analysis of the performance we find optimal parameters to be: \u2022 BPTT: 70, bs: 48.", "labels": [], "entities": [{"text": "BPTT", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.993259847164154}]}, {"text": "\u2022 Embedding size: 400, hidden size: 1150, num of layers: 3 We also experiment with MultinomialNB, Logistic Regression, Support Vector Machines, https://docs.fast.ai/text.html LSTM.", "labels": [], "entities": []}, {"text": "For LSTM we use fasttext word embeddings 5 having 300 dimensions trained on Wikipedia corpus, for representing words., shows the performances of all the models that we trained on the provided training dataset.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.8921099603176117}]}, {"text": "We also obtained the test dataset from the organizers and evaluated our trained models on the same.", "labels": [], "entities": []}, {"text": "The ULMFiT model achieved the best results with a F1-score of 0.861 on the training dataset and a F1-score of 0.701 on the test dataset.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9997182488441467}, {"text": "F1-score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9995912909507751}]}, {"text": "shows the performance of the top 5 models for Sub Task A of SemEval 2019 Task 9.", "labels": [], "entities": [{"text": "SemEval 2019 Task 9", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7997964024543762}]}, {"text": "Our team ranked 10th out of 34 participants.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of different models on the pro- vided train and test dataset for Sub Task A.", "labels": [], "entities": [{"text": "Sub Task A", "start_pos": 87, "end_pos": 97, "type": "TASK", "confidence": 0.7554312944412231}]}, {"text": " Table 4: Best performing models for SemEval Task 9:  Sub Task A.", "labels": [], "entities": [{"text": "SemEval Task 9", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.8917238910992941}]}]}