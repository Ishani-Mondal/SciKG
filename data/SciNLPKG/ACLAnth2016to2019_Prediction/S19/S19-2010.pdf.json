{"title": [{"text": "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)", "labels": [], "entities": [{"text": "Identifying and Categorizing Offensive Language in Social Media", "start_pos": 21, "end_pos": 84, "type": "TASK", "confidence": 0.7031939253211021}, {"text": "OffensEval)", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.6502541899681091}]}], "abstractContent": [{"text": "We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval).", "labels": [], "entities": [{"text": "SemEval-2019 Task 6", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8739900588989258}, {"text": "Identifying and Categorizing Offensive Language in Social Media (OffensEval)", "start_pos": 71, "end_pos": 147, "type": "TASK", "confidence": 0.7765946659174833}]}, {"text": "The task was based on anew dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets.", "labels": [], "entities": []}, {"text": "In sub-task A, the goal was to discriminate between offensive and non-offensive posts.", "labels": [], "entities": []}, {"text": "In sub-task B, the focus was on the type of offensive content in the post.", "labels": [], "entities": []}, {"text": "Finally, in sub-task C, systems had to detect the target of the offensive posts.", "labels": [], "entities": []}, {"text": "OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019.", "labels": [], "entities": []}, {"text": "In total, about 800 teams signed up to participate in the task, and 115 of them submitted results, which we present and analyze in this report.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen the proliferation of offensive language in social media platforms such as Facebook and Twitter.", "labels": [], "entities": []}, {"text": "As manual filtering is very time consuming, and as it can cause post-traumatic stress disorder-like symptoms to human annotators, there have been many research efforts aiming at automating the process.", "labels": [], "entities": []}, {"text": "The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content.", "labels": [], "entities": []}, {"text": "Examples of offensive content studied in previous work include hate speech (), cyberbulling), and aggression (.", "labels": [], "entities": []}, {"text": "Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks ().", "labels": [], "entities": [{"text": "abusive language detection sub-tasks", "start_pos": 149, "end_pos": 185, "type": "TASK", "confidence": 0.7018321976065636}]}, {"text": "Interestingly, none of this previous work has studied both the type and the target of the offensive language, which is our approach here.", "labels": [], "entities": []}, {"text": "Our task, OffensEval 1 , uses the Offensive Language Identification Dataset (OLID) 2 (), which we created specifically for this task.", "labels": [], "entities": [{"text": "Offensive Language Identification Dataset (OLID) 2", "start_pos": 34, "end_pos": 84, "type": "DATASET", "confidence": 0.6843869164586067}]}, {"text": "OLID is annotated following a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account.", "labels": [], "entities": []}, {"text": "Thus, it can relate to phenomena captured by previous datasets such as the one by.", "labels": [], "entities": []}, {"text": "Hate speech, for example, is commonly understood as an insult targeted at a group, whereas cyberbulling is typically targeted at an individual.", "labels": [], "entities": []}, {"text": "We defined three sub-tasks, corresponding to the three levels in our annotation schema: Sub-task A: Offensive language identification (104 participating teams) Sub-task B: Automatic categorization of offense types (71 participating teams) Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval.", "labels": [], "entities": [{"text": "Offensive language identification", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.7342565457026163}, {"text": "Offense target identification", "start_pos": 251, "end_pos": 280, "type": "TASK", "confidence": 0.6509293715159098}]}, {"text": "Section 3 presents the shared task description and the subtasks included in OffensEval.", "labels": [], "entities": []}, {"text": "Section 4 includes a brief description of OLID based on (.", "labels": [], "entities": [{"text": "OLID", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.48531967401504517}]}, {"text": "Section 5 discusses the participating systems and their results in the shared task.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training and testing material for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, which was built specifically for this task.", "labels": [], "entities": [{"text": "Offensive Language Identification Dataset (OLID) dataset", "start_pos": 71, "end_pos": 127, "type": "DATASET", "confidence": 0.6278834007680416}]}, {"text": "OLID was annotated using a hierarchical three-level annotation model introduced in.", "labels": [], "entities": [{"text": "OLID", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8276433348655701}]}, {"text": "Four examples of annotated instances from the dataset are presented in.", "labels": [], "entities": []}, {"text": "We use the annotation of each of the three layers in OLID fora sub-task in OffensEval as described below.", "labels": [], "entities": []}, {"text": "Given the strong imbalance between the number of instances in the different classes across the three tasks, we used the macro-averaged F1-score as the official evaluation measure for all three sub-tasks.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9345512986183167}]}, {"text": "At the end of the competition, we provided the participants with packages containing the results for each of their submissions, including tables and confusion matrices, and tables with the ranks listing all teams who competed in each sub-task.", "labels": [], "entities": []}, {"text": "For example, the confusion matrix for the best team in sub-task A is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Four tweets from the OLID dataset, with their labels for each level of the annotation model.", "labels": [], "entities": [{"text": "OLID dataset", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.934508204460144}]}, {"text": " Table 2: The teams that participated in OffensEval and  submitted system description papers.", "labels": [], "entities": []}, {"text": " Table 3: Distribution of label combinations in OLID.", "labels": [], "entities": [{"text": "OLID", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.6285606622695923}]}, {"text": " Table 4: F1-Macro for the top-10 teams followed by the rest of the teams grouped in ranges for all three sub-tasks.  Refer to", "labels": [], "entities": [{"text": "F1-Macro", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.992801308631897}]}, {"text": " Table 5: All the teams that participated in SemEval-2019 Task 6 with their ranks for each sub-task. The symbol '-'  indicates that the team did not participate in some of the subtasks. Please, refer to Table 4 to see the scores based  on a team's rank. The top team for each task is in bold, and the second-place team is underlined. Note: ASE -CSE  stands for Amrita School of Engineering -CSE, and BNU-HBKU stands for BNU-HKBU UIC NLP Team 2.", "labels": [], "entities": [{"text": "SemEval-2019 Task 6", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8861611286799113}, {"text": "Amrita School of Engineering -CSE", "start_pos": 361, "end_pos": 394, "type": "DATASET", "confidence": 0.8175994157791138}, {"text": "BNU-HBKU", "start_pos": 400, "end_pos": 408, "type": "METRIC", "confidence": 0.916597306728363}, {"text": "BNU-HKBU UIC NLP Team 2", "start_pos": 420, "end_pos": 443, "type": "DATASET", "confidence": 0.7780473470687866}]}]}