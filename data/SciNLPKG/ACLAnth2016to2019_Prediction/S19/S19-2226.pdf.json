{"title": [{"text": "ZQM at SemEval-2019 Task9: A Single Layer CNN Based on Pre-trained Model for Suggestion Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our system that competed at SemEval 2019 Task 9-SubTask A: \"Sug-gestion Mining from Online Reviews and Fo-rums\".", "labels": [], "entities": [{"text": "SemEval 2019 Task 9-SubTask", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.8919216692447662}, {"text": "Sug-gestion Mining", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.9877901673316956}]}, {"text": "Our system fuses the convolutional neural network and the latest BERT model to conduct suggestion mining.", "labels": [], "entities": [{"text": "BERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9850680232048035}, {"text": "suggestion mining", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.8046905100345612}]}, {"text": "In our system, the input of convolutional neural network is the embedding vectors which are drawn from the pre-trained BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9822906851768494}]}, {"text": "And to enhance the effectiveness of the whole system, the pre-trained BERT model is fine-tuned by provided datasets before the procedure of embedding vectors extraction.", "labels": [], "entities": [{"text": "BERT", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9845098853111267}, {"text": "embedding vectors extraction", "start_pos": 140, "end_pos": 168, "type": "TASK", "confidence": 0.6358885367711385}]}, {"text": "Empirical results show the effectiveness of our model which obtained 9th position out of 34 teams with F1 score equals to 0.715.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9858727157115936}]}], "introductionContent": [{"text": "Suggestion mining is defined as the extraction of suggestions from unstructured text ().", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9755407571792603}]}, {"text": "Suggestion mining is still a relatively young research area as compared to other natural language processing issues like sentiment analysis (.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9865994155406952}, {"text": "sentiment analysis", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.959653913974762}]}, {"text": "While suggestion mining is of great commercial value for organisations to improve the quality of their entities by considering the positive and negative opinions collected from platforms.", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.725804328918457}]}, {"text": "The target of this task is to automatically classify the sentences collected from online reviews and forums into two classes which are suggestion and non-suggestion respectively (.", "labels": [], "entities": []}, {"text": "BERT which stands for Bidirectional Encoder Representation from Transformers is the latest breakthrough in the field of NLP provided by Google Research.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9825024604797363}, {"text": "Bidirectional Encoder Representation from Transformers", "start_pos": 22, "end_pos": 76, "type": "TASK", "confidence": 0.7104131758213044}]}, {"text": "It has substantially advanced the state-of-the-art in many NLP tasks, especially in question answering.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.9004563987255096}, {"text": "question answering", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9098634421825409}]}, {"text": "More importantly, it provides a widely applicable tool for representation learning which can be generalized to many NLP tasks.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.9684021472930908}]}, {"text": "For this subtask, we firstly learn the word or sentence embeddings utilizing the pre-trained BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9903242588043213}]}, {"text": "Then the embedding vectors are extracted from BERT as the input of the subsequent model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9865374565124512}]}, {"text": "It is worth noting that we have finetuned the pre-trained BERT model with provided dataset before the embedding vectors are extracted.", "labels": [], "entities": [{"text": "BERT", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9425793290138245}]}, {"text": "In other words, this part is equivalent to the conventional embedding layer.", "labels": [], "entities": []}, {"text": "This strategy is a little bit like ELMO (.", "labels": [], "entities": [{"text": "ELMO", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.5644609928131104}]}, {"text": "As for the upper layer of this system, convolutional neural network (CNN) is adopted herein to process the features.", "labels": [], "entities": []}, {"text": "Although CNN is originally invented for tackling computer vision issues, while it has subsequently been shown to be effective for many NLP tasks.", "labels": [], "entities": [{"text": "tackling computer vision", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.8031783699989319}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the detailed architecture of our system.", "labels": [], "entities": []}, {"text": "Section 3 reports the experimental results on the given datasets.", "labels": [], "entities": []}, {"text": "Finally, conclusions are drawn in Section 4.", "labels": [], "entities": []}, {"text": "gives a high-level overview of our approach.", "labels": [], "entities": []}, {"text": "And we elaborate the details of implementation which mainly consists of following steps: (1) the preprocessing of raw data, (2) the word embedding learning via BERT model, (3) feature processing via CNN and sentences classification.", "labels": [], "entities": [{"text": "BERT", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9263460040092468}, {"text": "feature processing", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.7959467768669128}, {"text": "sentences classification", "start_pos": 207, "end_pos": 231, "type": "TASK", "confidence": 0.7218960523605347}]}], "datasetContent": [{"text": "As mentioned in Section 2.2, we conduct finetuning operation before extracting the word embedding vector from BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.8927645087242126}]}, {"text": "For fine-tuning, most hyperparameters are the same as the parameters  of pre-trained model, with the exception of batch size, learning rate and number of training epochs.", "labels": [], "entities": []}, {"text": "The mini-batch size is set at 32 and learning rate is set at 5e-5, the number of training epochs is configured as 5.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9071023464202881}]}, {"text": "The maximal length of sentences is configured as 50 and if the length of a sentence is less than 50, it will be padded with zero; otherwise, it will be truncated from the tail.", "labels": [], "entities": []}, {"text": "For the CNN component, the filter size m is configured as 5 and the number of filters N f is 64.", "labels": [], "entities": []}, {"text": "And the dropout rates we have tried are ranged from 0.1 to 0.7 with a step of 0.1.", "labels": [], "entities": []}, {"text": "The experimental results are shown in.", "labels": [], "entities": []}, {"text": "In order to prove the effectiveness of word embeddings derived from BERT, we also employ Word2vec as comparison and the corresponding best dropout rate is 0.4.", "labels": [], "entities": [{"text": "BERT", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.841796875}, {"text": "Word2vec", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9609436988830566}]}, {"text": "Obviously, no matter what the dropout rate is, our model consistently outperforms other models.", "labels": [], "entities": []}, {"text": "And the best dropout rate of our model is around 0.1 for abovementioned configuration.", "labels": [], "entities": []}, {"text": "While the best dropout rate may vary with other parameter configuration like filter size and the number of filters.", "labels": [], "entities": []}, {"text": "shows the Precision, Recall and F1 score in term of different classes.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990267753601074}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9954989552497864}, {"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9792814254760742}]}, {"text": "Obviously, the system performance on negative samples is better than the performance on positive samples which is consistent with our intuition for the reason that the number of negative samples overwhelms the number of positive samples.", "labels": [], "entities": []}, {"text": "Therefore, our system can learn more features of negative samples, which can help it recognize those negative samples accurately.", "labels": [], "entities": []}, {"text": "We also investigate the impact of the number of filters N f on the classification performance with fixing the filter size mat 5 and the dropout rate at 0.1.", "labels": [], "entities": [{"text": "classification", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.9631322026252747}]}, {"text": "The experimental result is shown as.", "labels": [], "entities": []}, {"text": "This model yields the best performance when the number of filters is equal to 64.", "labels": [], "entities": []}, {"text": "Less filters cannot capture enough information while too many filters can result in information redundancy to some extent.", "labels": [], "entities": []}, {"text": "The impact of filter size on classification accuracy is shown as.", "labels": [], "entities": [{"text": "classification", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9491900205612183}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9554365873336792}]}, {"text": "The most suitable filter size which means the window size of convolutional operation ism = 5.", "labels": [], "entities": []}, {"text": "It is difficult for this system to catch the global semantic information if the window size is too small.", "labels": [], "entities": []}, {"text": "While some local semantic information would be ignored if the window size of filter is too large.", "labels": [], "entities": []}, {"text": "Hence, choosing a filter with moderate size is helpful for the performance improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Apparently,  there is class imbalance that the number of neg- ative samples overwhelms the number of positive  samples both in training set and test set. So for  experiments, we fuse the train set and trail test set  into a larger training set, and then split 10% sam- ples as validation set randomly (8183 samples for  training and 909 samples for validation). We train  our model on the train set, tune the model param- eters on the validation set, evaluate the model per- formance on the test set.", "labels": [], "entities": []}, {"text": " Table 3: The classification accuracy of different class- es.", "labels": [], "entities": [{"text": "classification", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9496646523475647}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9762090444564819}]}]}