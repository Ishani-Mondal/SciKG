{"title": [{"text": "OleNet at SemEval-2019 Task 9: BERT based Multi-Perspective Models for Suggestion Mining", "labels": [], "entities": [{"text": "OleNet at SemEval-2019 Task 9", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.7379591941833497}, {"text": "BERT", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9909396171569824}, {"text": "Suggestion Mining", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.9841703772544861}]}], "abstractContent": [{"text": "This paper describes our system participated in Task 9 of SemEval-2019: the task is fo-cused on suggestion mining and it aims to classify given sentences into suggestion and non-suggestion classes in domain specific and cross domain training setting respectively.", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7487403452396393}]}, {"text": "We propose a multi-perspective architecture for learning representations by using different classical models including Convolutional Neu-ral Networks (CNN), Gated Recurrent Units (GRU), Feed Forward Attention (FFA), etc.", "labels": [], "entities": [{"text": "Feed Forward Attention (FFA)", "start_pos": 186, "end_pos": 214, "type": "TASK", "confidence": 0.6538397173086802}]}, {"text": "To leverage the semantics distributed in large amount of unsupervised data, we also have adopted the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model as an encoder to produce sentence and word representations.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformers (BERT)", "start_pos": 113, "end_pos": 175, "type": "TASK", "confidence": 0.7112888842821121}]}, {"text": "The proposed architecture is applied for both sub-tasks, and achieved f1-score of 0.7812 for subtask A, and 0.8579 for subtask B.", "labels": [], "entities": [{"text": "f1-score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9855572581291199}]}, {"text": "We won the first and second place for the two tasks respectively in the final competition.", "labels": [], "entities": []}], "introductionContent": [{"text": "Suggestion mining, which can be defined as the extraction of suggestions from unstructured text, where the term suggestions refers to the expressions of tips, advice, recommendations etc.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.935155600309372}]}, {"text": "(. For example, I would recommend doing the upgrade to be sure you have the best chance at trouble free operation. and Be sure to specify a room at the back of the hotel.", "labels": [], "entities": []}, {"text": "should be a suggestion for electronics and hotel separately.", "labels": [], "entities": []}, {"text": "Collecting suggestions is an integral step of any decision making process.", "labels": [], "entities": [{"text": "Collecting suggestions", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.916601687669754}, {"text": "decision making process", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.8814269304275513}]}, {"text": "A suggestion mining system could extract exact suggestion sentences from a retrieved document, which would enable the user to collect suggestions from a much larger number of pages than they could manually read over a short span of time.", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7045760303735733}]}, {"text": "Suggestion mining remains a relatively young area.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9895652532577515}]}, {"text": "So far, it has usually been defined as a problem of classifying sentences of a given text into suggestion and non-suggestion classes.", "labels": [], "entities": []}, {"text": "Mostly rule-based systems have so far been developed, and very few statistical classifiers have been proposed) ().", "labels": [], "entities": []}, {"text": "A related field to suggestion mining is sentiment classification which given a sentence or a document, it should infer the sentiment polarity e.g. positive, negative, neutral.", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7651145458221436}, {"text": "sentiment classification", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.9229498207569122}]}, {"text": "So, many classical sentiment classification systems can be used in suggestion mining like the widely used CNN-based models or RNN-based models.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.8070605397224426}, {"text": "suggestion mining", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8052374720573425}]}, {"text": "However, there are still many challenges in this suggestion mining task.", "labels": [], "entities": [{"text": "suggestion mining task", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8062882423400879}]}, {"text": "First of all, both of the subtasks suffers from severely lack of data.", "labels": [], "entities": []}, {"text": "Second, one of the subtasks requires the model should have transferability without seeing any of the target domain data.", "labels": [], "entities": []}, {"text": "To tackle those problems, knowledge transfer or transfer learning between domains would be desirable.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8702848553657532}]}, {"text": "In recent years, transfer learning techniques have been widely applied to solve domain adaptation problem, e.g. (. And in our system, considering the simplicity for training a model, we turn to taking use the power of large amount of unsupervised data for knowledge representations for both same domain and cross domain tasks.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9001562893390656}, {"text": "domain adaptation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7386565208435059}]}, {"text": "Recently researches have shown that pretraining unsupervised language model can be very effective for learning universal language representations by leveraging large amounts of unlabeled data, e.g. the pre-trained Bidirectional Encoder Representations from Transformers (BERT).", "labels": [], "entities": []}, {"text": "It has shown that BERT can be fine-tuned to create state-of-the-art models fora range of NLU tasks, such as question answering: An overall framework and pipeline of our system for suggestion mining and natural language inference.", "labels": [], "entities": [{"text": "BERT", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9915804266929626}, {"text": "question answering", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.8675037622451782}, {"text": "suggestion mining", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.7560794949531555}, {"text": "natural language inference", "start_pos": 202, "end_pos": 228, "type": "TASK", "confidence": 0.6581379373868307}]}, {"text": "To further make use of the model in our task, various of different task specified layers are devised.", "labels": [], "entities": []}, {"text": "The experiment on test datasets shows that with the devised task specified layers, a higher f1 scores can begot in both tasks, and moreover, benefiting from the large amount of unlabeled data, it is very easy to train cross domain models.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 describes the key models proposed for the SemEval 2019 Task 9 (.", "labels": [], "entities": [{"text": "SemEval 2019 Task 9", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8819960057735443}]}, {"text": "Section 3 shows the experiment details including dataset preprocessing method, experiment configurations, threshold selection strategy and the alternatives we explored with respect to sublayers and their combination, and performances of different models.", "labels": [], "entities": []}, {"text": "Finally, we conclude our analysis of the challenge, as well as some additional discussions of the future directions in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The statistics of datasets provided by SemEval 2019 Task 9 are show in.", "labels": [], "entities": [{"text": "SemEval 2019 Task 9", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.6924693435430527}]}, {"text": "In both subtasks, no extra data are used for training models.", "labels": [], "entities": []}, {"text": "As shown in, there are 8500   train examples, 592 labeled trial examples and 833 unlabeled test examples in subtask A.", "labels": [], "entities": []}, {"text": "Different from subtask A, there are only 808 labeled trial examples, and 824 unlabeled test examples in subtask B, no training data from same domain as test data is provided.", "labels": [], "entities": []}, {"text": "So, we use all labeled data in subtask A as the training data to do a transfer learning task to help learn subtask B.", "labels": [], "entities": []}, {"text": "In both subtasks, the trial sets are used to help select the best model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: SubtaskA models performances. CV f1- score is used to record cross validation dev set  scores, and the test score is generated by trained  model predicting on released labeled test data.", "labels": [], "entities": [{"text": "CV f1- score", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.6777748167514801}]}, {"text": " Table 3: Subtask B models performances. We use  labeled data from subtask A as training set, sub- task B trial data as dev set to select best hyper- parameters, and test score is generated by trained  model predicting on released labeled test data", "labels": [], "entities": []}]}