{"title": [{"text": "BUT-FIT at SemEval-2019 Task 7: Determining the Rumour Stance with Pre-Trained Deep Bidirectional Transformers", "labels": [], "entities": [{"text": "BUT-FIT", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9674652814865112}, {"text": "Determining the Rumour Stance", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.669114388525486}]}], "abstractContent": [{"text": "This paper describes our system submitted to SemEval 2019 Task 7: RumourEval 2019: Determining Rumour Veracity and Support for Rumours, Subtask A (Gorrell et al., 2019).", "labels": [], "entities": [{"text": "SemEval 2019 Task", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8588305115699768}, {"text": "Determining Rumour Veracity", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.7474424441655477}]}, {"text": "The challenge focused on classifying whether posts from Twitter and Reddit support, deny, query, or comment a hidden rumour, truthfulness of which is the topic of an underlying discussion thread.", "labels": [], "entities": []}, {"text": "We formulate the problem as a stance classification, determining the rumour stance of a post with respect to the previous thread post and the source thread post.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.7264317870140076}]}, {"text": "The recent BERT architecture was employed to build an end-to-end system which has reached the F1 score of 61.67 % on the provided test data.", "labels": [], "entities": [{"text": "BERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9458870887756348}, {"text": "F1 score", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.990321695804596}]}, {"text": "Without any hand-crafted feature, the system finished at the 2 nd place in the competition, only 0.2 % behind the winner.", "labels": [], "entities": []}], "introductionContent": [{"text": "Fighting false rumours at the internet is a tedious task.", "labels": [], "entities": [{"text": "Fighting false rumours", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9013853271802267}]}, {"text": "Sometimes, even understanding what an actual rumour is about may prove challenging.", "labels": [], "entities": [{"text": "understanding what an actual rumour is about", "start_pos": 16, "end_pos": 60, "type": "TASK", "confidence": 0.8687035867146083}]}, {"text": "And only then one can actually judge its veracity with an appropriate evidence.", "labels": [], "entities": [{"text": "veracity", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8973014950752258}]}, {"text": "The works of and Enayet and El-Beltagy (2017) focused on predictions of rumour veracity in thread discussions.", "labels": [], "entities": []}, {"text": "These works indicated that the veracity is correlated with discussion participants' stances towards the rumour.", "labels": [], "entities": [{"text": "veracity", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9942448735237122}]}, {"text": "Following this, the SubTask A SemEval-2019 Task 7 consisted in classifying whether the stance of each post in a given Twitter or Reddit thread supports, denies, queries or comments a hidden rumour.", "labels": [], "entities": [{"text": "SubTask A SemEval-2019 Task", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7060917243361473}]}, {"text": "Potential applications of such a function are wide, ranging from an analysis of popular events (political discussions, academy awards, etc.) to quickly disproving fake news during disasters.", "labels": [], "entities": [{"text": "analysis of popular events (political discussions, academy awards", "start_pos": 68, "end_pos": 133, "type": "TASK", "confidence": 0.8627988159656524}]}, {"text": "Stance classification (SC), in its traditional form, is concerned with determining the attitude of a source text towards a target text (.", "labels": [], "entities": [{"text": "Stance classification (SC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9366079688072204}]}, {"text": "It has been studied thoroughly for discussion threads (.", "labels": [], "entities": []}, {"text": "However, the objective of SubTask A SemEval-2019 Task 7 is to determine the stance to a hidden rumour which is not explicitly given (it can be often inferred from the source post of the discussion -the root of the tree-shaped discussion thread -as demonstrated in).", "labels": [], "entities": [{"text": "SubTask A SemEval-2019 Task 7", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.8099810957908631}]}, {"text": "The competitors were asked to classify the stance of the source post itself too.", "labels": [], "entities": []}, {"text": ".@AP I demand you retract the lie that people in #Ferguson were shouting \"kill the police\", local reporting has refuted your ugly racism The provided dataset was collected from Twitter and Reddit tree-shaped discussions.", "labels": [], "entities": [{"text": "Ferguson", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.8624590635299683}]}, {"text": "Stance labels were obtained via crowdsourcing.", "labels": [], "entities": []}, {"text": "The discussions deal with 9 recently popular topics -Sydney siege, Germanwings crash etc.", "labels": [], "entities": [{"text": "Sydney siege", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.7384375035762787}, {"text": "Germanwings crash", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9128652811050415}]}, {"text": "The approach followed in our work builds on recent advances in language representation models.", "labels": [], "entities": [{"text": "language representation", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.7020251750946045}]}, {"text": "We fine-tune a pre-trained end-toend BERT (Bidirectional Encoder Representations from Transformers) model, while using discussion's source post, target's previous post and the target post itself as inputs to determine the rumour stance of the target post.", "labels": [], "entities": [{"text": "BERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.67951899766922}]}, {"text": "Our implementation is available online.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented our models in PyTorch, taking advantage of the Hugging Face re-implementation (see Footnote 5), with the \"BERT-large-uncased\" setting, pre-trained using 24 transformer layers, having the hidden unit size of d = 1024, 16 attention heads, and 335M parameters.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9150335788726807}, {"text": "BERT-large-uncased", "start_pos": 121, "end_pos": 139, "type": "METRIC", "confidence": 0.9971869587898254}]}, {"text": "When building the ensemble, we picked learning rates from the interval [1e\u22126, 2e\u22126].", "labels": [], "entities": []}, {"text": "Each epoch iterates over the dataset in an ordered manner, starting by the shortest sequence.", "labels": [], "entities": []}, {"text": "We truncate sequences at maximum length l = 200 with a heuristic -firstly we truncate the document 1 to length l/2, if that is not enough, then we truncate the document 2 to  BERT big\u2212nosrc and BERT big\u2212noprev denote system instantiations with an empty source and an empty target post, respectively.", "labels": [], "entities": [{"text": "BERT", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9893201589584351}, {"text": "BERT", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.9855318665504456}]}, {"text": "Note that the accuracy is biased towards different training data priors as shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995989203453064}]}, {"text": "SemEval submissions are denoted by * . the same size.", "labels": [], "entities": [{"text": "SemEval submissions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9178424179553986}]}, {"text": "We keep the batch size of 32 examples and keep other hyperparameters the same as in the BERT paper.", "labels": [], "entities": [{"text": "BERT paper", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.8664117455482483}]}, {"text": "We use the same Adam optimizer with the L2 weight decay of 0.01 and no warmup.", "labels": [], "entities": [{"text": "L2 weight decay", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.7077638308207194}]}, {"text": "We trained the model on the GeForce RTX 2080 Ti GPU.", "labels": [], "entities": [{"text": "GeForce RTX 2080 Ti GPU", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.9170368432998657}]}, {"text": "The dataset contains a whole tree structure and metadata for each discussion from Twitter and Reddit.", "labels": [], "entities": []}, {"text": "The nature of the data differs across the sources (for example, the Reddit subset includes upvotes).", "labels": [], "entities": []}, {"text": "When analysing the data, we spotted several anomalies: \u2022 12 data points do not contain any text.", "labels": [], "entities": []}, {"text": "According to the task organizers, they were deleted by users at the time of data download and been left in the data not to break the conversational structure.", "labels": [], "entities": []}, {"text": "\u2022 The query stance of some examples taken from subreddit DebunkThis 9 is dependent on the domain knowledge.", "labels": [], "entities": [{"text": "DebunkThis 9", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.8846196532249451}]}, {"text": "The class of some examples is ambiguous; they should be probably labelled by multiple classes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Distribution of examples across classes  in the training/development/test data set. The ex- amples belong to 327/38/81 training/development/test  tree-structured discussions.", "labels": [], "entities": []}, {"text": " Table 2: Overview of the results. The values for each single model were obtained by averaging results of 11 mod- els. We report the mean and the standard deviation in these cases. #\u0398 denotes the number of parameters. Columns  F1 S to F1 C report individual F1 scores for each class. All ensemble models have the F1 score optimized on  the development dataset. BiLSTM+SelfAtt contains 4.2M parameters, without pre-trained BERT embeddings.", "labels": [], "entities": [{"text": "F1", "start_pos": 258, "end_pos": 260, "type": "METRIC", "confidence": 0.7724722623825073}, {"text": "F1 score", "start_pos": 313, "end_pos": 321, "type": "METRIC", "confidence": 0.9707421958446503}, {"text": "BERT", "start_pos": 422, "end_pos": 426, "type": "METRIC", "confidence": 0.9916330575942993}]}]}