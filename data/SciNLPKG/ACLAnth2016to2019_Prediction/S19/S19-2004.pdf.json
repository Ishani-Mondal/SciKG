{"title": [{"text": "Neural GRANNy at SemEval-2019 Task 2: A combined approach for better modeling of semantic relationships in semantic frame induction", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe our solutions for semantic frame and role induction subtasks of SemEval 2019 Task 2.", "labels": [], "entities": [{"text": "SemEval 2019 Task 2", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.757218211889267}]}, {"text": "Our approaches got the highest scores, and the solution for the frame induction problem officially took the first place.", "labels": [], "entities": [{"text": "frame induction problem", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.839838445186615}]}, {"text": "The main contributions of this paper are related to the semantic frame induction problem.", "labels": [], "entities": [{"text": "semantic frame induction problem", "start_pos": 56, "end_pos": 88, "type": "TASK", "confidence": 0.8449517339468002}]}, {"text": "We propose a combined approach that employs two different types of vector representations: dense representations from hidden layers of a masked language model, and sparse representations based on substitutes for the target word in the context.", "labels": [], "entities": []}, {"text": "The first one better groups synonyms, the second one is better at disambiguating homonyms.", "labels": [], "entities": []}, {"text": "Extending the context to include nearby sentences improves the results in both cases.", "labels": [], "entities": []}, {"text": "New Hearst-like patterns for verbs are introduced that prove to be effective for frame induction.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.8794566094875336}]}, {"text": "Finally, we propose an approach to selecting the number of clusters in agglomera-tive clustering.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semeval-2019 Task 2 consisted of three subtasks, this paper presents solutions to all three which were all performing better than other submitted approaches.", "labels": [], "entities": []}, {"text": "The first solution officially took the first place in the competition, the other two used tuning on the development set provided by the organizers, which was then interpreted as using additional corpora.", "labels": [], "entities": []}, {"text": "Semantic Frame Induction (Subtask A) is the task of grouping target word occurrences in a text corpus according to their frame (meaning and semantic arguments structure).", "labels": [], "entities": [{"text": "Semantic Frame Induction (Subtask A)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8101868203708104}]}, {"text": "Target words are usually verbs, nouns, and adjectives (these have argument structure; however in the shared task dataset only verbs were present).", "labels": [], "entities": []}, {"text": "For instance, the verbs rise, fall and climb in the sentences The dollar is rising, which makes Russian economy unstable and The dollar fell 1% in September after climbing 2% in August should be clustered together, while the verb climb in sentences like People climb mountains should be clustered separately.", "labels": [], "entities": []}, {"text": "For the sake of brevity, occurrences of different words sharing the same frame will be called synonyms, and occurrences of the same word belonging to different frames will be called homonyms.", "labels": [], "entities": []}, {"text": "This may violate the traditional meaning of these terms.", "labels": [], "entities": []}, {"text": "For instance, fall and rise are not considered synonyms in the classical sense.", "labels": [], "entities": []}, {"text": "Semantic Role Induction refers to finding realizations of semantic arguments in text and relating them to corresponding semantic frame slots.", "labels": [], "entities": [{"text": "Semantic Role Induction", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8038694063822428}]}, {"text": "Generic role induction (subtask B.2) requires a small number of frame-independent roles like Agent, Patient, Theme, etc.", "labels": [], "entities": [{"text": "Generic role induction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6207284728686014}]}, {"text": "Frame-specific role induction (subtask B.1) allows labeling arguments of each frame independently from other frames.", "labels": [], "entities": []}, {"text": "For instance, Microsoft in Microsoft bought Github and Google in Google opened new offices should be labeled as the same role in B.2 but maybe labeled differently in B.1.", "labels": [], "entities": []}, {"text": "For further details please refer to.", "labels": [], "entities": []}, {"text": "In this paper, we focused mainly on the Frame Induction subtask.", "labels": [], "entities": [{"text": "Frame Induction", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.855584055185318}]}, {"text": "The main contributions for this subtask are the following.", "labels": [], "entities": []}, {"text": "A combined approach to semantic frame induction is introduced, which clusters dense representations obtained from hidden layers of a masked LM first and sparse bag-ofwords representations of possible substitutes fora word in context afterward.", "labels": [], "entities": [{"text": "semantic frame induction", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6788201928138733}]}, {"text": "This approach resulted in better clustering of both synonyms and homonyms . New Hearst-like patterns designed specifically for verbs were used and they proved to be beneficial for Semantic Frame Induction.", "labels": [], "entities": [{"text": "Semantic Frame Induction", "start_pos": 180, "end_pos": 204, "type": "TASK", "confidence": 0.8360121250152588}]}, {"text": "Also, a simple but effective semi-supervised approach to selecting the number of clusters for agglomerative clustering was proposed.", "labels": [], "entities": []}, {"text": "Finally, we proposed ex-tending context with neighboring sentences which have shown consistent improvements for both of our representations.", "labels": [], "entities": []}, {"text": "For solving subtask B.2 we used a semi-supervised approach of training logistic regression over features that were partly designed and partly learned in an unsupervised fashion.", "labels": [], "entities": [{"text": "solving subtask B.2", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7676641345024109}]}, {"text": "To ensure the best performance on verbs that were not present in training data (the majority of examples in the test) we used cross-validation with a lexical split, to select optimal features and hyperparameters.", "labels": [], "entities": []}, {"text": "For solving subtask B.1 we trivially reused labels from B.2", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to limitations imposed by the task, we restricted ourselves to only using labeled data provided by the organizers.", "labels": [], "entities": []}, {"text": "For the majority of our experiments, we used the development set that consisted of 600 examples of 35 verbs clustered into 41 frames.", "labels": [], "entities": []}, {"text": "There are many examples of synonymy in this dataset but not so many of homonymy.", "labels": [], "entities": []}, {"text": "Almost all ambiguous verbs have less than 5 examples for all frames except their most frequent frame, hence we used only verbs join and believe (54/9 and 12/8 examples of their first/second most frequent frame respectively) to select hyperparameters likely resulting in a suboptimal performance on the test.", "labels": [], "entities": []}, {"text": "For internal evaluation of different representations and hyperparameters selection, we used the following procedure: the development set or its subset was clustered many times using agglomerative clustering with all feasible hyperparameter values, and maximum BCubed-f1 value (maxB3f1) was taken as a score for the representation.", "labels": [], "entities": []}, {"text": "This allowed us to compare clusterability of different representations while avoiding problems of selecting the number of clusters and other hyperparameters.", "labels": [], "entities": []}, {"text": "Of course, there is a possibility that other clustering algorithms might perform better with different representations, however, we didn't see improvements from using other clustering algorithms and stick to agglomerative clustering.: Sparse vs. dense representations, maxB3f1 Figure 1: Recall for synonyms and homonyms w.r.t. number of clusters for dense and sparse representations an appropriate pattern are better for disambiguating homonyms.", "labels": [], "entities": []}, {"text": "We denote the proportion of synonyms sharing common cluster as recall for synonyms and the proportion of homonyms put in separate clusters as recall for homonyms.", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9888157844543457}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9571275115013123}]}, {"text": "shows both metrics depending on the number of clusters for agglomerative clustering of the whole development set.", "labels": [], "entities": []}, {"text": "It is evident that until a relatively large number of clusters (30) almost all synonyms are correctly clustered together when using dense representations, yet homonyms are clustered together as well, which gives almost 1.0 recall for synonyms and nearly 0.0 recall for homonyms.", "labels": [], "entities": [{"text": "recall", "start_pos": 223, "end_pos": 229, "type": "METRIC", "confidence": 0.9964635968208313}, {"text": "recall", "start_pos": 258, "end_pos": 264, "type": "METRIC", "confidence": 0.9964513778686523}]}, {"text": "MaxB3f1 of approximately 0.94 is achieved at around 25-28 clusters (depending on the context size) where synonyms are still clustered almost perfectly.", "labels": [], "entities": [{"text": "MaxB3f1", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7057775855064392}]}, {"text": "At the same time, sparse representations split homonyms into different clusters even at very small numbers of clusters, but simultaneously split synonyms also, achieving lower maxB3f1 of 0.91 in a wider range of 25-40 clusters.", "labels": [], "entities": []}, {"text": "To solve this problem, our final solution clusters dense representations first and then splits large clusters containing examples of the same verb (to prevent splitting synonyms) into a small number of clusters to improve recall for homonyms.: Subtask-A, effect of pattern and context size synonyms are never clustered together.", "labels": [], "entities": [{"text": "recall", "start_pos": 222, "end_pos": 228, "type": "METRIC", "confidence": 0.9934905767440796}]}, {"text": "Dense representation with semi-supervised agglomerative clustering slightly underestimates the number of clusters in the test set (similarly to the development set) resulting in the highest recall due to merged synonyms.", "labels": [], "entities": [{"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9989646673202515}]}, {"text": "The combined approach splits some clusters hurting BCubed-recall a bit but increasing BCubed-precision, even more, resulting in better BCubed-f1.", "labels": [], "entities": []}, {"text": "The last row shows that selecting the number of clusters which maximizes silhouette score (unsupervised approach) instead of BCubed-f1 of the labeled subset results in much worse results, hence our semi-supervised approach is beneficial.", "labels": [], "entities": [{"text": "silhouette score", "start_pos": 73, "end_pos": 89, "type": "METRIC", "confidence": 0.9203682839870453}]}, {"text": "Finally, we noticed that the largest cluster had all the examples of both sell and buy, which were among the most frequent verbs in the test set.", "labels": [], "entities": []}, {"text": "In FrameNet, they are assigned to Commerce sell and Commerce buy frames respectively which is a questionable solution since these are just different ways to put into words the same type of event with the same participants (something like commercial-transfer-of-property).", "labels": [], "entities": []}, {"text": "We simply moved all examples of the verb sell into a separate cluster which gave significant improvement in BCubed-f1.", "labels": [], "entities": [{"text": "BCubed-f1", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.5186896324157715}]}, {"text": "However, this result is out of competition due to the manual postprocessing.", "labels": [], "entities": []}, {"text": "Yet, our best result without manual postprocessing is still ranked first.", "labels": [], "entities": []}, {"text": "In we report the results of clustering the test set depending on the pattern and the con-text size used to build sparse representations at phase 2.", "labels": [], "entities": []}, {"text": "In addition to standard metrics, we report maxB3F1 which excludes the effect of a suboptimal number of clusters selected on the comparison results.", "labels": [], "entities": [{"text": "maxB3F1", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9951818585395813}]}, {"text": "Our proposed pattern seems to give small but consistent improvement as well as context extension.", "labels": [], "entities": [{"text": "context extension", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7790564000606537}]}, {"text": "The context of 1-3 sentences on both sides is a reasonable choice for sparse representations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sparse vs. dense representations, maxB3f1", "labels": [], "entities": []}, {"text": " Table 2: Subtask-A, results on test. for post-eval re- sults, @ for manual postprocessing (out of competition)", "labels": [], "entities": []}, {"text": " Table 3: Subtask-A, effect of pattern and context size", "labels": [], "entities": []}, {"text": " Table 4: Subtask-B.2, results on test", "labels": [], "entities": []}]}