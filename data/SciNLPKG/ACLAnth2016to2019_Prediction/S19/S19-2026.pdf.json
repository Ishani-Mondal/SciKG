{"title": [{"text": "CoAStaL at SemEval-2019 Task 3: Affect Classification in Dialogue using Attentive BiLSTMs", "labels": [], "entities": [{"text": "Affect Classification in Dialogue", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.7819589897990227}]}], "abstractContent": [{"text": "This work describes the system presented by the CoAStaL Natural Language Processing group at University of Copenhagen.", "labels": [], "entities": [{"text": "CoAStaL Natural Language Processing", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.6550686955451965}]}, {"text": "The main system we present uses the same attention mechanism presented in (Yang et al., 2016).", "labels": [], "entities": []}, {"text": "Our overall model architecture is also inspired by their hierarchical classification model and adapted to deal with classification in dialogue by encoding information at the turn level.", "labels": [], "entities": []}, {"text": "We use different encodings for each turn to create a more expressive representation of dialogue context which is then fed into our classifier.", "labels": [], "entities": []}, {"text": "We also define a custom preprocessing step in order to deal with language commonly used in interactions across many social media outlets.", "labels": [], "entities": []}, {"text": "Our proposed system achieves a micro F1 score of 0.7340 on the test set and shows significant gains in performance compared to a system using dialogue level encoding.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9463203847408295}]}], "introductionContent": [{"text": "Recognizing emotion is crucial to human-human communication and has fora longtime been a goal in human-machine interaction.", "labels": [], "entities": [{"text": "Recognizing emotion", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8971698880195618}]}, {"text": "Although there has been growing interest in emotion detection across many fields (), much of the work has focused on developing empathetic systems using multimodal approaches i.e. speech and gestures as well as text ().", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7212592363357544}]}, {"text": "Approaching emotion detection as a multimodal problem certainly makes sense, as face-face human communication involves many modalities, however, this fails to consider all the communication that is increasingly happening solely via chat, or written means.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7576920688152313}]}, {"text": "Detecting emotion in textual dialogue without the other modalities, such as work done by Gupta et al., can allow us to improve a number of applications dealing with social media * Authors contributed equally interactions, opinion mining, and customer interactions, unfortunately, this is a great challenge that has remained largely unexplored.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 222, "end_pos": 236, "type": "TASK", "confidence": 0.7659091055393219}]}, {"text": "SemEval 2019 Task 3 attempts to encourage research in this direction.", "labels": [], "entities": [{"text": "SemEval 2019 Task 3", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.835045114159584}]}, {"text": "Given a user utterance and the previous two turns of context, the task consists in classifying the user utterance according to one of four emotion classes: happy, sad, angry or other.", "labels": [], "entities": []}, {"text": "For a full description of the task see (.", "labels": [], "entities": []}, {"text": "In this paper, we describe our turn-level attention model used to tackle this task, specifically using the attention mechanism presented in (.", "labels": [], "entities": []}, {"text": "Our model encodes turns in a conversation separately using an Attentive Bidirectional LSTM encoder.", "labels": [], "entities": []}, {"text": "In the model presented in the shared task, the turn encoders do not share parameters, achieving a micro F1 score of 0.7340.", "labels": [], "entities": [{"text": "micro F1 score", "start_pos": 98, "end_pos": 112, "type": "METRIC", "confidence": 0.801864743232727}]}, {"text": "The code for all experiments presented here is available.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The table shows the results of our models on  the EmoContext shared task test set.", "labels": [], "entities": [{"text": "EmoContext shared task test set", "start_pos": 60, "end_pos": 91, "type": "DATASET", "confidence": 0.8418396949768067}]}, {"text": " Table 1. From the re- sults we can observe that our proposed attentive  turn-level BiLSTM outperforms all baselines, in- cluding the task organizers LSTM model, with a  Micro F1 score of 0.7340. What is interesting  to note is that almost all of our proposed sim- ple SVM baselines also outperforms the baseline  LSTM, with even TURN-TFIDF-ADDED by a sig- nificant margin. In general we see that encoding  the dialogue on the turn level achieves better per- formance than its dialogue level counterparts.", "labels": [], "entities": [{"text": "Micro F1 score", "start_pos": 170, "end_pos": 184, "type": "METRIC", "confidence": 0.6870916585127512}]}, {"text": " Table 2: F1, precision and recall scores for each of  the emotion classes for two of our proposed models,  BILSTM-ATT and BILSTM-ATT-DIA", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.999562680721283}, {"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9996113181114197}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9965198040008545}, {"text": "BILSTM-ATT", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9607320427894592}, {"text": "BILSTM-ATT-DIA", "start_pos": 123, "end_pos": 137, "type": "METRIC", "confidence": 0.8618807792663574}]}]}