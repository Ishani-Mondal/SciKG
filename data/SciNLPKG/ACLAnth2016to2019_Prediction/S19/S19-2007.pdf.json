{"title": [{"text": "SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter", "labels": [], "entities": [{"text": "Multilingual Detection of Hate Speech", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.8867836952209472}]}], "abstractContent": [{"text": "The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter.", "labels": [], "entities": [{"text": "SemEval 2019 Task", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.910267968972524}, {"text": "detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter", "start_pos": 74, "end_pos": 182, "type": "TASK", "confidence": 0.6337047070264816}]}, {"text": "The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group.", "labels": [], "entities": []}, {"text": "HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams.", "labels": [], "entities": []}, {"text": "Data provided for the task are described by showing how they have been collected and annotated.", "labels": [], "entities": []}, {"text": "Moreover, the paper provides an analysis and discussion about the participant systems and the results they achieved in both subtasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hate Speech (HS) is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics.", "labels": [], "entities": [{"text": "Hate Speech (HS) is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other", "start_pos": 0, "end_pos": 221, "type": "Description", "confidence": 0.86514366227527}]}, {"text": "Given the huge amount of user-generated contents on the Web, and in particular on social media, the problem of detecting, and therefore possibly contrasting the HS diffusion, is becoming fundamental, for instance for fighting against misogyny and xenophobia.", "labels": [], "entities": [{"text": "HS diffusion", "start_pos": 161, "end_pos": 173, "type": "TASK", "confidence": 0.8642595708370209}]}, {"text": "Some key aspects feature online HS, such as virality, or presumed anonymity, which distinguish it from offline communication and make it potentially also more dangerous and hurtful.", "labels": [], "entities": []}, {"text": "Often hate speech fosters discrimination against particular categories and undermines equality, an everlasting issue for each civil society.", "labels": [], "entities": [{"text": "equality", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.951893150806427}]}, {"text": "Among the mainly targeted categories there are immigrants and women.", "labels": [], "entities": []}, {"text": "For the first target, especially raised by refugee crisis and political changes occurred in the last few years, several governments and policy makers are currently trying to address it, making especially interesting the development of tools for the identification and monitoring such kind of hate ( . For the second one instead, hate against the female gender is a long-time and well-known form of discrimination.", "labels": [], "entities": []}, {"text": "Both these forms of hate content impact on the development of society and maybe confronted by developing tools that automatically detect them.", "labels": [], "entities": []}, {"text": "A large number of academic events and shared tasks for different languages (i.e. English, Spanish, Italian, German, Mexican-Spanish, Hindi) took place in the very recent past which are centered on HS and related topics, thus reflecting the interest by the NLP community.", "labels": [], "entities": [{"text": "HS and related topics", "start_pos": 197, "end_pos": 218, "type": "TASK", "confidence": 0.8345894515514374}]}, {"text": "Let us mention the first and second edition of the Workshop on Abusive Language 1 (, the First Workshop on Trolling, Aggression and Cyberbullying (, that also included a shared task on aggression identification, the tracks on Automatic Misogyny Identification (AMI)) and on Authorship and Aggressiveness Analysis (MEX-A3T) proposed at the 2018 edition of IberEval 2 , the GermEval Shared Task on the Identification of Offensive Language (, and finally the Automatic Misogyny Identification task (AMI)) and the Hate Speech Detection task (HaSpeeDe) ( ) at EVALITA 2018 3 for investigating respectively misogyny and HS in Italian.", "labels": [], "entities": [{"text": "aggression identification", "start_pos": 185, "end_pos": 210, "type": "TASK", "confidence": 0.7283251583576202}, {"text": "Authorship and Aggressiveness Analysis (MEX-A3T)", "start_pos": 274, "end_pos": 322, "type": "TASK", "confidence": 0.7081378102302551}, {"text": "Hate Speech Detection task (HaSpeeDe)", "start_pos": 510, "end_pos": 547, "type": "METRIC", "confidence": 0.6559082993439266}]}, {"text": "HatEval consists in detecting hateful contents in social media texts, specifically in Twitter's posts, against two targets: immigrants and women.", "labels": [], "entities": [{"text": "detecting hateful contents in social media texts", "start_pos": 20, "end_pos": 68, "type": "TASK", "confidence": 0.7916968635150364}]}, {"text": "Moreover, the task implements a multilingual perspective where data for two widespread languages, English and Spanish, are provided for training and testing participant systems.", "labels": [], "entities": []}, {"text": "The motivations for organizing HatEval go beyond the advancement of the state of the art for HS detection for each of the involved languages and targets.", "labels": [], "entities": [{"text": "HS detection", "start_pos": 93, "end_pos": 105, "type": "TASK", "confidence": 0.9785992205142975}]}, {"text": "The variety of targets of hate and languages provides a unique comparative setting, both with respect to the amount of data collected and annotated applying the same scheme, and with respect to the results achieved by participants training their systems on those data.", "labels": [], "entities": []}, {"text": "Such comparative setting may help in shedding new light on the linguistic and communication behaviour against these targets, paving the way for the integration of HS detection tools in several application contexts.", "labels": [], "entities": [{"text": "HS detection", "start_pos": 163, "end_pos": 175, "type": "TASK", "confidence": 0.9775989055633545}]}, {"text": "Moreover, the participation of a very large amount of research groups in this task (see Section 4) has improved the possibility of in-depth investigation of the involved phenomena.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, the datasets released to the participants for training and testing the systems are described.", "labels": [], "entities": []}, {"text": "Section 3 presents the two subtasks and the measures we exploited in the evaluation.", "labels": [], "entities": []}, {"text": "Section 4 reports on approaches and results of the participant systems.", "labels": [], "entities": []}, {"text": "In Section 5, a preliminary analysis of common errors in top-ranked systems is proposed.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of the results considers different strategies and metrics for Subtasks A and B in order to allow more fine-grained scores.", "labels": [], "entities": []}, {"text": "Subtask A. Systems will be evaluated using standard evaluation metrics, including Accuracy, Precision, Recall and macro-averaged F 1 -score.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.999497652053833}, {"text": "Precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9952220320701599}, {"text": "Recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9939332604408264}, {"text": "F 1 -score", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9364084750413895}]}, {"text": "In order to provide a measure that is independent on the class size, the submissions will be ranked by macro-averaged F 1 -score, computed as described in).", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9129566997289658}]}, {"text": "The metrics will be computed as follows: Accuracy = number of correctly predicted instances total number of instances P recision = number of correctly predicted instances number of predicted labels Recall = number of correctly predicted labels number labels in the gold standard Subtask B. The evaluation of systems participating to Subtask B will be based on two criteria: (1) partial match and (2) exact match.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9993446469306946}, {"text": "Recall", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.9680609107017517}, {"text": "exact match", "start_pos": 400, "end_pos": 411, "type": "METRIC", "confidence": 0.9635775089263916}]}, {"text": "Regarding the partial match, each dimension to be predicted (HS , TR and AG) will be evaluated independently from the others using standard evaluation metrics, including accuracy, precision, recall and macro-averaged F 1 -score.", "labels": [], "entities": [{"text": "HS", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.8135718703269958}, {"text": "TR", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.8696316480636597}, {"text": "AG", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9746729135513306}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9993064403533936}, {"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9985758066177368}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9990888833999634}, {"text": "F 1 -score", "start_pos": 217, "end_pos": 227, "type": "METRIC", "confidence": 0.9602987319231033}]}, {"text": "We will report to the participants all the measures and a summary of the performance in terms of macro-averaged F 1 -score, computed as follows: Concerning the exact match, all the dimensions to be predicted will be jointly considered computing the Exact Match Ratio ().", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9542258232831955}, {"text": "Exact Match Ratio", "start_pos": 249, "end_pos": 266, "type": "METRIC", "confidence": 0.7032694419225057}]}, {"text": "Given the multi-label dataset consisting of n multi-label samples (x i , Y i ), where xi denotes the i-th instance and Y i represents the corresponding set of labels to be predicted (HS \u2208 {0, 1}, TR \u2208 {0, 1} and AG \u2208 {0, 1}), the Exact Match Ratio (EMR) will be computed as follows: where Z i denotes the set of labels predicted for the i-th instance and I is the indicator function.", "labels": [], "entities": [{"text": "Exact Match Ratio (EMR)", "start_pos": 230, "end_pos": 253, "type": "METRIC", "confidence": 0.7686939438184103}]}, {"text": "The submissions will be ranked by EMR.", "labels": [], "entities": [{"text": "EMR", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7944238185882568}]}, {"text": "This choice is motivated by the willingness to capture the difficulty of modeling the entire phenomenon, and therefore to identify the most dangerous behaviours against the targets.", "labels": [], "entities": []}, {"text": "In order to provide a benchmark for the comparison of the submitted systems, we considered two different baselines.", "labels": [], "entities": []}, {"text": "The first one (MFC baseline) is a trivial model that assigns the most frequent label, estimated on the training set, to all the instances in the test set.", "labels": [], "entities": []}, {"text": "The second one (SVC baseline) is a linear Support Vector Machine (SVM) based on a TF-IDF representation, where the hyper-parameters are the default values set by the scikit-learn Python library (Pedregosa et al., 2011).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution percentages across sets and cate- gories for English data. The percentages for the target  and aggressiveness categories are computed on the to- tal number of hateful tweets.", "labels": [], "entities": [{"text": "English data", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.8007987141609192}]}, {"text": " Table 2: Distribution percentages across sets and cate- gories for Spanish data. The percentages for the target  and aggressiveness categories are computed on the to- tal number of hateful tweets.", "labels": [], "entities": [{"text": "Spanish data", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.727385625243187}]}, {"text": " Table 3: Basic statistics of the results for the partici- pating system and baselines in Subtask A and Subtask  B expressed in terms of macro-averaged F 1 -score and  EMR respectively.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 152, "end_pos": 162, "type": "METRIC", "confidence": 0.9550336301326752}, {"text": "EMR", "start_pos": 168, "end_pos": 171, "type": "METRIC", "confidence": 0.9854082465171814}]}, {"text": " Table 4: Number of instances mislabeled by all the  three top-ranked systems, broken down by wrongly as- signed label.", "labels": [], "entities": []}]}