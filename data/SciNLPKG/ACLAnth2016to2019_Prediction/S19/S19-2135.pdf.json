{"title": [], "abstractContent": [{"text": "This article presents our approach for detecting a target of offensive messages in Twit-ter, including Individual, Group and Others classes.", "labels": [], "entities": []}, {"text": "The model we have created is an ensemble of simpler models, including Logistic Regression, Naive Bayes, Support Vector Machine and the interpolation between Logistic Regression and Naive Bayes with 0.25 coefficient of interpolation.", "labels": [], "entities": []}, {"text": "The model allows us to achieve 0.547 macro F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9341618418693542}]}], "introductionContent": [{"text": "Nowadays aggressive language on social media occurs more and more often.", "labels": [], "entities": []}, {"text": "Categories of hate speech can be very diverse and can deal with a wide range of issues such as misogyny, sexual orientation, religion and immigration.", "labels": [], "entities": []}, {"text": "Such types of speech can be found in posts in social networks, in Internet discussions, in comments on various articles and in responses to posts of famous persons.", "labels": [], "entities": []}, {"text": "This problem is receiving increasing amounts of attention and researchers are making attempts to build systems capable of recognizing such kinds of aggressive speech, offenses and insults in social networks.", "labels": [], "entities": []}, {"text": "This article presented our approach to hate speech detection, which we used for the challenge SemEval-2019 Task 6: OffensEval -Identifying and Categorizing Offensive Language in Social Media (),().", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7037552793820699}, {"text": "SemEval-2019 Task 6", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.827337384223938}, {"text": "OffensEval -Identifying and Categorizing Offensive Language in Social Media", "start_pos": 115, "end_pos": 190, "type": "TASK", "confidence": 0.6559143245220185}]}, {"text": "The task consisted of three sub-tasks and proposed to investigate the data extracted from Twitter for creating a classification system.", "labels": [], "entities": []}, {"text": "Sub-task A had the aim to identify offensive language and there were 860 unmarked English tweets for testing.", "labels": [], "entities": []}, {"text": "The post had to be non offensive if it did not contain any offense or profanity.", "labels": [], "entities": []}, {"text": "The main goal of the Sub-task B was to categorize offensive posts from Sub-task A (there were 240 English tweets for testing) to different offensive types: -Targeted Insults and Threats in cases when a post insults or treats to an individual, a group or an organization; -Untargeted in cases where a post has a non-targeting profanity and swearing.", "labels": [], "entities": []}, {"text": "Sub-task C focused on offense target identification.", "labels": [], "entities": [{"text": "offense target identification", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.8104906678199768}]}, {"text": "There were 213 English tweets which were marked as offensive in Sub-task A and Targeted Insult and Threats in Sub-task B for testing.", "labels": [], "entities": []}, {"text": "The classification was for three different groups: -Individual, when the target of the offensive post was a person; -Group, when the target of the offensive message was a group of people considered as a unit; -Other, when the target of the offensive tweet did not belong to any of the previous categories (e.g., a situation, an event, or another issue).", "labels": [], "entities": []}, {"text": "There are two datasets in English and in Spanish languages for analysis, and our team worked with English only.", "labels": [], "entities": []}, {"text": "The training dataset included 13200 tweets, 4400 of them were offensive ones, 3876 messages were labeled as 'Target Insult and Threats' and 524 ones as 'Untargeted'.", "labels": [], "entities": []}, {"text": "We focused our efforts on Sub-task C only, and the training dataset for it consisted of 2407 'individual' offensive posts, 1074 'group' ones and 395 tweets marked as 'other'.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Some relevant related works in the area are described in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 presents the preprocessing we applied for the dataset and the methodology we used for the model creating.", "labels": [], "entities": []}, {"text": "In Section 4 the results are described and analyzed.", "labels": [], "entities": []}, {"text": "In Section 5 we summarize our work and plan some steps for the future researches.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for each model with training dataset for the Subtask C", "labels": [], "entities": []}, {"text": " Table 2: Results of the classification with testing dataset for the Subtask C", "labels": [], "entities": []}]}