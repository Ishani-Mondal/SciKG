{"title": [{"text": "A Semantic Cover Approach for Topic Modeling", "labels": [], "entities": [{"text": "Topic Modeling", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8818902373313904}]}], "abstractContent": [{"text": "We introduce a novel topic modeling approach based on constructing a semantic set cover for clusters of similar documents.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.7487033009529114}]}, {"text": "Specifically , our approach first clusters documents using their Tf-Idf representation, and then covers each cluster with a set of topic words based on semantic similarity, defined in terms of a word embedding.", "labels": [], "entities": []}, {"text": "Computing a topic cover amounts to solving a minimum set cover problem.", "labels": [], "entities": []}, {"text": "Our evaluation compares our topic mod-eling approach to Latent Dirichlet Allocation (LDA) on three metrics: 1) qualitative topic match, measured using evaluations by Amazon Mechanical Turk (MTurk) workers, 2) performance on classification tasks using each topic model as a sparse feature representation, and 3) topic coherence.", "labels": [], "entities": []}, {"text": "We find that qualitative judgments significantly favor our approach , the method outperforms LDA on topic coherence, and is comparable to LDA on document classification tasks.", "labels": [], "entities": [{"text": "document classification tasks", "start_pos": 145, "end_pos": 174, "type": "TASK", "confidence": 0.7938822309176127}]}], "introductionContent": [{"text": "Topic modeling is one of the core research problems in natural language processing.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8519264757633209}, {"text": "natural language processing", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6386470794677734}]}, {"text": "Approaches to topic modeling range from simple vector comparisons to probabilistic graphical models.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7823507189750671}]}, {"text": "Nevertheless, despite the many approaches proposed over the years, probabilistic topic modeling methods in general, and Latent Dirichlet Allocation (LDA) () in particular, have become arguably the dominant paradigm.", "labels": [], "entities": [{"text": "probabilistic topic modeling", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.6792198618253072}, {"text": "Latent Dirichlet Allocation (LDA", "start_pos": 120, "end_pos": 152, "type": "METRIC", "confidence": 0.7972639799118042}]}, {"text": "For example, it remains the algorithm of choice in the Amazon's healthcare NLP toolkit.", "labels": [], "entities": [{"text": "Amazon's healthcare NLP toolkit", "start_pos": 55, "end_pos": 86, "type": "DATASET", "confidence": 0.7404467463493347}]}, {"text": "However, there have been concerns about the performance of probabilistic models, particularly in the context of datasets comprised of short documents, such as tweets (.", "labels": [], "entities": []}, {"text": "This is primarily because the sparsity posed by short texts makes it hard for the model to sufficiently account for word co-occurrences, which form the basis of the definition of a topic in the sense of a multinomial distribution over words.", "labels": [], "entities": []}, {"text": "Additionally, the language used on Twitter is informal in nature, uses slang and non-dictionary words, and often lacks proper grammatical structure.", "labels": [], "entities": []}, {"text": "Moreover, the complexity of the probabilistic topic modeling approaches makes it difficult to interpret the specific choices they make about topics and their constituent words.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7296378910541534}]}, {"text": "In this paper, we propose a novel approach to topic modeling which is conceptually simple and highly interpretable.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.875645637512207}]}, {"text": "Our approach is based on two hypotheses about the nature of short texts, such as tweets: first, that such texts can be grouped into relatively few disjoint clusters representing a similar mix of subjects (nominally, we call these clusters topics, recognizing that any such cluster maybe comprised of multiple topics), and second, that each such subject mix can be adequately summarized by a small number of concepts (words).", "labels": [], "entities": []}, {"text": "Both of these are distinct from LDA, which models a topic as a probability distribution over a large number of words.", "labels": [], "entities": []}, {"text": "While LDA models each text as a mixture of multiple topics, we assert that each tweet falls into a single cluster.", "labels": [], "entities": []}, {"text": "A more fundamental qualitative distinction of our approach from LDA is that it is deterministic in nature, and admits a much more compact representation of the corpus, since each topic, or cluster, is represented by only a small number of words.", "labels": [], "entities": []}, {"text": "To operationalize our hypotheses, we propose a two-step approach to topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7822756469249725}]}, {"text": "First, we cluster documents based on their similarity in terms of Tf-Idf feature representation.", "labels": [], "entities": []}, {"text": "Second, given the clustering, we attempt to find a set of words for each cluster that forms a description of the cluster.", "labels": [], "entities": []}, {"text": "Specifically, we use a word embedding, along with a document representation in the same semantic space, to cover each cluster with a small set of topic words that are semantically similar to the documents.", "labels": [], "entities": []}, {"text": "More precisely, we say that a word (concept) in a dictionary covers a document if it is among the k most similar words in the semantic embedding space.", "labels": [], "entities": []}, {"text": "To cover a collection of documents thereby becomes a minimum set cover problem instance.", "labels": [], "entities": []}, {"text": "While the set cover problem is computationally hard, it admits a fast greedy approximation algorithm, which we utilize to construct the topic descriptions for each document cluster.", "labels": [], "entities": [{"text": "set cover problem", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8085954387982687}]}, {"text": "Our evaluation combines qualitative and quantitative metrics.", "labels": [], "entities": []}, {"text": "We first qualitatively compare our approach to LDA by asking MTurk subjects for their judgments about the quality of respective choices of topics fora random sample of documents from a cluster.", "labels": [], "entities": [{"text": "LDA", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9585040807723999}]}, {"text": "We do this through two conceptually different ways, and observe a significant and systematic advantage of our approach over LDA.", "labels": [], "entities": []}, {"text": "Quantitatively, we compare our approach and LDA in terms of standard intrinsic topic coherence and performance in text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7918141484260559}]}, {"text": "On the intrinsic topic coherence metric, our approach fares significantly better than LDA for 4 out of the 5 datasets we use, and the two are comparable on the fifth dataset.", "labels": [], "entities": [{"text": "LDA", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.8850497603416443}]}, {"text": "Finally, we consider two classification tasks, spam and hate speech prediction, in which topic modeling is used as a sparse feature representation.", "labels": [], "entities": [{"text": "hate speech prediction", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7176088094711304}]}, {"text": "In this task, we find that both approaches yield similar performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach in comparison with LDA-the de facto standard in topic modelingboth in qualitative and quantitative terms.", "labels": [], "entities": [{"text": "topic modelingboth", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7505224347114563}]}, {"text": "Our qualitative evaluation involves human judgments about the appropriateness of topic choices fora subsample of texts.", "labels": [], "entities": []}, {"text": "We complement this with two quantitative metrics, one with respect to a standard topic coherence measure, and the second in using topic models for text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 147, "end_pos": 172, "type": "TASK", "confidence": 0.8111022114753723}]}, {"text": "Throughout, we refer to our approach asset cover.", "labels": [], "entities": []}, {"text": "Moreover, in our experiments, the Word2Vec vectors are derived by training a skip-gram model on the corpus, with a sliding window of size 4 and the number of dimensions set to 500.", "labels": [], "entities": [{"text": "Word2Vec vectors", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9345338046550751}]}, {"text": "Additionally, we compute the minimum 1-cover (i.e. \u03b4 = 0), that is, we ensure that all documents in the cluster are covered.", "labels": [], "entities": []}, {"text": "Given the common use of topic modeling in obtaining qualitative insight from text, our first evaluation approach involves human judgments of quality.", "labels": [], "entities": []}, {"text": "This evaluation echos other human evaluations of topic modeling, such as by for the topic-intrusion detection task.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.8131289482116699}, {"text": "topic-intrusion detection task", "start_pos": 84, "end_pos": 114, "type": "TASK", "confidence": 0.8343533873558044}]}, {"text": "Also noteworthy is the work by, who demonstrated the poor correlation of the popular perplexity metric () with human judgments.", "labels": [], "entities": []}, {"text": "For our qualitative evaluation, we setup a series of experiments on Amazon Mechanical Turk (MTurk).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 68, "end_pos": 98, "type": "DATASET", "confidence": 0.9252187609672546}]}, {"text": "For these tasks, we use 4 sets from the health news tweets collected by and YouTube comments about 23andMe (we provide specific details in a later section).", "labels": [], "entities": []}, {"text": "To ensure fairness to LDA-our chosen baseline-we do this in two different settings based on how we group documents into topically related subsets.", "labels": [], "entities": []}, {"text": "In the first set of MTurk experiments, where topics from both algorithms were shown in the same task, we asked human judges to score topics from 4 document clusters, collecting 20 responses for each.", "labels": [], "entities": []}, {"text": "For instance, the Fox News dataset contains a set of tweets posted in 2015 about the measles outbreak in California, linked to Disney theme parks.", "labels": [], "entities": [{"text": "Fox News dataset", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.9617912371953329}]}, {"text": "The topic words for the measles outbreak cluster identified by the two algorithms are shown in.", "labels": [], "entities": []}, {"text": "Here, we see that LDA picks certain irrelevant terms for the shown cluster sample ('u', 'rare'), while completely missing the term 'measles', which is a key subject of the documents (we note that we remove stop-words during preprocessing using NLTK).", "labels": [], "entities": []}, {"text": "The set cover approach, on the other hand, is able to identify highly pertinent words.", "labels": [], "entities": []}, {"text": "We refer to this experiment as Matched Clusters.", "labels": [], "entities": []}, {"text": "This can bethought of as reflecting the propriety of the chosen topic words conditional on the clusters of similar documents.", "labels": [], "entities": []}, {"text": "The average scores are shown in.", "labels": [], "entities": []}, {"text": "Cluster Sample   Our second set of experiments for clusters chosen independently for the two algorithms was conducted on a significantly larger scale.", "labels": [], "entities": []}, {"text": "We uploaded 5 clusters per dataset, and collected 40 responses per cluster, resulting in a total of 2000 data points, 1000 for each algorithm.", "labels": [], "entities": []}, {"text": "We refer to this experiment as Independent Clusters.", "labels": [], "entities": []}, {"text": "shows example topic words identified by LDA and Set Cover.", "labels": [], "entities": []}, {"text": "The advantages of the clustering step in our approach are evident in this example -the set cover cluster contains documents that are more closely related to one another.", "labels": [], "entities": []}, {"text": "More importantly, it is worth noting the choice of the term 'delay' in the set cover topic words -while the term does not itself appear in the entire cluster, it is semantically related to documents in the cluster referring to the long wait Maryland residents had to endure to sign up for Obamacare.", "labels": [], "entities": []}, {"text": "This is precisely the reason for using a wordembedding such as Word2Vec in our approachtopic words are not restricted to words in the cluster and yet appear to be semantically meaningful.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9598268270492554}]}, {"text": "The average judgments from MTurk for these experiments are reported in.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.6693058013916016}]}, {"text": "In both experiments, we can see that set cover consistently outperforms LDA, often by a large margin.", "labels": [], "entities": [{"text": "LDA", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.5458395481109619}]}, {"text": "We also performed a two sided independent samples t-test on the scores.", "labels": [], "entities": []}, {"text": "The differences between the means in are statistically significant; all but 23andMe for p < 0.0001, and the significance for 23andMe is for p < 0.05.", "labels": [], "entities": []}, {"text": "It is interesting to note that set cover performs slightly better in terms of evaluation scores in the Matched Clusters study, suggesting that it is judged favorably particularly in the context of random calibration and LDA.", "labels": [], "entities": []}, {"text": "Since the clusters are fixed in these experiments, the results reflect the particular advantage of the set cover method itself in choosing descriptive words fora collection of similar documents.", "labels": [], "entities": []}, {"text": "The Independent Clusters study, in contrast, serves more as an evaluation of each approach in an end-to-end fashion, and here, too, the difference is substantial.", "labels": [], "entities": [{"text": "Independent Clusters study", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.7378551959991455}]}, {"text": "However, the LDA scores in this case are generally comparable or higher than in the Matched Clusters experiments, which suggests that the advantage of set cover over LDA maybe primarily due to its better choice of topic words, which is its main novelty, rather than the clustering approach.", "labels": [], "entities": [{"text": "LDA", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.8300495147705078}]}], "tableCaptions": [{"text": " Table 1: Topic Words -Measles Outbreak, 2015", "labels": [], "entities": [{"text": "Topic Words -Measles Outbreak, 2015", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.5914375611713955}]}, {"text": " Table 2: Average Turker scores for Matched Clusters  on a 5-point Likert scale.", "labels": [], "entities": [{"text": "Average Turker scores", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8075574437777201}]}, {"text": " Table 3: Topic Words -Independent Clusters", "labels": [], "entities": []}]}