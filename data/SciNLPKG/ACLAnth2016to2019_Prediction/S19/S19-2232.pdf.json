{"title": [{"text": "Deep-Affix Named Entity Recognition of Geolocation Entities", "labels": [], "entities": [{"text": "Deep-Affix Named Entity Recognition of Geolocation Entities", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.5970986826079232}]}], "abstractContent": [{"text": "We present the Named Entity Recognition (NER) and disambiguation model used by the University of Arizona team (UArizona) for SemEval 2019 task 12.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6070291459560394}, {"text": "SemEval 2019 task 12", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.9325476586818695}]}, {"text": "We achieved fourth place on tasks 1 and 3.", "labels": [], "entities": []}, {"text": "We implemented a deep-affix based LSTM-CRF NER model for task 1, which utilizes only character, word, prefix and suffix information for the identification of geolocation entities.", "labels": [], "entities": []}, {"text": "Despite using just the training data provided by task organizers and not using any lexicon features, we achieved 78.85% strict micro F-score on task 1.", "labels": [], "entities": [{"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.7272686958312988}]}, {"text": "We used the unsupervised population heuristics for task 3 and achieved 52.99% strict micro-F1 score in this task.", "labels": [], "entities": [{"text": "strict micro-F1 score", "start_pos": 78, "end_pos": 99, "type": "METRIC", "confidence": 0.8528192838033041}]}], "introductionContent": [{"text": "Geoparsing is the task of detecting geolocation phrases in unstructured text and normalizing them to a unique identifier, e.g. GeoNames IDs.", "labels": [], "entities": []}, {"text": "Although many automatic resolvers have been released in the past years, their performance fluctuates when applied to different domains ().", "labels": [], "entities": []}, {"text": "Most have also not been applied to and evaluated on scientific publications.", "labels": [], "entities": []}, {"text": "The SemEval 2019 Shared Task 12: Toponym Resolution in Scientific Papers ( aims to boost the research on geoparsing for the scientific domain by focusing on epidemiology journal articles.", "labels": [], "entities": [{"text": "SemEval 2019 Shared Task 12", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7291898846626281}, {"text": "Toponym Resolution in Scientific Papers", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.8262814521789551}]}, {"text": "The task includes three sub-tasks: toponym detection, toponym disambiguation, and end-to-end toponym resolution.", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8514513075351715}, {"text": "toponym disambiguation", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.736972838640213}, {"text": "toponym resolution", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7610323429107666}]}, {"text": "The first one requires participants to detect the text boundaries of all toponym mentions in articles.", "labels": [], "entities": []}, {"text": "In toponym disambiguation, the toponym mentions are known, and the resolver has to align them to their precise coordinates through GeoNames IDs.", "labels": [], "entities": [{"text": "toponym disambiguation", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.7107793837785721}]}, {"text": "For the last sub-In this paper, we present the description of our system for SemEval 2019 Shared Task 12, in which we focus mainly on toponym detection.", "labels": [], "entities": [{"text": "SemEval 2019 Shared Task 12", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.7352774739265442}, {"text": "toponym detection", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7860200703144073}]}, {"text": "For this sub-task, we propose a recurrent neural network that combines word, character and affix information.", "labels": [], "entities": []}, {"text": "By making use of the baseline provided by the organizers for toponym disambiguation, we also obtain results for the end-to-end sub-task.", "labels": [], "entities": [{"text": "toponym disambiguation", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7179466784000397}]}], "datasetContent": [{"text": "Using the original fully annotated training set, we achieved 77.3% strict micro-Fscore (mean performance of 3 runs) on the validation set.", "labels": [], "entities": []}, {"text": "However, the organizers provided two additional large (but weakly) annotated NER datasets: P OS, which contains sentences having at least 1 location phrase, and N EG, which has sentences with no mention of location entities.", "labels": [], "entities": [{"text": "NER datasets", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.7920921146869659}]}, {"text": "We experimented with both these datasets in both joint and transfer learning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameters for training the model.", "labels": [], "entities": []}, {"text": " Table 2: Results of subtask 1 -toponym detection. We  include the best Micro F-score and best Macro F-score  of each team from their final 3 runs. Our model is  ranked fourth, despited the fact that it uses no external  knowledge.", "labels": [], "entities": [{"text": "subtask 1 -toponym detection", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.5990749180316925}, {"text": "Micro F-score", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.5240718722343445}, {"text": "F-score", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.5666024684906006}]}, {"text": " Table 3: Results of subtask 3 -end-to-end toponym res- olution. Our system is again ranked fourth.", "labels": [], "entities": []}]}