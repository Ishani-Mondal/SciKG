{"title": [{"text": "Team Kermit-the-frog at SemEval-2019 Task 4: Bias Detection Through Sentiment Analysis and Simple Linguistic Features", "labels": [], "entities": [{"text": "Bias Detection", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.9549383521080017}, {"text": "Sentiment Analysis", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8270280659198761}]}], "abstractContent": [{"text": "In this paper we describe our participation in the SemEval 2019 shared task on hyperpar-tisan news detection.", "labels": [], "entities": [{"text": "SemEval 2019 shared task", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.8534266948699951}, {"text": "hyperpar-tisan news detection", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.7295560042063395}]}, {"text": "We present the system that we submitted for final evaluation and the three approaches that we used: sentiment, bias-laden words and filtered n-gram features.", "labels": [], "entities": []}, {"text": "Our submitted model is a Linear SVM that solely relies on the negative sentiment of a document.", "labels": [], "entities": []}, {"text": "We achieved an accuracy of 0.621 and a f1 score of 0.694 in the competition, revealing the predictive power of negative sentiment for this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996793270111084}, {"text": "f1 score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9839446544647217}]}, {"text": "There was no major improvement by adding or substituting the features of the other two approaches that we tried.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the growing role of social media in politics it becomes evermore important to safeguard the integrity of information people consume.", "labels": [], "entities": []}, {"text": "News articles about important events in the world can affect political choices.", "labels": [], "entities": []}, {"text": "It is therefore crucial to pinpoint what information people know to be trustworthy, factual and unbiased.", "labels": [], "entities": []}, {"text": "One way to do this is by using a computational system that detects an author's or publisher's bias in a news article.", "labels": [], "entities": [{"text": "detects an author's or publisher's bias in a news article", "start_pos": 59, "end_pos": 116, "type": "TASK", "confidence": 0.694011906782786}]}, {"text": "In we have seen that it is possible to build such a system by relying on the writing characteristics of a text.", "labels": [], "entities": []}, {"text": "To shed more light on potential linguistic computational methods for hyperpartisan news detection, we present our participation in the SemEval 2019 shared task on hyperpartisan news detection, of which the purpose is to identify whether a news article contains hyperpartisan ( content.", "labels": [], "entities": [{"text": "hyperpartisan news detection", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.6612252195676168}, {"text": "SemEval 2019 shared task", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.7033819407224655}, {"text": "hyperpartisan news detection", "start_pos": 163, "end_pos": 191, "type": "TASK", "confidence": 0.6467994650204977}]}, {"text": "For our contribution, we set out to experiment with various types and levels of features, such as a) sentiment that could indicate an author's bias (), b) bias-laden words such as assertives, factives and hedges, and c) part-of-speech (from now on POS) filtered ngrams.", "labels": [], "entities": []}, {"text": "In the end, we decided to submit a model that only uses the negative sentiment of an article as a feature.", "labels": [], "entities": []}, {"text": "We obtained an accuracy of 0.621 and a f1 score of 0.694 on the by-article test set, which resulted to the 30th place in the competition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9997133612632751}, {"text": "f1 score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9845004379749298}, {"text": "by-article test set", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.792666514714559}]}, {"text": "On the by-publisher test set, the systems accuracy was 0.589 and its f1 score 0.623 (20th place).", "labels": [], "entities": [{"text": "by-publisher test set", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.775684674580892}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9986190795898438}, {"text": "f1 score 0.623", "start_pos": 69, "end_pos": 83, "type": "METRIC", "confidence": 0.9675546884536743}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: An overview of the data that we used for the  shared task.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation metrics (of the true class) across  different data sets of correctly detecting the hyperparti- san class.", "labels": [], "entities": []}, {"text": " Table 3: Performance of other models trained on the by-publisher training set on predicting hyperpartisan.", "labels": [], "entities": []}, {"text": " Table 4: Performance of models on the by-article test set submitted after the evaluation period. **only trained on  100k randomly obtained documents of the by-publisher set (with a balanced frequency distribution of labels).", "labels": [], "entities": []}]}