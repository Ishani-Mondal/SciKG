{"title": [{"text": "SWAP at SemEval-2019 Task 3: Emotion detection in conversations through Tweets, CNN and LSTM deep neural networks", "labels": [], "entities": [{"text": "Emotion detection", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9243809878826141}]}], "abstractContent": [{"text": "Emotion detection from user-generated contents is growing in importance in the area of natural language processing.", "labels": [], "entities": [{"text": "Emotion detection from user-generated contents", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8920945286750793}, {"text": "natural language processing", "start_pos": 87, "end_pos": 114, "type": "TASK", "confidence": 0.6387476225694021}]}, {"text": "The approach we proposed for the EmoContext task is based on the combination of a CNN and an LSTM using a concatenation of word embeddings.", "labels": [], "entities": []}, {"text": "A stack of convolutional neural networks (CNN) is used for capturing the hierarchical hidden relations among embedding features.", "labels": [], "entities": []}, {"text": "Meanwhile, along short-term memory network (LSTM) is used for capturing information shared among words of the sentence.", "labels": [], "entities": []}, {"text": "Each conversation has been formalized as a list of word embed-dings, in particular during experimental runs pre-trained Glove and Google word embed-dings have been evaluated.", "labels": [], "entities": []}, {"text": "Surface lexical features have been also considered, but they have been demonstrated to be not usefully for the classification in this specific task.", "labels": [], "entities": [{"text": "classification", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.9660871624946594}]}, {"text": "The final system configuration achieved a micro F1 score of 0.7089.", "labels": [], "entities": [{"text": "micro F1 score", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.8635651667912801}]}, {"text": "The python code of the system is fully available at https://github.", "labels": [], "entities": []}, {"text": "com/marcopoli/EmoContext2019.", "labels": [], "entities": [{"text": "EmoContext2019", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.9419446587562561}]}], "introductionContent": [{"text": "The task of emotion detection from a text is growing in importance as a consequence of a large number of possible applications in personalized systems.", "labels": [], "entities": [{"text": "emotion detection from a text", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.8626454949378968}]}, {"text": "This task can be considered as part of the sentiment analysis process also if it differs about the information collected.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9616438448429108}]}, {"text": "Sentiment Analysis aims to detect the polarity (positive, negative or neutral) about a topic of discussion or a specific aspect.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9309984445571899}]}, {"text": "On the contrary, Emotion Detection aims to associate an emotional label to textual content to explicitly understand what is the emotional state of the user while writing it.", "labels": [], "entities": [{"text": "Emotion Detection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9459977746009827}]}, {"text": "The final user behaviors are strongly influenced by the emotional state which she is in.", "labels": [], "entities": []}, {"text": "Following the studies of Ekman (, Plutchik,, and Frijda ( some emotions can be considered \"basics\" and consequently more important than others during everyday decisions.", "labels": [], "entities": []}, {"text": "Their identification is, therefore, one crucial aspect for applications in commerce, public health, disaster management, and trend analysis (consumer understanding).", "labels": [], "entities": [{"text": "disaster management", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7514167129993439}, {"text": "trend analysis (consumer understanding", "start_pos": 125, "end_pos": 163, "type": "TASK", "confidence": 0.6509277403354645}]}, {"text": "In the research area of emotion detection and sentiment analysis, many challenges are organized every for overcoming the state-of-the-art results.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8215781450271606}, {"text": "sentiment analysis", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9402830600738525}]}, {"text": "SemEval 1 is one of the most famous among them and it provides a large amount of data every year useful for supporting the research about the topic and commonly considered as state-of-the-art.", "labels": [], "entities": []}, {"text": "Recently the best results are obtained by machine learning approaches) based on recurrent neural networks (long short-term memory network) (.", "labels": [], "entities": []}, {"text": "These algorithms have quickly become the standard approach for solving the Emotion detection task placing great emphasis on the strategies used for formalizing the training data () and for optimizing hyper-parameters of the algorithms).", "labels": [], "entities": [{"text": "Emotion detection task", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.9589127500851949}]}], "datasetContent": [{"text": "We began to configure the proposed model pointing attention on the strategy to formalize records.", "labels": [], "entities": []}, {"text": "We decided to train our model for 10 epochs for each run using a batches size equal to 64 on the train dataset and validating the model on the dev dataset.", "labels": [], "entities": []}, {"text": "For each run, we vary the word embedding formalization.", "labels": [], "entities": []}, {"text": "1 are shown the results that allow us to observe how the concatenation of Google pre-trained word embeddings (GoEmb) and the words embeddings obtained by general tweets (GTEmb) is the most promising for the classification task in term of micro F1.", "labels": [], "entities": []}, {"text": "It is also important to note that the value of precision obtained by the concatenation of Glove pre-trained word embeddings (GLEmb) and the GTEmb set is the higher obtained but very unbalanced with the recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9990239143371582}, {"text": "recall", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.9857755899429321}]}, {"text": "This is a clear index of the instability of the model.", "labels": [], "entities": []}, {"text": "The second step performed in this tuning phase has been the inclusion of surface lexical features about the records and every single token.", "labels": [], "entities": []}, {"text": "In order to understand the influence of each set of lexical features on the final micro F1 score, we performed an ablation test.", "labels": [], "entities": [{"text": "micro F1 score", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.6997054517269135}]}, {"text": "2 demonstrate that lexical features, in this specific classification task and dataset do not contribute positively to the final performances of the model.", "labels": [], "entities": []}, {"text": "As a consequence of this observation, we decided to do not use them in our model.", "labels": [], "entities": []}, {"text": "Following the goal to make the model robust, we decided to train it for its final configuration also on data which comes from the dev set about the classes Happy, Sad and Angry.", "labels": [], "entities": []}, {"text": "Then we trained the model again for 10 times on 100 epochs, with a batch size of 64 using GoEmb + GTEmb for data embeddings with a validation set of 20% of training data and an early stop when the micro F1 of the validation would overcome 0.75.", "labels": [], "entities": [{"text": "GoEmb + GTEmb", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.8075298269589742}, {"text": "F1", "start_pos": 203, "end_pos": 205, "type": "METRIC", "confidence": 0.8586982488632202}]}, {"text": "We obtained three final models with micro F1 respectively of 0.7714, 0.8078 and 0.78163.", "labels": [], "entities": [{"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.654746413230896}]}, {"text": "We used these final models to classify the test set adopting a majority vote algorithm of the predictions.", "labels": [], "entities": []}, {"text": "This strategy has allowed us to reach a final evaluation score of 0.7089 in the final task leader-board.", "labels": [], "entities": [{"text": "final evaluation score", "start_pos": 40, "end_pos": 62, "type": "METRIC", "confidence": 0.8610644539197286}]}], "tableCaptions": [{"text": " Table 1: Results obtained by different formalization of records through word emebeddings.", "labels": [], "entities": []}, {"text": " Table 2: Results obtained by different formalization of records through word emebeddings.", "labels": [], "entities": []}]}