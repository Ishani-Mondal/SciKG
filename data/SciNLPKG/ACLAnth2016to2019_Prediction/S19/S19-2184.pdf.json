{"title": [{"text": "Team yeon-zi at SemEval-2019 Task 4: Hyperpartisan News Detection by De-noising Weakly-labeled Data", "labels": [], "entities": [{"text": "SemEval-2019 Task", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.874633401632309}, {"text": "Hyperpartisan News Detection", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.7101860642433167}]}], "abstractContent": [{"text": "This paper describes our system submitted to SemEval-2019 Task 4: Hyperpartisan News Detection.", "labels": [], "entities": [{"text": "SemEval-2019 Task 4", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8367483019828796}, {"text": "Hyperpartisan News Detection", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7031669119993845}]}, {"text": "We focus on removing the inherent noise in the hyperpartisanship dataset from both data-level and model-level by leveraging semi-supervised pseudo-labels and the state-of-the-art BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9444977641105652}]}, {"text": "Our model achieves 75.8% accuracy in the final by-article dataset without ensemble learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995163679122925}]}], "introductionContent": [{"text": "With the ever-growing usage of internet, the problem of fake news that spreads in a destructive speed has attracted many attention.", "labels": [], "entities": []}, {"text": "Fake news is a kind of news that is typically inflammatory, extremely one-sided (hyper-partisan) or untruthful to mislead the public into having distorted belief.", "labels": [], "entities": [{"text": "Fake news is a kind of news", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8530029824801854}]}, {"text": "Previous works attempted to solve fake news problem from various aspects, ranging from knowledge-based () to stylebased.", "labels": [], "entities": []}, {"text": "There are some publicly available fake news datasets, however, often too small in size to be suitable for neural approaches.", "labels": [], "entities": []}, {"text": "Recently, the organizers of SemEval2019 Task 4 () have released large-scale dataset to address fake news detection as a hyper-partisan news detection problem.", "labels": [], "entities": [{"text": "SemEval2019 Task 4", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8843342065811157}, {"text": "fake news detection", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.6530658602714539}]}, {"text": "The task is to determine whether a given article is hyper-partisan (extremely right-wing or leftwing) or not (mainstream).", "labels": [], "entities": []}, {"text": "Such task will allow for pre-screening of semi-automatic fake news detection, and more importantly, bring us one step closer to solving fully automated fake news detection.", "labels": [], "entities": [{"text": "semi-automatic fake news detection", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.632502444088459}, {"text": "solving fully automated fake news detection", "start_pos": 128, "end_pos": 171, "type": "TASK", "confidence": 0.6580248574415842}]}, {"text": "Initially, we focused on learning and utilizing useful features such as topic and sentiment infor- * * These two authors contributed equally. mation.", "labels": [], "entities": []}, {"text": "Considering the purpose of hyper-partisan news, we believed that the stance on politically sensitive topics would be crucial in determining hyperpartisanship.", "labels": [], "entities": []}, {"text": "However, experiments showed that the dataset contains some inherent noise that acted as a big barrier to learning a good classifier: 1) noisy text inputs from an article that contain domain-specific (i.e. political) words, slangs and spelling mistakes which are likely to be out of vocabulary (OOV).", "labels": [], "entities": []}, {"text": "2) noisy labels that mainly resulted from using publisher-level information for labeling articles (i.e. all articles from left/rightwing publishers are labeled as \"hyper-partisan\". For more detail, refer to Section 2).", "labels": [], "entities": []}, {"text": "Nevertheless, human-labeled large-scale dataset creation is a very expensive and time-consuming task, thus, it is crucial to find a better way to utilize this weakly-labeled dataset.", "labels": [], "entities": []}, {"text": "Therefore, we experimented with reducing noise to help models learn better.", "labels": [], "entities": []}, {"text": "In our work, we apply a semisupervised pseudo-labeling to de-noise the dataset) and leverage the state-of-the-art pretrained BERT to obtain a better representation of the noisy input.", "labels": [], "entities": [{"text": "BERT", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9935587048530579}]}, {"text": "We use a publicly available dataset \"SemEval 2019 Task 4 -Hyperpartisan News Detection\" 1 that are labeled in two different ways -publisher level and article level.", "labels": [], "entities": [{"text": "SemEval 2019 Task 4 -Hyperpartisan News Detection", "start_pos": 37, "end_pos": 86, "type": "TASK", "confidence": 0.6879490837454796}]}, {"text": "\u2022 Publisher-level (by-publisher): A total of 750K articles are labeled based on the political orientation of the publisher, without considering the content.", "labels": [], "entities": []}, {"text": "It has an equal ratio (375K/375K) between hyperpartisan and non-hyperpartisan.", "labels": [], "entities": []}, {"text": "Among the hyperpartisan samples, there's an equal ratio (187.5K/187.5K) between right and left political orientation.", "labels": [], "entities": []}, {"text": "\u2022 Article-level (by-article): A total of 645 articles labeled on article-level by checking the actual content.", "labels": [], "entities": []}, {"text": "The data contains only articles for which a consensus among the crowdsourcing workers existed.", "labels": [], "entities": []}, {"text": "Of these, 238 (37%) are hyperpartisan and 407 (63%) are not.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use BERT BASE model from) which has 12 layers (i.e., Transformer blocks) with a hidden size of 768 and 12 selfattention heads.", "labels": [], "entities": [{"text": "BERT", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.995305597782135}, {"text": "BASE", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.8000958561897278}]}, {"text": "In step 1, the parameters of BERT model were fixed after fine-tuned on bypublisher datset, then we trained classifier on byarticle dataset by using 16 batch size.", "labels": [], "entities": [{"text": "BERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.956739068031311}]}, {"text": "We used 10-fold cross-validation to choose the parameters of the classifier, since the size of by-article dataset is small.", "labels": [], "entities": []}, {"text": "In step 2, we used 16 batch size to train our LSTM for article model with a hidden size of 300 and LSTM for title model with a hidden size of 100.", "labels": [], "entities": []}, {"text": "The classifiers in step 1 and 2 both consist of two linear layers with ReLU and batch normalization in between.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9686346054077148}]}, {"text": "For the evaluation metric, we mainly considered accuracy and F1 score as the main indicator of performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9997063279151917}, {"text": "F1 score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9880813360214233}]}, {"text": "For analysis purpose, we also report precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9997283816337585}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9989761114120483}]}, {"text": "In the competition, there were two types of test sets (i.e. by-publisher test set and by-article test set).", "labels": [], "entities": []}, {"text": "However, all of the reported results are obtained from the by-article test set for fair and correct comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistic of hyperpartisan and politi- cal orientation on by-publisher dataset.", "labels": [], "entities": []}, {"text": " Table 2: Results of our model and other baseline models on the final by-article test set.", "labels": [], "entities": []}]}