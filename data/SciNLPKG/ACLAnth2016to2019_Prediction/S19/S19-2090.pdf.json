{"title": [{"text": "Tw-StAR at SemEval-2019 Task 5: N-gram embeddings for Hate Speech Detection in Multilingual Tweets", "labels": [], "entities": [{"text": "Hate Speech Detection in Multilingual Tweets", "start_pos": 54, "end_pos": 98, "type": "TASK", "confidence": 0.809785525004069}]}], "abstractContent": [{"text": "In this paper, we describe our contribution in SemEval-2019: subtask A of task 5 \"Multilin-gual detection of hate speech against immigrants and women in Twitter (HatEval)\".", "labels": [], "entities": [{"text": "Multilin-gual detection of hate speech", "start_pos": 82, "end_pos": 120, "type": "TASK", "confidence": 0.85332270860672}]}, {"text": "We developed two hate speech detection model variants through Tw-StAR framework.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7818283240000407}]}, {"text": "While the first model adopted one-hot encoding n-grams to train an NB classifier, the second generated and learned n-gram embeddings within a feedforward neural network.", "labels": [], "entities": []}, {"text": "For both models , specific terms, selected via MWT patterns, were tagged in the input data.", "labels": [], "entities": []}, {"text": "With two feature types employed, we could investigate the ability of n-gram embeddings to rival one-hot n-grams.", "labels": [], "entities": []}, {"text": "Our results showed that in English, n-gram embeddings outperformed one-hot n-grams.", "labels": [], "entities": []}, {"text": "However, representing Spanish tweets by one-hot n-grams yielded a slightly better performance compared to that of n-gram em-beddings.", "labels": [], "entities": []}, {"text": "The official ranking indicated that Tw-StAR ranked 9 th for English and 20 th for Spanish.", "labels": [], "entities": [{"text": "Tw-StAR", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.7541138529777527}]}], "introductionContent": [{"text": "Under the guise of free speech, social media systems have been misused by some users who embed hatred, offensive, racist or negative stereotyping contents within their shared posts.", "labels": [], "entities": []}, {"text": "Unfortunately, online Hate Speech (HS) is spreading widely, forming a serious problem that can lead to actual hate crimes.", "labels": [], "entities": [{"text": "Hate Speech (HS)", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8696189522743225}]}, {"text": "Many countries adopted laws prohibiting HS where people convicted of using HS can face large fines and even imprisonment.", "labels": [], "entities": [{"text": "HS", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9485218524932861}]}, {"text": "Although Twitter has its anti HS policy * , the increasing size of the daily-shared tweets in addition to multilingualism and informal writing issues evoke the necessity for automatic HS detection in tweets.", "labels": [], "entities": [{"text": "HS detection", "start_pos": 184, "end_pos": 196, "type": "TASK", "confidence": 0.9301046431064606}]}, {"text": "Hate speech detection problem has been addressed as a machine learning classification task.", "labels": [], "entities": [{"text": "Hate speech detection problem", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7390617802739143}, {"text": "machine learning classification task", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.7238133624196053}]}, {"text": "Recent studies proposed multiple HS detection models with different characteristic in terms of features, classification algorithms and implementation architectures.", "labels": [], "entities": [{"text": "HS detection", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9785826504230499}]}, {"text": "While some HS models employed hand-crafted features generated by NLP tools and external semantic resources, other models adopted text embedding features that are automatically learned from the corpus itself.", "labels": [], "entities": []}, {"text": "Both feature types were fed to train either traditional classifiers such as Support Vector Machines (SVM), Naive Bayes (NB) and so forth, or more complicated deep learning-based classifiers such as Convolutional Neural Network (CNN), Long ShortTerm Memory (LSTM) and Recurrent Neural Network (RNN) (.", "labels": [], "entities": []}, {"text": "The variety of hand-crafted features enabled obtaining reliable performances.", "labels": [], "entities": []}, {"text": "However, generating such features based on morphological NLP tools or semantic resources remains laborious.", "labels": [], "entities": []}, {"text": "In contrast, embedding features are easier to generate and can yield good HS classification results when used within deep learning architectures.", "labels": [], "entities": [{"text": "HS classification", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.9400584399700165}]}, {"text": "Nevertheless, producing good performances via deep neural systems requires providing large-sized labeled training data, tuning many hyper parameters and high computation/time cost.", "labels": [], "entities": []}, {"text": "In line with Tw-StAR framework (), we propose, here, an HS model based on the hypothesis that, pairing between ngram embeddings and less-complicated architectures i.e. feedforward neural network can lead to an efficient HS detection with least complexity.", "labels": [], "entities": [{"text": "HS detection", "start_pos": 220, "end_pos": 232, "type": "TASK", "confidence": 0.9647390842437744}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Unigrams+bigrams (TF threshold=2) and  8-gram embeddings results of NB/neural models for  train/dev sets.", "labels": [], "entities": [{"text": "TF threshold", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9604756832122803}]}, {"text": " Table 2: Tw-StAR official Codalab ranking.", "labels": [], "entities": [{"text": "Tw-StAR", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8628438115119934}, {"text": "Codalab ranking", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.8235719501972198}]}]}