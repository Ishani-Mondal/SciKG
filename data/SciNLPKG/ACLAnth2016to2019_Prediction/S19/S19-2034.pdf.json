{"title": [{"text": "EmoSense at SemEval-2019 Task 3: Bidirectional LSTM Network for Contextual Emotion Detection in Textual Conversations", "labels": [], "entities": [{"text": "Contextual Emotion Detection in Textual Conversations", "start_pos": 64, "end_pos": 117, "type": "TASK", "confidence": 0.7521339257558187}]}], "abstractContent": [{"text": "In this paper, we describe a deep-learning system for emotion detection in textual conversations that participated in SemEval-2019 Task 3 \"EmoContext\".", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7420420944690704}, {"text": "SemEval-2019 Task 3", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7926607926686605}]}, {"text": "We designed a specific architecture of bidirectional LSTM which allows not only to learn semantic and sentiment feature representation, but also to capture user-specific conversation features.", "labels": [], "entities": []}, {"text": "To fine-tune word embeddings using distant supervision we additionally collected a significant amount of emotional texts.", "labels": [], "entities": []}, {"text": "The system achieved 72.59% micro-average F 1 score for emotion classes on the test dataset, thereby significantly outper-forming the officially-released baseline.", "labels": [], "entities": [{"text": "micro-average F 1 score", "start_pos": 27, "end_pos": 50, "type": "METRIC", "confidence": 0.8540968298912048}]}, {"text": "Word embeddings and the source code were released for the research community.", "labels": [], "entities": []}], "introductionContent": [{"text": "Emotion detection has emerged as a challenging research problem that can make some valuable contribution not only in basic spheres like medicine, sociology and phycology but also in more innovative areas such as human-computer interaction.", "labels": [], "entities": [{"text": "Emotion detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9539256393909454}]}, {"text": "Nowadays, people increasingly communicate using text messages with dialogue systems, for which it is crucial to provide emotionally aware responses to users.", "labels": [], "entities": []}, {"text": "The SemEval-2019 Task 3 \"EmoContext\" is focused on the contextual emotion detection in textual conversation.", "labels": [], "entities": [{"text": "contextual emotion detection", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.6479258636633555}]}, {"text": "In EmoContext, given a textual user utterance along with 2 turns of context in a conversation, we must classify whether the emotion of the next user utterance is \"happy\", \"sad\", \"angry\" or \"others\" (4-point scale).", "labels": [], "entities": []}, {"text": "For a detailed description see.", "labels": [], "entities": []}, {"text": "In this paper, we present bidirectional LSTM for contextual emotion detection in textual conversations that participated in SemEval-2019 Task 3 \"EmoContext\".", "labels": [], "entities": [{"text": "contextual emotion detection", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6536986927191416}, {"text": "SemEval-2019 Task 3", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.8200704455375671}]}, {"text": "The proposed architecture aims to capture not only semantic and sentiment feature representation from the conversation turns, but also to capture user-specific conversation features.", "labels": [], "entities": []}, {"text": "We avoided using traditional NLP features like sentiment lexicons and hand-crafted linguistic features by substituting them with word embeddings which were calculated automatically from the text corpora.", "labels": [], "entities": []}, {"text": "Based on this paper, we make the following contributions 1 freely available for the research community: \u2022 The source code of the deep-learning system for emotion detection.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.8050195872783661}]}, {"text": "\u2022 Word embeddings fine-tuned for emotional detection in short texts.", "labels": [], "entities": [{"text": "emotional detection in short texts", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.8505716681480407}]}, {"text": "The rest of the article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief overview of the related work.", "labels": [], "entities": []}, {"text": "In section 3 we describe the proposed architecture of LSTM used in our system.", "labels": [], "entities": []}, {"text": "Section 4 is focused on the texts pre-processing and training process.", "labels": [], "entities": []}, {"text": "Section 5 lays emphasis on the different system architectures and approaches we have tried.", "labels": [], "entities": []}, {"text": "In conclusion, the performance of our system and further ways of research are presented.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the process of searching for optimal architecture, we experimented not only with the number of cells in layers, activation functions and regularization parameters but also with the architecture of the neural network.", "labels": [], "entities": []}, {"text": "Let us take a closer look at the latter type of experiments.", "labels": [], "entities": []}, {"text": "Comparison of various models presented in.", "labels": [], "entities": []}, {"text": "\u2022 LST M 1 is a model with one bidirectional LSTM unit for all three conversation turns.", "labels": [], "entities": []}, {"text": "\u2022 LST M 2 is a final model with two bidirectional LSTM units described in Section 2.", "labels": [], "entities": []}, {"text": "\u2022 LST M 3 is a model with three bidirectional LSTM unit, where each unit is intended to analyze the corresponding conversation turn.", "labels": [], "entities": []}, {"text": "\u2022 LST M w is LST M 2 with an additional regularization based on class weights.", "labels": [], "entities": []}, {"text": "\u2022 LST Ms is LST M 2 with an additional LSTM unit above concatenated layer.", "labels": [], "entities": []}, {"text": "\u2022 LST M a is LST M 2 with additional contextattention layer (.", "labels": [], "entities": []}, {"text": "Since LST M 2 demonstrated the best scores on the dev dataset, it was used in the final evaluation stage of the competition.", "labels": [], "entities": [{"text": "dev dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.7377666532993317}]}, {"text": "On the final test dataset, it achieved 72.59% micro-average F 1 score for emotional classes.", "labels": [], "entities": [{"text": "micro-average F 1 score", "start_pos": 46, "end_pos": 69, "type": "METRIC", "confidence": 0.8717736899852753}]}, {"text": "This is well above the official baseline released by task organizers, which was 58.68%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Emotion class label distribution in datasets.", "labels": [], "entities": [{"text": "Emotion class label distribution", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7644815146923065}]}, {"text": " Table 2: Comparison of various models on dev dataset using micro-average Precision, Recall and F 1 -score for  emotional classes. Baseline is an official baseline approach released by task organizers.", "labels": [], "entities": [{"text": "Precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9240555763244629}, {"text": "Recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.989563524723053}, {"text": "F 1 -score", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9840908497571945}]}]}