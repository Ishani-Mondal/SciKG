{"title": [{"text": "Team Ned Leeds at SemEval-2019 Task 4: Exploring Language Indicators of Hyperpartisan Reporting", "labels": [], "entities": [{"text": "SemEval-2019 Task 4", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.8335708777109782}, {"text": "Exploring Language Indicators of Hyperpartisan Reporting", "start_pos": 39, "end_pos": 95, "type": "TASK", "confidence": 0.5652539829413096}]}], "abstractContent": [{"text": "This paper reports an experiment carried out to investigate the relevance of several syntactic , stylistic and pragmatic features on the task of distinguishing between mainstream and partisan news articles.", "labels": [], "entities": [{"text": "distinguishing between mainstream and partisan news articles", "start_pos": 145, "end_pos": 205, "type": "TASK", "confidence": 0.617164113691875}]}, {"text": "The results of the evaluation of different feature sets and the extent to which various feature categories could affect the performance metrics are discussed and compared.", "labels": [], "entities": []}, {"text": "Among different combinations of features and classifiers, Random Forest classi-fier using vector representations of the headline and the text of the report, with the inclusion of 8 readability scores and few stylistic features yielded best result, ranking our team at the 9 th place at the SemEval 2019 Hyper-partisan News Detection challenge.", "labels": [], "entities": [{"text": "SemEval 2019 Hyper-partisan News Detection challenge", "start_pos": 290, "end_pos": 342, "type": "TASK", "confidence": 0.7770191629727682}]}], "introductionContent": [{"text": "Current influential technological megatrends, such as, smart phones and social networking often come with some unwanted side effects -prolific spread of false, biased and misleading information, for instance.", "labels": [], "entities": []}, {"text": "In the past few years, the potential threats and consequences of disseminating fake news has reinforced the discussion on the responsibility of social media and governments to tackle the issue, sooner rather than later.", "labels": [], "entities": []}, {"text": "Enabling users to report on and be informed of untruthful, deceitful and fraudulent content and sources is expected to become a type of guiding principle for those involved in publishing and disseminating content online.", "labels": [], "entities": []}, {"text": "There is a blurred line between deceptive writing and hyperpartisan reporting, producing extremely biased articles in favor of one political party, cause or individual, while preserving the format and appearance of professional articles.", "labels": [], "entities": []}, {"text": "Adherence to the ethics and rules of objective reporting is frequently debatable when it comes to political analysis in media articles.", "labels": [], "entities": []}, {"text": "While certain truthful facts are present, they are carefully entangled in a narrative package with biased views, populistic messages and divisive topics, using language that polarizes and flares emotions.", "labels": [], "entities": []}, {"text": "Rather than labeling and grading news articles on the truth continuum, researchers usually opt for identification of the phenomenological and contextual features of distinguishing hyperpartisanship in online news articles.", "labels": [], "entities": []}, {"text": "People use diverse set of cues extrapolated from published text and external knowledge and sources, when verifying the veracity of information imparted by others.", "labels": [], "entities": []}, {"text": "A large body of evidence documents the impact of deception has on language choices people make.", "labels": [], "entities": []}, {"text": "A notable body of work exists revealing insights into the language of deceit in interrogation context, court hearings (, or personal relationships (.", "labels": [], "entities": []}, {"text": "Empirical studies still remain the primary manner in which manifestation of deceptive human behavior online is studied.", "labels": [], "entities": []}, {"text": "Analysis of political language, partisan media, and news publishing (  were also guided broadly by the questions pertaining to detecting deception in written language.", "labels": [], "entities": [{"text": "detecting deception in written language", "start_pos": 127, "end_pos": 166, "type": "TASK", "confidence": 0.8057406544685364}]}, {"text": "In what follows, we highlight the primary findings of our empirical research in identifying tangible verbal indicators as they relate to our central commitment of detecting deception in text.", "labels": [], "entities": [{"text": "detecting deception in text", "start_pos": 163, "end_pos": 190, "type": "TASK", "confidence": 0.8135959357023239}]}, {"text": "In this paper we examine the impact of grammar and psycho-linguistic word categories, syntactic word connotations and text complexity metrics on the task of distinguishing hyperpartisanship in real news articles.", "labels": [], "entities": [{"text": "distinguishing hyperpartisanship in real news articles", "start_pos": 157, "end_pos": 211, "type": "TASK", "confidence": 0.7823942303657532}]}], "datasetContent": [{"text": "Two datasets of news articles were available for the SemEval 2019 Task 4: \"Hyperpartisan news detection\" (), one labeled \"byarticle\" by professional journalists, and the other labeled \"by-publisher\".", "labels": [], "entities": [{"text": "SemEval 2019 Task 4", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8667959421873093}, {"text": "Hyperpartisan news detection", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.6456945240497589}]}, {"text": "Our empirical study was focused on the former one, whose training dataset consists of 645 articles.", "labels": [], "entities": []}, {"text": "The testing dataset, which is not publicly released, are made available via TIRA ( , and it contains 628 by-article articles.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7470291256904602}]}, {"text": "It is balanced and consists of articles from previously unseen publishers in the training sets.", "labels": [], "entities": []}, {"text": "For evaluation purposes, we randomly choose 80% of the by-article data for training, and the remaining 20% for validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance comparison of models trained on the by-article dataset.", "labels": [], "entities": [{"text": "by-article dataset", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.7109352499246597}]}]}