{"title": [{"text": "Embeddia at SemEval-2019 Task 6: Detecting Hate with Neural Network and Transfer Learning Approaches", "labels": [], "entities": [{"text": "Detecting Hate", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.8859102427959442}]}], "abstractContent": [{"text": "SemEval-2019 Task 6 was OffensEval: Identifying and Categorizing Offensive Language in Social Media.", "labels": [], "entities": []}, {"text": "The task was further divided into three sub-tasks: offensive language identification , automatic categorization of offense types, and offense target identification.", "labels": [], "entities": [{"text": "offensive language identification", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.6833469967047373}, {"text": "offense target identification", "start_pos": 134, "end_pos": 163, "type": "TASK", "confidence": 0.7608106931050619}]}, {"text": "In this paper, we present the approaches used by the Embeddia team, who qualified as fourth, eighteenth and fifth on the three sub-tasks.", "labels": [], "entities": []}, {"text": "A different model was trained for each sub-task.", "labels": [], "entities": []}, {"text": "For the first sub-task, we used a BERT model fine-tuned on the provided dataset, while for the second and third tasks we developed a custom neural network architecture which combines bag-of-words features and automatically generated sequence-based features.", "labels": [], "entities": [{"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9940809607505798}]}, {"text": "Our results show that combining automatically and manually crafted features fed into a neural architecture outperform transfer learning approach on more unbalanced datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the years, computer-mediated communication, like the one on social media, has become one of the key ways people communicate and share opinions.", "labels": [], "entities": []}, {"text": "Computer-mediated communication differs in many ways, both technically and culturally, from more traditional communication technologies.", "labels": [], "entities": []}, {"text": "However, the ability to fully or partially hide our identity behind an internet persona leads people to type things they would never say to someone's face.", "labels": [], "entities": []}, {"text": "Not only is hate speech more likely to happen on the Internet, where anonymity is easily obtained and speakers are psychologically distant from their audience, but its online nature also gives it a far-reaching and determinative impact.", "labels": [], "entities": []}, {"text": "Although most forms of intolerance are not criminal, hate speech and other speech acts designed to harass and intimidate (rather than merely express criticism or dissent), deteriorate public discourse and opinions, which can lead to a more radicalized society.", "labels": [], "entities": []}, {"text": "Online communities, social media platforms, and technology companies have been investing heavily in ways to cope with offensive language to prevent abusive behavior in social media.", "labels": [], "entities": []}, {"text": "Social media companies Facebook, Twitter and Google's YouTube have greatly accelerated their removal of online hate speech, and report reviewing over two-thirds of complaints within 24 hours.", "labels": [], "entities": [{"text": "removal of online hate speech", "start_pos": 93, "end_pos": 122, "type": "TASK", "confidence": 0.7612322092056274}]}, {"text": "It has been proven in practice that naive word filtering systems do not manage to scale well to different forms of hate and aggression).", "labels": [], "entities": [{"text": "word filtering", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7063411325216293}]}, {"text": "The most promising strategy for detecting abusive language is to use advanced computational methods.", "labels": [], "entities": [{"text": "detecting abusive language", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.8681354522705078}]}, {"text": "This topic has attracted significant attention in recent years as evidenced in recent publications (.", "labels": [], "entities": []}, {"text": "The SemEval-2019 Task 6 -OffensEval: Identifying and Categorizing Offensive Language in Social Media () is to use machine learning text classification methods to identify offensive content and hate speech.", "labels": [], "entities": [{"text": "Identifying and Categorizing Offensive Language in Social Media", "start_pos": 37, "end_pos": 100, "type": "TASK", "confidence": 0.6972472332417965}]}, {"text": "The task organizers have provided anew dataset () comprised of Twitter posts which employs a three-level hierarchical labeling scheme, according to the three hierarchically posed sub-tasks, where each sub-task serves as a steppingstone for the next sub-task.", "labels": [], "entities": []}, {"text": "Sub-task A aims to identify offensive content, Sub-task B aims to classify offensive content as a targeted or untargeted offense, while Sub-task C aims to identify the target of the offense.", "labels": [], "entities": []}, {"text": "In this paper, we present the approaches used by the Embeddia team to tackle the three sub-tasks of SemEval-2019 Task 6: OffensEval.", "labels": [], "entities": [{"text": "SemEval-2019 Task 6", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7984182834625244}]}, {"text": "The Embeddia team qualified as fourth, eighteenth and fifth on Sub-tasks A, B and C, respectively.", "labels": [], "entities": []}, {"text": "The Embed-dia team used different neural architectures and transfer learning techniques).", "labels": [], "entities": []}, {"text": "We also explore if combining automatically generated sequence-based features with more traditional manual feature engineering techniques improves the classification performance and how different classifiers perform on unbalanced datasets.", "labels": [], "entities": []}, {"text": "Our results show that a combination of automatically and manually crafted features fed into a neural architecture outperforms the transfer learning approach on the more unbalanced datasets of Subtasks B and C.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present related work in the area of offensive and hate speech detection.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.6439529756704966}]}, {"text": "Section 3 describes in more detail the provided dataset and the methodology used for the task.", "labels": [], "entities": []}, {"text": "Section 4 reviews the results we obtained on the three sub-tasks with our models.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper and presents some ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The SemEval-2019 Shared Task 6: Identifying and Categorizing Offensive Language in Social Media was divided into three sub-tasks, namely offensive language identification (Sub-task A), automatic categorization of offense types (Subtask B) and offense target identification (Sub-task C).", "labels": [], "entities": [{"text": "Identifying and Categorizing Offensive Language in Social Media", "start_pos": 32, "end_pos": 95, "type": "TASK", "confidence": 0.6962374448776245}, {"text": "offensive language identification", "start_pos": 137, "end_pos": 170, "type": "TASK", "confidence": 0.6285894612471262}, {"text": "offense target identification", "start_pos": 243, "end_pos": 272, "type": "TASK", "confidence": 0.720267097155253}]}, {"text": "The organizers provided anew dataset called OLID () which includes tweets labeled according to the three-level hierarchical model.", "labels": [], "entities": [{"text": "OLID", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.8327234387397766}]}, {"text": "On the very first level, each tweet is labeled as offensive (OFF) or not offensive (NOT).", "labels": [], "entities": [{"text": "OFF", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9302580952644348}, {"text": "NOT", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.8013884425163269}]}, {"text": "All the offensive tweets are then labeled as targeted insults (TIN) or as untargeted insults (UNT), which simply contain profanity.", "labels": [], "entities": [{"text": "untargeted insults (UNT)", "start_pos": 74, "end_pos": 98, "type": "METRIC", "confidence": 0.659537935256958}]}, {"text": "On the last level, all targeted insults are categorized as targeting an individual (IND), a group (GRP) or other entity (OTH).", "labels": [], "entities": []}, {"text": "The dataset contains 14,100 tweets split into training and test sets.", "labels": [], "entities": []}, {"text": "The training set containing 13,240 tweets and the test set without labels were made available to the participants for the task.", "labels": [], "entities": []}, {"text": "The inspection of the dataset reveals that the classes at first level are slightly imbalanced with the imbalances between classes getting more prominent with each subsequent level.", "labels": [], "entities": []}, {"text": "A more detailed breakdown of the dataset is presented in.", "labels": [], "entities": []}, {"text": "We didn't use any additional datasets in any of the three sub-tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the submitted systems for each sub-task.", "labels": [], "entities": []}]}