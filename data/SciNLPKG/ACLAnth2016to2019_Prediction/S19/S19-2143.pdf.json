{"title": [{"text": "YNUWB at SemEval-2019 Task 6: K-max pooling CNN with average meta-embedding for identifying offensive language", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the system submitted to SemEval 2019 Task 6: OffensEval 2019.", "labels": [], "entities": [{"text": "SemEval 2019 Task 6: OffensEval 2019", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.7667178852217538}]}, {"text": "The task aims to identify and categorize offensive language in social media, we only participate in Sub-task A, which aims to identify offensive language.", "labels": [], "entities": []}, {"text": "In order to address this task, we propose a system based on a K-max pooling convolutional neural network model, and use an argument for averaging as a valid meta-embedding technique to get a meta-embedding.", "labels": [], "entities": []}, {"text": "Finally, we use a cyclic learning rate policy to improve model performance.", "labels": [], "entities": []}, {"text": "Our model achieves a Macro F1-score of 0.802 (ranked 9/103) in the Sub-task A.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8081054091453552}]}], "introductionContent": [{"text": "In the past ten years, with the popularity of the Internet, social media platforms such as facebook and twitter have gradually become important tools for people's daily communication, and users can publish their own content on these platforms.", "labels": [], "entities": []}, {"text": "As the number of people interacting on social media platforms increases, online aggression language behavior also grows, and now it has become a major source of social conflict.", "labels": [], "entities": []}, {"text": "Semeval 2019 Task 6 is proposed for identifying online offensive languages (.", "labels": [], "entities": [{"text": "Semeval 2019 Task 6", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9019337743520737}]}, {"text": "Its goal is to use computational methods to identify offense, aggression and hate speech in user-generated content on online social media platforms.", "labels": [], "entities": [{"text": "identify offense, aggression and hate speech in user-generated content on online social media", "start_pos": 44, "end_pos": 137, "type": "TASK", "confidence": 0.643580436706543}]}, {"text": "We can prevent abuse of offensive language by using this approach in social media platforms.", "labels": [], "entities": []}, {"text": "This task gives us some data from the social media platform, and classifies the content through computational analysis.", "labels": [], "entities": []}, {"text": "In this competition, we only participate in Subtask A: identification of offensive language.", "labels": [], "entities": [{"text": "identification of offensive language", "start_pos": 55, "end_pos": 91, "type": "TASK", "confidence": 0.8637918680906296}]}, {"text": "For this task, we use a deep learning method to build a K-max pooling convolutional neural network model which uses convolutional neural networks of different filters to extract features and preserves k largest eigenvalues during the pooling phase.", "labels": [], "entities": []}, {"text": "We use two different pre-training vector models (fastText and Glove) to obtain a more accurate meta-embedding with a simple averaging technique.", "labels": [], "entities": [{"text": "fastText", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.6705308556556702}, {"text": "Glove", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9637876152992249}]}, {"text": "In addition, we have adopted a Cyclic Learning Rate (CLR) strategy, which avoids the process to find the optimal learning rate, and the learning rate varies within a reasonable interval rather than monotonically.", "labels": [], "entities": []}, {"text": "Unlike the Adative Learning Rate, the CLR does not require additional calculations.", "labels": [], "entities": [{"text": "Adative Learning", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.8751033544540405}]}, {"text": "The rest of the paper is structured as follows: In section 2, we describe some of the relevant research work.", "labels": [], "entities": []}, {"text": "In section 3, we describe the task data and how to build the model.", "labels": [], "entities": []}, {"text": "In section 4, we describe the experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use 4-fold cross validation on the training data, because in this experiment we experimented with cross-validation of different k values, and found that the 4-fold cross-validation effect is the best.", "labels": [], "entities": []}, {"text": "In addition the batch size is to 512 and the epoch to 20.", "labels": [], "entities": [{"text": "epoch", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9498345255851746}]}, {"text": "In our model, the dimension of embeding is 300.", "labels": [], "entities": []}, {"text": "Between the embedding layer and the convolution layer we add the SpatialDropout1D layer with a value of 0.2.", "labels": [], "entities": []}, {"text": "In the convolution layer, we setup four convolution kernels of different window sizes, which are 1, 2, 3 and 4, the number of filters is 180, the kernel initializer is normal, the activation function is relu; in the pooling layer we set the k value to 3.", "labels": [], "entities": []}, {"text": "Before the fully connected layer, we add a dropout layer, and the rate is 0.6.", "labels": [], "entities": []}, {"text": "The activation function of the final output layer is sigmoid for binary classification.", "labels": [], "entities": []}, {"text": "The loss function of this model is binary crossentropy, and the optimizer is adam.", "labels": [], "entities": []}, {"text": "For the cyclical learning rate, we set the base learning rate to 0.001, the maximum learning rate to 0.002, the step size to 300, and the scaling factor gamma to 0.99994.", "labels": [], "entities": [{"text": "scaling factor gamma", "start_pos": 138, "end_pos": 158, "type": "METRIC", "confidence": 0.7936334411303202}]}], "tableCaptions": [{"text": " Table 1: Results for Sub-task A", "labels": [], "entities": []}, {"text": " Table 2. In this table, the results  of fastText, Glove, the word vector generated by  concatenating fastText and Glove and the vector", "labels": [], "entities": []}, {"text": " Table 2: Influence of word embedding in trial data set  for Sub-task A", "labels": [], "entities": []}]}