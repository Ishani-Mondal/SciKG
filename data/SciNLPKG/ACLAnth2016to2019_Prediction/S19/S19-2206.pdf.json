{"title": [{"text": "TueFact at SemEval 2019 Task 8: Fact checking in community question answering forums: context matters", "labels": [], "entities": [{"text": "SemEval 2019 Task 8", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.616814911365509}, {"text": "Fact checking in community question answering forums", "start_pos": 32, "end_pos": 84, "type": "TASK", "confidence": 0.8429057087217059}]}], "abstractContent": [{"text": "The SemEval 2019 Task 8 on Fact-Checking in community question answering forums aimed to classify questions into categories and verify the correctness of answers given on the QatarLiving public forum.", "labels": [], "entities": [{"text": "SemEval 2019 Task 8", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8528501987457275}, {"text": "Fact-Checking in community question answering forums", "start_pos": 27, "end_pos": 79, "type": "TASK", "confidence": 0.6319843182961146}, {"text": "QatarLiving public forum", "start_pos": 175, "end_pos": 199, "type": "DATASET", "confidence": 0.9575063387552897}]}, {"text": "The task was divided into two subtasks: the first classifying the question, the second the answers.", "labels": [], "entities": []}, {"text": "The Tue-Fact system described in this paper used different approaches for the two subtasks.", "labels": [], "entities": []}, {"text": "Subtask A makes use of word vectors based on a bag-of-word-ngram model using up to trigrams.", "labels": [], "entities": []}, {"text": "Predictions are done using multi-class logistic regression.", "labels": [], "entities": []}, {"text": "The official SemEval result lists an accuracy of 0.60.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996370077133179}]}, {"text": "Subtask B uses vector-ized character n-grams up to trigrams instead.", "labels": [], "entities": []}, {"text": "Predictions are done using a LSTM model and achieved an accuracy of 0.53 on the final Se-mEval Task 8 evaluation set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9994120597839355}, {"text": "Se-mEval Task 8 evaluation set", "start_pos": 86, "end_pos": 116, "type": "DATASET", "confidence": 0.5644667029380799}]}, {"text": "Ina comparison of contextual inputs to subtask B, it was determined that more contextual data improved results, but only up to a point.", "labels": [], "entities": []}], "introductionContent": [{"text": "The SemEval 2019 Task 8 on Fact-Checking gave us the opportunity to develop a system that evaluates the factual content of questions and answers in the field of community question answering forums.", "labels": [], "entities": [{"text": "SemEval 2019 Task 8", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7197198569774628}, {"text": "question answering forums", "start_pos": 171, "end_pos": 196, "type": "TASK", "confidence": 0.7750112513701121}]}, {"text": "This popular niche on the internet provides helpful information for specific interests, such as information on life in Qatar or elsewhere in the world, coding help on Stack Overflow, or answers to a wide range of questions on Quora, Reddit/r/Ask or Yahoo!", "labels": [], "entities": []}, {"text": "Often other resources are not at hand or misleading, and it proves difficult to find what one is looking for amid nonrelevant questions, let alone be sure the answers found are correct.", "labels": [], "entities": []}, {"text": "A system that can, with some degree of accuracy, pick out the factual questions and then predict the correctness of the given answers, is a means to ensure quality in community question answering forums.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9971081614494324}, {"text": "question answering forums", "start_pos": 177, "end_pos": 202, "type": "TASK", "confidence": 0.7867422997951508}]}, {"text": "There may also be many further applications in information retrieval -ordering search results based on estimated factuality in a web query or even identifying truthful answers in automatic question answering systems.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7702311277389526}, {"text": "question answering", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.7330383658409119}]}, {"text": "The task was divided into two subtasks.", "labels": [], "entities": []}, {"text": "While Subtask A required a classification of the questions into three distinct categories \"Factual\", \"Socializing\", and \"Opinion\", Subtask B took the subset of the \"Factual\" questions to classify them according to either \"True\", \"False\" or \"NonFactual\" in terms of the actual answer.", "labels": [], "entities": []}, {"text": "Similar tasks were already part of and).", "labels": [], "entities": []}, {"text": "The Tuefact system follows this division, even going as far as using different pre-processing to accommodate for the different needs.", "labels": [], "entities": []}, {"text": "While the question classification is done with a bag of word approach using word trigrams, the answer classification uses character trigrams.", "labels": [], "entities": [{"text": "question classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7370648086071014}]}, {"text": "The models used to make predictions are logistic regression for Subtask A and along short-term memory model (LSTM) for Subtask B. The following section describes the data provided for the tasks.", "labels": [], "entities": [{"text": "along short-term memory model (LSTM)", "start_pos": 78, "end_pos": 114, "type": "METRIC", "confidence": 0.7560038864612579}]}, {"text": "The next two sections describe the two components of our language independent system in detail together with a brief discussion of failed approaches and changes that lead to improvements.", "labels": [], "entities": []}, {"text": "In the last two sections we discuss further work to be done on the TueFact system and our conclusion about the current version of it.", "labels": [], "entities": [{"text": "TueFact system", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.9295219779014587}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Averaged 5-fold cross-validated accuracy on the development set for the character-based embeddings  variation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9789565205574036}]}, {"text": " Table 2: Averaged 5-fold cross-validated accuracy on the development set for the word-based embeddings varia- tion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9789386987686157}]}]}