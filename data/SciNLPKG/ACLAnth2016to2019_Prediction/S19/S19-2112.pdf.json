{"title": [{"text": "HHU at SemEval-2019 Task 6: Context Does Matter -Tackling Offensive Language Identification and Categorization with ELMo", "labels": [], "entities": [{"text": "HHU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8333274126052856}, {"text": "Tackling Offensive Language Identification", "start_pos": 49, "end_pos": 91, "type": "TASK", "confidence": 0.8164431750774384}, {"text": "ELMo", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.5949411392211914}]}], "abstractContent": [{"text": "We present our results for OffensEval: Identifying and Categorizing Offensive Language in Social Media (SemEval 2019-Task 6).", "labels": [], "entities": []}, {"text": "Our results show that context embeddings are important features for the three different sub-tasks in connection with classical machine and with deep learning.", "labels": [], "entities": []}, {"text": "Our best model reached place 3 of 75 in sub-task B with a macro F 1 of 0.719.", "labels": [], "entities": [{"text": "macro F 1", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.7633011738459269}]}, {"text": "Our approaches for sub-task A and C perform less well but could also deliver promising results.", "labels": [], "entities": []}], "introductionContent": [{"text": "User generated content in social media platforms such as Twitter often includes high levels of rude, offensive, or sometimes even hateful language.", "labels": [], "entities": []}, {"text": "The increasing vulgarity in online discussions and user comment sections have recently been discussed as relevant issues in society as well as in science.", "labels": [], "entities": []}, {"text": "The identification of offensiveness, aggression, and hate speech in user-generated content has been addressed in recent research ( and previous shared tasks (.", "labels": [], "entities": [{"text": "identification of offensiveness", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8455082178115845}]}, {"text": "However, detecting such content automatically is still challenging.", "labels": [], "entities": []}, {"text": "We developed classification models to identify offensive language, different categories of offense types, and targets of offensive language throughout the SemEval-2019 challenge on Identifying and Categorizing Offensive Language in Social Media ().", "labels": [], "entities": [{"text": "SemEval-2019 challenge on Identifying and Categorizing Offensive Language in Social Media", "start_pos": 155, "end_pos": 244, "type": "TASK", "confidence": 0.7334720898758281}]}], "datasetContent": [{"text": "The training dataset provided for this task is further described in.", "labels": [], "entities": []}, {"text": "For sub-task A: Offensive Language Detection 4.400 offensive (OFF) and 8.840 not offensive (NOT) tweets are given for the training and 240 offensive plus 620 not offensive tweets are given as test data.", "labels": [], "entities": [{"text": "Offensive Language Detection 4.400 offensive (OFF)", "start_pos": 16, "end_pos": 66, "type": "METRIC", "confidence": 0.6033100448548794}]}, {"text": "Sub-task B: Categorization of Offensive Language is provided with a training set of 3876 targeted insult (TIN) and 524 untargeted (UNT) tweets and a test set of 213 targeted insult plus 27 untargeted tweets.", "labels": [], "entities": []}, {"text": "The train data distribution for sub-task C: Offensive Language Target Identification are 2407 tweets targeting an individual (IND), 1074 targeting a group (GRP) and 395 targeting any other category (OTH).", "labels": [], "entities": [{"text": "Offensive Language Target Identification", "start_pos": 44, "end_pos": 84, "type": "TASK", "confidence": 0.7286902964115143}]}, {"text": "The test data given counts 100, 78 and 35, respectively.", "labels": [], "entities": []}, {"text": "We use's dataset for hate speech detection as an additional source of knowledge and to address the problem of overfitting in our deep learning models.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7748165329297384}]}, {"text": "Tweets were collected and filtered using a lexicon of common hate speech terms.", "labels": [], "entities": []}, {"text": "The remaining tweets were then each classified by at least 3 CrowdFlower users into the three categories hate speech, offensive language or none.", "labels": [], "entities": []}, {"text": "Each tweet was assigned a label, which was chosen based on a majority vote.", "labels": [], "entities": []}, {"text": "In order to match the labels to this task, we merged the categories: hate speech and offensive language into a single offensive label OFF and renamed the label none to NOT.", "labels": [], "entities": [{"text": "OFF", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.7077407240867615}, {"text": "NOT", "start_pos": 168, "end_pos": 171, "type": "DATASET", "confidence": 0.6762033104896545}]}, {"text": "This resulted in 20620 OFF and 4163 NOT labeled additional tweets.", "labels": [], "entities": [{"text": "OFF", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9799928069114685}, {"text": "NOT", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.932791531085968}]}, {"text": "It should be noted that our assumption to equate hate speech and offensive language does not apply in general.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the performance by different  models for sub-task A", "labels": [], "entities": []}, {"text": " Table 2: Overview of the performance by different  models for sub-task B", "labels": [], "entities": []}, {"text": " Table 3: Overview of feature impact on the logistic re- gression system with C = 0.011", "labels": [], "entities": []}, {"text": " Table 4: Overview of feature impact on the SVM sys- tem with C = 6, \u03b3 = 0.0001", "labels": [], "entities": [{"text": "SVM sys- tem", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.8415202796459198}]}, {"text": " Table 1 and Table 5.", "labels": [], "entities": []}, {"text": " Table 5: Test results for sub-task A", "labels": [], "entities": []}, {"text": " Table 6: Test results for sub-task B", "labels": [], "entities": []}]}