{"title": [{"text": "ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples", "labels": [], "entities": [{"text": "BERT", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9975383281707764}]}], "abstractContent": [{"text": "This paper describes our system, Joint Encoders for Stable Suggestion Inference (JESSI), for the SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums.", "labels": [], "entities": [{"text": "Stable Suggestion Inference (JESSI)", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.7720330556233724}, {"text": "SemEval 2019 Task 9", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.9003078192472458}, {"text": "Suggestion Mining from Online Reviews and Forums", "start_pos": 118, "end_pos": 166, "type": "TASK", "confidence": 0.9110837238175529}]}, {"text": "JESSI is a combination of two sentence encoders: (a) one using multiple pre-trained word embeddings learned from log-bilinear regression (GloVe) and translation (CoVe) models , and (b) one on top of word encodings from a pre-trained deep bidirectional transformer (BERT).", "labels": [], "entities": [{"text": "JESSI", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.855573296546936}]}, {"text": "We include a domain adver-sarial training module when training for out-of-domain samples.", "labels": [], "entities": []}, {"text": "Our experiments show that while BERT performs exceptionally well for in-domain samples, several runs of the model show that it is unstable for out-of-domain samples.", "labels": [], "entities": [{"text": "BERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9983147382736206}]}, {"text": "The problem is mitigated tremendously by (1) combining BERT with a non-BERT encoder, and (2) using an RNN-based classifier on top of BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.8683823347091675}, {"text": "BERT", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.6822504997253418}]}, {"text": "Our final models obtained second place with 77.78% F-Score on Subtask A (i.e. in-domain) and achieved an F-Score of 79.59% on Subtask B (i.e. out-of-domain), even without using any additional external data.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9984832406044006}, {"text": "F-Score", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9993010759353638}]}], "introductionContent": [{"text": "Opinion mining) is a huge field that covers many NLP tasks ranging from sentiment analysis, aspect extraction (Mukherjee and Liu, 2012), and opinion summarization (), among others.", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8003582656383514}, {"text": "sentiment analysis", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9545755386352539}, {"text": "aspect extraction", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.8204768300056458}, {"text": "opinion summarization", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.7130624502897263}]}, {"text": "Despite the vast literature on opinion mining, the task on suggestion mining has given little attention.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7249304950237274}, {"text": "suggestion mining", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7252321988344193}]}, {"text": "Suggestion mining) is the task of collecting and categorizing suggestions about a certain product.", "labels": [], "entities": [{"text": "Suggestion mining)", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9378210306167603}]}, {"text": "This is important because while opinions indirectly give hints on how to improve a product (e.g. analyzing reviews), suggestions are direct improvement requests (e.g. tips, advice, recommendations) from people who have used the product.", "labels": [], "entities": []}, {"text": "To this end, organized a shared task specifically on suggestion mining called SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums.", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7221834510564804}, {"text": "SemEval 2019 Task 9", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7494435161352158}, {"text": "Suggestion Mining from Online Reviews and Forums", "start_pos": 99, "end_pos": 147, "type": "TASK", "confidence": 0.8993202192442757}]}, {"text": "The shared task is composed of two subtasks, Subtask A and B.", "labels": [], "entities": []}, {"text": "In Subtask A, systems are tasked to predict whether a sentence of a certain domain (i.e. electronics) entails a suggestion or not given a training data of the same domain.", "labels": [], "entities": []}, {"text": "In Subtask B, systems are tasked to do suggestion prediction of a sentence from another domain (i.e. hotels).", "labels": [], "entities": [{"text": "suggestion prediction of a sentence", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.8362787783145904}]}, {"text": "Organizers observed four main challenges: (a) sparse occurrences of suggestions; (b) figurative expressions; (c) different domains; and (d) complex sentences.", "labels": [], "entities": []}, {"text": "While previous attempts) made use of human-engineered features to solve this problem, the goal of the shared task is to leverage the advancements seen on neural networks, by providing a larger dataset to be used on dataintensive models to achieve better performance.", "labels": [], "entities": []}, {"text": "This paper describes our system JESSI (Joint Encoders for Stable Suggestion Inference).", "labels": [], "entities": [{"text": "JESSI", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.7108806371688843}, {"text": "Stable Suggestion Inference)", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.816703587770462}]}, {"text": "JESSI is built as a combination of two neural-based encoders using multiple pre-trained word embeddings, including BERT (), a pre-trained deep bidirectional transformer that is recently reported to perform exceptionally well across several tasks.", "labels": [], "entities": [{"text": "JESSI", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8927729725837708}, {"text": "BERT", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9973645806312561}]}, {"text": "The main intuition behind JESSI comes from our finding that although BERT gives exceptional performance gains when applied to in-domain samples, it becomes unstable when applied to out-of-domain samples, even when using a domain adversarial training ( module.", "labels": [], "entities": [{"text": "BERT", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.997631311416626}]}, {"text": "This problem is mitigated using two tricks: (1) jointly training BERT with a CNNbased encoder, and (2) using an RNN-based encoder on top of BERT before feeding to the classifier.", "labels": [], "entities": [{"text": "BERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.7683128118515015}]}, {"text": "JESSI is trained using only the datasets given on the shared task, without using any additional external data.", "labels": [], "entities": [{"text": "JESSI", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8822109699249268}]}, {"text": "Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions.", "labels": [], "entities": [{"text": "JESSI", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.4698673188686371}, {"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9898342192173004}]}, {"text": "It also performs well on Subtask B with an F1 score of 79.59%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9864539802074432}]}], "datasetContent": [{"text": "In this section, we show our results and experiments.", "labels": [], "entities": []}, {"text": "We denote JESSI-A as our model for Subtask A (i.e., BERT\u2192CNN+CNN\u2192ATT), and JESSI-B as our model for Subtask B (i.e., BERT\u2192BISRU+CNN\u2192ATT+DOMADV).", "labels": [], "entities": [{"text": "BERT", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9695408344268799}, {"text": "BERT", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9501771926879883}]}, {"text": "The performance of the models is measured and compared using the F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9969214797019958}]}, {"text": "Ablation Studies We present in ablations on our models.", "labels": [], "entities": []}, {"text": "Specifically, we compare JESSI-A with the same model, but without the CNN-based encoder, without the BERT-based encoder, and with the CNN sentence encoder of the BERT-based encoder replaced with the BiSRU variant.", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.928396463394165}]}, {"text": "We also compare JESSI-B with the same  Ablation results for both subtasks using the provided trial sets.", "labels": [], "entities": [{"text": "JESSI-B", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.7780930399894714}, {"text": "Ablation", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9982656836509705}]}, {"text": "The + denotes a replacement of the BERT-based encoder, while the -denotes a removal of a specific component.", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.8293690085411072}]}, {"text": "model, but without the CNN-based encoder, without the BERT-based encoder, without the domain adversarial training module, and with the BiSRU sentence encoder of the BERT-based encoder replaced with the CNN variant.", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.8686636090278625}]}, {"text": "The ablation studies show several observations.", "labels": [], "entities": []}, {"text": "First, jointly combining both BERT-and CNN-based encoders help improve the performance on both subtasks.", "labels": [], "entities": [{"text": "BERT-and", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9199538826942444}]}, {"text": "Second, the more effective sentence encoder for the BERT-based encoder (i.e., CNN versus BiSRU) differs for each subtask; the CNN variant is better for Subtask A, while the BiSRU variant is better for Subtask B. Finally, the domain adversarial training module is very crucial in achieving a significant increase in performance.", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.8483379483222961}]}, {"text": "Out-of-Domain Performance During our experiments, we noticed that BERT is unstable when predicting out-of-domain samples, even when using the domain adversarial training module.", "labels": [], "entities": [{"text": "BERT", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9985163807868958}]}, {"text": "We show in: Summary statistics of the F-Scores of 10 runs of different models on the trial set of Subtask B when doing a 10-fold validation over the available training data.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9962776303291321}]}, {"text": "All models include the domain adversarial training module (+DOMADV), which is omitted for brevity.", "labels": [], "entities": []}, {"text": "bly, achieving varying F-Scores as low as zero and as high as 70.59, with a standard deviation of 31.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9992244243621826}]}, {"text": "Appending a CNN-based sentence encoder (i.e., BERT\u2192CNN) increases the performance, but worsens the stability of the model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9869869947433472}]}, {"text": "Appending an RNN-based sentence encoder (i.e., BERT\u2192BISRU) both increases the performance and improves the model stability.", "labels": [], "entities": [{"text": "BERT", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9946880340576172}, {"text": "BISRU", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.49793338775634766}]}, {"text": "Finally, combining a separate CNN-based encoder (i.e., JESSI-B) improves the performance and stability further.", "labels": [], "entities": [{"text": "JESSI-B", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.8020748496055603}]}, {"text": "presents how JESSI compared to the top performing models during the competition proper.", "labels": [], "entities": [{"text": "JESSI", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.618841826915741}]}, {"text": "Overall, JESSI-A ranks second out of 33 official submissions with an F-Score of 77.78%.", "labels": [], "entities": [{"text": "JESSI-A", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.6978626847267151}, {"text": "F-Score", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9996858835220337}]}, {"text": "Although we were notable to submit JESSI-B during the submission phase, JESSI-B achieves an F-Score of 79.59% on the official test set.", "labels": [], "entities": [{"text": "JESSI-B", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.8112984895706177}, {"text": "JESSI-B", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.7378016710281372}, {"text": "F-Score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.999487042427063}]}, {"text": "This performance is similar to the performance of the model that obtained sixth place in the competition.", "labels": [], "entities": []}, {"text": "We emphasize that JESSI does not use any labeled and external data for Subtask B, and thus is just exposed to the hotels domain using the available unlabeled trial dataset, containing 808 data instances.", "labels": [], "entities": [{"text": "JESSI", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.8420088291168213}]}, {"text": "We expect the model to perform better when additional data from the hotels domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary statistics of the F-Scores of 10  runs of different models on the trial set of Subtask  B when doing a 10-fold validation over the avail- able training data. All models include the domain  adversarial training module (+DOMADV), which  is omitted for brevity.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9945013523101807}]}, {"text": " Table 4: F-Scores of JESSI and top three models  for each subtask. Due to time constraints, we were  not able to submit JESSI-B during the competi- tion. For clarity, we also show our final official  submission (CNN\u2192ATT+DOMADV).", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9794697761535645}, {"text": "JESSI", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.7865155935287476}, {"text": "CNN\u2192ATT+DOMADV", "start_pos": 213, "end_pos": 227, "type": "DATASET", "confidence": 0.7854700446128845}]}]}