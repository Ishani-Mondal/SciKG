{"title": [{"text": "Improving Generalization in Coreference Resolution via Adversarial Training", "labels": [], "entities": [{"text": "Improving Generalization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9292734861373901}, {"text": "Coreference Resolution", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.9560649394989014}]}], "abstractContent": [{"text": "In order for coreference resolution systems to be useful in practice, they must be able to generalize to new text.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.952027291059494}]}, {"text": "In this work, we demonstrate that the performance of the state-of-the-art system decreases when the names of PER and GPE named entities in the CoNLL dataset are changed to names that do not occur in the training set.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 143, "end_pos": 156, "type": "DATASET", "confidence": 0.88161900639534}]}, {"text": "We use the technique of adversarial gradient-based training to retrain the state-of-the-art system and demonstrate that the retrained system achieves higher performance on the CoNLL dataset (both with and without the change of named entities) and the GAP dataset.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 176, "end_pos": 189, "type": "DATASET", "confidence": 0.9609927535057068}, {"text": "GAP dataset", "start_pos": 251, "end_pos": 262, "type": "DATASET", "confidence": 0.9625230729579926}]}], "introductionContent": [{"text": "Through the use of neural networks, performance on the task of coreference resolution has increased significantly over the last few years.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.9632688164710999}]}, {"text": "Still, neural systems trained on the standard coreference dataset have issues with generalization, as shown by.", "labels": [], "entities": []}, {"text": "One way to improve the understanding of how a system overfits a dataset is to study the change in the system's performance when the dataset is modified slightly in a focused and relevant manner.", "labels": [], "entities": []}, {"text": "We take this approach by modifying the test set so that each PER and GPE (person and geopolitical entity) named entity is different from those seen in training.", "labels": [], "entities": [{"text": "PER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9754221439361572}, {"text": "GPE", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8771965503692627}]}, {"text": "In other words, we ensure that there is no leakage of PER and GPE named entities from the training set into the test set.", "labels": [], "entities": [{"text": "PER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9276495575904846}, {"text": "GPE", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.5879833698272705}]}, {"text": "We demonstrate that the performance of the ( ) system, which is the current state-of-the-art, decreases when the named entities are replaced.", "labels": [], "entities": []}, {"text": "An example of a replacement that causes the system to make an error is given in.", "labels": [], "entities": []}, {"text": "The coreference between the two highlighted mentions is correctly predicted by the ( ) system, but after the specified replacement, the system incorrectly resolves \"he\" to a different name occurring outside this excerpt.", "labels": [], "entities": []}, {"text": "Various regularization techniques have been proposed for improving the generalization capability of neural networks, including dropout) and adversarial training (.", "labels": [], "entities": []}, {"text": "The model of ( , like most neural approaches, uses dropout.", "labels": [], "entities": []}, {"text": "In this work, we apply the adversarial fast-gradientsign-method (FGSM) described by to the model of ( , and show that this technique improves the model's generalization even when applied on top of dropout.", "labels": [], "entities": [{"text": "fast-gradientsign-method (FGSM)", "start_pos": 39, "end_pos": 70, "type": "METRIC", "confidence": 0.62063068151474}]}, {"text": "The CoNLL-2012 Shared Task dataset) has been the standard dataset used for both training and evaluating English coreference systems since the dataset was introduced.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task dataset)", "start_pos": 4, "end_pos": 35, "type": "DATASET", "confidence": 0.8719779253005981}]}, {"text": "The dataset includes seven genres that span multiple writing styles and multiple nationalities.", "labels": [], "entities": []}, {"text": "We demonstrate that the system of ( ) retrained with adversarial training achieves state-of-the-art performance on the original CoNLL-2012 dataset) as well as the CoNLL-2012 dataset with changed named entities.", "labels": [], "entities": [{"text": "CoNLL-2012 dataset", "start_pos": 128, "end_pos": 146, "type": "DATASET", "confidence": 0.9829005002975464}, {"text": "CoNLL-2012 dataset", "start_pos": 163, "end_pos": 181, "type": "DATASET", "confidence": 0.9765805602073669}]}, {"text": "Furthermore, the system trained with the adversarial method ex-hibits state-of-the-art performance on the GAP dataset (), a recently released dataset focusing on resolving pronouns to people's names in excerpts from Wikipedia.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.9440900683403015}]}, {"text": "The code and other relevant files for this project can be found via https://cogcomp.org/ page/publication_view/871.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained the ( ) model architecture with the adversarial approach on the CoNLL training set for 355000 iterations (the same number of iterations for which the original model was trained) with the same training hyperparameters used by original model.", "labels": [], "entities": [{"text": "CoNLL training set", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.9775760769844055}]}, {"text": "For comparing with the ( and ( ) systems, we use the pretrained models released by the authors.", "labels": [], "entities": []}, {"text": "The datasets used for evaluation are the CoNLL and GAP datasets.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9022015929222107}, {"text": "GAP datasets", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9150282144546509}]}, {"text": "shows the performance on the CoNLL test set, as measured by CoNLL F1, of the ( ) system with and without our adversarial training approach.", "labels": [], "entities": [{"text": "CoNLL test set", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9637555281321207}, {"text": "CoNLL", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.7016772031784058}, {"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.5860937833786011}]}, {"text": "The replacement of PER and GPE entities decreased the performance of the original system by more than 1 F1.", "labels": [], "entities": [{"text": "PER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9054508209228516}, {"text": "GPE", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.44951510429382324}, {"text": "F1", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9794811010360718}]}, {"text": "The GAP dataset) focuses on resolving pronouns to named people in excerpts from Wikipedia.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8933670222759247}]}, {"text": "The dataset, which is gender-balanced, consists of examples in which  Original\" refers to the original test set, and \"No Leakage\" refers to the test set modified with the replacement of named entities described in Section 4.", "labels": [], "entities": []}, {"text": "For each dataset, highest score for each dataset is bolded and is underlined if the difference between it and the other model's score is statistically significant (p < 0.20 per a stratified approximate randomization test similar to that of).", "labels": [], "entities": []}, {"text": "the system must determine whether a given pronoun refers to one, both, or neither of two given names.", "labels": [], "entities": []}, {"text": "Thus, the task can be viewed a binary classification task in which the input is a (pronoun, name) pair and the output is True if the pair is coreferent and False otherwise.", "labels": [], "entities": [{"text": "False", "start_pos": 156, "end_pos": 161, "type": "METRIC", "confidence": 0.9890220165252686}]}, {"text": "Performance is evaluated using the F1 score in this binary classification setup.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9715265035629272}]}, {"text": "shows the performance on the GAP test set of the (Lee et al., 2017) 3 and ( ) systems as well as the system trained with our adversarial method.", "labels": [], "entities": [{"text": "GAP test set", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.885750412940979}]}, {"text": "The adversarially trained system performs significantly better over the entire dataset in comparison to the previous systems, and the difference is consistent between genders.", "labels": [], "entities": []}, {"text": "In particular, we observe that the bias (i.e. ratio of female to male F1 score) is roughly the same (0.93) for the ( ) system with and without adversarial training and that this bias is better (i.e. the ratio is closer to 1) than that exhibited by the () system (0.87).", "labels": [], "entities": [{"text": "F1 score)", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9885102907816569}]}], "tableCaptions": [{"text": " Table 3: Results (CoNLL F1) on the CoNLL Test Set.  \"", "labels": [], "entities": [{"text": "CoNLL Test Set", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9823041756947836}]}, {"text": " Table 4: Results (F1 metric defined by (Webster et al.,  2018)) on the GAP Test Set. M refers to male pro- nouns, F refers to female pronouns, and O refers to the  full evaluation data. For each category, highest score  is bolded and underlined if difference between it and  next-highest score is statistically significant (p < 0.05  per the McNemar test", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9943601489067078}, {"text": "GAP Test Set", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.8350317080815634}, {"text": "McNemar test", "start_pos": 343, "end_pos": 355, "type": "DATASET", "confidence": 0.9392459094524384}]}]}