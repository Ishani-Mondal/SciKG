{"title": [{"text": "UNBNLP at SemEval-2019 Task 5 and 6: Using Language Models to Detect Hate Speech and Offensive Language", "labels": [], "entities": [{"text": "UNBNLP at SemEval-2019 Task 5", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.8400871872901916}]}], "abstractContent": [{"text": "In this paper we apply a range of approaches to language modeling-including word-level n-gram and neural language models, and character-level neural language models-to the problem of detecting hate speech and offensive language.", "labels": [], "entities": [{"text": "language modeling-including word-level n-gram", "start_pos": 48, "end_pos": 93, "type": "TASK", "confidence": 0.796593114733696}, {"text": "detecting hate speech and offensive language", "start_pos": 183, "end_pos": 227, "type": "TASK", "confidence": 0.7858990132808685}]}, {"text": "Our findings indicate that language models are able to capture knowledge of whether text is hateful or offensive.", "labels": [], "entities": []}, {"text": "However, our findings also indicate that more-conventional approaches to text classification often perform similarly or better.", "labels": [], "entities": [{"text": "text classification", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8348788917064667}]}], "introductionContent": [{"text": "SemEval 2019 Task 5 focuses on detecting hate speech in social media text, while Task 6 considers identifying offensive language.", "labels": [], "entities": [{"text": "SemEval 2019 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.5718318422635397}, {"text": "detecting hate speech in social media text", "start_pos": 31, "end_pos": 73, "type": "TASK", "confidence": 0.8468983939715794}]}, {"text": "Despite the differences between hate speech and offensive language, each of these tasks can be viewed as a binary classification problem.", "labels": [], "entities": []}, {"text": "In each case, goldstandard training data is provided on which supervised approaches can be trained.", "labels": [], "entities": []}, {"text": "In this paper we consider whether approaches to classification based on language models -including word-and character-level neural language models, as well as more-conventional (word-level) n-gram language models -are able to distinguish between hateful and not hateful, and offensive and not offensive, language.", "labels": [], "entities": []}, {"text": "We find that these approaches outperform a most-frequent class baseline, indicating that language models can capture some knowledge of whether text is hateful or offensive.", "labels": [], "entities": []}, {"text": "However, for task 5 -for which the testing data was made available for followup experiments -we also find that more-conventional approaches to supervised classification, such as naive Bayes and fastText, often give similar or better results.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 143, "end_pos": 168, "type": "TASK", "confidence": 0.6867072880268097}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Macro average F1-score (F), macro average precision (P), macro average recall (R), and accuracy (A)  on Task 5 subtask A using word-level n-gram and LSTM LMs, a character-level LM (Char), a word-level LSTM  and character-level LM augmented with a special class token (LSTM+CT and Char+CT), multinomial naive Bayes  (NB), and fastText on the development and test sets. The SVC and most-frequent class (MFC) baselines provided  by the shared task are also shown. The best result for each language, dataset, and evaluation measure is shown in  boldface.", "labels": [], "entities": [{"text": "Macro average F1-score (F)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.677152951558431}, {"text": "macro average precision (P)", "start_pos": 38, "end_pos": 65, "type": "METRIC", "confidence": 0.7355010310808817}, {"text": "macro average recall (R)", "start_pos": 67, "end_pos": 91, "type": "METRIC", "confidence": 0.7368502169847488}, {"text": "accuracy (A)", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.9209872782230377}, {"text": "fastText", "start_pos": 335, "end_pos": 343, "type": "METRIC", "confidence": 0.8720099925994873}]}, {"text": " Table 2: Macro-average F1-score (F) and accuracy (A) for each sub-task of Task 6 using word-level n-gram and  LSTM LMs, a character-level LM (Char), and a most-frequent class baseline (MFC). The best result for each  sub-task and evaluation measure is shown in boldface.", "labels": [], "entities": [{"text": "F1-score (F)", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.928145244717598}, {"text": "accuracy (A)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9342685639858246}]}]}