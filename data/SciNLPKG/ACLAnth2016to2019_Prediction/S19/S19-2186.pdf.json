{"title": [{"text": "Tintin at SemEval-2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens", "labels": [], "entities": [{"text": "Detecting Hyperpartisan News Article", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.8367571979761124}]}], "abstractContent": [{"text": "Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of Se-mEval 2019, is exclusively based on the tokens that makeup the documents and a standard supervised learning procedure.", "labels": [], "entities": [{"text": "Hyperpartisan News Detection task of Se-mEval 2019", "start_pos": 48, "end_pos": 98, "type": "TASK", "confidence": 0.769363305398396}]}, {"text": "It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first.", "labels": [], "entities": [{"text": "distinguishing documents published by hyperpartisan media outlets", "start_pos": 88, "end_pos": 153, "type": "TASK", "confidence": 0.8543453812599182}]}, {"text": "An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "This report presents the participation of Tintin (Centre for English Corpus Linguistics) in Task 4 of SemEval 2019 entitled Hyperpartisan News Detection.", "labels": [], "entities": [{"text": "SemEval 2019", "start_pos": 102, "end_pos": 114, "type": "TASK", "confidence": 0.8909510672092438}, {"text": "Hyperpartisan News Detection", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.674962192773819}]}, {"text": "This task is defined as follows by the organizers : \"Given a news article text, decide whether it follows a hyperpartisan argumentation, i.e., whether it exhibits blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person.\"", "labels": [], "entities": []}, {"text": "This question is related to the detection of fake news, a hot topic in our internet and social media world.", "labels": [], "entities": [{"text": "detection of fake news", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8647554069757462}]}, {"text": "There are, however, essential differences between these two tasks.", "labels": [], "entities": []}, {"text": "An article can be hyperpartisan without mentioning any fake content.", "labels": [], "entities": []}, {"text": "Another difference is that it is a news article (or even a claim) that is fake whereas a news article but also a media outlet (or publisher) can be considered as hyperpartisan.", "labels": [], "entities": []}, {"text": "The challenge organizers took these two possibilities (i.e. an article or a publisher can be hyperpartisan) into account by offering two test sets.", "labels": [], "entities": []}, {"text": "The main test set, the labels-by-article one, contained documents that had been assessed as hyperpartisan or not by human judges, while the documents 1 https://pan.webis.de/semeval19/semeval19-web in the secondary test set, the labels-by-publisher one, had been categorized according to whether their publishers were considered to be hyperpartisan or not by organizations that disseminate this type of evaluation.", "labels": [], "entities": []}, {"text": "In both these test sets, participants had to decide whether a document expresses a hyperpartisan point-of-view or not.", "labels": [], "entities": []}, {"text": "If the main task is particularly interesting, the secondary task is also relevant because it is about achieving through an automatic procedure what a series of organizations manually perform in away that is sometimes called into question as to its impartiality and quality.", "labels": [], "entities": []}, {"text": "However, in this context, the task would preferably be evaluated, not at the document level, but at the publisher level by providing several documents from a publisher and asking whether the publisher is biased or not.", "labels": [], "entities": []}, {"text": "Nevertheless, it can be assumed that many systems developed for categorizing publishers will start by evaluating each document separately and thus getting good performance in the current secondary task is at least a first step.", "labels": [], "entities": []}, {"text": "To take up these tasks, the question is how to determine automatically whether a document is hyperpartisan or not.", "labels": [], "entities": []}, {"text": "This question has not attracted much attention in the literature, but, very recently, proposed to use stylometric features such as characters, stop words and POStag n-grams, and readability measures.", "labels": [], "entities": []}, {"text": "They compared the effectiveness of this approach to several baselines including a classical bag-of-words feature approach 2.", "labels": [], "entities": []}, {"text": "Their stylistic approach obtained an accuracy of 0.75 in 3-fold cross-validation in which publishers present in the validation fold were unseen during the learning phase.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9994127750396729}]}, {"text": "The bag-of-words feature approach obtained an accuracy of 0.71, which is not much lower.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9995700716972351}]}, {"text": "These results were obtained on a small size corpus (due to the cost of the manual fact-checking needed for the fake-news part of the study) containing only nine different publishers.", "labels": [], "entities": []}, {"text": "It is therefore not evident that this corpus was large enough to evaluate the degree of generalizability of the bag-of-words approach, especially since) emphasizes that using bag-of-words features potentially related to the topic of the documents renders the resulting classifier not generalizable.", "labels": [], "entities": []}, {"text": "In contrast, the datasets prepared for the present challenge are significantly larger since the latest versions available contain more than 750,000 documents and more than 240 different media outlets.", "labels": [], "entities": []}, {"text": "Therefore, it seemed interesting to evaluate the effectiveness of a bag-of-words approach for the labels-by-publisher task, the one used by.", "labels": [], "entities": []}, {"text": "This is the purpose of this study.", "labels": [], "entities": []}, {"text": "Another reason why I chose to focus on the labelsby-publisher task is that I was unclear about what could be learned on the basis of the labels-bypublisher sets for the labels-by-article test set.", "labels": [], "entities": []}, {"text": "If one can think that some publishers almost always distribute hyperpartisan articles, it seems doubtful that this is the case for all of them.", "labels": [], "entities": []}, {"text": "The next sections of this paper describe the datasets, the developed system, and the obtained results as well as an analysis of the most important features.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Some of the 200 most useful features for predicting hyperpartisanship.", "labels": [], "entities": []}]}