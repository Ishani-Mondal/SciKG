{"title": [{"text": "ConvAI at SemEval-2019 Task 6: Offensive Language Identification and Categorization with Perspective and BERT", "labels": [], "entities": [{"text": "Offensive Language Identification", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.7098437050978342}, {"text": "BERT", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.99516761302948}]}], "abstractContent": [{"text": "This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media.", "labels": [], "entities": [{"text": "toxicity detection", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7360804975032806}, {"text": "identifying and categorizing offensive language in social media", "start_pos": 125, "end_pos": 188, "type": "TASK", "confidence": 0.6892125234007835}]}, {"text": "Perspective is an API, that serves multiple machine learning models for the improvement of conversations online, as well as a toxicity detection system, trained on a wide variety of comments from platforms across the Internet.", "labels": [], "entities": [{"text": "toxicity detection", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7107441425323486}]}, {"text": "BERT is a recently popular language representation model, fine tuned per task and achieving state of the art performance in multiple NLP tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9749677777290344}]}, {"text": "Perspective performed better than BERT in detecting toxicity, but BERT was much better in categorizing the offensive type.", "labels": [], "entities": [{"text": "Perspective", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.7911174893379211}, {"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9983652234077454}, {"text": "BERT", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9973189234733582}]}, {"text": "Both baselines were ranked surprisingly high in the SEMEVAL-2019 OFFENSE-VAL competition, Perspective in detecting an offensive post (12th) and BERT in categorizing it (11th).", "labels": [], "entities": [{"text": "SEMEVAL-2019", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.581671953201294}, {"text": "OFFENSE-VAL", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.365323930978775}, {"text": "BERT", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9985454082489014}]}, {"text": "The main contribution of this paper is the assessment of two strong baselines for the identification (Perspective) and the cat-egorization (BERT) of offensive language with little or no additional training data.", "labels": [], "entities": [{"text": "cat-egorization (BERT)", "start_pos": 123, "end_pos": 145, "type": "METRIC", "confidence": 0.6836423724889755}]}], "introductionContent": [{"text": "Offensive language detection refers to computational approaches for detecting abusive language, such as threats, insults, calumniation, discrimination, swearing), which could be targeted (at an individual or group) or not ().", "labels": [], "entities": [{"text": "Offensive language detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7247554659843445}]}, {"text": "These computational approaches are often used by moderators who face an increasing volume of abusive content and would like assistance in managing it efficiently.", "labels": [], "entities": []}, {"text": "Although offensive language detection is not anew task, the creation of large See, for example, https://goo.gl/VQNDNX.", "labels": [], "entities": [{"text": "offensive language detection", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.6540247897307078}, {"text": "VQNDNX", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.9308152198791504}]}, {"text": "corpora (, along with recent advances in pre-training text representations) allow for much more efficient approaches.", "labels": [], "entities": []}, {"text": "Furthermore, while new competitions and corpora are being introduced (), 2 there is a need for strong baselines to assess the performance of more complex systems.", "labels": [], "entities": []}, {"text": "This paper assesses two systems for the detection and categorization of offensive language, which require few or no task-specific annotated training instances.", "labels": [], "entities": [{"text": "detection and categorization of offensive language", "start_pos": 40, "end_pos": 90, "type": "TASK", "confidence": 0.7565438201030096}]}, {"text": "The first baseline is a Convolutional Neural Network (CNN) for toxicity detection, trained on millions of user comments from different online publishers, which is made publicly available through the Perspective API.", "labels": [], "entities": [{"text": "toxicity detection", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8105640411376953}]}, {"text": "This model requires no extra training or fine tuning and can be directly applied to score unseen posts.", "labels": [], "entities": []}, {"text": "The second strong baseline is the recently popular Bidirectional Encoder Representations from Transformers (BERT), a pre-trained model that has been reported to achieve state of the art performance in multiple NLP tasks with limited fine-tuning on task-specific training data.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformers (BERT)", "start_pos": 51, "end_pos": 113, "type": "TASK", "confidence": 0.6406952627003193}]}, {"text": "Section 2 below summarizes related work and Section 3 discusses the SEMEVAL-2019 OFFEN-SEVAL dataset we used.", "labels": [], "entities": [{"text": "SEMEVAL-2019 OFFEN-SEVAL dataset", "start_pos": 68, "end_pos": 100, "type": "DATASET", "confidence": 0.5897990862528483}]}, {"text": "In Section 4 we describe the two proposed baselines and we report experimental results in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes our work and suggests future directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of tweets, size of vocabulary, and average number of tokens per tweet, for each label (class).  In Subtask A, the labels are 'not offensive' (NOT) or 'offensive' (OFF). In Subtask B, the labels are 'not targeted  threat' (UNT) and 'targeted insult or threat' (TIN). In Subtask C, they are 'targeted insult or threat towards an  individual' (IND), 'towards a group' (GRP), or 'towards another target' (OTH).", "labels": [], "entities": []}, {"text": " Table 2: The tweets with the highest Perspective score  per abusiveness type, on a trial dataset of 320 tweets  shared by the competition organizers.", "labels": [], "entities": [{"text": "Perspective score", "start_pos": 38, "end_pos": 55, "type": "METRIC", "confidence": 0.9718108177185059}]}, {"text": " Table 3: Results for Subtask A.", "labels": [], "entities": [{"text": "Subtask A", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9472849369049072}]}, {"text": " Table 4: Results for Subtask B.", "labels": [], "entities": [{"text": "Subtask B", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9654717743396759}]}]}