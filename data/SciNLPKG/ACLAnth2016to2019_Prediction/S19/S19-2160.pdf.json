{"title": [{"text": "Dick-Preston and Morbo at SemEval-2019 Task 4: Transfer Learning for Hyperpartisan News Detection", "labels": [], "entities": [{"text": "SemEval-2019 Task 4", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8328731854756674}, {"text": "Hyperpartisan News Detection", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.6452500919500986}]}], "abstractContent": [{"text": "Ina world of information operations, influence campaigns, and fake news, classification of news articles as following hyperpartisan argu-mentation or not is becoming increasingly important.", "labels": [], "entities": [{"text": "classification of news articles", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.8486486524343491}]}, {"text": "We present a deep learning-based approach in which a pre-trained language model has been fine-tuned on domain-specific data and used for classification of news articles, as part of the SemEval-2019 task on hyper-partisan news detection.", "labels": [], "entities": [{"text": "classification of news articles", "start_pos": 137, "end_pos": 168, "type": "TASK", "confidence": 0.8807801157236099}, {"text": "hyper-partisan news detection", "start_pos": 206, "end_pos": 235, "type": "TASK", "confidence": 0.7235801219940186}]}, {"text": "The suggested approach yields accuracy and F1-scores around 0.8 which places the best performing classifier among the top-5 systems in the competition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9997747540473938}, {"text": "F1-scores", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9996647834777832}]}], "introductionContent": [{"text": "In today's polarized media and political landscapes, the challenge of determining whether a news article is biased or not is highly topical.", "labels": [], "entities": []}, {"text": "In the hyperpartisan news detection task ( of the International Workshop on Semantic Evaluation (SemEval) 2019, the task is to predict whether a given news article text follows a hyperpartisan (extreme one-sided) argumentation or not, i.e., whether it exhibits blind or prejudiced allegiance to one party, cause, or person ( . As part of this challenge, participating research teams got access to two datasets: 1. by-publisher: A well-balanced dataset consisting of 750,000 articles in which the data have been labeled by the overall bias of the publisher, as provided by journalists or factchecking sites.", "labels": [], "entities": [{"text": "hyperpartisan news detection task", "start_pos": 7, "end_pos": 40, "type": "TASK", "confidence": 0.737832710146904}, {"text": "International Workshop on Semantic Evaluation (SemEval) 2019", "start_pos": 50, "end_pos": 110, "type": "TASK", "confidence": 0.566870113213857}]}, {"text": "2. by-article: A smaller dataset consisting of 645 articles for which crowdsourcing workers have agreed on the labeling of the articles as being hyperpartisan (37%) or not (63%).", "labels": [], "entities": []}, {"text": "A similar but more well-balanced test dataset (to which the participating teams have not got direct access) has been used for evaluating the accuracy, precision, recall, and F1-score of systems developed by the participating research teams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9995755553245544}, {"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9987571239471436}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9992396831512451}, {"text": "F1-score", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.999601423740387}]}, {"text": "In this system description paper we present the results for the two participating research teams from the Swedish Defence Research Agency (FOI): 1) dick-preston and 2) morbo.", "labels": [], "entities": [{"text": "Swedish Defence Research Agency (FOI)", "start_pos": 106, "end_pos": 143, "type": "DATASET", "confidence": 0.7601197787693569}]}, {"text": "The teams contributed with separate systems for the early-bird deadline and for the final submission.", "labels": [], "entities": []}, {"text": "In the early phase we used traditional machine learning classifiers such as logistic regression and support vector machines (SVMs), built upon traditional text features such as word and character n-gram term frequencies (weighted with inverse document frequency).", "labels": [], "entities": []}, {"text": "These classifiers have been used as baselines to which more \"modern\" NLP classifiers have been compared.", "labels": [], "entities": []}, {"text": "For the final submission both teams made use of transfer learning-based Universal Language Model FineTuning (ULMFiT) models.", "labels": [], "entities": []}, {"text": "The difference in the teams' final systems is the percentage of data used for training/validation splits when fine-tuning the models and the number of epochs for which the models were trained.", "labels": [], "entities": []}, {"text": "Despite that only a few hundred examples were used for fine-tuning the pre-trained ULMFiT-models, accuracies and F1-scores of approximately 0.8 were achieved on the unseen test data.", "labels": [], "entities": [{"text": "ULMFiT-models", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.7366460561752319}, {"text": "accuracies", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9980512857437134}, {"text": "F1-scores", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9994831085205078}]}, {"text": "This resulted in a fifth place for the team dick-preston and seventh place for the team morbo out of 42 participating teams, as reported on the competition leaderboard . The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present the machine learning algorithms and features which have been used for building the hyperpartisan news article classifiers used in the competition.", "labels": [], "entities": []}, {"text": "In Section 3 we outline the conducted experiments, present the used hyperparameters, and describe the obtained results.", "labels": [], "entities": []}, {"text": "Finally, we present overall conclusions and discuss ideas for future work in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the first part of the competition we experimented with baseline classifiers to which we later on could compare the classification accuracy of more advanced algorithms on hold-out validation datasets constructed from the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.8360164761543274}]}, {"text": "The experiments with the baseline classifiers were performed using scikit-learn, while latter experiments have been carried out using various deep learning frameworks (including TensorFlow and Keras).", "labels": [], "entities": []}, {"text": "The final ULMFiT classifier implementations and experiments have been carried out using PyTorch and the fastai library.", "labels": [], "entities": [{"text": "ULMFiT classifier", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.71429443359375}, {"text": "PyTorch", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.8756380081176758}]}, {"text": "We first experimented with a number of simple classifiers which were used as baselines: \u2022 SVM (LinearSVC) \u2022 Logistic Regression \u2022 Random Forest  The embedding layer of our ULMFiT classifiers uses word embeddings with an embedding size of 400.", "labels": [], "entities": []}, {"text": "For the sequential linear layers that have been attached to the pre-trained LSTM layers we have used a momentum of 0.1 for the BatchNorm and a dropout probability p of 0.2 for the first linear layer and 0.1 the last linear layer.", "labels": [], "entities": [{"text": "BatchNorm", "start_pos": 127, "end_pos": 136, "type": "DATASET", "confidence": 0.9033271074295044}]}, {"text": "We have gradually unfreezed different blocks of the model to avoid catastrophic forgetting.", "labels": [], "entities": []}, {"text": "Different slanted learning rates and number of training epochs have been used for the different submitted FOI classifiers, but we have in general found learning rates around 0.01 to work well for fine-tuning just the last layer, and then using lower magnitude learning rates when unfreezing earlier layers.", "labels": [], "entities": []}, {"text": "We evaluated the fine-tuned ULMFiT classifiers by splitting the available by-article dataset into a training set (85 %) and a validation set (15 %).", "labels": [], "entities": [{"text": "ULMFiT classifiers", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7673650979995728}]}, {"text": "This was attempted on a few random splits for which we consistently reached accuracies on the validation set over 0.95.", "labels": [], "entities": []}, {"text": "In the end we submitted models trained on 85 % and 100 % of the training data but the one trained on 85 % performed the best, probably due to overfitting of the other model (which is natural since it was hard to know how many epochs the model should be trained for when not having any separate validation data to evaluate on).", "labels": [], "entities": []}, {"text": "When the best performing model was submitted for evaluation on the test set it obtained an accuracy of 0.80 which resulted in a fifth place in the final leaderboard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9995785355567932}]}, {"text": "In.2 we compare the accuracy and running time of our best performing ULMFiT classifier on the test data and contrast them to the corresponding measures for our FOI linear SVM classifier, a random baseline provided by the task organizers, and the classifier developed by the winning team.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9993500113487244}, {"text": "FOI linear SVM classifier", "start_pos": 160, "end_pos": 185, "type": "DATASET", "confidence": 0.6203295961022377}]}, {"text": "As can be seen, the accuracy of the ULMFiT classifier is marginally lower than the winning classifier, while the running time seems to be much lower 3 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9997249245643616}]}], "tableCaptions": []}