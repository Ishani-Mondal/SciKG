{"title": [], "abstractContent": [{"text": "In this paper, we present an approach for classifying news articles as biased (i.e., hyperpar-tisan) or unbiased, based on a convolutional neural network.", "labels": [], "entities": []}, {"text": "We experiment with various embedding methods (pretrained and trained on the training dataset) and variations of the convolutional neural network architecture and compare the results.", "labels": [], "entities": []}, {"text": "When evaluating our best performing approach on the actual test data set of the SemEval 2019 Task 4, we obtained relatively low precision and accuracy values, while gaining the highest recall rate among all 42 participating teams.", "labels": [], "entities": [{"text": "test data set of the SemEval 2019 Task 4", "start_pos": 59, "end_pos": 99, "type": "DATASET", "confidence": 0.7447031537691752}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.999147891998291}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9914122819900513}, {"text": "recall rate", "start_pos": 185, "end_pos": 196, "type": "METRIC", "confidence": 0.990561455488205}]}], "introductionContent": [{"text": "Hyperpartisan news detection describes the task of given a news article text, decide whether it follows a hyperpartisan argumentation, i.e., whether it exhibits blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person (.", "labels": [], "entities": [{"text": "Hyperpartisan news detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.679022471110026}]}, {"text": "In recent years, hyperpartisan news detection, which we consider synonymous to news bias detection, has attracted the interest of researchers and various approaches for news bias detection have been developed.", "labels": [], "entities": [{"text": "hyperpartisan news detection", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.6423563261826833}, {"text": "news bias detection", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7338000337282816}, {"text": "news bias detection", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7934633692105612}]}, {"text": "However, the definition of bias and the task set-up of identifying biased news articles differs from authors to authors.", "labels": [], "entities": [{"text": "identifying biased news articles", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.810088112950325}]}, {"text": "For instance, authors might consider the bias in terms of the writing style, while others might consider it in relation to fact selection (.", "labels": [], "entities": [{"text": "fact selection", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.7243872433900833}]}, {"text": "In this paper, we use the definition and data set of, which deliberately uses the generic definition outlined at the beginning.", "labels": [], "entities": []}, {"text": "Note that news bias detection differs from related tasks such as opinion finding, sentiment analysis, fake news detection (, claim assessment (, argumentation mining on news articles (, and personality detection based on texts (.", "labels": [], "entities": [{"text": "news bias detection", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.829628566900889}, {"text": "opinion finding", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7609363794326782}, {"text": "sentiment analysis", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.9255989193916321}, {"text": "fake news detection", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.6325543721516927}, {"text": "claim assessment", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.6636911779642105}, {"text": "argumentation mining on news articles", "start_pos": 145, "end_pos": 182, "type": "TASK", "confidence": 0.7987736105918884}, {"text": "personality detection", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.7239431738853455}]}, {"text": "From a technical perspective, in recent years deep learning techniques have outperformed traditional methods concerning various NLP tasks.", "labels": [], "entities": []}, {"text": "This also applies to news article classification tasks (.", "labels": [], "entities": [{"text": "news article classification tasks", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.6612803041934967}]}, {"text": "Indeed, in the SemEval Twitter sentiment analysis competition in, among the most popular (and apparently effective) deep learning techniques were convolutional neural networks (CNNs).", "labels": [], "entities": [{"text": "SemEval Twitter sentiment analysis competition", "start_pos": 15, "end_pos": 61, "type": "TASK", "confidence": 0.8882484436035156}]}, {"text": "Thus, we decided to build a hyperpartisan news classifier based on a CNN.", "labels": [], "entities": []}, {"text": "Next to our basic model, we also develop and evaluate variations of our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We developed our models using Keras v2.1.2 with a Tensorflow v1.0.0 backend.", "labels": [], "entities": []}, {"text": "Training the model was performed on a machine with 64GB memory and a GeForce GTX 1080 Ti GPU.", "labels": [], "entities": [{"text": "GeForce GTX 1080 Ti GPU", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.8644350051879883}]}, {"text": "We implemented and evaluated our basic model using several word-based embedding methods, where an embedding vector is generated for each unique word on the text corpus.", "labels": [], "entities": []}, {"text": "These embeddings can be categorized into two main categories: (1) pretrained word vectors and (2) custom word vectors (here, trained on the articles' content).", "labels": [], "entities": []}, {"text": "We fine-tuned the hyperparameters of our basic model using the dedicated validation data set.", "labels": [], "entities": []}, {"text": "In the end, we used the parameters as shown in.", "labels": [], "entities": []}, {"text": "Note that these optimal hyperparameters showed to be the best-performing ones on both Google's prebuilt word2vec and the custom word2vec which we had trained on our training data.", "labels": [], "entities": []}, {"text": "presents the evaluation results for all used embedding methods.", "labels": [], "entities": []}, {"text": "The embedding models gave similar results, with some slight differences in the model accuracies for some of them.", "labels": [], "entities": []}, {"text": "Thus, we decided to go with one of the better performing models for the final SemEval test runs (see Sec. 3.3.3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of biased and unbiased articles  (\"a.\" for articles) in the training and validation data set.", "labels": [], "entities": [{"text": "training and validation data set", "start_pos": 83, "end_pos": 115, "type": "DATASET", "confidence": 0.6925881385803223}]}, {"text": " Table 3: Evaluation results on the custom validation split using various embedding methods.", "labels": [], "entities": []}]}