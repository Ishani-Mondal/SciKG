{"title": [], "abstractContent": [{"text": "Automatically generating animation from natural language text finds application in a number of areas e.g. movie script writing, instructional videos, and public safety.", "labels": [], "entities": [{"text": "movie script writing", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.6626702646414439}]}, {"text": "However, translating natural language text into animation is a challenging task.", "labels": [], "entities": [{"text": "translating natural language text", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.8656514585018158}]}, {"text": "Existing text-to-animation systems can handle only very simple sentences, which limits their applications.", "labels": [], "entities": []}, {"text": "In this paper, we develop a text-to-animation system which is capable of handling complex sentences.", "labels": [], "entities": []}, {"text": "We achieve this by introducing a text simplification step into the process.", "labels": [], "entities": []}, {"text": "Building on an existing animation generation system for screenwriting, we create a robust NLP pipeline to extract information from screen-plays and map them to the system's knowledge base.", "labels": [], "entities": []}, {"text": "We develop a set of linguistic transformation rules that simplify complex sentences.", "labels": [], "entities": []}, {"text": "Information extracted from the simplified sentences is used to generate a rough storyboard and video depicting the text.", "labels": [], "entities": []}, {"text": "Our sentence simplification module outperforms existing systems in terms of BLEU and SARI metrics.We further evaluated our system via a user study: 68 % participants believe that our system generates reasonable animation from input screen-plays.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9978753328323364}, {"text": "SARI", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.8302735090255737}]}], "introductionContent": [{"text": "Generating animation from texts can be useful in many contexts e.g. movie script writing (;, instructional videos (), and public safety ().", "labels": [], "entities": [{"text": "movie script writing", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.6199003159999847}]}, {"text": "Text-toanimation systems can be particularly valuable for screenwriting by enabling faster iteration, prototyping and proof of concept for content creators.", "labels": [], "entities": []}, {"text": "In this paper, we propose a text-to-animation generation system.", "labels": [], "entities": [{"text": "text-to-animation generation", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7445603907108307}]}, {"text": "Given an input text describing a certain activity, the system generates a rough animation of the text.", "labels": [], "entities": []}, {"text": "We are addressing a practical setting, where we do not have any annotated data for training a supervised end-to-end system.", "labels": [], "entities": []}, {"text": "The aim is not to generate a polished, final animation, but a pre-visualization of the input text.", "labels": [], "entities": []}, {"text": "The purpose of the system is not to replace writers and artists, but to make their work more efficient and less tedious.", "labels": [], "entities": []}, {"text": "We are aiming fora system which is robust and could be deployed in a production environment.", "labels": [], "entities": []}, {"text": "Existing text-to-animation systems for screenwriting ( \u00a72) visualize stories by using a pipeline of Natural Language Processing (NLP) techniques for extracting information from texts and mapping them to appropriate action units in the animation engine.", "labels": [], "entities": []}, {"text": "The NLP modules in these systems translate the input text into predefined intermediate action representations and the animation generation engine produces simple animation from these representations.", "labels": [], "entities": []}, {"text": "Although these systems can generate animation from carefully handcrafted simple sentences, translating real screenplays into coherent animation still remains a challenge.", "labels": [], "entities": []}, {"text": "This can be attributed to the limitations of the NLP modules used with regard to handling complex sentences.", "labels": [], "entities": []}, {"text": "In this paper, we try to address the limitations of the current text-to-animation systems.", "labels": [], "entities": []}, {"text": "Main contributions of this paper are: We propose a screenplay parsing architecture which generalizes well on different screenplay formats ( \u00a73.1).", "labels": [], "entities": [{"text": "screenplay parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8092425167560577}]}, {"text": "We develop a rich set of linguistic rules to reduce complex sentences into simpler ones to facilitate information extraction ( \u00a73.2).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.8184849917888641}]}, {"text": "We develop anew NLP pipeline to generate animation from actual screenplays ( \u00a73).", "labels": [], "entities": []}, {"text": "The potential applications of our contributions are not restricted to just animating screenplays.", "labels": [], "entities": []}, {"text": "The techniques we develop are fairly general and can be used in other applications as well e.g. in-", "labels": [], "entities": []}], "datasetContent": [{"text": "There are no standard corpora for text-toanimation generation.", "labels": [], "entities": [{"text": "text-toanimation generation", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7214999496936798}]}, {"text": "It is also not clear how should such systems be evaluated and what should be the most appropriate evaluation metric.", "labels": [], "entities": []}, {"text": "Nevertheless, it is important to assess how our system is performing.", "labels": [], "entities": []}, {"text": "We evaluate our system using two types of evaluation: Intrinsic and Extrinsic.", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9853613972663879}, {"text": "Extrinsic", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.988814651966095}]}, {"text": "Intrinsic evaluation is for evaluating the NLP pipeline of our system using the BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9944965243339539}]}, {"text": "Extrinsic evaluation is an end-to-end qualitative evaluation of our text-to-animation generation system, done via a user study.", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7487910985946655}]}, {"text": "To evaluate the performance of our proposed NLP pipeline, 500 Descriptions components from the test set were randomly selected.", "labels": [], "entities": []}, {"text": "Three annotators manually translated these 500 Descriptions components into simplified sentences and extracted all the necessary ARFs from the simplified sentences.", "labels": [], "entities": []}, {"text": "This is a time intensive process and took around two months.", "labels": [], "entities": []}, {"text": "30 % of the Descriptions blocks contain verbs not in the list of 92 animation verbs.", "labels": [], "entities": []}, {"text": "There are approximately 1000 sentences in the test set, with average length of 12 words.", "labels": [], "entities": []}, {"text": "Each Descriptions component is also annotated by the three annotators for the ARFs.", "labels": [], "entities": [{"text": "ARFs", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9116924405097961}]}, {"text": "Taking inspiration from the text simplification community (, we use the BLEU score () for evaluating our simplification and information extraction modules.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9693692326545715}, {"text": "information extraction", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.773309201002121}]}, {"text": "For each simplified sentence s i we have 3 corresponding references r 1 i , r 2 i and r 3 i . We also evaluate using the SARI ( score to evaluate our text simplification module.", "labels": [], "entities": [{"text": "SARI ( score", "start_pos": 121, "end_pos": 133, "type": "METRIC", "confidence": 0.9443330764770508}]}, {"text": "We also evaluate the system's output for action representation fields against gold annotations.", "labels": [], "entities": []}, {"text": "In our case, some of the fields can have multiple (2 or 3) words such as owner, target, prop, action, origin action, manner, location and direction.", "labels": [], "entities": []}, {"text": "We use BLEU 1 as the evaluation metric to measure the BOW similarity between system output and ground truth references.", "labels": [], "entities": [{"text": "BLEU 1", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9813179969787598}, {"text": "BOW similarity", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.9692502021789551}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "In identifying owner, target and prop, the system tends to use a fixed long mention, while annotators prefer short mentions for the same character/object.", "labels": [], "entities": []}, {"text": "The score of prop is relatively lower than all other fields, which is caused by a systematic SRL mapping error.", "labels": [], "entities": [{"text": "prop", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9682992696762085}, {"text": "SRL mapping", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.6693238615989685}]}, {"text": "The relatively high accuracy on the action field indicates the consistency between system output and annotator answers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9992899894714355}, {"text": "consistency", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9814567565917969}]}, {"text": "Annotation on the emotion ARF is rather subjective.", "labels": [], "entities": [{"text": "Annotation on the emotion ARF", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5254739165306092}]}, {"text": "Responses on the this field are biased and noisy.", "labels": [], "entities": []}, {"text": "The BLEU 1 score on this is relatively low.", "labels": [], "entities": [{"text": "BLEU 1 score", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9827343622843424}]}, {"text": "For the other non-textual ARFs, we use precision and recall to measure the system's behavior.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9994879961013794}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9988759160041809}]}, {"text": "These fields are subjective: annotators tend to give different responses for the same input sentence.", "labels": [], "entities": []}, {"text": "rotation and translation have Boolean values.", "labels": [], "entities": [{"text": "rotation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9326634407043457}]}, {"text": "Annotators agree on these two fields inmost of the sentences.", "labels": [], "entities": []}, {"text": "The system, on the other hand, fails to identify actions involving rotation.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"Carl closes CARL 's door sharply\" all four annotators think that this sentence involves rotation, which is not found by the system.", "labels": [], "entities": [{"text": "Carl closes CARL 's door", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.5518091643850008}]}, {"text": "This is due to the specificity of rules on identifying these two fields.", "labels": [], "entities": []}, {"text": "speed, duration and start time have high precision and low recall.", "labels": [], "entities": [{"text": "speed", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9844016432762146}, {"text": "duration", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9988252520561218}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9994291663169861}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9990907907485962}]}, {"text": "This indicates the inconsistency in annotators' answers.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"Woody runs around to the back of the pizza truck\", two annotators give 2 seconds and another gives 1 second in duration.", "labels": [], "entities": [{"text": "duration", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9603067636489868}]}, {"text": "These fields are subjective and need the opinion of the script author or the director.", "labels": [], "entities": []}, {"text": "In the future, we plan to involve script editors in the evaluation process.", "labels": [], "entities": []}, {"text": "We conducted a user study to evaluate the performance of the system qualitatively.", "labels": [], "entities": []}, {"text": "The focus of the study was to evaluate (from the end user's perspective) the performance of the NLP component w.r.t. generating reasonable animations.", "labels": [], "entities": []}, {"text": "We developed a questionnaire consisting of 20 sentence-animation video pairs.", "labels": [], "entities": []}, {"text": "The animations were generated by our system.", "labels": [], "entities": []}, {"text": "The questionnaire was filled by 22 participants.", "labels": [], "entities": []}, {"text": "On an average it took around 25 minutes fora user to complete the study.", "labels": [], "entities": []}, {"text": "We asked users to evaluate, on a five-point Likert scale  the text information was depicted in the video and how much of the information in the video was present in the text (.", "labels": [], "entities": []}, {"text": "The 68.18 % of the participants rated the overall pre-visualization as neutral or above.", "labels": [], "entities": []}, {"text": "The rating was 64.32 % (neutral or above) for the conservation of textual information in the video, which is reasonable, given limitations of the system that are not related to the NLP component.", "labels": [], "entities": []}, {"text": "For the last question, 75.90 % (neutral or above) agreed that the video did not have extra information.", "labels": [], "entities": []}, {"text": "In general, there seemed to be reasonable consensus in the responses.", "labels": [], "entities": []}, {"text": "Besides the limitations of our system, disagreement can be attributed to the ambiguity and subjectivity of the task.", "labels": [], "entities": []}, {"text": "We also asked the participants to describe qualitatively what textual information, if any, was missing from the videos.", "labels": [], "entities": []}, {"text": "Most of the missing information was due to limitations of the overall system rather than the NLP component: facial expression information was not depicted because the character 3-D models are deliberately designed without faces, so that animators can draw on them.", "labels": [], "entities": []}, {"text": "Information was also missing in the videos if it referred to objects or actions that do not have a close enough match in the object list or animations list.", "labels": [], "entities": []}, {"text": "Furthermore, the animation component only supports animations referring to a character or object as a whole, not parts, (e.g. \"Ben raises his head\" is not supported).", "labels": [], "entities": []}, {"text": "However, there were some cases where the NLP component can be improved.", "labels": [], "entities": []}, {"text": "For example, lexical simplification failed to map the verb \"watches\" to the similar animation \"look\".", "labels": [], "entities": []}, {"text": "In one case, syntactic simplification created only two simplified sentences fora verb which had three subjects in the original sentence.", "labels": [], "entities": []}, {"text": "Ina few cases, lexical simplification successfully mapped to the most similar animation (e.g.\"argue\" to \"talk\") but the participants were not satisfied -they were expecting a more exact animation.", "labels": [], "entities": []}, {"text": "We plan to address these shortcomings in future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Differences between system output and annotator responses", "labels": [], "entities": [{"text": "Differences", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9484537839889526}]}, {"text": " Table 5: Results on textual ARFs ( %)", "labels": [], "entities": [{"text": "textual ARFs", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.4568505883216858}]}]}