{"title": [{"text": "Hyperpartisan News Detection with Generic Semi-supervised Features", "labels": [], "entities": [{"text": "Hyperpartisan News Detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6831061840057373}]}], "abstractContent": [{"text": "In this paper we describe our participation to the Hyperpartisan News Detection shared task at SemEval 2019.", "labels": [], "entities": [{"text": "Hyperpartisan News Detection shared task at SemEval 2019", "start_pos": 51, "end_pos": 107, "type": "TASK", "confidence": 0.7775472849607468}]}, {"text": "Motivated by the late arrival of Doris Martin, we test a previously developed document classification system which consists of a combination of clustering features implemented on top of some simple shallow local features.", "labels": [], "entities": [{"text": "document classification", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.6694268882274628}]}, {"text": "We show how leverag-ing distributional features obtained from large in-domain unlabeled data helps to easily and quickly develop a reasonably good performing system for detecting hyperpartisan news.", "labels": [], "entities": [{"text": "detecting hyperpartisan news", "start_pos": 169, "end_pos": 197, "type": "TASK", "confidence": 0.8626806934674581}]}, {"text": "The system and models generated for this task are publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "The definition of hyperpartisan according to the Hyperpartisan News Detection shared task at SemEval 2019 () is the following: \"Given a news article text, decide whether it follows a hyperpartisan argumentation, i.e., whether it exhibits blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person\".", "labels": [], "entities": [{"text": "Hyperpartisan News Detection shared task at SemEval 2019", "start_pos": 49, "end_pos": 105, "type": "TASK", "confidence": 0.6367844343185425}]}, {"text": "Putting it simply, the task is, given a news article, to decide whether such document is hyperpartisan (true) or not (false).", "labels": [], "entities": []}, {"text": "This task is related to the Stance Detection ( and automatic detection of fake news) tasks, which are getting increasing attention within the Natural Language Processing community (.", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.9604619145393372}, {"text": "automatic detection of fake news) tasks", "start_pos": 51, "end_pos": 90, "type": "TASK", "confidence": 0.8365875950881413}]}, {"text": "In this sense, it could be the case that hyperpartisanism is conveyed by some elements of fake news within the article, usually with the objective of spreading propaganda and manipulate readers towards a particular stance on a specific topic.", "labels": [], "entities": []}, {"text": "The SemEval 2019 task 4 aims to address the problem of hyperpartisan news detection at docu-1 https://pan.webis.de/semeval19/ semeval19-web/index.html ment level, without trying to distinguish specific elements or indicators of hyperpartisanism in each article.", "labels": [], "entities": [{"text": "SemEval 2019 task 4", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8650598078966141}, {"text": "hyperpartisan news detection", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.6692430973052979}]}, {"text": "Two sets of data were released to participants.", "labels": [], "entities": []}, {"text": "The first part (bypublisher) is annotated at publisher level.", "labels": [], "entities": []}, {"text": "This means that if a publisher is thought to be spreading hyperpartisan news, then all its articles are annotated as hyperpartisan.", "labels": [], "entities": []}, {"text": "The bypublisher set contains 750K articles divided in 600K documents for training and a validation set of 150K documents.", "labels": [], "entities": []}, {"text": "The second part (byarticle) has been annotated at article level via crowdsourcing and consists of 645 articles for training and 628 documents for the test.", "labels": [], "entities": []}, {"text": "The test set is hidden in TIRA (  and it is used for the official evaluation scores of the task.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.8133058547973633}]}, {"text": "It should be noted that, unlike the byarticle test set, the byarticle training set was not balanced (407 false vs 238 true).", "labels": [], "entities": [{"text": "byarticle test set", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.720383902390798}]}, {"text": "We address this task using an existing document classification system, mostly due to the fact that we joined the task just a week before the final submission deadline.", "labels": [], "entities": [{"text": "document classification", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.697911411523819}]}, {"text": "However, and despite the lack of time to implement specific features for the task, we obtained quite good results with a simple and very general feature set in which the most meaningful feature was the use of pre-trained clusters obtained from the English Wikipedia and the Gigaword 5th edition.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 248, "end_pos": 265, "type": "DATASET", "confidence": 0.9014780223369598}, {"text": "Gigaword 5th edition", "start_pos": 274, "end_pos": 294, "type": "DATASET", "confidence": 0.8819982608159384}]}, {"text": "Out of 42 participants, our official submission obtained 0.737 accuracy whereas the winner of the task scored 0.822.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9970397353172302}]}, {"text": "In addition to our official participation, in this paper we also describe a second round of experiments performed after the official submission deadline.", "labels": [], "entities": []}, {"text": "The objective was to establish whether using clusters trained on domain-specific data would improve the results with respect to those obtained by using clusters based on general domain text such as Wikipedia and Gigaword.", "labels": [], "entities": []}, {"text": "As it turned out, this second round of experiments allowed us to considerably improve the results (0.761) with respect to our official scores in the task (0.737), confirming that training clusters on domain-specific data, although smaller, helps to address the hyperpartisan news detection task.", "labels": [], "entities": [{"text": "news detection task", "start_pos": 275, "end_pos": 294, "type": "TASK", "confidence": 0.7448243300120035}]}], "datasetContent": [{"text": "We train ixa-pipe-doc with the default parameters, performing 100 iterations with a 5 count cutoff.", "labels": [], "entities": []}, {"text": "4  We only tested three types of local features which were already implemented in the system: the current token, the character ngrams of each token (2:6 range) and word prefixes (0-4 characters of each token).", "labels": [], "entities": []}, {"text": "Due to our late arrival to the task, we combined the best local features with our pre-trained clusters from Wikipedia and Gigaword for the official results described in section 4.1.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.868506908416748}]}, {"text": "For the second round of experiments of section 4.2, we used the clusters trained using the bypublisher and Fake News datasets.", "labels": [], "entities": [{"text": "Fake News datasets", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.9091851909955343}]}, {"text": "The number of clusters trained with each algorithm and data source was the following: 100-800 clusters using the Clark and Word2vec methods, and 1000 classes with the Brown algorithm.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.7277395725250244}]}, {"text": "The best combination of features were obtained by performing every possible permutation between them in a 5-fold cross validation setting using the byarticle training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: 5-fold cross validation for official results on the byarticle training set. CW600: Clark Wikipedia 600  clusters; W2VG200: Word2vec Gigaword 200 clusters.", "labels": [], "entities": [{"text": "byarticle training set", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.6185318430264791}, {"text": "Clark Wikipedia 600  clusters", "start_pos": 93, "end_pos": 122, "type": "DATASET", "confidence": 0.9553680866956711}, {"text": "W2VG200", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.8898518681526184}, {"text": "Word2vec Gigaword 200 clusters", "start_pos": 133, "end_pos": 163, "type": "DATASET", "confidence": 0.8549869954586029}]}, {"text": " Table 2: Official results on TIRA test set. CW600: Clark Wikipedia 600 clusters; W2VG200: Word2vec Gigaword  200 clusters.", "labels": [], "entities": [{"text": "TIRA test set", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8143351276715597}, {"text": "Clark Wikipedia 600 clusters", "start_pos": 52, "end_pos": 80, "type": "DATASET", "confidence": 0.9608801305294037}, {"text": "Word2vec Gigaword  200 clusters", "start_pos": 91, "end_pos": 122, "type": "DATASET", "confidence": 0.8608592003583908}]}, {"text": " Table 3: 5-fold cross validation for the second round of results on the byarticle training set. W2VHP300: Word2vec  Hyperpartisan bypublisher 300 clusters; W2VFN400: Word2vec Fake News 400 clusters.", "labels": [], "entities": [{"text": "byarticle training set", "start_pos": 73, "end_pos": 95, "type": "DATASET", "confidence": 0.6187391479810079}, {"text": "Word2vec  Hyperpartisan bypublisher 300 clusters", "start_pos": 107, "end_pos": 155, "type": "DATASET", "confidence": 0.8152376174926758}, {"text": "Word2vec Fake News 400 clusters", "start_pos": 167, "end_pos": 198, "type": "DATASET", "confidence": 0.8744534969329834}]}, {"text": " Table 4: Second round results. W2VHP300: Word2vec Hyperpartisan bypublisher 300 clusters; W2VFN400:  Word2vec Fake News 400 clusters.", "labels": [], "entities": [{"text": "W2VHP300", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9198324680328369}, {"text": "Word2vec Hyperpartisan bypublisher 300 clusters", "start_pos": 42, "end_pos": 89, "type": "DATASET", "confidence": 0.852947223186493}, {"text": "Word2vec Fake News 400 clusters", "start_pos": 102, "end_pos": 133, "type": "DATASET", "confidence": 0.8621354579925538}]}]}