{"title": [{"text": "Are abusive language classification results reproducible?", "labels": [], "entities": [{"text": "abusive language classification", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6536804437637329}]}], "abstractContent": [{"text": "This paper summarizes the participation of Stop PropagHate team at SemEval 2019.", "labels": [], "entities": [{"text": "Stop PropagHate team at SemEval 2019", "start_pos": 43, "end_pos": 79, "type": "DATASET", "confidence": 0.7304190297921499}]}, {"text": "Our approach is based on replicating one of the most relevant works on the literature, using word embeddings and LSTM.", "labels": [], "entities": []}, {"text": "After circumventing some of the problems of the original code, we found poor results when applying it to the HatEval contest (F1=0.45).", "labels": [], "entities": [{"text": "F1", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9971731901168823}]}, {"text": "We think this is due mainly to inconsistencies in the data of this contest.", "labels": [], "entities": []}, {"text": "Finally, for the OffensEval the classifier performed well (F1=0.74), proving to have a better performance for offense detection than for hate speech.", "labels": [], "entities": [{"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9987673759460449}, {"text": "offense detection", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.8487429618835449}]}], "introductionContent": [{"text": "In the last few years, several evaluation tasks in the context of hate speech detection and categorization have been created.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.8001831571261088}]}, {"text": "Some of these tasks include e.g., EVALITA ( and TRAC-1 (.", "labels": [], "entities": [{"text": "EVALITA", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.4382432997226715}, {"text": "TRAC-1", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.6251426339149475}]}, {"text": "These type of initiatives promote the development of different but comparable solutions for the same problem, within a short period of time, which is an interesting contribution fora research field.", "labels": [], "entities": []}, {"text": "In this paper, we describe the participation of team \"Stop PropagHate\" in the HatEval and OffensEval tasks of SemEval 2019.", "labels": [], "entities": []}, {"text": "The main goal of both tasks is to improve the classification of Hate Speech and Offensive Language.", "labels": [], "entities": [{"text": "classification", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.9595475196838379}]}, {"text": "Some of the works in the literature achieve a very competitive performance, e.g. obtain an F1 score of 0.93 when using deep learning for classifying hate speech in one of the most commonly used baseline datasets (e.g.).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9871639311313629}, {"text": "classifying hate speech", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.8319317698478699}]}, {"text": "In this context, we have a specific objective with our approach: we aim to reproduce a state-of-the-art classifier as described in the literature of this topic.", "labels": [], "entities": []}, {"text": "We choose to reproduce the study by, not only because of the good performance of the developed models, but also because in this work the authors published their code.", "labels": [], "entities": []}, {"text": "Considering the amount of parameters available for definition and tuning in a machine learning classification pipeline, a precise and extensive definition of an experiment's parameters is not simple and is hardly ever provided.", "labels": [], "entities": [{"text": "machine learning classification pipeline", "start_pos": 78, "end_pos": 118, "type": "TASK", "confidence": 0.6907305121421814}]}, {"text": "Thus, having the code of the experiment is the best way to understand not only which steps were conducted, but also how those steps were indeed executed.", "labels": [], "entities": []}, {"text": "This is a highly cited paper, which can be regarded as an indicator of its relevance in the area.", "labels": [], "entities": []}, {"text": "In this paper, we describe our journey in the process of replication and the results achieved when applying this classifier in both shared tasks.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: Section 2 briefly reviews the literature, Section 3 presents our methodology, Section 4 describes the tasks and preliminary experiences with the data, Section 5 shows our official results in the shared tasks, and we report the conclusions of our work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our methodology, we use a standard dataset, so that we could compare our results with the original paper we are replicating.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 score over the different iterations of the  cross-validation procedure. Experiment conducted for  replicating the paper from Badjatiya et al. (2017).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.977395236492157}, {"text": "replicating", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.9595961570739746}]}, {"text": " Table 3: Achieved F1 score during cross validation (CV) and testing for the baseline experiments.", "labels": [], "entities": [{"text": "Achieved", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.995050847530365}, {"text": "F1 score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9376564621925354}]}, {"text": " Table 4: Achieved F1 score during cross validation (CV) and testing for the HatEval experiments.", "labels": [], "entities": [{"text": "Achieved", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.994853675365448}, {"text": "F1 score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9221828579902649}, {"text": "HatEval experiments", "start_pos": 77, "end_pos": 96, "type": "DATASET", "confidence": 0.9274699687957764}]}, {"text": " Table 5: Achieved F1 score during cross validation (CV) and testing for the OffensEval experiments.", "labels": [], "entities": [{"text": "Achieved", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9959715008735657}, {"text": "F1 score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.931304931640625}, {"text": "OffensEval experiments", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.8562905490398407}]}, {"text": " Table 6: Achieved performance in the shared tasks.  Model 1 corresponds to the original classifier from the  replicated paper (LSTM 50d + xgBoost). Model 2 cor- responds to the same as Model 1 plus adding hatebase  features, and finally, Model 3 corresponds to the same  as Model 1 plus adding hatebase and sentiment fea- tures.", "labels": [], "entities": []}]}