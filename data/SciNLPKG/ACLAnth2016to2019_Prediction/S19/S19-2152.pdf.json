{"title": [{"text": "my at SemEval-2019 Task 9: Exploring BERT for Suggestion Mining", "labels": [], "entities": [{"text": "BERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.984616219997406}, {"text": "Suggestion Mining", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.9837658703327179}]}], "abstractContent": [{"text": "This paper presents our system to the SemEval-2019 Task 9, Suggestion Mining from Online Reviews and Forums.", "labels": [], "entities": [{"text": "SemEval-2019 Task 9", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8095306754112244}, {"text": "Suggestion Mining from Online Reviews", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.8806632876396179}]}, {"text": "The goal of this task is to extract suggestions such as the expressions of tips, advice, and recommendations.", "labels": [], "entities": []}, {"text": "We explore Bidirectional Encoder Representations from Transformers (BERT) focus-ing on target domain pre-training in Subtask A which provides training and test datasets in the same domain.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformers (BERT", "start_pos": 11, "end_pos": 72, "type": "TASK", "confidence": 0.6692375200135368}]}, {"text": "In Subtask B, the cross domain suggestion mining task, we apply the idea of distant supervision.", "labels": [], "entities": [{"text": "cross domain suggestion mining task", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.6932034254074096}]}, {"text": "Our system obtained the third place in Subtask A and the fifth place in Subtask B, which demonstrates its efficacy of our approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "In SemEval2019 Task 9, participants are required to build a model which can classify given sentences into suggestion and non-suggestion classes.", "labels": [], "entities": [{"text": "SemEval2019 Task 9", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8613575100898743}]}, {"text": "We participate in two sub-tasks: domain specific suggestion mining task (Subtask A 1 ) and cross domain suggestion mining task (Subtask B 2 ).", "labels": [], "entities": [{"text": "domain specific suggestion mining task", "start_pos": 33, "end_pos": 71, "type": "TASK", "confidence": 0.6666673362255097}, {"text": "cross domain suggestion mining task", "start_pos": 91, "end_pos": 126, "type": "TASK", "confidence": 0.6719864547252655}]}, {"text": "In Subtask A, the test dataset belongs to the same domain as the training and development datasets.", "labels": [], "entities": []}, {"text": "These datasets are extracted from the suggestion forum for windows platform.", "labels": [], "entities": []}, {"text": "In Subtask B, training and test datasets belong to separate domains.", "labels": [], "entities": []}, {"text": "More specifically, the domain of the training dataset is entries from the windows forum and that of the test dataset is hotel reviews.", "labels": [], "entities": []}, {"text": "Example sentences used in these tasks are listed in.", "labels": [], "entities": []}, {"text": "For a description of these tasks please refer to (.", "labels": [], "entities": []}, {"text": "Subtask A can be viewed as a binary text classification task.", "labels": [], "entities": [{"text": "binary text classification task", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.7576420307159424}]}, {"text": "Recently, pre-training models, such as OpenAI GPI (  Subtask A (windows platform) P: xbox dev mode companion work ipv6 please...", "labels": [], "entities": [{"text": "OpenAI GPI", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.8862318396568298}]}, {"text": "N: I do not want to convert MSIs.", "labels": [], "entities": []}, {"text": "Subtask B (hotel review) P: If you can, upgrade to an Ocean Front Room.", "labels": [], "entities": []}, {"text": "N: it doesn't have a very clean look.: Example sentences in the test dataset.", "labels": [], "entities": []}, {"text": "P means a positive sentence, i.e. suggestion sentence and N denotes a negative sentence, i.e. non-suggestion sentence.", "labels": [], "entities": []}, {"text": "(, have gained much attention with their ability to improve a number of downstream tasks.", "labels": [], "entities": []}, {"text": "These models are pre-trained using unlabeled corpora and then fine-tuned on labeled datasets.", "labels": [], "entities": []}, {"text": "We apply BERT to this task because it has achieved state-of-the-art performance in several text classification tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9940741658210754}, {"text": "text classification tasks", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.8337254524230957}]}, {"text": "The difference to the original BERT model is that we further pre-train BERT model using an unlabeled corpus related to this domain.", "labels": [], "entities": []}, {"text": "More concretely, we extract documents from a windows forum and run additional steps of pre-training using these documents, starting from the pre-trained BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9146435856819153}]}, {"text": "In Subtask B, we apply the idea of distant supervision which has been firstly proposed by.", "labels": [], "entities": [{"text": "distant supervision", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.6679737120866776}]}, {"text": "Distant supervision is a weakly supervised learning framework which tries to automatically generate noisy training examples.", "labels": [], "entities": []}, {"text": "Specifically, we use the rule based system which is provided by the task organizer for creating a noisy training dataset and train the model based on them.", "labels": [], "entities": []}, {"text": "Our system significantly outperforms baseline methods on two subtasks.", "labels": [], "entities": []}, {"text": "These results demonstrate its efficacy of our approaches, target domain pre-training in Subtask A and distant supervision in Subtask B.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics of datasets.", "labels": [], "entities": []}, {"text": " Table 4: Results of Subtask B. BERT BASE (win- dows) is the model trained on windows corpus pro- vided in Subtask A.", "labels": [], "entities": [{"text": "BERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9994387030601501}, {"text": "BASE", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.8841065168380737}]}]}