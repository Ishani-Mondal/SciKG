{"title": [{"text": "Harvey Mudd College at SemEval-2019 Task 4: The Clint Buchanan Hyperpartisan News Detector", "labels": [], "entities": [{"text": "SemEval-2019 Task", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8730111122131348}, {"text": "Hyperpartisan News Detector", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.7673433820406595}]}], "abstractContent": [{"text": "We investigate the recently developed Bidirec-tional Encoder Representations from Transformers (BERT) model (Devlin et al., 2018) for the hyperpartisan news detection task.", "labels": [], "entities": [{"text": "Bidirec-tional Encoder Representations from Transformers (BERT)", "start_pos": 38, "end_pos": 101, "type": "TASK", "confidence": 0.6415558904409409}, {"text": "hyperpartisan news detection task", "start_pos": 138, "end_pos": 171, "type": "TASK", "confidence": 0.7694742679595947}]}, {"text": "Using a subset of hand-labeled articles from Se-mEval as a validation set, we test the performance of different parameters for BERT models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.5615905523300171}]}, {"text": "We find that accuracy from two different BERT models using different proportions of the articles is consistently high, with our best-performing model on the validation set achieving 85% accuracy and the best-performing model on the test set achieving 77%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9990597367286682}, {"text": "BERT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.6820851564407349}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9988930821418762}]}, {"text": "We further determined that our model exhibits strong consistency, labeling independent slices of the same article identically.", "labels": [], "entities": [{"text": "consistency", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9809674620628357}]}, {"text": "Finally, we find that randomizing the order of word pieces dramatically reduces validation accuracy (to approximately 60%), but that shuffling groups of four or more word pieces maintains an accuracy of about 80%, indicating the model mainly gains value from local context.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9477713704109192}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.998376727104187}]}], "introductionContent": [{"text": "SemEval Task 4 () tasked participating teams with identifying news articles that are misleading to their readers, a phenomenon often associated with \"fake news\" distributed by partisan sources.", "labels": [], "entities": [{"text": "SemEval Task 4", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8212627371152242}]}, {"text": "We approach the problem through transfer learning to fine-tune a model for the document classification task.", "labels": [], "entities": [{"text": "document classification task", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.815041164557139}]}, {"text": "We use the BERT model based on the implementation of the github repository pytorch-pretrained-bert 1 on some of the data provided by Task 4 of SemEval.", "labels": [], "entities": [{"text": "BERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9944304823875427}]}, {"text": "BERT has been used to learn useful representations fora variety of natural language tasks, achieving state of the art performance in these tasks after being fine-tuned.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9705745577812195}]}, {"text": "It is a language representation model that is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.", "labels": [], "entities": []}, {"text": "Thus, it maybe able to adequately account for complex characteristics as such blind, prejudiced reasoning and extreme bias that are important to reliably identifying hyperpartisanship in articles.", "labels": [], "entities": []}, {"text": "We show that BERT performs well on hyperpartisan sentiment classification.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9799758791923523}, {"text": "hyperpartisan sentiment classification", "start_pos": 35, "end_pos": 73, "type": "TASK", "confidence": 0.6664301951726278}]}, {"text": "We use unsupervised learning on the set of 600,000 source-labeled articles provided as part of the task, then train using supervised learning for the 645 hand-labeled articles.", "labels": [], "entities": []}, {"text": "We believe that learning on sourcelabeled articles would bias our model to learn the partisanship of a source, instead of the article.", "labels": [], "entities": []}, {"text": "Additionally, the accuracy of the model on validation data labeled by article differs heavily when the articles are labeled by publisher.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995021820068359}]}, {"text": "Thus, we decided to use a small subset of the hand-labeled articles as our validation set for all of our experiments.", "labels": [], "entities": []}, {"text": "As the articles are too large for the model to be trained on the full text each time, we consider the number of word-pieces that the model uses from each article a hyperparameter.", "labels": [], "entities": []}, {"text": "A second major issue we explore is what information the model is using to make decisions.", "labels": [], "entities": []}, {"text": "This is particularly important for BERT because neural models are often viewed like black boxes.", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.43556857109069824}]}, {"text": "This view is problematic fora task like hyperpartisan news detection where users may reasonably want explanations as to why an article was flagged.", "labels": [], "entities": [{"text": "hyperpartisan news detection", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6740175485610962}]}, {"text": "We specifically explore how much of the article is needed by the model, how consistent the model behaves on an article, and whether the model focuses on individual words and phrases or if it uses more global understanding.", "labels": [], "entities": []}, {"text": "We find that the model only needs a short amount of context (100 word pieces), is very consistent throughout an article, and most of the model's accuracy arises from locally examining the article.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9989534616470337}]}, {"text": "In this paper, we demonstrate the effectiveness of BERT models for the hyperpartisan news classification task, with validation accuracy as high as 85% and test accuracy as high as 77% 2 . We also make significant investigations into the importance of different factors relating to the articles and training in BERT's success.", "labels": [], "entities": [{"text": "hyperpartisan news classification task", "start_pos": 71, "end_pos": 109, "type": "TASK", "confidence": 0.6629133448004723}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.6085786819458008}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.8921846151351929}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes previous work on the BERT model and semi-supervised learning.", "labels": [], "entities": [{"text": "BERT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.8593250513076782}]}, {"text": "Section 3 outlines our model, data, and experiments.", "labels": [], "entities": []}, {"text": "Our results are presented in Section 4, with their ramifications discussed in Section 5.", "labels": [], "entities": []}, {"text": "We close with an introduction to our system's namesake, fictional journalist Clint Buchanan, in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first investigate the impact of pre-training on BERT-BASE's performance.", "labels": [], "entities": [{"text": "BERT-BASE", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.803186297416687}]}, {"text": "We then compare the performance of BERT-BASE with BERT-LARGE.", "labels": [], "entities": [{"text": "BERT-BASE", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9926818013191223}, {"text": "BERT-LARGE", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9701629281044006}]}, {"text": "For both, we vary the number of word-pieces from each article that are used in training.", "labels": [], "entities": []}, {"text": "We perform tests with 100, 250 and 500 word pieces.", "labels": [], "entities": []}, {"text": "We also explore whether and how the BERT models we use classify different parts of each individual article.", "labels": [], "entities": [{"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.8804821372032166}]}, {"text": "Since the model can only consider a limited number of word pieces and not a full article, we test how the model judges different sections of the same article.", "labels": [], "entities": []}, {"text": "Here, we are interested in the extent to which the same class will be assigned to each segment of an article.", "labels": [], "entities": []}, {"text": "Finally, we test whether the model's behavior varies if we randomly shuffle word-pieces from the articles during training.", "labels": [], "entities": []}, {"text": "Our goal in this experiment is to understand whether the model focuses on individual words and phrases or if it achieves more global understanding.", "labels": [], "entities": []}, {"text": "We alter the the size of the chunks to be shuffled (N ) in each iteration of this experiment, from shuffling individual word-pieces (N = 1) to shuffling larger multiword chunks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Validation accuracy for BERT-base with and  without Unsupervised Pre-training (UP).", "labels": [], "entities": [{"text": "Validation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8241230249404907}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9431009888648987}, {"text": "BERT-base", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9813453555107117}, {"text": "Unsupervised Pre-training (UP", "start_pos": 62, "end_pos": 91, "type": "METRIC", "confidence": 0.7273382395505905}]}, {"text": " Table 2: Validation Accuracy on BERT-LARGE across sequence length and learning rate.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9004761576652527}, {"text": "BERT-LARGE", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.996229350566864}]}, {"text": " Table 3: BERT-LARGE across permute ngrams.", "labels": [], "entities": [{"text": "BERT-LARGE", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9983300566673279}]}]}