{"title": [{"text": "Fermi at SemEval-2019 Task 8: An elementary but effective approach to Question Discernment in Community QA Forums", "labels": [], "entities": [{"text": "SemEval-2019 Task 8", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.730377991994222}, {"text": "Question Discernment in Community QA Forums", "start_pos": 70, "end_pos": 113, "type": "TASK", "confidence": 0.8879123230775198}]}], "abstractContent": [{"text": "Online Community Question Answering Forums (cQA) have gained massive popularity within recent years.", "labels": [], "entities": [{"text": "Online Community Question Answering Forums (cQA)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7111034281551838}]}, {"text": "The rise in users for such forums have led to the increase in the need for automated evaluation for question comprehension and fact evaluation of the answers provided by various participants in the forum.", "labels": [], "entities": []}, {"text": "Our team, Fermi, participated in sub-task A of Task 8 at SemEval 2019-which tackles the first problem in the pipeline of factual evaluation in cQA forums, i.e., deciding whether a posed question asks fora factual information , an opinion/advice or is just socializing.", "labels": [], "entities": [{"text": "factual evaluation in cQA forums", "start_pos": 121, "end_pos": 153, "type": "TASK", "confidence": 0.632100522518158}]}, {"text": "This information is highly useful in segregating factual questions from non-factual ones which highly helps in organizing the questions into useful categories and trims down the problem space for the next task in the pipeline for fact evaluation among the available answers.", "labels": [], "entities": [{"text": "fact evaluation", "start_pos": 230, "end_pos": 245, "type": "TASK", "confidence": 0.6915839314460754}]}, {"text": "Our system uses the embeddings obtained from Universal Sentence Encoder combined with XGBoost for the classification sub-task A.", "labels": [], "entities": []}, {"text": "We also evaluate other combinations of embeddings and off-the-shelf machine learning algorithms to demonstrate the efficacy of the various representations and their combinations.", "labels": [], "entities": []}, {"text": "Our results across the evaluation test set gave an accuracy of 84% and received the first position in the final standings judged by the organizers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9997077584266663}]}], "introductionContent": [{"text": "The massive rise in popularity of Community Question Answering (cQA) forums like StackOverflow, Quora, Yahoo!", "labels": [], "entities": [{"text": "Community Question Answering (cQA) forums", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.7850329194750104}]}, {"text": "Answers and Google Groups have led to an effective means of information dissemination for topic-centered communities to share and engage in knowledge consumption needs.", "labels": [], "entities": []}, {"text": "After a considerable time, information becoming obsolete is a major problem which results in change of many of the facts that were previously true.", "labels": [], "entities": []}, {"text": "Another problem is that most of the forums lack exhaustive moderation and controlwhich results in high-latency quality checks and eventually results in the sharing of non-factual information.", "labels": [], "entities": []}, {"text": "Various factors are responsible for this -primarily being ignorance or misunderstanding and sometimes, maliciousness of the responder to the questions (.", "labels": [], "entities": []}, {"text": "In the pipeline of detection of whether the given responses to a question are indeed factual, the necessary first step is to discern what category the question asked in the cQA forum falls into.", "labels": [], "entities": []}, {"text": "As an example, \"What is Domino's customer service number?\" is a factual question as it asks fora fact rather than an opinion or discourse.", "labels": [], "entities": []}, {"text": "In contrast, consider the question \"Can someone recommend a good pediatrician in Mumbai?\" asks for an opinion rather than a particular factual information as opinions on the matter of a good pediatrician maybe subjective and depend on various other factors the conclusion of which is not universally true.", "labels": [], "entities": []}, {"text": "We tackle the problem proposed by organizers () in sub-task A as a multi-class classification problem, i.e., categorizing questions in cQA forums into one of the following three categories: 1.", "labels": [], "entities": []}, {"text": "Factual: The question is asking for factual information, which can be answered by checking various information sources, and it is not ambiguous.", "labels": [], "entities": []}, {"text": "(e.g., \"What is the currency used in Taiwan?\")", "labels": [], "entities": []}, {"text": "2. Opinion: The question asks for an opinion or an advice, not fora fact.", "labels": [], "entities": []}, {"text": "(e.g., \"Can somebody recommend good restaurants around the SF Bay Area?\")", "labels": [], "entities": []}, {"text": "3. Socializing: Not areal question, but intended for socializing or for chatting.", "labels": [], "entities": []}, {"text": "This can also mean expressing an opinion or sharing some information, without really asking anything of general interest.", "labels": [], "entities": []}, {"text": "(e.g., \"What was your first bike?\")", "labels": [], "entities": []}, {"text": "Our submission involves the use of pre-trained models for generating sentence embeddings from existing trained models and then employing the use of off-the-shelf machine learning algorithms for the multi-class prediction problem.", "labels": [], "entities": [{"text": "multi-class prediction problem", "start_pos": 198, "end_pos": 228, "type": "TASK", "confidence": 0.8517665465672811}]}, {"text": "The approach is described in Section 3 where we describe our methodology in detail.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Dev Set Accuracy and Macro-F-1 scores (in percentage) for Sub-Task A of Task 8", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9742616415023804}]}]}