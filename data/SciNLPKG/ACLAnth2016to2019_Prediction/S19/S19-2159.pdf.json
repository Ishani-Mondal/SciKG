{"title": [{"text": "Clark Kent at SemEval-2019 Task 4: Stylometric Insights into Hyperpartisan News Detection", "labels": [], "entities": [{"text": "SemEval-2019 Task", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.8073287308216095}, {"text": "Stylometric Insights into Hyperpartisan News Detection", "start_pos": 35, "end_pos": 89, "type": "TASK", "confidence": 0.7009250968694687}]}], "abstractContent": [{"text": "In this paper, we present a news bias prediction system, which we developed as part of a SemEval 2019 task.", "labels": [], "entities": [{"text": "news bias prediction", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7819657325744629}, {"text": "SemEval 2019 task", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7469213604927063}]}, {"text": "We developed an XG-Boost based system which uses character and word level n-gram features represented using TF-IDF, count vector based correlation matrix, and predicts if an input news article is a hyper-partisan news article.", "labels": [], "entities": []}, {"text": "Our model was able to achieve a precision of 68.3% on the test set provided by the contest organizers.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9995763897895813}]}, {"text": "We also run our model on the Buz-zFeed corpus and find XGBoost with simple character level N-Gram embeddings to be performing well with an accuracy of around 96%.", "labels": [], "entities": [{"text": "Buz-zFeed corpus", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9687404930591583}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.999446451663971}]}], "introductionContent": [{"text": "The problem of hyperpartisan news detection) is based on predicting whether a news article is biased towards a specific political wing or not.", "labels": [], "entities": [{"text": "hyperpartisan news detection", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.7350020805994669}]}, {"text": "The problem falls under the category of classification problems, and the task is to classify an article as being extremely one-sided or not.", "labels": [], "entities": []}, {"text": "A closely related problem is that of fake new detection wherein the task is to analyze the veracity of an article, and classify it based on some predefined degrees of truthfulness.", "labels": [], "entities": [{"text": "fake new detection", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7524024248123169}]}, {"text": "Our problem has a high societal relevance, since one-sided news poses a great threat to democracy, particularly in the context of conducting fair elections.", "labels": [], "entities": []}, {"text": "In this paper, we discuss our approach to solving this problem used during the contest Hyper Partisan News Detection, a competition task at SemEval 2019 ().", "labels": [], "entities": [{"text": "Hyper Partisan News Detection", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.5820001438260078}]}, {"text": "More formally, our problem definition is: Definition 1 (Hyperpartisan News Detection) We are given a set of news articles A, where each article a i is marked with two labels: a Boolean label hyperpartisan hi which indicates if article a i is biased towards apolitical wing, and a bias label bi \u2208 {left, right, left-center, right-center, least} which indicates which wing the article is biased towards.", "labels": [], "entities": [{"text": "Hyperpartisan News Detection", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6286933422088623}]}, {"text": "If hi = True, then bi \u2208 {left, right}; if hi = False, then bi \u2208 {least, left-center, right-center}.", "labels": [], "entities": [{"text": "False", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9950399994850159}]}, {"text": "The objective is to learn a classifier C which predicts the hyperpartisan label h j for an unknown news article a j . In this work, we identify the role of various traditional NLP features in determining the degree of partisanship.", "labels": [], "entities": []}, {"text": "We utilise standard term-frequency and inverse document frequency vector features computed for uni, bi and tri-grams obtained from the corpus.", "labels": [], "entities": []}, {"text": "We do this feature extraction at both character and word level and then train a gradient boosted decision tree as a classifier for identifying partisanship.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7798316776752472}, {"text": "identifying partisanship", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.7811442911624908}]}, {"text": "We also compare other methods of classification such as SVM, KNN, Naive Bayes and Logistic Regression for the task using the same vector features.", "labels": [], "entities": []}, {"text": "Furthermore, experiments exploiting the metadata information were also performed (explained in detail in the scalar features in section 3.2).", "labels": [], "entities": []}, {"text": "The experiments were performed on two corpora, the BuzzFeed corpus (created in) and the training corpus released by the task organisers (the SemEval corpus).", "labels": [], "entities": [{"text": "BuzzFeed corpus", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9510297179222107}, {"text": "SemEval corpus", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.6910269558429718}]}, {"text": "Further we also discuss the results obtained on the final test corpus released for the final evaluation of the task in section 4.1.", "labels": [], "entities": []}, {"text": "Due to computation infeasibility over the larger training corpus, we do not compute vector features for the SemEval corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "We divide this section into three parts -experimental setup, results on the BuzzFeed corpus, and results on the SemEval corpus.", "labels": [], "entities": [{"text": "BuzzFeed corpus", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.9383779764175415}, {"text": "SemEval corpus", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.7355202883481979}]}, {"text": "The article polarity and title polarity features were computed using SentiWordNet 1 (.", "labels": [], "entities": []}, {"text": "All the vector features were computed using the scikit-learn package.", "labels": [], "entities": []}, {"text": "To split the data into training and testing sets, we used 5-fold cross-validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Vector feature results (BuzzFeed Corpus  only).", "labels": [], "entities": []}, {"text": " Table 2: Scalar features results.", "labels": [], "entities": []}, {"text": " Table 3: Results for the submitted model.", "labels": [], "entities": []}]}