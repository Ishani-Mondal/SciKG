{"title": [{"text": "LaSTUS/TALN at SemEval-2019 Task 6: Identification and Categorization of Offensive Language in Social Media with Attention-based Bi-LSTM model", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a bidirectional Long-Short Term Memory network for identifying offensive language in Twitter.", "labels": [], "entities": []}, {"text": "Our system has been developed in the context of the Se-mEval 2019 Task 6 which comprises three different sub-tasks, namely A: Offensive Language Detection, B: Categorization of Offensive Language, C: Offensive Language Target Identification.", "labels": [], "entities": [{"text": "Se-mEval 2019 Task 6", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.7218788638710976}, {"text": "Offensive Language Detection", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.6453325748443604}, {"text": "Offensive Language Target Identification", "start_pos": 200, "end_pos": 240, "type": "TASK", "confidence": 0.5852816179394722}]}, {"text": "We used a pre-trained Word Embeddings in tweet data, including information about emojis and hashtags.", "labels": [], "entities": []}, {"text": "Our approach achieves good performance in the three sub-tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the amount of user generated content in social media is increasing at an exponential pace, detecting offensive language and harmful content automatically in an efficient way is a very important issue for the society.", "labels": [], "entities": []}, {"text": "Recent work has shown that offensive language in various forms such as hate speech, cyberbullying, profanity and harassment has negative effects especially in adolescents.", "labels": [], "entities": []}, {"text": "The shared task, Categorizing Offensive Language in Social Media (SemEval 2019 -Task 6), focuses on improving identification of offensive language by considering type and target of the offense into account ().", "labels": [], "entities": [{"text": "identification of offensive language", "start_pos": 110, "end_pos": 146, "type": "TASK", "confidence": 0.8139844536781311}]}, {"text": "The task is composed of three sub-tasks.", "labels": [], "entities": []}, {"text": "Sub-task A aims to identify if a given tweet is offensive or not (annotated as OFF or NOT).", "labels": [], "entities": [{"text": "OFF", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9411908388137817}, {"text": "NOT", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.5482157468795776}]}, {"text": "Sub-task B aims to categorize the offense type in offensive tweets into two categories: targeted (TIN) or untargeted (UNT) meaning that if a tweet contains an insult or threat to an individual, a group or something else or if a tweet contains non-targeted offense such as general profanity or non-acceptable language.", "labels": [], "entities": [{"text": "untargeted (UNT)", "start_pos": 106, "end_pos": 122, "type": "METRIC", "confidence": 0.6353989169001579}]}, {"text": "Lastly, Sub-task C aims to identify the target type of targeted offensive posts.", "labels": [], "entities": []}, {"text": "The target type is supposed to be classified as individual, group or other for the rest (annotated as IND, GRP or OTH).", "labels": [], "entities": [{"text": "IND", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.7397447824478149}, {"text": "GRP", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.667810320854187}, {"text": "OTH", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.569085955619812}]}, {"text": "We submitted three different runs for each sub-task.", "labels": [], "entities": []}, {"text": "The training dataset released by the shared task organizers, consists of 14,100 English tweets with one annotation layer per task with a hierarchical annotation scheme where each annotation level is related to an independent sub-task.", "labels": [], "entities": []}, {"text": "The methods used to collect this dataset is described in ().", "labels": [], "entities": []}, {"text": "Examples from the dataset with annotations at the end are given below: \"@USER That shit weird!", "labels": [], "entities": [{"text": "USER", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.7343551516532898}]}, {"text": "Lol OFF (offensive) --\" \"@USER @USER You are an embarrassing citizen!!", "labels": [], "entities": [{"text": "OFF", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9829216003417969}]}, {"text": "OFF TIN -\" \"@USER @USER Liberals ruin everything!", "labels": [], "entities": [{"text": "OFF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5641167759895325}, {"text": "TIN", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7245841026306152}, {"text": "USER", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.6636660695075989}]}, {"text": "OFF TIN GRP\" This paper describes a bidirectional Long Short Term Memory network (biLSTM) model with an Attention layer to identify offensive language in Twitter.", "labels": [], "entities": [{"text": "OFF TIN GRP", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.6141943335533142}]}, {"text": "The rest of the paper is organized as follows: In section 2, we introduce an overview of the work related to identification of offensive language.", "labels": [], "entities": [{"text": "identification of offensive language", "start_pos": 109, "end_pos": 145, "type": "TASK", "confidence": 0.9080088287591934}]}, {"text": "In Section 3 we describe our model structure and differences between the different runs submitted for each sub-task.", "labels": [], "entities": []}, {"text": "In Section 4 we provide the results and discuss the performance of the system.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we conclude giving an outline for the future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. For  this sub-task we have achieved the highest score  with the system we did not train with an additional  dataset. F1 scores and accuracies of all submis- sions for the subsequent tasks are seen in", "labels": [], "entities": [{"text": "F1", "start_pos": 127, "end_pos": 129, "type": "METRIC", "confidence": 0.9991528987884521}, {"text": "accuracies", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.9292387366294861}]}, {"text": " Table 1: Results of different submissions for Sub-task A.", "labels": [], "entities": []}, {"text": " Table 2: Results of different submissions for Sub-task B.", "labels": [], "entities": []}, {"text": " Table 3: Results of different submissions for Sub-task C.", "labels": [], "entities": []}, {"text": " Table 4: Overall results and best rankings", "labels": [], "entities": []}]}