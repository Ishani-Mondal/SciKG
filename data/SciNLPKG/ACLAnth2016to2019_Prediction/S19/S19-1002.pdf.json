{"title": [{"text": "Word Usage Similarity Estimation with Sentence Representations and Automatic Substitutes", "labels": [], "entities": [{"text": "Word Usage Similarity Estimation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8353200256824493}]}], "abstractContent": [{"text": "Usage similarity estimation addresses the semantic proximity of word instances in different contexts.", "labels": [], "entities": [{"text": "Usage similarity estimation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8888747294743856}]}, {"text": "We apply contextualized (ELMo and BERT) word and sentence embeddings to this task, and propose supervised models that leverage these representations for prediction.", "labels": [], "entities": [{"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9888584613800049}]}, {"text": "Our models are further assisted by lexical substitute annotations automatically assigned to word instances by context2vec, a neural model that relies on a bidirectional LSTM.", "labels": [], "entities": []}, {"text": "We perform an extensive comparison of existing word and sentence representations on benchmark datasets addressing both graded and binary similarity.", "labels": [], "entities": []}, {"text": "The best performing models outperform previous methods in both settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional word embeddings, like Word2Vec and GloVe, merge different meanings of a word in a single vector representation ().", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9306274652481079}]}, {"text": "These pre-trained embeddings are fixed, and stay the same independently of the context of use.", "labels": [], "entities": []}, {"text": "Current contextualized sense representations, like ELMo and BERT, go to the other extreme and model meaning as word usage.", "labels": [], "entities": [{"text": "BERT", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9910935759544373}]}, {"text": "They provide a dynamic representation of word meaning adapted to every new context of use.", "labels": [], "entities": []}, {"text": "In this work, we perform an extensive comparison of existing static and dynamic embeddingbased meaning representation methods on the usage similarity (Usim) task, which involves estimating the semantic proximity of word instances in different contexts (.", "labels": [], "entities": []}, {"text": "Usim differs from a classical Semantic Textual Similarity task by the focus on a particular word in the sentence.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.6831338927149773}]}, {"text": "We evaluate on this task word and context representations obtained using pre-trained uncontextualized word grass clippings can be brought out to the landfill at anytime for no *charge* and may not be placed in city cans . embeddings (GloVe) (), with and without dimensionality reduction (SIF) (; context representations obtained from a bidirectional LSTM (context2vec) (; contextualized word embeddings derived from a LSTM bidirectional language model (ELMo) () and generated by a Transformer (BERT); doc2vec ( and Universal Sentence Encoder representations).", "labels": [], "entities": [{"text": "BERT", "start_pos": 494, "end_pos": 498, "type": "METRIC", "confidence": 0.8949991464614868}]}, {"text": "All these embedding-based methods provide direct assessments of usage similarity.", "labels": [], "entities": []}, {"text": "The best representations are used as features in supervised models for Usim prediction, trained on similarity judgments.", "labels": [], "entities": [{"text": "Usim prediction", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.9122285544872284}]}, {"text": "We combine direct Usim assessments, made by the embedding-based methods, with a substitutebased Usim approach.", "labels": [], "entities": []}, {"text": "Building upon previous work that used manually selected in-context substitutes as a proxy for Usim (, we propose to automatize the annotation collection step in order to scale up the method and make it operational on unrestricted text.", "labels": [], "entities": []}, {"text": "We exploit annotations assigned to words in context by the context2vec lexical substitution model, which relies on word and context representations learned by a bidirectional LSTM from a large corpus.", "labels": [], "entities": []}, {"text": "The main contributions of this paper can be summarized as follows: \u2022 we provide a direct comparison of a wide range of word and sentence representation methods on the Usage Similarity (Usim) task and show that current contextualized representations can successfully predict Usim; \u2022 we propose to automatize, and scale up, previous substitute-based Usim prediction methods; \u2022 we propose supervised models for Usim prediction which integrate embedding and lexical substitution features; \u2022 we propose a methodology for collecting new training data for supervised Usim prediction from datasets annotated for related tasks.", "labels": [], "entities": [{"text": "Usim prediction", "start_pos": 408, "end_pos": 423, "type": "TASK", "confidence": 0.8412609994411469}, {"text": "Usim prediction", "start_pos": 560, "end_pos": 575, "type": "TASK", "confidence": 0.7436345517635345}]}, {"text": "We test our models on benchmark datasets containing gold graded and binary word Usim judgments (.", "labels": [], "entities": []}, {"text": "From the compared embeddingbased approaches, the BERT model gives best results on both types of data, providing a straightforward way for word usage similarity calculation.", "labels": [], "entities": [{"text": "BERT", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9936694502830505}, {"text": "word usage similarity calculation", "start_pos": 138, "end_pos": 171, "type": "TASK", "confidence": 0.69241002202034}]}, {"text": "Our supervised model performs on par with BERT on the graded and binary Usim tasks, when using embedding-based representations and clean lexical substitutes.", "labels": [], "entities": [{"text": "BERT", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9945729374885559}]}], "datasetContent": [{"text": "We use the training and test datasets of the SemEval-2007 Lexical Substitution (LexSub) task, which contain instances of target words in sentential context handlabelled with meaning-preserving substitutes.", "labels": [], "entities": [{"text": "SemEval-2007 Lexical Substitution (LexSub) task", "start_pos": 45, "end_pos": 92, "type": "TASK", "confidence": 0.7980664116995675}]}, {"text": "A subset of the LexSub data (10 instances x 56 lemmas) has additionally been annotated with graded pairwise Usim judgments.", "labels": [], "entities": [{"text": "LexSub data", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.9915172159671783}]}, {"text": "Each sentence pair received a rating (on a scale of 1-5) by multiple annotators, and the average judgment for each pair was retained.", "labels": [], "entities": []}, {"text": "derive two additional scores from Usim annotations that denote how easy it is to partition a lemma's usages into sets describing distinct senses: Uiaa, the inter-annotator agreement fora given lemma, taken as the average pairwise Spearman's \u03c1 correlation between ranked judgments of the annotators; and Umid, the proportion of midrange judgments overall instances fora lemma and all annotators.", "labels": [], "entities": [{"text": "pairwise Spearman's \u03c1 correlation", "start_pos": 221, "end_pos": 254, "type": "METRIC", "confidence": 0.5982880771160126}]}, {"text": "In our experiments, we use 2,466 sentence pairs from the Usim data for training, development and testing of different automatic Usim prediction methods.", "labels": [], "entities": [{"text": "Usim data", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.7626908421516418}, {"text": "Usim prediction", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.761587917804718}]}, {"text": "Our models rely on substitutes automatically assigned to words in context using context2vec, and on various word and sentence embedding representations.", "labels": [], "entities": []}, {"text": "We also train a model using the gold substitutes, to test how well our models perform when substitute quality is high.", "labels": [], "entities": []}, {"text": "Performance of the different models is evaluated by measuring how well they approximate the Usim scores assigned by annotators.", "labels": [], "entities": []}, {"text": "shows examples of sentence pairs from the Usim dataset () with the GOLD substitutes and Usim scores assigned by the annotators.", "labels": [], "entities": [{"text": "Usim dataset", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8077521920204163}]}, {"text": "The Usim score is high for similar instances, and decreases for instances that describe different meanings.", "labels": [], "entities": []}, {"text": "The semantic proximity of two instances is also reflected in the similarity of their substitutes sets.", "labels": [], "entities": []}, {"text": "For comparison, we also give in the Table the substitutes selected for these instances by the automatic context2vec substitution method used in our experiments (more details in Section 4.2).", "labels": [], "entities": []}, {"text": "The third dataset we use in our experiments is the recently released Word-in-Context (WiC) dataset), version 0.1.", "labels": [], "entities": [{"text": "Word-in-Context (WiC) dataset", "start_pos": 69, "end_pos": 98, "type": "DATASET", "confidence": 0.653548139333725}]}, {"text": "WiC provides pairs of contextualized target word instances describing the same or different meaning, framing in-context sense identification as a binary classification task.", "labels": [], "entities": []}, {"text": "For example, a sentence pair for the noun stream is: ['Stream of consciousness' -'Two streams of development run through American history'].", "labels": [], "entities": []}, {"text": "A system is expected to be able to identify that stream does not have the same meaning in the two sentences.", "labels": [], "entities": []}, {"text": "WiC sentences were extracted from example usages in WordNet), and Wiktionary.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9557061791419983}]}, {"text": "Instance pairs were automatically labeled as positive (T) or negative (F) (corresponding to the same/different sense) using information in the lexicographic resources, such as presence in the same or different synsets.", "labels": [], "entities": []}, {"text": "Each word is represented by at most three instances in WiC, and repeated sentences are excluded.", "labels": [], "entities": []}, {"text": "It is important to note that meanings represented in the WiC dataset are coarser-grained than WordNet senses.", "labels": [], "entities": [{"text": "WiC dataset", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.8626229166984558}]}, {"text": "This was ensured by excluding WordNet synsets describing highly sim-ilar meanings (sister senses, and senses belonging to the same supersense).", "labels": [], "entities": []}, {"text": "The human-level performance upper-bound on this binary task, as measured on two 100-sentence samples, is 80.5%.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement is also high, at 79%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.8206543922424316}]}, {"text": "The dataset comes with an official train/dev/test split containing 7,618, 702 and 1,366 sentence pairs, respectively.", "labels": [], "entities": []}, {"text": "Direct Usim Prediction Correlation results between Usim judgments and the cosine similarity of the embedding representations described in Section 4.1 are found in.", "labels": [], "entities": []}, {"text": "Detailed results for all context window combinations are given in Appendix B. We observe that target word BERT embeddings give best performance in this task.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9551723003387451}, {"text": "BERT", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9712141752243042}]}, {"text": "Selecting a context window around (or including) the target word does not always help, on the contrary it can harm the models.", "labels": [], "entities": []}, {"text": "Context2vec sentence representations are the next best performing representation, after BERT, but their correlation is much lower.", "labels": [], "entities": [{"text": "BERT", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9527653455734253}, {"text": "correlation", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.9679058790206909}]}, {"text": "The simple GloVe-based SIF approach for sentence representation, which consists in applying dimensionality reduction to a weighted average of GloVe vectors of the words in a sentence, is much superior to the simple average of GloVe vectors and even better than doc2vec sentence representations, obtaining a correlation comparable to: Spearman \u03c1 correlation of different sentence and word embeddings on the Usim dataset using different context window sizes (cw).", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7406569421291351}, {"text": "Spearman \u03c1 correlation", "start_pos": 334, "end_pos": 356, "type": "METRIC", "confidence": 0.7213473717371622}, {"text": "Usim dataset", "start_pos": 406, "end_pos": 418, "type": "DATASET", "confidence": 0.908533126115799}]}, {"text": "For BERT and ELMo, top refers to the top layer, and av refers to the average of layers (3 for ELMo, and the last 4 for BERT). that of USE.", "labels": [], "entities": [{"text": "BERT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8754956722259521}, {"text": "BERT", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9883058667182922}, {"text": "USE", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.8655176162719727}]}, {"text": "Graded Usim To evaluate the performance of our supervised models, we measure the correlation of the predictions with human similarity judgments on the Usim dataset using Spearman's \u03c1.", "labels": [], "entities": [{"text": "Usim dataset", "start_pos": 151, "end_pos": 163, "type": "DATASET", "confidence": 0.7306974232196808}]}, {"text": "Results reported in are the average of the correlations obtained for each target word with gold and automatic substitutes (from the two substitute pools), and for each type of features, substitutebased and embedding-based (cosine similarities from BERT and context2vec).", "labels": [], "entities": [{"text": "BERT", "start_pos": 248, "end_pos": 252, "type": "METRIC", "confidence": 0.8878918886184692}]}, {"text": "We also report results with the additional CoInCo training data.", "labels": [], "entities": [{"text": "CoInCo training data", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.8306280771891276}]}, {"text": "Unsurprisingly, the best results are obtained by the methods that use the gold substitutes.", "labels": [], "entities": []}, {"text": "This is consistent with previous analyses by who found overlap in manually-proposed substitutes to correlate with Usim judgments.", "labels": [], "entities": []}, {"text": "The lower performance of features that rely on automatically selected substitutes (AUTO-LSCNC and AUTO-PPDB) demonstrates the impact of substitute quality on the contribution of this type of features.", "labels": [], "entities": [{"text": "AUTO-LSCNC", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.5638474822044373}, {"text": "AUTO-PPDB", "start_pos": 98, "end_pos": 107, "type": "DATASET", "confidence": 0.5945225358009338}]}, {"text": "The addition of CoInCo data does not seem to help the models, as results are slightly lower than in the only Usim setting.", "labels": [], "entities": []}, {"text": "This can be due to the fact that CoInCo data contains only extreme cases of similarity (SAME/DIFF) and no intermediate ratings.", "labels": [], "entities": [{"text": "CoInCo data", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.8445475697517395}, {"text": "similarity", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9898069500923157}, {"text": "SAME/DIFF)", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.7624567151069641}]}, {"text": "The slight improvement in the combined settings over embedding-based models is not significant in AUTO-LSCNC substitutes, but it is for gold substitutes (p < 0.001).", "labels": [], "entities": [{"text": "AUTO-LSCNC", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.4958840012550354}]}, {"text": "For comparison to the topic-modelling approach of, we evaluate on the 34 lemmas used in their experiments.", "labels": [], "entities": []}, {"text": "They report a correlation calculated overall instances.", "labels": [], "entities": []}, {"text": "With the exception of the substitute-only setting with PPDB candidates, all of our Usim models get higher correlation than their model (\u03c1 = 0.202), with \u03c1 = 0.512 for the combination of AUTO-LSCNC substitutes and embeddings.", "labels": [], "entities": [{"text": "correlation", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9552150964736938}]}, {"text": "The average of the per target word correlation in (\u03c1 = 0.388) is still lower than that of our AUTO-LSCNC model in the combined setting (\u03c1 = 0.500).", "labels": [], "entities": [{"text": "AUTO-LSCNC", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.6620863080024719}]}, {"text": "Binary Usim We evaluate the predictions of our binary classifiers by measuring accuracy on the test portion of the WiC dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9994858503341675}, {"text": "WiC dataset", "start_pos": 115, "end_pos": 126, "type": "DATASET", "confidence": 0.9193306267261505}]}, {"text": "Results for the best configurations for each training set are reported in.", "labels": [], "entities": []}, {"text": "Experiments on the development set showed that target word BERT representations and USE sentence embeddings are the best-suited for WiC.", "labels": [], "entities": [{"text": "BERT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.968282163143158}]}, {"text": "Therefore, 'embedding-based features' here refers to these two representations.", "labels": [], "entities": []}, {"text": "Results on the development set can be found in Appendix D. All configurations obtain higher accuracy than the previous best reported result on this dataset (59.4)), obtained using DeConf vectors, which are multi-prototype embeddings based on WordNet knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9986674785614014}, {"text": "WordNet knowledge", "start_pos": 242, "end_pos": 259, "type": "DATASET", "confidence": 0.9369002878665924}]}, {"text": "Similar to the graded Usim experiments, adding substitute-based features to embedding features slightly improves the accuracy of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9992431402206421}]}, {"text": "Also, combining the CoInCo and WiC data for training does not have a clear impact on results, even in this binary classification setting.", "labels": [], "entities": []}, {"text": "We measure the quality of the substitutes retained in the automatic ranking produced by context2vec after filtering against gold substitute annotations in LexSub data.", "labels": [], "entities": [{"text": "LexSub data", "start_pos": 155, "end_pos": 166, "type": "DATASET", "confidence": 0.9897499084472656}]}, {"text": "Here, we only use the portion of LexSub data that does not contain Usim judgments.", "labels": [], "entities": [{"text": "LexSub data", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9772946238517761}]}, {"text": "We measure filtered substitute quality against the gold standard using the F1-score, and the proportion of false positives (FP) overall positives (TP+FP).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9981786012649536}, {"text": "proportion of false positives (FP) overall positives (TP+FP)", "start_pos": 93, "end_pos": 153, "type": "METRIC", "confidence": 0.7580066216843468}]}, {"text": "shows results for annotations assigned by context2vec using the the LexSub/CoInCo pool of substitutes (AUTO-LSCNC).", "labels": [], "entities": [{"text": "LexSub/CoInCo pool", "start_pos": 68, "end_pos": 86, "type": "DATASET", "confidence": 0.9077377468347549}, {"text": "AUTO-LSCNC", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.8003471493721008}]}, {"text": "shows results for context2vec annotations with the PPDB pool of substitutes (AUTO-PPDB).", "labels": [], "entities": [{"text": "AUTO-PPDB", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.8385593295097351}]}], "tableCaptions": [{"text": " Table 2: Spearman \u03c1 correlation of different sentence  and word embeddings on the Usim dataset using differ- ent context window sizes (cw). For BERT and ELMo,  top refers to the top layer, and av refers to the average  of layers (3 for ELMo, and the last 4 for BERT).", "labels": [], "entities": [{"text": "Usim dataset", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.9067871570587158}, {"text": "BERT", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.7583968639373779}, {"text": "BERT", "start_pos": 262, "end_pos": 266, "type": "METRIC", "confidence": 0.9516823887825012}]}, {"text": " Table 3: Graded Usim results: Spearman's \u03c1 correlation results between supervised model predictions and graded  annotations on the Usim test set. The first column reports results obtained using gold substitute annotations for each  target word instance. The last two columns give results with automatic substitutes selected among all substitutes  proposed for the word in the LexSub and CoInCo datasets (AUTO-LSCNC), or paraphrases in the PPDB XXL  package (AUTO-PPDB). The Embedding-based configuration uses cosine similarities from BERT and context2vec.", "labels": [], "entities": [{"text": "Usim test set", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.7333030899365743}, {"text": "LexSub and CoInCo datasets", "start_pos": 377, "end_pos": 403, "type": "DATASET", "confidence": 0.787992998957634}, {"text": "BERT", "start_pos": 535, "end_pos": 539, "type": "METRIC", "confidence": 0.9188805222511292}]}, {"text": " Table 4: Binary Usim results: Accuracy of models on the WiC test set. The Embedding-based configuration  includes cosine similarities of BERT target and USE. The Combined setting uses, in addition, substitute overlap  features (AUTO-PPDB).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9733343124389648}, {"text": "WiC test set", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.8810893893241882}, {"text": "BERT", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9965533018112183}, {"text": "AUTO-PPDB", "start_pos": 229, "end_pos": 238, "type": "METRIC", "confidence": 0.9739554524421692}]}, {"text": " Table 5: Results of different substitute filtering strate- gies applied to annotations assigned by context2vec  when using the LexSub/CoInCo pool of substitutes  (AUTO-LSCNC).", "labels": [], "entities": [{"text": "LexSub/CoInCo pool", "start_pos": 128, "end_pos": 146, "type": "DATASET", "confidence": 0.9031596332788467}]}, {"text": " Table 6: Results of different substitute filtering strate- gies applied to annotations assigned by context2vec  when using the PPDB pool of substitutes (AUTO-", "labels": [], "entities": [{"text": "AUTO", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.7882150411605835}]}, {"text": " Table 7: Correlations of sentence and word embed- dings on the Usim dataset using different context win- dow sizes (cw). For BERT and ELMo, top refers to the  top layer, and av refers to the average of layers (3 for  ELMo, and the last 4 for BERT). concat 4 refers to the  concatenation of the last 4 layers of BERT.", "labels": [], "entities": [{"text": "Usim dataset", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9313840270042419}, {"text": "BERT", "start_pos": 243, "end_pos": 247, "type": "METRIC", "confidence": 0.9537534713745117}]}, {"text": " Table 9: Results of feature ablation experiments for systems trained and tested on the Usim dataset with gold  substitutes (Gold) as well as automatic substitutes from different pools, Lexsub/CoInCo (AUTO-LSCNC) and PPDB  (AUTO-PPDB). Rows indicate the feature that is removed each time. Numbers correspond to the average Spearman  \u03c1 correlation on the development set across target words.", "labels": [], "entities": [{"text": "Usim dataset", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.9699943959712982}, {"text": "Spearman  \u03c1 correlation", "start_pos": 323, "end_pos": 346, "type": "METRIC", "confidence": 0.7174382905165354}]}]}