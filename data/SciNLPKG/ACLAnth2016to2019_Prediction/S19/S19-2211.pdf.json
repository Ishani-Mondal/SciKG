{"title": [{"text": "INRIA at SemEval-2019 Task 9: Suggestion Mining Using SVM with Handcrafted Features", "labels": [], "entities": [{"text": "INRIA", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8911809921264648}, {"text": "SemEval-2019 Task 9", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7984244426091512}, {"text": "Suggestion Mining", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.9904473423957825}]}], "abstractContent": [{"text": "We present the INRIA approach to the suggestion mining task at SemEval 2019.", "labels": [], "entities": [{"text": "INRIA", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.6916941404342651}, {"text": "suggestion mining task", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7976217766602834}]}, {"text": "The task consists of two subtasks: suggestion mining under single-domain (Subtask A) and cross-domain (Subtask B) settings.", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7607073187828064}]}, {"text": "We used the Support Vector Machines algorithm trained on handcrafted features, function words, sentiment features, digits, and verbs for Subtask A, and handcrafted features for Subtask B. Our best run archived a F1-score of 51.18% on Subtask A, and ranked in the top ten of the submissions for Subtask B with 73.30% F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9992731213569641}, {"text": "F1-score", "start_pos": 316, "end_pos": 324, "type": "METRIC", "confidence": 0.9991338849067688}]}], "introductionContent": [{"text": "Suggestion mining can be viewed as a task of extracting suggestions from unstructured text samples (.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9723154604434967}]}, {"text": "The task goes beyond the sentiment polarity detection and is useful fora variety of purposes, e.g., organizations can improve their products basing on the suggestions from online sources without the need of manually analyzing large amounts of unstructured data.", "labels": [], "entities": [{"text": "sentiment polarity detection", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.8906963467597961}]}, {"text": "In this first edition of the suggestion mining SemEval task (), two settings of the task are addressed: single-domain (or domainspecific) suggestion mining, where the training, development, and test sets belong to the same domain (in the context of this shared task, suggestion forum for Windows platform developers), and cross-domain setting, where training and development/test sets belong to different domains (training on developer suggestion forums and testing on hotel reviews).", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7340223491191864}]}, {"text": "In the both domains, only explicit expressions of suggestions are considered: lexical cues of a suggestion are explicitly mentioned in the text ().", "labels": [], "entities": []}, {"text": "We approach the task from a machine-learning perspective as a binary classification of given sentences into suggestion and non-suggestion classes.", "labels": [], "entities": []}, {"text": "We propose a straightforward approach that can be applied when the availability of training/evaluation data and external linguistic resources is scarce, and evaluate it in the context of this shared task.", "labels": [], "entities": []}, {"text": "We were particularly interested in evaluating our approach under cross-domain conditions (Subtask B), since this setting is more common in a real-word scenario of the task.", "labels": [], "entities": []}, {"text": "Further, we briefly describe the datasets used in the competition and focus on the configuration of our system.", "labels": [], "entities": []}], "datasetContent": [{"text": "Classifier We used the scikit-learn (Pedregosa et al., 2011) implementation of the Support Vector Machines (SVM) algorithm, which is considered among the best-performing algorithms for text classification tasks in general, including when cross-domain conditions and binary classification are concerned, and for the suggestion mining task in particular (.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 185, "end_pos": 210, "type": "TASK", "confidence": 0.796911378701528}, {"text": "suggestion mining task", "start_pos": 315, "end_pos": 337, "type": "TASK", "confidence": 0.8447074492772421}]}, {"text": "We set the class weight parameter to 'balanced' and the penalty parameter (C) to 0.01 for Subtask A and to 0.0001 for Subtask B, tuning the parameters according to the results on the development data.", "labels": [], "entities": [{"text": "penalty parameter (C)", "start_pos": 56, "end_pos": 77, "type": "METRIC", "confidence": 0.9015743732452393}]}, {"text": "Weighting scheme We used term frequency (tf ) weighting scheme, i.e., the number of times a term occurs in a sentence.", "labels": [], "entities": [{"text": "term frequency (tf ) weighting", "start_pos": 25, "end_pos": 55, "type": "METRIC", "confidence": 0.9015916486581167}]}, {"text": "Evaluation For the evaluation of our system, we conducted experiments on the development sets for Subtasks A and B measuring the results in terms of precision, recall, and F1-score for the positive class (the official metric).", "labels": [], "entities": [{"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.99960857629776}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9993529915809631}, {"text": "F1-score", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9995660185813904}]}, {"text": "For training our system, we used only the data provided by the organizers: when evaluating on the development data, we trained our system on the training datasets, while when evaluating on the test data, we merged the training and Subtask A development sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. A more detailed description  of the datasets used in the shared task can be found  in (", "labels": [], "entities": []}, {"text": " Table 1: Suggestion mining datasets statistics.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.982553631067276}]}, {"text": " Table 2: Ablation study of the feature types used for  Subtask A.", "labels": [], "entities": [{"text": "Subtask A", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8780798316001892}]}, {"text": " Table 3. The results for  the rule-based baseline approach proposed by the  organizers are also presented.", "labels": [], "entities": []}, {"text": " Table 3: Results for the INRIA and the baseline ap- proaches on the development (dev.) and test sets for  Subtasks A and B.", "labels": [], "entities": [{"text": "INRIA", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.5528897643089294}]}, {"text": " Table 4: Results for the original and test-like distribu- tions of positive classes.", "labels": [], "entities": []}, {"text": " Table 5: Results for the original and train-like distribu- tions of positive classes.", "labels": [], "entities": []}]}