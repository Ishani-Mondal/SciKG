{"title": [{"text": "Duluth at SemEval-2019 Task 4: The Pioquinto Manterola Hyperpartisan News Detector", "labels": [], "entities": [{"text": "Hyperpartisan News Detector", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.5857576727867126}]}], "abstractContent": [{"text": "This paper describes the Pioquinto Manterola Hyperpartisan News Detector, which participated in SemEval-2019 Task 4.", "labels": [], "entities": [{"text": "Pioquinto Manterola Hyperpartisan News Detector", "start_pos": 25, "end_pos": 72, "type": "TASK", "confidence": 0.5854044318199157}, {"text": "SemEval-2019 Task 4", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8537937005360922}]}, {"text": "Hyperpartisan news is highly polarized and takes a very biased or one-sided view of a particular story.", "labels": [], "entities": []}, {"text": "We developed two variants of our system, the more successful was a Logistic Regression classifier based on unigram features.", "labels": [], "entities": [{"text": "Logistic Regression classifier", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.75543612241745}]}, {"text": "This was our official entry in the task, and it placed 23 rd of 42 participating teams.", "labels": [], "entities": []}, {"text": "Our second variant was a Convolutional Neural Network that did not perform as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media has become a vital source of news for many people.", "labels": [], "entities": []}, {"text": "It makes it possible to share useful information widely and in a timely fashion, and yet can also be misused to spread biased, misleading, or dangerous content.", "labels": [], "entities": []}, {"text": "Hyperpartisan news is a particular worry in that it is premised on absolute allegiance to one particular point of view, and seeks to reinforce potentially misinformed opinions held by its readers.", "labels": [], "entities": [{"text": "Hyperpartisan news", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.809346467256546}]}, {"text": "This has led to very real consequences in this world.", "labels": [], "entities": []}, {"text": "A tragic example can be found in Myanmar, where Buddhist ultranationalists relied on social media to spread hyperpartisan and fake news in order to promote hatred and violence against different Muslim communities.", "labels": [], "entities": []}, {"text": "While related, Hyperpartisan news is not the same as fake news.", "labels": [], "entities": []}, {"text": "The former shows a high degree of bias, whereas the latter is more so an outright fabrication.", "labels": [], "entities": []}, {"text": "However, the techniques applied to detecting both are similar.", "labels": [], "entities": [{"text": "detecting", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.9674785733222961}]}, {"text": "For example, detected fake news by training Support Vector Machines using ngrams, punctuation, and measures of readability.", "labels": [], "entities": []}, {"text": "() used likes of articles as features for building a Logistic Regression classifier for fake news detection.", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.6637686987717947}]}, {"text": "identified hyperpartisan news through the use of style and readability features, and also employed a technique known as unmasking () to distinguish between hyperpartisan and mainstream news.", "labels": [], "entities": []}], "datasetContent": [{"text": "The formal task evaluation was carried on virtual systems provided by the organizers using the TIRA system (Potthast et al., 2019).", "labels": [], "entities": []}, {"text": "We trained both our LR and CNN on the entire by-article training corpus and saved the resulting models to disk (so they could be ported over to the evaluation system).", "labels": [], "entities": []}, {"text": "We decided to use LR and CNN as our two entries to the task, since during our development phase they had very similar results on 10-fold cross validation : LR accuracy was 0.77 \u00b1 0.06 while CNN was at 0.75 \u00b1 0.05.", "labels": [], "entities": [{"text": "CNN", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.7942447662353516}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.8491084575653076}, {"text": "CNN", "start_pos": 190, "end_pos": 193, "type": "METRIC", "confidence": 0.9196956157684326}]}, {"text": "However, on the official evaluation run (using a held outset of test data the systems had never seen), the CNN performed poorly and only attained accuracy of 0.58.", "labels": [], "entities": [{"text": "CNN", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.7934079170227051}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9996585845947266}]}, {"text": "LR on the other hand reached accuracy of 0.70 and so was selected by the organizers as our official entry to the task.", "labels": [], "entities": [{"text": "LR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5613084435462952}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9996944665908813}]}, {"text": "Other evaluation metrics including Precision (P), Recall (R), and F1 are shown in.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.9451172798871994}, {"text": "Recall (R)", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9657862335443497}, {"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9996448755264282}]}, {"text": "The confusion matrix for our LR system is shown in and for the CNN system in.", "labels": [], "entities": []}, {"text": "In these matrices the distribution of corrector gold standard answers are shown in the columns (with sums 314) and the system predictions are shown across in the rows.", "labels": [], "entities": []}, {"text": "While the evaluation phase test data is balanced between the classes H and M, the by-article training data was not (238 H versus 407 M).", "labels": [], "entities": []}, {"text": "shows that LR predicted a somewhat more balanced distribution of classes (266 H vs. 362 M), which is reflected in the relatively similar Precision and Recall scores found in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9973469972610474}, {"text": "Recall", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.5666041374206543}]}, {"text": "However, shows that the CNN produced a much more skewed result (67 H vs. 561 M) which led to very high Precision for the CNN (0.87) while the Recall was extremely low (0.18).", "labels": [], "entities": [{"text": "CNN", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.9410427808761597}, {"text": "Precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9996638298034668}, {"text": "CNN", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.9398311972618103}, {"text": "Recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.587316632270813}]}, {"text": "We hypothesize that the difference between the distribution of classes in the training versus evaluation data at least partially explains this result.", "labels": [], "entities": []}, {"text": "Given more examples of mainstream news (M),   both models learned this class more thoroughly and so tended to classify articles into this category.", "labels": [], "entities": [{"text": "mainstream news (M)", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7543288171291351}]}, {"text": "The LR model appears to be more robust in that it performed at approximately the same level of accuracy both during development phase cross validation and the final evaluation round (despite the difference in the distribution of classes).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.99912029504776}]}, {"text": "The CNN on the other hand appears to have been very negatively affected by the shift in the distribution of classes from training to evaluation data, and performed significantly worse on the evaluation data as compared with cross validation on the training set.", "labels": [], "entities": []}, {"text": "We are uncertain as to the causes of the CNN result.", "labels": [], "entities": [{"text": "CNN result", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.8007411956787109}]}, {"text": "It is important to note that the by-article data is relatively small and that this may put the CNN at a disadvantage.", "labels": [], "entities": [{"text": "CNN", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.8036563992500305}]}, {"text": "We also noticed that the accuracy of the CNN on the training data was 1.00 and much lower on the evaluation data, which is a common sign of overfitting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9996664524078369}, {"text": "CNN", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9581567645072937}]}], "tableCaptions": [{"text": " Table 1: Final Evaluation Results.", "labels": [], "entities": []}, {"text": " Table 2: LR Confusion Matrix.", "labels": [], "entities": []}, {"text": " Table 3: CNN Confusion Matrix.", "labels": [], "entities": [{"text": "CNN Confusion Matrix", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.8705006639162699}]}]}