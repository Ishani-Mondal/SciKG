{"title": [{"text": "RumourEval 2019: Determining Rumour Veracity and Support for Rumours", "labels": [], "entities": [{"text": "Determining Rumour Veracity", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.8306543827056885}]}], "abstractContent": [{"text": "Since the first RumourEval shared task in 2017, interest in automated claim validation has greatly increased, as the danger of \"fake news\" has become a mainstream concern.", "labels": [], "entities": [{"text": "RumourEval shared task in 2017", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.6561768054962158}, {"text": "automated claim validation", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.6365292270978292}]}, {"text": "However automated support for rumour verification remains in its infancy.", "labels": [], "entities": [{"text": "rumour verification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.9493905603885651}]}, {"text": "It is therefore important that a shared task in this area continues to provide a focus for effort, which is likely to increase.", "labels": [], "entities": []}, {"text": "Rumour verification is characterised by the need to consider evolving conversations and news updates to reach a verdict on a rumour's veracity.", "labels": [], "entities": [{"text": "Rumour verification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9664667248725891}]}, {"text": "As in RumourEval 2017 we provided a dataset of dubious posts and ensuing conversations in social media, annotated both for stance and veracity.", "labels": [], "entities": []}, {"text": "The social media rumours stem from a variety of breaking news stories and the dataset is expanded to include Reddit as well as new Twitter posts.", "labels": [], "entities": []}, {"text": "There were two concrete tasks; rumour stance prediction and rumour verification, which we present in detail along with results achieved by participants.", "labels": [], "entities": [{"text": "rumour stance prediction", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.9214025934537252}, {"text": "rumour verification", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8822020590305328}]}, {"text": "We received 22 system submissions (a 70% increase from RumourEval 2017) many of which used state-of-the-art methodology to tackle the challenges involved.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The UK fact-checking charity Full Fact provides a roadmap 7 for development of automated fact checking.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.7156420648097992}]}, {"text": "They cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area.", "labels": [], "entities": []}, {"text": "Shared datasets area crucial part of the joint endeavour.", "labels": [], "entities": []}, {"text": "Datasets for rumour resolution are still relatively few, and likely to be in increasing demand.", "labels": [], "entities": [{"text": "rumour resolution", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.9513864815235138}]}, {"text": "In addition to the data from RumourEval 2017, the dataset released by is also suitable for veracity classification.", "labels": [], "entities": [{"text": "RumourEval 2017", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.6604825556278229}, {"text": "veracity classification", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.9440608024597168}]}, {"text": "It includes 51 true rumours and 60 false rumours, where each rumour includes a stream of tweets associated with it.", "labels": [], "entities": []}, {"text": "Twitter 15 and 16 datasets () contain claim propagation trees and combine tasks of rumour detection and verification in one four-way classification task (Non-rumour, True, False, Unverified).", "labels": [], "entities": [{"text": "Twitter 15 and 16 datasets", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.764749550819397}, {"text": "rumour detection", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.8318133354187012}, {"text": "False", "start_pos": 172, "end_pos": 177, "type": "METRIC", "confidence": 0.951014518737793}]}, {"text": "A Sina Weibo corpus is also available (, in which 5000 posts are classified for veracity, but responses are not available.", "labels": [], "entities": [{"text": "Sina Weibo corpus", "start_pos": 2, "end_pos": 19, "type": "DATASET", "confidence": 0.747887114683787}]}, {"text": "Partially generated statistical claim checking data is now becoming available in the context of the FEVER shared task, mentioned above, but is not suitable for this type of work.", "labels": [], "entities": [{"text": "statistical claim checking", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.6474544405937195}, {"text": "FEVER", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.8975799679756165}]}, {"text": "Twitter continues to be a highly relevant platform for rumour verification, being popular with the public as well as politicians.", "labels": [], "entities": [{"text": "rumour verification", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.9738782346248627}]}, {"text": "RumourEval 2019 also includes Reddit data, thus providing more diversity in the types of users, more focussed discussions and longer texts.", "labels": [], "entities": [{"text": "RumourEval 2019", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9216170608997345}]}, {"text": "In task A, stance classification, care must betaken to accommodate the skew towards the \"comment\" class, which dominates, as well as being the least helpful type of data in establishing rumour veracity.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.9541892111301422}, {"text": "establishing rumour veracity", "start_pos": 173, "end_pos": 201, "type": "TASK", "confidence": 0.7234142124652863}]}, {"text": "Therefore we used macro-averaged F1 to evaluate performance on task A.", "labels": [], "entities": [{"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.8481059074401855}]}, {"text": "In task B participants supply a true/false classification for each rumour, as well as a confidence score.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 88, "end_pos": 104, "type": "METRIC", "confidence": 0.9659390151500702}]}, {"text": "Macro-averaged F1 was again the score of choice to evaluate the overall classification.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.95039302110672}]}, {"text": "For the confidence score, a root mean squared error (RMSE, a popular metric that differs only from the Brier score in being its square root) was calculated relative to a reference confidence of 1.", "labels": [], "entities": [{"text": "root mean squared error", "start_pos": 28, "end_pos": 51, "type": "METRIC", "confidence": 0.796024888753891}, {"text": "RMSE", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.5439392924308777}, {"text": "Brier score", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.8985690772533417}]}, {"text": "Unverified rumours were considered correctly annotated if they received a confidence score of zero regardless of true/false classification.", "labels": [], "entities": []}, {"text": "The previous RumourEval task used accuracy as the evaluation metric, but that approach allowed higher scores to be obtained through less sensitivity to minority classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9989171028137207}]}, {"text": "For the stance task, 80% of test items were comments, and this is the least interesting class.", "labels": [], "entities": [{"text": "stance task", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9390811324119568}]}, {"text": "For the verification task, class imbalance is not so extreme, with 50%\"false\" in the dataset and close to 40% \"true\" (the remainder are \"unverified\").", "labels": [], "entities": [{"text": "verification task", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.926515519618988}]}, {"text": "Whilst participants weren't evaluated on accuracy for task A, we note that generally speaking, teams that obtained higher macro F1 scores also obtained higher accuracies, and that around 50% of the teams obtained accuracies higher than might be obtained simply by assigning all items to the comment class (majority baseline).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9989423155784607}, {"text": "macro F1 scores", "start_pos": 122, "end_pos": 137, "type": "METRIC", "confidence": 0.6610716183980306}, {"text": "accuracies", "start_pos": 159, "end_pos": 169, "type": "METRIC", "confidence": 0.9596851468086243}]}, {"text": "However, the correlation between accuracy and macro F1 was only 0.47, and use of macro F1 revealed that three teams surged ahead.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9996386766433716}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.8968604803085327}]}, {"text": "For task B, where class imbalance was less pronounced, the relationship between accuracy and macro F1 was much closer, with a correlation of 0.87, though again, F1 was the better differentiator.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9996390342712402}, {"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.6993915438652039}, {"text": "F1", "start_pos": 161, "end_pos": 163, "type": "METRIC", "confidence": 0.9992788434028625}]}, {"text": "Interestingly, RMSE showed a stronger relationship with macro F1 than with accuracy (correlations -0.92 vs -0.77).", "labels": [], "entities": [{"text": "RMSE", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.476989209651947}, {"text": "F1", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.7292054295539856}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9997100234031677}]}], "tableCaptions": [{"text": " Table 3: Task A corpus", "labels": [], "entities": []}, {"text": " Table 4: Task B corpus", "labels": [], "entities": [{"text": "B", "start_pos": 15, "end_pos": 16, "type": "METRIC", "confidence": 0.8539959192276001}]}, {"text": " Table 5: Results table. Ranking is in brackets.", "labels": [], "entities": [{"text": "Ranking", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9850651621818542}]}]}