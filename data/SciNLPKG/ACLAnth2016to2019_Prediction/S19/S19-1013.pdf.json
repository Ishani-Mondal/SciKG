{"title": [{"text": "Deconstructing multimodality: visual properties and visual context inhuman semantic processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Multimodal semantic models that extend linguistic representations with additional perceptual input have proved successful in a range of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Recent research has successfully used neural methods to automatically create visual representations for words.", "labels": [], "entities": []}, {"text": "However, these works have extracted visual features from complete images, and have not examined how different kinds of visual information impact performance.", "labels": [], "entities": []}, {"text": "In contrast, we construct multimodal models that differentiate between internal visual properties of the objects and their external visual context.", "labels": [], "entities": []}, {"text": "We evaluate the models on the task of decoding brain activity associated with the meanings of nouns, demonstrating their advantage over those based on complete images.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal models combining linguistic and visual information have enjoyed a growing interest in the field of semantics.", "labels": [], "entities": []}, {"text": "Recent research has shown that such models outperform purely linguistic models on a range of NLP tasks, including modelling semantic similarity), lexical entailment (, and metaphor identification (.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.8143222033977509}]}, {"text": "Despite this success, little is known about the nature of semantic information learned from images and why it is useful.", "labels": [], "entities": []}, {"text": "For instance, some concepts maybe better characterised by their own (internal) visual properties and others by the (external) visual context, in which they appear.", "labels": [], "entities": []}, {"text": "However, existing neural multimodal semantic approaches use entire images to learn visual word representations, without differentiating between these two kinds of visual information.", "labels": [], "entities": []}, {"text": "In contrast, we investigate whether differentiating between internal visual properties and external visual context is beneficial compared to learning visual representations from complete images.", "labels": [], "entities": []}, {"text": "We construct three multimodal models combining linguistic and visual information: using (1) internal visual features extracted from an object's bounding box, (2) external visual features outside the bounding box, i.e. the visual context, and (3) visual features extracted from complete images.", "labels": [], "entities": []}, {"text": "visualises the different visual information extracted from an image.", "labels": [], "entities": []}, {"text": "We use skip-gram () as our linguistic model and extract visual representations from a convolutional neural network (CNN) pretrained on the ImageNet classification task.", "labels": [], "entities": [{"text": "ImageNet classification task", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.7641070485115051}]}, {"text": "We evaluate the models in their ability to decode patterns of brain activity associated with the meanings of nouns, obtained via brain imaging.", "labels": [], "entities": []}, {"text": "This choice of task allows us to assess the importance of each type of visual information inhuman semantic processing.", "labels": [], "entities": []}, {"text": "Specifically, we perform two experiments: (1) using the Visual Genome () dataset of images where objects are manually annotated with bounding boxes, and (2) using images retrieved from Google Image Search and automatically segmenting them using a Faster R-CNN (FRCNN) model.", "labels": [], "entities": []}, {"text": "We find that all of our multimodal models are able to decode brain activity patterns and that the models relying on internal visual properties are superior to all others.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first experiment with a set of manuallyannotated images from Visual Genome and then with images where objects and their bounding boxes have been automatically detected using FR-CNN networks.", "labels": [], "entities": [{"text": "Visual Genome", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.8806466460227966}]}], "tableCaptions": [{"text": " Table 1: Average decoding accuracies for the models trained on Visual Genome per participant and the mean over  participants. Vis=visual, MM=multimodal, COMBINED=explicitly differentiates internal and external features.", "labels": [], "entities": [{"text": "COMBINED", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9820446968078613}]}, {"text": " Table 2: Average decoding accuracies over the nine  participants for the semantic models trained on the au- tomatically annotated images. Naming convention fol- lows Table 1", "labels": [], "entities": []}]}