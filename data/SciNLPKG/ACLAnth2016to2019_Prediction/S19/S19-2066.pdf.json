{"title": [{"text": "Amobee at SemEval-2019 Tasks 5 and 6: Multiple Choice CNN Over Contextual Embedding", "labels": [], "entities": []}], "abstractContent": [{"text": "This article describes Amobee's participation in \"HatEval: Multilingual detection of hate speech against immigrants and women in Twitter\" (task 5) and \"OffensEval: Identifying and Categorizing Offensive Language in Social Media\" (task 6).", "labels": [], "entities": [{"text": "Multilingual detection of hate speech", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.8418109655380249}]}, {"text": "The goal of task 5 was to detect hate speech targeted to women and immigrants.", "labels": [], "entities": []}, {"text": "The goal of task 6 was to identify and categorized offensive language in social media, and identify offense target.", "labels": [], "entities": [{"text": "categorized offensive language in social media", "start_pos": 39, "end_pos": 85, "type": "TASK", "confidence": 0.780573437611262}]}, {"text": "We present a novel type of convolutional neural network called \"Multiple Choice CNN\" (MC-CNN) that we used over our newly developed contextual embedding, Rozental et al.", "labels": [], "entities": []}, {"text": "For both tasks we used this architecture and achieved 4th place out of 69 participants with an F 1 score of 0.53 in task 5, in task 6 achieved 2nd place (out of 75) in Sub-task B-automatic categorization of offense types (our model reached places 18/2/7 out of 103/75/65 for sub-tasks A, B and C respectively in task 6).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9853512048721313}]}], "introductionContent": [{"text": "Offensive language and hate speech identification are sub-fields of natural language processing that explores the automatic inference of offensive language and hate speech with its target from textual data.", "labels": [], "entities": [{"text": "hate speech identification", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.6629137794176737}]}, {"text": "The motivation to explore these sub-fields is to possibly limit the hate speech and offensive language on user-generated content, particularly, on social media.", "labels": [], "entities": []}, {"text": "One popular social media platform for researchers to study is Twitter, asocial network website where people \"tweet\" short posts.", "labels": [], "entities": []}, {"text": "Each post may contain URLs and/or mentions of other entities on twitter.", "labels": [], "entities": []}, {"text": "Among these \"tweets\" we can find various opinions of people regarding political events, public figures, products, etc.", "labels": [], "entities": []}, {"text": "Hence, Twitter data turned into one of the main data sources for both academia and industry.", "labels": [], "entities": [{"text": "Twitter data", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.7784892916679382}]}, {"text": "Its unique insights are relevant for business intelligence, marketing and e-governance.", "labels": [], "entities": []}, {"text": "This data also benefits NLP tasks such as sentiment analysis, offensive language detection, topic extraction, etc.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9771652221679688}, {"text": "offensive language detection", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6183837155501047}, {"text": "topic extraction", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.9092454612255096}]}, {"text": "Both the OffensEval 2019 task () and HatEval 2019 task are part of the SemEval-2019 workshop.", "labels": [], "entities": [{"text": "SemEval-2019 workshop", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.7850212752819061}]}, {"text": "OffensEval has 3 subtasks with over 65 groups who participate in each sub-task and HatEval has 2 sub-tasks with 69 groups.", "labels": [], "entities": [{"text": "OffensEval", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8638345003128052}]}, {"text": "Word embedding is one of the most popular representations of document vocabulary in lowdimensional vector.", "labels": [], "entities": []}, {"text": "It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.", "labels": [], "entities": []}, {"text": "For this work, word embedding was created with a model similar to Bidirectional Encoder Representations from Transformers (BERT),.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformers (BERT)", "start_pos": 66, "end_pos": 128, "type": "TASK", "confidence": 0.6786414906382561}]}, {"text": "BERT is a language representation model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9409845471382141}]}, {"text": "As a result, the pre-trained BERT representations can be fine-tuned to create state-of-the-art models fora wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.", "labels": [], "entities": [{"text": "BERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.7451582551002502}, {"text": "question answering", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.8532199859619141}, {"text": "language inference", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.6726535111665726}]}, {"text": "Besides the word embedding, BERT generates a classification token, which can be used for text classification tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9322203397750854}, {"text": "text classification tasks", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.8437051971753439}]}, {"text": "This paper describes our system for the OffensEval 2019 and HatEval 2019 tasks, where our new contribution is the use of contextual embedding (modified BERT) together with an appropriate network architecture for such embeddings . The paper is organized as follows: Section 2 describes the datasets we used and the pre-process phase.", "labels": [], "entities": [{"text": "BERT", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.996093213558197}]}, {"text": "Section 3 describes our system architecture and presents the MC-CNN.", "labels": [], "entities": []}, {"text": "In section 4 we present the results of both tasks -the OffensEval and HatEval.", "labels": [], "entities": [{"text": "HatEval", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.8730894923210144}]}, {"text": "Finally, in section 5 we review and conclude the system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: F 1 Score And Accuracy Of MC-CNN  Comparing To Baselines At HatEval.", "labels": [], "entities": [{"text": "F 1 Score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8758910894393921}, {"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9790976643562317}]}]}