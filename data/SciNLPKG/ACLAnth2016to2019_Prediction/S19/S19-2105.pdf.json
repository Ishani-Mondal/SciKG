{"title": [{"text": "NLP at SemEval-2019 Task 6: Detecting Offensive language using Neural Networks", "labels": [], "entities": [{"text": "Detecting Offensive language", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.8622207641601562}]}], "abstractContent": [{"text": "In this paper we built several deep learning architectures to participate in shared task Of-fensEval: Identifying and categorizing Offensive language in Social media by semEval-2019 (Zampieri et al., 2019b).", "labels": [], "entities": []}, {"text": "The dataset was annotated with three level annotation schemes and task was to detect between offensive and not offensive, categorization and target identification in offensive contents.", "labels": [], "entities": [{"text": "target identification", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.7064480483531952}]}, {"text": "Deep learning models with POS information as feature were also leveraged for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.9702448844909668}]}, {"text": "The three best models that performed best on individual sub tasks are stacking of CNN-Bi-LSTM with Attention, BiLSTM with POS information added with word features and Bi-LSTM for third task.", "labels": [], "entities": [{"text": "stacking", "start_pos": 70, "end_pos": 78, "type": "TASK", "confidence": 0.9506556987762451}, {"text": "CNN-Bi-LSTM", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.8249741196632385}, {"text": "BiLSTM", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.8240363597869873}]}, {"text": "Our models achieved a Macro F1 score of 0.7594, 0.5378 and 0.4588 in Task(A,B,C) respectively with rank of 33 rd , 54 th and 52 nd out of 103, 75 and 65 submissions .", "labels": [], "entities": [{"text": "F1 score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9304645955562592}]}], "introductionContent": [{"text": "Due to the exponential rise in the usage of internet user generated content in the form of blogs, posts, comments etc.", "labels": [], "entities": []}, {"text": "Some users also using this platform to target any individual or any particular group on social media on the basis of certain attributes, sharing different views.", "labels": [], "entities": []}, {"text": "Many studies have been conducted on offensive language, hate speech, cyberbullying, profanity, aggression detection.", "labels": [], "entities": [{"text": "aggression detection", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.7233176380395889}]}, {"text": "These contents are major concern for governments, so robust computational systems need to be developed to tackle these posts to maintain social harmony.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Related work have been discussed in section 2, Methodology have been described in Section 3 followed by data sets and other settings used to solve the tasks in Section 4.", "labels": [], "entities": []}, {"text": "Results and analysis of the models is described in Section 5 with the limitation of the models in Error Analysis in section 6.", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.6150985807180405}]}, {"text": "Section 7 contains the conclusion and Future scope.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Keras with Tensorflow as backend,Scikitlearn library for implementation.", "labels": [], "entities": []}, {"text": "For every dataset we use 80:20 for 80% to use in Training and using grid search to learn batch size and epochs.", "labels": [], "entities": []}, {"text": "Experiments were performed using stratified 5-fold cross validation to train all the classes according to their proportion and 20% of remaining data were used as testing the model.", "labels": [], "entities": []}, {"text": "We are reporting our results on Training data provided by orgainsers by standard Precision, Recall and F-score by averaging all the cross fold results.", "labels": [], "entities": [{"text": "Training data", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.7456284463405609}, {"text": "Precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.99864262342453}, {"text": "Recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9575194716453552}, {"text": "F-score", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.9968242645263672}]}, {"text": "Categorical cross entropy loss function and Adam optimiser were used for training . In the experiment we use publicly available Glove embedding by).", "labels": [], "entities": []}, {"text": "We used batch size of and dropout of (0.1,0.2,0.3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data set:Task A", "labels": [], "entities": []}, {"text": " Table 2: Data set: Task B", "labels": [], "entities": []}, {"text": " Table 3: Data set:Task C", "labels": [], "entities": []}, {"text": " Table 4: Cross validation: Task A", "labels": [], "entities": [{"text": "Cross validation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8874587714672089}]}, {"text": " Table 5: Cross validation: Task B", "labels": [], "entities": [{"text": "Cross validation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8973473608493805}]}, {"text": " Table 6: Cross validation: Task C", "labels": [], "entities": [{"text": "Cross validation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8782457113265991}]}, {"text": " Table 7: Test Set: Task A", "labels": [], "entities": []}, {"text": " Table 8: Test Set: Task B", "labels": [], "entities": []}, {"text": " Table 9: Test Set: Task C", "labels": [], "entities": []}]}