{"title": [{"text": "Beyond Context: A New Perspective for Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Most word embeddings today are trained by optimizing a language modeling goal of scoring words in their context, modeled as a multi-class classification problem.", "labels": [], "entities": []}, {"text": "Despite the successes of this assumption, it is incomplete: in addition to its context, orthographical or morphological aspects of words can offer clues about their meaning.", "labels": [], "entities": []}, {"text": "In this paper, we define anew modeling framework for training word embeddings that captures this intuition.", "labels": [], "entities": []}, {"text": "Our framework is based on the well-studied problem of multi-label classification and, consequently , exposes several design choices for featurizing words and contexts, loss functions for training and score normalization.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.7842428088188171}, {"text": "score normalization", "start_pos": 200, "end_pos": 219, "type": "TASK", "confidence": 0.64057457447052}]}, {"text": "Indeed, standard models such as CBOW and FAST-TEXT are specific choices along each of these axes.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8741040825843811}, {"text": "FAST-TEXT", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.6049835085868835}]}, {"text": "We show via experiments that by combining feature engineering with embedding learning, our method can outperform CBOW using only 10% of the training data in both the standard word embedding evaluations and also text classification experiments.", "labels": [], "entities": [{"text": "text classification", "start_pos": 211, "end_pos": 230, "type": "TASK", "confidence": 0.7373863458633423}]}], "introductionContent": [{"text": "The distributional hypothesis has been a cornerstone in NLP.", "labels": [], "entities": []}, {"text": "For example, writes: . .", "labels": [], "entities": []}, {"text": "the complete meaning of a word is always contextual, and no study of meaning apart from a complete context can betaken seriously.", "labels": [], "entities": []}, {"text": "Operationally, in modern NLP, word embeddings capture this idea and are typically trained using neural language models or word collocations (e.g..", "labels": [], "entities": []}, {"text": "Is word meaning exclusively defined by its context?", "labels": [], "entities": []}, {"text": "In this paper, we argue that while the word usage plays a crucial role in defining its meaning (perhaps, centrally so), it is not the only mechanism that endows meaning to words.", "labels": [], "entities": []}, {"text": "Indeed, Firth writes in the paragraph before the above quote: . .", "labels": [], "entities": []}, {"text": "a certain component of the meaning of a word is described when you say what sort of a word it is, that is when you identify it morphologically.", "labels": [], "entities": []}, {"text": "The composition of a word, (i.e., its orthography and morphology) may offer cues about its meaning even if the word is not commonly used, thus allowing us to understand unseen words.", "labels": [], "entities": []}, {"text": "For example, we can elide over misspellings of words (e.g., Bbeijing) because we observe the similarities in the orthography between words.", "labels": [], "entities": []}, {"text": "By ignoring word-level information, many existing off-the-shelf word embedding approaches suffer from two shortcomings.", "labels": [], "entities": []}, {"text": "First, they need a great amount of training data to get high quality word embeddings.", "labels": [], "entities": []}, {"text": "Second, even with large amounts of training data, some words (e.g., neologisms, misspellings, technical terms) will not be seen frequently enough to provide statistical support for good embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we are motivated by the observation that both the context of a word and its own internal information contribute to word meaning.", "labels": [], "entities": []}, {"text": "To model this in an easy-to-extend manner, we need anew perspective about training word embeddings that not only admits arbitrary word and context features, but also supports conceptual tools to systematically reason about the various model design aspects in terms of familiar modeling techniques.", "labels": [], "entities": []}, {"text": "A common method for training word embeddings is to construct a word prediction problem, and obtain the word embeddings as aside effect.", "labels": [], "entities": [{"text": "word prediction problem", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.796884149312973}]}, {"text": "One instantiation of the word prediction task, namely CBOW (, frames it as the multi-class classification problem of predicting a word given a context.", "labels": [], "entities": [{"text": "word prediction task", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8221468925476074}, {"text": "predicting a word given a context", "start_pos": 117, "end_pos": 150, "type": "TASK", "confidence": 0.7535543938477834}]}, {"text": "We argue that the task is more appropriately framed as multi-label classification -multiple words can fit in the same context.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.6914853453636169}]}, {"text": "Moreover, since the label set (all words) is massive, word prediction is an instance of eXtreme Multi-label Learning (XML).", "labels": [], "entities": [{"text": "word prediction", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.7993816435337067}]}, {"text": "Framing word prediction as an XML problem allows us to define a unifying framework for word embeddings.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7248898893594742}]}, {"text": "Consequently, we can systematically analyze the problem of training word embeddings using lessons from the XML literature.", "labels": [], "entities": []}, {"text": "In particular, we can featurize both inputs and outputs -in our case, contexts and words.", "labels": [], "entities": []}, {"text": "Apart from featurization, loss functions and normalization of probability are also design choices available.", "labels": [], "entities": []}, {"text": "We show that our approach subsumes several standard word embedding learning methods: specific design choices give us familiar models such as CBOW () and FAST-TEXT ( . Our experiments study the interplay between the amount of data needed to train embeddings, and the features for words and contexts.", "labels": [], "entities": [{"text": "FAST-TEXT", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.816813051700592}]}, {"text": "We show that, when trained on the same amount of data, using word and context features outperforms the original CBOW and FASTTEXT on both the standard analogy evaluation and a variant where words have introduce typographical errors.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9539974927902222}]}, {"text": "Featurizing words and contexts reduces data dependency for training and can achieve similar results as CBOW and FASTTEXT trained on a 10x larger corpus.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.841000497341156}]}, {"text": "Finally, we also show that the trained embeddings offer better representations for an text classification evaluation.", "labels": [], "entities": [{"text": "text classification evaluation", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.8675612608591715}]}, {"text": "In summary, the contributions of this work are: (i) We propose anew family of models for word embeddings that allow both word orthography and context to inform its embeddings via user designed features.", "labels": [], "entities": []}, {"text": "(ii) We show that our model family generalizes several well-known methods such as CBOW and FASTTEXT.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.8387244939804077}, {"text": "FASTTEXT", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.670867919921875}]}, {"text": "(iii) Our experiments show that exploiting word and context features gives better embeddings using significantly lower amounts of training data.", "labels": [], "entities": []}, {"text": "(iv) Our experiments also show that while global normalization is the more appropriate formulation, in practice, the av-1 In order to have a fair comparison, we always use the CBOW variant of FASTTEXT in this paper.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.5190885663032532}]}, {"text": "erage number of words in a context is too small for global normalization to prove advantageous.", "labels": [], "entities": [{"text": "global normalization", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7356218695640564}]}], "datasetContent": [{"text": "In this section, we empirically verify that our approach: (i) achieves similar performance as CBOW using the same training set and features, (ii) can outperform CBOW and FASTTEXT with only 10% of the training data if extra features are used, (iii) creates embeddings that generalize better by evaluating analogies on datasets with misspellings, and, (iv) offers a better feature representation for an extrinsic evaluation of text classification.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.6704314947128296}, {"text": "text classification", "start_pos": 425, "end_pos": 444, "type": "TASK", "confidence": 0.7527134418487549}]}, {"text": "The overall goal of our experiments is to understand the dependence between dataset size, features, and the quality of embeddings produced.", "labels": [], "entities": []}, {"text": "We conduct our experiments on the training set of the 1 billion language model benchmark corpus (), consisting of 700M words (with 550K unique words).", "labels": [], "entities": [{"text": "1 billion language model benchmark corpus", "start_pos": 54, "end_pos": 95, "type": "DATASET", "confidence": 0.6385063976049423}]}, {"text": "For all experiments, the embedding size of W and V is 300 and the context window size is set to five words to the left and right of a word.", "labels": [], "entities": []}, {"text": "The hyper-parameter \u03b1 from Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 27, "end_pos": 29, "type": "DATASET", "confidence": 0.9059951305389404}]}, {"text": "8 is 0.001 (following CBOW's implementation).", "labels": [], "entities": [{"text": "CBOW", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.924691915512085}]}, {"text": "To compare to CBOW and FAST-TEXT, we use log loss on all our experiments.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.781303346157074}, {"text": "FAST-TEXT", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.6980357766151428}]}, {"text": "We optimized the loss using Adam ( and used dropout with keep probability 0.6.", "labels": [], "entities": [{"text": "keep probability 0.6", "start_pos": 57, "end_pos": 77, "type": "METRIC", "confidence": 0.9695987900098165}]}, {"text": "Based on preliminary trials, we projected the matrix W onto a unit ball.", "labels": [], "entities": []}, {"text": "For efficient learning, we also used the negative sampling approach from word2vec.", "labels": [], "entities": []}, {"text": "More experiment setup details are available in supplementary material.", "labels": [], "entities": []}, {"text": "summarizes all the feature templates we used.", "labels": [], "entities": []}, {"text": "In the table, the feature Gazetteers is a set of lists containing names of entities from Wikipedia, grouped by category.", "labels": [], "entities": []}, {"text": "Each list represents an entity type such as cities, organizations, days of the week, etc.", "labels": [], "entities": []}, {"text": "If the current word matches one of the words in the list, the corresponding feature is activated.", "labels": [], "entities": []}, {"text": "The Quirk feature is a collection of prefixes and suffixes types from.", "labels": [], "entities": []}, {"text": "For example, un-is a negative prefix and -ness is a noun suffix.", "labels": [], "entities": []}, {"text": "For the context feature function \u03c8, we summed the above features overall the context words.", "labels": [], "entities": []}, {"text": "We implement our model using Pytorch . We train our model on a Nvidia DGX machine using one Tesla (16G video memory) GPU.", "labels": [], "entities": []}, {"text": "We train 70 epochs for both local log loss and global log loss.", "labels": [], "entities": []}, {"text": "For all experiments, prediction accuracy is used as the evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8804661631584167}]}, {"text": "In this subsection, we evaluate our models on these traditional analogy evaluation tests (), in particular the Google and the MSR analogy tests.", "labels": [], "entities": []}, {"text": "words as features, our model achieves better performance than CBOW on the Google analogy set, and close performance on the MSR set.", "labels": [], "entities": [{"text": "Google analogy set", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.7771878043810526}, {"text": "MSR set", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.86589315533638}]}, {"text": "This difference might be caused by the different optimization algorithms.", "labels": [], "entities": []}, {"text": "The global model is close in performance to the local model -this is is expected because the global model is approximated by local model when the density of label is small, and the global model is optimizing fora more stringent goal.", "labels": [], "entities": []}, {"text": "With all features, our models (both local and global) outperform both CBOW and FAST-TEXT by a large margin.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.863919734954834}, {"text": "FAST-TEXT", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.7380375862121582}]}, {"text": "Second, we compare our model (trained on 10% of the data) with FASTTEXT using an open vocabulary).", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.5439267158508301}]}, {"text": "The first row FASTTEXT is trained on 10% of the data.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.7598117589950562}]}, {"text": "Our model significantly outperforms it.", "labels": [], "entities": []}, {"text": "To understand the impact of the extra features, we compare it with FASTTEXT trained on the entire corpus (second row).", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9676795601844788}]}, {"text": "On the MSR set, our 10% model with extra feature still outperforms FASTTEXT.", "labels": [], "entities": [{"text": "MSR set", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9452861249446869}, {"text": "FASTTEXT", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.5611483454704285}]}, {"text": "We believe the underperformance of our 10% model against the 100% FASTTEXT embeddings on the Google data maybe due to the fact that the data contains more complex relations between words, and our feature templates may not be expressive enough.", "labels": [], "entities": [{"text": "Google data", "start_pos": 93, "end_pos": 104, "type": "DATASET", "confidence": 0.8417191505432129}]}], "tableCaptions": [{"text": " Table 1: Extra feature templates for learning word em- beddings. This table uses the word Beijing as an ex- ample. We use gazetteers from the EDISON pack- age (Sammons et al., 2016).", "labels": [], "entities": [{"text": "EDISON pack- age", "start_pos": 143, "end_pos": 159, "type": "DATASET", "confidence": 0.9387453347444534}]}, {"text": " Table 2: Comparison between different models trained  on 10% corpus using a closed vocabulary. The last two  rows use features from Table 1.", "labels": [], "entities": []}, {"text": " Table 4: Misspelling evaluation results. Different degrees mean how many words in the quadruple has been  changed into a misspelling word. FASTTEXT is trained on 10% and 100% corpus while our model trained on  10% corpus with extra features. CBOW can not generate embeddings for OOV words, which means we can not  compare with CBOW on this task.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.7468186020851135}]}, {"text": " Table 5: Dataless task evaluation results. FASTTEXT  and CBOW are trained on 100% corpus while our  model trained on only 10% corpus.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.5568416714668274}, {"text": "CBOW", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8060120940208435}]}]}