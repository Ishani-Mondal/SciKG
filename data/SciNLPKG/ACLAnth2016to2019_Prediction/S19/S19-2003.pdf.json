{"title": [], "abstractContent": [{"text": "This paper presents Unsupervised Lexical Frame Induction, Task 2 of the International Workshop on Semantic Evaluation in 2019.", "labels": [], "entities": [{"text": "Lexical Frame Induction", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7027571201324463}, {"text": "International Workshop on Semantic Evaluation", "start_pos": 72, "end_pos": 117, "type": "TASK", "confidence": 0.5400820016860962}]}, {"text": "Given a set of prespecified syntactic forms in context, the task requires that verbs and their arguments be clustered to resemble semantic frame structures.", "labels": [], "entities": []}, {"text": "Results are useful in identifying polysemous words, i.e., those whose frame structures are not easily distinguished, as well as discerning semantic relations of the arguments.", "labels": [], "entities": []}, {"text": "Evaluation of unsupervised frame induction methods fell into two tracks: Task A) Verb Clustering based on FrameNet 1.7; and B) Argument Clustering, with B.1) based on FrameNet's core frame elements, and B.2) on VerbNet 3.2 semantic roles.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7172299921512604}]}, {"text": "The shared task attracted nine teams, of whom three reported promising results.", "labels": [], "entities": []}, {"text": "This paper describes the task and its data, reports on methods and resources that these systems used, and offers a comparison to human annotation.", "labels": [], "entities": []}, {"text": "Enrique Amig\u00f3, Julio Gonzalo, Javier Artiles, and Felisa Verdejo.", "labels": [], "entities": []}, {"text": "2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints.", "labels": [], "entities": []}, {"text": "Retr., 12(4):461-486.der Panchenko.", "labels": [], "entities": [{"text": "Retr.", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9196212887763977}]}, {"text": "2019. Hm 2 at semeval 2019 task 2: Unsupervised frame induction using contextu-alized and uncontextualized word embeddings.", "labels": [], "entities": [{"text": "Unsupervised frame induction", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6373670697212219}]}], "introductionContent": [{"text": "SemEval 2019 Task 2 focused on the unsupervised semantic labeling of a set of prespecified (semantically) unlabeled structures ().", "labels": [], "entities": [{"text": "semantic labeling of a set of prespecified (semantically) unlabeled structures", "start_pos": 48, "end_pos": 126, "type": "TASK", "confidence": 0.7781207114458084}]}, {"text": "Unsupervised learning methods analyze these structures () to augment them with semantic labels).", "labels": [], "entities": []}, {"text": "The shape of the manually labeled input frames is constrained to an acyclic connected tree of lexical items (words and multi-word units) of maximum depth 1, where just one root governs several arguments.", "labels": [], "entities": []}, {"text": "The task used Berkeley FrameNet (FN) ( and Q., guidelines for this task, to determine the arguments and label them with semantic information.", "labels": [], "entities": []}, {"text": "We compared the proposed system results for unsupervised semantic tagging with that of human annotated (or, gold-standard) data in three different subtasks).", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.6865735352039337}]}, {"text": "To evaluate the systems, we computed distributional similarities between  their generated unsupervised labeled data and human annotated reference data.", "labels": [], "entities": []}, {"text": "For computing similarities we used general purpose numeral methods of text clustering, in particular BCUBED F-SCORE ( as the single figure of merit to rank the systems.", "labels": [], "entities": [{"text": "text clustering", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7429457902908325}, {"text": "BCUBED", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9679714441299438}, {"text": "F-SCORE", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.5265151858329773}]}, {"text": "The most important result of the shared task is the creation of a benchmark fora future complex task.", "labels": [], "entities": []}, {"text": "This benchmark includes a moderately sized, manually annotated set of frames, where only the verbs of each were included, along with their core frame elements (which uniquely define a frame as Ruppenhofer et al. describe).", "labels": [], "entities": []}, {"text": "To complement FN's core frame elements that have highly specific meanings, the benchmark also includes the annotated argument structures of the verbs based on the generic semantic roles proposed for verb classes in VerbNet 3.2 (.", "labels": [], "entities": []}, {"text": "The benchmark comes with simplified annotation guidelines and a modular annotation sys-tem with browsing and editing capabilities.", "labels": [], "entities": []}, {"text": "1 Complementing the benchmarking are several state-ofthe-art competing baselines, from the participants, that serve as a point of departure for improvements in the future.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 contextualizes this task; Section 3 offers a detailed task-description; Section 4 describes the data; Section 5 introduces the evaluation metrics and baselines; Section 6 characterizes the participating systems and unsupervised methods that participants used; Section 7 provides evaluation scores and additional insight about the data; and Section 8 presents concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset consists of manual annotations for verb-headed frame structures anchored in tokenized sentences.", "labels": [], "entities": []}, {"text": "These frame structures were manually annotated using the guidelines for this task (Q..", "labels": [], "entities": []}, {"text": "For example, as already illustrated, the verb come from.v is annotated in terms of FN's ORIGIN frame and its core FEs, as Example 1 shows.", "labels": [], "entities": [{"text": "FN's ORIGIN frame", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.7404839992523193}, {"text": "FEs", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.8117564916610718}]}, {"text": "(1) Criticism of futures COMES FROM Wall Street.", "labels": [], "entities": [{"text": "Criticism of futures COMES", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.874815970659256}, {"text": "Wall Street", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.9154385030269623}]}, {"text": "Shared task participants received a development set consisting of 600 records from a total of 4,620 records, where shows the statistics.", "labels": [], "entities": []}, {"text": "The development set contained gold annotations for all three subtasks.", "labels": [], "entities": []}, {"text": "For all subtasks, as figure of merit, here we report the performance of participating systems with measures for evaluating text clustering techniques, including the classic measures of Purity (PU), inverse-Purity (IPU), and their harmonic mean (PIF) (), as well as the harmonic mean for BCubed precision and recall (i.e., BCP, BCR, and BCF, respectively) (.", "labels": [], "entities": [{"text": "text clustering", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.6784694939851761}, {"text": "Purity", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9768195748329163}, {"text": "inverse-Purity (IPU)", "start_pos": 198, "end_pos": 218, "type": "METRIC", "confidence": 0.9345870763063431}, {"text": "harmonic mean (PIF)", "start_pos": 230, "end_pos": 249, "type": "METRIC", "confidence": 0.7328021764755249}, {"text": "precision", "start_pos": 294, "end_pos": 303, "type": "METRIC", "confidence": 0.7407404184341431}, {"text": "recall", "start_pos": 308, "end_pos": 314, "type": "METRIC", "confidence": 0.9945961833000183}]}, {"text": "To compute these measures for the pairing of reference-labeled data and unsupervised-labeled data (with each having an exact set of annotated items), we built a contingency table T with rows for gold labels and columns for unsupervised system labels.", "labels": [], "entities": []}, {"text": "We filled the table with the number of intersecting items, as done in cross-tabulation of results in classification tasks to compute precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9987956285476685}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9954698085784912}]}, {"text": "For Task A (Section 3), T tracks the unsupervised system labels and the gold reference labels assigned to verbs.", "labels": [], "entities": [{"text": "T", "start_pos": 24, "end_pos": 25, "type": "METRIC", "confidence": 0.9376518130302429}]}, {"text": "For Task B.1, we labeled the rows and columns of T with tuples (l v , la ), where l v labels the frame evoking verb and la labels the FE filler.", "labels": [], "entities": [{"text": "FE", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9634543061256409}]}, {"text": "For Task B.2, the rows and columns in T track the unsupervised system labels and the gold reference labels (generic semantic roles) assigned to arguments.", "labels": [], "entities": []}, {"text": "These performance measures reflect a notion of similarity between the distribution of unsupervised labels and that of the gold reference labels, given certain criteria.", "labels": [], "entities": []}, {"text": "Specifically, they define the notions of consistency and completeness of automatically generated clusters based on the evaluation data.", "labels": [], "entities": [{"text": "consistency", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9848428964614868}]}, {"text": "Each method measures consistency and completeness in its own way, and alone may lack sufficient information fora clear understanding and analysis of system performance.", "labels": [], "entities": [{"text": "consistency", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9840930700302124}]}, {"text": "But, as the single metric for system ranking, we used the BCF measure, given its satisfactory behavior in certain situations.", "labels": [], "entities": [{"text": "BCF measure", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9642735123634338}]}, {"text": "Note that we modeled the task and its evaluation as hard clustering, where a record receives only one label, without overlap in any generated category of items.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotation and Data Statistical Summary", "labels": [], "entities": [{"text": "Data Statistical Summary", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.5154846211274465}]}, {"text": " Table 2: Summary of Results. The BASELINE for  Task A is 1CPH, and for B.1 and B.2 is 1CPHG.  Best results appear in bold face; discarded results  are crossed out. Table 6 lists all other baselines.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995546936988831}]}, {"text": " Table  Table 6 extends Table 2. Section 5 defines the ab- breviations. A horizontal line separates participat- ing systems and the baselines.", "labels": [], "entities": []}, {"text": " Table 5: Frame types with the highest (5a) and  the lowest (5b) confidence (Conf) by number  of records (#Rec) with double annotator agree- ment. #VF reports the number of distinct verb  forms that evoke a frame.", "labels": [], "entities": []}, {"text": " Table 6: Complete System Results and Baselines", "labels": [], "entities": []}]}