{"title": [{"text": "HELP: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning", "labels": [], "entities": [{"text": "HELP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7527329325675964}, {"text": "Monotonicity Reasoning", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.6721616089344025}]}], "abstractContent": [{"text": "Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference (NLI).", "labels": [], "entities": [{"text": "natural language inference (NLI)", "start_pos": 89, "end_pos": 121, "type": "TASK", "confidence": 0.6685386051734289}]}, {"text": "Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so-called monotonicity reasoning.", "labels": [], "entities": [{"text": "phrase replacements", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7257223874330521}]}, {"text": "Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model ar-chitectures themselves.", "labels": [], "entities": [{"text": "monotonicity reasoning", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7413332462310791}]}, {"text": "To investigate this issue , we introduce anew dataset, called HELP, for handling entailments with lexical and logical phenomena.", "labels": [], "entities": [{"text": "HELP", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.7576006054878235}]}, {"text": "We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena.", "labels": [], "entities": []}, {"text": "The results showed that our data augmentation improved the overall accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9986438155174255}]}, {"text": "We also find that the improvement is better on monotonic-ity inferences with lexical replacements than on downward inferences with disjunction and modification.", "labels": [], "entities": []}, {"text": "This suggests that some types of inferences can be improved by our data augmentation while others are immune to it.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding.", "labels": [], "entities": [{"text": "Natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7941503971815109}, {"text": "natural language understanding", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.6691206296284994}]}, {"text": "This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (.", "labels": [], "entities": []}, {"text": "Large crowdsourced datasets such as SNLI) and MultiNLI () have been created from naturally-occurring texts for training and testing neural models on NLI.", "labels": [], "entities": []}, {"text": "Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis sentences.", "labels": [], "entities": [{"text": "prediction of entailment labels", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.8703728914260864}]}, {"text": "Moreover, these standard datasets come with the so-called", "labels": [], "entities": []}], "datasetContent": [{"text": "The resulting dataset has 36K inference pairs consisting of upward monotone, downward monotone, non-monotone, conjunction, and disjunction.", "labels": [], "entities": []}, {"text": "The number of vocabulary items is 15K.", "labels": [], "entities": []}, {"text": "We manually checked the naturalness of randomly sampled 500 sentence pairs, of which 146 pairs were unnatural.", "labels": [], "entities": []}, {"text": "As mentioned in previous work, there are some cases where WordNet for substitution leads to unnatural sentences due to the context mismatch; e.g., an example such as P: You have no driving happening \u21d2 H: You have no driving experience, where P is obtained from H by replacing experience by its hypernym happening.", "labels": [], "entities": []}, {"text": "Since our intention is to explore possible ways to augment training data for monotonicity reasoning, we include these cases in the training dataset.", "labels": [], "entities": []}, {"text": "We use HELP as additional training material for three neural models for NLI and evaluate them on test sets dealing with monotonicity reasoning.", "labels": [], "entities": []}, {"text": "Models We used three models: BERT (Devlin et al., 2019), BiLSTM+ELMo+Attn (, and ESIM ().", "labels": [], "entities": [{"text": "BERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9951233267784119}, {"text": "BiLSTM+ELMo+Attn", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.76415513753891}]}, {"text": "Training data We used three different training sets and compared their performance; MultiNLI (392K), MultiNLI+MQ (the dataset for multiple quantifiers introduced in Section 1; Geiger et al., 2018) (892K), and MultiNLI+HELP (429K).", "labels": [], "entities": [{"text": "HELP", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.8171659708023071}]}, {"text": "Test data We used four test sets: (i) the GLUE diagnostic dataset (  conjunction, and disjunction sections), (ii) FraCaS (the generalized quantifier section), (iii) the SICK () test set, and (iv) MultiNLI matched/mismatched test set.", "labels": [], "entities": [{"text": "GLUE diagnostic dataset", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.7115691304206848}, {"text": "SICK () test set", "start_pos": 169, "end_pos": 185, "type": "DATASET", "confidence": 0.6782716363668442}, {"text": "MultiNLI matched/mismatched test set", "start_pos": 196, "end_pos": 232, "type": "DATASET", "confidence": 0.658374547958374}]}, {"text": "We used the Matthews correlation coefficient (ranging) as the evaluation metric for GLUE.", "labels": [], "entities": [{"text": "Matthews correlation coefficient (ranging)", "start_pos": 12, "end_pos": 54, "type": "METRIC", "confidence": 0.6801080455382665}, {"text": "GLUE", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.7085564136505127}]}, {"text": "Regarding other datasets, we used accuracy as the metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9993498921394348}]}, {"text": "We also check if our data augmentation does not decrease the performance on MultiNLI.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9473344683647156}]}, {"text": "shows that adding HELP to MultiNLI improved the accuracy of all models on GLUE, FraCaS, and SICK.", "labels": [], "entities": [{"text": "HELP", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.965515673160553}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9994907379150391}, {"text": "GLUE", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.8930392861366272}, {"text": "FraCaS", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.9402207136154175}]}, {"text": "Regarding MultiNLI, note that adding data for downward inference can be harmful for performing upward inference, because lexical replacements work in an opposite way in downward environments.", "labels": [], "entities": []}, {"text": "However, our data augmentation minimized the decrease in performance on MultiNLI.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9262959361076355}]}, {"text": "This suggests that models managed to learn the relationships between downward operators and their arguments from HELP.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Evaluation results on the GLUE diagnostic dataset, FraCaS, SICK, and MultiNLI (MNLI). The number  in parentheses is the number of problems in each test set. is the difference from the model trained on MNLI.", "labels": [], "entities": [{"text": "GLUE diagnostic dataset", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.7712790369987488}, {"text": "FraCaS", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.9446030855178833}, {"text": "MNLI", "start_pos": 211, "end_pos": 215, "type": "DATASET", "confidence": 0.8750725388526917}]}]}