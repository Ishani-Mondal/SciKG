{"title": [{"text": "Duluth at SemEval-2019 Task 6: Lexical Approaches to Identify and Categorize Offensive Tweets", "labels": [], "entities": [{"text": "Duluth at SemEval-2019 Task 6", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.8564108610153198}]}], "abstractContent": [{"text": "This paper describes the Duluth systems that participated in SemEval-2019 Task 6, Identifying and Categorizing Offensive Language in Social Media (OffensEval).", "labels": [], "entities": [{"text": "SemEval-2019 Task 6", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8776942690213522}, {"text": "Identifying and Categorizing Offensive Language in Social Media (OffensEval)", "start_pos": 82, "end_pos": 158, "type": "TASK", "confidence": 0.7083885100754824}]}, {"text": "For the most part these systems took traditional Machine Learning approaches that built classifiers from lexical features found in manually labeled training data.", "labels": [], "entities": []}, {"text": "However, our most successful system for classifying a tweet as offensive (or not) was a rule-based blacklist approach, and we also experimented with combining the training data from two different but related SemEval tasks.", "labels": [], "entities": []}, {"text": "Our best systems in each of the three OffensE-val tasks placed in the middle of the comparative evaluation, ranking 57 th of 103 in task A, 39 th of 75 in task B, and 44 th of 65 in task C.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media is notorious for providing a platform for offensive, toxic, and hateful speech.", "labels": [], "entities": []}, {"text": "There is a pressing need for NLP tools that can identify and moderate this type of content.", "labels": [], "entities": []}, {"text": "The OffensEval task () focuses on identifying offensive language in tweets, and determining if specific individuals or groups are being targeted.", "labels": [], "entities": []}, {"text": "Our approach was to rely on traditional Machine Learning methods as implemented by) to build classifiers from manually labeled examples of offensive tweets.", "labels": [], "entities": []}, {"text": "Our models relied on lexical features, including character ngrams, words, and word ngrams.", "labels": [], "entities": []}, {"text": "We also included a dictionary based black-list approach, and experimented with combining training data from two related yet different SemEval-2019 tasks.", "labels": [], "entities": []}, {"text": "Offensive language is an umbrella term that covers hate speech, cyber-bullying, abusive language, profanity, and soon.", "labels": [], "entities": []}, {"text": "Recognizing offensive language is an important first step in dealing with different kinds of problematic text.", "labels": [], "entities": [{"text": "Recognizing offensive language", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9119165937105814}]}, {"text": "Language that is offensive may simply violate community standards regarding the use of profanity, but in other cases may crossover to become abusive, threatening, or dangerous.", "labels": [], "entities": []}, {"text": "Detecting such language has proven to be a challenging problem, at least in part because it remains difficult to make distinctions between the casual and even friendly use of profanity versus more serious forms of offensive language).", "labels": [], "entities": [{"text": "Detecting", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9605691432952881}]}], "datasetContent": [{"text": "OffensEval is made up of three tasks that were carried out in stages during January 2019.", "labels": [], "entities": [{"text": "OffensEval", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.825959324836731}]}, {"text": "Task A is to classify a tweet as being offensive (OFF) or not (NOT).", "labels": [], "entities": [{"text": "OFF", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9638105630874634}]}, {"text": "Task B takes the offensive tweets from task A and decides if they are targeted insults (TIN) or not (UNT).", "labels": [], "entities": [{"text": "TIN", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.96250319480896}, {"text": "UNT", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.6326566338539124}]}, {"text": "Task C looks at the targeted insults from task B and classifies them as being directed against an individual (IND), group (GRP) or other entity (OTH).", "labels": [], "entities": []}, {"text": "Task A provides 13,240 training tweets, of which 8,840 (66.7%) were not offensive (NOT).", "labels": [], "entities": []}, {"text": "Task B is made up of the 4,400 training tweets that were offensive (OFF), where 3,876 (88.1%) of these are targeted insults (TIN).", "labels": [], "entities": [{"text": "OFF", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9866799712181091}]}, {"text": "Task C includes the targeted insults from task B, of which 2,407 (62.1%) were targeted against individuals (IND) and 1,074 (27.7%) were against groups (GRP).", "labels": [], "entities": []}, {"text": "Additional details about the task data can be found in () The distribution of classes in the evaluation data was similar.", "labels": [], "entities": []}, {"text": "Task A has 860 evaluation tweets of which 620 (72%) were not offensive.", "labels": [], "entities": []}, {"text": "Task B includes 240 offensive evaluation tweets, where 213 (89%) were targeted insults.", "labels": [], "entities": []}, {"text": "These made up the evaluation data for task C, where 100 (47%) were against individuals, and 78 (37%) were against groups.", "labels": [], "entities": []}, {"text": "The amount of training data is modest, particularly for tasks B and C.", "labels": [], "entities": []}, {"text": "In addition, the classes in Task B are quite skewed.", "labels": [], "entities": []}, {"text": "Given these factors we decided to rely on traditional Machine Learning techniques, since these have the potential to perform well even with limited training data.", "labels": [], "entities": []}, {"text": "The official rankings in OffensEval were based on macro-averaged F1, and accuracy was also reported.", "labels": [], "entities": [{"text": "OffensEval", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8783256411552429}, {"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9496698975563049}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9997765421867371}]}, {"text": "The results of the Duluth Systems are summarized in.", "labels": [], "entities": [{"text": "Duluth Systems", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9068910479545593}]}, {"text": "X-TOP is the 1 st ranked system in each task.", "labels": [], "entities": []}, {"text": "X-Baseline assigns each test tweet to the most frequent class in the training data.", "labels": [], "entities": []}, {"text": "Next, we'll examine the results from each task in more detail.", "labels": [], "entities": []}, {"text": "In the confusion matrices provided, the distribution of gold answers (ground truth) is shown on the rows, and the system predictions are on the columns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Duluth OffensEval Results", "labels": [], "entities": [{"text": "Duluth OffensEval Results", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.7642494837443033}]}, {"text": " Table 1. X-TOP is the 1 st ranked system  in each task. X-Baseline assigns each test tweet to  the most frequent class in the training data.", "labels": [], "entities": []}, {"text": " Table 2: Task A Confusion Matrices", "labels": [], "entities": []}, {"text": " Table 5: Task A Feature Analysis -A-Sub3", "labels": [], "entities": []}, {"text": " Table 6: Task B Duluth Systems", "labels": [], "entities": []}, {"text": " Table 7: Task C Duluth Systems", "labels": [], "entities": []}]}