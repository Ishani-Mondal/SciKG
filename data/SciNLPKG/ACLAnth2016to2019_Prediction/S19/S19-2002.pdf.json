{"title": [{"text": "HLT@SUDA at SemEval-2019 Task 1: UCCA Graph Parsing as Constituent Tree Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a simple UCCA semantic graph parsing approach.", "labels": [], "entities": [{"text": "UCCA semantic graph parsing", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6663354635238647}]}, {"text": "The key idea is to convert a UCCA semantic graph into a constituent tree, in which extra labels are deliberately designed to mark remote edges and discontinuous nodes for future recovery.", "labels": [], "entities": []}, {"text": "In this way, we can make use of existing syntactic parsing techniques.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7061033248901367}]}, {"text": "Based on the data statistics, we recover discontinuous nodes directly according to the output labels of the constituent parser and use a biaffine classification model to recover the more complex remote edges.", "labels": [], "entities": []}, {"text": "The classification model and the constituent parser are simultaneously trained under the multi-task learning framework.", "labels": [], "entities": []}, {"text": "We use the multilingual BERT as extra features in the open tracks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.965286910533905}]}, {"text": "Our system ranks the first place in the six English/German closed/open tracks among seven participating systems.", "labels": [], "entities": []}, {"text": "For the seventh cross-lingual track, where there is little training data for French, we propose a language embedding approach to utilize English and German training data, and our result ranks the second place.", "labels": [], "entities": []}], "introductionContent": [{"text": "Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework for semantic annotation proposed by. between two non-terminal nodes is represented by the label on the edge.", "labels": [], "entities": [{"text": "Universal Conceptual Cognitive Annotation (UCCA)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7390129779066358}]}, {"text": "One node may have multiple parents, among which one is annotated as the primary parent, marked by solid line edges, and others as remote parents, marked by dashed line edges.", "labels": [], "entities": []}, {"text": "The primary edges form a tree structure, whereas the remote edges enable reentrancy, forming directed acyclic graphs (DAGs).", "labels": [], "entities": []}, {"text": "The second feature of UCCA is the existence of nodes with discontinuous leaves, known as discontinuity.", "labels": [], "entities": [{"text": "discontinuity", "start_pos": 89, "end_pos": 102, "type": "METRIC", "confidence": 0.9511681795120239}]}, {"text": "For example, node 3 in is discontinuous because some terminal nodes it spans are not its descendants.", "labels": [], "entities": []}, {"text": "first propose a transition-based UCCA Parser, which is used as the baseline in the closed tracks of this shared task.", "labels": [], "entities": []}, {"text": "Based on the recent progress on transitionbased parsing techniques, they propose a novel set of transition actions to handle both discontinuous and remote nodes and design useful features based on bidirectional LSTMs.", "labels": [], "entities": [{"text": "transitionbased parsing", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.570033922791481}]}, {"text": "then extend their previous approach and propose to utilize the annotated data with other semantic formalisms such as abstract meaning representation (AMR), universal dependencies (UD), and bilexical Semantic Dependencies (SDP), via multi-task learning, which is used as the baseline in the open tracks.", "labels": [], "entities": [{"text": "abstract meaning representation (AMR)", "start_pos": 117, "end_pos": 154, "type": "TASK", "confidence": 0.7540278434753418}]}, {"text": "In this paper, we present a simple UCCA semantic graph parsing approach by treating UCCA semantic graph parsing as constituent parsing.", "labels": [], "entities": [{"text": "UCCA semantic graph parsing", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.589728981256485}, {"text": "UCCA semantic graph parsing", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.7052456140518188}, {"text": "constituent parsing", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7389869391918182}]}, {"text": "We first convert a UCCA semantic graph into a constituent tree by removing discontinuous and remote phenomena.", "labels": [], "entities": []}, {"text": "Extra labels encodings are deliberately designed to annotate the conversion process and to recover discontinuous and remote structures.", "labels": [], "entities": []}, {"text": "We heuristically recover discontinuous nodes according to the output labels of the constituent parser, since most discontinuous nodes share the same pattern according to the data statistics.", "labels": [], "entities": []}, {"text": "As for the more complex remote edges, we use a biaffine classification model for their recovery.", "labels": [], "entities": []}, {"text": "We directly employ the graph-based constituent parser of and jointly train the parser and the biaffine classification model via multi-task learning.", "labels": [], "entities": []}, {"text": "For the open tracks, we use the publicly available multilingual BERT as extra features.", "labels": [], "entities": [{"text": "BERT", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9085665941238403}]}, {"text": "Our system ranks the first place in the six English/German closed/open tracks among seven participating systems.", "labels": [], "entities": []}, {"text": "For the seventh cross-lingual track, where there is little training data for French, we propose a language embedding approach to utilize English and German training data, and our result ranks the second place.", "labels": [], "entities": []}], "datasetContent": [{"text": "Except BERT, all the data we use, including the linguistic features and word embeddings, are provided by the shared task organizer).", "labels": [], "entities": [{"text": "BERT", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9861446619033813}]}, {"text": "We also adopt the averaged F1 score as the main evaluation metrics returned by the official evaluation scripts (.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.980552613735199}]}, {"text": "We train each model for at most 100 iterations, and early stop training if the peak performance does not increase in 10 consecutive iterations.", "labels": [], "entities": []}, {"text": "shows the results on the dev data.", "labels": [], "entities": []}, {"text": "We have experimented with different settings to gain insights on the contributions of different components.", "labels": [], "entities": []}, {"text": "For the single-language models, it is clear that using pre-trained word embeddings outperforms using randomly initialized word embeddings by more than 1% F1 score on both English and German.", "labels": [], "entities": [{"text": "F1", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9995349645614624}]}, {"text": "Finetuning the pre-trained word embeddings leads to consistent yet slight performance improvement.", "labels": [], "entities": []}, {"text": "In the open tracks, replacing word embedding with the BERT representation is also useful on English (2.8% increase) and German (1.2% increase).", "labels": [], "entities": [{"text": "BERT", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9882621169090271}]}, {"text": "Concatenating pre-trained word embeddings with BERT outputs leads is also beneficial.", "labels": [], "entities": [{"text": "BERT", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9173269271850586}]}, {"text": "For the multilingual models, using randomly initialized word embeddings is better than pretrained word embeddings, which is contradictory to the single-language results.", "labels": [], "entities": []}, {"text": "We suspect this is due to that the pre-trained word embeddings are independently trained for different languages and would lie in different semantic spaces with-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of discontinuous structures  in the English-Wiki data, which is similar in the  German data.", "labels": [], "entities": [{"text": "English-Wiki data", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9038361310958862}, {"text": "German data", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.8102164566516876}]}, {"text": " Table 2: Results on the dev data.", "labels": [], "entities": []}]}