{"title": [{"text": "ConSSED at SemEval-2019 Task 3: Configurable Semantic and Sentiment Emotion Detector", "labels": [], "entities": [{"text": "Configurable Semantic and Sentiment Emotion Detector", "start_pos": 32, "end_pos": 84, "type": "TASK", "confidence": 0.552664781610171}]}], "abstractContent": [{"text": "This paper describes our system participating in the SemEval-2019 Task 3: EmoContext: Contextual Emotion Detection in Text.", "labels": [], "entities": [{"text": "SemEval-2019 Task 3", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8729926745096842}, {"text": "EmoContext: Contextual Emotion Detection in Text", "start_pos": 74, "end_pos": 122, "type": "TASK", "confidence": 0.6271856256893703}]}, {"text": "The goal was to fora given textual dialogue, i.e. a user utterance along with two turns of context, identify the emotion of user utterance as one of the emotion classes: Happy, Sad, Angry or Others.", "labels": [], "entities": []}, {"text": "Our system: ConSSED is a configu-rable combination of semantic and sentiment neural models.", "labels": [], "entities": []}, {"text": "The official task submission achieved a micro-average F1 score of 75.31 which placed us 16th out of 165 participating systems.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9727191627025604}]}], "introductionContent": [{"text": "Emotion detection is crucial in developing a \"smart\" social (chit-chat) dialogue system (.", "labels": [], "entities": [{"text": "Emotion detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9166684150695801}]}, {"text": "Like many sentence classification tasks, classifying emotions requires not only understanding of single sentence, but also capturing contextual information from entire conversations.", "labels": [], "entities": [{"text": "sentence classification tasks", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7997402548789978}, {"text": "classifying emotions", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.9001797735691071}]}, {"text": "For the competition we were invited to create a system for emotion detection of user utterance from short textual dialogue i.e. a user utterance along with two turns of context ().", "labels": [], "entities": [{"text": "emotion detection of user utterance from short textual dialogue", "start_pos": 59, "end_pos": 122, "type": "TASK", "confidence": 0.7244120240211487}]}, {"text": "The number of emotion classes has been limited to four (Happy, Sad, Angry and Others).", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly shows the related work.", "labels": [], "entities": []}, {"text": "Section 3 elaborates on our approach.", "labels": [], "entities": []}, {"text": "It shows preprocessing step and architecture of our system.", "labels": [], "entities": []}, {"text": "Section 4 describes the data set, used word embeddings and hyper-parameters, adopted research methodology and experiments with results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes our work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of our experiments are shown in.", "labels": [], "entities": []}, {"text": "We have divided them into two stages: validation of the baseline systems and our solution.", "labels": [], "entities": [{"text": "validation", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.969992995262146}]}, {"text": "For the first stage, we used the 2-layer bidirectional LSTM model (BiLSTM) with all the word embedding presented in section 4.2 and compared this approach to the baseline model prepared by the organizers (Baseline).", "labels": [], "entities": []}, {"text": "The model using NTUA 310 embedding (73.34) performed best, compared to the Baseline, we have an improvement of about fifteen percent.", "labels": [], "entities": [{"text": "NTUA 310 embedding", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.6932814319928488}]}, {"text": "The second best model was a solution using ELMo embedding (72.42).", "labels": [], "entities": []}, {"text": "From sentiment embeddings the best was Emo2Vec (71.18).", "labels": [], "entities": [{"text": "Emo2Vec", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9351778626441956}]}, {"text": "The second stage was focused on the validation of the ConSSED model.", "labels": [], "entities": [{"text": "ConSSED model", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.7105794548988342}]}, {"text": "In this experiment, we trained six models to verify all possible pairs of semantic embedding-sentiment embedding.", "labels": [], "entities": []}, {"text": "The results show that the use of the ConSSED model allows better results than corresponding baseline systems.", "labels": [], "entities": []}, {"text": "As we could have guessed from the first stage, the best was a combination of NTUA 310 and Emo2Vec (75.31), which was our official solution during the competition.", "labels": [], "entities": [{"text": "NTUA 310", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.8220700323581696}]}, {"text": "In parentheses, we presented the results without the use of Others Class Regularizer.", "labels": [], "entities": []}, {"text": "As we can see, the use of this component improves the results but only slightly.", "labels": [], "entities": []}, {"text": "In addition, after the competition, we have rerun the search for hyper-parameters (this time increasing the number of iterations) for the ConSSED-   NTUA 310-Emo2Vec model, which give us a better result than our official competition result (76.64).", "labels": [], "entities": [{"text": "ConSSED-   NTUA 310-Emo2Vec model", "start_pos": 138, "end_pos": 171, "type": "DATASET", "confidence": 0.8494522213935852}]}, {"text": "Hyper-parameters found for ConSSED-NTUA 310-Emo2Vec models and differences between them are shown in.", "labels": [], "entities": [{"text": "ConSSED-NTUA 310-Emo2Vec", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.8114459216594696}]}], "tableCaptions": [{"text": " Table 1: Data sets statistics.", "labels": [], "entities": []}, {"text": " Table 3: Results of our experiments on the test set. The values without the use of Others Class Regularizer are  shown in parentheses. Bolded model indicate our official solution in the competition. Experiment with an asterisk  was carried out after the end of the competition.", "labels": [], "entities": []}, {"text": " Table 4: Comparison between two ConSSED-NTUA 310-Emo2Vec models: official Competition Model and  Best Model trained after the end of the competition.", "labels": [], "entities": []}]}