{"title": [{"text": "SemEval-2019 Task 1: Cross-lingual Semantic Parsing with UCCA", "labels": [], "entities": [{"text": "SemEval-2019", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9512715339660645}, {"text": "Cross-lingual Semantic Parsing", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6524126827716827}, {"text": "UCCA", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.882862389087677}]}], "abstractContent": [{"text": "We present the SemEval 2019 shared task on Universal Conceptual Cognitive Annotation (UCCA) parsing in English, German and French, and discuss the participating systems and results.", "labels": [], "entities": [{"text": "SemEval 2019 shared task", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.8166935592889786}, {"text": "Universal Conceptual Cognitive Annotation (UCCA) parsing", "start_pos": 43, "end_pos": 99, "type": "TASK", "confidence": 0.7143021747469902}]}, {"text": "UCCA is a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation.", "labels": [], "entities": [{"text": "UCCA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9229226112365723}, {"text": "semantic representation", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.8429988026618958}]}, {"text": "UCCA poses a challenge for existing parsing techniques, as it exhibits reen-trancy (resulting in DAG structures), discon-tinuous structures and non-terminal nodes corresponding to complex semantic units.", "labels": [], "entities": [{"text": "UCCA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8565624952316284}]}, {"text": "The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings.", "labels": [], "entities": []}, {"text": "Full results can be found in the task's website https://competitions.", "labels": [], "entities": []}, {"text": "codalab.org/competitions/19160.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Participants in the task were evaluated in four settings: 1.", "labels": [], "entities": []}, {"text": "English in-domain setting, using the Wiki corpus.", "labels": [], "entities": [{"text": "Wiki corpus", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9486372172832489}]}, {"text": "2. English out-of-domain setting, using the Wiki corpus as training and development data, and 20K Leagues as test data.", "labels": [], "entities": [{"text": "Wiki corpus", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9701959788799286}]}, {"text": "3. German in-domain setting, using the 20K Leagues corpus.", "labels": [], "entities": [{"text": "20K Leagues corpus", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.9230147401491801}]}, {"text": "4. French setting with no training data, using the 20K Leagues as development and test data.", "labels": [], "entities": [{"text": "20K Leagues", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8209584951400757}]}, {"text": "In order to allow both even ground comparison between systems and using hitherto untried resources, we held both an open and a closed track for submissions in the English and German settings.", "labels": [], "entities": []}, {"text": "Closed track submissions were only allowed to use the gold-standard UCCA annotation distributed for the task in the target language, and were limited in their use of additional resources.", "labels": [], "entities": []}, {"text": "Concretely, the only additional data they were allowed to use is that used by TUPA, which consists of automatic annotations provided by spaCy: 10 POS tags, syntactic dependency relations, and named entity types and spans.", "labels": [], "entities": [{"text": "TUPA", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.8788532018661499}]}, {"text": "In addition, the closed track only allowed the use of word embeddings provided by fastText ( for all languages.", "labels": [], "entities": []}, {"text": "Systems in the open track, on the other hand, were allowed to use any additional resource, such as UCCA annotation in other languages, dictionaries or datasets for other tasks, provided that they make sure not to use any additional gold standard annotation over the same text used in the UCCA 10 http://spacy.io 11 http://fasttext.cc corpora.", "labels": [], "entities": [{"text": "UCCA 10 http://spacy.io 11 http://fasttext.cc corpora", "start_pos": 288, "end_pos": 341, "type": "DATASET", "confidence": 0.8673080146312714}]}, {"text": "In both tracks, we required that submitted systems are not trained on the development data.", "labels": [], "entities": []}, {"text": "We only held an open track for French, due to the paucity of training data.", "labels": [], "entities": []}, {"text": "The four settings and two tracks result in a total of 7 competitions.", "labels": [], "entities": []}, {"text": "The following scores an output graph Unlabeled precision, recall and F 1 are the same, but without requiring that 1 = 2 for the edges to match.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.953027606010437}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9996123909950256}, {"text": "F 1", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9924138784408569}]}, {"text": "We evaluate these measures for primary and remote edges separately.", "labels": [], "entities": []}, {"text": "For a more finegrained evaluation, we additionally report precision, recall and F 1 on edges of each category.) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9995916485786438}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9996191263198853}, {"text": "F 1", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9945240616798401}, {"text": "VNU-HCM", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.8959946036338806}]}, {"text": "Some of the teams participated in more than one track and two systems (HLT@SUDA and CUNY-PekingU) participated in all the tracks.", "labels": [], "entities": [{"text": "CUNY-PekingU", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.8693124055862427}]}, {"text": "We are not aware of any such annotation, but include this restriction for completeness.", "labels": [], "entities": []}, {"text": "The official evaluation script providing both coarse-grained and fine-grained scores can be found in https://github.com/huji-nlp/ucca/blob/ master/scripts/evaluate_standard.py.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data splits of the corpora used for the shared task.", "labels": [], "entities": []}, {"text": " Table 3: Statistics of the corpora used for the shared  task.", "labels": [], "entities": []}, {"text": " Table 4: Official F1-scores for each system in each  track. Prim.: primary edges, Rem.: remote edges.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.7193563580513}, {"text": "Prim.", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9816240668296814}, {"text": "Rem.", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.8969728946685791}]}]}