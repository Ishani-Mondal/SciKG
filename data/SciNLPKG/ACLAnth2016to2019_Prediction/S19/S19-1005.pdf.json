{"title": [{"text": "Multi-Label Transfer Learning for Multi-Relational Semantic Similarity", "labels": [], "entities": [{"text": "Multi-Label Transfer Learning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8242731889088949}, {"text": "Similarity", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.653765082359314}]}], "abstractContent": [{"text": "Multi-relational semantic similarity datasets define the semantic relations between two short texts in multiple ways, e.g., similarity, relatedness, and soon.", "labels": [], "entities": []}, {"text": "Yet, all the systems to date designed to capture such relations target one relation at a time.", "labels": [], "entities": []}, {"text": "We propose a multi-label transfer learning approach based on LSTM to make predictions for several relations simultaneously and aggregate the losses to update the parameters.", "labels": [], "entities": []}, {"text": "This multi-label regression approach jointly learns the information provided by the multiple relations, rather than treating them as separate tasks.", "labels": [], "entities": []}, {"text": "Not only does this approach outperform the single-task approach and the traditional multi-task learning approach, it also achieves state-of-the-art performance on all but one relation of the Human Activity Phrase dataset.", "labels": [], "entities": [{"text": "Human Activity Phrase dataset", "start_pos": 191, "end_pos": 220, "type": "DATASET", "confidence": 0.5537315979599953}]}], "introductionContent": [{"text": "Semantic similarity, or relating short texts or sentences 1 in a semantic space -be those phrases, sentences or short paragraphs -is a task that requires systems to determine the degree of equivalence between the underlying semantics of the two sentences.", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.826375275850296}]}, {"text": "Although relatively easy for humans, this task remains one of the most difficult natural language understanding problems.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 81, "end_pos": 111, "type": "TASK", "confidence": 0.7425192793210348}]}, {"text": "The task has been receiving significant interest from the research community.", "labels": [], "entities": []}, {"text": "For instance, from 2012 to 2017, the International Workshop on Semantic Evaluation (SemEval) has been holding the Semantic Textual Similarity (STS) shared tasks (, dedicated to tackling this problem, with close to 100 team submissions each year.", "labels": [], "entities": [{"text": "International Workshop on Semantic Evaluation (SemEval)", "start_pos": 37, "end_pos": 92, "type": "TASK", "confidence": 0.5699780955910683}, {"text": "Semantic Textual Similarity (STS) shared tasks", "start_pos": 114, "end_pos": 160, "type": "TASK", "confidence": 0.7612458057701588}]}, {"text": "In some semantic similarity datasets, an example consists of a sentence pair and a single annotated similarity score, while in others, each pair In this work, we do not consider word level similarity.", "labels": [], "entities": []}, {"text": "We refer to the latter as multi-relational semantic similarity tasks.", "labels": [], "entities": []}, {"text": "The inclusion of multiple annotations per example is motivated by the fact that there can be different relations, namely different types of similarity between two sentences.", "labels": [], "entities": []}, {"text": "So far, these relations have been treated as separate tasks, where a model trains and tests on one relation at a time while ignoring the rest.", "labels": [], "entities": []}, {"text": "However, we hypothesize that each relation may contain useful information about the others, and training on only one relation inevitably neglects some relevant information.", "labels": [], "entities": []}, {"text": "Thus, training jointly on multiple relations may improve performance on one or more relations.", "labels": [], "entities": []}, {"text": "We propose a joint multi-label transfer learning setting based on LSTM, and show that it can bean effective solution for the multi-relational semantic similarity tasks.", "labels": [], "entities": []}, {"text": "Due to the small size of multirelational semantic similarity datasets and the recent success of LSTM-based sentence representations (, the model is pre-trained on a large corpus and transfer learning is applied using fine-tuning.", "labels": [], "entities": [{"text": "LSTM-based sentence representations", "start_pos": 96, "end_pos": 131, "type": "TASK", "confidence": 0.7164470354715983}]}, {"text": "In our setting, the network is jointly trained on multiple relations by outputting multiple predictions (one for each relation) and aggregating the losses during back-propagation.", "labels": [], "entities": []}, {"text": "This is different from the traditional multi-task learning setting where the model makes one prediction at a time, switching between the tasks.", "labels": [], "entities": []}, {"text": "We treat the multi-task setting and the single-task setting (i.e., where a separate model is learned for each relation) as baselines, and show that the multi-label setting outperforms them in many cases, achieving state-of-the-art performance on all but one relation of the Human Activity Phrase dataset).", "labels": [], "entities": [{"text": "Human Activity Phrase dataset", "start_pos": 274, "end_pos": 303, "type": "DATASET", "confidence": 0.5510099828243256}]}, {"text": "In addition to success on multi-relational semantic similarity tasks, the multi-label transfer learning setting that we propose can easily be paired with other neural network architectures and applied to any dataset with multiple annotations available for each training instance.", "labels": [], "entities": []}], "datasetContent": [{"text": "To show the effectiveness of the multi-label transfer learning setting, we experiment on three semantic similarity datasets with multiple relations annotated, and use one LSTM-based sentence encoder that has been very successful in many downstream tasks.", "labels": [], "entities": [{"text": "multi-label transfer learning", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7855790456136068}]}, {"text": "We study three semantic similarity datasets with multiple relations with texts of different lengths, spanning phrases, sentences, and short paragraphs.", "labels": [], "entities": []}, {"text": "Human Activity Phrase (: a collection of pairs of phrases regarding human activities, annotated with the following four different relations.", "labels": [], "entities": []}, {"text": "\u2022 Similarity (SIM): The degree to which the two activity phrases describe the same thing, semantic similarity in a strict sense.", "labels": [], "entities": [{"text": "Similarity (SIM)", "start_pos": 2, "end_pos": 18, "type": "METRIC", "confidence": 0.7106218338012695}]}, {"text": "Example of high similarity phrases: to watch a film and to see a movie.", "labels": [], "entities": []}, {"text": "\u2022 Relatedness (REL): The degree to which the activities are related to one another, a general semantic association between two phrases.", "labels": [], "entities": [{"text": "Relatedness (REL)", "start_pos": 2, "end_pos": 19, "type": "METRIC", "confidence": 0.8807161897420883}]}, {"text": "Example of strongly related phrases: to give a gift and to receive a present.", "labels": [], "entities": []}, {"text": "\u2022 Motivational alignment (MA): The degree to which the activities are (typically) done with similar motivations.", "labels": [], "entities": [{"text": "Motivational alignment (MA)", "start_pos": 2, "end_pos": 29, "type": "TASK", "confidence": 0.8170915603637695}]}, {"text": "Example of phrases with potentially similar motivations: to eat dinner with family members and to visit relatives.", "labels": [], "entities": []}, {"text": "\u2022 Perceived actor congruence (PAC): The degree to which the activities are expected to be done by the same type of person.", "labels": [], "entities": [{"text": "Perceived actor congruence (PAC)", "start_pos": 2, "end_pos": 34, "type": "METRIC", "confidence": 0.7459091941515604}]}, {"text": "An example of a pair with a high PAC score: to pack a suitcase and to travel to another state.", "labels": [], "entities": [{"text": "PAC score", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9682076573371887}]}, {"text": "The phrases are generated, paired and scored on Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.9334825078646342}]}, {"text": "3 https://www.mturk.com/ scores range from 0 to 4 for SIM, REL and MA, and \u22122 to 2 for PAC.", "labels": [], "entities": [{"text": "SIM", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.41334542632102966}, {"text": "REL", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9235968589782715}, {"text": "MA", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.950890302658081}, {"text": "PAC", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.6560060977935791}]}, {"text": "The evaluation is based on the Spearman's \u03c1 correlation coefficient between the systems' predicted scores and the human annotations.", "labels": [], "entities": [{"text": "Spearman's \u03c1 correlation coefficient", "start_pos": 31, "end_pos": 67, "type": "METRIC", "confidence": 0.6530208468437195}]}, {"text": "There are 1,000 pairs in the dataset.", "labels": [], "entities": []}, {"text": "We also use the supplemental 1,373 pairs from in which 1,000 pairs are randomly selected for training and the rest are used for development.", "labels": [], "entities": []}, {"text": "We then treat the original 1,000 pairs as a held-out test set so that our results are directly comparable with those previously reported.", "labels": [], "entities": []}, {"text": "SICK The relatedness score ranges from 1 to 5, and Pearson's r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral.", "labels": [], "entities": [{"text": "SICK", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6010238528251648}, {"text": "Pearson's r", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.8439016540845236}]}, {"text": "There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split.", "labels": [], "entities": []}, {"text": "The sentence pairs are generated from image and video caption datasets before being paired up using some algorithm.", "labels": [], "entities": []}, {"text": "Due to the lack of human supervision in the process, some sentence pairs display minimal difference in semantic components, making the SICK tasks simpler than the others we study.", "labels": [], "entities": [{"text": "SICK tasks", "start_pos": 135, "end_pos": 145, "type": "TASK", "confidence": 0.9136013090610504}]}, {"text": "Typed-Similarity (Agirre et al., 2013b): a collection of meta-data describing books, paintings, films, museum objects and archival records taken from Europeana, 4 presented as the pilot track in the SemEval 2013 STS shared task.", "labels": [], "entities": [{"text": "SemEval 2013 STS shared task", "start_pos": 199, "end_pos": 227, "type": "TASK", "confidence": 0.6767705082893372}]}, {"text": "Typically, the items consist of title, subject, description, and soon, describing a cultural heritage item and, sometimes, a thumbnail of the item itself.", "labels": [], "entities": []}, {"text": "For the purpose of measuring semantic similarity, we concatenate all the textual entries such as title, creator, subject and description into a short paragraph that is used as input, although the annotations might be informed of the image aspects of the meta-data.", "labels": [], "entities": []}, {"text": "Each pair of items is annotated on eight dimensions of similarity: general similarity, author, people involved, time, location, event or action involved, subject and description.", "labels": [], "entities": []}, {"text": "There are 750 pairs in the train split, of which we randomly sample 500 for training and 250 for development, and 721 in the test split.", "labels": [], "entities": []}, {"text": "Pearson's r is used for evaluation.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.72588711977005}]}, {"text": "In each experiment, we use stochastic gradient descent and a batch size of 16.", "labels": [], "entities": []}, {"text": "We tune the learning rate over {0.1, 0.5, 1, 5} and number of epochs over {10, 20}.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9587962031364441}]}, {"text": "For each dataset discussed above, we tune these hyperparameters on the development set.", "labels": [], "entities": []}, {"text": "All other hyperparameters maintain their values from the original code.", "labels": [], "entities": []}, {"text": "5 In the single-task setting, the model is trained and tested on each relation, ignoring the annotations of other relations.", "labels": [], "entities": []}, {"text": "In the multi-task settings, the model is trained and tested on all the relations in a dataset.", "labels": [], "entities": []}, {"text": "In the multitask setting, relations are presented to the model in the order they are listed in the result tables within each batch.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "For every experiment (represented by a cell in the tables), 30 runs with different random seeds are recorded and the average is reported.", "labels": [], "entities": []}, {"text": "For each relation (each column in the tables), let the true mean performance of multi-label learning, singletask baseline and multi-task baseline be \u00b5 MLL , \u00b5 single , \u00b5 MTL , respectively.", "labels": [], "entities": []}, {"text": "Two one-sided Student's t-tests are conducted to test if multi-label learning outperforms the baselines for that relation.", "labels": [], "entities": []}, {"text": "The significance level is chosen to be 0.05.", "labels": [], "entities": [{"text": "significance level", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9823309183120728}]}, {"text": "A down-arrow \u2193 indicates that our proposed multilabel learning underperforms a baseline, while an up-arrow \u2191 indicates that our proposed multi-label learning outperforms a baseline.", "labels": [], "entities": []}, {"text": "https://github.com/facebookresearch/InferSent", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performance in Pearson's r on the Typed-Similarity dataset, in accordance with the specification of  the dataset to allow for direct comparison with previous results. The results of single task and multi-task learning  (MTL) are followed by \u2191 if it is statically significantly lower than those of multi-label learning (MLL), and they  are followed by \u2193 otherwise.", "labels": [], "entities": []}, {"text": " Table 2: The performance in Spearman's \u03c1 on the Hu- man Activity Phrase dataset.", "labels": [], "entities": [{"text": "Hu- man Activity Phrase dataset", "start_pos": 49, "end_pos": 80, "type": "DATASET", "confidence": 0.8545998632907867}]}, {"text": " Table 3: The performance in Pearson's r on the SICK  dataset, in accordance with the specification of the  dataset to allow for direct comparison with previous re- sults.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.7912764946619669}, {"text": "SICK  dataset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8869019448757172}]}]}