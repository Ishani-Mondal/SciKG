{"title": [{"text": "nlpUP at SemEval-2019 Task 6: A Deep Neural Language Model for Offensive Language Detection", "labels": [], "entities": [{"text": "Offensive Language Detection", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.8188020388285319}]}], "abstractContent": [{"text": "This paper presents our submission for the SemEval shared task 6, sub-task A on the identification of offensive language.", "labels": [], "entities": [{"text": "SemEval shared task 6", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8943351805210114}, {"text": "identification of offensive language", "start_pos": 84, "end_pos": 120, "type": "TASK", "confidence": 0.8885572552680969}]}, {"text": "Our proposed model, C-BiGRU, combines a Convolu-tional Neural Network (CNN) with a bidirec-tional Recurrent Neural Network (RNN).", "labels": [], "entities": []}, {"text": "We utilize word2vec to capture the semantic similarities between words.", "labels": [], "entities": []}, {"text": "This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non-offensive tweets.", "labels": [], "entities": []}, {"text": "In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets.", "labels": [], "entities": []}, {"text": "Our model achieved a macro F1-score of 79.40% on the SemEval dataset.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.942611813545227}, {"text": "SemEval dataset", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.8844923675060272}]}], "introductionContent": [{"text": "The ever-increasing amount of user-generated data introduces new challenges in terms of automatic content moderation, especially regarding hate speech and offensive language detection.", "labels": [], "entities": [{"text": "automatic content moderation", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.6464924216270447}, {"text": "offensive language detection", "start_pos": 155, "end_pos": 183, "type": "TASK", "confidence": 0.7871014873186747}]}, {"text": "User content mostly consists of microposts, where the context of a post can be missing or inferred only from current events.", "labels": [], "entities": []}, {"text": "The challenge of automatic identification and detection of online aggressiveness has therefore gained increasing popularity in the scientific community over the last years.", "labels": [], "entities": [{"text": "automatic identification", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.5576375275850296}]}, {"text": "Several recent workshops and conferences such as TRAC (), ALW2, and GermEval ( show the growing importance of this subject.", "labels": [], "entities": [{"text": "TRAC", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.5844579339027405}, {"text": "ALW2", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.7793177962303162}]}, {"text": "The SemEval 2019 shared task 6 () further addresses this topic by introducing the Offensive Language Identification Dataset (OLID), which consists of tweets, labeled with a three-level annotation model (.", "labels": [], "entities": [{"text": "SemEval 2019 shared task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.815396785736084}]}, {"text": "Sub-task A is composed of a binary classification problem of whether a tweet in the dataset is offensive or not.", "labels": [], "entities": []}, {"text": "Sub-task B focuses on different categories of offensive language and the goal of sub-task C is to identify the targeted individual of an offensive tweet.", "labels": [], "entities": []}, {"text": "In the following paper, we present our contribution to sub-task A.", "labels": [], "entities": []}, {"text": "After the related work section, we outline our conducted experiments in section 3 and further describe the used baseline model, as well as the submitted model.", "labels": [], "entities": []}, {"text": "In section 4 we report the results of our experiments on the OLID dataset and the additionally used GermEval dataset.", "labels": [], "entities": [{"text": "OLID dataset", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9558605551719666}, {"text": "GermEval dataset", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.9075620174407959}]}, {"text": "Section 5 discusses our results and section 6 concludes our work and describes possible future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: All results in table form (CV = cross- validation; gold = gold test set).", "labels": [], "entities": []}]}