{"title": [{"text": "Team Taurus at SemEval-2019 Task 9: Expert-informed pattern recognition for suggestion mining", "labels": [], "entities": [{"text": "SemEval-2019 Task 9", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.833928644657135}, {"text": "Expert-informed pattern recognition", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.5792056818803152}]}], "abstractContent": [{"text": "This paper presents our submissions to SemEval-2019 Task9, Suggestion Mining.", "labels": [], "entities": [{"text": "Suggestion Mining", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.845287412405014}]}, {"text": "Our system is one in a series of systems in which we compare an approach using expert-defined rules with a comparable one using machine learning.", "labels": [], "entities": []}, {"text": "We target tasks with a syntactic or semantic component that might be better described by a human understanding the task than by a machine learner only able to count features.", "labels": [], "entities": []}, {"text": "For Semeval-2019 Task 9, the expert rules clearly outperformed our machine learning model when training and testing on equally balanced testsets.", "labels": [], "entities": [{"text": "Semeval-2019 Task 9", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8360473314921061}]}], "introductionContent": [{"text": "In the field of natural language processing, approaches featuring machine learning (ML) nowadays predominate.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6551863551139832}, {"text": "machine learning (ML)", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.6734378278255463}]}, {"text": "These have been shown to be quite effective with a wide range of tasks, including text mining, authorship attribution, and text classification.", "labels": [], "entities": [{"text": "text mining", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.8464689552783966}, {"text": "authorship attribution", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.8415751457214355}, {"text": "text classification", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8392853736877441}]}, {"text": "They are particularly suited for dealing with large data volumes and are robust in the sense that they can handle quite 'noisy' data.", "labels": [], "entities": []}, {"text": "Unlike expert-informed approaches where rules need to be hand-crafted and apply only under predefined conditions, ML approaches learn from training data and are able to extrapolate.", "labels": [], "entities": [{"text": "ML", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9663572311401367}]}, {"text": "Proponents of ML approaches tend to dismiss the enterprise of hand-crafting rules as difficult, errorprone, time-consuming, and generally ineffective as even an extensive set of complex rules is bound to be incomplete and difficult to maintain.", "labels": [], "entities": [{"text": "ML", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9801304340362549}]}, {"text": "Over the past years we have conducted a number of studies directed at the extraction of actionable information from microblogs.", "labels": [], "entities": []}, {"text": "These include a range of topic areas and domains, including the detection of threatening tweets, the identification of potentially contaminated food supplements in forum posts, and topic/event detection in tweets about natural disasters (floods, earthquakes), about traffic flow ( and about outbreaks of the flu.", "labels": [], "entities": [{"text": "topic/event detection in tweets about natural disasters (floods, earthquakes)", "start_pos": 181, "end_pos": 258, "type": "TASK", "confidence": 0.7957122581345695}]}, {"text": "In each case we have been exploring the role of the human expert in an expert-informed pattern recognition approach and a comparable ML approach, seeking out the strengths and weaknesses of either and attempting to arrive at a superior hybrid approach.", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7957754731178284}]}, {"text": "Semeval-2019, Task 9, Suggestion Mining (), appeared to be another task that would lend itself to human rule building.", "labels": [], "entities": [{"text": "Suggestion Mining", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9726604223251343}, {"text": "human rule building", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.598147432009379}]}, {"text": "As suggestions maybe phrased in many different ways, successfully recognising that an utterance contains a suggestion requires human understanding of the context.", "labels": [], "entities": []}, {"text": "Also, the amount of available training data was quite limited so that bringing into play the human expert's knowledge of the forms that suggestions may take would bean advantage, even though the task was not too clearly defined.", "labels": [], "entities": []}, {"text": "As a counterpart to the human rules, we built a machine learning system.", "labels": [], "entities": []}, {"text": "For easier comparison of the patterns suggested by the machine learner and by the human expert, the learning component was a rather simple odds-based technique which still proved competitive in.", "labels": [], "entities": []}, {"text": "As features we used character and token n-grams as well as syntactic patterns.", "labels": [], "entities": []}, {"text": "In addition, the machine learner was somewhat expert-informed, as it was provided with several word lists related to suggestions.", "labels": [], "entities": []}, {"text": "Below we first describe the rule systems built by the human expert (Section 2) and the machine learner (Section 3) in some more detail.", "labels": [], "entities": []}, {"text": "Then we proceed to the quantitative evaluation (Section 4), followed by a qualitative analysis of the evaluation phase (Section 5).", "labels": [], "entities": []}, {"text": "We conclude with a discussion of what we learned in this shared task (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this discussion, we do not want to compare to other systems in the shared task.", "labels": [], "entities": []}, {"text": "For this we refer the reader to the task description paper (, where it can be seen that more intricate machine learning systems, especially those using pre-trained language models, perform much better.", "labels": [], "entities": []}, {"text": "We will rather examine how our internally comparable systems behave on the four item sets.", "labels": [], "entities": []}, {"text": "In order to make the measurements compatible, we first have to make some adjustments.", "labels": [], "entities": []}, {"text": "Both trial sets contained equal numbers of suggestions and non-suggestions, so that precision and recall were equally valuable.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9993658661842346}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.999479353427887}]}, {"text": "In the evaluation sets, there were more non-suggestions than suggestions, so that precision has more influence than recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9992314577102661}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9981094598770142}]}, {"text": "For comparison we needed to recalculate precision (and hence F 1 ; recall is unchanged) by  extrapolating the systems' behaviour to a balanced testset.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9994282126426697}, {"text": "F 1", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9954564869403839}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9900633692741394}]}, {"text": "E.g., the expert rules had 80 false accepts on a total of 146, so a precision of 66/146 i.e. 0.4521.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.99893718957901}]}, {"text": "Ina balanced dataset, 87 instead of 746 non-suggestions, not 80/746, but 9.3298/87 would be falsely accepted.", "labels": [], "entities": []}, {"text": "Precision would be 66/(66 + 9.3298) i.e. 0.8761.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.991746187210083}]}, {"text": "We stress that we are not trying to make out results look more impressive, but merely want to make behaviour on trial and evaluation sets comparable.", "labels": [], "entities": []}, {"text": "shows the evaluation results for our three systems and the organisers' baseline, with the adjusted scores shown in the columns marked EvalBal.", "labels": [], "entities": [{"text": "EvalBal", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.7758241295814514}]}, {"text": "Here, we can see that the enormous drops in quality between TrialA and EvalA were an illusion, caused by the difference in balance.", "labels": [], "entities": [{"text": "TrialA", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9506403207778931}, {"text": "EvalA", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.7451255917549133}]}, {"text": "Also, our three systems are now consistent in their order: expert rules outperform machine learning, but the (very eclectic) combination scores yet a bit better.", "labels": [], "entities": []}, {"text": "In fact, we now see that all systems gain precision when going from TrialA to EvalA, but at the cost of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9993459582328796}, {"text": "TrialA", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.9287521243095398}, {"text": "EvalA", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.715523898601532}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.998931348323822}]}, {"text": "For the machine learner, this is not exclusively due to overtraining in threshold optimization, as we see below in the discussion of Table 2.", "labels": [], "entities": [{"text": "threshold optimization", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7103067338466644}]}, {"text": "A discussion of the results for the expert rules can be found in Section 5.", "labels": [], "entities": []}, {"text": "The lower degree of change for the baseline can probably be explained by the rather general level of the rules there.", "labels": [], "entities": [{"text": "change", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9591550827026367}]}, {"text": "Going from Subtask A to Subtask B, the machine learner suffers a substantial loss in quality, which is understandable as it is trained only on Subtask A data.", "labels": [], "entities": []}, {"text": "Interestingly, there are almost no differences between TrialB and EvalB.", "labels": [], "entities": [{"text": "TrialB", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.9541257619857788}, {"text": "EvalB", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8776088356971741}]}, {"text": "For the expert rules, the situation is rather different.", "labels": [], "entities": []}, {"text": "From TrialA to TrialB, we see a precision gain and recall loss, leading to a slight increase in F 1 ; but the move from TrialB to EvalB leads to a serious drop in both precision and recall.", "labels": [], "entities": [{"text": "precision gain", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9839200973510742}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9980483055114746}, {"text": "F 1", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9946088492870331}, {"text": "EvalB", "start_pos": 130, "end_pos": 135, "type": "DATASET", "confidence": 0.7629212141036987}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9994773268699646}, {"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9958261847496033}]}, {"text": "In, we show the F 1 -score for each individual feature type, as well as for the sum for comparison.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9851135760545731}]}, {"text": "On TrialA, the most useful individual feature types appear to betoken bi-and trigrams and   character n-grams.", "labels": [], "entities": [{"text": "TrialA", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.9336895942687988}]}, {"text": "The other feature types lag behind, but do help reach a higher score with the sum of all feature scores.", "labels": [], "entities": []}, {"text": "Syntactic rewrites by themselves have a much lower F 1 , but this is due to their low recall potential (precision 0.7267, recall 0.3682).", "labels": [], "entities": [{"text": "F 1", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9955600500106812}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9988031387329102}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9917928576469421}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9958978295326233}]}, {"text": "When moving to EvalA, we see that character n-grams and token unigrams maintain their quality, indicating that the same kind of words are being used, but higher n-grams and syntax lose severely, which suggests that EvalA is more different from the training data than TrialA in how words are combined.", "labels": [], "entities": [{"text": "TrialA", "start_pos": 267, "end_pos": 273, "type": "DATASET", "confidence": 0.8388743996620178}]}, {"text": "When we proceed to Subtask B, all feature types lose quality.", "labels": [], "entities": []}, {"text": "As we moved to a different domain, and a different genre, this is not surprising.", "labels": [], "entities": []}, {"text": "Machine learning depends on having training and test data that is as similar as possible.", "labels": [], "entities": [{"text": "Machine learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8391417264938354}]}, {"text": "It is encouraging to see that the syntactic n-grams do manage to perform similarly to Subtask A. This means that machine learning at a more abstract level is able to move to another domain more easily.", "labels": [], "entities": []}, {"text": "If we examine which features are responsible for the recognition, we see that all play some role.", "labels": [], "entities": []}, {"text": "There are, however, some more effective ones.", "labels": [], "entities": []}, {"text": "We show ten of these in.", "labels": [], "entities": []}, {"text": "Note that this is a manual selection, as simply taking the ten highest scoring ones would show only two basic patterns in various guises, e.g. Please as several character n-grams, as token unigram, in a token bigram, and as an adverbial in a syntactic n-gram.", "labels": [], "entities": []}, {"text": "Some of the ten patterns need explanation: CG7 re#shou is part of the token bigram there should.", "labels": [], "entities": []}, {"text": "SRFC S V VP A VBx OD NP A PP means that a sentence is being rewritten as a verb phrase (i.e. a sequence of verb with possibly additional internal adverbials), followed by an adverbial realized by a non-finite verb, then a direct object realized by a noun phrase, and finally an adverbial realized by a prepositional phrase.", "labels": [], "entities": [{"text": "SRFC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8908941149711609}]}, {"text": "If we search for examples, we find e.g. Please allow the access to phone filesystem..", "labels": [], "entities": []}, {"text": "It turns out that Stanford CoreNLP marks Please as a verb, placing allow as head of an \"xcomp\" clause, which confuses our analysis transformer and makes allow an adverbial.", "labels": [], "entities": []}, {"text": "This is clearly wrong but, as it is done consistently, this pattern still provides a good marker.", "labels": [], "entities": []}, {"text": "SCFFCCL VP AVB MD(should) MVB indicates that within a verb phrase, we find both the modal should and a past participle of one of the verbs for something that is advisable.", "labels": [], "entities": [{"text": "SCFFCCL VP AVB MD(should) MVB", "start_pos": 0, "end_pos": 29, "type": "METRIC", "confidence": 0.7609255127608776}]}, {"text": "SCFFCCL S CS AJP(nice) A SBAR(if) represents a sentence with a subject complement nice and an adverbial clause headed by if.", "labels": [], "entities": [{"text": "SCFFCCL S CS AJP", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.5963996201753616}]}, {"text": "And represents a sentence with a main verb with lemma suggest, combined with a period as punctuation, which nicely rules out questions with suggest.", "labels": [], "entities": []}, {"text": "All these patterns have a good precision, but their recall is obviously limited.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.999372661113739}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9995075464248657}]}, {"text": "The first two manage to get about 5 percent on Subtask A, then we quickly drop to 2.5 and 1.", "labels": [], "entities": []}, {"text": "In general, recall is similar for TrialA and EvalA.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9995125532150269}, {"text": "TrialA", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8850008249282837}, {"text": "EvalA", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.8826468586921692}]}, {"text": "However, the strongest markers are absent altogether in Subtask B, where suggestions are apparently worded differently.", "labels": [], "entities": []}, {"text": "Only the final two syntactic n-grams show a significant presence, which is inline with the discussion on feature types above.", "labels": [], "entities": []}, {"text": "Upon inspection of the results obtained on the evaluation set with the expert rules, we specifically looked at the false accepts and false rejects.", "labels": [], "entities": []}, {"text": "Contrary to what we thought might happen, only very few cases were missed out on due to omissions in the lexicon.", "labels": [], "entities": []}, {"text": "With cases that we missed (i.e. where we failed to recognize a suggestion) we did not find any clear clues as to what could be added to the patterns already specified in our rules set.", "labels": [], "entities": []}, {"text": "There were some cases involving imperatives that we missed due to the fact that the punctuation mark we relied upon appeared to be absent, while gleaning the sentence type (interrogative) from the input final punctuation mark failed in cases where the input consisted of two or more sentences.", "labels": [], "entities": []}, {"text": "As for false accepts, we found that several cases were wrongly taken to be suggestions on the basis of a matching imperative pattern.", "labels": [], "entities": []}, {"text": "Here the earlier problem of being unable to distinguish imperative verb forms from infinitives and present tenses no longer presented itself.", "labels": [], "entities": []}, {"text": "Instead, word forms that are ambiguous between noun and verb such as map and phone were mistakenly held to be imperative verbs.", "labels": [], "entities": []}, {"text": "Other false accepts were cases where otherwise highly successful rules proved to be too limited in their scope.", "labels": [], "entities": []}, {"text": "For example, the pattern AUXwould -Vinfin would match would allow but then the sentence actually continues with but so that what initially looks like a suggestion turns out to bean observation, e.g. The control would allow for the use of the contact picker but allows for manual entry and deletion of contacts.", "labels": [], "entities": [{"text": "AUXwould -Vinfin", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.5789479116598765}]}, {"text": "Similar cases involving but were found with other patterns.", "labels": [], "entities": []}, {"text": "In the cases of single instances slight modifications of the rules might have avoided wrongfully identifying something as a suggestion but there is very little evidence to go by, so there is no telling how it would impact on a different dataset.", "labels": [], "entities": []}, {"text": "We also noticed that some patterns occurred (almost) exclusively in Subtask A or B.", "labels": [], "entities": []}, {"text": "Building separate rule sets for the two tasks might hence have been beneficial.", "labels": [], "entities": []}, {"text": "However, the whole point of the exercise in Task 9 was to see whether a rule set can be ported to anew, mostly unknown, domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Quality of submitted systems and organiser baseline.", "labels": [], "entities": []}, {"text": " Table 2: F 1 -score for each test set for each feature type, using model learned from training material for Subtask  A, and oracle thresholds.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9770507514476776}]}, {"text": " Table 3: Ten high-odds features (excluding correlated ones), with effectiveness on all test sets. Each cell contains  observation counts in suggestions/non-suggestions. Hash marks indicate spaces and out-of-sentence positions. The  asterisk (*) means we disagree on one of the non-suggestions, but did use the provided labels.", "labels": [], "entities": []}]}