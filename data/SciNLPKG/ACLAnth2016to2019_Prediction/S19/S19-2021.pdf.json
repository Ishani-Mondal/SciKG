{"title": [{"text": "CAiRE HKUST at SemEval-2019 Task 3: Hierarchical Attention for Dialogue Emotion Classification", "labels": [], "entities": [{"text": "CAiRE HKUST", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.4416845738887787}, {"text": "SemEval-2019 Task", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8232494592666626}, {"text": "Hierarchical Attention for Dialogue Emotion Classification", "start_pos": 36, "end_pos": 94, "type": "TASK", "confidence": 0.6472073495388031}]}], "abstractContent": [{"text": "Detecting emotion from dialogue is a challenge that has not yet been extensively surveyed.", "labels": [], "entities": [{"text": "Detecting emotion from dialogue", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9247834980487823}]}, {"text": "One could consider the emotion of each dialogue turn to be independent , but in this paper, we introduce a hierarchical approach to classify emotion, hypothesizing that the current emotional state depends on previous latent emotions.", "labels": [], "entities": []}, {"text": "We benchmark several feature-based clas-sifiers using pre-trained word and emotion embeddings, state-of-the-art end-to-end neural network models, and Gaussian processes for automatic hyper-parameter search.", "labels": [], "entities": []}, {"text": "In our experiments, hierarchical architectures consistently give significant improvements, and our best model achieves a 76.77% F1-score on the test set.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9979744553565979}]}], "introductionContent": [{"text": "Customer service can be challenging for both the givers and receivers of services, leading to emotions on both sides.", "labels": [], "entities": []}, {"text": "Even human service-people who are trained to deal with such situations struggle to do so, partly because of their own emotions.", "labels": [], "entities": []}, {"text": "Neither do automated systems succeed in such scenarios.", "labels": [], "entities": []}, {"text": "What if we could teach machines how to react under these emotionally stressful situations of dealing with angry customers?", "labels": [], "entities": []}, {"text": "This paper represents work on the SemEval 2019 shared task (, which aims to bring more research on teaching machines to be empathetic, specifically by contextual emotion detection in text.", "labels": [], "entities": [{"text": "SemEval 2019 shared task", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8005150109529495}, {"text": "contextual emotion detection in text", "start_pos": 151, "end_pos": 187, "type": "TASK", "confidence": 0.7311556756496429}]}, {"text": "Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others.", "labels": [], "entities": []}, {"text": "The training dataset contains *Equal contribution.", "labels": [], "entities": [{"text": "training dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.6402422785758972}, {"text": "Equal contribution", "start_pos": 31, "end_pos": 49, "type": "METRIC", "confidence": 0.9749042689800262}]}, {"text": "15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes.", "labels": [], "entities": []}, {"text": "The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion ().", "labels": [], "entities": []}, {"text": "However, these flat models do notwork very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information.", "labels": [], "entities": []}, {"text": "Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (.", "labels": [], "entities": []}, {"text": "We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance's emotion better.", "labels": [], "entities": []}, {"text": "Naturally, the next step is to be able to detect emotion with a hierarchical structure.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this task of extracting emotional knowledge in a hierarchical setting has not yet been extensively explored in the literature.", "labels": [], "entities": []}, {"text": "Therefore, in this paper, we investigate this problem in depth with several strong hierarchical baselines and by using a large variety of pre-trained word embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the evaluation metrics used in the experiment, followed by results on feature-based, end-to-end, and ensemble approaches and Gaussian process search.", "labels": [], "entities": []}, {"text": "The task is evaluated with a micro F1 score for the three emotion classes, i.e., Happy, Sad and Angry, and by taking the harmonic mean of the precision and the recall.", "labels": [], "entities": [{"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9761174321174622}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9992378950119019}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9987364411354065}]}, {"text": "This scoring function has been provided by the challenge organizers (Chatterjee et al., 2019b).", "labels": [], "entities": []}, {"text": "From, we can see that the DeepMoji features outperforms all the other features by a large margin.", "labels": [], "entities": []}, {"text": "Indeed, DeepMoji has been trained using a large emotion corpus, which is compatible with the current task.", "labels": [], "entities": []}, {"text": "Emoji2Vec get a very low F1-score since it includes only emojis, and indeed, by adding GLoVe, a more general embedding, we achieve better performance.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.999336302280426}]}, {"text": "For the end-toend approach, hierarchical biLSTM with GLoVe word embedding achieves the highest score with a 75.64% F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9987236857414246}]}, {"text": "Our ensemble achieves a higher score compared to individual models.", "labels": [], "entities": []}, {"text": "The best ensemble model achieves a 76.77% F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9963155388832092}]}, {"text": "As shown in, the ensemble method is effective to maximize the performance from a bag of models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The table shows F1 score on flat and hier- archical end-to-end models. GP denotes as Gaus- sian process.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9786658585071564}, {"text": "GP", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9391089081764221}]}, {"text": " Table 3: The table shows F1 score on different  ensemble models. XGB denotes XGBoost with  ELMo and DeepMoji features. ALL denotes all  ensemble models.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9816551506519318}]}]}