{"title": [{"text": "SemEval 2019 Task 10: Math Question Answering", "labels": [], "entities": [{"text": "SemEval 2019 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8924626310666403}, {"text": "Math Question Answering", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6466312805811564}]}], "abstractContent": [{"text": "We report on the SemEval 2019 task on math question answering.", "labels": [], "entities": [{"text": "SemEval 2019 task", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8221067984898885}, {"text": "math question answering", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6810850898424784}]}, {"text": "We provided a question set derived from Math SAT practice exams, including 2778 training questions and 1082 test questions.", "labels": [], "entities": [{"text": "Math SAT practice exams", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.7271820828318596}]}, {"text": "For a significant subset of these questions, we also provided SMT-LIB logical form annotations and an interpreter that could solve these logical forms.", "labels": [], "entities": [{"text": "SMT-LIB logical form annotations", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.8560351580381393}]}, {"text": "Systems were evaluated based on the percentage of correctly answered questions.", "labels": [], "entities": []}, {"text": "The top system correctly answered 45% of the test questions, a considerable improvement over the 17% random guessing baseline.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "For each subtask, the main evaluation metric was simply question accuracy, i.e. the number of correctly answered questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8573195934295654}]}, {"text": "We provided a Python script that took as input a list of JSON datum { id: <id>, response: \"<response>\" }, where <id> is the integer index of a question and <response> is the guessed response (either a choice key or a numeric string).", "labels": [], "entities": []}, {"text": "Its output was the number of correct responses divided by the total number of questions in the subtask.", "labels": [], "entities": []}, {"text": "While the main evaluation metric included no penalties for guessing, we also computed a secondary metric that implements the actual evaluation metric used to score these SATs.", "labels": [], "entities": []}, {"text": "This metric is the number of correct questions, minus 1/4 point for each incorrect guess.", "labels": [], "entities": []}, {"text": "We include this metric to challenge participants to investigate highprecision QA systems.", "labels": [], "entities": []}, {"text": "For each subtask, we provided a simple Python baseline that reads in a JSON file containing the evaluation questions, and randomly guesses a choice key for each multiple choice question, and \"0\" for each numeric-answer question.", "labels": [], "entities": []}, {"text": "To train their systems, participants were permitted to use the following public resources: (a) the provided SAT training data and annotations, (b) data collected in MAWPS (, (c) AQuA.", "labels": [], "entities": [{"text": "SAT training data", "start_pos": 108, "end_pos": 125, "type": "DATASET", "confidence": 0.6521533131599426}, {"text": "MAWPS", "start_pos": 165, "end_pos": 170, "type": "DATASET", "confidence": 0.9190303087234497}, {"text": "AQuA", "start_pos": 178, "end_pos": 182, "type": "DATASET", "confidence": 0.5233880281448364}]}, {"text": "Participants were also welcome to use standard public corpora for training word vector representations, language models, etc.", "labels": [], "entities": []}, {"text": "shows the teams (and their affiliations) that submitted systems that beat the baseline in at least one task., 5, and 6 compares the performance of these systems.", "labels": [], "entities": []}, {"text": "The AiFu systems) outperformed the other entries by a wide margin.", "labels": [], "entities": [{"text": "AiFu", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7442112565040588}]}], "tableCaptions": [{"text": " Table 1: Data resources for the three subtasks. Note that the fourth column (\"Total\") also includes a small minority  of questions that do not fall into the three major categories.", "labels": [], "entities": []}, {"text": " Table 3: Results on the overall task (only showing  entries that exceeded baseline performance on at least  one subtask). System rank on each metric is shown in  parentheticals.", "labels": [], "entities": []}, {"text": " Table 4: Results on the closed-vocabulary algebra sub- task (only showing entries that exceeded baseline per- formance on at least one subtask). System rank on each  metric is shown in parentheticals.", "labels": [], "entities": []}, {"text": " Table 5: Results on the open-vocabulary algebra sub- task (only showing entries that exceeded baseline per- formance on at least one subtask). System rank on each  metric is shown in parentheticals.", "labels": [], "entities": []}, {"text": " Table 6: Results on the geometry subtask (only show- ing entries that exceeded baseline performance on at  least one subtask). System rank on each metric is  shown in parentheticals.", "labels": [], "entities": []}]}