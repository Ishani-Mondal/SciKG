{"title": [{"text": "HATEMINER at SemEval-2019 Task 5: Hate speech detection against Immigrants and Women in Twitter using a Multinomial Naive Bayes Classifier", "labels": [], "entities": [{"text": "HATEMINER", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7750371694564819}, {"text": "SemEval-2019 Task", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.829736977815628}, {"text": "Hate speech detection", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8587804039319357}]}], "abstractContent": [{"text": "This paper describes our participation in the SemEval 2019 Task 5-Multilingual Detection of Hate.", "labels": [], "entities": [{"text": "SemEval 2019 Task 5-Multilingual Detection of Hate", "start_pos": 46, "end_pos": 96, "type": "TASK", "confidence": 0.9274068474769592}]}, {"text": "This task aims to identify hate speech against two specific targets, immigrants and women.", "labels": [], "entities": []}, {"text": "We compare and contrast the performance of different word and sentence level embeddings on the state-of-the-art classification algorithms.", "labels": [], "entities": []}, {"text": "Our final submission is a Multinomial binarized Naive Bayes model for both the subtasks in the English version.", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter is a micro-blogging platform where people exchange ideas using short messages called tweets.", "labels": [], "entities": []}, {"text": "Users can propagate their notions, including hatred against an individual or a group, to the entire global population with a latency of a few seconds.", "labels": [], "entities": []}, {"text": "This poses a unique challenge of developing systems that can automatically identify and mitigate hate speech.", "labels": [], "entities": [{"text": "identify and mitigate hate speech", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.7359885454177857}]}, {"text": "Although twitter condemns hate speech through its hateful conduct policy 1 , enforcing it is difficult.", "labels": [], "entities": [{"text": "twitter condemns hate speech", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.6565596014261246}]}, {"text": "There are several reasons for this.", "labels": [], "entities": []}, {"text": "Tweets often contain emoticons, emojis, language slangs, hashtags and other noisy data.", "labels": [], "entities": []}, {"text": "Often, offensive and abusive language maybe erroneously perceived as hate speech and hence it is important to distinguish offensive, abusive and hateful languages ().", "labels": [], "entities": []}, {"text": "These problems are exacerbated by the fact that even humans find it difficult to delineate offensive and hateful language.", "labels": [], "entities": []}, {"text": "Many approaches have been put forward to detect hate speech.", "labels": [], "entities": [{"text": "detect hate speech", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7073343396186829}]}, {"text": "Bag of words and ngram features are effective in hate speech detection as well as the detection of abusive and offensive content (.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7073727250099182}]}, {"text": "com/en/rules-and-policies/ hateful-conduct-policy lexical resources to lookup certain words that contribute significantly to hate speech but such features, when used in isolation may not be very effective.", "labels": [], "entities": []}, {"text": "SVM (, Naive Bayes ( and Logistic Regression () are some of the classifiers used in this domain.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8601800799369812}]}, {"text": "Most of the above methods are targeted to detect general hate speech.", "labels": [], "entities": []}, {"text": "Through this task, we aim to identify hate speech, specifically against immigrants and women.", "labels": [], "entities": [{"text": "identify hate speech", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8191994627316793}]}, {"text": "used lexicon resources to identify misogynistic comments.", "labels": [], "entities": []}, {"text": "used an ensemble of random forest, gradient boosting and logistic regression with bag of words, ngram and lexical features to discern hatred against women.", "labels": [], "entities": []}, {"text": "We did not find significant work in detection of hate speech in English against immigrants.", "labels": [], "entities": [{"text": "detection of hate speech in English", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.8564678430557251}]}], "datasetContent": [{"text": "In this section, we describe the experimental settings used in our research.", "labels": [], "entities": []}, {"text": "All our code is publicly available in a github repository.", "labels": [], "entities": []}, {"text": "The evaluation metrics for subtask A are precision(HS), recall(HS) and F 1 -score(HS).", "labels": [], "entities": [{"text": "precision(HS)", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9385510683059692}, {"text": "recall(HS)", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9671824127435684}, {"text": "F 1 -score(HS)", "start_pos": 71, "end_pos": 85, "type": "METRIC", "confidence": 0.9790520412581307}]}, {"text": "Macro averaged F 1 -score(HS,TR,AG) and Exact Match Ratio (EMR) are the evaluation metrics for subtask-B.", "labels": [], "entities": [{"text": "Macro averaged F 1 -score(HS,TR,AG)", "start_pos": 0, "end_pos": 35, "type": "METRIC", "confidence": 0.7260871628920237}, {"text": "Exact Match Ratio (EMR)", "start_pos": 40, "end_pos": 63, "type": "METRIC", "confidence": 0.9520621399084727}]}, {"text": "Submissions are ranked based on F 1 -score(HS) and EMR for subtask-A and subtask-B, respectively.", "labels": [], "entities": [{"text": "F 1 -score(HS)", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9788178716387067}, {"text": "EMR", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9955971837043762}]}], "tableCaptions": [{"text": " Table 1: Dataset composition. HS: Hate Speech TR:  Target AG:Aggressiveness", "labels": [], "entities": [{"text": "Target AG", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.8153309524059296}]}, {"text": " Table 2: Pretrained word and sentence embeddings results. For each classifier family, the best score is made bold.", "labels": [], "entities": []}, {"text": " Table 3: Multinomial Naive Bayes Classifier results  with word ngram range, stemming and binarization", "labels": [], "entities": [{"text": "Naive Bayes Classifier", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7627565066019694}]}, {"text": " Table 4: Results on the dev set", "labels": [], "entities": []}]}