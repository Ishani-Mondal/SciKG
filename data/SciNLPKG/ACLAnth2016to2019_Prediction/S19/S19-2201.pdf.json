{"title": [{"text": "DOMLIN at SemEval-2019 Task 8: Automated Fact Checking exploiting Ratings in Community Question Answering Forums", "labels": [], "entities": [{"text": "SemEval-2019 Task 8", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.630294660727183}, {"text": "Automated Fact Checking exploiting Ratings in Community Question Answering Forums", "start_pos": 31, "end_pos": 112, "type": "TASK", "confidence": 0.7570503532886506}]}], "abstractContent": [{"text": "In the following, we describe our system developed for the Semeval2019 Task 8.", "labels": [], "entities": [{"text": "Semeval2019 Task 8", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7149174610773722}]}, {"text": "We fine-tuned a BERT checkpoint on the qatar living forum dump and used this checkpoint to train a number of models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9947829842567444}, {"text": "qatar living forum dump", "start_pos": 39, "end_pos": 62, "type": "DATASET", "confidence": 0.7624234706163406}]}, {"text": "Our hand-in for subtask A consists of a fine-tuned classifier from this BERT checkpoint.", "labels": [], "entities": [{"text": "BERT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9837138652801514}]}, {"text": "For subtask B, we first have a classifier deciding whether a comment is factual or non-factual.", "labels": [], "entities": []}, {"text": "If it is factual, we retrieve intra-forum evidence and using this evidence, have a classifier deciding the comment's ve-racity.", "labels": [], "entities": []}, {"text": "We trained this classifier on ratings which we crawled from qatarliving.com.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper contains our system description for the SemEval2019 task 8 about Fact Checking in Community Forums.", "labels": [], "entities": [{"text": "SemEval2019 task 8", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8836855093638102}, {"text": "Fact Checking in Community Forums", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.9378160953521728}]}, {"text": "The task 8 is divided into two subtasks: In subtask A, the goal is to determine whether a question asks fora factual answer, an opinion or is just posed to socialize.", "labels": [], "entities": []}, {"text": "In subtask B, if we have a question asking fora factual answer, we classify the answers to such a question into three categories, namely the answer is either true, false or non-factual, i.e. it does not answer the question in a factual way.", "labels": [], "entities": []}, {"text": "For subtask A, we trained a BERT classifier on the training set and optimized hyper-parameters on the development set.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9974517226219177}]}, {"text": "For subtask B, we decided to tackle the challenge with two binary classifiers: Firstly, we decide whether a comment is factual or not.", "labels": [], "entities": []}, {"text": "If our classifier decides that a comment is factual, we retrieve intra-forum evidence to determine the comment's veracity using a textual entailment approach.", "labels": [], "entities": []}, {"text": "Given the small training set for subtask B, we decided to leverage openly available information on qatarliving.com to create a medium-sized training set.", "labels": [], "entities": []}, {"text": "We found that comments on qatarliving.com are sometimes associated with ratings 1 (ranging from 1 to 5) and discovered that high ratings often correspond to replies answering the question in a true way.", "labels": [], "entities": []}, {"text": "If a comment has recieved a low rating, we inferred that the comment was most likely not helpful to answer the question and therefore we decided to treat it as a false reply.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Effect of cleaning the dataset", "labels": [], "entities": []}, {"text": " Table 2: Accuracy for different features for subtask A", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991000890731812}]}, {"text": " Table 3: Results on training set of subtask B", "labels": [], "entities": []}, {"text": " Table 4: Results of different runs for subtask B on test- set", "labels": [], "entities": [{"text": "test- set", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.7933899164199829}]}]}