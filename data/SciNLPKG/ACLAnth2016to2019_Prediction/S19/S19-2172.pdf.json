{"title": [{"text": "TakeLab at SemEval-2019 Task 4: Hyperpartisan News Detection", "labels": [], "entities": [{"text": "Hyperpartisan News Detection", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.7190080682436625}]}], "abstractContent": [{"text": "In this paper, we demonstrate the system built to solve the SemEval-2019 task 4: Hyperpar-tisan News Detection (Kiesel et al., 2019), the task of automatically determining whether an article is heavily biased towards one side of the political spectrum.", "labels": [], "entities": [{"text": "Hyperpar-tisan News Detection (Kiesel et al., 2019)", "start_pos": 81, "end_pos": 132, "type": "TASK", "confidence": 0.6853146910667419}]}, {"text": "Our system receives an article in its raw, textual form, analyzes it, and predicts with moderate accuracy whether the article is hyperpartisan.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9980029463768005}]}, {"text": "The learning model used was primarily trained on a manually pre-labeled dataset containing news articles.", "labels": [], "entities": []}, {"text": "The system relies on the previously constructed SVM model, available in the Python Scikit-Learn library.", "labels": [], "entities": []}, {"text": "We ranked 6th in the competition of 42 teams with an accuracy of 79.1% (the winning team had 82.2%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9997069239616394}]}], "introductionContent": [{"text": "The ability to quickly, precisely, and efficiently discern if a given article is hyperpartisan can prove to be beneficial in a multitude of different scenarios.", "labels": [], "entities": []}, {"text": "Should we, for example, wish to evaluate if a certain news publisher delivers politically biased content, the best way to do so would be analyzing that very content.", "labels": [], "entities": []}, {"text": "However, the sheer amount of articles modern news companies produce nowadays asks for an automated approach to the problem.", "labels": [], "entities": []}, {"text": "Spotting bias in text is both a well-known and challenging natural language problem.", "labels": [], "entities": [{"text": "Spotting bias", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.895767480134964}]}, {"text": "As bias can manifest itself in a covert or ambiguous manner, it is often hard even for an experienced reader to detect it.", "labels": [], "entities": []}, {"text": "There was some research done on similar issues before (), but none specifically on the subject of hyperpartisan news.", "labels": [], "entities": []}, {"text": "The system described in this paper was built for Task 4 of the SemEval-2019 competition.", "labels": [], "entities": [{"text": "SemEval-2019 competition", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.8155489265918732}]}, {"text": "The goal of the system, asset by the task, is to predict, as accurately as possible, whether a given article is hyperpartisan.", "labels": [], "entities": []}, {"text": "While there were other criteria for evaluating the performance of the model (precision, recall, F1), we decided to optimize the program for the accuracy criterion, as the rankings were based solely on this measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9994990825653076}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9981039762496948}, {"text": "F1", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9990442395210266}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.999177873134613}]}, {"text": "Accuracy, in this context, indicates the ratio of correctly predicted articles to the total number of articles.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9955452680587769}]}, {"text": "The final model reached an accuracy of 79.1%, which presents a decent score, considering the complexity of the problem and the available technology.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996709823608398}]}, {"text": "The system we built is based on the SVM model publicly available in Python's SciKit-Learn library).", "labels": [], "entities": []}, {"text": "Our model was trained on a handful of carefully chosen features derived from the given dataset and our understanding of the nature of bias.", "labels": [], "entities": []}, {"text": "The dataset was split into a high-quality, manually labelled set of articles, and a large, but sub-par set of automatically labelled articles.", "labels": [], "entities": []}, {"text": "By experimenting with the datasets, models, and features, we managed to create a system which ranked 6th in this competition.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset, which was provided by the task organizers, was divided into two separate clusters.", "labels": [], "entities": []}, {"text": "The first and larger cluster consisted of one million news articles labelled solely by the political affiliation of the publisher.", "labels": [], "entities": []}, {"text": "The second and much smaller cluster consisted of one thousand news articles labelled by people who read and evaluated them.", "labels": [], "entities": []}, {"text": "There was a substantial difference in labelling between these two datasets, so the quality and accuracy of the smaller dataset greatly overshadowed the abundance of articles in the larger dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9980849027633667}]}, {"text": "Furthermore, the articles mostly came from large U.S. news publishers, such as: Fox News, CNN, Vox, etc.", "labels": [], "entities": []}, {"text": "This predominance of prominent U.S. news networks and the substantial difference in quality largely impacted our feature design and, ultimately, our model selection.", "labels": [], "entities": []}, {"text": "The task itself was not divided into subtasks, but the submission on these two different datasets was regarded as a different subtask.", "labels": [], "entities": []}, {"text": "The final test set, on which the main leaderboard is made, consisted solely of the articles from the smaller and more accurate dataset.", "labels": [], "entities": []}, {"text": "First, we trained our model with the much larger but subpar by-publisher dataset, and then finetuned it using the much smaller but more precise by-article dataset.", "labels": [], "entities": []}, {"text": "Labels of the by-publisher dataset weren't as precise as the smaller dataset, which is why the accuracy results were lower, in the 58\u221262% range.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9994195699691772}]}, {"text": "We can see that adding a sentiment factor as a feature didn't change the overall accuracy for this dataset, but we should mention that it did increase the smaller dataset's accuracy much more.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.998969554901123}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.997452437877655}]}, {"text": "Adding the number of occurrences of biased publisher names quoted in the article increased the accuracy by about 1.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9979166388511658}]}, {"text": "The largest increase inaccuracy came with adding the date as a feature.", "labels": [], "entities": []}, {"text": "Adding only the month as a feature increased the accuracy by 1.5%, but adding both month and year of article publication saw an additional increase of about 0.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9995854496955872}]}, {"text": "Adding a counter of named entities (in particular: nationalities, religious groups, and political groups) increased the overall accuracy by just under 1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9996052384376526}]}, {"text": "Finally, the model with all of the features explained above was trained on the by-article dataset.", "labels": [], "entities": [{"text": "by-article dataset", "start_pos": 79, "end_pos": 97, "type": "DATASET", "confidence": 0.734972283244133}]}, {"text": "While validating our results, the dataset was divided into five parts.", "labels": [], "entities": []}, {"text": "4 /5 were used for training, and the remaining 1 /5 was used for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.9504466652870178}]}, {"text": "The datasets were permutated, and, in the end, we took the average of the five permutations.", "labels": [], "entities": []}, {"text": "We trained the model using 10 different classifiers.", "labels": [], "entities": []}, {"text": "The validation results are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Validation results for our first SVC model on  the by-publisher dataset, with particular features added  one by one.", "labels": [], "entities": []}, {"text": " Table 2: Validation results for our first SVC model  on the by-article dataset, with particular features added  one by one.", "labels": [], "entities": [{"text": "by-article dataset", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.7243189364671707}]}, {"text": " Table 3: Validation results for the final model on the  by-article dataset. Classifier used for submission is in  boldface.", "labels": [], "entities": [{"text": "by-article dataset", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.7684347033500671}]}, {"text": " Table 4: Final rankings on the main task. Our submis- sion is in boldface.", "labels": [], "entities": []}]}