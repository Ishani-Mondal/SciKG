{"title": [{"text": "SemEval-2019 Task 4: Hyperpartisan News Detection", "labels": [], "entities": [{"text": "SemEval-2019 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8821547627449036}, {"text": "Hyperpartisan News Detection", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.684128095706304}]}], "abstractContent": [{"text": "Hyperpartisan news is news that takes an extreme left-wing or right-wing standpoint.", "labels": [], "entities": []}, {"text": "If one is able to reliably compute this meta information , news articles maybe automatically tagged, this way encouraging or discouraging readers to consume the text.", "labels": [], "entities": []}, {"text": "It is an open question how successfully hyperpartisan news detection can be automated, and the goal of this SemEval task was to shed light on the state of the art.", "labels": [], "entities": [{"text": "hyperpartisan news detection", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6860052545865377}, {"text": "SemEval task", "start_pos": 108, "end_pos": 120, "type": "TASK", "confidence": 0.8758423626422882}]}, {"text": "We developed new resources for this purpose, including a manually labeled dataset with 1,273 articles, and a second dataset with 754,000 articles, labeled via distant supervision.", "labels": [], "entities": []}, {"text": "The interest of the research community in our task exceeded all our expectations: The datasets were downloaded about 1,000 times, 322 teams registered, of which 184 configured a virtual machine on our shared task cloud service TIRA, of which in turn 42 teams submitted a valid run.", "labels": [], "entities": []}, {"text": "The best team achieved an accuracy of 0.822 on a balanced sample (yes : no hyperpartisan) drawn from the manually tagged corpus; an ensemble of the submitted systems increased the accuracy by 0.048.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993098974227905}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9993346333503723}]}], "introductionContent": [{"text": "Yellow journalism has established itself in social media, nowadays often linked to phenomena like clickbait, fake news, and hyperpartisan news.", "labels": [], "entities": [{"text": "Yellow journalism", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6886098384857178}]}, {"text": "Clickbait has been its first \"success story\": When the viral spreading of pieces of information was first observed in social networks, some investigated how to manufacture such events for profit.", "labels": [], "entities": [{"text": "Clickbait", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8909068703651428}]}, {"text": "Unlike for \"natural\" viral content, however, readers had to be directed to a web page containing the to-be-spread information alongside paid-for advertising, so that only teasers and not the information itself could be shared.", "labels": [], "entities": []}, {"text": "Then, to maximize their virality, data-driven optimization revealed that teaser messages which induce curiosity, or any other kind of strong emotion, spread best.", "labels": [], "entities": []}, {"text": "The many forms of such teasers that have emerged since are collectively called clickbait.", "labels": [], "entities": []}, {"text": "New publishing houses arose around viral content, which brought clickbait into the mainstream.", "labels": [], "entities": []}, {"text": "Traditional news publishers, struggling for their share of the attention market that is asocial network, adopted clickbait into their toolbox, too, despite its violation of journalistic codes of ethics.", "labels": [], "entities": []}, {"text": "The content spread using clickbait used to be mostly harmless trivia-entertainment and distraction to some, spam to others-, but in the wake of the 2016 United States presidential election, \"fake news\" came to widespread public attention.", "labels": [], "entities": []}, {"text": "While certainly not anew phenomenon in yellow journalism, its viral success on social media was a surprise to many.", "labels": [], "entities": [{"text": "yellow journalism", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7457263469696045}]}, {"text": "Part of this success was then attributed to so-called hyperpartisan news publishers, which report strongly in favor of one political position and in fierce disagreement with its opponents.", "labels": [], "entities": []}, {"text": "Clinging to hyperpartisanship often entails stretching the truth, if not breaking it with fake news, whose highly emotional content makes them spread exceptionally fast, like clickbait.", "labels": [], "entities": []}, {"text": "Given the hype surrounding fake news, activists, industry, and research are now paying a lot of attention to mitigating the problem, such as trying to check facts in news items.", "labels": [], "entities": []}, {"text": "Clickbait and hyperpartisan news, however, have been less studied.", "labels": [], "entities": [{"text": "Clickbait", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9055628180503845}]}, {"text": "In previous work, we sought to help close this gap from both ends: for clickbait detection, part of our group created a large-scale evaluation dataset) and setup an ongoing competition for the best detection approach ().", "labels": [], "entities": [{"text": "clickbait detection", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7791597545146942}]}, {"text": "For hyperpartisan news detection), we teamed up to follow a similar approach that led to the Hyperpartisan News Detection task at SemEval-2019.", "labels": [], "entities": [{"text": "hyperpartisan news detection", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6387850840886434}, {"text": "Hyperpartisan News Detection task at SemEval-2019", "start_pos": 93, "end_pos": 142, "type": "TASK", "confidence": 0.6897772451241811}]}, {"text": "This paper reports on the results of this task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We gathered a crowdsourced dataset of 1,273 articles, each labeled manually by 3 annotators.", "labels": [], "entities": []}, {"text": "These articles were published by active hyperpartisan and mainstream websites and were all assured to contain political news.", "labels": [], "entities": []}, {"text": "Annotators were asked to rate each article's bias on the following 5-point Likert scale: We removed all articles from the dataset with low agreement score and the aggregated rating of \"not sure\" (see Vincent and Mestre for more details).", "labels": [], "entities": []}, {"text": "We then binarized the labels to hyperpartisan (average rating of 4 or 5) and not (average rating of 1 or 2).", "labels": [], "entities": []}, {"text": "The final by-article set achieved an inter-annotator agreement of 0.5 Krippendorff's alpha.", "labels": [], "entities": []}, {"text": "Of the remaining 1,273 articles, 645 were published as a training dataset, whereas the other 628 (50% hyperpartisan and 50% not) were kept private for the evaluation.", "labels": [], "entities": []}, {"text": "To ensure that classifiers could not profit from overfitting to publisher style, we made sure there was no overlap between the publishers of the articles between these two sets.", "labels": [], "entities": []}, {"text": "To allow for methods that require huge amounts of training data, we compiled a dataset of 754,000 articles, each labeled as per the bias of their respective publisher.", "labels": [], "entities": []}, {"text": "To create this dataset, we cross-checked two publicly available news publisher bias lists compiled by media professionals from BuzzFeed news 2 and Media Bias Fact Check.", "labels": [], "entities": [{"text": "Media Bias Fact Check", "start_pos": 147, "end_pos": 168, "type": "DATASET", "confidence": 0.8223133385181427}]}, {"text": "The former was created by BuzzFeed journalists as a basis fora news article, whereas the latter is Media Bias Fact Check's main product.", "labels": [], "entities": [{"text": "Media Bias Fact Check", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.7680560648441315}]}, {"text": "While both lists contain several hundred news publishers, they disagree only for nine, which we removed from our dataset.", "labels": [], "entities": []}, {"text": "We then crawled, archived, and post-processed the articles available on the publishers' web sites and Facebook feeds.", "labels": [], "entities": []}, {"text": "We archived all articles using a specialized tool () that removes pop-overs and similar things preventing the article content from being loaded.", "labels": [], "entities": []}, {"text": "After filtering out publishers that did not mainly publish political articles or had no political section to which we could restrict our crawl, we were left with 383 publishers.", "labels": [], "entities": []}, {"text": "For each of the publishers' web sites we wrote a content-wrapper to extract the article content and relevant metadata from the HTML DOM.", "labels": [], "entities": []}, {"text": "We then removed all articles that were too short to contain news, 4 that are not written in English, <article id=\"0182515\" published-at=\"2007-01-22\" title=\"They're crumbling\"> <p>What a pleasant surprise to see Jacques Leslie, a journalist and real expert on dams, with along <a href=\"http://www.nytimes.com/2007/01/22/opinion/22leslie.2.html ?ex=1327122000&amp;amp;en=42caf99f05e4cba8&amp;amp;ei=5090&amp;amp;partner= rssuserland&amp;amp;emc=rss\" type=\"external\">op-ed</a> on the hallowed pages of the New York Times.", "labels": [], "entities": [{"text": "the New York Times", "start_pos": 499, "end_pos": 517, "type": "DATASET", "confidence": 0.7511379420757294}]}, {"text": "Leslie, author of <a href=\"\" type=\"internal\">Deep Water: The Epic Struggle Over Dams, Displaced People and the Environment</a>, highlights the threat posed by poorly maintained and increasingly failing dams around the country:</p> <p>Unlike, say, waterways and sanitation plants, a majority of dams -56 percent of those inventoried -are privately owned, which is one reason dams are among the country's most dangerous structures.", "labels": [], "entities": []}, {"text": "Many private owners can't afford to repair aging dams; some owners go so far as to resist paying by tying up official repair demands in court or campaigning to weaken state dam safety laws.</p> <p>Kinda makes you want to find out what is upstream.</p> </article> or that contain obvious encoding errors.", "labels": [], "entities": []}, {"text": "The final dataset consisted of 754,000 articles, split into a public training set (600,000 articles), a public validation set (150,000 articles) and a non-public test set (4,000 articles).", "labels": [], "entities": []}, {"text": "Like for the by-article dataset, we ensured that there is no overlap of publishers between the sets.", "labels": [], "entities": []}, {"text": "Each set consists of 50% articles from non-hyperpartisan publishers and 50% articles from hyperpartisan publishers, the latter again being 50% from left-wing and 50% from right-wing publishers.", "labels": [], "entities": []}, {"text": "The submitted systems can also be distinguished by whether and how they used the large, distantly-supervised by-publisher dataset.", "labels": [], "entities": []}, {"text": "Though much larger than the by-article set, its labels are noisy, whereas the opposite holds for the by-article dataset.", "labels": [], "entities": []}, {"text": "One of the key challenges faced by many teams was how to train a powerful expressive model on the smaller dataset without overfitting.", "labels": [], "entities": []}, {"text": "Most teams made use of the larger dataset in some form or another.", "labels": [], "entities": []}, {"text": "A challenge faced by some of the teams was that the test split of the by-article dataset was balanced between classes, whereas the corresponding training dataset was not.", "labels": [], "entities": []}, {"text": "Several systems trained the whole or part of their system on the by-publisher dataset.", "labels": [], "entities": []}, {"text": "Some extracted features like n-grams (e.g., Sally Smedley), word clusters (Doris Martin), or neural network word embeddings (e.g., Clint Buchanan).", "labels": [], "entities": []}, {"text": "Others used the larger dataset to perform hyperparameter search (e.g., Miles Clarkson).", "labels": [], "entities": [{"text": "hyperparameter search", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.6672753393650055}]}, {"text": "Many teams trained their models using the by-publisher dataset only (Pistachon, Joseph Rouletabille, Xenophilius Lovegood, Peter Brinkmann, and Kit Kittredge).", "labels": [], "entities": []}, {"text": "To reduce the noise in the distantly-supervised data, some teams used only a subset of it.", "labels": [], "entities": []}, {"text": "Yeon Zi, Borat Sagdiyev and the Anhk Morpork Times fitted a model on the by-article dataset and ran it on the by-publisher one: the articles of the by-publisher dataset that were misclassified by this model, were presumed to be noisy and filtered out.", "labels": [], "entities": [{"text": "Anhk Morpork Times", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9556656082471212}]}], "tableCaptions": [{"text": " Table 2: Accuracy, precision, recall, and F 1 -measure  for the by-article meta learning test dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996856451034546}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9995583891868591}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.999544084072113}, {"text": "F 1 -measure", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.987437292933464}, {"text": "by-article meta learning test dataset", "start_pos": 65, "end_pos": 102, "type": "DATASET", "confidence": 0.5529409050941467}]}]}