{"title": [{"text": "An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling", "labels": [], "entities": [{"text": "Syntax-Agnostic Proto-Role Labeling", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.7442997892697653}]}], "abstractContent": [{"text": "Semantic proto-role labeling (SPRL) is an alternative to semantic role labeling (SRL) that moves beyond a categorical definition of roles, following Dowty's feature-based view of proto-roles.", "labels": [], "entities": [{"text": "Semantic proto-role labeling (SPRL)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8080333769321442}, {"text": "semantic role labeling (SRL)", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8208871781826019}]}, {"text": "This theory determines agent-hood vs. patienthood based on a participant's instantiation of more or less typical agent vs. patient properties, such as, for example, volition in an event.", "labels": [], "entities": []}, {"text": "To perform SPRL, we develop an ensemble of hierarchical models with self-attention and concurrently learned predicate-argument-markers.", "labels": [], "entities": [{"text": "SPRL", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.9893520474433899}]}, {"text": "Our method is competitive with the state-of-the art, overall outperforming previous work in two formulations of the task (multi-label and multi-variate Likert scale prediction).", "labels": [], "entities": [{"text": "Likert scale prediction", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.5447110931078593}]}, {"text": "In contrast to previous work, our results do not depend on gold argument heads derived from supplementary gold tree banks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deciding on a linguistically sound, clearly defined and broadly applicable inventory of semantic roles is a long-standing issue in linguistic theory and natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 153, "end_pos": 180, "type": "TASK", "confidence": 0.6420292754968008}]}, {"text": "To alleviate issues found with classical thematic role inventories, argued for replacing categorical roles with a feature-based, composite notion of semantic roles, introducing the theory of semantic proto-roles (SPR).", "labels": [], "entities": []}, {"text": "At its core, it proposes two prominent, composite role types: proto-agent and proto-patient.", "labels": [], "entities": []}, {"text": "Proto-roles represent multi-faceted, possibly graded notions of agenthood or patienthood.", "labels": [], "entities": []}, {"text": "For example, consider the following sentence from Bram Stoker's Dracula (1897): (1) He opened it [the letter] and read it gravely.", "labels": [], "entities": []}, {"text": "argument is considered an agent or patient follows from the proto-typical properties the argument exhibits: e.g., being manipulated is proto-typical for patient, while volition is proto-typical for an agent.", "labels": [], "entities": []}, {"text": "Hence, in both events of (1) the count is determined as agent, and the letter as patient.", "labels": [], "entities": []}, {"text": "Only recently two SPR data sets have been published.", "labels": [], "entities": [{"text": "SPR data sets", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.9093607266743978}]}, {"text": "developed a property-based proto-role annotation schema with 18 properties.", "labels": [], "entities": []}, {"text": "One Amazon Mechanical Turk crowd worker (selected in a pilot annotation) answered questions such as how likely is it that the argument mentioned with the verb changes location?", "labels": [], "entities": []}, {"text": "on a 5-point Likert or responded inapplicable.", "labels": [], "entities": [{"text": "Likert", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9281202554702759}]}, {"text": "This dataset (news domain) will henceforth be denoted by SPR1.", "labels": [], "entities": []}, {"text": "Based on the experiences from the SPR1 annotation process, published SPR2 which follows a similar annotation schema.", "labels": [], "entities": [{"text": "SPR2", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8641440272331238}]}, {"text": "However, in contrast to SPR1, the new data set contains doubly annotated data from the web domain for 14 refined properties.", "labels": [], "entities": []}, {"text": "Our work makes the following contributions: In Section \u00a72, we provide an overview of previous SPRL work and outline a common weakness: reliance on gold syntax trees or gold argument heads derived from them.", "labels": [], "entities": [{"text": "SPRL", "start_pos": 94, "end_pos": 98, "type": "TASK", "confidence": 0.9663839340209961}]}, {"text": "To alleviate this issue, we propose a span-based, hierarchical neural model ( \u00a73) which learns marker embeddings to highlight the predicate-argument structures of events.", "labels": [], "entities": []}, {"text": "show that our model, when combined in a simple voter ensemble, outperforms all previous works.", "labels": [], "entities": []}, {"text": "A single model performs only slightly worse, albeit having weaker dependencies than previous methods.", "labels": [], "entities": []}, {"text": "In our analysis, we (i) perform ablation experiments to analyze the contributions of different model components.", "labels": [], "entities": []}, {"text": "(ii) we observe that the small SPR data size introduces a severe sensitivity to different random initializations of our neural model.", "labels": [], "entities": [{"text": "SPR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9674519896507263}]}, {"text": "We find that combining multiple models in a simple voter ensemble makes SPRL predictions not only slightly better but also significantly more robust.", "labels": [], "entities": [{"text": "SPRL", "start_pos": 72, "end_pos": 76, "type": "TASK", "confidence": 0.9808679223060608}]}, {"text": "We share our code with the community and make it publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We use the same data setup and split as Teichert et al.;;.", "labels": [], "entities": []}, {"text": "For determining the gold labels, we also conform to prior works and (i) collapse classes in the multi-label setup from {N A, 1, 2, 3} and {4, 5} to classes '\u2212' and '+' and (ii) treat N A as 1 in the Likert regression formulation.", "labels": [], "entities": []}, {"text": "For doubly annotated data (SPR2), the Likert scores are averaged; in the multi-label setup we consider values \u2265 4 as '+' and map lesser scores to '\u2212'.", "labels": [], "entities": []}, {"text": "More data and pre-processing details are described in the Supplement \u00a7A.1.", "labels": [], "entities": [{"text": "Supplement \u00a7A.1", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.6841813524564108}]}, {"text": "Baselines As baselines we present the results from previous systems: the state-of-the-art by is denoted in our tables as RUD'18, the linear feature-based classifier by as REI'15 and the CRF developed by as TEI'17.", "labels": [], "entities": [{"text": "RUD'18", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.8041852116584778}, {"text": "REI'15", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.6756220459938049}, {"text": "TEI'17", "start_pos": 206, "end_pos": 212, "type": "DATASET", "confidence": 0.7817146182060242}]}, {"text": "Like previous works, we use macro F1 as the global performance metric in the multi-label scenario and macro-averaged Pearson's \u03c1 (arithmetic mean over the correlation coefficients for each property, details can be found in the Supplement A.1) We refer to the system results as reported by.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.7266164422035217}, {"text": "Pearson's \u03c1", "start_pos": 117, "end_pos": 128, "type": "METRIC", "confidence": 0.9179219603538513}, {"text": "Supplement A.1", "start_pos": 227, "end_pos": 241, "type": "DATASET", "confidence": 0.6882806867361069}]}, {"text": "The most recent work, which evaluates large language models on a variety of tasks including SPRL, is denoted by.", "labels": [], "entities": [{"text": "SPRL", "start_pos": 92, "end_pos": 96, "type": "TASK", "confidence": 0.8849965333938599}]}, {"text": "In this case, we present the micro F1 results as reported in their paper.", "labels": [], "entities": [{"text": "micro F1", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.6339070498943329}]}, {"text": "Model instantiation We introduce four main models: (i) Marker: our basic, span-based singlemodel system.", "labels": [], "entities": [{"text": "Model instantiation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6937841773033142}]}, {"text": "For (ii) MarkerE, we fit an ensemble of 50 Markers with different random seeds.", "labels": [], "entities": []}, {"text": "Computationally, training 50 neural models in this task is completely feasible since neither SPR1 nor SPR2 contain more than 10,000 training examples (parallelized training took approximately 2 hours).", "labels": [], "entities": []}, {"text": "The ensemble predicts unseen testing data by combining the models' decisions in a simple majority vote when performing multi-label prediction or, when in the regression setup, by computing the mean of the output scores (for every property).", "labels": [], "entities": []}, {"text": "We also introduce (iii) MarkerB, and (iv) MarkerEB.", "labels": [], "entities": []}, {"text": "These two systems differ in only one aspect from the previously mentioned models: instead of GloVe word vectors, we feed contextual vectors extracted from the BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.8734821677207947}]}, {"text": "More precisely, we use the transformer model BERT-base-uncased 2 and sum the inferred activations over the last four layers.", "labels": [], "entities": [{"text": "BERT-base-uncased", "start_pos": 45, "end_pos": 62, "type": "METRIC", "confidence": 0.994388997554779}]}, {"text": "The resulting vectors are concatenated to GloVe vectors and then processed by the Bi-LSTM.", "labels": [], "entities": []}, {"text": "We fit all models with gradient descent and apply early stopping on the development data (maximum average Pearson's \u03c1 for multi-variate Likert regression, maximum macro F1 for the multi-label task).", "labels": [], "entities": [{"text": "early stopping", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9703969657421112}, {"text": "Pearson's \u03c1", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9739239811897278}, {"text": "F1", "start_pos": 169, "end_pos": 171, "type": "METRIC", "confidence": 0.8483720421791077}]}, {"text": "Further hyper parameter choices and details about the training are listed in Appendix \u00a7A.2.", "labels": [], "entities": [{"text": "Appendix \u00a7A.2", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.8768508632977804}]}], "tableCaptions": [{"text": " Table 1: SPR1 results. bold: better than all previous work; bold: overall best.", "labels": [], "entities": [{"text": "SPR1", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.40039071440696716}]}, {"text": " Table 2: SPR2 results. bold: better than previous work and/or baselines; bold: overall best.", "labels": [], "entities": [{"text": "SPR2", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.5517083406448364}]}, {"text": " Table 3: Main results, system properties and requirements of SPRL systems. Overall best system is marked in bold,  best system using GloVe is underlined, best single-model system is marked by  \u2020. STL: supervised transfer-learning  (e.g., RUD'18: pre-training on MT task). C-embeddings: contextual word embeddings (BERT-base", "labels": [], "entities": [{"text": "MT task", "start_pos": 263, "end_pos": 270, "type": "TASK", "confidence": 0.884913295507431}, {"text": "BERT-base", "start_pos": 315, "end_pos": 324, "type": "METRIC", "confidence": 0.9913158416748047}]}, {"text": " Table 4: McNemar significance test results of Mark- erEB against MarkerB. Counts of properties for which  a significance category applies (NS: #properties with  insignificant difference).", "labels": [], "entities": [{"text": "Mark- erEB", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.6550779243310293}]}, {"text": " Table 5: Multi-labeling F1 macro scores for different  MarkerEB model configurations over SPR1 and SPR2.", "labels": [], "entities": [{"text": "F1 macro scores", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9072094758351644}]}, {"text": " Table 6: Hyper parameter configuration.", "labels": [], "entities": [{"text": "Hyper parameter configuration", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.573226531346639}]}]}