{"title": [], "abstractContent": [{"text": "Community Question Answering forums are very popular nowadays, as they represent effective means for communities to share information around particular topics.", "labels": [], "entities": [{"text": "Community Question Answering forums", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6474425792694092}]}, {"text": "But the information shared on these forums is often not corrector misleading.", "labels": [], "entities": []}, {"text": "This paper presents the ColumbiaNLP submission for the SemEval-2019 Task 8: Fact-Checking in Community Question Answering Forums.", "labels": [], "entities": [{"text": "ColumbiaNLP", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9555000066757202}, {"text": "SemEval-2019 Task 8", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8518094221750895}, {"text": "Fact-Checking in Community Question Answering Forums", "start_pos": 76, "end_pos": 128, "type": "TASK", "confidence": 0.8191689948240916}]}, {"text": "We show how fine-tuning a language model on a large unan-notated corpus of old threads from Qatar Living forum helps us to classify question types (factual, opinion, socializing) and to judge the factuality of answers on the shared task labeled data from the same forum.", "labels": [], "entities": [{"text": "Qatar Living forum", "start_pos": 92, "end_pos": 110, "type": "DATASET", "confidence": 0.9090050458908081}]}, {"text": "Our system finished 4th and 2nd on Subtask A (question type classification) and B (answer factuality prediction), respectively, based on the official metric of accuracy.", "labels": [], "entities": [{"text": "question type classification", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.7169164220492045}, {"text": "answer factuality prediction)", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.6659955307841301}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9966347813606262}]}], "introductionContent": [{"text": "Community Question Answering (cQA) forums such as StackOverflow, Yahoo!", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7301244139671326}]}, {"text": "Answers, and Quora are very popular nowadays, as they represent effective means for communities to share information and to collectively satisfy their information needs.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.48593029379844666}, {"text": "Quora", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.7641542553901672}]}, {"text": "Questions asked on these sites can be of different types, and the answers can often be false, misleading or irrelevant.", "labels": [], "entities": []}, {"text": "SemEval-2019 Task 8 is structured around two subtasks.", "labels": [], "entities": [{"text": "SemEval-2019 Task 8", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.6621666749318441}]}, {"text": "Subtask A is a question classification task, where the questions types are: \u2022 Factual: The question is asking for factual information, which can be answered by checking various information sources, and it is not ambiguous (e.g., \"What is Ooredoo customer service number?\").", "labels": [], "entities": [{"text": "question classification task", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.788959930340449}, {"text": "Factual", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9379713535308838}]}, {"text": "\u2022 Opinion: The question asks for an opinion or an advice, not fora fact.", "labels": [], "entities": []}, {"text": "(e.g., \"Can anyone recommend a good Vet in Doha?\"\") \u2022 Socializing: Not areal question, but intended for socializing or for chatting.", "labels": [], "entities": []}, {"text": "This can also mean expressing an opinion or sharing some information, without really asking anything of general interest (e.g., \"What was your first car?\")", "labels": [], "entities": [{"text": "expressing an opinion or sharing some information, without really asking anything of general interest (e.g., \"What was your first car?\")", "start_pos": 19, "end_pos": 155, "type": "Description", "confidence": 0.6120703300604453}]}, {"text": "Subtask B is an answer classification task: are the answers to factual questions factual or not, and if they are factual are they true or false: \u2022 Factual -TRUE: The answer is True and can be proven with an external resource.", "labels": [], "entities": [{"text": "answer classification task", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.7857241133848826}, {"text": "Factual -TRUE", "start_pos": 147, "end_pos": 160, "type": "METRIC", "confidence": 0.8864121238390604}]}, {"text": "(Q: \"I wanted to know if there were any specific shots and vaccinations I should get before coming over [to Doha].\"; A: \"Yes there are; though it varies depending on which country you come from.", "labels": [], "entities": []}, {"text": "In the UK; the doctor has a list of all countries and the vaccinations needed for each.\").", "labels": [], "entities": []}, {"text": "\u2022 Factual -FALSE: The answer gives a factual response, but it is False, it is partially false or the responder is unsure about (Q:\"Can I bring my pitbulls to Qatar?\"; A: \"Yes you can bring it but be careful this kind of dog is very dangerous.\").", "labels": [], "entities": [{"text": "Factual -FALSE", "start_pos": 2, "end_pos": 16, "type": "METRIC", "confidence": 0.8878920276959738}, {"text": "False", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9958409667015076}]}, {"text": "\u2022 Non-Factual: When the answer does not provide factual information to the question; it can bean opinion or an advice that cannot be verified (e.g., \"It's better to buy anew one.\").", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Size of Subtask A dataset (question types).", "labels": [], "entities": []}, {"text": " Table 2: Size of Subtask B dataset (answer types).", "labels": [], "entities": []}, {"text": " Table 5: Average Cosine Similarity scores of contextual representations of each answer to every  other answer in the thread", "labels": [], "entities": [{"text": "Average Cosine Similarity scores", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.8158949762582779}]}, {"text": " Table 6: Ablation scores for Subtask B on the final test  set for Task 8.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992544054985046}]}]}