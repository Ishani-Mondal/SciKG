{"title": [{"text": "SolomonLab at SemEval-2019 Task 8: Question Factuality and Answer Veracity Prediction in Community Forums", "labels": [], "entities": [{"text": "SolomonLab at SemEval-2019 Task 8", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.8306020736694336}, {"text": "Answer Veracity Prediction", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.8142689863840739}]}], "abstractContent": [{"text": "We describe our system for SemEval-2019, Task 8 on \"Fact-Checking in Community Question Answering Forums (cQA)\".", "labels": [], "entities": [{"text": "SemEval-2019", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.9262521266937256}, {"text": "Fact-Checking in Community Question Answering Forums (cQA)\"", "start_pos": 52, "end_pos": 111, "type": "TASK", "confidence": 0.704957558049096}]}, {"text": "cQA forums are very prevalent nowadays, as they provide an effective means for communities to share knowledge.", "labels": [], "entities": [{"text": "cQA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9092456698417664}]}, {"text": "Unfortunately, this shared information is not always factual and fact-verified.", "labels": [], "entities": []}, {"text": "In this task, we aim to identify factual questions posted on cQA and verify the veracity of answers to these questions.", "labels": [], "entities": [{"text": "cQA", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.7047523856163025}]}, {"text": "Our approach relies on data augmentation and aggregates cues from several dimensions such as semantics, linguistics, syntax, writing style and evidence obtained from trusted external sources.", "labels": [], "entities": []}, {"text": "In subtask A, our submission is ranked 3rd, with an accuracy of 83.14%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9996615648269653}]}, {"text": "Our current best solution stands 1st on the leader-board with 88% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9992632269859314}]}, {"text": "In subtask B, our present solution is ranked 2nd, with 58.33% MAP score.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9760780036449432}]}], "introductionContent": [{"text": "With the rising popularity of online community question answering (cQA) systems such as Quora, StackOverflow, and Qatar Living forum (QLF), the amount of information shared over these platforms is also increasing rapidly with time.", "labels": [], "entities": [{"text": "question answering (cQA)", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.837528133392334}, {"text": "Qatar Living forum (QLF)", "start_pos": 114, "end_pos": 138, "type": "DATASET", "confidence": 0.9210647145907084}]}, {"text": "These forums provide effective information sharing mechanism to their users who can seek answers to their queries as well as post answers to the questions.", "labels": [], "entities": []}, {"text": "However, the information shared on such platforms may not always be factual and correct.", "labels": [], "entities": []}, {"text": "The responders may misunderstand the question being asked or merely ignore certain specific details.", "labels": [], "entities": []}, {"text": "At times, the information shared may even be false or ambiguous in the desired context.", "labels": [], "entities": []}, {"text": "This is aggravated by lack of moderation and systematic control on cQA forums.", "labels": [], "entities": [{"text": "cQA forums", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.9355145990848541}]}, {"text": "The Semeval-2019 Task 8 1 on \"Fact Checking in Community Ques- * * Equal Contribution 1 http://alt.qcri.org/semeval2019/index.php?id=tasks tion Answering Forums\" aims to solve this reallife problem.", "labels": [], "entities": [{"text": "Fact Checking in Community Ques", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8850706338882446}]}, {"text": "The above task tries to explore the veracity of an answers to a question posted on QLF.", "labels": [], "entities": [{"text": "QLF", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.8805579543113708}]}, {"text": "While the precedent tasks such as SemEval (, address the issue of ranking answers according to their relevance to a question, the task-at-hand is the first one to consider the correctness of answers.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.889326810836792}]}, {"text": "This task is formulated as a two-stage problem.", "labels": [], "entities": []}, {"text": "The first stage aims to identify the user posts asking for factual information.", "labels": [], "entities": []}, {"text": "The answers to the identified factual questions are then fact-verified in the second stage.", "labels": [], "entities": []}, {"text": "Both the subtasks are designed as 3-class supervised classification problems.", "labels": [], "entities": []}, {"text": "More specifically, the first stage or subtask A addresses the problem of determining whether the posted question asks for factual information, an opinion/advice or is just meant for socializing.", "labels": [], "entities": []}, {"text": "For example, \"what is Ooredoo customer service number?\" asks for factual information, whereas \"What was your first car?\" is socializing and \"which is the best bank around?\" is seeking guidance/opinion.", "labels": [], "entities": []}, {"text": "Each data sample in subtask A is a question posted by a user consisting of a subject, body and meta information (user ID, username, and the category of question, e.g., \"Education,\" \"Visa and Permits\", \"Welcome to Qatar\" etc.).", "labels": [], "entities": []}, {"text": "The second stage or subtask B focuses on determining whether an answer to a factual question is true, false or does not constitute a proper answer, in which case, it is labeled as non-factual.", "labels": [], "entities": []}, {"text": "For example, to the question \"Can I bring my pitbulls to Qatar?\", Answer A1: \"Yes, you can bring it but be careful this kind of dog is very dangerous\" is factual-false 2 , Answer A2: \"No, you cannot as they are banned\" is factual-true 2 and Answer A3: \"There goes another job opportunity for the sake of two lovely animals.", "labels": [], "entities": []}, {"text": "The data is organized as a question-answer tuple: question posted by a user and an answer (body, username and answer ID) posted by the same or another user.", "labels": [], "entities": []}, {"text": "It has been ensured that all the questions in this task are factual questions.", "labels": [], "entities": []}, {"text": "Our approach to solving this task is based on extracting rich-feature representation from the input and training a classifier to make predictions.", "labels": [], "entities": []}, {"text": "The feature representation integrates knowledge from various complementary sources, such as the question/answer content, the content of other answers in the thread, evidence from trustworthy external sources of information, and the relevance of an answer to the question.", "labels": [], "entities": []}, {"text": "For subtask A, we rely on question content (semantic, linguistic and syntactic cues), whereas the evidence from external sources and answer relevancy to the question are essential aspects for subtask B.", "labels": [], "entities": []}, {"text": "For both the subtasks, we also leverage a data augmentation approach which facilitates the generalization ability of learned classifier on unseen test data as well as ameliorates the class imbalance issues present in the training data.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 gives an overview of our system.", "labels": [], "entities": []}, {"text": "Section 3-5 describe the details of our approach.", "labels": [], "entities": []}, {"text": "Section 6 demonstrates the experimental results.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now utilize all features as portrayed in for subtask A and for subtask B.", "labels": [], "entities": []}, {"text": "We train two separate SVM classifiers (Burges, 1998) on respective features for 3-class classification for both the subtasks.", "labels": [], "entities": []}, {"text": "We use 10-fold cross validation for hyper-parameter tuning of SVM based on which, we choose \"linear\" kernel with C=0.5 (regularization parameter) for all the demonstrated experiments.", "labels": [], "entities": []}, {"text": "All the results are reported on the test data with accuracy, recall, and F1 measure as evaluation metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9996384382247925}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.99933260679245}, {"text": "F1 measure", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9844521284103394}]}, {"text": "Additionally, we calculate Mean Average Precision (MAP) for subtask B, where the 'True' instances are considered relevant examples (in the context of Information Retrieval).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 27, "end_pos": 55, "type": "METRIC", "confidence": 0.9776433606942495}, {"text": "Information Retrieval", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.7240306437015533}]}, {"text": "MAP measures the capability of the system to predict 'True' instances with higher confidence.", "labels": [], "entities": []}, {"text": "shows the performance of the proposed system (PS).", "labels": [], "entities": []}, {"text": "From the results, we can observe that our PS (excluding syntactic features) achieves an impressive performance with accuracy of 84.12% and 72.17% F1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9996180534362793}, {"text": "F1", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.9997163414955139}]}, {"text": "Our submission (with all the features in PS + (POS and QType)) achieved similar performance and ranked 3rd on the leaderboard (83% acc., 71% F1) with only a marginal difference with respect to the first (84% acc., 72% F1 ) and second-ranked (83% acc., 72% F1 ) systems.", "labels": [], "entities": [{"text": "F1", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9936445951461792}, {"text": "F1", "start_pos": 218, "end_pos": 220, "type": "METRIC", "confidence": 0.9857450723648071}, {"text": "F1", "start_pos": 256, "end_pos": 258, "type": "METRIC", "confidence": 0.9802377223968506}]}, {"text": "To push our system's performance even further, we experimented in the post-evaluation phase and achieved 88.10% accuracy and 77.37% F1, highest on the post-evaluation leaderboard . This current best solution leverages QType features, extensive data augmentation using bagging technique and excludes writing style features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9975866079330444}, {"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9994853734970093}]}], "tableCaptions": [{"text": " Table 2: Writing Style Based Feature Distribution across", "labels": [], "entities": [{"text": "Writing Style Based Feature Distribution", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.7760385274887085}]}, {"text": " Table 4: Experimental Results. Subtask A (Left) and Subtask B(Right)", "labels": [], "entities": []}]}