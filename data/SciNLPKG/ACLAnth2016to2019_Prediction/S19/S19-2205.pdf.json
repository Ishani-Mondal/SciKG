{"title": [{"text": "TMLab SRPOL at SemEval-2019 Task 8: Fact Checking in Community Question Answering Forums", "labels": [], "entities": [{"text": "TMLab SRPOL at SemEval-2019 Task 8", "start_pos": 0, "end_pos": 34, "type": "DATASET", "confidence": 0.6787457366784414}, {"text": "Fact Checking in Community Question Answering Forums", "start_pos": 36, "end_pos": 88, "type": "TASK", "confidence": 0.9072082042694092}]}], "abstractContent": [{"text": "The article describes our submission to SemE-val 2019 Task 8 on Fact-Checking in Community Forums.", "labels": [], "entities": [{"text": "SemE-val 2019 Task 8", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7571489661931992}, {"text": "Fact-Checking in Community Forums", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.768313467502594}]}, {"text": "The systems under discussion participated in Subtask A: decide whether a question asks for factual information, opinion/advice or is just socializing.", "labels": [], "entities": []}, {"text": "Our primary submission was ranked as the second one among all participants in the official evaluation phase.", "labels": [], "entities": []}, {"text": "The article presents our primary solution: Deeply Regularized Residual Neural Network (DRR NN) with Universal Sentence En-coder embeddings.", "labels": [], "entities": []}, {"text": "This is followed by a description of two contrastive solutions based on ensemble methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community question answering forums are good platforms for knowledge sharing; hence, they are widely used sources of information.", "labels": [], "entities": [{"text": "Community question answering forums", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6781634241342545}]}, {"text": "The growing popularity of such knowledge exchange leads to a growing need to automate the process of verifying the post quality.", "labels": [], "entities": []}, {"text": "The first step, often overlooked, is to categorize each question and establish what kind of information the user seeks.", "labels": [], "entities": []}, {"text": "Question classification has been mainly used to support question answering systems.", "labels": [], "entities": [{"text": "Question classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8024488985538483}, {"text": "question answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8959640860557556}]}, {"text": "Two main method types have been proposed in the literature: (1) rule-based approaches with linguistic features), and (2) machine learning approaches ().", "labels": [], "entities": []}, {"text": "These methods are rather simple, due to the fact that question classification is often just a preprocessing step in a larger task.", "labels": [], "entities": [{"text": "question classification", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.8388468027114868}]}, {"text": "However, we can observe some recent advances in this area, such as ULMFiT (, which achieves state-of-the-art performance on the TREC dataset ().", "labels": [], "entities": [{"text": "ULMFiT", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.6617866158485413}, {"text": "TREC dataset", "start_pos": 128, "end_pos": 140, "type": "DATASET", "confidence": 0.8901928067207336}]}, {"text": "The present article describes our systems submitted to the SemEval 2019 competition Task 8 subtask A on question classification.", "labels": [], "entities": [{"text": "SemEval 2019 competition Task 8 subtask", "start_pos": 59, "end_pos": 98, "type": "TASK", "confidence": 0.8839357992013296}, {"text": "question classification", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.7397769391536713}]}, {"text": "The competition data set consisted of QatarLiving forum questions classified as FACTUAL, OPINION or SOCIALIZING.", "labels": [], "entities": [{"text": "competition data set", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.823054850101471}, {"text": "QatarLiving forum questions", "start_pos": 38, "end_pos": 65, "type": "DATASET", "confidence": 0.8479784727096558}, {"text": "FACTUAL", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.7350286841392517}, {"text": "OPINION", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9285321235656738}]}, {"text": "The training data contained only 1,118 questions.", "labels": [], "entities": []}, {"text": "Moreover, according to our evaluation, human-level accuracy on this data set was about 0.75, which was relatively low.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9956517815589905}]}, {"text": "Therefore, we approached the task as a challenging classification problem.", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents our experiments with preprocessing methods.", "labels": [], "entities": []}, {"text": "Section 3 describes our official submission, where we propose an architecture utilizing several regularization methods to address the problem of the small data set.", "labels": [], "entities": []}, {"text": "For comparative purposes, section 4 presents two ensemble models as contrastive examples.", "labels": [], "entities": []}, {"text": "Section 5 provides the results achieved by the models.", "labels": [], "entities": []}, {"text": "Lastly, section 6 concludes the discussion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Official results of our submissions on the test  set.", "labels": [], "entities": []}]}