{"title": [{"text": "SINAI-DL at SemEval-2019 Task 5: Recurrent networks and data augmentation by paraphrasing", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.580973207950592}]}], "abstractContent": [{"text": "This paper describes the participation of the SINAI-DL team at Task 5 in SemEval 2019, called HatEval.", "labels": [], "entities": [{"text": "SemEval 2019", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.8746009171009064}]}, {"text": "We have applied some classic neural network layers, like word embeddings and LSTM, to build a neural classifier for both proposed tasks.", "labels": [], "entities": []}, {"text": "Due to the small amount of training data provided compared to what is expected for an adequate learning stage in deep architectures, we explore the use of paraphrasing tools as source for data augmentation.", "labels": [], "entities": []}, {"text": "Our results show that this method is promising, as some improvement has been found over non-augmented training sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "We have participated in, which encourage participants to identify hate speech in tweets.", "labels": [], "entities": []}, {"text": "The small amount of training data provided makes difficult to train a deep architecture, so strategies like transfer learning and data augmentation are explored in our work.", "labels": [], "entities": []}, {"text": "A trained model for word embeddings in the two languages targeted by the tasks have been considered as transfer learning approach.", "labels": [], "entities": []}, {"text": "Paraphrasing the tweets has also been tested for data augmentation, doubling the number of tweets available for training the network.", "labels": [], "entities": []}, {"text": "Our results are promising for English, but no improvements have been found for Spanish.", "labels": [], "entities": []}, {"text": "Further analysis on the results and the quality of the paraphrasing tools used is needed, but the scores obtained in English encourage us to consider paraphrasing as a promising help in deep learning for natural language processing.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 introduces the two main strategies used to train the neural network: data augmentation and transfer learning.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.7727189362049103}, {"text": "transfer learning", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.877470850944519}]}, {"text": "In Section 3, task data is analyzed in order to define hyperparameters values.", "labels": [], "entities": []}, {"text": "Section 4 describes the neural network architecture applied.", "labels": [], "entities": []}, {"text": "Section 6 gives more details on the paraphrasing approach used to generate more training data.", "labels": [], "entities": []}, {"text": "Experiments and results are given in Section 7.", "labels": [], "entities": [{"text": "Section 7", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.846058577299118}]}, {"text": "Finally, Section 8 closes the contribution with some conclusions and proposals for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have performed several experiments to find good hyperparameters, but also evaluated the two main strategies proposed in our approach: transfer learning and data augmentation.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.9086301922798157}, {"text": "data augmentation", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.7633036673069}]}, {"text": "In order to verify how transfer learning is good enough, i.e. how the predefined weights for GloVe could be further adjusted or not, we have checked the performance of the model trained on the official training set and evaluated on the development dataset on.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8995394706726074}]}, {"text": "Results in shows that a small but consistent improvement is obtained if weights can be readjusted.", "labels": [], "entities": []}, {"text": "Next, we have empirically evaluated how paraphrasing helps to produce a better model or not.", "labels": [], "entities": []}, {"text": "On these experiments the embeddings are always trainable.", "labels": [], "entities": []}, {"text": "The paraphrasing tools allowed us to double the number of tweets in the training dataset.", "labels": [], "entities": []}, {"text": "Thus, for English we have 18,000 tweets and: Experiments on data augmentation 9,000 for Spanish.", "labels": [], "entities": []}, {"text": "Here different effects are noticiable.", "labels": [], "entities": []}, {"text": "For English a slight improvement on macro F-Score metric is reported, and for Spanish the effect is very negative.", "labels": [], "entities": [{"text": "F-Score metric", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.8959879577159882}]}, {"text": "We have submitted predictions on the test set on models trained only on task B, so for task A we have submitted only predicted labels for HS column.", "labels": [], "entities": []}, {"text": "For English, the training data has been the augmented official training set with paraphrased tweets.", "labels": [], "entities": []}, {"text": "For Spanish, only the training tweets provided by the organizers have been used to produce the model.", "labels": [], "entities": []}, {"text": "The official results obtained in this task are shown in 8 Conclusions and future work Our proposal explores how transferred embeddings and data augmentation may help in a text classification task like HatEval.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 171, "end_pos": 195, "type": "TASK", "confidence": 0.8356650273005167}]}, {"text": "Paraphrashing does not report clear benefits.", "labels": [], "entities": []}, {"text": "This can be due to the quality of the paraphrasing and the fact that new generated tweets are not very realistic.", "labels": [], "entities": []}, {"text": "Other augmentation strategies could be explored, like forward-backward translation.", "labels": [], "entities": [{"text": "forward-backward translation", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.5912885665893555}]}, {"text": "We have found also the models trained exhibits high variance.", "labels": [], "entities": [{"text": "variance", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9918084740638733}]}, {"text": "That means that we are overfitting the model on training data, so despite the use of the dropout technique,  early stopping, fewer parameters or more training data could help to produce a more robuts model.", "labels": [], "entities": []}, {"text": "Another possible improvement is on how final labels are decided.", "labels": [], "entities": []}, {"text": "Our system takes the final outputs of the last sigmoid layer as probabilities, so when the value is higher than 0.5 fora class, then the label is 1, 0 otherwise.", "labels": [], "entities": []}, {"text": "We could try to set the thresholds using a SVM classifier on this sigmoid vector.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Length of tweets covering 80 % and 90 % of  cases.", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9985916018486023}]}, {"text": " Table 6: Official HatEval results for our submissions.", "labels": [], "entities": [{"text": "Official HatEval", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.5472224652767181}]}]}