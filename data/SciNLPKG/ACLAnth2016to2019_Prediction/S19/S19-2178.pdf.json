{"title": [{"text": "Team Kit Kittredge at SemEval-2019 Task 4: LSTM Voting System", "labels": [], "entities": [{"text": "SemEval-2019 Task", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.802226334810257}]}], "abstractContent": [{"text": "This paper describes the approach of team Kit Kittredge to SemEval 2019 Task 4: Hyper-partisan News Detection.", "labels": [], "entities": [{"text": "SemEval 2019 Task 4", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.9042109549045563}, {"text": "Hyper-partisan News Detection", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.7389310002326965}]}, {"text": "The goal was binary classification of news articles into the categories of \"biased\" or \"unbiased\".", "labels": [], "entities": [{"text": "binary classification of news articles", "start_pos": 13, "end_pos": 51, "type": "TASK", "confidence": 0.7514419555664062}]}, {"text": "We had two software submissions: one a simple bag-of-words model, and the second an LSTM (Long Short Term Memory) neural network, which was trained on a subset of the original dataset selected by a voting system of other LSTMs.", "labels": [], "entities": []}, {"text": "This method did not prove much more successful than the baseline, however, due to the models' tendency to learn publisher-specific traits instead of general bias.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the proliferation of online news agencies after the rise of the Internet, access to information about what is going on in the world has never been more widespread.", "labels": [], "entities": []}, {"text": "How that information is presented, however, can have a large influence on what conclusions the reader draws from it.", "labels": [], "entities": []}, {"text": "Being able to automatically identify hyperpartisanship (bias or adherence to one party or faction over others) in a news article would help individuals in their news consumption and potentially result in a better-informed population.", "labels": [], "entities": []}, {"text": "As suggested, the challenge approached in this paper is that of hyperpartisan news detection: a binary classification problem (biased or unbiased) with news articles as data.", "labels": [], "entities": [{"text": "hyperpartisan news detection", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6123766104380289}]}, {"text": "This task can be considered as related to stance detection and in general, sentiment analysis.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.9785699248313904}, {"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.965222179889679}]}, {"text": "The challenge was organized as Task 4 for SemEval) (.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9344623684883118}]}, {"text": "Final submissions were submitted through TIRA, with the test datasets hidden . First in this paper, Section 2 includes an introduction of the provided dataset and a description of preprocessing techniques used for our approach.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.8048237562179565}]}, {"text": "Section 3 describes the first submitted software, a bag-of-words model.", "labels": [], "entities": []}, {"text": "Section 4 continues with our second approach, an LSTM trained on a subset of the original dataset, and a description of how that subset was selected.", "labels": [], "entities": []}, {"text": "Section 5 presents our results on the test set and Section 6 delves into analysis, presenting potential reasons why the models did not perform very well.", "labels": [], "entities": []}], "datasetContent": [{"text": "The idea behind the voting system was to pare down the original dataset, reducing noise and therefore focusing on the data points where bias was most salient -and could as such be picked up by models trained on different publishers.", "labels": [], "entities": []}, {"text": "Theoretically these articles would all have characteristics common to biased articles of all publishers.", "labels": [], "entities": []}, {"text": "When put together, then, the hope was that a model trained on this subset of the dataset would learn those common characteristics and not just the publisher-specific ones.", "labels": [], "entities": []}, {"text": "There are many things that could have gone wrong with this system, however.", "labels": [], "entities": []}, {"text": "Our cutoff for the news article length may have been too short, for example.", "labels": [], "entities": []}, {"text": "Secondly, Since the three models used for voting did not have very high accuracy on each other's datasets in the first place, the level of noise may not have been reduced at all.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.994097113609314}]}, {"text": "Furthermore, the original training dataset was four times as large as the validation dataset, and the byarticle dataset was far smaller than either.", "labels": [], "entities": [{"text": "byarticle dataset", "start_pos": 102, "end_pos": 119, "type": "DATASET", "confidence": 0.7605133950710297}]}, {"text": "Their subsets after the voting system was applied were equally unbalanced.", "labels": [], "entities": []}, {"text": "When combined and used for training the final LSTM, it could have been unbalanced enough that the model learned mostly from the data from the original dataset, and the features from those publishers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the test datasets.", "labels": [], "entities": []}]}