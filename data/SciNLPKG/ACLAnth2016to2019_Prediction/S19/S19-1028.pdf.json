{"title": [{"text": "On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference", "labels": [], "entities": [{"text": "Adversarial Removal of Hypothesis-only Bias in Natural Language Inference", "start_pos": 3, "end_pos": 76, "type": "TASK", "confidence": 0.7316964334911771}]}], "abstractContent": [{"text": "Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases.", "labels": [], "entities": []}, {"text": "Adversarial learning may help models ignore sensitive biases and spurious correlations in data.", "labels": [], "entities": []}, {"text": "We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases.", "labels": [], "entities": []}, {"text": "Our analyses indicate that the representations learned via adversarial learning maybe less biased, with only small drops in NLI accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9473843574523926}]}], "introductionContent": [{"text": "Popular datasets for Natural Language Inference (NLI) -the task of determining whether one sentence (premise) likely entails another (hypothesis) -contain hypothesis-only biases that allow models to perform the task surprisingly well by only considering hypotheses while ignoring the corresponding premises.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.8350517451763153}]}, {"text": "For instance, such a method correctly predicted the examples in as contradictions.", "labels": [], "entities": []}, {"text": "As datasets may always contain biases, it is important to analyze whether, and to what extent, models are immune to or rely on known biases.", "labels": [], "entities": []}, {"text": "Furthermore, it is important to build models that can overcome these biases.", "labels": [], "entities": []}, {"text": "Recent work in NLP aims to build more robust systems using adversarial methods).", "labels": [], "entities": []}, {"text": "In particular, attempted to use adversarial training to remove demographic attributes from text data, with limited success.", "labels": [], "entities": []}, {"text": "Inspired by this line of work, we use adversarial learning to add small components to an existing and popular NLI system that has been used to learn general sentence representations ( techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples.", "labels": [], "entities": []}, {"text": "In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning.", "labels": [], "entities": []}, {"text": "The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance.", "labels": [], "entities": []}, {"text": "Our results suggest that progress on this goal may depend on which adversarial learning techniques are used.", "labels": [], "entities": []}, {"text": "Although recent work has applied adversarial learning to NLI, this is the first work to our knowledge that explicitly studies NLI models designed to ignore hypothesis-only biases.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experimental setup Out of 10 NLI datasets, found that the Stanford Natural Language Inference dataset (SNLI;) contained the most (or worst) hypothesisonly biases-their hypothesis-only model outperformed the majority baseline by roughly 100% (going from roughly 34% to 69%).", "labels": [], "entities": [{"text": "Stanford Natural Language Inference dataset (SNLI", "start_pos": 58, "end_pos": 107, "type": "DATASET", "confidence": 0.8004181810787746}]}, {"text": "Because of the large magnitude of these biases, confirmed by Tsuchiya (2018) and, we focus on SNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 94, "end_pos": 98, "type": "TASK", "confidence": 0.5924962759017944}]}, {"text": "We use the standard SNLI split and report validation and test results.", "labels": [], "entities": [{"text": "SNLI split", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.8070241808891296}]}, {"text": "We also test on SNLI-hard, a subset of SNLI that filtered such that it may not contain unwanted artifacts.", "labels": [], "entities": []}, {"text": "We apply both adversarial techniques to InferSent (, which serves as our general NLI architecture.", "labels": [], "entities": []}, {"text": "Following the standard training details used in InferSent, we encode premises and hypotheses separately using bi-directional long short-term memory (BiLSTM) networks).", "labels": [], "entities": []}, {"text": "Premises and hypotheses are initially mapped (token-by-token) to) representations.", "labels": [], "entities": []}, {"text": "We use max-pooling over the BiLSTM states to extract premise and hypothesis representations and, following, combine the representations by concatenating their vectors, their difference, and their multiplication (element-wise).", "labels": [], "entities": []}, {"text": "We use the default training hyper-parameters in the released InferSent codebase.", "labels": [], "entities": [{"text": "InferSent codebase", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.9196214377880096}]}, {"text": "3 These include setting the initial learning rate to 0.1 and the decay rate to 0.99, using SGD optimization and dividing the learning rate by 5 at every epoch when the accuracy deceases on the validation set.", "labels": [], "entities": [{"text": "decay rate", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9471902549266815}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9984307885169983}]}, {"text": "The default settings also include stopping training either when the learning rate drops below 10 \u22125 or after 20 epochs.", "labels": [], "entities": []}, {"text": "In both adversarial settings, the hyper-parameters are swept through {0.05, 0.1, 0.2, 0.4, 0.8, 1.0}.", "labels": [], "entities": []}, {"text": "reports the results on SNLI, with the configurations that performed best on the validation set for each of the adversarial methods.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.7800757884979248}]}, {"text": "As expected, both training methods perform worse than our unmodified, non-adversarial InferSent baseline on SNLI's test set, since they remove biases that maybe useful for performing this Code developed is available at https: //github.com/azpoliak/robust-nli.", "labels": [], "entities": [{"text": "SNLI's test set", "start_pos": 108, "end_pos": 123, "type": "DATASET", "confidence": 0.9302947819232941}]}], "tableCaptions": [{"text": " Table 2: Accuracies for the approaches. Baseline refers  to the unmodified, non-adversarial InferSent.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9964950680732727}]}, {"text": " Table 3: Indicator words and how correlated they are with CONTRADICTION predictions. The parentheses  indicate hyper-parameter values: (\u03bb Loss , \u03bb Enc ) for AdvCls and (\u03bb Rand , \u03bb Enc ) for AdvDat. Baseline refers to the  unmodified InferSent.", "labels": [], "entities": [{"text": "InferSent", "start_pos": 234, "end_pos": 243, "type": "DATASET", "confidence": 0.8900949358940125}]}]}