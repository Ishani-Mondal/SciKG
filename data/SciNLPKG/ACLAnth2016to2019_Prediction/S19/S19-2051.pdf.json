{"title": [{"text": "SCIA at SemEval-2019 Task 3: Sentiment analysis in textual conversations using Deep Learning", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9650214314460754}]}], "abstractContent": [{"text": "In this paper we present our submission for SemEval-2019 Task 3: EmoContext.", "labels": [], "entities": [{"text": "SemEval-2019 Task 3", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8225899140040079}, {"text": "EmoContext", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.8315157890319824}]}, {"text": "The task consisted of classifying a textual dialogue into one of four emotion classes: happy, sad, angry or others.", "labels": [], "entities": []}, {"text": "Our approach tried to improve on multiple aspects, preprocessing with an emphasis on spell-checking and ensembling with four different models: Bi-directional con-textual LSTM (BC-LSTM), categorical Bi-LSTM (CAT-LSTM), binary convolutional Bi-LSTM (BIN-LSTM) and Gated Recurrent Unit (GRU).", "labels": [], "entities": []}, {"text": "On the leader-board, we submitted two systems that obtained a micro F1 score (F1\u00b5) of 0.711 and 0.712.", "labels": [], "entities": [{"text": "F1 score (F1\u00b5)", "start_pos": 68, "end_pos": 82, "type": "METRIC", "confidence": 0.8595458269119263}]}, {"text": "After the competition , we merged our two systems with ensem-bling, which achieved a F1\u00b5 of 0.7324 on the test dataset.", "labels": [], "entities": [{"text": "F1\u00b5", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9994827508926392}]}], "introductionContent": [{"text": "Rapid progress in natural language processing with the rise of deep learning has brought increasing attention on tasks such as text classification and sentiment analysis.", "labels": [], "entities": [{"text": "text classification", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7940518260002136}, {"text": "sentiment analysis", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.9386323690414429}]}, {"text": "Most of the work in that field was made using social media due to the large amount of data available.", "labels": [], "entities": []}, {"text": "The task addressed in this paper focuses on emotion detection within conversations from social media.", "labels": [], "entities": [{"text": "emotion detection within conversations from social media", "start_pos": 44, "end_pos": 100, "type": "TASK", "confidence": 0.8548637287957328}]}, {"text": "The key point is that we need to take into account multiple speakers and capture a global emotion out of their conversation.", "labels": [], "entities": []}, {"text": "It becomes a challenge when facing different users who each have a different way to express their emotions depending on their personalities.", "labels": [], "entities": []}, {"text": "Ina dialogue, users have an initial emotional state, and their mood will shift as the dialogue goes on.", "labels": [], "entities": []}, {"text": "Therefore, the task of labelling a turn-based conversation with the right emotion is even more challenging.", "labels": [], "entities": [{"text": "labelling a turn-based conversation", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.8090848475694656}]}, {"text": "State of the art approaches consist of using language models () to pre-train the model on the general NLP task of language modeling before fine-tuning on specific tasks like classification or translation.", "labels": [], "entities": [{"text": "classification or translation", "start_pos": 174, "end_pos": 203, "type": "TASK", "confidence": 0.6622272729873657}]}, {"text": "The language model approach used by,) was especially successful for this kind of tasks.", "labels": [], "entities": []}, {"text": "For the specific task of emotion classification in textual conversation, achieved a F1\u00b5 score of 0.7134 on the same dataset, using an architecture based on LSTM.", "labels": [], "entities": [{"text": "emotion classification in textual conversation", "start_pos": 25, "end_pos": 71, "type": "TASK", "confidence": 0.7667120158672333}, {"text": "F1\u00b5 score", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9838546216487885}]}, {"text": "For sentiment analysis, other successful approaches also used Bi-LSTM () as well as transfer learning.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9799075126647949}, {"text": "Bi-LSTM", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9259462356567383}, {"text": "transfer learning", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.920137882232666}]}, {"text": "In this paper, we present two sub-systems that are composed of four deep-learning models (using Bi-LSTM, GRU and CNN).", "labels": [], "entities": []}, {"text": "Those two sub-systems competed independently at.", "labels": [], "entities": []}, {"text": "After the final evaluation, we merged both sub-systems, taking advantage of ensemble learning.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Part 2 gives an overview of our approach.", "labels": [], "entities": []}, {"text": "Our preprocessing methods, the description of our models, and our ensembling approach are all described in Part 3.", "labels": [], "entities": []}, {"text": "Part 4 shows the obtained results and in Part 5, we give a conclusion with remarks for future works.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F1\u00b5 score on the test set for each model.", "labels": [], "entities": [{"text": "F1\u00b5 score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9745594561100006}]}]}