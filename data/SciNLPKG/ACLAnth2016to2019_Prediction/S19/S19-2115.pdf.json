{"title": [], "abstractContent": [{"text": "In this paper, we describe our submissions to SemEval-2019 task 6 contest.", "labels": [], "entities": [{"text": "SemEval-2019 task 6 contest", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7546639889478683}]}, {"text": "We tackled all three sub-tasks in this task \"OffensEval-Identifying and Categorizing Offensive Language in Social Media\".", "labels": [], "entities": []}, {"text": "In our system called JCTICOL (Jerusalem College of Technology Identifies and Categorizes Offensive Language), we applied various supervised ML methods.", "labels": [], "entities": [{"text": "JCTICOL (Jerusalem College of Technology Identifies and Categorizes Offensive Language)", "start_pos": 21, "end_pos": 108, "type": "TASK", "confidence": 0.7884266922871271}, {"text": "ML", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.9821804761886597}]}, {"text": "We applied various combinations of word/character n-gram features using the TF-IDF scheme.", "labels": [], "entities": []}, {"text": "In addition, we applied various combinations of seven basic preprocessing methods.", "labels": [], "entities": []}, {"text": "Our best submission, an RNN model was ranked at the 25 th position out of 65 submissions for the most complex sub-task (C).", "labels": [], "entities": []}], "introductionContent": [{"text": "Offensive language is frequent in social media.", "labels": [], "entities": []}, {"text": "For instance, ScanSafe's monthly \"Global Threat Report\" reported that up to 80% of blogs contained offensive contents and 74% included porn in the format of the image, video, or offensive language.", "labels": [], "entities": [{"text": "Global Threat Report\"", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.7820832431316376}]}, {"text": "There are people that take advantage of the perceived anonymity of computer-mediated communication, using this to write in behavior that many of them would not consider in real life.", "labels": [], "entities": []}, {"text": "Online news and social networking services, online communities, social media platforms, and various computer companies have been investing a lot of effort, time and money to cope with offensive language in order to prevent abusive behavior.", "labels": [], "entities": []}, {"text": "Computational methods are among the most effective strategies to identify various types of aggression, offense, and hate speech in usergenerated content (e.g., comments, microblogs, posts, and tweets).", "labels": [], "entities": [{"text": "identify various types of aggression, offense, and hate speech in usergenerated content (e.g., comments, microblogs, posts, and tweets", "start_pos": 65, "end_pos": 199, "type": "Description", "confidence": 0.7371860337257385}]}, {"text": "Detection of offensive language has been investigated in recent years in various studies () and various workshops such as ALW (Abusive Language Online) and TRAC.", "labels": [], "entities": [{"text": "Detection of offensive language", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8600593209266663}, {"text": "TRAC", "start_pos": 156, "end_pos": 160, "type": "DATASET", "confidence": 0.6987460255622864}]}, {"text": "In this paper, we describe our submissions to SemEval-2019 task 6 contest.", "labels": [], "entities": [{"text": "SemEval-2019 task 6 contest", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7546639889478683}]}, {"text": "In Task-6, OffensEval, there are three different sub-tasks.", "labels": [], "entities": []}, {"text": "Sub-task A deals with offensive language identification.", "labels": [], "entities": [{"text": "offensive language identification", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.6307960649331411}]}, {"text": "Sub-task B deals with the automatic categorization of offense types.", "labels": [], "entities": []}, {"text": "Subtask C deals with offense target identification.", "labels": [], "entities": [{"text": "offense target identification", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7719606359799703}]}, {"text": "The report of the OffensEval task is described in and the description of the OLID dataset that was used for the competition is in.", "labels": [], "entities": [{"text": "OLID dataset", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.8170084953308105}]}, {"text": "The structure of the rest of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses work related to offensive language in social media, tweet classification, and data preprocessing.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7774650752544403}, {"text": "data preprocessing", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.6885281950235367}]}, {"text": "Section 3 presents, in general, the task description.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the submitted models and their experimental results.", "labels": [], "entities": []}, {"text": "Section 6 summarizes and suggests ideas for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have submitted 17 models: 6 models to task 6-A, 6 models to task 6-B, and 5 models to task 6-C.", "labels": [], "entities": []}, {"text": "We applied the Python module called Scikitlearn (Pedregosa et al., 2011) using the TF-IDF scheme called TfidfTransformer and we applied various supervised ML methods with various numbers of n-gram features, skip word/char ngrams) and combinations of pre-processing types.", "labels": [], "entities": []}, {"text": "While all teams' submissions in all three subtasks of task 6 were ranked according to their FMeasure scores, we were wrong in all these subtasks in the sense that we submitted models according to their accuracy scores.", "labels": [], "entities": [{"text": "FMeasure", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.7460542321205139}, {"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9977473616600037}]}, {"text": "Most of our submitted models were RNN models.", "labels": [], "entities": []}, {"text": "Each RNN model was a bidirectional RNN with 4 hidden layers, with different numbers of LSTMs, values of Dropout, and number of vectors of GloVe ().", "labels": [], "entities": [{"text": "GloVe", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9553624987602234}]}, {"text": "Additional explanations to our RNN models, which are given in the next paragraphs are mainly based on the explanations given by Nikolai Janakiev in \"Practical Text Classification with Python and Keras\" 4 . We used the Tokenizer utility class, which converts a text corpus into a list of integers.", "labels": [], "entities": [{"text": "Practical Text Classification", "start_pos": 149, "end_pos": 178, "type": "TASK", "confidence": 0.5814222991466522}]}, {"text": "Each 3 https://scikitlearn.org/stable/modules/generated/sklearn.feature_extraction.", "labels": [], "entities": []}, {"text": "text.TfidfTransformer.html#sklearn.feature_extraction.text.Tfi dfTransformer integer maps to a value in a dictionary that encodes the entire corpus, with the dictionary's keys being the vocabulary terms themselves.", "labels": [], "entities": []}, {"text": "We chose to use the Twitter-aware tokenizer, designed to be flexible and easy to adapt to new domains and tasks (e.g., for tweet processing).", "labels": [], "entities": [{"text": "tweet processing", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.7326325178146362}]}, {"text": "We used the word embeddings method.", "labels": [], "entities": []}, {"text": "This method represents words as dense word vectors, which are trained, unlike the one-hot encoding which is hardcoded.", "labels": [], "entities": []}, {"text": "The word embeddings map the statistical structure of the language used in the corpus.", "labels": [], "entities": []}, {"text": "Their aim is to map semantic meaning into a geometric space.", "labels": [], "entities": []}, {"text": "This geometric space is then called the embedding space.", "labels": [], "entities": []}, {"text": "This method would map semantically similar words close on the embedding space.", "labels": [], "entities": []}, {"text": "There are two options to get such a word embedding.", "labels": [], "entities": []}, {"text": "One way is to train the word embeddings during the training of our neural network.", "labels": [], "entities": []}, {"text": "The other way is to use a precomputed embedding space that utilizes a larger corpus.", "labels": [], "entities": []}, {"text": "Among the most popular methods are GloVe (Global Vectors for Word Representation) developed by the Stanford NLP Group () and Word2Vec developed by.", "labels": [], "entities": [{"text": "Word Representation)", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7819193104902903}, {"text": "Stanford NLP Group", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.8014443119366964}]}, {"text": "GloVe applies a co-occurrence matrix and by using matrix factorization while Word2Vec applies neural networks.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.921975314617157}]}, {"text": "Word2Vec is more accurate and GloVe is faster to compute.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9697185754776001}, {"text": "GloVe", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.7499305605888367}]}, {"text": "We used the GloVe method for our model.", "labels": [], "entities": []}, {"text": "presents the main characteristics and results of our six submitted models to task 6-A.", "labels": [], "entities": []}, {"text": "The models are presented in descending order according to their F-measure score on the test set.: Results of our 6 models in task-A.", "labels": [], "entities": [{"text": "F-measure score", "start_pos": 64, "end_pos": 79, "type": "METRIC", "confidence": 0.9716062247753143}]}], "tableCaptions": [{"text": " Table 1: Results of our 6 models in task-A.", "labels": [], "entities": []}, {"text": " Table 2: Results of our 6 models in task-B.", "labels": [], "entities": []}]}