{"title": [{"text": "YNU-HPCC at SemEval-2019 Task 9: Using a BERT and CNN-BiLSTM-GRU Model for Suggestion Mining", "labels": [], "entities": [{"text": "YNU-HPCC at SemEval-2019 Task 9", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.8421057105064392}, {"text": "BERT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.992327868938446}, {"text": "Suggestion Mining", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.9790036678314209}]}], "abstractContent": [{"text": "Consumer opinions towards commercial entities are generally expressed through online reviews , blogs, and discussion forums.", "labels": [], "entities": []}, {"text": "These opinions largely express positive and negative sentiments towards a given entity; however, they may also contain suggestions for improving the entity.", "labels": [], "entities": []}, {"text": "In this task, we extract suggestions from a given unstructured text, in contrast to the traditional opinion mining systems.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7410592138767242}]}, {"text": "This type suggestion mining is more applicable and extends capabilities.", "labels": [], "entities": []}, {"text": "In this paper , we propose the use of bidirectional en-coder representation learned from transformers (BERT) to address the problem of domain specific suggestion mining in task A.", "labels": [], "entities": [{"text": "BERT", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9863913059234619}, {"text": "domain specific suggestion mining", "start_pos": 135, "end_pos": 168, "type": "TASK", "confidence": 0.6356134116649628}]}, {"text": "In detail, BERT is also used to extract feature vectors and perform fine-tuning for the task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9983605742454529}]}, {"text": "For Task B, we applied an ensemble model to combine the BiLSTM, CNN, and GRU models, which can perform cross domain suggestion mining.", "labels": [], "entities": [{"text": "cross domain suggestion mining", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.6265154853463173}]}, {"text": "Officially released results show that our system performs better than the baseline algorithm does.", "labels": [], "entities": []}], "introductionContent": [{"text": "Suggestion mining is used to extract advice from text such as that provided in online reviews, blogs, discussion forums, and social media platforms where consumers share their opinions towards commercial entities like brands, services, and products.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9791669845581055}, {"text": "extract advice from text such as that provided in online reviews, blogs, discussion forums, and social media platforms where consumers share their opinions towards commercial entities like brands, services, and products", "start_pos": 29, "end_pos": 248, "type": "Description", "confidence": 0.7959791984823015}]}, {"text": "Most of the traditional sentiment analysis methods are emotion classifications.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9151704609394073}, {"text": "emotion classifications", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7277925908565521}]}, {"text": "Opinion mining can improve service and quality.", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8138247430324554}]}, {"text": "Such systems have become an effective way for marketing, economics, politics, and advertising.", "labels": [], "entities": []}, {"text": "The application of suggestion mining provides the motivation, for the SemEval 2019 Task 9 (, which contains two subtasks that classify given sentences into suggestion and non-suggestion classes.", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7311663925647736}, {"text": "SemEval 2019 Task 9", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.6801223158836365}]}, {"text": "Subtask A requires a system to achieve domain specific training, whereby the test dataset will belong to the same domain as the training and development datasets.", "labels": [], "entities": []}, {"text": "This was part of a suggestion forum for windows platform developers.", "labels": [], "entities": []}, {"text": "Subtask B applies the system to cross domain training, where training, development, and test datasets will belong to different domains.", "labels": [], "entities": []}, {"text": "Training and development datasets will remain the same as Subtask A, while the test dataset will belong to the domain of hotel reviews.", "labels": [], "entities": []}, {"text": "There are many methods in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.970520555973053}]}, {"text": "In many reports on this subject, it has been implied that these models help improve classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.9646316170692444}]}, {"text": "Successful models include convolutional neural networks (CNN), long short-term memory (LSTM), and bi-directional LSTM (BiLSTM).", "labels": [], "entities": []}, {"text": "C-NN can capture local n-gram features, while LST-M can maintain memory in the pipelines and solve the problem of long sequence dependence in neural networks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a bidirectional encoder representation learned from transformers (BERT) model) for Task A. It comprises two phases.", "labels": [], "entities": [{"text": "BERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9836246967315674}]}, {"text": "The first phase is called pre-training and is similar to word embedding.", "labels": [], "entities": []}, {"text": "The second phase is called fine-tuning and uses a pre-trained language model to complete specific NLP downstream tasks.", "labels": [], "entities": []}, {"text": "We used a pre-trained model that was provided by Google AI team.", "labels": [], "entities": []}, {"text": "It included weights for the pre-trained model and a vocab file that maps component words of sentences to indexes of words.", "labels": [], "entities": []}, {"text": "It also included the JSON file, which specifies the model hyper-parameters.", "labels": [], "entities": [{"text": "JSON file", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.8183640837669373}]}, {"text": "Finetuning was applied to sequence classification: the BERT directly takes the final hidden state of the first token, adds a layer of weight, and then softmax predicts the label probability.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7762434780597687}, {"text": "BERT", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.995672345161438}]}, {"text": "The structure is shown in.", "labels": [], "entities": []}, {"text": "For Task B, we apply the bert model to test data, the score is 0.343, it is very low.", "labels": [], "entities": []}, {"text": "the reason is that the task B is cross-domain training, so we intro-", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report the experiments were conducted to evaluate the proposed models on both sub-tasks.", "labels": [], "entities": []}, {"text": "We also report the results of the official review.", "labels": [], "entities": []}, {"text": "The details of the experiment are described as follows.", "labels": [], "entities": []}, {"text": "Classification performance of the submissions will be evaluated based on binary F 1 -score for the positive class.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9297570139169693}]}, {"text": "Binary F 1 -score will range from 1 to 0.", "labels": [], "entities": [{"text": "Binary F 1 -score", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.8669845342636109}]}], "tableCaptions": [{"text": " Table 1: The experiment results.", "labels": [], "entities": []}, {"text": " Table 2: The best-tuned parameters.", "labels": [], "entities": []}]}