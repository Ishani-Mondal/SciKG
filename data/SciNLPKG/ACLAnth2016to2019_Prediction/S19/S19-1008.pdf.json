{"title": [{"text": "Pre-trained Contextualized Character Embeddings Lead to Major Improvements in Time Normalization: a Detailed Analysis", "labels": [], "entities": [{"text": "Time Normalization", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.6839477121829987}]}], "abstractContent": [{"text": "Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks.", "labels": [], "entities": []}, {"text": "But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied.", "labels": [], "entities": []}, {"text": "We derive character-level contextual em-beddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes.", "labels": [], "entities": [{"text": "Flair", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.8964438438415527}, {"text": "error reduction", "start_pos": 207, "end_pos": 222, "type": "METRIC", "confidence": 0.9605783224105835}]}, {"text": "We analyze the sources of these improvements, and find that pre-trained contextual character em-beddings are more robust to term variations, infrequent terms, and cross-domain changes.", "labels": [], "entities": []}, {"text": "We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization .", "labels": [], "entities": []}], "introductionContent": [{"text": "Pre-trained language models (LMs) such as ELMo (,, OpenAI GPT (),) and Bert ( have shown great improvements in NLP tasks ranging from sentiment analysis to named entity recognition to question answering.", "labels": [], "entities": [{"text": "OpenAI GPT", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8996880054473877}, {"text": "sentiment analysis", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.9533405005931854}, {"text": "named entity recognition", "start_pos": 156, "end_pos": 180, "type": "TASK", "confidence": 0.5983348190784454}, {"text": "question answering", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.8645186424255371}]}, {"text": "These models are trained on huge collections of unlabeled data and produce contextualized word embeddings, i.e., each word receives a different vector representation in each context, rather than a single common vector representation regardless of context as in word2vec ( and GloVe.", "labels": [], "entities": []}, {"text": "Research is ongoing to study these models and determine where their benefits are coming from).", "labels": [], "entities": []}, {"text": "The analyses have focused on wordlevel models, yet character-level models have been shown to outperform word-level models in some NLP tasks, such as text classification (, named entity recognition (, and time normalization).", "labels": [], "entities": [{"text": "text classification", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8133545517921448}, {"text": "named entity recognition", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.6148518125216166}]}, {"text": "Thus, there is a need to study pre-trained contextualized character embeddings, to see if they also yield improvements, and if so, to analyze where those benefits are coming from.", "labels": [], "entities": []}, {"text": "All of the pre-trained word-level contextual embedding models include some character or subword components in their architecture.", "labels": [], "entities": []}, {"text": "For example, Flair is a forward-backward LM trained over characters using recurrent neural networks (RNNs), that generates pre-trained contextual word embeddings by concatenating the forward LM's hidden state for the word's last character and the backward LM's hidden state for the word's first character.", "labels": [], "entities": [{"text": "Flair", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8125202059745789}]}, {"text": "Flair achieves state-of-the-art or competitive results on part-of-speech tagging and named entity tagging).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7134388536214828}, {"text": "named entity tagging", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.6578775346279144}]}, {"text": "Though they do not pre-train a LM, similarly apply a bidirectional long short term memory network (LSTM) layer on all characters of a sentence and generate contextual word embeddings by concatenating the forward and backward LSTM hidden states of the first and last character in each word.", "labels": [], "entities": []}, {"text": "Together with other techniques, they achieve state-of-the-art performance on part-of-speech and morphological tagging.", "labels": [], "entities": []}, {"text": "However, both and discard all other contextual character embeddings, and no analyses of the models are performed at the character-level.", "labels": [], "entities": []}, {"text": "In the current paper, we derive pre-trained contextual character embeddings from Flair's forwardbackward LM trained on a 1-billion word corpus of English (, and observe if these embeddings yield the same large improvements for character-level tasks as yielded by pre-trained contextual word embeddings for word-level tasks.", "labels": [], "entities": [{"text": "Flair's forwardbackward LM trained on a 1-billion word corpus of English", "start_pos": 81, "end_pos": 153, "type": "DATASET", "confidence": 0.7677755554517111}]}, {"text": "We aim to analyze where improvements come from (e.g., term variations, low frequency words) and what they depend on (e.g., embedding size, context size).", "labels": [], "entities": []}, {"text": "We focus on the task of parsing time normalizations ( , where large gains of character-level models over word-level models have been observed ().", "labels": [], "entities": [{"text": "parsing time normalizations", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8942821820576986}]}, {"text": "This task involves finding and composing pieces of a time expression to infer time intervals, so for example, the expression 3 days ago could be normalized to the interval.", "labels": [], "entities": []}, {"text": "We first take a state-of-the-art neural network for parsing time normalizations and replace its randomly initialized character embeddings with pre-trained contextual character embeddings.", "labels": [], "entities": [{"text": "parsing time normalizations", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.8895431955655416}]}, {"text": "After showing that this yields major performance improvements, we analyze the improvements to understand why pre-trained contextual character embeddings are so useful.", "labels": [], "entities": []}, {"text": "Our contributions are: \u2022 We derive pre-trained contextual character embeddings from Flair (, apply them to a state-of-the art time normalizer (, and obtain major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% error reduction in clinical notes.", "labels": [], "entities": [{"text": "Flair", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.6075345873832703}, {"text": "error reduction", "start_pos": 227, "end_pos": 242, "type": "METRIC", "confidence": 0.9344689846038818}]}, {"text": "\u2022 We demonstrate that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and crossdomain changes.", "labels": [], "entities": []}, {"text": "\u2022 We quantify the amount of context leveraged by pre-trained contextual character embeddings.", "labels": [], "entities": []}, {"text": "\u2022 We show that pre-trained contextual character embeddings remove the need for features like part-of-speech and capitalization.", "labels": [], "entities": []}, {"text": "Bi-GRUs Softmax Outputs the spans of characters that belong to each time expression and labels them with their corresponding time entity; and b) time entity composition using a simple set of rules that links relevant entities together while respecting the entity type constraints imposed by the SCATE schema.", "labels": [], "entities": [{"text": "time entity composition", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.599920262893041}]}, {"text": "These two tasks are run sequentially using the predicted output of the sequence tagger as input to the rule-based time entity composition system.", "labels": [], "entities": []}, {"text": "In this paper, We focus on the character-level time entity identifier that is the foundation of's model.", "labels": [], "entities": []}, {"text": "The sequence tagger is a multi-output RNN with three different input features, shown in.", "labels": [], "entities": [{"text": "sequence tagger", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6547976136207581}]}, {"text": "Features are mapped through an embedding layer, then fed into stacked bidirectional Gated Recurrent Units (bi-GRUs), and followed by a softmax layer.", "labels": [], "entities": []}, {"text": "There are three types of outputs per's encoding of the SCATE schema, so there is a separate stack of bi-GRUs and a softmax for each output type.", "labels": [], "entities": []}, {"text": "We keep the original neural architecture and parameter settings in, and experiment with the following embedding layers: Rand(128): the original setting of  ning Flair forward-backward character-level LM Flair's forward and backward character-level language models over the text, and concatenating the hidden states from forward and backward character-level LMs for each character . We evaluate in the clinical and news domains, the former being more than 9 times larger and the latter having a more diverse set of labels.", "labels": [], "entities": []}, {"text": "Three different evaluation metrics are used for parsing time normalization tasks: identification of time entities, which evaluates the predicted span (offsets) and the SCATE type for each entity; parsing of time entities, which evaluates the span, the SCATE type, and properties for each time entity; interval extraction, which interprets parsed annotations as intervals along the timeline and measures the fraction of the correctly parsed intervals.", "labels": [], "entities": [{"text": "parsing time normalization tasks", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.9127361625432968}, {"text": "parsing of time entities", "start_pos": 196, "end_pos": 220, "type": "TASK", "confidence": 0.8571272939443588}, {"text": "interval extraction", "start_pos": 301, "end_pos": 320, "type": "TASK", "confidence": 0.667485773563385}]}, {"text": "The SemeEval task description paper () has more details on dataset statistics and evaluation metrics.", "labels": [], "entities": [{"text": "SemeEval task description", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.8305316766103109}]}, {"text": "Pre-trained contextual embeddings provide additional benefits: Cont(4096) significantly outperforms Rand(4096) on all datasets (p < 0.001 in all cases).", "labels": [], "entities": [{"text": "Rand", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9485437870025635}]}, {"text": "We conclude that pre-trained contextual character embeddings provide more than just greater model capacity.", "labels": [], "entities": []}, {"text": "shows how pre-trained contextual character embeddings improve performance on both term variations and low frequency words.", "labels": [], "entities": []}, {"text": "We define term variations as time entities that appear in the training data in the following patterns: both upper-case and lower-case, e.g., DAY, Day, and day; abbreviation with and without punctuation, e.g., AM and A.M.; or same stem, e.g.,  over Rand(4096) on time entities with (+var) and without (-var) term variations.", "labels": [], "entities": [{"text": "DAY", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.8384356498718262}, {"text": "Rand(4096)", "start_pos": 248, "end_pos": 258, "type": "DATASET", "confidence": 0.865370437502861}]}, {"text": "Cont(4096) is always better than Rand(4096) so all differences are positive, but the improvements in +var are much larger than those of -var in the news domain (+8.4 vs. +1.6 and +15.0 vs. +8.7).", "labels": [], "entities": []}, {"text": "In the clinical domain, where 9 times more training data is available, both +var and -var yield similar improvements.", "labels": [], "entities": []}, {"text": "We conclude that pre-trained contextual character embeddings are mostly helpful with term variations in low data scenarios.", "labels": [], "entities": []}, {"text": "We define infrequent terms as time entities that occur in the training set 10 or fewer times.", "labels": [], "entities": []}, {"text": "In the dev and test sets, 73.9-86.9% of terms are infrequent, with about one third of infrequent terms being numerical show the improvements in F 1 of the Cont(4096) over Rand(4096) on frequent (>10) and infrequent (\u226410) terms.", "labels": [], "entities": [{"text": "F 1", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9825842678546906}]}, {"text": "Cont(4096) is always better than Rand(4096), and in both domains the improvements on low frequency terms are always greater than those on high frequency terms (+8.1 vs. +2.4 in news dev, +17.6 vs. +5.0 in news test, etc.).", "labels": [], "entities": [{"text": "Cont", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8699437379837036}]}, {"text": "We conclude that pre-trained contextual character embeddings improve the representations of low frequency words in both low and high data settings.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on Identification (Ident.), Parsing  and Interval extraction (Interv.) of time expressions  for News and Clinical domain. Rand(128)-ori refers  to the original implementation, and Rand(128) and  Cont(4096) refer to our re-implementation 1 .", "labels": [], "entities": [{"text": "Parsing  and Interval extraction (Interv.)", "start_pos": 46, "end_pos": 88, "type": "METRIC", "confidence": 0.7217433537755694}]}, {"text": " Table 4: Effect of domain change on performance: (F 1 )  on News and Clinical datasets.", "labels": [], "entities": [{"text": "News and Clinical datasets", "start_pos": 61, "end_pos": 87, "type": "DATASET", "confidence": 0.8620220571756363}]}, {"text": " Table 5: Effect of features on performance: Perfor- mance (F 1 ) with different feature sets, including char- acters (C), part-of-speech tags (P), and unicode charac- ter categories (U).", "labels": [], "entities": []}]}