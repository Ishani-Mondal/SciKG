{"title": [{"text": "SNU IDS at SemEval-2019 Task 3: Addressing Training-Test Class Distribution Mismatch in Conversational Classification", "labels": [], "entities": [{"text": "SNU IDS", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.5734918266534805}, {"text": "Addressing Training-Test Class Distribution Mismatch", "start_pos": 32, "end_pos": 84, "type": "TASK", "confidence": 0.8614530444145203}]}], "abstractContent": [{"text": "We present several techniques to tackle the mismatch in class distributions between training and test data in the Contextual Emotion Detection task of SemEval 2019, by extending the existing methods for class imbalance problem.", "labels": [], "entities": [{"text": "Contextual Emotion Detection task", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.7071420177817345}]}, {"text": "Reducing the distance between the distribution of prediction and ground truth, they consistently show positive effects on the performance.", "labels": [], "entities": []}, {"text": "Also we propose a novel neural architecture which utilizes representation of overall context as well as of each utterance.", "labels": [], "entities": []}, {"text": "The combination of the methods and the models achieved micro F1 score of about 0.766 on the final evaluation.", "labels": [], "entities": [{"text": "micro", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9785642623901367}, {"text": "F1 score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.8816716969013214}]}], "introductionContent": [{"text": "A new task whose goal is to predict the emotion of the last speaker given a sequence of text messages has been designed, and coined Contextual Emotion Detection (EmoContext;.", "labels": [], "entities": [{"text": "Contextual Emotion Detection", "start_pos": 132, "end_pos": 160, "type": "TASK", "confidence": 0.6007923583189646}]}, {"text": "Though predicting the emotion of a single utterance or sentence, i.e. emotion detection, is a welldiscussed subject in natural language understanding literature, EmoContext has several novel challenges.", "labels": [], "entities": [{"text": "predicting the emotion of a single utterance or sentence", "start_pos": 7, "end_pos": 63, "type": "TASK", "confidence": 0.8171625468466017}, {"text": "emotion detection", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.6864880621433258}]}, {"text": "Firstly, the class distribution of training data is significantly different from that of the test data.", "labels": [], "entities": []}, {"text": "Consequently, a model trained on the training data might not perform well on the test data.", "labels": [], "entities": []}, {"text": "There have been efforts made to address the problem of learning from training data sets that have imbalanced class distribution, i.e. the class imbalance problem (.", "labels": [], "entities": []}, {"text": "However, they are not applicable to our case, since the imbalance appears only in the test data while the training data can be viewed to be balanced.", "labels": [], "entities": []}, {"text": "We extend the existing methods of addressing the class imbalance problem to be applicable for more general cases where the distributions of the collected training data differ from those of the real population or the data attest time, under the assumption that the validation set is organized carefully to reflect the practical distribution.", "labels": [], "entities": []}, {"text": "From experiments and analyses, we show that the proposed methods reduce the difference between two distributions and as a result improve the performance of the model.", "labels": [], "entities": []}, {"text": "The additional challenge we have to consider arises from the fact that utterances having identical surface form may have different meanings due to sarcasm, irony, or etc..", "labels": [], "entities": []}, {"text": "Thus a model should track the emotional transitions within a dialogue.", "labels": [], "entities": []}, {"text": "To grasp the context of utterances, we propose a semihierarchical encoder structure.", "labels": [], "entities": []}, {"text": "Lastly, the texts contain lots of non-standard words, e.g. emoticons and emojis.", "labels": [], "entities": []}, {"text": "This makes it difficult to exploit typical pre-trained embeddings such as GloVe).", "labels": [], "entities": []}, {"text": "Therefore, we adopt pretrained embeddings which are specialized for handling non-standard words.", "labels": [], "entities": []}, {"text": "We show that the proposed model largely outperforms the baseline of task organizers by experiments.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of single model approaches on  the test set.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of ensemble approaches on the  test set. 10 models were used for each ensemble result.", "labels": [], "entities": []}]}