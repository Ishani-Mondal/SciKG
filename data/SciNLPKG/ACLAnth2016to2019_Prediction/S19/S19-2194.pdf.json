{"title": [{"text": "Columbia at SemEval-2019 Task 7: Multi-task Learning for Stance Classification and Rumour Verification", "labels": [], "entities": [{"text": "Stance Classification", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.959669440984726}, {"text": "Rumour Verification", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7685241401195526}]}], "abstractContent": [{"text": "The paper presents Columbia team's participation in the SemEval 2019 Shared Task 7: Ru-mourEval 2019.", "labels": [], "entities": [{"text": "SemEval 2019 Shared Task 7: Ru-mourEval 2019", "start_pos": 56, "end_pos": 100, "type": "TASK", "confidence": 0.8300666660070419}]}, {"text": "Detecting rumour on social networks has been a focus of research in recent years.", "labels": [], "entities": [{"text": "Detecting rumour on social networks", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8813047647476197}]}, {"text": "Previous work suffered from data sparsity, which potentially limited the application of more sophisticated neural architecture to this task.", "labels": [], "entities": []}, {"text": "We mitigate this problem by proposing a multi-task learning approach together with language model fine-tuning.", "labels": [], "entities": []}, {"text": "Our attention-based model allows different tasks to leverage different level of information.", "labels": [], "entities": []}, {"text": "Our system ranked 6th overall with an F1-score of 36.25 on stance classification and F1 of 22.44 on rumour verification.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9996894598007202}, {"text": "stance classification", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.9368124008178711}, {"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9997885823249817}]}], "introductionContent": [{"text": "The ubiquity of social media is allowing unverified news and rumours to spread easily.", "labels": [], "entities": []}, {"text": "Efforts have been made to explore automated methods for rumour detection and verification, and has shown promising potential to tackle this issue at scale.", "labels": [], "entities": [{"text": "rumour detection and verification", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.7733801603317261}]}, {"text": "RumourEval 2019 Shared Task 7 tackles the problem of predicting the veracity of rumours and stance of replies.", "labels": [], "entities": [{"text": "RumourEval 2019 Shared Task 7", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.8270098209381104}, {"text": "predicting the veracity of rumours", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.8024738192558288}]}, {"text": "It consists of two subtasks: task A (SDQC), in which stance (support, deny, querying, comment) of responses to a rumourous statement are predicted, and task B (verification), in which the statement's veracity is to be predicted.", "labels": [], "entities": []}, {"text": "Size of training data provided for Task A is 5,217 and for Task B is 327.", "labels": [], "entities": [{"text": "Size", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9877619743347168}]}, {"text": "In this paper, we proposed several methods to alleviate data sparsity and unleash the power of sophisticated neural models: 1.", "labels": [], "entities": []}, {"text": "Jointly learning to perform rumour verification and stance detection.", "labels": [], "entities": [{"text": "rumour verification", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.9212462604045868}, {"text": "stance detection", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.9774249494075775}]}, {"text": "Training a neural * Equal contribution.", "labels": [], "entities": []}, {"text": "network on limited amount of data fora single task is hard, especially in a sentence classification task.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.8254748384157816}]}, {"text": "This is because of the weak supervision signal caused by the information asymmetry between the source text and the target labels.", "labels": [], "entities": []}, {"text": "With supervision signal from multiple tasks, a neural network can exploit information in the training data more thoroughly.", "labels": [], "entities": []}, {"text": "To predict the stance of a post, we want to selectively pay attention to some other posts that are relevant to this post.", "labels": [], "entities": []}, {"text": "We use a QKV-style attention (Query, Key, Value) () to summarize the post context into a single vector (where in practice one attention head is usually enough).", "labels": [], "entities": []}, {"text": "In addition, we use representations at different levels for different tasks.", "labels": [], "entities": []}, {"text": "3. Using language model fine-tuning for stance classification.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.9260967075824738}]}, {"text": "We use the Universal Language Model Fine-tuning (ULMFiT)) to improve our stance classifier.", "labels": [], "entities": []}, {"text": "We begin with a generic language model trained on the Wikitext 103 dataset (.", "labels": [], "entities": [{"text": "Wikitext 103 dataset", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.9413190484046936}]}, {"text": "This dataset consists of a large collection of pre-processed English Wikipedia articles.", "labels": [], "entities": []}, {"text": "This enables the language model to properly model the general properties of language.", "labels": [], "entities": []}, {"text": "Next, we fine-tune this language model on task specific data: RumourEval2019 dataset.", "labels": [], "entities": [{"text": "RumourEval2019 dataset", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.8910864293575287}]}, {"text": "Finally, a classification layer is added and the model is initialized with parameters from the fined-tuned language model.", "labels": [], "entities": []}, {"text": "Our system, which relies on these three key factors, are now publicly available.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of two systems on test set.", "labels": [], "entities": []}]}