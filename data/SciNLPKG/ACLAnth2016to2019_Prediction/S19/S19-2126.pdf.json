{"title": [{"text": "NLPR@SRPOL at SemEval-2019 Task 6: Linguistically enhanced deep learning offensive sentence classifier", "labels": [], "entities": [{"text": "sentence classifier", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7172404527664185}]}], "abstractContent": [{"text": "The paper presents a system developed for the SemEval-2019 competition Task 5 hat-Eval Basile et al.", "labels": [], "entities": [{"text": "SemEval-2019 competition Task 5", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.866290271282196}]}, {"text": "(2019) (team name: LU Team) and Task 6 OffensEval Zampieri et al.", "labels": [], "entities": []}, {"text": "(2019b) (team name: NLPR@SRPOL), where we achieved 2 nd position in Subtask C. The system combines in an ensemble several models (LSTM, Transformer, OpenAI's GPT, Random forest, SVM) with various embeddings (custom, ELMo, fastText, Universal Encoder) together with additional linguistic features (number of blacklisted words, special characters , etc.).", "labels": [], "entities": [{"text": "OpenAI's GPT", "start_pos": 149, "end_pos": 161, "type": "DATASET", "confidence": 0.7992442846298218}]}, {"text": "The system works with a multi-tier blacklist and a large corpus of crawled data, annotated for general offensiveness.", "labels": [], "entities": []}, {"text": "In the paper we do an extensive analysis of our results and show how the combination of features and embedding affect the performance of the models .", "labels": [], "entities": []}], "introductionContent": [{"text": "In 2017 two-thirds of all adults in the United States have experienced some form of online harassment.", "labels": [], "entities": []}, {"text": "This, together with various episodes of online harassment, boosted research on the general problem of recognizing and/or filtering offensive language on the Internet.", "labels": [], "entities": []}, {"text": "Still, recognizing if a sentence expresses hate speech against immigrants or women, understanding if a sentence is offensive to a group of people, an individual or others -these tasks continue to be very difficult for neural networks and machine learning models to accomplish.", "labels": [], "entities": []}, {"text": "In order to do this, various implementations have been proposed; for the most successful recent approaches see;; Wulczyn et al.", "labels": [], "entities": []}, {"text": "Due to the topic of the SemEval-2019 Tasks 5 and 6, the present paper contains offensive expressions spelled out in full.", "labels": [], "entities": [{"text": "SemEval-2019 Tasks", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8775434494018555}]}, {"text": "These are solely illustrations of the problems under consideration.", "labels": [], "entities": []}, {"text": "They should not be interpreted as expressing our views in any way.;;;.", "labels": [], "entities": []}, {"text": "Most of them use various combination of features to recognize these characteristics.", "labels": [], "entities": []}, {"text": "This article presents a system that we have implemented for recognizing if a sentence is offensive.", "labels": [], "entities": []}, {"text": "The system was developed for two SemEval-2019 competition tasks: Task 5 hatEval \"Multilingual detection of hate speech against immigrants and women in Twitter\" (team name: LU Team) and Task 6 OffensEval \"Identifying and categorizing offensive language in social media\" () (team name: NLPR@SRPOL).", "labels": [], "entities": [{"text": "SemEval-2019 competition tasks", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8657132188479105}, {"text": "hatEval", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.7895582318305969}, {"text": "Multilingual detection of hate speech against immigrants and women in Twitter", "start_pos": 81, "end_pos": 158, "type": "TASK", "confidence": 0.8626483245329424}, {"text": "Identifying and categorizing offensive language in social media", "start_pos": 204, "end_pos": 267, "type": "TASK", "confidence": 0.7841409258544445}, {"text": "NLPR@SRPOL", "start_pos": 284, "end_pos": 294, "type": "DATASET", "confidence": 0.6802137096722921}]}, {"text": "shows the results that we achieved with our system in the SemEval-2019 competitions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we give a high level overview of the datasets we used for training our models for the  To each sentence, the linguists assigned one of the three labels: \u2022 OFF -offensive sentence, \u2022 NOT -not offensive sentence, \u2022 Nonsense -random collection of words or non-English (removed from the corpus).", "labels": [], "entities": [{"text": "OFF", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.9972858428955078}, {"text": "NOT", "start_pos": 198, "end_pos": 201, "type": "METRIC", "confidence": 0.8879150152206421}]}, {"text": "In cases of disagreement between linguists, we chose the most popular label, if applicable, or obtained an expert annotation.", "labels": [], "entities": []}, {"text": "We calculated Fleiss' kappa for inter-annotator agreement, which extends Cohen's kappa to more than two raters.", "labels": [], "entities": []}, {"text": "For random ratings Fleiss' \u03ba = 0, while for perfect agreement \u03ba = 1.", "labels": [], "entities": []}, {"text": "Our \u03ba was equal to 0.62, which falls in the \"substantial agreement\" category, according to.", "labels": [], "entities": []}, {"text": "The remaining part of the corpus was assessed automatically with a blacklist-based filter.", "labels": [], "entities": []}, {"text": "Dataset for Task 6 The OLID dataset () contains Offensive and Not Offensive sentences.", "labels": [], "entities": [{"text": "OLID dataset", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.8173193633556366}]}, {"text": "The Offensive sentences are further categorized into: \u2022 TIN -targeted insults and threats, \u2022 UNT -untargeted. and the targeted (TIN) category was further subdivided into: \u2022 IND -individual target, \u2022 GRP -group target, \u2022 OTH -a target that is neither an individual nor a group.", "labels": [], "entities": [{"text": "UNT", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9487826824188232}, {"text": "OTH", "start_pos": 220, "end_pos": 223, "type": "METRIC", "confidence": 0.9319713711738586}]}, {"text": "Our full offensive language corpus, described in the previous subsection, was used for this task.", "labels": [], "entities": []}, {"text": "The OFF sentences were further annotated for the two categories while the NOT sentences were not further annotated.", "labels": [], "entities": [{"text": "OFF sentences", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7711984813213348}, {"text": "NOT sentences", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.7977851331233978}]}, {"text": "All the additional classes were added automatically by a wordlist-based annotator.", "labels": [], "entities": []}, {"text": "Dataset for Task 5 The dataset for Task 5 () contained the classes: \u2022 HATE -hate speech against women or immigrants, \u2022 NOHATE -no hate speech against women or immigrants.", "labels": [], "entities": [{"text": "HATE", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9969262480735779}, {"text": "NOHATE", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9733331799507141}]}, {"text": "Given that we participated in the Task 5 Subtask A, we annotated our corpus only with these two labels.", "labels": [], "entities": []}, {"text": "Using a mixture of automated and manual annotation, we were able to add around 30k sentences from our dataset for this task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Macro F1 for selected Transformer models  with different combinations of features", "labels": [], "entities": []}, {"text": " Table 6: Statistics for our offensive language cor- pus. The Kaggle dataset was collected by Impermium  (2012). The Twitter dataset was compiled from 4  sources: Davidson et al. (2017), Cachola et al. (2018),", "labels": [], "entities": [{"text": "Kaggle dataset", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9498650431632996}]}, {"text": " Table 9: Macro F1 for all the models on all the Tasks and on the state-of-the-art (SOTA) data.", "labels": [], "entities": [{"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8011133074760437}]}]}