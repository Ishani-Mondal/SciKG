{"title": [{"text": "YNU DYX at SemEval-2019 Task 9: A Stacked BiLSTM Model for Suggestion Mining Classification", "labels": [], "entities": [{"text": "SemEval-2019 Task 9", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.5959626833597819}, {"text": "Suggestion Mining Classification", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.9814783136049906}]}], "abstractContent": [{"text": "In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A: Suggestion Mining from Online Reviews and Forums.", "labels": [], "entities": [{"text": "SemEval 2019 Task 9-SubTask A", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.7531709790229797}, {"text": "Suggestion Mining from Online Reviews and Forums", "start_pos": 97, "end_pos": 145, "type": "TASK", "confidence": 0.8798688054084778}]}, {"text": "We use Word2Vec to learn the distributed representations from sentences.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.9547854065895081}]}, {"text": "This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context.", "labels": [], "entities": []}, {"text": "We perform an ensemble to improve the effectiveness of our model.", "labels": [], "entities": []}, {"text": "Our official submission results achieve an F 1-score 0.5659.", "labels": [], "entities": [{"text": "F 1-score 0.5659", "start_pos": 43, "end_pos": 59, "type": "METRIC", "confidence": 0.9768068790435791}]}], "introductionContent": [{"text": "Suggestions in the Oxford Dictionary are defined as ideas or plans for consideration.", "labels": [], "entities": [{"text": "Suggestions", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8653831481933594}, {"text": "Oxford Dictionary", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.9647078514099121}]}, {"text": "Some of the listed synonyms of suggestions are proposal, proposition, recommendation, advice, hint, tip, clue.", "labels": [], "entities": []}, {"text": "In general, other types of text and suggestions are easily distinguished by the definition of the suggestion.", "labels": [], "entities": []}, {"text": "Suggestion mining can be defined as the extraction of suggestions from unstructured text, where the term 'suggestions' refers to the expressions of tips, advice, recommendations etc.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9756775200366974}]}, {"text": "We often see comments on products in product forums which are recommended or not recommended, and some users will consider whether to purchase the product based on these comments.", "labels": [], "entities": []}, {"text": "Suggestion mining is also defined as automatic extraction of recommendations from a given text.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9823252558708191}, {"text": "automatic extraction of recommendations from a given text", "start_pos": 37, "end_pos": 94, "type": "TASK", "confidence": 0.829732283949852}]}, {"text": "These texts that express user suggestions can usually be found in social media platforms, blogs, or product online forums (.", "labels": [], "entities": []}, {"text": "Suggestion mining remains a relatively young area compared to Sentiment Analysis (, due to the lack of a large number of tagged datasets.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9805207550525665}, {"text": "Sentiment Analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9072223007678986}]}, {"text": "SemEval 2019 Task 9-SubTask A is mainly a binary classification, identifying sentences which express suggestions in a given text.", "labels": [], "entities": [{"text": "SemEval 2019 Task 9-SubTask A", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5604655325412751}]}, {"text": "And we need to classify each sentence of given text, the categories being suggestions or non suggestions.", "labels": [], "entities": []}, {"text": "This is similar to the polarity analysis of emotions, as positive or negative instances, respectively.", "labels": [], "entities": []}, {"text": "In the past, classification problems in natural language processing were solved by traditional methods, such as sentiment analysis which were handled by classifiers such as Naive Bayes)and SVMs).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.9429916441440582}]}, {"text": "However, deep neural networks achieve increasing performance compared to traditional methods, due to their ability to learn more abstract features from large amounts of data, producing state-of-the-art results in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 213, "end_pos": 231, "type": "TASK", "confidence": 0.9571380317211151}]}, {"text": "The SubTask-A is part of SemEval 2019 Task9: Suggestion Mining from Online Reviews and Forums, and is concerned with classifying suggestions forum for Windows platform developerssuggestions or non suggestions.", "labels": [], "entities": [{"text": "SemEval 2019 Task9: Suggestion Mining from Online Reviews and Forums", "start_pos": 25, "end_pos": 93, "type": "TASK", "confidence": 0.7456028000874952}]}, {"text": "There are 34 teams who participated in the task(.", "labels": [], "entities": []}, {"text": "In this paper we describe our system designed for this task.", "labels": [], "entities": []}, {"text": "First, we model the sentence and establish the vector representation of the sentence through), a Stacked Bidirectional Long-Short Memory Network(SBiLSTM) for enriching word representations with context.", "labels": [], "entities": []}, {"text": "Finally, the sentence representation is projected into the label space through a Dense Layer.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 provides the details of the proposed model; Data Processing and analysis are discussed in section 3.", "labels": [], "entities": [{"text": "Data Processing", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7236983478069305}]}, {"text": "Experiments and results are described in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we draw conclusions in Sec-tion 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Python based neural network library, Keras 1 , for the implementation.", "labels": [], "entities": []}, {"text": "We train and validate our models on the training and validation sets provided by the organizer.", "labels": [], "entities": []}, {"text": "The official evaluation metric is based on macro average F 1 -score measure.", "labels": [], "entities": [{"text": "macro average F 1 -score measure", "start_pos": 43, "end_pos": 75, "type": "METRIC", "confidence": 0.7914150272096906}]}, {"text": "More details about the data and the evaluation metrics can be found in the task description paper (.", "labels": [], "entities": []}, {"text": "SBiLSTM: For the Stacked BiLSTM, the first layer BiLSTM units = 256, and the second layer BiLSTM units = 180.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.897911012172699}]}, {"text": "Optimization: Optimization is carried outwith Adaptive Moment Estimation(Adam)), using the default learning rate 0.001, and hyperparameters \u03b2 1 = 0.9, \u03b2 2 = 0.999 . Loss Function: Usually the multi-classification problem uses categorical crossentropy as the loss function.", "labels": [], "entities": []}, {"text": "But our system uses binary crossentropy in this binary classification.", "labels": [], "entities": []}, {"text": "As shown in, the number of unknown words in the dataset in Word2Vec are less than those in Glove.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9451653361320496}]}, {"text": "To reduce the number of unknown words in the embedding, making the context semantics better learned by the model.", "labels": [], "entities": []}, {"text": "We randomly assign the vectors of unknown words.", "labels": [], "entities": []}, {"text": "And we experimented with the embedding of the words Word2Vec and Glove, and found that the results of Word2Vec performed better than Glove.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9778791069984436}, {"text": "Word2Vec", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.9200547933578491}]}, {"text": "Glove 394 0.5422 Word2Vec 231 0.5482 We compare the two network structures of Stacked LSTM and Stacked BiLSTM.", "labels": [], "entities": [{"text": "Glove 394 0.5422 Word2Vec 231 0.5482", "start_pos": 0, "end_pos": 36, "type": "DATASET", "confidence": 0.9013235072294871}]}, {"text": "As can be seen from the results in, the performance of the Stacked BiLSTM is better than that of LST-M.", "labels": [], "entities": []}, {"text": "0.55 0.5307 0.60 0.5214 0.65 0.5422 Ensemble 0.5659", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 shows the label dis- tribution for the dataset.", "labels": [], "entities": [{"text": "label dis- tribution", "start_pos": 19, "end_pos": 39, "type": "METRIC", "confidence": 0.7905488014221191}]}]}