{"title": [{"text": "UniMelb at SemEval-2019 Task 12: Multi-model Combination for Toponym Resolution", "labels": [], "entities": [{"text": "SemEval-2019 Task 12", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8007915814717611}, {"text": "Toponym Resolution", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8435665965080261}]}], "abstractContent": [{"text": "This paper describes our submission to SemEval-2019 Task 12 on toponym resolution in scientific papers.", "labels": [], "entities": [{"text": "SemEval-2019 Task 12", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8711678385734558}, {"text": "toponym resolution", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8193866312503815}]}, {"text": "We train separate NER models for toponym detection over text extracted from tables vs. text from the body of the paper, and train another auxiliary model to eliminate mis-detected toponyms.", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8442615270614624}]}, {"text": "For to-ponym disambiguation, we use an SVM clas-sifier with hand-engineered features.", "labels": [], "entities": [{"text": "to-ponym disambiguation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.617752730846405}]}, {"text": "Our best model achieved a strict micro-F1 score of 80.92% and overlap micro-F1 score of 86.88% in the toponym detection subtask, ranking 2nd out of 8 teams on F1 score.", "labels": [], "entities": [{"text": "overlap micro-F1 score", "start_pos": 62, "end_pos": 84, "type": "METRIC", "confidence": 0.9331551790237427}, {"text": "F1 score", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9611865282058716}]}, {"text": "For toponym dis-ambiguation and end-to-end resolution, we officially ranked 2nd and 3rd, respectively.", "labels": [], "entities": [{"text": "toponym dis-ambiguation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7942923605442047}, {"text": "end-to-end resolution", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7431124150753021}]}], "introductionContent": [{"text": "Toponym resolution (TR) refers to the task of automatically assigning geographic references to place names in text, which has applications in question answering and information retrieval tasks, user geolocation prediction (, and historical research (.", "labels": [], "entities": [{"text": "Toponym resolution (TR)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8677781879901886}, {"text": "assigning geographic references to place names in text", "start_pos": 60, "end_pos": 114, "type": "TASK", "confidence": 0.6801978498697281}, {"text": "question answering and information retrieval tasks", "start_pos": 142, "end_pos": 192, "type": "TASK", "confidence": 0.7345816940069199}, {"text": "user geolocation prediction", "start_pos": 194, "end_pos": 221, "type": "TASK", "confidence": 0.7574625015258789}]}, {"text": "This paper describes our system entry to the Toponym resolution in scientific paper task of SemEval.", "labels": [], "entities": [{"text": "Toponym resolution", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8819063901901245}]}, {"text": "The task consists of three subtasks: toponym detection, toponym disambiguation, and end-to-end toponym resolution.", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8465434610843658}, {"text": "toponym disambiguation", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7408243417739868}, {"text": "toponym resolution", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7537012994289398}]}, {"text": "For the toponym detection task, we extract tables from the full text and train separate BiLSTM-ATTN models for each.", "labels": [], "entities": [{"text": "toponym detection", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.9040759205818176}]}, {"text": "For tables, the model captures the horizontal row-wise structure of the table.", "labels": [], "entities": []}, {"text": "For non-table content, the model can capture syntactic and semantic features.", "labels": [], "entities": []}, {"text": "In both cases, we use a deep contextualized word representationELMo () -to represent each token.", "labels": [], "entities": []}, {"text": "After detecting toponyms, we use an organization name detection model to eliminate misdetected toponyms that are actually part of an organization name.", "labels": [], "entities": [{"text": "organization name detection", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6529974738756815}]}, {"text": "For the toponym disambiguation task, we first construct a candidate set by searching toponyms on GeoNames.", "labels": [], "entities": [{"text": "toponym disambiguation task", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.7944450179735819}]}, {"text": "1 Then, we manually construct features based on search results, and finally, train an SVM model to disambiguate the locations.", "labels": [], "entities": []}, {"text": "For the end-to-end resolution task, we pipeline the two aforementioned steps.", "labels": [], "entities": [{"text": "end-to-end resolution", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6575484275817871}]}, {"text": "Our work makes the following contributions: \u2022 we show that training separate models for table and non-table portions of the paper is better than simply training one model over the full text; \u2022 we show that contextualized word representation boosts performance; \u2022 we show that auxiliary organization name recognition model is helpful for toponym detection, and better than training a single named entity recognizer (NER).", "labels": [], "entities": [{"text": "auxiliary organization name recognition", "start_pos": 276, "end_pos": 315, "type": "TASK", "confidence": 0.578988216817379}, {"text": "toponym detection", "start_pos": 337, "end_pos": 354, "type": "TASK", "confidence": 0.8924610912799835}]}, {"text": "shows our workflow on the toponym detection task, which consist of 4 parts: (a) preprocessing, which contains tokenization, table extraction and sentence segmentation; (b) training and inference for the toponym detection model; (c) post-processing, to combine detected words into toponyms; and (d) refinement of the results by incorporating an auxiliary model.", "labels": [], "entities": [{"text": "toponym detection task", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7991523245970408}, {"text": "table extraction", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7471472918987274}, {"text": "sentence segmentation", "start_pos": 145, "end_pos": 166, "type": "TASK", "confidence": 0.7300229370594025}, {"text": "toponym detection", "start_pos": 203, "end_pos": 220, "type": "TASK", "confidence": 0.743074506521225}]}], "datasetContent": [{"text": "The model architecture used for the toponym detection task is depicted in.", "labels": [], "entities": [{"text": "toponym detection task", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8666684230168661}]}, {"text": "We use the Adam () optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.999, = 10 \u22129 and an initial learning rate of 1e \u22123 . A dropout () rate of 0.5 is used to prevent overfitting.", "labels": [], "entities": []}, {"text": "shows the subtask 1 performance (precision, recall and F1 score) of different models on the table and non-table parts.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995379447937012}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9986912608146667}, {"text": "F1 score)", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9829989274342855}]}, {"text": "Strict and overlapping micro measures results are reported.", "labels": [], "entities": []}, {"text": "In the strict measure, model outputs are considered to match with the gold standard annotations if they cover the exact same span of text; whereas in the overlapping measure, the model output is considered to match if it overlap in span with the goldstandard.", "labels": [], "entities": []}, {"text": "From, we see that self-attention improves the results on are further improved by incorporating the auxiliary model.", "labels": [], "entities": []}, {"text": "Finally, the combined model perform is better than a single model on entire articles.", "labels": [], "entities": []}, {"text": "shows the subtask 1 performance of different word representations.", "labels": [], "entities": []}, {"text": "From that, we find that using ELMo representation is much better than using GloVe embeddings.", "labels": [], "entities": []}, {"text": "The reason is that our tokenization method separates many words like I'm, let's, which ELMo can generate a contextualized representation for, while GloVe cannot.", "labels": [], "entities": []}, {"text": "Furthermore, there are many numbers and OOVs in the tables, the GloVe embedding for which is a random 300-dimensional vector that does not provide useful context information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of different word representations", "labels": [], "entities": []}]}