{"title": [{"text": "YNU-HPCC at SemEval-2019 Task 6: Identifying and Categorising Offensive Language on Twitter", "labels": [], "entities": []}], "abstractContent": [{"text": "This document describes the submission of team YNU-HPCC to SemEval-2019 for three Sub-tasks of Task 6: Sub-task A, Sub-task B, and Sub-task C.", "labels": [], "entities": []}, {"text": "We have submitted four systems to identify and categorise offensive language.", "labels": [], "entities": []}, {"text": "The first subsystem is an attention-based 2-layer bidirectional long short-term memory (BiLSTM).", "labels": [], "entities": []}, {"text": "The second subsystem is a voting ensemble of four different deep learning architectures.", "labels": [], "entities": []}, {"text": "The third subsystem is a stacking ensemble of four different deep learning architectures.", "labels": [], "entities": []}, {"text": "Finally, the fourth subsystem is a bidirectional encoder representations from transformers (BERT) model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9692845344543457}]}, {"text": "Among our models, in Sub-task A, our first subsystem performed the best, ranking 16th among 103 teams; in Sub-task B, the second subsystem performed the best, ranking 12th among 75 teams; in Sub-task C, the fourth subsystem performed best, ranking 4th among 65 teams.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identifying offensive language () on Twitter is a particularly challenging task because of the informal and creative writing style, with the improper use of grammar, figurative language, misspellings and slang, etc.", "labels": [], "entities": [{"text": "Identifying offensive language", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9161026080449423}]}, {"text": "In previous attempts of the task, OffensEval was generally tackled using hand-crafted features and/or sentiment lexicons by feeding them to classifiers such as Support Vector Machines (SVM).", "labels": [], "entities": []}, {"text": "These approaches require a laborious feature-engineering process, which may also need domain-specific knowledge, usually resulting in both redundant and missing features.", "labels": [], "entities": []}, {"text": "However, in recent years, artificial neural networks for feature learning have achieved good results in this field).", "labels": [], "entities": [{"text": "feature learning", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.745136946439743}]}, {"text": "SemEval-2019 Task 6 consists of three Subtasks (Symeon Symeonidis, 2017): \u2022 Sub-task A: Offensive language identification; \u2022 Sub-task B: Automatic categorisation of offense types; \u2022 Sub-task C: Offense target identification.", "labels": [], "entities": [{"text": "Offensive language identification", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.7424999872843424}, {"text": "Automatic categorisation of offense types", "start_pos": 137, "end_pos": 178, "type": "TASK", "confidence": 0.7728470087051391}, {"text": "Offense target identification", "start_pos": 194, "end_pos": 223, "type": "TASK", "confidence": 0.685986320177714}]}, {"text": "In this document, we present four systems that competed at).", "labels": [], "entities": []}, {"text": "The first model is a 2-layer BiLSTM, equipped with an attention mechanism.", "labels": [], "entities": []}, {"text": "The second is voting scheme that combines a 2-layer BiLSTM, Capsule Network, 2-layer bidirectional gated recurrent unit, and the first model.", "labels": [], "entities": []}, {"text": "The third model is a stacking scheme that combines a 2-layer BiLSTM, Capsule Network, 2-layer bidirectional gated recurrent unit, and the first model.", "labels": [], "entities": []}, {"text": "In addition, the above three models, for the word representation, we have used the glove vector.", "labels": [], "entities": [{"text": "word representation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7562828958034515}]}, {"text": "The fourth model is BERT-BASE (Jacob Devlin, 2018), which was released last year by Google AI Language.", "labels": [], "entities": [{"text": "BERT-BASE", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9965144991874695}]}, {"text": "The remainder of this document is organised as follows.", "labels": [], "entities": []}, {"text": "The related work is described in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 reports our methodology and data.", "labels": [], "entities": [{"text": "methodology", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9866846203804016}]}, {"text": "Section 4 reports our result.", "labels": [], "entities": []}, {"text": "The conclusions are summarised in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distribution of label combinations in the data", "labels": [], "entities": []}, {"text": " Table 2: Results for Sub-task A", "labels": [], "entities": []}, {"text": " Table 3: Results for Sub-task B", "labels": [], "entities": []}, {"text": " Table 4: Results for Sub-task C", "labels": [], "entities": []}]}