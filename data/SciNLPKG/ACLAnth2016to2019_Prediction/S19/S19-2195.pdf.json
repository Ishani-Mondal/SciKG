{"title": [{"text": "GWU NLP at SemEval-2019 Task 7: Hybrid Pipeline for Rumour Veracity and Stance Classification on Social Media", "labels": [], "entities": [{"text": "GWU NLP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.928392082452774}, {"text": "Rumour Veracity", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7654143571853638}, {"text": "Stance Classification", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.8457557857036591}]}], "abstractContent": [{"text": "Social media plays a crucial role as the main resource news for information seekers online.", "labels": [], "entities": []}, {"text": "However , the unmoderated feature of social media platforms lead to the emergence and spread of un-trustworthy contents which harm individuals or even societies.", "labels": [], "entities": []}, {"text": "Most of the current automated approaches for automatically determining the ve-racity of a rumor are not generalizable for novel emerging topics.", "labels": [], "entities": [{"text": "determining the ve-racity of a rumor", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.6607217540343603}]}, {"text": "This paper describes our hybrid system comprising rules and a machine learning model which makes use of replied tweets to identify the veracity of the source tweet.", "labels": [], "entities": []}, {"text": "The proposed system in this paper achieved 0.435 F-Macro instance classification, and 0.262 F-macro and 0.801 RMSE in rumor verification tasks in Task7 of Se-mEval 2019.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9815298914909363}, {"text": "rumor verification", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.777290016412735}]}], "introductionContent": [{"text": "The number of users who rely on social media to seek daily news and information rises daily, but not all information online is trustworthy.", "labels": [], "entities": []}, {"text": "The unmoderated feature of social media makes the emergence and diffusion of misinformation even more intense.", "labels": [], "entities": []}, {"text": "Consequently, the propagation of misinformation online could harm an individual or even a society.", "labels": [], "entities": []}, {"text": "Most of the current approaches on verifying credibility perform well for the unfold topics which are already verified by a trustworthy resource.", "labels": [], "entities": []}, {"text": "However, the performance suffers when it comes to real life application for dealing with the emerging rumors which are priorly unknown.", "labels": [], "entities": []}, {"text": "Identifying the emerging rumor and veracity of the rumor by relying on previous observations is a challenging task as the new emerging rumor could be entirely new regarding the event, propagation pattern, and also the provenance.", "labels": [], "entities": []}, {"text": "Despite these challenges, many researchers have been studying the generalizable metrics that could be aggregated from the source, replied posts, or network information (.", "labels": [], "entities": []}, {"text": "Our first mission in this paper is to automatically determine the veracity of rumors as part of the SemEval task.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 100, "end_pos": 112, "type": "TASK", "confidence": 0.9108263552188873}]}, {"text": "SemEval is an ongoing shared task for evaluations of computational sentiment analysis systems.", "labels": [], "entities": [{"text": "computational sentiment analysis", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.6289189656575521}]}, {"text": "Task 7 (RumourEval19) () is one of the twelve tasks, consisting of two subtasks.", "labels": [], "entities": []}, {"text": "Task A is about stance orientation of people as supporting, denying, querying or commenting (SDQC) in a rumor discourse and Task B is about the verification of a given rumor.", "labels": [], "entities": [{"text": "stance orientation of people as supporting, denying, querying or commenting (SDQC) in a rumor discourse", "start_pos": 16, "end_pos": 119, "type": "TASK", "confidence": 0.5960863154185446}]}, {"text": "We propose a hybrid model with rules and a neural network machine learning scheme for both tasks.", "labels": [], "entities": []}, {"text": "For task A we rely on the text content of the post, and its parent.", "labels": [], "entities": []}, {"text": "In Task B not only do we aggregate contextual information of the source of the rumor but also using the veracity orientation of the others in the same conversation.", "labels": [], "entities": []}, {"text": "We devise some rules to improve the performance of the model on query, deny, and support cases which are relatively essential classes in the verification tasks.", "labels": [], "entities": []}, {"text": "Integrating the rule-based component we could reach a better performance in both tasks in comparison with a model which only relied on a machine learning approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset provided for this task contains Twitter and Reddit conversation threads associated with rumors about nine different topics on Twitter and thirty different topics on Reddit.", "labels": [], "entities": []}, {"text": "The Ottawa shootings, Charlie Hebdo, the Ferguson unrest, Germanwings crash, and Putin missing are some of the rumors in this dataset.", "labels": [], "entities": [{"text": "Germanwings crash", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9005157649517059}]}, {"text": "The overall size of the data including the development and evaluation set is 65 rumors on Reddit and 37 rumors with 381 conversations on Twitter.", "labels": [], "entities": []}, {"text": "illustrates all the information of underlying replies and source rumor in both social media platforms.", "labels": [], "entities": []}, {"text": "shows the distribution of the tags for both tasks across different platforms.", "labels": [], "entities": []}, {"text": "According to the table, the stance orientation of the rumor conversations varies between Twitter and Reddit.", "labels": [], "entities": []}, {"text": "In general, Reddit users leave more comments than Twitter users and this is regardless of the rumor veracity.", "labels": [], "entities": []}, {"text": "In false rumors Twitter conversations are more oriented toward denial than Reddit's conversations; however, Twitter users support and deny false rumors to relatively the same extent.", "labels": [], "entities": []}, {"text": "Twitter users are more supportive and ask more questions in regards to true rumors than the Reddit users, but they both deny true rumors to almost the same amount.", "labels": [], "entities": []}, {"text": "Interestingly, in both platforms, people question unverified rumors more than true and false rumors.", "labels": [], "entities": []}, {"text": "For the source of conversation, Reddit and Twitter are significantly different.", "labels": [], "entities": []}, {"text": "Regardless of the veracity, the source in Reddit conversations is more skewed to the query than the other stance tags, while Twitter is more toward the support.", "labels": [], "entities": []}, {"text": "Despite some common characteristics Reddit and Twitter users behave differently when it comes to rumors.", "labels": [], "entities": []}, {"text": "Reddit users do not deny the TRUE or UNVERI-FIED rumors and question more when the rumor is false, yet Twitter users support more without any inquiries.", "labels": [], "entities": [{"text": "TRUE or UNVERI-FIED rumors", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.5399924665689468}]}, {"text": "It is worth noting that the conclusions mentioned in this section could only be valid for the data provided and in other conditions the same correlations might not be present.", "labels": [], "entities": []}, {"text": "The shared task dataset is split into training, development and test sets by the SemEval-2019 task organizers.", "labels": [], "entities": []}, {"text": "We conducted and tuned the optimal set of hyperparameters by testing the performance on the development set and the output of the final model on the test set evaluated by the organizers.", "labels": [], "entities": []}, {"text": "The statistics of the dataset are shown in.", "labels": [], "entities": []}, {"text": "In this section, we discuss the experimental results in both tasks.", "labels": [], "entities": []}, {"text": "shows overall and per category results for Task A and B.", "labels": [], "entities": []}, {"text": "The proposed model achieved 0.435 F-Macro instance classification, and 0.262 F-macro and 0.801 RMSE in rumor verification tasks.", "labels": [], "entities": [{"text": "F-Macro instance classification", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.6745265126228333}, {"text": "F-macro", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.963171124458313}, {"text": "RMSE", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9838423728942871}, {"text": "rumor verification", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8161226809024811}]}, {"text": "In overall evaluation, we ranked as the third group in Task B and tenth in Task A out of twenty-five teams.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of source (Src) conversations and  replies (Rep) on Reddit and Twitter in the training, de- velopment and Evaluation sets.", "labels": [], "entities": [{"text": "Number of source (Src) conversations and  replies (Rep)", "start_pos": 10, "end_pos": 65, "type": "METRIC", "confidence": 0.581241692105929}]}, {"text": " Table 2: Accuracy and F score (macro-averaged) results on the development and test sets of Task A and B.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995618462562561}, {"text": "F score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9855317771434784}]}]}