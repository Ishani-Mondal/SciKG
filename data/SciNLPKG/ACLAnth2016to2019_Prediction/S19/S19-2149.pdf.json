{"title": [{"text": "SemEval-2019 Task 8: Fact Checking in Community Question Answering Forums", "labels": [], "entities": [{"text": "Fact Checking in Community Question Answering Forums", "start_pos": 21, "end_pos": 73, "type": "TASK", "confidence": 0.8993327787944249}]}], "abstractContent": [{"text": "We present SemEval-2019 Task 8 on Fact Checking in Community Question Answering Forums, which features two subtasks.", "labels": [], "entities": [{"text": "SemEval-2019 Task 8", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8000743190447489}, {"text": "Fact Checking in Community Question Answering Forums", "start_pos": 34, "end_pos": 86, "type": "TASK", "confidence": 0.9046239001410348}]}, {"text": "Subtask A is about deciding whether a question asks for factual information vs. an opinion/advice vs. just socializing.", "labels": [], "entities": [{"text": "Subtask A", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8396513760089874}]}, {"text": "Subtask B asks to predict whether an answer to a factual question is true, false or not a proper answer.", "labels": [], "entities": []}, {"text": "We received 17 official submissions for subtask A and 11 official submissions for Subtask B. For subtask A, all systems improved over the majority class baseline.", "labels": [], "entities": []}, {"text": "For Subtask B, all systems were below a majority class baseline, but several systems were very close to it.", "labels": [], "entities": []}, {"text": "The leaderboard and the data from the competition can be found at http://competitions.", "labels": [], "entities": []}, {"text": "codalab.org/competitions/20022.", "labels": [], "entities": [{"text": "codalab.org/competitions/20022", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.7671285748481751}]}], "introductionContent": [], "datasetContent": [{"text": "Both subtasks are three-way classification problems.", "labels": [], "entities": []}, {"text": "In subtask A, the questions were to be classified as FACTUAL, OPINION, or SOCIALIZING.", "labels": [], "entities": [{"text": "FACTUAL", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9046100378036499}, {"text": "OPINION", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9618383646011353}]}, {"text": "Similarly, in subtask B there were also three target categories for the answers: FACTUAL -TRUE, FACTUAL -FALSE, and NON-FACTUAL.", "labels": [], "entities": [{"text": "FACTUAL -TRUE", "start_pos": 81, "end_pos": 94, "type": "METRIC", "confidence": 0.6693523128827413}, {"text": "FACTUAL -FALSE", "start_pos": 96, "end_pos": 110, "type": "METRIC", "confidence": 0.6041293541590372}, {"text": "NON-FACTUAL", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.6949687004089355}]}, {"text": "We further scored the submissions based on Accuracy, macro-F1, and average recall (AvgRec).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9994661211967468}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.8913260102272034}, {"text": "AvgRec", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9816092848777771}]}, {"text": "For subtask B, we also report mean average precision (MAP), where the FACTUAL -TRUE instances were considered to be positive, and the remaining ones were negative.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.9339765707651774}, {"text": "FACTUAL -TRUE", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.6991704305013021}]}, {"text": "The official evaluation measure for both subtasks was Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9991395473480225}]}], "tableCaptions": [{"text": " Table 1: Subtask A: Distribution of the factuality la- bels for the questions.", "labels": [], "entities": []}, {"text": " Table 2: Subtask B: Distribution of the factuality la- bels for the answers.", "labels": [], "entities": []}, {"text": " Table 3: Subtask A: Results for question classification based on the official submissions, evaluated on the test set.  (Some teams did not submit system description papers, and thus we have no citations for their systems.)", "labels": [], "entities": [{"text": "question classification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8092558979988098}]}, {"text": " Table 4: Subtask B: Results for answer classification based on the official submissions, evaluated on the test set.", "labels": [], "entities": [{"text": "answer classification", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.9533937573432922}]}]}