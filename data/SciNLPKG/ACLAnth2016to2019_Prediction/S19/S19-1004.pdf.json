{"title": [{"text": "Composition of Sentence Embeddings: Lessons from Statistical Relational Learning", "labels": [], "entities": [{"text": "Statistical Relational Learning", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.8353424270947775}]}], "abstractContent": [{"text": "Various NLP problems-such as the prediction of sentence similarity, entailment, and discourse relations-are all instances of the same general task: the modeling of semantic relations between a pair of textual elements.", "labels": [], "entities": [{"text": "prediction of sentence similarity", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.8646102249622345}]}, {"text": "A popular model for such problems is to embed sentences into fixed size vectors, and use composition functions (e.g. concatenation or sum) of those vectors as features for the prediction.", "labels": [], "entities": []}, {"text": "At the same time, composition of embeddings has been a main focus within the field of Statistical Relational Learning (SRL) whose goal is to predict relations between entities (typically from knowledge base triples).", "labels": [], "entities": [{"text": "Statistical Relational Learning (SRL)", "start_pos": 86, "end_pos": 123, "type": "TASK", "confidence": 0.8486761699120203}]}, {"text": "In this article, we show that previous work on relation prediction between texts implicitly uses compositions from baseline SRL models.", "labels": [], "entities": [{"text": "relation prediction between texts", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.9029629081487656}]}, {"text": "We show that such compositions are not expressive enough for several tasks (e.g. natural language inference).", "labels": [], "entities": []}, {"text": "We build on recent SRL models to address textual relational problems, showing that they are more expressive, and can alleviate issues from simpler compositions.", "labels": [], "entities": []}, {"text": "The resulting models significantly improve the state of the art in both transferable sentence representation learning and relation prediction.", "labels": [], "entities": [{"text": "transferable sentence representation learning", "start_pos": 72, "end_pos": 117, "type": "TASK", "confidence": 0.7143514230847359}, {"text": "relation prediction", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.9160583913326263}]}], "introductionContent": [{"text": "Predicting relations between textual units is a widespread task, essential for discourse analysis, dialog systems, information retrieval, or paraphrase detection.", "labels": [], "entities": [{"text": "Predicting relations between textual units", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.876415205001831}, {"text": "discourse analysis", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7346898466348648}, {"text": "information retrieval", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.7573771774768829}, {"text": "paraphrase detection", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.8796943128108978}]}, {"text": "Since relation prediction often requires a form of understanding, it can also be used as a proxy to learn transferable sentence representations.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.9557749032974243}]}, {"text": "Several tasks that are useful to build sentence representations are derived directly from text structure, without human annotation: sentence order prediction (, the prediction of previous and subsequent sentences, or the prediction of explicit discourse markers between sentence pairs (.", "labels": [], "entities": [{"text": "sentence order prediction", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.7083940505981445}, {"text": "prediction of explicit discourse markers between sentence pairs", "start_pos": 221, "end_pos": 284, "type": "TASK", "confidence": 0.7993964180350304}]}, {"text": "Human labeled relations between sentences can also be used for that purpose, e.g. inferential relations (.", "labels": [], "entities": []}, {"text": "While most work on sentence similarity estimation, entailment detection, answer selection, or discourse relation prediction seemingly uses task-specific models, they all involve predicting whether a relation R holds between two sentences s 1 and s 2 . This genericity has been noticed in the literature before () and it has been leveraged for the evaluation of sentence embeddings within the SentEval framework.", "labels": [], "entities": [{"text": "sentence similarity estimation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7396809458732605}, {"text": "entailment detection", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7888374030590057}, {"text": "answer selection", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.8910130858421326}, {"text": "discourse relation prediction", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.6691848834355673}]}, {"text": "A straightforward way to predict the probability of (s 1 , R, s 2 ) being true is to represent s 1 and s 2 with d-dimensional embeddings h 1 and h 2 , and to compute sentence pair features f (h 1 , h 2 ), where f is a composition function (e.g. concatenation, product, . .", "labels": [], "entities": []}, {"text": "). A softmax classifier g \u03b8 can learn to predict R with those features.", "labels": [], "entities": []}, {"text": "g \u03b8 \u2022 f can be seen as a reasoning based on the content of h 1 and h 2 (.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: -we review composition functions used in textual relational learning and show that they lack expressiveness (section 2); -we draw analogies with existing SRL models (section 3) and design new compositions inspired from SRL (section 4); -we perform extensive experiments to test composition functions and show that some of them can improve the learning of representations and their downstream uses (section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "The SentEval framework () provides a general evaluation for transferable sentence representations, with open source evaluation code.", "labels": [], "entities": [{"text": "transferable sentence representations", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.8124069968859354}]}, {"text": "One only needs to specify a sentence encoder function, and the framework performs classification tasks or relation prediction tasks using cross-validated logistic regression on embeddings or composed sentence embeddings.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7532455623149872}]}, {"text": "Tasks include sentiment analysis, entailment, textual similarity, textual relatedness, and paraphrase detection.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.9662846326828003}, {"text": "paraphrase detection", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.8802763819694519}]}, {"text": "These tasks area rich way to train or evaluate sentence representations since in a triple (s 1 , R, s 2 ), we can see (R, s 2 ) as a label for s 1.", "labels": [], "entities": []}, {"text": "Unfortunately, the relational tasks hard-code the composition function from equation 4.", "labels": [], "entities": []}, {"text": "From our previous analysis, we believe this composition function favors the use of contextual/lexical similarity rather than high-level reasoning and can penalize representations based on more semantic aspects.", "labels": [], "entities": []}, {"text": "This bias could harm research since semantic representation is an important next step for sentence embedding.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7459942698478699}]}, {"text": "Training/evaluation datasets are also arguably flawed with respect to relational aspects since several recent studies ( show that InferSent, despite being state of the art on SentEval evaluation tasks, has poor performance when dealing with asymmetrical tasks and non-additive composition of words.", "labels": [], "entities": []}, {"text": "In addition to providing new ways of training sentence encoders, we will also extend the SentEval evaluation framework with a more expressive composition function when dealing with relational transfer tasks, which improves results even when the sentence encoder was not trained with it.", "labels": [], "entities": []}, {"text": "Our goal is to show that transferable sentence representation learning and relation prediction tasks can be improved when our expressive compositions are used instead of the composition from equation 4.", "labels": [], "entities": [{"text": "transferable sentence representation learning", "start_pos": 25, "end_pos": 70, "type": "TASK", "confidence": 0.7671398818492889}, {"text": "relation prediction", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8431501686573029}]}, {"text": "We train our relational model adaptations on two relation prediction base tasks (T ), one supervised (T = NLI ) and one unsupervised (T = Disc) described below, and evaluate sentence/relation representations on base and transfer tasks using the SentEval framework in order to quantify the generalization capabilities of our models.", "labels": [], "entities": []}, {"text": "Since we use minor modifications of InferSent and SentEval, our experiments are easily reproducible.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Transfer evaluation tasks. N = number of training examples; C = number of classes if applicable. h 1 , h 2  are sentence representations, f m,s a composition function from section 4.", "labels": [], "entities": [{"text": "Transfer evaluation tasks", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8871841828028361}]}, {"text": " Table 3: SentEval and base task evaluation results for the models trained on natural language inference (T = NLI );  AllNLI is used for training. All scores are accuracy percentages, except STS14, which is Pearson correlation  percentage. AVG denotes the average of the SentEval scores.", "labels": [], "entities": [{"text": "AllNLI", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.8039032220840454}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9985644221305847}, {"text": "Pearson correlation  percentage", "start_pos": 207, "end_pos": 238, "type": "METRIC", "confidence": 0.846174955368042}, {"text": "AVG", "start_pos": 240, "end_pos": 243, "type": "METRIC", "confidence": 0.9976369142532349}]}, {"text": " Table 4: SentEval and base task evaluation results for the models trained on discourse connective prediction (T =  Disc). All scores are accuracy percentages, except STS14, which is Pearson correlation percentage. AVG denotes  the average of the SentEval scores.", "labels": [], "entities": [{"text": "discourse connective prediction", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.6701389153798422}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9988735318183899}, {"text": "Pearson correlation percentage", "start_pos": 183, "end_pos": 213, "type": "METRIC", "confidence": 0.9258537491162618}, {"text": "AVG", "start_pos": 215, "end_pos": 218, "type": "METRIC", "confidence": 0.9969336986541748}]}, {"text": " Table 5: Comparison models from previous work. InferSent represents the original results from Conneau et al.  (2017), SkipT is SkipThought from Kiros et al. (2015), and BoW is our re-evaluation of GloVe Bag of Words from  Conneau et al. (2017). AVG denotes the average of the SentEval scores..", "labels": [], "entities": [{"text": "BoW", "start_pos": 170, "end_pos": 173, "type": "METRIC", "confidence": 0.7928729057312012}, {"text": "AVG", "start_pos": 246, "end_pos": 249, "type": "METRIC", "confidence": 0.9966850876808167}]}, {"text": " Table 6: Results for sentence relation tasks using an alternative composition function (f C \u03b2 ,\u2212 ) during evaluation.  AVG denotes the average of the three tasks.", "labels": [], "entities": [{"text": "AVG", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9981392621994019}]}]}