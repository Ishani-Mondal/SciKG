{"title": [{"text": "YNU-HPCC at SemEval-2019 Task 8: Using A LSTM-Attention Model for Fact-Checking in Community Forums", "labels": [], "entities": [{"text": "YNU-HPCC at SemEval-2019 Task 8", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.8390233874320984}]}], "abstractContent": [{"text": "The objective of the task, Fact-Checking in Community Forums , is to determine whether an answer to a factual question is true, false, or whether it even constitutes a proper answer.", "labels": [], "entities": [{"text": "Fact-Checking in Community Forums", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.7994367629289627}]}, {"text": "In this paper, we propose a system that uses along short-term memory with attention mechanism (LSTM-Attention) model to complete the task.", "labels": [], "entities": []}, {"text": "The LSTM-Attention model uses two LSTM(Long Short-Term Memory) to extract the features of the question and answer pair.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.906898021697998}]}, {"text": "Then, each of the features is sequentially composed using the Attention mechanism, con-catenating the two vectors into one.", "labels": [], "entities": []}, {"text": "Finally, the concatenated vector is used as input for the MLP (Multi-Layer Perceptron) and the MLP's output layer uses the softmax function to classify the provided answers into three categories.", "labels": [], "entities": []}, {"text": "This model is capable of extracting the features of the question and answer pair well.", "labels": [], "entities": []}, {"text": "The results show that the proposed system outper-forms the baseline algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many questions pertaining to various fields are posted to QA forums by users everyday, where they collect answers.", "labels": [], "entities": []}, {"text": "However, the answers do not always address the question asked.", "labels": [], "entities": []}, {"text": "Indeed, in some cases, the answer has nothing to do with the question.", "labels": [], "entities": []}, {"text": "There are several reasons why this is the case.", "labels": [], "entities": []}, {"text": "For example, the responder could have misunderstood the question and so provided a wrong answer.", "labels": [], "entities": []}, {"text": "Most QA forums have little control over the quality of the answers posted.", "labels": [], "entities": []}, {"text": "Moreover, in our dynamic world, the true answer was true in the past, but it maybe false now . presents an example from the Qatar Living forum.", "labels": [], "entities": [{"text": "Qatar Living forum", "start_pos": 124, "end_pos": 142, "type": "DATASET", "confidence": 0.9700146913528442}]}, {"text": "In this case, all three answers could be considered to be good since they formally answer the question.", "labels": [], "entities": []}, {"text": "Nevertheless, a1 contains false information, whereas a2 and a3 are correct, as can be established from the official government website.", "labels": [], "entities": []}, {"text": "In this study, we aim to solve the problem of detecting true factual information in online forums.", "labels": [], "entities": [{"text": "detecting true factual information in online forums", "start_pos": 46, "end_pos": 97, "type": "TASK", "confidence": 0.7975145833832877}]}, {"text": "Given a question requesting factual information, the goal is to classify the provided answers into the following categories.", "labels": [], "entities": []}, {"text": "(i) Factual -True: The answer is true and can be proved by cross referencing with an external resource.", "labels": [], "entities": []}, {"text": "(ii) Factual -False: The answer gives a factual response, but it is either false, partially false, or the responder is uncertain about their response.", "labels": [], "entities": []}, {"text": "(iii) Non-Factual: The answer does not provide factual information relevant to the question; it is either an opinion or an advice that cannot be verified.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, various approaches have been proposed for the purposes of fact-checking in community forums (, such as long short-term memory ( . In this paper, we provide an LSTM-Attention model for fact-checking in community question answering forums.", "labels": [], "entities": [{"text": "question answering forums", "start_pos": 241, "end_pos": 266, "type": "TASK", "confidence": 0.7684426108996073}]}, {"text": "In our approach, we use pretrained word vectors for word embedding.", "labels": [], "entities": []}, {"text": "The LSTM layer is used to extract features from the question and answer sentences.", "labels": [], "entities": []}, {"text": "Finally, these features are used by the Attention Mechanism () with a focus on extracting useful information from the features that are significantly relevant to the current output.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as fol- 2 LSTM-Attention Model for Fact-Checking shows the architecture of our model.", "labels": [], "entities": [{"text": "fol- 2 LSTM-Attention", "start_pos": 44, "end_pos": 65, "type": "METRIC", "confidence": 0.6818032264709473}]}, {"text": "First, a sentence is transformed into a feature matrix.", "labels": [], "entities": []}, {"text": "The feature matrix is then passed into the LSTM to extract salient features.", "labels": [], "entities": []}, {"text": "A simple tokenizer is used to transform each sentence into an array of tokens, which constitute the input to the model.", "labels": [], "entities": []}, {"text": "This is then mapped into a feature matrix or sentence matrix by an embedding layer.", "labels": [], "entities": []}, {"text": "The n-gram features are extracted when the feature matrix passes through the LSTM, and the output of the LSTM is passed into the SelfAttention layer.", "labels": [], "entities": []}, {"text": "This layer composes the useful features to output the final regression results by means of a linear decoder.", "labels": [], "entities": []}], "datasetContent": [{"text": "The system was scored based on Accuracy, macro-F1, and AvgRec where the \"Factual -True\" instances were considered to be positive, and the remaining instances to be negative.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9977818131446838}, {"text": "AvgRec", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.950641393661499}]}], "tableCaptions": []}