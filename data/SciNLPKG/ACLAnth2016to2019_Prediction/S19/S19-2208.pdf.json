{"title": [{"text": "DBMS-KU at SemEval-2019 Task 9: Exploring Machine Learning Approaches in Classifying Text as Suggestion or Non-Suggestion", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the participation of DBMS-KU team in the SemEval 2019 Task 9, that is, suggestion mining from online reviews and forums.", "labels": [], "entities": [{"text": "SemEval 2019 Task 9", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8703175932168961}, {"text": "suggestion mining from online reviews and forums", "start_pos": 92, "end_pos": 140, "type": "TASK", "confidence": 0.8352920157568795}]}, {"text": "To deal with this task, we explore several machine learning approaches , i.e., Random Forest (RF), Logistic Regression (LR), Multinomial Naive Bayes (MNB), Linear Support Vector Classification (LSVC), Sublinear Support Vector Classification (SSVC), Convolutional Neural Network (CNN), and Variable Length Chromosome Genetic Algorithm-Naive Bayes (VLCGA-NB).", "labels": [], "entities": [{"text": "Sublinear Support Vector Classification (SSVC)", "start_pos": 201, "end_pos": 247, "type": "TASK", "confidence": 0.753593989780971}]}, {"text": "Our system obtains reasonable results of F1-Score 0.47 and 0.37 on the evaluation data in Subtask A and Subtask B, respectively.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9995884299278259}]}, {"text": "In particular, our obtained results outperform the baseline in Subtask A. Interestingly, the results seem to show that our system could perform well in classifying Non-suggestion class.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nowadays, a huge number of texts are posted in online reviews or discussion forums.", "labels": [], "entities": []}, {"text": "Such media can be a valuable source for obtaining a suggestion about products or services (.", "labels": [], "entities": []}, {"text": "The obtained suggestion is not only useful for readers but also important information for stakeholders ( . Indeed, such advice can be used to improving the quality of products or giving helpful recommendations (.", "labels": [], "entities": []}, {"text": "However, identifying a suggestion from a lot of reviews or comments needs extra effort and time.", "labels": [], "entities": []}, {"text": "Moreover, such online texts are mostly in unstructured form ().", "labels": [], "entities": []}, {"text": "Thus, automatically mining the suggestion from given texts is challenging and significant . Suggestion mining is relatively anew research interest in text classification tasks.", "labels": [], "entities": [{"text": "Suggestion mining", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.9889898002147675}, {"text": "text classification tasks", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.8321581681569418}]}, {"text": "Several studies have initiated to mining suggestions from online texts (.", "labels": [], "entities": []}, {"text": "Particularly, ( have tried to identify suggestions from customer reviews.", "labels": [], "entities": []}, {"text": "Meanwhile,) have mined such advice by using Twitter or discussion forums dataset.", "labels": [], "entities": []}, {"text": "Then, ( have utilized WikiHow and open domain corpora for their work.", "labels": [], "entities": []}, {"text": "However, they concluded that it is not easy to identify suggestion texts automatically.", "labels": [], "entities": []}, {"text": "In other words, it still has room to improving the classification result in the suggestion mining task.", "labels": [], "entities": [{"text": "suggestion mining task", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.8565308252970377}]}, {"text": "The task of suggestion mining from online reviews and forums, namely, Task 9 (, is opened in the International Workshop on Semantic Evaluation 2019.", "labels": [], "entities": [{"text": "suggestion mining", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7184588611125946}]}, {"text": "This paper delineates the participation of DBMS-KU team in both Subtask A and Subtask B of Task 9 of.", "labels": [], "entities": []}, {"text": "To address these two Subtasks, we utilize several approaches, namely, Random Forest (RF), Logistic Regression (LR), Multinomial Naive Bayes (MNB), Linear Support Vector Classification (LSVC), Sublinear Support Vector Classification (SSVC), Convolutional Neural Network (CNN), and Variable Length Chromosome Genetic Algorithm Naive Bayes (VLCGA-NB).", "labels": [], "entities": [{"text": "Sublinear Support Vector Classification (SSVC)", "start_pos": 192, "end_pos": 238, "type": "TASK", "confidence": 0.7480156677109855}]}, {"text": "The obtained results of our experiments are encouraging and show a promising improvement in identifying Suggestion and Non-suggestion.", "labels": [], "entities": [{"text": "Suggestion", "start_pos": 104, "end_pos": 114, "type": "TASK", "confidence": 0.6274281740188599}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 explains the problem definition, problem formulation, and dataset.", "labels": [], "entities": [{"text": "problem formulation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7337570190429688}]}, {"text": "Section 3 presents the tools and libraries used in this work.", "labels": [], "entities": []}, {"text": "Section 4 describes our employed methods.", "labels": [], "entities": []}, {"text": "Section 5 repre-sents our experiments that consist of data preprocessing, parameter, and evaluation measurement.", "labels": [], "entities": []}, {"text": "Section 6 discusses our obtained results.", "labels": [], "entities": []}, {"text": "Finally, we conclude this work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset used in Task 9 of SemEval-2019 is divided into training, trial, and evaluation parts ().", "labels": [], "entities": [{"text": "SemEval-2019", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.8276150822639465}]}, {"text": "The dataset consists of three columns: id, sentence, and label (see).", "labels": [], "entities": []}, {"text": "The provided dataset is imbalanced in which, overall, Non-suggestion class is larger than Suggestion one.", "labels": [], "entities": []}, {"text": "We conducted the experiments with the seven classification methods in this section.", "labels": [], "entities": []}, {"text": "We employed the datasets from both Subtasks for evaluating the performance of those classification methods.", "labels": [], "entities": []}, {"text": "We compared the performance of seven classification methods against the baseline provided by the organizer.", "labels": [], "entities": []}, {"text": "Classifier performance evaluation using accuracy is often considered as a suited measurement.", "labels": [], "entities": [{"text": "Classifier performance evaluation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7450626492500305}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9985596537590027}]}, {"text": "However, the datasets from both subtasks were imbalanced.", "labels": [], "entities": []}, {"text": "Majority class is often reckoned by the classifier, thus, higher accuracy will be achieved for it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9990378618240356}]}, {"text": "Therefore, in this research, we used Precision, Recall, and F1-Score as the main evaluation measures.", "labels": [], "entities": [{"text": "Precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9991180300712585}, {"text": "Recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9889019727706909}, {"text": "F1-Score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9990777969360352}]}, {"text": "Accuracy measurement (Equation) was still used in the fitness function of VLCGA-NB.", "labels": [], "entities": [{"text": "Accuracy measurement (Equation)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.8456199765205383}, {"text": "VLCGA-NB", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9107751250267029}]}, {"text": "F1-Score (Equation) computation relied on Precision (Equation) and Recall (Equation) measurements.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9669614434242249}, {"text": "Precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9959121942520142}, {"text": "Recall (Equation)", "start_pos": 67, "end_pos": 84, "type": "METRIC", "confidence": 0.9297599047422409}]}, {"text": "As the Suggestion results were more concerned, the evaluation of this competition was the Suggestion results' F1-Score.", "labels": [], "entities": [{"text": "Suggestion", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.6843595504760742}, {"text": "F1-Score", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9852860569953918}]}, {"text": "where : Evaluation measurement for VLCGA-NB was done in every generation using Fitness Function based on the result of Naive Bayes classification.", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.925094485282898}, {"text": "VLCGA-NB", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.7147417068481445}]}, {"text": "The Fitness value was found by addition of accuracy, F1-Score of Suggestion and F1-Score of Non-suggestion, which were defined as follows:", "labels": [], "entities": [{"text": "Fitness", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9941720366477966}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9996922016143799}, {"text": "F1-Score of Suggestion", "start_pos": 53, "end_pos": 75, "type": "METRIC", "confidence": 0.8306394616762797}, {"text": "F1-Score", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9975801706314087}]}], "tableCaptions": [{"text": " Table 2: Metrics Report Subtask A", "labels": [], "entities": []}, {"text": " Table 3: Metrics Report Subtask B", "labels": [], "entities": []}]}