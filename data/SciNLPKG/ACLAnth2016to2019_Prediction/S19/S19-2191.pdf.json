{"title": [{"text": "BLCU NLP at SemEval-2019 Task 7: An Inference Chain-based GPT Model for Rumour Evaluation", "labels": [], "entities": [{"text": "BLCU NLP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8941679298877716}, {"text": "SemEval-2019 Task 7", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.5471105674902598}, {"text": "Rumour Evaluation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8793886601924896}]}], "abstractContent": [{"text": "Researchers have been paying increasing attention to rumour evaluation due to the rapid spread of unsubstantiated rumours on social media platforms, including SemEval 2019 task 7.", "labels": [], "entities": [{"text": "rumour evaluation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.9540059268474579}, {"text": "SemEval 2019 task 7", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7451360523700714}]}, {"text": "However, labelled data for learning rumour veracity is scarce, and labels in rumour stance data are highly disproportionate, making it challenging fora model to perform supervised-learning adequately.", "labels": [], "entities": []}, {"text": "We propose an inference chain-based system, which fully utilizes conversation structure-based knowledge in the limited data and expand the training data in minority categories to alleviate class imbalance.", "labels": [], "entities": []}, {"text": "Our approach obtains 12.6% improvement upon the baseline system for subtask A, ranks 1st among 21 systems in sub-task A, and ranks 4th among 12 systems in subtask B.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the universality of the Internet, social media has become the main channel for acquiring and exchanging information.", "labels": [], "entities": []}, {"text": "However, the free flow of information has given rise to the prevalence of rumours, among which fake ones are harmful since they are generally convincing and hard to distinguish.", "labels": [], "entities": []}, {"text": "To address this problem, we need automatic rumour veracity classification on social media.", "labels": [], "entities": [{"text": "rumour veracity classification", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.7805381218592325}]}, {"text": "A large amount of rumour stance instances on social media have been employed to assist the model in making better predictions regards the rumour's veracity.", "labels": [], "entities": []}, {"text": "Rumour stance classification and rumour veracity classification are two subtasks of and SemEval 2019 Task 7 (.", "labels": [], "entities": [{"text": "Rumour stance classification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9462673664093018}, {"text": "rumour veracity classification", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.7988241910934448}, {"text": "SemEval 2019 Task 7", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.6479151248931885}]}, {"text": "Subtask A predicts the stance of a post replying to a rumourous post, in terms of supporting, denying, querying and commenting the rumour.", "labels": [], "entities": []}, {"text": "Subtask B anticipates the veracity of a rumour as true or false given the rumourous post and a set of additional resources.", "labels": [], "entities": []}, {"text": "Apart from variations in models, research in this area mainly focuses on the special characteristics of data coming from social media: conversation structure, rich intrinsic features, skewed distribution toward the comment class in rumour stance data and scarcity of available data for rumour veracity classification.", "labels": [], "entities": [{"text": "rumour veracity classification", "start_pos": 286, "end_pos": 316, "type": "TASK", "confidence": 0.8546964327494303}]}, {"text": "While most pioneering works treated rumour evaluation as a single-tweet task, attempts to utilize the conversation structure included pairing source and replies together to makeup input (, and adopting the full conversation thread as input in the time sequence (.", "labels": [], "entities": [{"text": "rumour evaluation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.9090249836444855}]}, {"text": "With the realization of rich features hidden in tweet contexts, was one of the first who extracted them and combined them with model input.", "labels": [], "entities": []}, {"text": "The feature sets were augmented during the following work.", "labels": [], "entities": []}, {"text": "In trying to acquire more comprehensive information, not only features of the tweet for prediction were taken into consideration, but also features from its conversation context.", "labels": [], "entities": []}, {"text": "To address the class imbalance problem, transformed subtask A into a two-step classification task: they first classified comments and noncomments, and then categorized non-comments into the other three classes.", "labels": [], "entities": []}, {"text": "Finally, in order to makeup for the absence of abundant accessible training data in the rumour veracity classification task, external resources usage such as Wikipedia dumps and news articles was encouraged in both RumourEval contests.", "labels": [], "entities": [{"text": "rumour veracity classification task", "start_pos": 88, "end_pos": 123, "type": "TASK", "confidence": 0.890479326248169}]}, {"text": "In our work, we find that simply taking the whole conversation as input is inadequate.", "labels": [], "entities": []}, {"text": "We have to recognize the role each part assumes in the conversation thread and mark them accordingly in the input.", "labels": [], "entities": []}, {"text": "Instead of filtering features solely based on the system performance, we pre-fer to run a feature selection before adding to the system and choose those that can bring a high deviation degree between data categories.", "labels": [], "entities": []}, {"text": "Following the feature extraction work of Enayet and ElBeltagy (2017), we consider introducing more features from the conversation context to further assist model judgment.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7013248056173325}]}, {"text": "We alleviate class imbalance instance classification by expanding training data in the under-represented classes with pre-screened external data from similar datasets.", "labels": [], "entities": [{"text": "class imbalance instance classification", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.6689589619636536}]}, {"text": "At last, we approach the data insufficiency issue by setting an average length limit and cutting the overlength ones to enlarge training data.", "labels": [], "entities": []}, {"text": "We propose an inference chain-based system for this paper.", "labels": [], "entities": []}, {"text": "A conversation thread starts with a source tweet, and follows by replies, in which each one responds to an earlier one in time sequence.", "labels": [], "entities": []}, {"text": "When we infer the stance of one tweet, the source or earlier replies in the same thread can give abundant additional hints.", "labels": [], "entities": []}, {"text": "Therefore, we take each conversation thread as an inference chain and concentrate on utilizing it to solve the data issues discussed earlier.", "labels": [], "entities": []}, {"text": "Our approach for both tasks is fine-tuned on Generative Pre-trained Transformer (OpenAI GPT) (), a model that has performed well in 9 NLP tasks.", "labels": [], "entities": []}, {"text": "Our work primarily focuses on subtask A rumour stance classification, in which we expand training data from similar datasets, extract features and join separate parts to form input according to their roles in inference chain.", "labels": [], "entities": [{"text": "A rumour stance classification", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6640196740627289}]}, {"text": "For subtask B rumour veracity classification, we apply similar feature extraction and input concatenation process, except for replacing the data expansion step with data slicing.", "labels": [], "entities": [{"text": "subtask B rumour veracity classification", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.7517018854618073}, {"text": "feature extraction", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7462597787380219}]}, {"text": "With the above implementation, our model outperforms all other systems in subtask A and places 4th in subtask B in SemEval 2019.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on data expansion, input format adjustment and word & tweet-level feature adding for subtask A, and perform data slicing and feature adding for subtask B.", "labels": [], "entities": [{"text": "data expansion", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7229473888874054}, {"text": "input format adjustment", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.6022785206635793}, {"text": "word & tweet-level feature adding", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.5755899727344513}, {"text": "data slicing", "start_pos": 131, "end_pos": 143, "type": "TASK", "confidence": 0.7019009739160538}]}, {"text": "The dataset we used for this task is obtained from.", "labels": [], "entities": []}, {"text": "We primarily focus on achieving a higher macro f1 score for both subtasks.", "labels": [], "entities": [{"text": "macro f1 score", "start_pos": 41, "end_pos": 55, "type": "METRIC", "confidence": 0.6366234421730042}]}, {"text": "We also display the comparison of the results on the test set between our official system and the baseline systems provided by the organizer.", "labels": [], "entities": []}, {"text": "The following are the steps we conduct experiments toward creating the system for official submission.", "labels": [], "entities": []}, {"text": "Subtask A Our experiments conduct on the development set for subtask A is shown in Table 2.", "labels": [], "entities": []}, {"text": "With the first input format (A), we achieve an initial result of 53.48%.", "labels": [], "entities": []}, {"text": "Combining wordlevel, tweet-level features (B), and processing data expansion (C) brings an increase of 0.99% and 1.61% respectively.", "labels": [], "entities": []}, {"text": "After converting to the second input format (D), we've seen arise of 2.03% compared to A.", "labels": [], "entities": [{"text": "arise", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9720057845115662}]}, {"text": "Training data expansion (E) is also implemented on this input, but a decrease of 0.86% is observed.", "labels": [], "entities": [{"text": "Training data expansion (E)", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.6719444592793783}]}, {"text": "We suspect that after enhancing the percentage of the source and parent tweet in new input, data dissimilarity brought by the external dataset is aggravated.", "labels": [], "entities": []}, {"text": "However, this inferior position is reversed by employing features.", "labels": [], "entities": []}, {"text": "Word-level features (F) alone bring a 2.04% growth.", "labels": [], "entities": [{"text": "Word-level features (F)", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.5510344564914703}]}, {"text": "Though tweet-level features alone haven't led in any extra increase, adding them together with word-level features (G) produces a result above 56%.", "labels": [], "entities": []}, {"text": "Our final result for subtask A is ensembled on three runs (F1, F2, G) that achieve the best performance on the development set.", "labels": [], "entities": [{"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9932378530502319}]}, {"text": "GPT(2nd input)+DE+WF+TF 0.5631: Ablation results on the development set for subtask A.", "labels": [], "entities": [{"text": "GPT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7754071354866028}, {"text": "DE+WF+TF 0.5631", "start_pos": 15, "end_pos": 30, "type": "METRIC", "confidence": 0.8754748106002808}, {"text": "Ablation", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9964174032211304}]}, {"text": "MacroF means macro f1 score.", "labels": [], "entities": [{"text": "macro f1 score", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.6052224636077881}]}, {"text": "1st input consists of conversation context and target tweet, while a 2nd input constitutes of source tweet, other tweets, parent tweet and target tweet.", "labels": [], "entities": []}, {"text": "WF and TF refer to word-level and tweet-level features respectively.", "labels": [], "entities": [{"text": "TF", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9046303033828735}]}, {"text": "Subtask B Our experiments conducted on the development set for subtask B is shown in.", "labels": [], "entities": []}, {"text": "Word features bring an increase of 3.71%.", "labels": [], "entities": []}, {"text": "Tweet features prove to be disruptive and lead to a drop of up to 11.6%.", "labels": [], "entities": []}, {"text": "So we discard this type of feature in subtask B entirely.", "labels": [], "entities": []}, {"text": "The result we've submitted for subtask B is ensembled on two runs (B) that get the highest values on the development set.", "labels": [], "entities": []}, {"text": "Final Result Our final result on the test set and the comparison with the baseline systems are shown in  Our system is 12.6% higher than the baseline system in macro f1 for subtask A, but 8.3% and 5.6% lower in macro f1 for subtask B compared with BranchLSTM and NileTMRG respectively.", "labels": [], "entities": [{"text": "BranchLSTM", "start_pos": 248, "end_pos": 258, "type": "DATASET", "confidence": 0.8737385869026184}, {"text": "NileTMRG", "start_pos": 263, "end_pos": 271, "type": "DATASET", "confidence": 0.8958964347839355}]}, {"text": "Our system ranks first in subtask A and fourth in subtask B.", "labels": [], "entities": []}, {"text": "Official: Official submission results on the test set for our system and the organizers' baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of tweets between classes before  and after data expansion in the training set for subtask  A.", "labels": [], "entities": []}, {"text": " Table 3: Ablation results on the development set for  subtask B. DS refers to training data slicing. WF and  TF are the same meaning as Table 2.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99483323097229}, {"text": "TF", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9270239472389221}]}, {"text": " Table 4: Official submission results on the test set for  our system and the organizers' baselines.", "labels": [], "entities": []}, {"text": " Table 5: Classification report on the test set for both  subtasks.", "labels": [], "entities": []}, {"text": " Table 6: ESIM results on the development set for both  subtasks.", "labels": [], "entities": []}]}