{"title": [{"text": "Tom Jumbo-Grumbo at SemEval-2019 Task 4: Hyperpartisan News Detection with GloVe vectors and SVM", "labels": [], "entities": [{"text": "Hyperpartisan News Detection", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.7155378858248392}]}], "abstractContent": [{"text": "In this paper, we describe our attempt to learn bias from news articles.", "labels": [], "entities": [{"text": "learn bias from news articles", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7635284423828125}]}, {"text": "From our experiments , it seems that although there is a correlation between publisher bias and article bias, it is challenging to learn bias directly from the publisher labels.", "labels": [], "entities": []}, {"text": "On the other hand, using few manually-labeled samples can increase the accuracy metric from around 60% to near 80%.", "labels": [], "entities": [{"text": "accuracy metric", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.9761267304420471}]}, {"text": "Our system is computationally inexpensive and uses several standard document representations in NLP to train an SVM or LR clas-sifier.", "labels": [], "entities": []}, {"text": "The system ranked 4th in the SemEval-2019 task.", "labels": [], "entities": [{"text": "SemEval-2019 task", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9034899473190308}]}, {"text": "The code is released for repro-ducibility 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Bias is the inclination or prejudice for or against one person or group.", "labels": [], "entities": [{"text": "Bias", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.5225224494934082}]}, {"text": "News articles that contain extreme bias fail to provide fair and multifaceted views for readers and can create polarization within the society (.", "labels": [], "entities": []}, {"text": "A system that can detect bias in news articles is thus relevant, especially in a time where an increasing number of people consume news from online sources that might not be trustful.", "labels": [], "entities": []}, {"text": "The SemEval-2019 task aims to detect hyperpartisan news given the text of the news article, where hyperpartisan news is defined to bean article that overtly favors aside or view.", "labels": [], "entities": []}, {"text": "The details of the task can be found in.", "labels": [], "entities": []}, {"text": "We are provided with a dataset of two parts.", "labels": [], "entities": []}, {"text": "The first part is labeled by the publishers (e.g. if a publisher is decided to be a hyperpartisan source, all its articles are labeled as hyperpartisan), and split into a training and validation set with no overlapping publishers (which we will refer to as training-1 and validation-1).", "labels": [], "entities": []}, {"text": "The second part is crowdsourced and labeled per article (which we will call training-2).", "labels": [], "entities": []}, {"text": "1 https://github.com/chialun-yeh/ SemEval2019 Due to the large number of labeled samples, we decide to use a supervised classification approach, where features are extracted from the text and used to train a classifier.", "labels": [], "entities": []}, {"text": "Bag-of-words (BoW), TFIDF weighting, and n-grams have been shown to be strong baselines ().", "labels": [], "entities": [{"text": "TFIDF weighting", "start_pos": 20, "end_pos": 35, "type": "METRIC", "confidence": 0.8664976060390472}]}, {"text": "Other features such as Part-OfSpeech (POS), counts of sentiment and bias words have also been studied (.", "labels": [], "entities": [{"text": "Part-OfSpeech (POS)", "start_pos": 23, "end_pos": 42, "type": "METRIC", "confidence": 0.8229478895664215}]}, {"text": "Ina similar setting, uses features such as n-gram of characters, readability scores, dictionary, and the ratio of quoted words to separate hyperpartisan news from the mainstream.", "labels": [], "entities": [{"text": "dictionary", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9976497292518616}]}, {"text": "They trained a random forest classifier and achieved an accuracy of 75%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9996476173400879}]}, {"text": "build a neural network to predict the political ideology of news articles to be either left, right or center.", "labels": [], "entities": [{"text": "predict the political ideology of news articles", "start_pos": 26, "end_pos": 73, "type": "TASK", "confidence": 0.6787512983594622}]}, {"text": "They combine information from the headlines, the links within an article, and the content.", "labels": [], "entities": []}, {"text": "They use a CNN for the headlines, a Node2Vec ( to model the links and a hierarchical attention network (HAN) ( to extract features from the content.", "labels": [], "entities": [{"text": "hierarchical attention network (HAN)", "start_pos": 72, "end_pos": 108, "type": "METRIC", "confidence": 0.6436563283205032}]}, {"text": "They compare the model with several baselines, including a BoW LR model, a fully-connected feedforward network, and networks with only the individual components.", "labels": [], "entities": [{"text": "BoW LR", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.7721409499645233}]}, {"text": "Their proposed model performs the best.", "labels": [], "entities": []}, {"text": "However, their system is trained and evaluated on only data with publisher labels.", "labels": [], "entities": []}, {"text": "They randomly split them into training and testing sets, with overlapping publishers.", "labels": [], "entities": []}, {"text": "The main contribution of the paper is two-fold.", "labels": [], "entities": []}, {"text": "First, we analyze the problem of using the dataset labeled by publishers, concluding that it is difficult due to the noisy labels.", "labels": [], "entities": []}, {"text": "Second, we train SVM classifiers with different representations: TFDIF, doc2vec and GloVe pre-trained vectors.", "labels": [], "entities": []}, {"text": "The 300-dimensional GloVe vectors obtain the best crossvalidation accuracy as well as the performance metrics on the official test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9810710549354553}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the data pre-processing.", "labels": [], "entities": []}, {"text": "In section 3, we present the two systems that we devise and explain how one motivates the other.", "labels": [], "entities": []}, {"text": "In section 4, we present the performance of the final system.", "labels": [], "entities": []}, {"text": "We outline our main conclusions and future work in section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Selected publishers with their bias categories and percentage of biased articles in the dataset.", "labels": [], "entities": []}, {"text": " Table 2: Validation accuracy after fine-tuning. Without  (c) means that the training set is not cleaned with the  pre-processing step (c). Cleaning helps improve accu- racy.", "labels": [], "entities": [{"text": "Validation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8492267727851868}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9420812726020813}]}, {"text": " Table 3: Accuracy of different GloVe vector dimen- sions.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9959694147109985}]}, {"text": " Table 4: Accuracy of our model that is trained using  training-2. The majority baseline is 63% accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963786005973816}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9996299743652344}]}]}