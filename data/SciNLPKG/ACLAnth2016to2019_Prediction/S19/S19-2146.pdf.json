{"title": [{"text": "Team Bertha von Suttner at SemEval-2019 Task 4: Hyperpartisan News Detection using ELMo Sentence Representation Convolutional Network", "labels": [], "entities": [{"text": "SemEval-2019 Task", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.848687618970871}, {"text": "Hyperpartisan News Detection", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6849138935407003}]}], "abstractContent": [{"text": "This paper describes the participation of team \"bertha-von-suttner\" in the SemEval2019 task 4 Hyperpartisan News Detection task.", "labels": [], "entities": [{"text": "SemEval2019 task 4 Hyperpartisan News Detection task", "start_pos": 75, "end_pos": 127, "type": "TASK", "confidence": 0.8867139135088239}]}, {"text": "Our system 1 uses sentence representations from averaged word embeddings generated from the pre-trained ELMo model with Convolutional Neural Networks and Batch Normalization for predicting hyperpartisan news.", "labels": [], "entities": [{"text": "predicting hyperpartisan news", "start_pos": 178, "end_pos": 207, "type": "TASK", "confidence": 0.8984818259874979}]}, {"text": "The final predictions were generated from the averaged predictions of an ensemble of models.", "labels": [], "entities": []}, {"text": "With this architecture, our system ranked in first place, based on accuracy, the official scoring metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.999619722366333}]}], "introductionContent": [{"text": "Hyperpartisan news is typically defined as news which exhibits an extremely biased opinion in favour of one side, or unreasoning allegiance to one party (.", "labels": [], "entities": [{"text": "Hyperpartisan news", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.778287947177887}]}, {"text": "SemEval-2019 Task 4 on \"Hyperpartisan News Detection\" () is a document-level classification task which requires building a precise and reliable algorithm to automatically discriminate hyperpartisan news from more balanced stories.", "labels": [], "entities": [{"text": "Hyperpartisan News Detection\"", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6859335601329803}, {"text": "document-level classification task", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.7605528434117635}]}, {"text": "One of the major challenges of this task is that the model must have the ability to adapt to a large range of article sizes.", "labels": [], "entities": []}, {"text": "In one of the training data sets, the by-publisher corpus, the average article length is 796 tokens, but the longest document has 93,714 tokens.", "labels": [], "entities": []}, {"text": "Most state-of-the-art neural network approaches for document classification use a token sequence as network input.", "labels": [], "entities": [{"text": "document classification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7575068771839142}]}, {"text": "This implies either a high computational cost when a very large maximum sequence length is used to fully represent the longest articles, or alternatively potentially a significant loss of information if the sequence length is restricted to a manageable number of initial tokens from the document.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the ELMo Sentence Representation Convolutional (ESRC) Network.", "labels": [], "entities": [{"text": "ELMo Sentence Representation Convolutional (ESRC)", "start_pos": 32, "end_pos": 81, "type": "TASK", "confidence": 0.7195759245327541}]}, {"text": "We first pre-calculate sentence level embeddings as the average of ELMo ( word embeddings for each sentence, and represent the document as a sequence of such sentence embeddings.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.927664041519165}]}, {"text": "We then apply a lightweight convolutional Neural Network (CNN), along with Batch Normalization (BN), to learn the document representations and predict the hyperpartisan classification.", "labels": [], "entities": [{"text": "Batch Normalization (BN)", "start_pos": 75, "end_pos": 99, "type": "METRIC", "confidence": 0.768549120426178}]}, {"text": "Two types of data set have been made available for the task.", "labels": [], "entities": []}, {"text": "The by-publisher corpus contains 750K articles which were automatically classified based on a categorization of the political bias of the news source.", "labels": [], "entities": []}, {"text": "This dataset was split into a training set of 600K articles and a validation set of 150K articles, where all the articles in the validation set originated from sources not in the training set.", "labels": [], "entities": []}, {"text": "The second set, by-article, contains just 645 articles which were labelled manually.", "labels": [], "entities": []}, {"text": "The final evaluation  was carried out on a dataset of 628 articles which were also labelled manually.", "labels": [], "entities": []}, {"text": "We created several models based on the two datasets and evaluated them using cross-validation on the by-article training set (as the final test set was not available to the participants and it was only available fora maximum of three evaluations).", "labels": [], "entities": []}, {"text": "In order to investigate the usefulness of the by-publisher training data for training a model that performs well on the manually annotated by-article corpus, we experimented with various kinds of pre-training and fine-tuning, and found that any kind of use of the by-publisher corpus was actually harmful and decreased the usefulness of the model.", "labels": [], "entities": []}, {"text": "A CNN model which used ELMobased sentence embeddings to represent the article, and was trained on the by-article set only, turned out to outperform all other attempts.", "labels": [], "entities": []}], "datasetContent": [{"text": "The generated ELMo embedding contains three vectors for each word, where each vector corresponds to one of the output layers from the pretrained model.", "labels": [], "entities": []}, {"text": "We average the three vectors to generate word representations which contain morphological and contextual information, and compute the sentence vectors by averaging all the word vectors in each sentence.", "labels": [], "entities": []}, {"text": "We take a maximum of 200 words for each sentence and a maximum of 200 sentences for each article.", "labels": [], "entities": []}, {"text": "If a document has fewer than 200 sentences, we pad the number of sentences out to 200.", "labels": [], "entities": []}, {"text": "Our models are built by using the Keras library with a Tensorflow backend.", "labels": [], "entities": [{"text": "Keras library", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8985495269298553}]}, {"text": "All the results are shown in.", "labels": [], "entities": []}, {"text": "The table shows for each model the accuracy obtained on the by-article training set, and for the submitted models, the by-publisher test set, and the hidden by-article test set (which unlike the other two, was not available to participants).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9994520545005798}]}, {"text": "In order to investigate the correlation between the two datasets, we first built the ESRC-publisher model which is trained on a randomly selected 100K out of the 750K articles from the by-publisher corpus, as it is impractical to generate ELMo embeddings for the entire corpus.", "labels": [], "entities": []}, {"text": "We also fine-tuned the ESRC-publisher model based on the by-article set to obtain the ESRC-publisher-article model by freezing the weights of all but the last layer of the model.", "labels": [], "entities": [{"text": "ESRC-publisher-article", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.8418710231781006}]}, {"text": "Finally we trained the ESRC-article model only on the by-article set, one version without and one version (ESRC-article-BN) with the additional batch normalization (BN) layer.", "labels": [], "entities": [{"text": "ESRC-article-BN", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.8642147183418274}]}, {"text": "The accuracy for the ERC-publisher model is from evaluating on the whole by-article training set, while all other evaluations on the by-article training set were carried out using a 10-fold cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994626641273499}]}, {"text": "However, because of the very limited size of that corpus, the evaluation part of each fold was also used for early stopping and model selection within each fold.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.7805432081222534}]}, {"text": "For the evaluation on the hidden test set, we selected the best three models from the 10-folds, according to the accuracy on the evaluation set of each fold to form an averaged ensemble model, ESRC-article-BN-Ens.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9961838126182556}, {"text": "ESRC-article-BN-Ens", "start_pos": 193, "end_pos": 212, "type": "DATASET", "confidence": 0.9333061575889587}]}, {"text": "For comparison, the table also shows the results for an earlier version of the model, GloVe-article, which used GloVe word embeddings (6 billion words, 300 dimensional) to represent up to the first 400 words of the article and did not use batch normalization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: System comparison (accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9980394244194031}]}]}