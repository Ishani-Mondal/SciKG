{"title": [{"text": "ntuer at SemEval-2019 Task 3: Emotion Classification with Word and Sentence Representations in RCNN", "labels": [], "entities": [{"text": "Emotion Classification", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9178245365619659}, {"text": "RCNN", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.8130922913551331}]}], "abstractContent": [{"text": "In this paper we present our model on the task of emotion detection in textual conversations in SemEval-2019.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7420478761196136}]}, {"text": "Our model extends the Recurrent Convolutional Neural Network (RCNN) by using external fine-tuned word representations and DeepMoji sentence representations.", "labels": [], "entities": []}, {"text": "We also explored several other competitive pre-trained word and sentence representations including ELMo, BERT and InferSent but found inferior performance.", "labels": [], "entities": [{"text": "BERT", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9961162805557251}]}, {"text": "In addition, we conducted extensive sensitivity analysis, which empirically shows that our model is relatively robust to hyper-parameters.", "labels": [], "entities": []}, {"text": "Our model requires no handcrafted features or emotion lexicons but achieved good performance with a micro-F1 score of 0.7463.", "labels": [], "entities": []}], "introductionContent": [{"text": "Emotions are psychological and physiological states generated in humans in reaction to internal or external events.", "labels": [], "entities": []}, {"text": "Messages inhuman conversations inherently convey emotions.", "labels": [], "entities": []}, {"text": "With the rise of social media platforms such as Twitter, as well as chatbots such as Amazon Alexa, there is an emerging need for machines to understand human emotions in conversations, which has a wide range of applications such as opinion analysis in customer support) and providing emotion-aware responses (.) is designed to promote research in this task.", "labels": [], "entities": [{"text": "opinion analysis in customer support", "start_pos": 232, "end_pos": 268, "type": "TASK", "confidence": 0.8018469929695129}]}, {"text": "This task is to detect emotions in textual conversations.", "labels": [], "entities": []}, {"text": "Each conversation is composed of three turns of utterances and the objective is to detect the emotion of the last utterance given the first two utterances as the context.", "labels": [], "entities": []}, {"text": "The emotions in this classification task include happy, sad, angry and others, adapted from the well-known Ekman's six basic emotions: anger, disgust, fear, happiness, sadness, and surprise.", "labels": [], "entities": []}, {"text": "The evaluation criteria is micro-averaged F1 score since the data is extremely unbalanced, as shown in.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9858923256397247}]}, {"text": "In recent years, pre-trained word and sentence representations achieved very competitive performance in many NLP tasks, e.g., fine-tuned word embeddings using distant training and tweet sentence representations DeepMoji () on sentiment analysis, and contextualized word representations BERT (Devlin et al., 2018) on 11 NLP tasks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 226, "end_pos": 244, "type": "TASK", "confidence": 0.8381463587284088}, {"text": "BERT", "start_pos": 286, "end_pos": 290, "type": "METRIC", "confidence": 0.9904623627662659}]}, {"text": "Motivated by these successes, in this task we explored different word and sentence representations.", "labels": [], "entities": []}, {"text": "We then fed these representations into a Recurrent Convolutional Neural Network (RCNN)) for classification.", "labels": [], "entities": []}, {"text": "RCNN includes a Long short-term memory (LSTM) network) to capture word ordering information and a max-pooling layer () to learn discriminative features.", "labels": [], "entities": [{"text": "RCNN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8616846799850464}, {"text": "word ordering information", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.7504541973272959}]}, {"text": "We also experimented LSTM and CNN in our preliminary analysis but achieved worse performance as compared to RCNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.7188873291015625}, {"text": "RCNN", "start_pos": 108, "end_pos": 112, "type": "DATASET", "confidence": 0.9351218938827515}]}, {"text": "Our final system adopted fine-tuned word embeddings and DeepMoji as our choices of word and sentence representations, respectively, due to their superior performance on the validation dataset.", "labels": [], "entities": []}, {"text": "The code is publicly available at Github 1 .", "labels": [], "entities": [{"text": "Github 1", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8638401031494141}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Total number of conversations and their distributions over each emotion class for each dataset split.  Average number of tokens per utterance for each dataset split are also reported.", "labels": [], "entities": []}, {"text": " Table 2: Micro-F1 score on the test set using different  word representations.", "labels": [], "entities": []}, {"text": " Table 3: Micro-F1 score on the test set using different  sentence representations.", "labels": [], "entities": []}]}