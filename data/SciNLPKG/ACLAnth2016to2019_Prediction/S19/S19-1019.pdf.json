{"title": [{"text": "Acquiring Structured Temporal Representation via Crowdsourcing: A Feasibility Study", "labels": [], "entities": [{"text": "Acquiring Structured Temporal Representation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7290333583950996}]}], "abstractContent": [{"text": "Temporal Dependency Trees area structured temporal representation that represents temporal relations among time expressions and events in a text as a dependency tree structure.", "labels": [], "entities": []}, {"text": "Compared to traditional pair-wise temporal relation representations, temporal dependency trees facilitate efficient annotations, higher inter-annotator agreement, and efficient computations.", "labels": [], "entities": []}, {"text": "However, annotations on temporal dependency trees so far have only been done by expert annotators, which is costly and time-consuming.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a method to crowdsource temporal dependency tree annotations, and show that this representation is intuitive and can be collected with high accuracy and agreement through crowdsourc-ing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.996208667755127}]}, {"text": "We produce a corpus of temporal dependency trees, and present a baseline temporal dependency parser, trained and evaluated on this new corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Temporal relation extraction is an important NLP task fora range of downstream applications, such as question answering, summarization, and storyline generation.", "labels": [], "entities": [{"text": "Temporal relation extraction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9289033810297648}, {"text": "question answering", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.9249810874462128}, {"text": "summarization", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.9656299352645874}, {"text": "storyline generation", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.8471819162368774}]}, {"text": "This task has attracted a significant amount of research interest.", "labels": [], "entities": []}, {"text": "One practical challenge in temporal relation extraction is to represent the temporal relations in a text in away that is feasible for manual annotation and producing training data for machine learning models.", "labels": [], "entities": [{"text": "temporal relation extraction", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6828228036562601}]}, {"text": "Given a text of n events and time expressions, there are n 2 possible relations if the temporal relation between all pairs of events and time expressions is annotated.", "labels": [], "entities": []}, {"text": "This quickly becomes infeasible even fora text of modest length.", "labels": [], "entities": []}, {"text": "One way to address this problem is to represent the temporal relations in a text as a Temporal Dependency Tree (TDT) structure).", "labels": [], "entities": []}, {"text": "TDT models all time expressions and events in a text as \"nodes\" in a dependency tree, and temporal relations between each time/event and its parent time/event as \"edges\" in the tree.", "labels": [], "entities": []}, {"text": "gives an example text and its TDT.", "labels": [], "entities": [{"text": "TDT", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.47892051935195923}]}, {"text": "Each (parent, child) pair in is annotated with a temporal relation.", "labels": [], "entities": []}, {"text": "The number of temporal relations that need to be annotated in a text is therefore linear to the number of events and time expressions in a text, making the annotation task feasible.", "labels": [], "entities": []}, {"text": "At the same time, additional temporal relations can be inferred as needed based on the TDT structure.", "labels": [], "entities": []}, {"text": "For example, in since \"1918\" includes the \"born\" event and \"1929\" includes the \"won\" event, it can be inferred that the \"born\" event occurred before the \"won\" event.", "labels": [], "entities": []}, {"text": "By providing annotators with detailed guidelines and training them in multiple iterations, have shown that the TDT representation can be annotated with high interannotator agreement.", "labels": [], "entities": []}, {"text": "further show that a neural ranking model can be successfully trained on the corpus.", "labels": [], "entities": []}, {"text": "However, this \"traditional\" approach to annotation is timeconsuming and expensive.", "labels": [], "entities": []}, {"text": "The question we want to answer in this paper is whether TDT can be performed with crowdsourcing, a method that has gained popularity as a means to acquire linguistically annotated data quickly and cost-effectively for NLP research.", "labels": [], "entities": [{"text": "TDT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9767062664031982}]}, {"text": "Crowdsourcing has been used to annotate data fora wide range of NLP tasks that include question answering, word similarity, text entailment, word sense disambiguation, machine translation, information extraction, summarization, and semantic role labeling (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8414286971092224}, {"text": "word similarity", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.7173917144536972}, {"text": "text entailment", "start_pos": 124, "end_pos": 139, "type": "TASK", "confidence": 0.6948556154966354}, {"text": "word sense disambiguation", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.6708747347195944}, {"text": "machine translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.8147658407688141}, {"text": "information extraction", "start_pos": 189, "end_pos": 211, "type": "TASK", "confidence": 0.8287624418735504}, {"text": "summarization", "start_pos": 213, "end_pos": 226, "type": "TASK", "confidence": 0.9879595041275024}, {"text": "semantic role labeling", "start_pos": 232, "end_pos": 254, "type": "TASK", "confidence": 0.6318208575248718}]}, {"text": "The key to acquiring high quality data via crowdsourcing is to make sure that the tasks are intuitive or can be decomposed into intuitive subtasks.", "labels": [], "entities": []}, {"text": "In this paper, we show that it is possible to acquire high quality temporal dependency structures through crowdsourcing, and that a temporal dependency parser can be successfully trained on crowdsourced TDTs.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first explain in detail how we setup this dependency tree crowdsourcing annotation task ( \u00a72).", "labels": [], "entities": []}, {"text": "In ( \u00a73) we present experimental results that show that if temporal dependency structures are broken into smaller subtasks, high inter-annotator agreement can be achieved.", "labels": [], "entities": []}, {"text": "In ( \u00a74), we show that crowdsource data can be used to successfully train temporal dependency parsers, including an attentionbased neural model ( \u00a74).", "labels": [], "entities": [{"text": "temporal dependency parsers", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.6905384063720703}]}, {"text": "We discuss related work ( \u00a75) and conclude with future work ( \u00a76).", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: we introduce an effective approach to crowdsource structured temporal annotations, a relatively complex annotation task; (2) we build an English temporal dependency tree corpus through crowdsourcing that we plan to make publicly available; and (3) we experiment with automatic temporal dependency parsers on this new corpus and report competitive results.", "labels": [], "entities": []}, {"text": "To facilitate quality control in crowdsourcing and agreement evaluation, we distinguish two subsets of the TimeBank dataset: (1) TB-small is a small subset of 10 short Wall Street Journal news documents with 59 matrix verbs.", "labels": [], "entities": [{"text": "agreement evaluation", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7710826396942139}, {"text": "TimeBank dataset", "start_pos": 107, "end_pos": 123, "type": "DATASET", "confidence": 0.9732317626476288}, {"text": "Wall Street Journal news documents", "start_pos": 168, "end_pos": 202, "type": "DATASET", "confidence": 0.8384491682052613}]}, {"text": "(2) TBdense consists of the same 36 documents as in the TimeBank-Dense corpus).", "labels": [], "entities": [{"text": "TimeBank-Dense corpus", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.9811230003833771}]}, {"text": "It contains 654 matrix verbs.", "labels": [], "entities": []}, {"text": "TB-small and TBdense are annotated by both crowd workers and experts.", "labels": [], "entities": [{"text": "TB-small", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9219889640808105}, {"text": "TBdense", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.8049795627593994}]}], "datasetContent": [{"text": "Crowdsourcing annotations on the full TimeBank corpus was performed.", "labels": [], "entities": [{"text": "TimeBank corpus", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9736701548099518}]}, {"text": "We report Inter-Annotator Agreement (IAA) scores in First, crowdsourced majority annotations on TB-dense are evaluated against expert annotations, representing the quality of the crowdsourced data.", "labels": [], "entities": []}, {"text": "For this comparison, the standard dependency parsing evaluation metrics) are used as our IAA scores: structureonly annotation subtask is evaluated with the Unlabeled Attachment Agreement (UAA) score, relation-only annotation subtask is evaluated with the Label Only Agreement (LOA) score, and full pipeline annotation is evaluated with the Labeled Attachment Agreement (LAA) score.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7465499043464661}]}, {"text": "Second, crowd worker annotations are compared against each other, indicating the difficulty, consistency, and confidence of the crowdsourced data.", "labels": [], "entities": [{"text": "consistency", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9942817687988281}]}, {"text": "Since crowd workers annotate isolated events/times instead of full dependency structures, the standard dependency parsing metrics are not applicable for this comparison . Therefore, we adopt the Worker Agreements With Aggregate (WAWA) metric (Ning et al., 2018a) as our IAA scores.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.6936510503292084}, {"text": "IAA", "start_pos": 270, "end_pos": 273, "type": "METRIC", "confidence": 0.5717898607254028}]}, {"text": "WAWA indicates the average number of crowd worker responses agreed with the aggregate answer (i.e. majority aggregation for each annotation instance), representing the agreements among crowd workers and how consistent their annotations are with each other.", "labels": [], "entities": [{"text": "WAWA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7064074277877808}]}, {"text": "As shown in the table, high accuracies and agreements are achieved for both the subtasks of structure annotation and relation-only annotation (above 80%).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9957181811332703}]}, {"text": "Statistics on our corpus and other similar TimeBank-based temporal relation corpora are presented in.", "labels": [], "entities": [{"text": "TimeBank-based temporal relation corpora", "start_pos": 43, "end_pos": 83, "type": "DATASET", "confidence": 0.8177565783262253}]}, {"text": "As the number of temporal relations is linear to the number of events and time expressions in a text, fewer temporal relations need to be annotated in our corpus.", "labels": [], "entities": []}, {"text": "In comparison, the recently crowdsourced temporal structure corpus MATRES (, see Section  We experiment with a state-of-the-art attentionbased neural temporal dependency parser (Zhang and Xue, 2018a) 2 on our newly annotated data.", "labels": [], "entities": [{"text": "attentionbased neural temporal dependency parser", "start_pos": 128, "end_pos": 176, "type": "TASK", "confidence": 0.5935559153556824}]}, {"text": "Our training data consists of two parts.", "labels": [], "entities": []}, {"text": "The first part is the crowdsourced temporal dependency annotations over the TimeBank documents (excluding documents that are in the dev and test sets in the TimeBank-Dense corpus 3 ).", "labels": [], "entities": [{"text": "TimeBank documents", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.9779025614261627}, {"text": "TimeBank-Dense corpus 3", "start_pos": 157, "end_pos": 180, "type": "DATASET", "confidence": 0.9514880776405334}]}, {"text": "The second part is our expert-annotated TDTs on the TimeBankDense training set documents.", "labels": [], "entities": [{"text": "TimeBankDense training set documents", "start_pos": 52, "end_pos": 88, "type": "DATASET", "confidence": 0.9837387204170227}]}, {"text": "The parser is tuned and evaluated on our expert TDT annotations on the TimeBank-Dense dev and test sets, respectively.", "labels": [], "entities": [{"text": "TimeBank-Dense dev and test sets", "start_pos": 71, "end_pos": 103, "type": "DATASET", "confidence": 0.9101943612098694}]}, {"text": "This neural model represents words with bi-LSTM vectors and uses an attention-based mechanism to represent multi-word time expressions and events.", "labels": [], "entities": []}, {"text": "We also experiment with two baseline parsers from Zhang and Xue (2018a): (1) a simple baseline that takes an event's immediate previous time expression or event as its parent and assigns the majority \"overlap\" as the temporal relation between them; and (2) a logistic regression model that represents time expressions and events with their time/event type features, lexical features, and distance features.", "labels": [], "entities": []}, {"text": "shows the performance of these systems on our data.: Parsing results of the simple baseline, logistic regression baseline, and the neural temporal dependency model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-Annotator Agreement scores between  crowdsourced and expert annotations, and IAAs among  crowd worker annotations.", "labels": [], "entities": [{"text": "IAAs", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9459342956542969}]}, {"text": " Table 2. As the number of tempo- ral relations is linear to the number of events and  time expressions in a text, fewer temporal relations  need to be annotated in our corpus. In compari- son, the recently crowdsourced temporal structure  corpus MATRES (", "labels": [], "entities": []}, {"text": " Table 2: Documents, timex, events, and temporal rela- tion statistics in various temporal corpora.", "labels": [], "entities": []}, {"text": " Table 3: Parsing results of the simple baseline, logis- tic regression baseline, and the neural temporal depen- dency model.", "labels": [], "entities": []}]}