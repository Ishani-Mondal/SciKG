{"title": [{"text": "Probing What Different NLP Tasks Teach Machines about Function Word Comprehension", "labels": [], "entities": [{"text": "NLP Tasks Teach Machines about Function Word Comprehension", "start_pos": 23, "end_pos": 81, "type": "TASK", "confidence": 0.5690470412373543}]}], "abstractContent": [{"text": "We introduce a set of nine challenge tasks that test for the understanding of function words.", "labels": [], "entities": [{"text": "understanding of function words", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.8793735206127167}]}, {"text": "These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words).", "labels": [], "entities": []}, {"text": "Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7156268060207367}]}, {"text": "Our results show that pretraining on language mod-eling performs the best on average across our probing tasks, supporting its widespread use for pretraining state-of-the-art NLP models, and CCG supertagging and NLI pretraining perform comparably.", "labels": [], "entities": []}, {"text": "Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives , e.g., that NLI helps the comprehension of negation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many recent advances in NLP have been driven by new approaches to representation learningi.e., the design of models whose primary aim is to yield representations of words or sentences that are useful fora range of downstream applications).", "labels": [], "entities": []}, {"text": "Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.92862269282341}]}, {"text": "Varying these factors can significantly impact performance on abroad range of NLP tasks (.", "labels": [], "entities": []}, {"text": "This paper investigates the role of pretraining objectives of sentence encoders, with respect to their capacity to understand function words (e.g., prepositions, conjunctions).", "labels": [], "entities": []}, {"text": "Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant.", "labels": [], "entities": []}, {"text": "We ask whether the linguistic properties implicitly captured by pretraining objectives measurably affect the types of linguistic information encoded in the learned representations.", "labels": [], "entities": []}, {"text": "To this end, we explore whether qualitatively different objectives lead to demonstrably different sentence representations.", "labels": [], "entities": []}, {"text": "We focus our analysis on function words because they play a key role in compositional meaning-e.g., introducing and identifying discourse referents or representing relationships between entities or ideas-and are not yet considered to be well-modeled by distributional semantics ().", "labels": [], "entities": []}, {"text": "Our results suggest that different pretraining objectives give rise to differences in function word comprehension; for instance, we see that natural language inference helps understanding negation, and CCG supertagging helps recognizing meaningful sentence boundaries.", "labels": [], "entities": []}, {"text": "However, overall, we find that the observed differences are not always straightforwardly interpretable, and further investigation is needed to determine which specific aspects of pretraining tasks yield good representations of function words.", "labels": [], "entities": []}, {"text": "The analyses we present contribute new results in an ongoing line of research aimed at providing a finer-grained understanding of what neural networks capture about linguistic structure (.", "labels": [], "entities": []}, {"text": "Our contributions are: To reach . .", "labels": [], "entities": []}, {"text": "turn left up a small alleyway \u2192 do not turn right up the alleyway . .", "labels": [], "entities": []}, {"text": "turn left up a small alleyway \u2192 Turn right up the alleyway . .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: MNLI development set accuracy for each pre- trained model.", "labels": [], "entities": [{"text": "MNLI development", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6425072848796844}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9236314296722412}]}]}