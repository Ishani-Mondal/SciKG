{"title": [{"text": "MCScript2.0: A Machine Comprehension Corpus Focused on Script Events and Participants", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce MCScript2.0, a machine comprehension corpus for the end-to-end evaluation of script knowledge.", "labels": [], "entities": []}, {"text": "MCScript2.0 contains approx. 20,000 questions on approx. 3,500 texts, crowdsourced based on anew collection process that results in challenging questions.", "labels": [], "entities": [{"text": "MCScript2.0", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8831486105918884}]}, {"text": "Half of the questions cannot be answered from the reading texts, but require the use of commonsense and, in particular, script knowledge.", "labels": [], "entities": []}, {"text": "We give a thorough analysis of our corpus and show that while the task is not challenging to humans, existing machine comprehension models fail to perform well on the data, even if they make use of a commonsense knowledge base.", "labels": [], "entities": []}], "introductionContent": [{"text": "People have access to a wide range of commonsense knowledge that is naturally acquired during their lifetime.", "labels": [], "entities": []}, {"text": "They make frequent use of such knowledge while speaking to each other, which makes communication highly efficient.", "labels": [], "entities": []}, {"text": "The conversation in Example 1 illustrates this: For Max, it is natural to assume that Rachel paid during her restaurant visit, even if this fact was not mentioned by Rachel.", "labels": [], "entities": []}, {"text": "(1) Rachel: \"Yesterday, I went to this new Argentinian restaurant to have dinner.", "labels": [], "entities": []}, {"text": "I enjoyed a tasty steak.\"", "labels": [], "entities": []}, {"text": "Max: \"What did you pay?\"", "labels": [], "entities": []}, {"text": "Script knowledge is one of the most important types of commonsense knowledge and subsumes such information (.", "labels": [], "entities": []}, {"text": "It is defined as knowledge about everyday situations, also referred to as scenarios.", "labels": [], "entities": []}, {"text": "It subsumes information about the actions that take place during such situations, and their typical temporal order, referred to as events, as well as the persons and objects that typically play a role in the situation, referred to as participants.", "labels": [], "entities": []}, {"text": "In the example, script knowledge subsumes the fact that the paying event is apart of the eating in a restaurant scenario.", "labels": [], "entities": []}, {"text": "Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections, and the manual induction of script knowledge via crowdsourcing (.", "labels": [], "entities": []}, {"text": "Script knowledge has been represented both symbolically and with neural models.", "labels": [], "entities": []}, {"text": "Scripts have been evaluated mostly intrinsically ().", "labels": [], "entities": []}, {"text": "An exception is MCScript (), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work.", "labels": [], "entities": []}, {"text": "Previous work has shown, however, that script knowledge is not required for performing well on the data set (.", "labels": [], "entities": []}, {"text": "Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated text understanding.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.6893172860145569}]}, {"text": "Our work closes this gap: We present MCScript2.0, a reading comprehension corpus focused on script events and participants.", "labels": [], "entities": []}, {"text": "It contains more than 3,400 texts about everyday scenarios, together with more than 19,000 multiple-choice questions on these texts.", "labels": [], "entities": []}, {"text": "All data were collected via crowdsourcing.", "labels": [], "entities": []}, {"text": "About half of the questions require the use of commonsense and script knowledge for finding the correct answer (like the question in Example 1), a notably higher number than in MCScript.", "labels": [], "entities": []}, {"text": "We show that in comparison to MCScript, commonsense-based questions in MCScript2.0 are T (...)", "labels": [], "entities": [{"text": "T", "start_pos": 87, "end_pos": 88, "type": "METRIC", "confidence": 0.9826944470405579}]}, {"text": "We put our ingredients together to make sure they were at the right temperature, preheated the oven, and pulled out the proper utensils.", "labels": [], "entities": []}, {"text": "We then prepared the batter using eggs and some other materials we purchased and then poured them into a pan.", "labels": [], "entities": []}, {"text": "After baking the cake in the oven for the time the recipe told us to, we then double checked to make sure it was done by pushing a knife into the center.", "labels": [], "entities": []}, {"text": "We saw some crumbs sticking to the knife when we pulled it out so we knew it was ready to eat ! Q1 When did they put the pan in the oven and bake it according to the instructions?", "labels": [], "entities": []}, {"text": "Q2 What did they put in the oven?", "labels": [], "entities": []}, {"text": "also harder to answer, even fora model that makes use of a commonsense database.", "labels": [], "entities": []}, {"text": "Thus, we argue that MCScript2.0 is the first resource which makes it possible to evaluate how far models are able to exploit script knowledge for automated text understanding.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.682770699262619}]}, {"text": "shows a text snippet from a text in MCScript2.0, together with two questions with answer alternatives . To find an answer for question 1, information about the temporal order of the steps for baking a cake is required: The cake is put in the oven after mixing the batter, and not after eating it-a piece of information not given in the text, since the event of putting the cake in the oven is not explicitly mentioned.", "labels": [], "entities": []}, {"text": "Similarly, one needs script knowledge about which participants are typically involved in which events to know that the cake mix rather than the utensils is put into the oven.", "labels": [], "entities": []}, {"text": "Both incorrect answer candidates are distractive: The utensils as well as the action of eating the cake are mentioned in the text, but wrong answers to the question.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We present anew collecting method for challenging questions whose answers require commonsense knowledge and in particular script knowledge, as well as anew resource that was created with this method.", "labels": [], "entities": []}, {"text": "1 More text samples are given in the Supplemental Material.", "labels": [], "entities": [{"text": "Supplemental Material", "start_pos": 37, "end_pos": 58, "type": "DATASET", "confidence": 0.8036847412586212}]}, {"text": "\u2022 We show that the task is simple for humans, but that existing benchmark models, including a top-scoring machine comprehension model that utilizes a resource for commonsense knowledge, struggle on the questions in MCScript2.0; especially on questions that require commonsense knowledge.", "labels": [], "entities": []}, {"text": "\u2022 We compare MCScript2.0 to MCScript, the first machine comprehension resource for evaluating models of script knowledge.", "labels": [], "entities": []}, {"text": "We show that in comparison to MCScript, the number of questions that require script knowledge is increased by a large margin and that such questions are hard to answer.", "labels": [], "entities": []}, {"text": "Consequently, we argue that our dataset provides a more robust basis for future research on text understanding models that use script knowledge.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8472012579441071}]}], "datasetContent": [{"text": "MCScript () is the first machine comprehension dataset designed to evaluate script knowledge in an end-to-end machine comprehension application, and to our knowledge the only other existing extrinsic evaluation dataset for script knowledge.", "labels": [], "entities": []}, {"text": "Recent research has shown, however, that commonsense knowledge is not required for good performance on the dataset (.", "labels": [], "entities": []}, {"text": "We argue that this is due to the way in which questions were collected.", "labels": [], "entities": []}, {"text": "During the collection process, workers were not shown a text, but only a very short description of the text scenario.", "labels": [], "entities": []}, {"text": "As a result, many questions ask about general aspects of the scenario, without referring to actual details.", "labels": [], "entities": []}, {"text": "This leads to the problem that there are many questions with standardized answers, i.e. questions that can be answered irrespective of a concrete reading text.", "labels": [], "entities": []}, {"text": "Examples 2 and 3 show two such cases, where the correct answer is almost exclusively in the shower and on the stove, independent of the text or even scenario.", "labels": [], "entities": []}, {"text": "(2) Where did they wash their hair?", "labels": [], "entities": []}, {"text": "(3) Where did they make the scrambled eggs?", "labels": [], "entities": []}, {"text": "found that such information can essentially be learned from only the training data, using a simple logistic regression classifier and surface features regarding words in the text, question and answer candidates.", "labels": [], "entities": []}, {"text": "Also, many questions require vague inference over general commonsense knowledge rather than script knowledge.", "labels": [], "entities": []}, {"text": "Example 4 illustrates this: The simple fact that planting a tree gets easier if you have help is not subsumed by script knowledge about planting a tree, but a more general type of commonsense knowledge.", "labels": [], "entities": []}, {"text": "We inspected a random sample of 50 questions from the publicly available development set that were misclassified by the logistic model of.", "labels": [], "entities": []}, {"text": "We found that for over 90% of the inspected questions, the use of script knowledge would be only marginally relevant.", "labels": [], "entities": []}, {"text": "We present anew data collection method and corpus that results in a larger number of challenging questions that require script knowledge.", "labels": [], "entities": []}, {"text": "In particular, we define a revised question collection procedure, which ensures a large proportion of nontrivial commonsense questions.", "labels": [], "entities": [{"text": "question collection", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8038082718849182}]}, {"text": "We test three benchmark models on MCScript2.0 that were also evaluated on MCScript, so a direct comparison is possible.", "labels": [], "entities": []}, {"text": "For the experiments, we split the data into a training set (14,191 questions on 2,500 texts), a development set (2,020 questions on 355 texts) and a test set (3,610 questions on 632 texts).", "labels": [], "entities": []}, {"text": "All texts of 5 randomly chosen scenarios were assigned completely to the test set, so apart of the test scenarios are unseen during training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of question labels, before validation.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy on test set, and on script/text-based  questions (acc scr , acc txt ) on MCScript2.0. The maxi- mum per column is printed in bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9971914887428284}]}, {"text": " Table 3: Accuracy on the test set and on script/text- based questions (acc scr , acc txt ) on MCScript. The  maximum per column is printed in bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.995353102684021}]}]}