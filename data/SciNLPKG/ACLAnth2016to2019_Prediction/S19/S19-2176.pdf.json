{"title": [{"text": "Using BERT Representations for Detecting Hyperpartisan News", "labels": [], "entities": [{"text": "BERT", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9709089994430542}, {"text": "Detecting Hyperpartisan News", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.8910250266393026}]}], "abstractContent": [{"text": "We describe the system submitted by the Jack Ryder team to SemEval-2019 Task 4 on Hy-perpartisan News Detection.", "labels": [], "entities": [{"text": "Jack Ryder team to SemEval-2019 Task 4", "start_pos": 40, "end_pos": 78, "type": "DATASET", "confidence": 0.6659592177186694}, {"text": "Hy-perpartisan News Detection", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.6492607990900675}]}, {"text": "The task asked participants to predict whether a given article is hyperpartisan, i.e., extreme-left or extreme-right.", "labels": [], "entities": []}, {"text": "We propose an approach based on BERT with fine-tuning, which was ranked 7th out 28 teams on the distantly supervised dataset, where all articles from a hyperpartisan/non-hyperpartisan news outlet are considered to be hyperpartisan/non-hyperpartisan.", "labels": [], "entities": [{"text": "BERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9906027317047119}, {"text": "fine-tuning", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9501299262046814}]}, {"text": "On a manually annotated test dataset, where human an-notators double-checked the labels, we were ranked 29th out of 42 teams.", "labels": [], "entities": []}], "introductionContent": [{"text": "SemEval-2019 Task 4 () asks to distinguish between articles that are extremely one-sided, i.e., extreme-left or extreme-right, and such that are not.", "labels": [], "entities": []}, {"text": "The organizers provided two datasets: 1.", "labels": [], "entities": []}, {"text": "By article: A small dataset of 645 manually annotated articles (BA in the following).", "labels": [], "entities": [{"text": "BA", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9959588646888733}]}, {"text": "2. By publisher: A large dataset of 750,000 articles annotated using distant supervision, where an article is considered hyperpartisan if its source is labeled as such (BP in the following).", "labels": [], "entities": [{"text": "BP", "start_pos": 169, "end_pos": 171, "type": "METRIC", "confidence": 0.984921395778656}]}, {"text": "The set is separated into 600,000 articles for training (BP-train) and 150,000 articles for validation.", "labels": [], "entities": [{"text": "BP-train", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8567825555801392}]}, {"text": "Furthermore, two test sets, one annotated by article (BA-test) and one annotated by publisher (BP-test), were hidden from the participants and they were used forgetting the final scores for the competition.", "labels": [], "entities": [{"text": "BA-test", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9583672881126404}, {"text": "BP-test", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.8866171836853027}]}, {"text": "The task is a binary classification one, where each article is to be assigned one of two possible classes: hyperpartisan and nonhyperpartisan.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed a number of experiments in order to select the best models to submit as official runs for the competition.", "labels": [], "entities": []}, {"text": "The best model for the by-publisher dataset was selected on BP-val after training the models on BP-train.", "labels": [], "entities": [{"text": "BP-val", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9834166765213013}, {"text": "BP-train", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9801875948905945}]}, {"text": "Since there was no validation set for the by-article dataset and BA was too small to be divided into training and validation sets, we trained our models on BP and we selected the best-performing one on BA.", "labels": [], "entities": [{"text": "BA", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.981830894947052}, {"text": "BA", "start_pos": 202, "end_pos": 204, "type": "DATASET", "confidence": 0.9273744225502014}]}, {"text": "Table 1 shows the obtained accuracy values on BPval (By-publisher) and BA (By-article) datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9989315867424011}, {"text": "BPval", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.7685670852661133}, {"text": "BA", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9946845173835754}]}, {"text": "As we can observe, the performance of the TF.IDF model is behind those when using BERT, both with and without fine-tuning.", "labels": [], "entities": [{"text": "BERT", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9955574870109558}]}, {"text": "As a result, we opted for the two BERT models, trained on BP+BA, as our submissions for the competition.", "labels": [], "entities": [{"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9852946996688843}, {"text": "BP+BA", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.7012937466303507}]}, {"text": "shows the results on the hidden test sets.", "labels": [], "entities": []}, {"text": "When developing the model for the submission, we focused on the datasets with by-publisher annotation.", "labels": [], "entities": []}, {"text": "This is probably the reason why we performed much better on the by-publisher hidden test set, 7th out 28 teams, than on the hidden byarticle test set, 29th out of 42 teams.", "labels": [], "entities": []}, {"text": "Another possible reason for the low results on the by-article hidden test set is overfitting on BP: our model might have learned to discriminate the publishers appearing in BP instead of the required labels hyperpartisan / non-hyperpartisan.", "labels": [], "entities": [{"text": "BP", "start_pos": 96, "end_pos": 98, "type": "DATASET", "confidence": 0.8468462824821472}]}, {"text": "Recalling that BP-val does not contain any articles from the publishers in BP-train, we conducted an experiment to see whether there was a correlation between the articles in the different partitions of the provided dataset.", "labels": [], "entities": [{"text": "BP-val", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9621602892875671}, {"text": "BP-train", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9592270851135254}]}, {"text": "In particular, we created a recurrent model with a single layer of 1,024 GRUs, and we trained it on 80% of the data and we evaluated it on the remaining 20%.", "labels": [], "entities": []}, {"text": "The model achieved 99.99% accuracy at predicting whether the article was from BP-train or from BP-val.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9995803236961365}, {"text": "BP-train", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9161468744277954}, {"text": "BP-val", "start_pos": 95, "end_pos": 101, "type": "DATASET", "confidence": 0.882213830947876}]}, {"text": "We further performed three additional experiments with the fine-tuned BERT model: (i) training and evaluating on BP-train, (ii) training on the first half of BP-train and evaluating on the second half of BP-train, and (iii) training on BP-train and evaluating on BP-val.", "labels": [], "entities": [{"text": "BERT", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9918922185897827}, {"text": "BP-train", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.9237697720527649}, {"text": "BP-train", "start_pos": 204, "end_pos": 212, "type": "DATASET", "confidence": 0.9248448014259338}, {"text": "BP-train", "start_pos": 236, "end_pos": 244, "type": "DATASET", "confidence": 0.9621230959892273}, {"text": "BP-val", "start_pos": 263, "end_pos": 269, "type": "DATASET", "confidence": 0.9703662991523743}]}, {"text": "shows the accuracy for BERT at each epoch when using titles for the three configurations (i)-(iii) above: T is BP-train, T1 and T2 are the two halves of BP-train, and V is BA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996840953826904}, {"text": "BERT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.8741424083709717}, {"text": "BA", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.9958462119102478}]}, {"text": "Figure 2 reports the performance for the same experiments when using the body text of the articles as input.", "labels": [], "entities": []}, {"text": "While the accuracy for the curves T-T and T1-T2 is close to 100% or is monotonically increasing with respect to the number of training epochs, the curve T-V does not show the same behavior, suggesting that 58.42% is close to the best performance that can be achieved in this setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997290968894958}]}, {"text": "The accuracy values, although not directly comparable, show that there is a huge gap between the performance on a dataset with the same set of publishers (T-T and T1-T2) vs. on a dataset where the news comes from a different set of publishers (T-V), thus supporting our hypothesis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991087317466736}]}], "tableCaptions": [{"text": " Table 1: Validation results: Accuracy for our TF.IDF  and BERT models on the by-publisher validation (BP- val) and on the by-article (BA) sets. The training was  performed on BP-train and BP, respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9971736669540405}, {"text": "BERT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.996664822101593}, {"text": "BP- val)", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.7367867976427078}]}, {"text": " Table 2: Testing results: Accuracy on the hidden test  sets for the BERT models we submitted. Both models  were trained on BP+BA.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9863113760948181}, {"text": "BERT", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.6353760957717896}, {"text": "BP+BA", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.6311142345269521}]}]}