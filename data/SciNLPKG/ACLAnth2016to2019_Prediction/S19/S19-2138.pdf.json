{"title": [{"text": "UM-IU@LING at SemEval-2019 Task 6: Identifying Offensive Tweets Using BERT and SVMs", "labels": [], "entities": [{"text": "Identifying Offensive Tweets", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.8892147739728292}, {"text": "BERT", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9852212071418762}]}], "abstractContent": [{"text": "This paper describes the UM-IU@LING's system for the SemEval 2019 Task 6: OffensEval.", "labels": [], "entities": [{"text": "SemEval 2019 Task 6: OffensEval", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.8344953755537668}]}, {"text": "We take a mixed approach to identify and categorize hate speech in social media.", "labels": [], "entities": []}, {"text": "In subtask A, we fine-tuned a BERT based classifier to detect abusive content in tweets, achieving a macro F 1 score of 0.8136 on the test data, thus reaching the 3rd rank out of 103 submissions.", "labels": [], "entities": [{"text": "BERT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9978296160697937}, {"text": "macro F 1 score", "start_pos": 101, "end_pos": 116, "type": "METRIC", "confidence": 0.8139466047286987}]}, {"text": "In subtasks B and C, we used a linear SVM with selected character n-gram features.", "labels": [], "entities": []}, {"text": "For subtask C, our system could identify the target of abuse with a macro F 1 score of 0.5243, ranking it 27th out of 65 submissions.", "labels": [], "entities": [{"text": "macro F 1 score", "start_pos": 68, "end_pos": 83, "type": "METRIC", "confidence": 0.7444913759827614}]}], "introductionContent": [{"text": "With the increased influence of social media on modern society, large amounts of user-generated content emerge on the internet.", "labels": [], "entities": []}, {"text": "Besides the exchange of ideas, we also see an exponential increase of aggressive and potentially harmful content, for example, hate speech.", "labels": [], "entities": []}, {"text": "If we consider the amount of user-generated data, it is impractical to manually identify the malicious speech.", "labels": [], "entities": []}, {"text": "Thus we need to develop methods to detect offensive speech automatically through computational models.", "labels": [], "entities": []}, {"text": "However, this task is challenging because natural language is fraught with ambiguities, and language in social media is extremely noisy.", "labels": [], "entities": []}, {"text": "Here we present our method to automatically identifying offensive content in tweets.", "labels": [], "entities": [{"text": "automatically identifying offensive content in tweets", "start_pos": 30, "end_pos": 83, "type": "TASK", "confidence": 0.7387513021628062}]}, {"text": "We primarily focus on detecting whether a tweet contains offensive content or not (subtask A), and then determining the target of the offensive content (subtask C).", "labels": [], "entities": []}, {"text": "For subtask A, we use pre-trained word embeddings by fine-tuning the BERT model) for detecting offensive tweets.", "labels": [], "entities": [{"text": "BERT", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9990226030349731}]}, {"text": "For subtasks B and C, BERT did not perform well, either because of limited training data or because we did not find the appropriate hyperparameters.", "labels": [], "entities": [{"text": "BERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.99875807762146}]}, {"text": "Thus we use an SVM classifier with character n-grams as features.", "labels": [], "entities": []}, {"text": "We accidentally flipped the predicted labels in our submission to subtask B, which is why we do not report results of subtask B here.", "labels": [], "entities": []}, {"text": "Among all teams participating in OffensEval, our models ranks 3rd out of 103 on subtask A and 27th out of 65 on subtask C.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The official UM-IU@LING result for subtask  A, in comparison to the baselines.", "labels": [], "entities": [{"text": "UM-IU@LING result", "start_pos": 23, "end_pos": 40, "type": "METRIC", "confidence": 0.7126446962356567}]}, {"text": " Table 2: Results on the trial data for subtask A.", "labels": [], "entities": []}, {"text": " Table 3: Misclassified examples for subtask A from the trial data. Usernames are anonymized.", "labels": [], "entities": []}, {"text": " Table 4: The official UM-IU@LING results (SVM) for  subtask C.", "labels": [], "entities": [{"text": "UM-IU@LING results (SVM)", "start_pos": 23, "end_pos": 47, "type": "METRIC", "confidence": 0.5570862080369677}]}]}