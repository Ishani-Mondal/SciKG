{"title": [{"text": "CAMsterdam at SemEval-2019 Task 6: Neural and graph-based feature extraction for the identification of offensive tweets", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe the CAMsterdam team entry to the SemEval-2019 Shared Task 6 on offensive language identification in Twitter data.", "labels": [], "entities": [{"text": "SemEval-2019 Shared Task 6", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.6684092581272125}, {"text": "offensive language identification", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.650732010602951}]}, {"text": "Our proposed model learns to extract tex-tual features using a multi-layer recurrent network , and then performs text classification using gradient-boosted decision trees (GBDT).", "labels": [], "entities": [{"text": "text classification", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7685658931732178}]}, {"text": "A self-attention architecture enables the model to focus on the most relevant areas in the text.", "labels": [], "entities": []}, {"text": "We additionally learn globally optimised em-beddings for hashtags using node2vec, which are given as additional tweet features to the GBDT classifier.", "labels": [], "entities": [{"text": "GBDT classifier", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.9370335638523102}]}, {"text": "Our best model obtains 78.79% macro F1-score on detecting offensive language (subtask A), 66.32% on categorising offence types (targeted/untargeted; subtask B), and 55.36% on identifying the target of offence (subtask C).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.873881995677948}]}], "introductionContent": [{"text": "The SemEval-2019 shared task 6 ('OffensEval') involved three sub-parts: the classification of tweets as offensive or not (subtask A), classifying whether they are targeted insults or not (subtask B), and finally whether the targeted insults are aimed at an individual, group or otherwise (subtask C).", "labels": [], "entities": [{"text": "SemEval-2019 shared task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7729997237523397}]}, {"text": "Further details maybe found in the shared task report).", "labels": [], "entities": []}, {"text": "Here we describe CAMsterdam's competition entry.", "labels": [], "entities": [{"text": "CAMsterdam", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.6278730034828186}]}, {"text": "In recent years, there has been a growing interest in the automatic detection of offensive opinions expressed in online texts, including those posted in discussion forums, news article comment sections, and social networks.", "labels": [], "entities": [{"text": "automatic detection of offensive opinions expressed in online texts", "start_pos": 58, "end_pos": 125, "type": "TASK", "confidence": 0.8530105815993415}]}, {"text": "Such detection is not straightforwardly a matter of identifying texts containing obscene words; offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience.", "labels": [], "entities": []}, {"text": "Therefore the task is a challenging one, but one with real world impact: if measures can betaken to identify and curtail trolling, the toxicity of the internet canto some extent be reduced.", "labels": [], "entities": []}, {"text": "There is evidence that online harassment is connected with oppression, violence and suicide, and there may moreover be reasons for concern about the perpetrator's wellbeing along with that of the victims (.", "labels": [], "entities": []}, {"text": "Our approach to the task extends the work of, who extract features from tweets using an RNN for subsequent use in a gradient-boosted decision tree (GBDT) (.", "labels": [], "entities": []}, {"text": "Firstly, we experiment with changes to the RNN, including the use of self-attention ( and ELMo embeddings.", "labels": [], "entities": []}, {"text": "Secondly, we add additional features to the GBDT, including globally-optimised hashtag embeddings learned from a graph of tweet contents using node2vec.", "labels": [], "entities": [{"text": "GBDT", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8839892745018005}]}, {"text": "We show that this method of learning distributional information about hashtags improves performance over just learning their embeddings within a RNN.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Count of tweets in each category of OLID  (Zampieri et al., 2019a).", "labels": [], "entities": [{"text": "Count", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9608651995658875}, {"text": "OLID", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.41580697894096375}]}, {"text": " Table 2: Ablation test for features, with results reported on our held-out development set for subtask A.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9910477995872498}]}, {"text": " Table 3: Accuracy and macro F1 results on the official  subtask A test set. All three models have the same hy- perparameters.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987305998802185}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.8465895056724548}, {"text": "official  subtask A test set", "start_pos": 47, "end_pos": 75, "type": "DATASET", "confidence": 0.7171699106693268}]}, {"text": " Table 4: Accuracy and macro F1 results on the official  subtask B test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989234805107117}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.8737212419509888}, {"text": "official  subtask B test set", "start_pos": 47, "end_pos": 75, "type": "DATASET", "confidence": 0.7771499752998352}]}, {"text": " Table 5: Accuracy and macro F1 results on the official  subtask C test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988101720809937}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.8498570322990417}, {"text": "official  subtask C test set", "start_pos": 47, "end_pos": 75, "type": "DATASET", "confidence": 0.8526489973068238}]}]}