{"title": [{"text": "Pardeep at SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media Using Deep Learning", "labels": [], "entities": [{"text": "SemEval-2019 Task 6", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7404857675234476}]}], "abstractContent": [{"text": "The rise of social media has made information exchange faster and easier among the people.", "labels": [], "entities": [{"text": "information exchange", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.8131635189056396}]}, {"text": "However, in recent times, the use of offensive language has seen an upsurge in social media.", "labels": [], "entities": []}, {"text": "The main challenge fora service provider is to correctly identify such offensive posts and take necessary action to monitor and control their spread.", "labels": [], "entities": []}, {"text": "In this work, we try to address this problem by using sophisticated deep learning techniques like LSTM, Bidirectional LSTM and Bidirectional GRU.", "labels": [], "entities": []}, {"text": "Our proposed approach solves 3 different Sub-tasks provided in the SemEval-2019 task 6 which incorporates identification of offensive tweets as well as their categorization.", "labels": [], "entities": []}, {"text": "We obtain significantly better results in the leader-board for Sub-task B and decent results for Sub-task A and Sub-task C validating the fact that the proposed models can be used for automating the offensive post-detection task in social media.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media has revolutionized the way of communication among the people.", "labels": [], "entities": []}, {"text": "It is an instant communication medium which connects people allover the world and shares their views.", "labels": [], "entities": []}, {"text": "But, some people misuse this freedom by using the offensive language through posts or comments to defame, insult or target an individual or a group of individuals.", "labels": [], "entities": []}, {"text": "The mainstream media have reported various cases of suicide and depression due to trolling and cyberbullying in social media.", "labels": [], "entities": []}, {"text": "Hence it becomes worrisome for the corporates, government organizations and security agencies to either stop or mitigate this type of behavior of the users.", "labels": [], "entities": []}, {"text": "Manually it is impossible to check the negative behavior of users due to the volume, velocity and variety of data coming from the social networks.", "labels": [], "entities": []}, {"text": "Hence there is an utmost need to develop a system which automatically identifies and categorizes the offensive language in social networks.", "labels": [], "entities": []}, {"text": "To tackle these issues) aimed exactly at that need and organized a task in identifying and categorizing offensive language in social media.", "labels": [], "entities": []}, {"text": "This task is divided into three Sub-tasks.", "labels": [], "entities": []}, {"text": "Sub-task A -Offensive language identification.", "labels": [], "entities": [{"text": "Offensive language identification", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.7974195679028829}]}, {"text": "Sub-task B -Categorization of offense types.", "labels": [], "entities": []}, {"text": "Sub-task C -Offense target identification.", "labels": [], "entities": [{"text": "Offense target identification", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.5914691785971323}]}, {"text": "All the three Sub-tasks are related to each other.", "labels": [], "entities": []}, {"text": "In Sub-task A, we have to identify whether a given set of tweets is offensive or not.", "labels": [], "entities": []}, {"text": "It is a binary classification task based on tweet text.", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.7765823801358541}]}, {"text": "In Sub-task B, the main challenge is to categorize the tweets which are offensive in Sub-task A into targeted or untargeted.", "labels": [], "entities": []}, {"text": "Sub-task C is comparatively challenging than other two Sub-tasks due to the multi-class nature.", "labels": [], "entities": []}, {"text": "Its goal is to identify the tweets which are targeted in Sub-task B and categorized those tweets into individual, group or others.", "labels": [], "entities": []}, {"text": "Our approach for the SemEval-2019 task 6 (identifying and categorizing offensive language in social media) comprises of deep learning models: Bidirectional LSTM, Bidirectional GRU and standard LSTM.", "labels": [], "entities": [{"text": "SemEval-2019 task", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9010159075260162}, {"text": "identifying and categorizing offensive language in social media", "start_pos": 42, "end_pos": 105, "type": "TASK", "confidence": 0.6374291479587555}, {"text": "Bidirectional GRU", "start_pos": 162, "end_pos": 179, "type": "METRIC", "confidence": 0.8094929158687592}]}, {"text": "These are popularly used deep learning sequence models applied in many text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.8711461623509725}]}, {"text": "We used the pre-trained word level embedding GloVe (Global Vectors for Word Representation) to get vector representations for words that appeared in tweets and used these representations as features for training the models.", "labels": [], "entities": [{"text": "Word Representation)", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7755273381868998}]}, {"text": "To check the performance of models, 10 fold cross-validation was applied on the given training data.", "labels": [], "entities": []}, {"text": "We compared the results of the above-mentioned models with various baselines such as Logistic Regression, Support Vector Machine, Gradient Boosting and XGBoost.", "labels": [], "entities": []}, {"text": "The baseline models are reasonably good but they have poor classification Accuracy as compared to deep learning models.", "labels": [], "entities": [{"text": "classification", "start_pos": 59, "end_pos": 73, "type": "METRIC", "confidence": 0.9710313677787781}, {"text": "Accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8802501559257507}]}, {"text": "This paper presents the description of our approaches and results for SemEval-2019 task 6.", "labels": [], "entities": [{"text": "SemEval-2019 task", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.9047273099422455}]}], "datasetContent": [{"text": "The dataset provided by the task organizers is OLID (Offensive Language Identification).", "labels": [], "entities": [{"text": "OLID", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9511208534240723}, {"text": "Offensive Language Identification)", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.6827248707413673}]}, {"text": "The details of data and annotation are available in ().", "labels": [], "entities": []}, {"text": "For Sub-task A, this dataset contains tweets labeled into the following two categories: offensive (OFF) and not offensive (NOT).", "labels": [], "entities": [{"text": "OFF", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9071926474571228}]}, {"text": "For Sub-task B, tweets are labeled into the following two categories: targeted input (TIN) and untargeted (UNT).", "labels": [], "entities": [{"text": "untargeted (UNT)", "start_pos": 95, "end_pos": 111, "type": "METRIC", "confidence": 0.7308575659990311}]}, {"text": "For Sub-task C, the given tweets are classified into the following three categories: group (GP), individual (IND) and others (OTH).", "labels": [], "entities": [{"text": "OTH", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.8695809245109558}]}, {"text": "Out of 13,240 training samples of Subtask A, 4404 samples have been allocated to Subtask B and 3,877 samples have been allocated to Sub-task C.", "labels": [], "entities": []}, {"text": "All the tweets are in English language.", "labels": [], "entities": []}, {"text": "The statistics of the dataset and some instances of tweets with their labels are shown in and  For implementing the models, we use Keras ( and Scikit-learn (Pedregosa et al., 2011) python framework libraries.", "labels": [], "entities": []}, {"text": "The experimental details and model configuration are shown in.", "labels": [], "entities": []}, {"text": "For the effectiveness of models, we add a small proportion of dropout.", "labels": [], "entities": []}, {"text": "For GRU model, we specify the number of Recurrent Units.", "labels": [], "entities": []}, {"text": "In terms of training, we use categorical cross Entropy as a loss function with ADAM as the optimization function.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9524691104888916}]}, {"text": "All the models are tested using 10 fold cross-validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the offensive dataset", "labels": [], "entities": [{"text": "offensive dataset", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.6979486644268036}]}, {"text": " Table 4: Results of the proposed deep learning approaches including baselines on the Sub-task A training data  using 10-fold cross validation", "labels": [], "entities": []}, {"text": " Table 5. As  it is evident from these results that Bidirectional  GRU performed better than other two deep learn- ing models with F1 Score of 0.69. To analyze the  correct label of a tweet, we also show the confu- sion matrix which shows correct class predictions  along diagonal lines. Our team ranked 74 out of  104 participating teams.", "labels": [], "entities": [{"text": "Bidirectional  GRU", "start_pos": 52, "end_pos": 70, "type": "METRIC", "confidence": 0.8355793356895447}, {"text": "F1 Score", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9927458167076111}]}, {"text": " Table 5: Results for Sub-task A (Binary Classification)", "labels": [], "entities": [{"text": "Binary Classification", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.7982299029827118}]}, {"text": " Table 6: Results for Sub-task B (Binary classification)", "labels": [], "entities": [{"text": "Binary classification", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.9195492565631866}]}, {"text": " Table 7: Results for Sub-task C (Multi-Class  Classification)", "labels": [], "entities": []}, {"text": " Table 8: Shows per-class performance of our proposed models for Sub-task A.", "labels": [], "entities": []}, {"text": " Table 9: Shows per-class performance of our proposed models for Sub-task B.", "labels": [], "entities": []}]}