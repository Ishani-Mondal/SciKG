{"title": [{"text": "Modeling Context Words as Regions: An Ordinal Regression Approach to Word Embedding", "labels": [], "entities": []}], "abstractContent": [{"text": "Vector representations of word meaning have found many applications in the field of natural language processing.", "labels": [], "entities": [{"text": "Vector representations of word meaning", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8758076190948486}, {"text": "natural language processing", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.655110647281011}]}, {"text": "Word vectors intuitively represent the average context in which a given word tends to occur , but they cannot explicitly model the diversity of these contexts.", "labels": [], "entities": []}, {"text": "Although region representations of word meaning offer a natural alternative to word vectors, only few methods have been proposed that can effectively learn word regions.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew word embedding model which is based on SVM regression.", "labels": [], "entities": []}, {"text": "We show that the underlying ranking interpretation of word contexts is sufficient to match, and sometimes outperform, the performance of popular methods such as Skip-gram.", "labels": [], "entities": []}, {"text": "Furthermore, we show that by using a quadratic kernel, we can effectively learn word regions, which outper-form existing unsupervised models for the task of hypernym detection.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.8136475086212158}]}], "introductionContent": [{"text": "Word embedding models such as Skip-gram () and GloVe () represent words as vectors of typically around 300 dimensions.", "labels": [], "entities": []}, {"text": "The relatively lowdimensional nature of these word vectors makes them ideally suited for representing textual input to neural network models.", "labels": [], "entities": []}, {"text": "Moreover, word embeddings have been found to capture many interesting regularities (, which makes it possible to use them as a source of semantic and linguistic knowledge, and to align word embeddings with visual features a given threshold \u03bb).", "labels": [], "entities": []}, {"text": "First, we empirically show that effective word embeddings can be learned from purely ordinal information, which stands in contrast to the probabilistic view taken by e.g. Skip-gram and GloVe.", "labels": [], "entities": []}, {"text": "Specifically, we propose anew word embedding model which uses (a ranking equivalent of) max-margin constraints to impose the requirement that pt \u00b7 \u02dc p c should be a monotonic function of the probability P (c|t) of seeing c in the context oft.", "labels": [], "entities": []}, {"text": "Geometrically, this means that, like Skip-gram, our model associates with each context word a number of parallel hyperplanes.", "labels": [], "entities": []}, {"text": "However, unlike in the Skip-gram model, only the relative position of these hyperplanes is imposed (i.e. if \u03bb 1 < \u03bb 2 < \u03bb 3 then H \u03bb 2 c should occur between H \u03bb 1 c and H \u03bb 3 c ).", "labels": [], "entities": []}, {"text": "Second, by using a quadratic kernel for the max-margin constraints, we obtain a model that can represent context words as a set of nested ellipsoids, as illustrated in(b).", "labels": [], "entities": []}, {"text": "From these nested ellipsoids we can then estimate a Gaussian which acts as a convenient region based word representation.", "labels": [], "entities": []}, {"text": "Note that our model thus jointly learns a vector representation for each word (i.e. the target word representations) as well as a region based representation (i.e. the nested ellipsoids representing the context words).", "labels": [], "entities": []}, {"text": "We present experimental results which show that the region based representations are effective for measuring synonymy and hypernymy.", "labels": [], "entities": []}, {"text": "Moreover, perhaps surprisingly, the region based modeling of context words also benefits the target word vectors, which match, and in some cases outperform the vectors obtained by standard word embedding models on various benchmark evaluation tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate both the vector and region representations produced by our model.", "labels": [], "entities": []}, {"text": "In our experiments, we have used the Wikipedia dump from November 2nd, 2015 consisting of 1,335,766,618 tokens.", "labels": [], "entities": [{"text": "Wikipedia dump from November 2nd, 2015 consisting of 1,335,766,618 tokens", "start_pos": 37, "end_pos": 110, "type": "DATASET", "confidence": 0.9412984577092257}]}, {"text": "We used a basic text preprocessing strategy, which involved removing punctuations, removing HTML/XML tags and lowercasing all tokens.", "labels": [], "entities": []}, {"text": "We have removed words with less than 10 occurrences in the entire corpus.", "labels": [], "entities": []}, {"text": "We used the Apache sentence segmentation tool 2 to detect sentence boundaries.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7075356543064117}]}, {"text": "In all our experiments, we have set the number of dimensions as 300, which was found to be a good choice in previous work, e.g. ().", "labels": [], "entities": []}, {"text": "We use a context window of 10 words before and after the target word, but without crossing sentence boundaries.", "labels": [], "entities": []}, {"text": "The number of iterations for SGD was set to 20.", "labels": [], "entities": [{"text": "SGD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9194132685661316}]}, {"text": "The results of all baseline models have been obtained using their publicly available implementations.", "labels": [], "entities": []}, {"text": "We have used 10 negative samples in the word2vec code, which gave better results than the default value of 5.", "labels": [], "entities": [{"text": "word2vec code", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9107708036899567}]}, {"text": "For the baseline models, we have used the default settings, apart from the D-GloVe model for which no default values were provided by the authors.", "labels": [], "entities": []}, {"text": "For D-GloVe, we have therefore tuned the parameters using the ranges discussed in).", "labels": [], "entities": []}, {"text": "Specifically we have used the parameters that gave the best results on the Google Analogy Test Set (see below).", "labels": [], "entities": [{"text": "Google Analogy Test Set", "start_pos": 75, "end_pos": 98, "type": "DATASET", "confidence": 0.9269499778747559}]}, {"text": "As baselines we have used the following standard word embedding models: the Skip-gram (SG) and Continuous Bag-of-Words (CBOW) models , proposed in (), the GloVe model 4 , proposed in (, and the D-GloVe model proposed in.", "labels": [], "entities": []}, {"text": "We have also compared against the Gaussian word embedding model 6 from, using the means of the Gaussians as vector representations, and the Gaussians themselves as region representations.", "labels": [], "entities": []}, {"text": "As in (, we consider two variants: one with diagonal covariance matrices (Gauss-D) and one with spherical covariance matrices (Gauss-S).", "labels": [], "entities": []}, {"text": "For our model, we will consider the following configurations: Reg-li-cos word vectors, obtained using linear kernel, compared using cosine similarity; Reg-li-eucl word vectors, obtained using linear kernel, compared using Euclidean distance; Reg-qu-cos word vectors, obtained using quadratic kernel, compared using cosine similarity; Reg-qu-eucl word vectors, obtained using quadratic kernel, compared using Euclidean distance; Reg-li-prod Gaussian word regions, obtained using linear kernel, compared using the inner product E; Reg-li-wprod Gaussian word regions estimated using the weighted variant, obtained using linear kernel, compared using the inner product E; Reg-li-JS Gaussian word regions, obtained using linear kernel, compared using the JensenShannon divergence; Reg-li-wJS Gaussian word regions estimated using the weighted variant, obtained using linear kernel, compared using Jensen-Shannon divergence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the analogy completion task  (accuracy). Reg-li-* and Reg-qu-* are our models  with a linear and quadratic kernel.", "labels": [], "entities": [{"text": "analogy completion task", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.8426727851231893}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9992197751998901}]}, {"text": " Table 2: Results for similarity estimation (Spearman \u03c1). Reg-li-* and Reg-qu-* are our models with a  linear and quadratic kernel.", "labels": [], "entities": [{"text": "similarity estimation", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.777627021074295}]}, {"text": " Table 3: Results for McRae feature norms (F1).  Reg-li and Reg-qu are our models with a linear and  quadratic kernel.", "labels": [], "entities": [{"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9852126836776733}]}, {"text": " Table 5: Results for HyperLex (Spearman \u03c1). Reg- li-* and Reg-qu-* are our models with a linear and  quadratic kernel.", "labels": [], "entities": []}, {"text": " Table 4 Interesting, for verbs we  find that Skip-gram substantially outperforms the  region based models, which is in accordance with  our findings in the word similarity experiments.", "labels": [], "entities": []}]}