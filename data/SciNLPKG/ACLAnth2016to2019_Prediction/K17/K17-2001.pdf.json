{"title": [], "abstractContent": [{"text": "The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation required systems to be trained and tested in each of 52 typologically diverse languages.", "labels": [], "entities": [{"text": "supervised morphological generation", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.6015249689420065}]}, {"text": "In sub-task 1, submitted systems were asked to predict a specific inflected form of a given lemma.", "labels": [], "entities": []}, {"text": "In sub-task 2, systems were given a lemma and some of its specific inflected forms, and asked to complete the inflectional paradigm by predicting all of the remaining inflected forms.", "labels": [], "entities": []}, {"text": "Both sub-tasks included high, medium, and low-resource conditions.", "labels": [], "entities": []}, {"text": "Sub-task 1 received 24 system submissions, while sub-task 2 received 3 system submissions.", "labels": [], "entities": []}, {"text": "Following the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared task, all but one of the submissions included a neural component.", "labels": [], "entities": [{"text": "SIGMORPHON 2016 shared task", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.5069266930222511}]}, {"text": "The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data.", "labels": [], "entities": []}, {"text": "However, different biasing and data augmentation resulted in non-identical sets of inflected forms being predicted correctly, suggesting that there is room for future improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphology interacts with both syntax and phonology.", "labels": [], "entities": []}, {"text": "As a result, explicitly modeling morphology has been shown to aid a number of tasks inhuman language technology (HLT), including machine translation (MT) (, speech recognition (, parsing (Seeker and C \u00b8 etino\u02c7gluetino\u02c7glu, 2015), keyword spotting (, and word embedding ().", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 129, "end_pos": 153, "type": "TASK", "confidence": 0.8176113307476044}, {"text": "speech recognition", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7430828511714935}, {"text": "parsing (Seeker and C \u00b8 etino\u02c7gluetino\u02c7glu, 2015)", "start_pos": 179, "end_pos": 228, "type": "TASK", "confidence": 0.5958995759487152}, {"text": "keyword spotting", "start_pos": 230, "end_pos": 246, "type": "TASK", "confidence": 0.7837072312831879}]}, {"text": "Dedicated systems for modeling morphological patterns and complex word forms have received less attention from the HLT community than tasks that target other levels of linguistic structure.", "labels": [], "entities": []}, {"text": "Recently, however, there has been a surge of work in this area, representing a renewed interest in morphology and the potential to use advances in machine learning to attack a fundamental problem in string-to-string transformations: the prediction of one morphologically complex word form from another.", "labels": [], "entities": [{"text": "prediction of one morphologically complex word form", "start_pos": 237, "end_pos": 288, "type": "TASK", "confidence": 0.843794081892286}]}, {"text": "This increased interest in morphology as an independent set of problems within HLT arrives at a particularly opportune time, as morphology is also undergoing a methodological renewal within theoretical linguistics where it is moving towards increased interdisciplinary work and quantitative methodologies (Moscoso del Prado.", "labels": [], "entities": []}, {"text": "Pushing the HLT research agenda forward in the domain of morphology promises to lead to mutually highly beneficial dialogue between the two fields.", "labels": [], "entities": [{"text": "HLT research", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.7612883746623993}]}, {"text": "Rich morphology is the norm among the languages of the world.", "labels": [], "entities": []}, {"text": "The linguistic typology database WALS shows that 80% of the world's languages mark verb tense through morphology while 65% mark grammatical case).", "labels": [], "entities": []}, {"text": "The more limited inflectional system of English may help to explain the fact that morphology has received less attention in the computational literature than it is arguably due.", "labels": [], "entities": []}, {"text": "The CoNLL-SIGMORPHON 2017 shared task worked to promote the development of robust systems that can learn to perform cross-linguistically: Example training data from sub-task 1.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2017 shared task", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.8630166947841644}]}, {"text": "Each training example maps a lemma and inflection to an inflected form, The inflection is a bundle of morphosyntactic features.", "labels": [], "entities": []}, {"text": "Note that inflected forms (and lemmata) can encompass multiple words.", "labels": [], "entities": []}, {"text": "In the test data, the last column (the inflected form) must be predicted by the system.", "labels": [], "entities": []}, {"text": "reliable morphological inflection and morphological paradigm cell filling using varying amounts of training data.", "labels": [], "entities": [{"text": "morphological paradigm cell filling", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.5808751434087753}]}, {"text": "We note that this is also the first CoNLL-hosted shared task to focus on morphology.", "labels": [], "entities": []}, {"text": "The task itself featured training and development data from 52 languages representing a range of language families.", "labels": [], "entities": []}, {"text": "Many of the languages included were extremely low-resource, e.g., Quechua, Navajo, and Haida.", "labels": [], "entities": []}, {"text": "The chosen languages also encompassed diverse morphological properties and inflection processes.", "labels": [], "entities": []}, {"text": "Whenever possible, three data conditions were given for each language: low, medium, and high.", "labels": [], "entities": []}, {"text": "In the inflection sub-task, these corresponded to seeing 100 examples, 1,000 examples, and 10,000 examples respectively in the training data for almost all languages.", "labels": [], "entities": []}, {"text": "The results show that encoder-decoder recurrent neural network models (RNNs) can perform very well even with small training sets, if they are augmented with various mechanisms to cope with the low-resource setting.", "labels": [], "entities": []}, {"text": "The shared task training, development, and test data are released publicly.", "labels": [], "entities": []}], "datasetContent": [{"text": "This year's shared task contained two sub-tasks, which represented slightly different learning scenarios that might be faced by an HLT engineer or (roughly speaking) a human learner.", "labels": [], "entities": []}, {"text": "Beyond manually vetted 2 data for training, development and test, monolingual corpus data (Wikipedia dumps) was also provided for both of the sub-tasks.: Example training and test data from sub-task 2 in Spanish.", "labels": [], "entities": []}, {"text": "At training time, the system is provided with complete paradigms, i.e., tables of all inflections fora given lemma, like the example at top.", "labels": [], "entities": []}, {"text": "At test time, the system is asked to complete partially filled paradigms, like the example at bottom; note that the inflectional features for the missing paradigm cells are provided in the input.", "labels": [], "entities": []}, {"text": "The CoNLL-SIGMORPHON 2017 shared task is the second shared task in a series that began with the SIGMORPHON 2016 shared task on morphological reinflection ().", "labels": [], "entities": [{"text": "SIGMORPHON 2016 shared task", "start_pos": 96, "end_pos": 123, "type": "TASK", "confidence": 0.7103395313024521}]}, {"text": "In contrast to 2016, it happens that both of the 2017 sub-tasks actually involve only inflection, not reinflection.", "labels": [], "entities": []}, {"text": "Nonetheless, we kept \"reinflection\" in this year's title to make it easier to refer to the series of tasks.", "labels": [], "entities": []}, {"text": "Each team participating in a given sub-task was asked to submit 156 versions of their system, where each version was trained using a different training set (3 training sizes \u00d7 52 languages) and its corresponding development set.", "labels": [], "entities": []}, {"text": "We evaluated each submitted system on its corresponding test set, i.e., the test set for its language.", "labels": [], "entities": []}, {"text": "We computed three evaluation metrics: (i) Overall 1-best test-set accuracy, i.e., is the predicted paradigm cell correct?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.7936221361160278}]}, {"text": "(ii) average Levenshtein distance, i.e., how badly does the predicted form disagree with the answer?", "labels": [], "entities": [{"text": "average Levenshtein distance", "start_pos": 5, "end_pos": 33, "type": "METRIC", "confidence": 0.6941050390402476}]}, {"text": "(iii) Fullparadigm accuracy, i.e., is the complete paradigm correct?", "labels": [], "entities": [{"text": "Fullparadigm", "start_pos": 6, "end_pos": 18, "type": "METRIC", "confidence": 0.8927687406539917}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9436418414115906}]}, {"text": "This final metric only truly makes sense in sub-task 2, where full paradigms are given for evaluation.", "labels": [], "entities": []}, {"text": "For each sub-task, the three data conditions (low, medium, and high) resulted in a learning curve.", "labels": [], "entities": []}, {"text": "For each system in each condition, we report the average metrics across all 52 languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Total number of lemmata and forms available for sampling, and number of distinct lemmata present in each data  condition in Task 1. For almost all languages, these were spread across 10000,1000, and 100 forms in the High, Medium, and  Low conditions, respectively, and 1000 forms in each Dev and Test set. For  \u2020-marked languages, there was not enough total  data to support these numbers. Bengali had 4423 forms in the High condition, and Dev and Test sets of 100 forms each. Haida  had 6840 forms in the High condition and Dev and Test sets of 100 forms. Scottish Gaelic had no High condition, a Medium  condition of 681 forms, and Dev and Test sets of 50 forms each. The three last columns indicate how many inflected forms  have undergone changes in a prefix (Pr), a change in a suffix (Su), or a stem-internal change (Ap) versus the given lemma form.", "labels": [], "entities": []}, {"text": " Table 4: Quantity of data available in sub-task 2. For each  possible part of speech in each language, we present the  range in the number of forms that comprise a paradigm as  an indication of the difficulty of the task of forming a full  paradigm. These ranges were computed using the data in the  Train Medium condition.", "labels": [], "entities": [{"text": "Quantity", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9161093235015869}]}, {"text": " Table 7: Sub-task 1 results: Per-form accuracy (in %age  points) and average Levenshtein distance from the correct  form (in characters), averaged across the 52 languages with  all languages weighted equally. The columns represent the  different training size conditions. Systems marked with  \u2020  used external resources. Accuracies marked with  \u2021 indi- cate that the submission did not include all 52 languages and  should not be compared to the other accuracies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.955308735370636}, {"text": "Levenshtein distance", "start_pos": 78, "end_pos": 98, "type": "METRIC", "confidence": 0.8199820220470428}, {"text": "Accuracies", "start_pos": 322, "end_pos": 332, "type": "METRIC", "confidence": 0.9948514103889465}]}, {"text": " Table 8: Sub-task 2 results: Per-form accuracy (in %age  points) and average Levenshtein distance from the correct  form (in characters).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9834251999855042}, {"text": "Levenshtein distance", "start_pos": 78, "end_pos": 98, "type": "METRIC", "confidence": 0.874032199382782}]}, {"text": " Table 10: Sub-task 1 High Condition Part 1.", "labels": [], "entities": []}, {"text": " Table 11: Sub-task 1 High Condition Part 2.", "labels": [], "entities": []}, {"text": " Table 12: Sub-task 1 High Condition Part 3.", "labels": [], "entities": []}, {"text": " Table 13: Sub-task 1 Medium Condition Part 1.", "labels": [], "entities": []}, {"text": " Table 15: Sub-task 1 Medium Condition Part 3.", "labels": [], "entities": []}, {"text": " Table 16: Sub-task 1 Low Condition Part 1.", "labels": [], "entities": []}, {"text": " Table 18: Sub-task 1 Low Condition Part 3.", "labels": [], "entities": []}]}