{"title": [{"text": "Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification", "labels": [], "entities": [{"text": "Tell Me Why", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8398654262224833}, {"text": "Question Answering", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7759228646755219}, {"text": "Answer Justification", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.9393514394760132}]}], "abstractContent": [{"text": "For many applications of question answering (QA), being able to explain why a given model chose an answer is critical.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.879781186580658}]}, {"text": "However, the lack of labeled data for answer justifications makes learning this difficult and expensive.", "labels": [], "entities": [{"text": "answer justifications", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8005502820014954}]}, {"text": "Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications, where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either.", "labels": [], "entities": []}, {"text": "We propose a neural network architecture for QA that reranks answer justifications as an intermediate (and human-interpretable) step in answer selection.", "labels": [], "entities": [{"text": "QA", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9323791861534119}, {"text": "answer justifications", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7823455929756165}, {"text": "answer selection", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.8608061373233795}]}, {"text": "Our approach is informed by a set of features designed to combine both learned representations and explicit features to capture the connection between questions, answers, and answer justifications.", "labels": [], "entities": []}, {"text": "We show that with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9% rated highly relevant) and answer selection (+6% P@1).", "labels": [], "entities": [{"text": "IR", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.8558593392372131}, {"text": "justification ranking", "start_pos": 114, "end_pos": 135, "type": "METRIC", "confidence": 0.8783406913280487}, {"text": "P@1", "start_pos": 190, "end_pos": 193, "type": "METRIC", "confidence": 0.931893507639567}]}], "introductionContent": [{"text": "Developing interpretable machine learning (ML) models, that is, models where a human user can understand what the model is learning, is considered by many to be crucial for ensuring usability and accelerating progress.", "labels": [], "entities": [{"text": "interpretable machine learning (ML)", "start_pos": 11, "end_pos": 46, "type": "TASK", "confidence": 0.6773071388403574}]}, {"text": "For many applications of question answering (QA), i.e., finding short answers to natural language questions, simply providing an answer is not sufficient.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8892682790756226}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance on the AI2 Kaggle questions, measured  by precision-at-one (P@1).  *  s indicate that the difference be- tween the corresponding model and the IR baseline is sta- tistically significant (  *  indicates p < 0.05 and  *  *  indicates  p < 0.001) and  \u2020 s indicate significance compared to IR ++ ,  All significance values were determined through a one-tailed  bootstrap resampling test with 100,000 iterations.", "labels": [], "entities": [{"text": "AI2 Kaggle questions", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.6670062939325968}, {"text": "precision-at-one (P@1)", "start_pos": 64, "end_pos": 86, "type": "METRIC", "confidence": 0.8292780121167501}]}, {"text": " Table 4: Example justifications from the our model and their  associated ratings.", "labels": [], "entities": []}, {"text": " Table 6: Summary of the findings of the 30 question error  analysis. Note that a given question may fall into more than  one category.", "labels": [], "entities": []}]}