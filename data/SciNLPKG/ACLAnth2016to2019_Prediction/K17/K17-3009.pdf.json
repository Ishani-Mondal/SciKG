{"title": [{"text": "Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.7498860657215118}]}], "abstractContent": [{"text": "We present an update to UDPipe 1.0 (Straka et al., 2016), a trainable pipeline which performs sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.743219792842865}, {"text": "POS tagging", "start_pos": 131, "end_pos": 142, "type": "TASK", "confidence": 0.8691037595272064}, {"text": "dependency parsing", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7746719717979431}]}, {"text": "We provide models for all 50 languages of UD 2.0, and furthermore, the pipeline can be trained easily using data in CoNLL-U format.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Universal Dependencies project () seeks to develop cross-linguistically consistent treebank annotation of morphology and syntax for many languages.", "labels": [], "entities": []}, {"text": "The latest version of UD () consists of 70 dependency treebanks in 50 languages.", "labels": [], "entities": []}, {"text": "As such, the UD project represents an excellent data source for developing multi-lingual NLP tools which perform sentence segmentation, tokenization, POS tagging, lemmatization and dependency tree parsing.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7432883679866791}, {"text": "tokenization", "start_pos": 136, "end_pos": 148, "type": "TASK", "confidence": 0.9575374126434326}, {"text": "POS tagging", "start_pos": 150, "end_pos": 161, "type": "TASK", "confidence": 0.8594542443752289}, {"text": "dependency tree parsing", "start_pos": 181, "end_pos": 204, "type": "TASK", "confidence": 0.718755841255188}]}, {"text": "The goal of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies (CoNLL 2017 UD Shared Task) is to stimulate research in multi-lingual dependency parsers which process raw text only.", "labels": [], "entities": [{"text": "Multilingual Parsing from Raw Text to Universal Dependencies (CoNLL 2017 UD Shared Task)", "start_pos": 40, "end_pos": 128, "type": "TASK", "confidence": 0.7209868292013805}]}, {"text": "The overview of the task and the results are presented in.", "labels": [], "entities": []}, {"text": "This paper describes UDPipe ( 1 -an open-source tool which automatically generates sentence segmentation, tokenization, POS tagging, lemmatization and dependency trees, using UD version 2 treebanks as training data.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.7377491146326065}, {"text": "POS tagging", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.7712348401546478}]}, {"text": "The contributions of this paper are: \u2022 Description of UDPipe 1.1 Baseline System, which was used to provide baseline models for CoNLL 2017 UD Shared Task and preprocessed test sets for the CoNLL 2017 UD Shared Task participants.", "labels": [], "entities": [{"text": "UDPipe 1.1 Baseline System", "start_pos": 54, "end_pos": 80, "type": "DATASET", "confidence": 0.6480467692017555}, {"text": "CoNLL 2017 UD Shared Task", "start_pos": 128, "end_pos": 153, "type": "DATASET", "confidence": 0.8831699609756469}, {"text": "CoNLL 2017 UD Shared Task participants", "start_pos": 189, "end_pos": 227, "type": "DATASET", "confidence": 0.9335813721021017}]}, {"text": "UDPipe 1.1 provided a strong baseline for the task, placing as the 13 th (out of 33) best system in the official ranking.", "labels": [], "entities": [{"text": "UDPipe 1.1", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9436897039413452}]}, {"text": "The UDPipe 1.1 Baseline System is described in Section 3.", "labels": [], "entities": [{"text": "UDPipe 1.1 Baseline System", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.7883453816175461}]}, {"text": "\u2022 Description of UDPipe 1.2 Participant System, an improved variant of UDPipe 1.1, which was used as a contestant system in the CoNLL 2017 UD Shared Task, finishing 8 thin the official ranking, while keeping very low software requirements.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 128, "end_pos": 153, "type": "DATASET", "confidence": 0.8161127924919128}]}, {"text": "The UDPipe 1.2 Participant System is described in Section 4.", "labels": [], "entities": []}, {"text": "\u2022 Evaluation of search-based oracle and several transition-based system on UD 2.0 dependency trees (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "There are three testing collections in CoNLL 2017 UD Shared Task: UD 2.0 test data, new parallel treebank (PUD) sets, and four surprise languages.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 39, "end_pos": 64, "type": "DATASET", "confidence": 0.8627671837806702}, {"text": "UD 2.0 test data", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.7436223775148392}]}, {"text": "The UDPipe 1.1 Baseline System models were completely trained, released and \"frozen\" on the UD 2.0 training and development data with anew split (see the previous Section 3.5) already in the training phase of the CoNLL 2017 UD Shared Task on the UD 2.0 training data, unlike the participant systems, which could use the full training data for training and development data for tuning.", "labels": [], "entities": [{"text": "UDPipe 1.1 Baseline System", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.8251949846744537}, {"text": "UD 2.0 training and development data", "start_pos": 92, "end_pos": 128, "type": "DATASET", "confidence": 0.83777783314387}, {"text": "CoNLL 2017 UD Shared Task", "start_pos": 213, "end_pos": 238, "type": "DATASET", "confidence": 0.8524177670478821}, {"text": "UD 2.0 training data", "start_pos": 246, "end_pos": 266, "type": "DATASET", "confidence": 0.8178994655609131}]}, {"text": "We used the UDPipe 1.1 Baseline System models for evaluation of the completely new parallel treebank (PUD) set and completely new surprise languages in the following way: For the new parallel treebank sets we utilized the \"main\" treebank for each language (e.g., for Finish fi instead of fi ftb).", "labels": [], "entities": []}, {"text": "This arbitrary decision was a lucky one -after the shared task evaluation, the performance on the parallel treebanks was shown to be significantly worse if different treebanks than the \"main\" were used (even if they were larger or provided higher LAS on their own test set).", "labels": [], "entities": [{"text": "LAS", "start_pos": 247, "end_pos": 250, "type": "METRIC", "confidence": 0.9261259436607361}]}, {"text": "The reason seem to be the inconsistencies among the treebanks of the same languagethe Universal Dependencies are yet not so universal as everyone would like.", "labels": [], "entities": []}, {"text": "To parse the surprise languages, we employed a baseline model which resulted in highest LAS F1-score on the surprise language sample dataresulting in Finnish FTB, Polish, Finnish FTB and Slovak models for the surprise languages Buryat, Kurmanji, North S\u00e1mi and Upper Sorbian, respectively.", "labels": [], "entities": [{"text": "LAS F1-score", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.8512293100357056}, {"text": "surprise language sample dataresulting", "start_pos": 108, "end_pos": 146, "type": "DATASET", "confidence": 0.7114221453666687}]}, {"text": "Naturally, most words of a surprise language are not recognized by a baseline model fora different language.", "labels": [], "entities": []}, {"text": "Conveniently, the UPOS tags and FEATS are shared across languages, allowing the baseline model to operate similarly to a delexicalized parser.", "labels": [], "entities": [{"text": "FEATS", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9909365773200989}]}, {"text": "The official CoNLL 2017 UD Shared Task evaluation was performed using a TIRA platform), which provided virtual machines for every participants' systems.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task evaluation", "start_pos": 13, "end_pos": 49, "type": "DATASET", "confidence": 0.8512167036533356}]}, {"text": "During test data evaluation, the machines were disconnected from the internet, and reset after the evaluation finished -this way, the entire test sets were kept private even during the evaluation.", "labels": [], "entities": []}, {"text": "In addition to official results, we also report results of supplementary experiments.", "labels": [], "entities": []}, {"text": "These were evaluated after the shared task, using the released test data ().", "labels": [], "entities": []}, {"text": "All results are produced using the official evaluation script.", "labels": [], "entities": []}, {"text": "Because only plain text (and not gold tokenization) is used as input, all results are in fact F1-scores and always take tokenization performance into account.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9967220425605774}]}, {"text": "The complete UDPipe 1.2 Participant System scores are shown in.", "labels": [], "entities": [{"text": "UDPipe 1.2 Participant System", "start_pos": 13, "end_pos": 42, "type": "DATASET", "confidence": 0.7487342655658722}]}, {"text": "We also include LAS F1-score of the UDPipe 1.1 Baseline System for reference.", "labels": [], "entities": [{"text": "LAS F1-score", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.7657002508640289}, {"text": "UDPipe 1.1 Baseline System", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.8955575823783875}]}, {"text": "Note that due to time constraints, some UDPipe 1.2 Participant System submitted models did not generate any XPOS and lemmas.", "labels": [], "entities": [{"text": "UDPipe 1.2 Participant System submitted", "start_pos": 40, "end_pos": 79, "type": "DATASET", "confidence": 0.7822998881340026}]}, {"text": "In these cases, we show XPOS and lemmatization results using post-competition models and typeset them in italic.: Joint segmentation and parsing in UDPipe 1.2 Participant System, optimized to maximize parsing likelihood, in comparison with sequential segmentation and parsing.", "labels": [], "entities": [{"text": "XPOS", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9742861986160278}, {"text": "parsing", "start_pos": 137, "end_pos": 144, "type": "TASK", "confidence": 0.9527707099914551}, {"text": "parsing", "start_pos": 201, "end_pos": 208, "type": "TASK", "confidence": 0.9637035727500916}, {"text": "parsing", "start_pos": 268, "end_pos": 275, "type": "TASK", "confidence": 0.9414607286453247}]}, {"text": "In order to make the extensive results more visual, we show relative difference of baseline LAS score using the grey bars (on a scale that ignores 3 outliers).", "labels": [], "entities": [{"text": "LAS score", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.8643631041049957}]}, {"text": "We use this visualization also in later tables, always showing relative difference to the first occurrence of the metric in question.", "labels": [], "entities": []}, {"text": "The effect of enlarging training data using other treebanks of the same language (Section 4.2) is evaluated in.", "labels": [], "entities": []}, {"text": "We include only those treebanks in which the enlarged training data result in better LAS score and compare the performance to cases in which only the original training data is used.", "labels": [], "entities": [{"text": "LAS score", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9382699131965637}]}, {"text": "The impact of tokenizer dimension 64 compared to dimension 24 can be found in.", "labels": [], "entities": []}, {"text": "We also include the effect of not using the suffix rules for multi-word token splitting, and not using multi-word token splitting at all.", "labels": [], "entities": [{"text": "multi-word token splitting", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6204810738563538}, {"text": "multi-word token splitting", "start_pos": 103, "end_pos": 129, "type": "TASK", "confidence": 0.6712877154350281}]}, {"text": "As expected, for many languages the dimension 64 does not change the results, but yields superior performance for languages with either difficult tokenization or sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.6650859862565994}]}, {"text": "The improvement resulting from joint sentence segmentation and parsing is evaluated in.", "labels": [], "entities": [{"text": "joint sentence segmentation", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.677465558052063}]}, {"text": "While the LAS and UAS F1-scores of the joint approach improves, the sentence segmentation F1-score deteriorates significantly.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9251953363418579}, {"text": "UAS F1-scores", "start_pos": 18, "end_pos": 31, "type": "METRIC", "confidence": 0.5784096121788025}, {"text": "sentence segmentation F1-score", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.6205497980117798}]}, {"text": "The overall effect of search-based oracle with various transition systems on parsing accuracy is.", "labels": [], "entities": [{"text": "parsing", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9684473872184753}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9668927788734436}]}, {"text": "The search-based oracle improves results in all cases, but the increase is only slight if a dynamic oracle is also used.", "labels": [], "entities": []}, {"text": "Note however that dynamic oracles for non-projective systems are usually either very inefficient (for link2, only O(n 8 ) dynamic oracle is proposed in) or not known (as is the case for the swap system).", "labels": [], "entities": []}, {"text": "Furthermore, if only a static oracle is used, partially or fully non-projective systems yield better overall performance than a projective one.", "labels": [], "entities": []}, {"text": "Yet, a dynamic oracle improves performance of the projective system to the extent it yield better results (which is further improved by utilizing also a search-based oracle).", "labels": [], "entities": []}, {"text": "The influence of beam size on UAS and LAS scores is analyzed in.", "labels": [], "entities": [{"text": "UAS", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.3754616975784302}, {"text": "LAS", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.8800585269927979}]}, {"text": "According to the results, tuning beam size for every treebank independently is worse than using large beam size all the time.", "labels": [], "entities": []}, {"text": "Finally, model size and runtime performance of individual UDPipe components are outlined in Table 8.", "labels": [], "entities": []}, {"text": "The median of complete model size is circa 13MB and the speed of full processing (tokenization, tagging and parsing with beam size 5) is approximately 1700 words per second on a single core of an Intel Xeon E5-2630 2.4GHz processor.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameters for joint segmentation and parsing.", "labels": [], "entities": [{"text": "joint segmentation", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.72648024559021}, {"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.8649770617485046}]}, {"text": " Table 2: Full results of UDPipe 1.2 Participant System and LAS F1-score of UDPipe 1.1 Baseline  System for reference. The results in italic are not part of the official results and were generated using  post-competition models due to time constraints.", "labels": [], "entities": [{"text": "UDPipe 1.2 Participant System", "start_pos": 26, "end_pos": 55, "type": "DATASET", "confidence": 0.7873218059539795}, {"text": "LAS F1-score", "start_pos": 60, "end_pos": 72, "type": "METRIC", "confidence": 0.8591415882110596}, {"text": "UDPipe 1.1 Baseline  System", "start_pos": 76, "end_pos": 103, "type": "DATASET", "confidence": 0.9002057462930679}]}, {"text": " Table 3: The effect of additional training data from other treebanks of the same language in UDPipe 1.2  Participant System.", "labels": [], "entities": [{"text": "UDPipe 1.2  Participant System", "start_pos": 94, "end_pos": 124, "type": "DATASET", "confidence": 0.8805504590272903}]}, {"text": " Table 5: Joint segmentation and parsing in UD- Pipe 1.2 Participant System, optimized to maxi- mize parsing likelihood, in comparison with se- quential segmentation and parsing.", "labels": [], "entities": [{"text": "Joint segmentation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6233130097389221}, {"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9031925797462463}, {"text": "UD- Pipe 1.2", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.8958903700113297}]}, {"text": " Table 7: UDPipe 1.2 Participant System parsing  scores with various beam sizes.", "labels": [], "entities": [{"text": "UDPipe 1.2 Participant System parsing", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6284832656383514}]}, {"text": " Table 4: Impact of tokenizer dimension 64 versus 24, no suffix rules for multi-word token splitting, and  no multi-word token splitting at all in the UDPipe 1.2 Participant System.", "labels": [], "entities": [{"text": "multi-word token splitting", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.6770617763201395}, {"text": "multi-word token splitting", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.6925162474314371}, {"text": "UDPipe 1.2 Participant System", "start_pos": 151, "end_pos": 180, "type": "DATASET", "confidence": 0.813206136226654}]}, {"text": " Table 6: The overall effect of search-based oracle on various transition systems.", "labels": [], "entities": []}]}