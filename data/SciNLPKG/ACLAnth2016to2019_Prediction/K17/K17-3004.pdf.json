{"title": [{"text": "IMS at the CoNLL 2017 UD Shared Task: CRFs and Perceptrons Meet Neural Networks", "labels": [], "entities": [{"text": "IMS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8769644498825073}, {"text": "CoNLL 2017 UD Shared Task", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.8560490250587464}]}], "abstractContent": [{"text": "This paper presents the IMS contribution to the CoNLL 2017 Shared Task.", "labels": [], "entities": [{"text": "IMS", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8936905860900879}, {"text": "CoNLL 2017 Shared Task", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.791795164346695}]}, {"text": "In the preprocessing step we employed a CRF POS/morphological tagger and a neural tagger predicting supertags.", "labels": [], "entities": []}, {"text": "On some languages , we also applied word segmenta-tion with the CRF tagger and sentence seg-mentation with a perceptron-based parser.", "labels": [], "entities": []}, {"text": "For parsing we took an ensemble approach by blending multiple instances of three parsers with very different architectures.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9748277068138123}]}, {"text": "Our system achieved the third place overall and the second place for the surprise languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the IMS contribution to the CoNLL 2017 UD Shared Task (.", "labels": [], "entities": [{"text": "IMS", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.926849901676178}, {"text": "CoNLL 2017 UD Shared Task", "start_pos": 48, "end_pos": 73, "type": "DATASET", "confidence": 0.8001478314399719}]}, {"text": "Our submission to the Shared Task (ST) ranked third.", "labels": [], "entities": [{"text": "Shared Task (ST)", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8037826299667359}]}, {"text": "Our overall approach relies on established techniques for improving accuracies of dependency parsers, including strong preprocessing, supertagging and parser combination.", "labels": [], "entities": []}, {"text": "The task was to predict dependency trees from raw text.", "labels": [], "entities": []}, {"text": "To make the ST more accessible to participants, the organizers provided baseline predictions for all preprocessing steps (including word and sentence segmentation and POS/morphological feature predictions) using the baseline UDPipe system ().", "labels": [], "entities": [{"text": "word and sentence segmentation", "start_pos": 132, "end_pos": 162, "type": "TASK", "confidence": 0.6354001462459564}]}, {"text": "We scrutinized the baseline and considered where we could improve over it.", "labels": [], "entities": []}, {"text": "It turns out that, although the UDPipe baseline is a strong one, considerable parsing accuracy improvements can be gained by improving the preprocessing steps.", "labels": [], "entities": [{"text": "UDPipe baseline", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.8276403844356537}, {"text": "parsing", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9640128016471863}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9185925126075745}]}, {"text": "In particular, we applied our own POS/morphology tagging using a CRF tagger and supertagging (Ouchi et al., * All three authors contributed equally.) with a neural tagger.", "labels": [], "entities": [{"text": "POS/morphology tagging", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.6934332251548767}]}, {"text": "Additionally, we performed our own word and/or sentence segmentation on a subset of the test sets.", "labels": [], "entities": [{"text": "word and/or sentence segmentation", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.6661032835642496}]}, {"text": "For the parsing step we applied an ensemble approach using three different parsers, sometimes using multiple instances of the same parser: one graph-based parser trained with the perceptron; one transition-based beam search parser also trained with the perceptron; and one greedy transition-based parser trained with neural networks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9779781699180603}]}, {"text": "The parser outputs were combined through blending (also known as reparsing;) using the Chu-Liu-Edmonds algorithm (.", "labels": [], "entities": []}, {"text": "The final test runs were carried out on the TIRA platform () where participants were assigned a virtual machine.", "labels": [], "entities": [{"text": "TIRA platform", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.882453054189682}]}, {"text": "To ensure that our final test run would finish on time on the VM, we established a time budget for each treebank and set a goal that a full test run should finish within 24 hours.", "labels": [], "entities": [{"text": "VM", "start_pos": 62, "end_pos": 64, "type": "DATASET", "confidence": 0.9780809283256531}]}, {"text": "Thus we applied a combination search under a time constraint to limit the maximal number of instances of the individual parsers.", "labels": [], "entities": []}, {"text": "An interesting aspect of the ST was the introduction of four surprise languages.", "labels": [], "entities": [{"text": "ST", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.8487096428871155}]}, {"text": "These languages were only announced one week before the test phase at which point the participants were provided with roughly 20 gold standard sentences for each language.", "labels": [], "entities": []}, {"text": "Unfortunately, among the allowed external resources the amount of parallel data for the surprise languages was rather limited.", "labels": [], "entities": []}, {"text": "This prevented us from using cross-lingual techniques or multilingual word vectors.", "labels": [], "entities": []}, {"text": "We therefore resorted to blending models trained on the small samples as well as delexicalized models trained on other source languages.", "labels": [], "entities": []}, {"text": "Another challenge of the ST were 14 parallel new test domains for the known languages.", "labels": [], "entities": [{"text": "ST", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.8599385023117065}]}, {"text": "Since the UD annotation scheme is applied on all of the treebanks, this suggests that the training data of the same language from different domains could be combined.", "labels": [], "entities": []}, {"text": "We made several experiments in this direction and trained models on merged treebanks for most of the parallel test sets (Section 7).", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses our preprocessing steps, including word and sentence segmentation, POS and morphological tagging, and supertagging.", "labels": [], "entities": [{"text": "word and sentence segmentation", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.6140080541372299}, {"text": "POS and morphological tagging", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.7655814737081528}]}, {"text": "In Section 3 we describe the three baseline parsers, while blending is reviewed in Section 4.", "labels": [], "entities": [{"text": "blending", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9265937805175781}]}, {"text": "In Section 5 we go through our pipeline and show results on the development data.", "labels": [], "entities": []}, {"text": "Sections 6 and 7 describe our approaches to the surprise languages and parallel test sets, respectively.", "labels": [], "entities": []}, {"text": "Our official test set results are shown in Section 8 and Section 9 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate the aforementioned methods on the 55 treebanks for which development data was available.", "labels": [], "entities": []}, {"text": "For the 14 additional parallel datasets (PUD) we used parsers trained on their corresponding languages.", "labels": [], "entities": []}, {"text": "For several languages there were more than one treebank in the training data for the corresponding PUD test set.", "labels": [], "entities": [{"text": "PUD test set", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.7368927498658498}]}, {"text": "This begs the question as to whether the models used for the PUD test sets should be trained only on the primary treebank, or on the combination of all training sets corresponding to that language.", "labels": [], "entities": [{"text": "PUD test sets", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.7868488232294718}]}, {"text": "For the main treebanks, initial experiments indicated that this was a bad idea and parsers performed better when training sets were not combined.", "labels": [], "entities": []}, {"text": "However, for the PUD test sets we had no information on the annotation scheme nor the domain, which made it difficult to decide whether to use only the primary training set or all available.", "labels": [], "entities": [{"text": "PUD test sets", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9069744547208151}]}, {"text": "For each language with multiple training sets, we trained one parser on each training set as well as on their concatenation.", "labels": [], "entities": []}, {"text": "We applied these models on the development sets and created a confusion matrix.", "labels": [], "entities": []}, {"text": "Without prior knowledge about the PUD treebanks, we estimated the expected LAS as the average LAS of the development sets and chose the model that maximizes the estimation.", "labels": [], "entities": [{"text": "PUD treebanks", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8163236677646637}, {"text": "LAS", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9841034412384033}, {"text": "LAS", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9708870649337769}]}, {"text": "shows such a confusion matrix for Swedish using the TN parser.", "labels": [], "entities": []}, {"text": "The expected LAS for PUD (right-most column) is highest when trained on the concatenation of the two treebanks.", "labels": [], "entities": [{"text": "LAS", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9985800981521606}, {"text": "PUD", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.7474296689033508}]}, {"text": "This observation held for all the languages with multiple treebanks that we tested and we therefore used models trained on the concatenation of all training data with two exceptions: For Czech time prevented us from training models and creating a confusion matrix and we only used models trained on the primary treebank.", "labels": [], "entities": []}, {"text": "For Finish FTB the README distributed with the treebank states that this treebank is a conversion that tries to approximate the primary Finish treebank.", "labels": [], "entities": [{"text": "Finish FTB", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.39033395051956177}, {"text": "README", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.978264570236206}, {"text": "Finish treebank", "start_pos": 136, "end_pos": 151, "type": "DATASET", "confidence": 0.8365043103694916}]}, {"text": "This suggests that it does not entirely conform to the Finish UD standard.", "labels": [], "entities": [{"text": "Finish UD standard", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.8723832368850708}]}, {"text": "We assumed that the Finish PUD test set would be closer to the primary treebank and therefore chose to use only the model trained on the primary treebank.", "labels": [], "entities": [{"text": "Finish PUD test set", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.6941661387681961}]}], "tableCaptions": [{"text": " Table 1: F1 scores for word segmentation and  gains in LAS for TP.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9996209144592285}, {"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7350460588932037}, {"text": "LAS", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.8064813613891602}]}, {"text": " Table 2: F1 score for sentence segmentation and  gains in LAS for TP.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9785465896129608}, {"text": "sentence segmentation", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7177468985319138}, {"text": "LAS", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.7943746447563171}]}, {"text": " Table 3: Average (across 55 treebanks) gains in  parsing accuracies (LAS) for incremental changes  to UDPipe preprocessing baseline.", "labels": [], "entities": [{"text": "parsing accuracies (LAS)", "start_pos": 50, "end_pos": 74, "type": "METRIC", "confidence": 0.7451977372169495}]}, {"text": " Table 4: Development results. The treebanks for which we did our own word and/or sentence segmen- tation are marked with \u2297 and respectively. The TP and TN models correspond to TP-l2r TN-l2r-vec,  respectively.", "labels": [], "entities": []}, {"text": " Table 5: Parsing accuracy (LAS) for surprise lan- guages: the three best delexicalized TP-l2r parsers  and lexicalized parser obtained by leave-one-out  jackknifing.", "labels": [], "entities": [{"text": "accuracy (LAS)", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.9296180307865143}]}, {"text": " Table 6: Confusion matrix for Swedish with ex- pected LAS on Swedish PUD.", "labels": [], "entities": []}]}