{"title": [{"text": "A Semi-universal Pipelined Approach to the CoNLL 2017 UD Shared Task", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 43, "end_pos": 68, "type": "DATASET", "confidence": 0.9097682714462281}]}], "abstractContent": [{"text": "This paper presents the TRL team's system submitted for the CoNLL 2017 Shared Task, \"Multilingual Parsing from Raw Text to Universal Dependencies.\"", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.7878269255161285}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.8130028247833252}]}, {"text": "We ran the system for all languages with our own fully pipelined components without relying on either pre-trained baseline or machine learning techniques.", "labels": [], "entities": []}, {"text": "We used only the universal part-of-speech tags and distance between words, and applied deter-ministic rules to assign labels.", "labels": [], "entities": []}, {"text": "The delex-icalized models are suitable for cross-lingual transfer or universal approaches.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7285973429679871}]}, {"text": "Experimental results show that our model performed well in some metrics and leads discussion on topics such as contribution of each component and on syntactic similarities among languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "We tested dependency-based syntactic parsing in 49 languages on Universal Dependencies () using 81 corpora from the UD version 2.0 datasets ( ).", "labels": [], "entities": [{"text": "dependency-based syntactic parsing", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6321088075637817}, {"text": "UD version 2.0 datasets", "start_pos": 116, "end_pos": 139, "type": "DATASET", "confidence": 0.8347921222448349}]}, {"text": "The task is described in the overview paper) and the whole system is evaluated on the TIRA platform ().", "labels": [], "entities": [{"text": "TIRA platform", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.8246274292469025}]}, {"text": "Instead of merely pursuing higher scores in the shared task, we adopted several strategies in the design of our parser: Self-contained system.", "labels": [], "entities": []}, {"text": "To keep capabilities to control the input and output of the system, we use only our own components for the whole pipeline including sentence splitter, tokenizer, lemmatizer, PoS tagger, dependency parser and role labeler.", "labels": [], "entities": [{"text": "sentence splitter", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7544131278991699}, {"text": "PoS tagger", "start_pos": 174, "end_pos": 184, "type": "TASK", "confidence": 0.7503852546215057}, {"text": "dependency parser", "start_pos": 186, "end_pos": 203, "type": "TASK", "confidence": 0.7491946220397949}]}, {"text": "We do not rely on any existing preprocessors such as UDPipe () and SyntaxNet (.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9129903316497803}]}, {"text": "When there are multiple corpora in a language with different annotation strategies, our system does not optimize models for each corpus, because the real applications do not assume such specific corpora.", "labels": [], "entities": []}, {"text": "We use merely simple statistics with parts of speech of each word and distance between words, and induced deterministic rules.", "labels": [], "entities": []}, {"text": "Neither higher order models nor word embeddings are used, thus our system is fully controllable with linguistic knowledge.", "labels": [], "entities": []}, {"text": "Components in the pipeline can be divided and optimized independently so that they are interchangeable with other corresponding components such as the UDPipe tokenizer.", "labels": [], "entities": []}, {"text": "Our dependency parser relies only on Universal PoS tags and does not use an extended PoS, lemma nor features annotated by a specific tokenizer.", "labels": [], "entities": []}, {"text": "Our system was composed under these constraints at the sacrifice of overall scores but it performed marginally well, achieving the best participant scores in a number of metrics.", "labels": [], "entities": []}, {"text": "The major contributions in this report are as follows: 1.", "labels": [], "entities": []}, {"text": "Report of runs without UDPipe with very different results than those obtained from other participants.", "labels": [], "entities": []}, {"text": "2. Experiments in cross-lingual and universal scenarios by using delexicalized statistics of different languages.", "labels": [], "entities": []}, {"text": "3. Simple and reusable techniques to induce rules for PoS tagging and relation labeling.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.8997335135936737}, {"text": "relation labeling", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7757479250431061}]}, {"text": "Section 2 describes each components in our pipeline.", "labels": [], "entities": []}, {"text": "Section 3 reports our results, including ablation studies and additional experiments in cross-lingual and multilingual settings.", "labels": [], "entities": [{"text": "ablation", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9401258826255798}]}, {"text": "Section 4 shows some related prior work related to our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the advantages of Universal Dependencies is the capability to test the language independent model and cross-lingual transfer learning.", "labels": [], "entities": [{"text": "cross-lingual transfer learning", "start_pos": 109, "end_pos": 140, "type": "TASK", "confidence": 0.7619069615999857}]}, {"text": "As described in Section 2.4, our dependency parsing models without any lexical information are very general.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7745680809020996}]}, {"text": "They therefore can be applied to other languages enabling us to test a universal language model.", "labels": [], "entities": []}, {"text": "compares the UAS scores with the cross-lingual and universal settings.", "labels": [], "entities": [{"text": "UAS", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6334452629089355}]}, {"text": "The 'Own model' column shows the original score, the 'Best transfer' column shows the score using the model that performed the best among different languages, and the 'Universal' column shows the score obtained with the combined statistics extracted from all of the multilingual corpora.", "labels": [], "entities": []}, {"text": "Numbers in bold denote that the transfer or universal model outperformed the language specific model.", "labels": [], "entities": []}, {"text": "Japanese (ja) and Korean (ko) were not tested here because they did not use compatible models.", "labels": [], "entities": []}, {"text": "The experimental result shows the best models for applying low-resource languages: fi for bxr, cs for kmr, tr for sme and hr for hsb.", "labels": [], "entities": []}, {"text": "Also for relatively low-resource languages such as Kazakh (kk) and Ukrainian (uk), the models with larger corpora outperformed their own models.", "labels": [], "entities": []}, {"text": "For four French (fr) corpora, the Portuguese (pt) model performed as well as the French model.", "labels": [], "entities": []}, {"text": "This suggests the model with three different French corpora generated a noisy model.", "labels": [], "entities": []}, {"text": "It is interesting to consider the 'neighbor' languages in terms of syntax.", "labels": [], "entities": []}, {"text": "English and Swedish (sv) selected each other as the closest languages, which suggests that they are selected not only be-  cause of the size of the training corpora.", "labels": [], "entities": []}, {"text": "It is also notable that two variants of Norwegian (no) were closest for different languages (Danish (da) and Swedish).", "labels": [], "entities": []}, {"text": "Even the universal model performed well.", "labels": [], "entities": []}, {"text": "The drop in UAS scores from the language-specific result was only 4.5 points on average.", "labels": [], "entities": [{"text": "UAS", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.5417643189430237}]}, {"text": "This shows our method is general enough for multilingual design.", "labels": [], "entities": []}, {"text": "Not only for low-resource languages such as Ukrainian and surprise languages, but also for Russian (ru) and Slovak (sk), the universal model outperformed the language specific model.", "labels": [], "entities": []}, {"text": "shows the difference in UAS scores when we did not apply one of the sets of rules to change the dependency structures described in Section 2.4 and LAS scores without one of refinements for relation labels described in Section 2.5.", "labels": [], "entities": [{"text": "UAS", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.3806881010532379}, {"text": "LAS", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.9565061926841736}]}, {"text": "The identification of multi word tokens did notwork well as expected, and the word level rules made little contribution.", "labels": [], "entities": [{"text": "identification of multi word tokens", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.7791104197502137}]}], "tableCaptions": [{"text": " Table 4: Overall F1 scores over test data (see Section 3.1).", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9985223412513733}]}, {"text": " Table 5: UAS-F1 scores with language specific  models, and transfer models (see Section 3.2.", "labels": [], "entities": []}]}