{"title": [{"text": "CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.8588175922632217}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.8218681812286377}]}], "abstractContent": [{"text": "The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets.", "labels": [], "entities": [{"text": "Computational Natural Language Learning (CoNLL)", "start_pos": 18, "end_pos": 65, "type": "TASK", "confidence": 0.6532804540225438}]}, {"text": "In 2017, one of two tasks was devoted to learning dependency parsers fora large number of languages, in a real-world setting without any gold-standard annotation on input.", "labels": [], "entities": [{"text": "learning dependency parsers", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6288650035858154}]}, {"text": "All test sets followed a unified annotation scheme, namely that of Universal Dependencies.", "labels": [], "entities": []}, {"text": "In this paper, we define the task and evaluation methodology , describe data preparation, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular.", "labels": [], "entities": [{"text": "parsing research", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.9362438023090363}, {"text": "dependency parsing", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8413044214248657}]}, {"text": "For the first time dependency treebanks in more than ten languages were available for learning parsers.", "labels": [], "entities": []}, {"text": "Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications.", "labels": [], "entities": []}, {"text": "While the two tasks ( were extremely important insetting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to-1 kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible.", "labels": [], "entities": []}, {"text": "CoNLL 2017 has picked up the threads of those pioneering tasks and addressed these two issues.", "labels": [], "entities": [{"text": "CoNLL 2017", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9408813714981079}]}, {"text": "The focus of the 2017 task was learning syntactic dependency parsers that can work in a realworld setting, starting from raw text, and that can work over many typologically different languages, even surprise languages for which there is little or no training data, by exploiting a common syntactic annotation standard.", "labels": [], "entities": [{"text": "syntactic dependency parsers", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6570677359898885}]}, {"text": "This task has been made possible by the Universal Dependencies initiative (UD) (), which has developed treebanks for 50+ languages with crosslinguistically consistent annotation and recoverability of the original raw texts.", "labels": [], "entities": []}, {"text": "Participating systems had to find labeled syntactic dependencies between words, i.e., a syntactic head for each word, and a label classifying the type of the dependency relation.", "labels": [], "entities": []}, {"text": "No gold-standard annotation (tokenization, sentence segmentation, lemmas, morphology) was available in the input text.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.6908537596464157}]}, {"text": "However, teams wishing to concentrate just on parsing were able to use segmentation and morphology predicted by the baseline UDPipe system ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9746063947677612}]}], "datasetContent": [{"text": "The standard evaluation metric of dependency parsing is the labeled attachment score (LAS), i.e., the percentage of nodes with correctly assigned reference to parent node, including the label (type) of the relation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.766394317150116}, {"text": "labeled attachment score (LAS)", "start_pos": 60, "end_pos": 90, "type": "METRIC", "confidence": 0.8625767827033997}]}, {"text": "When parsers are applied to raw text, the metric must be adjusted to the possibility that the number of nodes in gold-standard annotation and in the system output vary.", "labels": [], "entities": []}, {"text": "Therefore, the evaluation starts with aligning system nodes and gold nodes.", "labels": [], "entities": []}, {"text": "A dependency relation cannot be counted as correct if one of the nodes could not be aligned to a gold node.", "labels": [], "entities": []}, {"text": "LAS is then re-defined as the harmonic mean (F 1 ) of precision P and recall R, where Note that attachment of all nodes including punctuation is evaluated.", "labels": [], "entities": [{"text": "LAS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8953626751899719}, {"text": "harmonic mean (F 1 )", "start_pos": 30, "end_pos": 50, "type": "METRIC", "confidence": 0.8019100427627563}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.8536667823791504}, {"text": "recall R", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9802570343017578}]}, {"text": "LAS is computed separately for each of the 81 test files and a macroaverage of all these scores serves as the main metric for system ranking in the task.", "labels": [], "entities": [{"text": "LAS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9649685025215149}]}, {"text": "Key goals of any empirical evaluation are to ensure a blind evaluation, its replicability, and its reproducibility.", "labels": [], "entities": []}, {"text": "To facilitate these goals, we employed the cloud-based evaluation platform TIRA (), which implements the evaluation as a service paradigm.", "labels": [], "entities": []}, {"text": "In doing so, we depart from the traditional submission of system output to shared tasks, which lacks in these regards, toward the submission of working software.", "labels": [], "entities": []}, {"text": "Naturally, software submissions bring about additional overhead for both organizers and participants, whereas the goal of an evaluation platform like TIRA is to reduce this overhead to a bearable level.", "labels": [], "entities": []}, {"text": "Still being an early prototype, though, TIRA fulfills this goal only with some reservations.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.45110026001930237}]}, {"text": "Nevertheless, the scale of the CoNLL 2017 UD Shared Task also served as a test of scalability of the evaluation as a service paradigm in general as well as that of TIRA in particular.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.8193287968635559}]}, {"text": "Traditionally, evaluations in shared tasks are halfblind (the test data are shared with participants while the ground truth is withheld), whereas outside shared tasks, say, during paper-writing, evaluations are typically pseudo-blind (the test data and they achieved a non-zero score on each of the 81 test files; a zero score signaled a bug, in which case the task moderator would make the diagnostic output visible to the participants.", "labels": [], "entities": []}, {"text": "Such interaction was only possible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop.", "labels": [], "entities": []}, {"text": "Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.6490579843521118}]}], "tableCaptions": [{"text": " Table 2: Ranking of the participating systems by  the main evaluation metric, the labeled attach- ment F 1 -score, macro-averaged over 81 test sets.  Pairs of systems with significantly (p < 0.05) dif- ferent LAS are separated by a line. Names of  several teams are abbreviated in the table: LyS- FASTPARSE, OpenU NLP Lab, Orange -Deski\u00f1  and\u00b4UFALand\u00b4 and\u00b4UFAL -UDPipe 1.2. Citations refer to the  corresponding system-description papers in this  volume.", "labels": [], "entities": [{"text": "attach- ment F 1 -score", "start_pos": 91, "end_pos": 114, "type": "METRIC", "confidence": 0.7325044402054378}, {"text": "OpenU NLP Lab", "start_pos": 309, "end_pos": 322, "type": "DATASET", "confidence": 0.9319407343864441}]}, {"text": " Table 3: Average CLAS F 1 score.", "labels": [], "entities": [{"text": "Average CLAS F 1 score", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8755966424942017}]}, {"text": " Table 4: Tokenization, word segmentation and  sentence segmentation (ordered by word F 1  scores; out-of-order scores in the other two  columns are bold).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7966000437736511}, {"text": "sentence segmentation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7350692301988602}, {"text": "word F 1  scores", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.7723622992634773}]}, {"text": " Table 5: Universal POS tags, features and lemmas  (ordered by UPOS F 1 scores).", "labels": [], "entities": [{"text": "UPOS F 1 scores", "start_pos": 63, "end_pos": 78, "type": "METRIC", "confidence": 0.623648464679718}]}, {"text": " Table 6: Average attachment score on the 55 \"big\"  treebanks.", "labels": [], "entities": [{"text": "Average attachment score", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8899882237116495}]}, {"text": " Table 7: Average attachment score on the 4 sur- prise languages: Buryat (bxr), Kurmanji (kmr),  North S\u00e1mi (sme) and Upper Sorbian (hsb).", "labels": [], "entities": [{"text": "attachment score", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.8450927734375}]}, {"text": " Table 8: Average attachment score on the 8  small treebanks: French ParTUT, Galician Tree- Gal, Irish, Kazakh, Latin, Slovenian SST, Uyghur  and Ukrainian.", "labels": [], "entities": [{"text": "Average attachment score", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.84753946463267}, {"text": "French", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.7969055771827698}, {"text": "ParTUT", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.7589787840843201}, {"text": "Galician Tree- Gal", "start_pos": 77, "end_pos": 95, "type": "DATASET", "confidence": 0.8050702661275864}]}, {"text": " Table 9: Average attachment score on the 14 par- allel test sets (PUD).", "labels": [], "entities": [{"text": "Average attachment score", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.838837206363678}, {"text": "par- allel test sets (PUD)", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.7970421314239502}]}, {"text": " Table 10: Treebank ranking by best parser LAS.  Bold CLAS is higher than the preceding one. Best  F 1 of word and sentence segmentation is also  shown. ISO 639 language codes are optionally fol- lowed by a treebank code.", "labels": [], "entities": [{"text": "word and sentence segmentation", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.583568885922432}]}, {"text": " Table 11: Classification of participating systems. The second column repeats the main system ranking.", "labels": [], "entities": []}]}