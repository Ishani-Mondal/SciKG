{"title": [{"text": "Learning Contextual Embeddings for Structural Semantic Similarity using Categorical Information", "labels": [], "entities": [{"text": "Structural Semantic Similarity", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.7549963394800822}]}], "abstractContent": [{"text": "Tree kernels (TKs) and neural networks are two effective approaches for automatic feature engineering.", "labels": [], "entities": [{"text": "automatic feature engineering", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.5967554946740469}]}, {"text": "In this paper, we combine them by modeling context word similarity in semantic TKs.", "labels": [], "entities": []}, {"text": "This way, the latter can operate subtree matching by applying neural-based similarity on tree lexical nodes.", "labels": [], "entities": [{"text": "subtree matching", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7500797212123871}]}, {"text": "We study how to learn representations for the words in context such that TKs can exploit more focused information.", "labels": [], "entities": []}, {"text": "We found that neural em-beddings produced by current methods do not provide a suitable contextual similarity.", "labels": [], "entities": []}, {"text": "Thus, we define anew approach based on a Siamese Network, which produces word representations while learning a binary text similarity.", "labels": [], "entities": []}, {"text": "We set the latter considering examples in the same category as similar.", "labels": [], "entities": []}, {"text": "The experiments on question and sentiment classification show that our semantic TK highly improves previous results .", "labels": [], "entities": [{"text": "question and sentiment classification", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.7251210957765579}]}], "introductionContent": [{"text": "Structural Kernels () can automatically represent syntactic and semantic structures in terms of substructures, showing high accuracy in several tasks, e.g., relation extraction ( and sentiment analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9979470372200012}, {"text": "relation extraction", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.9256715476512909}, {"text": "sentiment analysis", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.9391607344150543}]}, {"text": "At the same time, deep learning has demonstrated its effectiveness on a plethora of NLP tasks such as Question Answering (QA), and parsing (, to name a few.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.8754282474517823}, {"text": "parsing", "start_pos": 131, "end_pos": 138, "type": "TASK", "confidence": 0.9705331325531006}]}, {"text": "Deep learning models (DLMs) usually do not include traditional features; they extract relevant signals from distributed representations of words, by applying a sequence of linear and nonlinear functions to the input.", "labels": [], "entities": []}, {"text": "Word representations are learned from large corpora, or directly from the training data of the task at hand.", "labels": [], "entities": []}, {"text": "Clearly, joining the two approaches above would have the advantage of easily integrating structures with kernels, and lexical representations with embeddings into learning algorithms.", "labels": [], "entities": []}, {"text": "In this respect, the Smoothed Partial Tree Kernel (SPTK) is a noticeable approach for using lexical similarity in tree structures.", "labels": [], "entities": [{"text": "Smoothed Partial Tree Kernel (SPTK)", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.6100170271737235}]}, {"text": "SPTK can match different tree fragments, provided that they only differ in lexical nodes.", "labels": [], "entities": []}, {"text": "Although the results were excellent, the used similarity did not consider the fact that words in context assume different meanings or weights for the final task, i.e., it does not consider the context.", "labels": [], "entities": []}, {"text": "In contrast, SPTK would benefit to use specific word similarity when matching subtrees corresponding to different constituency.", "labels": [], "entities": []}, {"text": "For example, the two questions: -What famous model was married to Billy Joel?", "labels": [], "entities": []}, {"text": "-What famous model of the Universe was proposed? are similar in terms of structures and words but clearly have different meaning and also different categories: the first asks fora human (the answer is Christie Brinkley) whereas the latter asks for an entity (an answer could be the Expanding Universe).", "labels": [], "entities": []}, {"text": "To determine that such questions are not similar, SPTK would need different embeddings for the word model in the two contexts, i.e., those related to person and science, respectively.", "labels": [], "entities": []}, {"text": "In this paper, we use distributed representations generated by neural approaches for computing the lexical similarity in TKs.", "labels": [], "entities": []}, {"text": "We carryout an extensive comparison between different methods, i.e., word2vec, using CBOW and SkipGram, and Glove, in terms of their impact on convolution semantic TKs for question classification (QC).", "labels": [], "entities": [{"text": "Glove", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.756412148475647}, {"text": "question classification (QC)", "start_pos": 172, "end_pos": 200, "type": "TASK", "confidence": 0.8303321003913879}]}, {"text": "We experimented with composing word vectors and alternative embedding methods for bigger unit of text to obtain context specific vectors.", "labels": [], "entities": []}, {"text": "Unfortunately, the study above showed that standard ways to model context are not effective.", "labels": [], "entities": []}, {"text": "Thus, we propose a novel application of Siamese Networks to learn word vectors in context, i.e., a representation of a word conditioned on the other words in the sentence.", "labels": [], "entities": []}, {"text": "Since a comprehensive and large enough corpus of disambiguated senses is not available, we approximate them with categorical information: we derive a classification task that consists in deciding if two words extracted from two sentences belong to the same sentence category.", "labels": [], "entities": []}, {"text": "We use the obtained contextual word representations in TKs.", "labels": [], "entities": []}, {"text": "Our new approach tested on two tasks, question and sentiment classification, shows that modeling the context further improves the semantic kernel accuracy compared to only using standard word embeddings.", "labels": [], "entities": [{"text": "question and sentiment classification", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.6742613166570663}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9828848838806152}]}], "datasetContent": [{"text": "We compare SPTK models with our tree kernel model using neural word embeddings (NSPTK) on question classification (QC), a central task for question answering, and on sentiment classfication (SC).", "labels": [], "entities": [{"text": "question classification (QC)", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.8002066135406494}, {"text": "question answering", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.8136484026908875}, {"text": "sentiment classfication (SC)", "start_pos": 166, "end_pos": 194, "type": "TASK", "confidence": 0.8032757461071014}]}, {"text": "The QC dataset () contains a set of questions labelled according to a twolayered taxonomy, which describes their expected: QC test set accuracies (%) of NSPTK, given embeddings with window size equal to 5, and dimensionality ranging from 50 to 1,000.", "labels": [], "entities": [{"text": "QC dataset", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9030510783195496}, {"text": "QC test set accuracies", "start_pos": 123, "end_pos": 145, "type": "METRIC", "confidence": 0.6051803901791573}]}, {"text": "The coarse layer maps each question into one of 6 classes: Abbreviation, Description, Entity, Human, Location and Number.", "labels": [], "entities": []}, {"text": "Our experimental setting mirrors the setting of the original study: we train on 5,452 questions and test on 500.", "labels": [], "entities": []}, {"text": "The SC dataset is the one of SemEval Twitter'13 for message-level polarity classification ().", "labels": [], "entities": [{"text": "SC dataset", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.659868523478508}, {"text": "SemEval Twitter'13", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.8331078588962555}, {"text": "message-level polarity classification", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.8445417483647665}]}, {"text": "The dataset is organized in a training, development and test sets containing respectively 9,728, 1,654 and 3,813 tweets.", "labels": [], "entities": []}, {"text": "Each tweet is labeled as positive, neutral or negative.", "labels": [], "entities": []}, {"text": "The only preprocessing step we perform on tweets is to replace user mentions and url with a <USER> and <URL> token, respectively.", "labels": [], "entities": []}, {"text": "In the cross-validation experiments, we use the training data to produce the training and test folds, whereas we use the original test set as our validation set for tuning the parameters of the network.", "labels": [], "entities": []}, {"text": "Learning high quality word embeddings requires large textual corpora.", "labels": [], "entities": []}, {"text": "We train all the vectors for QC on the ukWaC corpus, also used in Croce et al.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.986523300409317}]}, {"text": "(2011) to obtain LSA vectors.", "labels": [], "entities": []}, {"text": "The corpus includes an annotation layer produced with TreeTagger 2 . We process the documents by attaching the POS-tag marker to each lemma.", "labels": [], "entities": []}, {"text": "We trained paragraph2vec vectors using the Gensim 3 toolkit.", "labels": [], "entities": [{"text": "Gensim 3 toolkit", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.6990998586018881}]}, {"text": "Word embeddings for the SC task are learned on a corpus of 50M English tweets collected from the Twitter API over two months, using word2vec and setting the dimension to 100.", "labels": [], "entities": [{"text": "SC task", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9100803732872009}]}, {"text": "We use GloVe word embeddings (300 dimensions), and we fix them during training.", "labels": [], "entities": []}, {"text": "Embeddings for words that are not present in    The size of the forward and backward states of the BiGRUs is set to 100, so the resulting concatenated state has 200 dimensions.", "labels": [], "entities": []}, {"text": "The number of stacked bidirectional networks is three and it was tuned on a development set.", "labels": [], "entities": []}, {"text": "This allows the network to have high capacity, fit the data, and have the best generalization ability.", "labels": [], "entities": []}, {"text": "The final layer learns higher order representations of the words in context.", "labels": [], "entities": []}, {"text": "We did not use dropout as a regularization mechanism since it did not show a significant difference on the performance of the network.", "labels": [], "entities": []}, {"text": "The network parameters are trained using the Adam optimizer, with a learning rate of 0.001.", "labels": [], "entities": []}, {"text": "The training examples are fed to the network in mini-batches.", "labels": [], "entities": []}, {"text": "The latter are balanced between positive and negative examples by picking 32 pairs of sentences sharing the same category, and 32 pairs of sentences from different categories.", "labels": [], "entities": []}, {"text": "Batches of 64 sentences are fed to the network.", "labels": [], "entities": []}, {"text": "The number of words sampled from each sentence is fixed to 4, and for this reason the final loss is computed over 256 pairs of words in context, for each mini-batch.", "labels": [], "entities": []}, {"text": "The network is then trained for 5 epochs, storing the parameters corresponding to the best registered accuracy on the validation set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9360571503639221}]}, {"text": "Those weights are later loaded and used to encode the words in a sentence by taking their corresponding output states from the last BiGRU unit.", "labels": [], "entities": []}, {"text": "72.8: SC results for NSPTK with word embeddings and the word-in-context embeddings.", "labels": [], "entities": [{"text": "SC", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.6552515625953674}]}, {"text": "Runs of selected systems are also reported.", "labels": [], "entities": []}, {"text": "models using SVM-Light-TK, an SVM-Light extension (Joachims, 1999) with tree kernel support.", "labels": [], "entities": []}, {"text": "We modified the software to lookup specific vectors for each word in a sentence.", "labels": [], "entities": []}, {"text": "We preprocessed each sentence with the LTH parser 4 and used its output to construct the LCT.", "labels": [], "entities": [{"text": "LCT", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.8322031497955322}]}, {"text": "We used the parameters for the QC classifiers from Croce et al.", "labels": [], "entities": []}, {"text": "(2011), while we selected them on the Twitter'13 dev.", "labels": [], "entities": []}, {"text": "set for the SC task.", "labels": [], "entities": [{"text": "SC task", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.8649897277355194}]}, {"text": "shows the QC accuracy of NSPTK with CBOW, SkipGram and GloVe.", "labels": [], "entities": [{"text": "QC", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.736140787601471}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.8477083444595337}, {"text": "CBOW", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.9133821725845337}, {"text": "SkipGram", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9515430927276611}]}, {"text": "The results are reported for vector dimensions (dim) ranging from 50 to 1000, with a fixed window size of 5.", "labels": [], "entities": []}, {"text": "The performance for the CBOW hierarchical softmax (hs) and negative sampling (ns), and for the SkipGram hs settings are similar.", "labels": [], "entities": [{"text": "CBOW hierarchical softmax", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.6446290810902914}, {"text": "SkipGram", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.927598774433136}]}, {"text": "For the SkipGram ns settings, the accuracy is slightly lower for smaller dimension sizes.", "labels": [], "entities": [{"text": "SkipGram ns", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.8723646402359009}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9996993541717529}]}, {"text": "GloVe embeddings yield a lower accuracy, which steadily increases with the size of the embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9994957447052002}]}, {"text": "In general, a higher dimension size produces higher accuracy, but also makes the training more expensive.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.999055802822113}]}, {"text": "500 dimensions seem a good trade-off between performance and computational cost.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: QC test set accuracies (%) of NSPTK, given em- beddings with window size equal to 5, and dimensionality  ranging from 50 to 1,000.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9468984007835388}]}, {"text": " Table 3: QC cross-validation accuracies (%) of NSPTK given  embeddings with the selected dimensionalities.", "labels": [], "entities": [{"text": "QC cross-validation accuracies", "start_pos": 10, "end_pos": 40, "type": "METRIC", "confidence": 0.5478615760803223}]}, {"text": " Table 4: QC accuracies for the word embeddings (CBOW  vectors with 500 dimensions, trained using hierarchical soft- max) and paragraph2vec.", "labels": [], "entities": []}, {"text": " Table 5: QC accuracies for NSPTK, using the word-in- context vector produced by the stacked BiGRU encoder  trained with the Siamese Network. Word vectors are trained  with CBOW (hs) and have 500 dimensions.", "labels": [], "entities": [{"text": "Siamese Network", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.8243045508861542}, {"text": "CBOW", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.7420182824134827}]}, {"text": " Table 6: SC results for NSPTK with word embeddings and  the word-in-context embeddings. Runs of selected systems  are also reported.", "labels": [], "entities": [{"text": "NSPTK", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.7338100075721741}]}]}