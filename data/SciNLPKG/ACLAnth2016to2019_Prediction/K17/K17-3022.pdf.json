{"title": [], "abstractContent": [{"text": "We present the Uppsala submission to the CoNLL 2017 shared task on parsing from raw text to universal dependencies.", "labels": [], "entities": [{"text": "Uppsala submission to the CoNLL 2017 shared task", "start_pos": 15, "end_pos": 63, "type": "DATASET", "confidence": 0.7405131049454212}, {"text": "parsing from raw text", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.8270363360643387}]}, {"text": "Our system is a simple pipeline consisting of two components.", "labels": [], "entities": []}, {"text": "The first performs joint word and sentence segmentation on raw text; the second predicts dependency trees from raw words.", "labels": [], "entities": [{"text": "joint word and sentence segmentation", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.6900570869445801}]}, {"text": "The parser bypasses the need for part-of-speech tagging, but uses word embeddings based on universal tag distributions.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7270991504192352}]}, {"text": "We achieved a macro-averaged LAS F1 of 65.11 in the official test run and obtained the 2nd best result for sentence segmentation with a score of 89.03.", "labels": [], "entities": [{"text": "LAS F1", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.858171284198761}, {"text": "sentence segmentation", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.7854279279708862}]}, {"text": "After fixing two bugs, we obtained an unofficial LAS F1 of 70.49.", "labels": [], "entities": [{"text": "LAS F1", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.8551173806190491}]}], "introductionContent": [{"text": "The CoNLL 2017 shared task differs from most previous multilingual dependency parsing tasks not only by using cross-linguistically consistent syntactic representations from the UD project (, but also by requiring systems to start from raw text, as opposed to presegmented and (often) pre-annotated words and sentences.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6382003923257192}]}, {"text": "Since systems are only evaluated on their output dependency trees (and indirectly on the word and sentence segmentation implicit in these trees), developers are free to choose what additional linguistic features (if any) to predict as part of the parsing process.", "labels": [], "entities": []}, {"text": "The Uppsala team has adopted a minimalistic stance in this respect and developed a system that does not predict any linguistic structure over and above a segmentation into sentences and words and a dependency structure over the words of each sentence.", "labels": [], "entities": [{"text": "Uppsala team", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9354843199253082}]}, {"text": "In particular, the system makes no use of part-of-speech tags, morphological features, or lemmas, despite the fact that these annotations are available in the training and development data.", "labels": [], "entities": []}, {"text": "In this way, we go against a strong tradition in dependency parsing, which has generally favored pipeline systems with part-of-speech tagging as a crucial component, a tendency that has probably been reinforced by the widespread use of data sets with gold tags from the early CoNLL tasks).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8573631346225739}]}, {"text": "Even models that perform joint inference, like those of and, depend heavily on part-of-speech tags, so we were unlikely to reach top scores in the shared task without them.", "labels": [], "entities": []}, {"text": "However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories.", "labels": [], "entities": []}, {"text": "In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags ().", "labels": [], "entities": []}, {"text": "The Uppsala system is a very simple pipeline consisting of two main components.", "labels": [], "entities": [{"text": "Uppsala", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9717957377433777}]}, {"text": "The first is a model for joint sentence and word segmentation, which uses the BiRNN-CRF framework of to predict sentence and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis.", "labels": [], "entities": [{"text": "joint sentence and word segmentation", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.629958713054657}, {"text": "BiRNN-CRF", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.7676858305931091}]}, {"text": "The latter are handled by a simple dictionary lookup or by an encoder-decoder network.", "labels": [], "entities": []}, {"text": "We use a single universal model regardless of writing system, but train separate models for each language.", "labels": [], "entities": []}, {"text": "The segmentation component is described in more detail in Section 2.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9828469157218933}]}, {"text": "The second main component of our system is a greedy transition-based parser that predicts the dependency tree given the raw words of a sentence.", "labels": [], "entities": []}, {"text": "The starting point for this model is the transitionbased parser described in, which relies on a BiLSTM to learn informative features of words in context and a feed-forward network for predicting the next parsing transition.", "labels": [], "entities": []}, {"text": "The parser uses the arc-hybrid transition system () with greedy inference and a dynamic oracle for exploration during training).", "labels": [], "entities": []}, {"text": "For the shared task, the parser has been modified to use character-based representations instead of part-ofspeech tags and to use pseudo-projective parsing to capture non-projective dependencies).", "labels": [], "entities": []}, {"text": "The parsing component is further described in Section 3.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9758170247077942}]}, {"text": "Our original plans included training a single universal model on data from all languages, with cross-lingual word embeddings, but in the limited time available we could only start exploring two simple enhancements.", "labels": [], "entities": []}, {"text": "First, we constructed word embeddings based on the RSV model, using universal part-of-speech tags as contexts (Section 4).", "labels": [], "entities": []}, {"text": "Secondly, we used multilingual training data for languages with little or no training data (Section 5).", "labels": [], "entities": []}, {"text": "Our system was trained only on the training sets provided by the organizers (Nivre et al., 2017a).", "labels": [], "entities": []}, {"text": "We did not make any use of large unlabeled data sets, parallel data sets, or word embeddings derived from such data.", "labels": [], "entities": []}, {"text": "After evaluation on the official test sets (), run on the TIRA server), the Uppsala system ranked 23 of 33 systems with respect to the main evaluation metric, with a macro-average LAS F1 of 65.11.", "labels": [], "entities": [{"text": "TIRA server", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9216458797454834}, {"text": "Uppsala system", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.9684647619724274}, {"text": "LAS F1", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.8754876554012299}]}, {"text": "We obtained the 2nd highest score for sentence segmentation overall (89.03), and top scores for word segmentation on several languages (but with relatively high variance).", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7394476979970932}, {"text": "word segmentation", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7372027486562729}]}, {"text": "However, after the test phase was concluded, we discovered two bugs that had affected the results negatively.", "labels": [], "entities": []}, {"text": "For comparison, we therefore also include post-evaluation results obtained after eliminating the bugs but without changing anything else, resulting in a macro-average LAS F1 of 70.49.", "labels": [], "entities": [{"text": "LAS F1", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.8504318296909332}]}, {"text": "Because of the nature of one of the bugs, the corrected results were obtained by running our system on a local server instead of the official TIRA server (see Section 6).", "labels": [], "entities": [{"text": "TIRA server", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.7986291646957397}]}, {"text": "We discuss our results in Section 6 and refer to the shared task overview paper) fora thorough description of the task and an overview of the results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Hyper-parameters for segmentation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.9796121716499329}]}, {"text": " Table 2: Hyper-parameter values for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9758240580558777}]}, {"text": " Table 4: Results for LAS F1, sentence and word segmentation. Test = official test score; Corr = corrected  score; Diff = difference Corr \u2212 Baseline.", "labels": [], "entities": [{"text": "LAS F1", "start_pos": 22, "end_pos": 28, "type": "TASK", "confidence": 0.7056869864463806}, {"text": "sentence and word segmentation", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.6152585372328758}, {"text": "Corr = corrected  score", "start_pos": 90, "end_pos": 113, "type": "METRIC", "confidence": 0.8384090662002563}, {"text": "Diff = difference Corr", "start_pos": 115, "end_pos": 137, "type": "METRIC", "confidence": 0.8942453265190125}]}, {"text": " Table 5: LAS F1 scores comparing cross-lingual  and monolingual models.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8838965892791748}, {"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.509202241897583}]}]}