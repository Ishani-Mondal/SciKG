{"title": [{"text": "A non-projective greedy dependency parser with bidirectional LSTMs", "labels": [], "entities": []}], "abstractContent": [{"text": "The LyS-FASTPARSE team presents BIST-COVINGTON, a neural implementation of the Covington (2001) algorithm for non-projective dependency parsing.", "labels": [], "entities": [{"text": "BIST-COVINGTON", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9803709983825684}, {"text": "non-projective dependency parsing", "start_pos": 110, "end_pos": 143, "type": "TASK", "confidence": 0.6286989251772562}]}, {"text": "The bidirectional LSTM approach by Kiperwasser and Goldberg (2016) is used to train a greedy parser with a dynamic oracle to mitigate error propagation.", "labels": [], "entities": [{"text": "mitigate error propagation", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.6498496731122335}]}, {"text": "The model participated in the CoNLL 2017 UD Shared Task.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 30, "end_pos": 55, "type": "DATASET", "confidence": 0.7648322343826294}]}, {"text": "In spite of not using any ensemble methods and using the baseline segmentation and PoS tagging, the parser obtained good results on both macro-average LAS and UAS in the big treebanks category (55 languages), ranking 7th out of 33 teams.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.6404256373643875}]}, {"text": "In the all treebanks category (LAS and UAS) we ranked 16th and 12th.", "labels": [], "entities": [{"text": "UAS)", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8089323043823242}]}, {"text": "The gap between the all and big categories is mainly due to the poor performance on four parallel PUD treebanks, suggesting that some 'suf-fixed' treebanks (e.g. Spanish-AnCora) perform poorly on cross-treebank settings, which does not occur with the corresponding 'unsuffixed' treebank (e.g. Spanish).", "labels": [], "entities": []}, {"text": "By changing that, we obtain the 11th best LAS among all runs (official and unofficial).", "labels": [], "entities": [{"text": "LAS", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9971563816070557}]}, {"text": "The code is made available at https://github.com/CoNLL-UD-2017/LyS-FASTPARSE", "labels": [], "entities": [{"text": "CoNLL-UD-2017", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8699978590011597}]}], "introductionContent": [{"text": "Dependency parsing is one of the core structured prediction tasks researched by computational linguists, due to the potential advantages that obtaining the syntactic structure of a text has in many natural language processing applications, such as machine translation, sentiment analysis () or information extraction (.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8743728995323181}, {"text": "machine translation", "start_pos": 248, "end_pos": 267, "type": "TASK", "confidence": 0.8262380659580231}, {"text": "sentiment analysis", "start_pos": 269, "end_pos": 287, "type": "TASK", "confidence": 0.9425206184387207}, {"text": "information extraction", "start_pos": 294, "end_pos": 316, "type": "TASK", "confidence": 0.8516216576099396}]}, {"text": "The goal of a dependency parser is to analyze the syntactic structure of sentences in one or several human languages by obtaining their analyses in the form of dependency trees.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.8092264831066132}]}, {"text": "Let w = [w 1 , w 2 , ..., w |w| ] bean input sentence, a dependency tree for w is an edge-labeled directed tree T = (V, E) where V = {0, 1, 2, . .", "labels": [], "entities": []}, {"text": ", |w|} is the set of nodes and E = V \u00d7 D \u00d7 V is the set of labeled arcs.", "labels": [], "entities": []}, {"text": "Each arc, of the form (i, d, j), corresponds to a syntactic dependency between the words w i and w j ; where i is the index of the headword, j is the index of the child word and dis the dependency type representing the kind of syntactic relation between them.", "labels": [], "entities": []}, {"text": "We will write id \u2212 \u2192 j as shorthand for (i, d, j) \u2208 E and we will omit the dependency types when they are not relevant.", "labels": [], "entities": []}, {"text": "A dependency tree is said to be non-projective if it contains two arcs i \u2212 \u2192 j and k \u2212 \u2192 l where min(i, j) < min(k, l) < max(i, j) < max(k, l), i.e., if there is any pair of arcs that cross when they are drawn over the sentence, as shown in.", "labels": [], "entities": []}, {"text": "Unrestricted non-projective parsing allows more accurate syntactic representations than projective parsing, but it comes at a higher computational cost, as there is more flexibility in how the tree can be arranged so that more operations are usually needed to explore the much larger search space.", "labels": [], "entities": [{"text": "projective parsing", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.76731738448143}]}, {"text": "Non-projective transition-based parsing has been actively explored in the last decade).", "labels": [], "entities": []}, {"text": "The success of neural networks and word embeddings for pro-He gave a talk yesterday about parsing: A non-projective dependency tree jective dependency parsing) also encouraged research on neural nonprojective models (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.9750359654426575}, {"text": "jective dependency parsing", "start_pos": 132, "end_pos": 158, "type": "TASK", "confidence": 0.6372368633747101}]}, {"text": "However, to the best of our knowledge, no neural implementation is available of unrestricted nonprojective transition-based parsing with a dynamic oracle.", "labels": [], "entities": []}, {"text": "Here, we present such an implementation for the algorithm using bidirectional long short-term memory networks (LSTM), which is the main contribution of this paper.", "labels": [], "entities": []}, {"text": "The system is evaluated at the CoNLL 2017 UD Shared Task: end-to-end multilingual parsing using Universal Dependencies (.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.9057113289833069}, {"text": "multilingual parsing", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.5545211881399155}]}, {"text": "The goal is to obtain a Universal Dependencies v2.0 representation () of a collection of raw texts in different languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We here describe the official treebanks used in the shared task ( \u00a73.1), the general setup used to train the models ( \u00a73.2) and some exceptions to said general setup that were applied to special cases ( \u00a73.3).", "labels": [], "entities": []}, {"text": "We also discuss the experimental results obtained by our system in the shared task ( \u00a73.4).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BIST-COVINGTON results on the test  sets, for those treebanks from which a training set  was provided (small and big treebanks categories)", "labels": [], "entities": [{"text": "BIST-COVINGTON", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.997258186340332}]}, {"text": " Table 3:  BIST-COVINGTON results on the  dev set, for those treebanks that have an offi- cial dev set (all treebanks except French-ParTUT,  Irish, Galician-TreeGal, Kazakh, Slovenian-SST,  Kazakh, Uyghur and Ukrainian). indicates the  model was also trained with external word embed- dings (E).", "labels": [], "entities": [{"text": "BIST-COVINGTON", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9961404204368591}]}, {"text": " Table 4: LAS on the surprise languages sample  sets for: (1) top 3 best performing monolingual  models for which there is an official training tree- bank and (2) a multilingual model trained on the  first 2 000 sentences of each of such treebanks. For  the multilingual models, the last column shows its  performance on the test sets (subscripts indicate  our ranking in that language)", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9805983304977417}]}]}