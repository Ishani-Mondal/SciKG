{"title": [{"text": "Stanford's Graph-based Neural Dependency Parser at the CoNLL 2017 Shared Task Combining Global Models for Parsing Universal Dependencies IMS at the CoNLL 2017 UD Shared Task: CRFs and Perceptrons Meet Neural Networks The HIT-SCIR System for End-to-End Parsing of Universal Dependencies A System for Multilingual Dependency Parsing based on Bidirectional LSTM Feature Representations Parsing with Context Embeddings Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe UParse: the Edinburgh system for the CoNLL 2017 UD shared task TurkuNLP: Delexicalized Pre-training of Word Embeddings for Dependency Parsing The parse is darc and full of errors: Universal dependency parsing with transition-based and graph- based algorithms A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing A non-DNN Feature Engineering Approach to Dependency Parsing -FBAML at CoNLL 2017 Shared Task A non-projective greedy dependency parser with bidirectional LSTMs Delexicalized transfer parsing for low-resource languages using transformed and combined treebanks A Transition-based System for Universal Dependency Parsing Corpus Selection Approaches for Multilingual Parsing from Raw Text to Universal Dependencies CLCL (Geneva) DINN Parser: a Neural Network Dependency Parser Ten Years Later A Fast and Lightweight System for Multilingual Dependency Parsing A rule-based system for cross-lingual parsing of Romance languages with Universal Dependencies Conference Program", "labels": [], "entities": [{"text": "Universal dependency parsing", "start_pos": 663, "end_pos": 691, "type": "TASK", "confidence": 0.6318951745827993}, {"text": "Geneva) DINN Parser", "start_pos": 1246, "end_pos": 1265, "type": "DATASET", "confidence": 0.8113993704319}]}], "abstractContent": [], "introductionContent": [{"text": "This volume contains papers describing systems submitted to the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overview paper summarizing the task, its features, evaluation methodology for the main and additional metrics, and some interesting observations about the submitted systems and the task as a whole.", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.8319127708673477}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 88, "end_pos": 122, "type": "TASK", "confidence": 0.7507926166057587}]}, {"text": "This Shared Task (http://universaldependencies.org/conll17/) can be seen as an extension of the CoNLL 2007 Shared Task on parsing, but there are many important differences that make this year's task unique with several \"firsts\".", "labels": [], "entities": [{"text": "CoNLL 2007 Shared Task", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.8729005604982376}]}, {"text": "Most importantly, the data for this task come from the Universal Dependencies project (http://universaldependencies.org), which provides annotated treebanks fora large number of languages using the same annotation scheme for all of them.", "labels": [], "entities": []}, {"text": "In the shared task setting, this allows for more meaningful comparison between systems as well as languages, since differences are much more likely due to true parser differences rather than differences caused by annotation schemes.", "labels": [], "entities": []}, {"text": "In addition, the number of languages for which training data were available is unprecedented fora single shared task: a total of 64 treebanks in 45 languages have been provided for training the systems.", "labels": [], "entities": []}, {"text": "Additional data have been provided too, as were some baseline systems for those who wanted to try only some particular aspect of parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9655020236968994}]}, {"text": "Overall, the task can be described as \"closed\", since only pre-approved data could be used.", "labels": [], "entities": []}, {"text": "For evaluation, there were 81 datasets (standard datasets for the treebank languages provided for training, plus more test sets in known languages, but based on a specially created and annotated parallel corpus, and four surprise language test sets).", "labels": [], "entities": []}, {"text": "Participants had to process all the test sets.", "labels": [], "entities": []}, {"text": "The TIRA platform has been used for evaluation, as was the case already for the CoNLL 2015 and 2016 Shared Tasks, meaning that participants had to provide their code on a designated virtual machine to be run by the organizers to produce official results.", "labels": [], "entities": [{"text": "CoNLL 2015 and 2016 Shared Tasks", "start_pos": 80, "end_pos": 112, "type": "DATASET", "confidence": 0.7643011212348938}]}, {"text": "However, test data have been published after the official evaluation period, and participants could run their systems at home to produce additional results they were allowed to include in the system description papers.", "labels": [], "entities": []}, {"text": "There was one main evaluation metric -Labeled Attachment Score -for the main ranking table evaluating dependency parsing performance, plus additional metrics for tokenization, word and sentence segmentation, POS tagging, lemmatization and disambiguation of morphological features, and separate metrics computed for interesting subsets of the evaluation data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7944532334804535}, {"text": "word and sentence segmentation", "start_pos": 176, "end_pos": 206, "type": "TASK", "confidence": 0.5779074430465698}, {"text": "POS tagging", "start_pos": 208, "end_pos": 219, "type": "TASK", "confidence": 0.8540151715278625}]}, {"text": "A total of 32 systems ran successfully and have been ranked (http://universaldependencies. org/conll17/results.html).", "labels": [], "entities": []}, {"text": "While there are clear overall winners, we would like to thank all participants for working hard on their submissions and adapting their systems not only to the datasets available, but also to the evaluation platform.", "labels": [], "entities": []}, {"text": "We would like to thank all of them for their effort, since it is the participants who are the core of any shared task's success.", "labels": [], "entities": []}, {"text": "We would like to thank the CoNLL organizers for their support and the reviewers for helping to improve the submitted system papers.", "labels": [], "entities": [{"text": "CoNLL organizers", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.8513330221176147}]}, {"text": "Special thanks go to Martin Potthast of the TIRA platform for handling such a large number of systems, running often for several hours each, and for being very responsive and helpful to us and all system participants, round the clock during the evaluation week and beyond.", "labels": [], "entities": [{"text": "TIRA platform", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8381859958171844}]}, {"text": "We also thank to the 200+ people working on the Universal Dependencies project during the past three years, without whom there would be no data.", "labels": [], "entities": [{"text": "Universal Dependencies project", "start_pos": 48, "end_pos": 78, "type": "DATASET", "confidence": 0.7850121259689331}]}], "datasetContent": [], "tableCaptions": []}