{"title": [{"text": "Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies", "labels": [], "entities": [{"text": "Multilingual Parsing from Raw Text", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.8294702887535095}]}], "abstractContent": [{"text": "This paper describes LIMSI's submission to the CoNLL 2017 UD Shared Task, which is focused on small treebanks, and how to improve low-resourced parsing only by ad hoc combination of multiple views and resources.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 47, "end_pos": 72, "type": "DATASET", "confidence": 0.864129900932312}]}, {"text": "We present our approach for low-resourced parsing, together with a detailed analysis of the results for each test treebank.", "labels": [], "entities": []}, {"text": "We also report extensive analysis experiments on model selection for the PUD treebanks, and on annotation consistency among UD treebanks.", "labels": [], "entities": [{"text": "PUD treebanks", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.9224092066287994}, {"text": "UD treebanks", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.8105006217956543}]}], "introductionContent": [{"text": "This paper describes LIMSI's submission to the, dedicated to parsing Universal Dependencies () on a wide array of languages.", "labels": [], "entities": [{"text": "parsing Universal Dependencies", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.8422894279162089}]}, {"text": "Our team's work is focused on small treebanks, under 1,000 training sentences.", "labels": [], "entities": []}, {"text": "To improve low-resourced parsing, we propose to leverage base parsers, either monolingual or cross-lingual, by combining them with a cascading method: each parser in turn annotates some of the tokens, and has access to previous predictions on other tokens to help current prediction; in the end each token is annotated by exactly one parser.", "labels": [], "entities": []}, {"text": "Compared to the official baseline, this combination method yields significant improvements on several small treebanks, as well as a few larger ones.", "labels": [], "entities": []}, {"text": "Overall, according to the official results, our system achieves 67.72 LAS and is ranked 17th out of 33 participants, while the baseline (UDPipe 1.1) achieves 68.35 LAS and is ranked 13th.", "labels": [], "entities": [{"text": "LAS", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9874221086502075}, {"text": "UDPipe 1.1", "start_pos": 137, "end_pos": 147, "type": "DATASET", "confidence": 0.6986079216003418}, {"text": "LAS", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9726868271827698}]}, {"text": "This is mostly due to huge drops from the baseline on a few languages, for which we submitted one of the official baseline models.", "labels": [], "entities": []}, {"text": "Analyzing these drops (see \u00a74.5) unveils that strong annotation divergences remain among UD treebanks of the same language.", "labels": [], "entities": []}, {"text": "If for these treebanks we had submitted the exact same models as the baseline submission, our system would have been ranked 9th, achieving 68.90 LAS.", "labels": [], "entities": [{"text": "LAS", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.9545843005180359}]}, {"text": "In the unofficial, postevaluation ranking, it is ranked 12th.", "labels": [], "entities": []}, {"text": "In \u00a72, we present the design of our system, the base parsers we use and how we combine them.", "labels": [], "entities": []}, {"text": "Official results are reported in \u00a73; the strategy adopted for each group of treebanks is presented in \u00a74, along with per-treebank detailed analyses.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Overall results of the shared task, as  published by the organizers. '*' denotes the best  scores among the three systems. For each group  of treebanks, the number of treebanks it contains  is indicated in parentheses. The last column corre- sponds to the unofficial ranking, which also takes  into account later improvements achieved by other  teams. The missing ranks are unknown.", "labels": [], "entities": []}, {"text": " Table 2: Tokenization and LAS results on the tree- banks with custom tokenization.", "labels": [], "entities": [{"text": "LAS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.8988848328590393}]}, {"text": " Table 3: LAS results on the large treebanks. The models selected on development data are underlined.  For 3 languages, we selected other models: UDPipe+PanParser for la proiel, a monolingual Cascade  (using UDPipe and PanParser) for hi and it.", "labels": [], "entities": [{"text": "PanParser", "start_pos": 219, "end_pos": 228, "type": "DATASET", "confidence": 0.9411607384681702}]}, {"text": " Table 4: LAS results on the medium treebanks. The models selected on development data are underlined.  For pt, ro, sl, id, ur, sl sst and en partut, we rather selected UDPipe+PanParser.", "labels": [], "entities": [{"text": "PanParser", "start_pos": 176, "end_pos": 185, "type": "DATASET", "confidence": 0.7746874094009399}]}, {"text": " Table 5: LAS results on the small treebanks. For the last 4 columns (surprise languages), 'gold seg.'  results use gold segmentation and gold tagging. '*' denotes parsers whose monolingual training data  is smaller than the data used by the UDPipe baseline, hence important score differences. The models  selected on development data are underlined. The Multi-source line reports the scores of two models:  'Generic multi-source delex' for sme, and 'Multi-source delex' for hsb, with the heuristic retaining  only treebanks over 2,000 sentences.", "labels": [], "entities": [{"text": "UDPipe baseline", "start_pos": 242, "end_pos": 257, "type": "DATASET", "confidence": 0.8981709480285645}]}, {"text": " Table 6: LAS results on the PUD treebanks. For lang pud, 'main' denotes the lang treebank.  Variants 1 are cs cac, en lines, fr sequoia, es ancora, fi ftb, it partut, pt br,  ru syntagrus and sv lines. Variants 2 are cs cltt, en partut and fr partut.  For each language, the largest treebank is annotated with ' + ' (considering numbers of tokens: fi ftb  contains more sentences than fi, but they are shorter), and '  *  ' indicates treebanks with important domain  adaptation issues (i.e. that contain neither Wikipedia nor news data).", "labels": [], "entities": [{"text": "PUD treebanks", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.8789286315441132}]}]}