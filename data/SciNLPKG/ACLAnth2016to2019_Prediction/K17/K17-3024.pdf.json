{"title": [{"text": "CLCL (Geneva) DINN Parser: a Neural Network Dependency Parser Ten Years Later", "labels": [], "entities": [{"text": "CLCL (Geneva) DINN Parser", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8092575669288635}]}], "abstractContent": [{"text": "This paper describes the University of Geneva's submission to the CoNLL 2017 shared task Multilingual Parsing from Raw Text to Universal Dependencies (listed as the CLCL (Geneva) entry).", "labels": [], "entities": [{"text": "CoNLL 2017 shared task Multilingual Parsing from Raw Text to Universal Dependencies", "start_pos": 66, "end_pos": 149, "type": "TASK", "confidence": 0.8030194640159607}, {"text": "CLCL (Geneva) entry", "start_pos": 165, "end_pos": 184, "type": "DATASET", "confidence": 0.8353178381919861}]}, {"text": "Our submitted parsing system is the grandchild of the first transition-based neural network dependency parser, which was the University of Geneva's entry in the CoNLL 2007 multilingual dependency parsing shared task, with some improvements to speed and portability.", "labels": [], "entities": [{"text": "transition-based neural network dependency parser", "start_pos": 60, "end_pos": 109, "type": "TASK", "confidence": 0.6182509958744049}, {"text": "CoNLL 2007 multilingual dependency parsing shared task", "start_pos": 161, "end_pos": 215, "type": "TASK", "confidence": 0.7439816083226886}]}, {"text": "These results provide a baseline for investigating how far we have come in the past ten years of work on neu-ral network dependency parsing.", "labels": [], "entities": [{"text": "neu-ral network dependency parsing", "start_pos": 105, "end_pos": 139, "type": "TASK", "confidence": 0.5760434046387672}]}], "introductionContent": [{"text": "The system described in this paper is the grandchild of the first transition-based neural network dependency parser, which was the University of Geneva's entry in the CoNLL 2007 multilingual dependency parsing shared task).", "labels": [], "entities": [{"text": "transition-based neural network dependency parser", "start_pos": 66, "end_pos": 115, "type": "TASK", "confidence": 0.637574988603592}, {"text": "CoNLL 2007 multilingual dependency parsing shared task", "start_pos": 167, "end_pos": 221, "type": "TASK", "confidence": 0.7906097599438259}]}, {"text": "The system has undergone some developments and modifications, in particular the faster discriminative version introduced by, but in many respects the design and implementation of this parser is unchanged since 2007.", "labels": [], "entities": []}, {"text": "One of our motivations for our submission to this CoNLL 2017 multilingual dependency parsing shared task is to provide a baseline to evaluate to what extent recent advances in neural network models and training do in fact improve performance over \"traditional\" recurrent neural networks.", "labels": [], "entities": [{"text": "CoNLL 2017 multilingual dependency parsing shared task", "start_pos": 50, "end_pos": 104, "type": "TASK", "confidence": 0.8197961364473615}]}, {"text": "We are listed in the table of results as the CLCL (Geneva) entry.", "labels": [], "entities": [{"text": "CLCL (Geneva)", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.8521479964256287}]}, {"text": "As with previous work using the Incremental Neural Network architecture (e.g., the main philosophy of our submission is that we build language universal inductive biases into the model structure of the recurrent neural network, but we do not do any feature engineering.", "labels": [], "entities": []}, {"text": "Training the neural network induces language-specific hidden representations automatically.", "labels": [], "entities": []}, {"text": "To provide such a baseline, we use UDPipe for all pre-processing (, and Malt Parser for all projectivisation).", "labels": [], "entities": []}, {"text": "The only exception is our strategy for surprise languages, discussed below.", "labels": [], "entities": []}, {"text": "These goals match well the aim of the 2017 Universal dependencies shared task, described in the introductory overview.", "labels": [], "entities": []}, {"text": "This task makes true cross-linguistic comparison possible thanks to the universal dependency annotation project, which underlies the data used in this shared task.", "labels": [], "entities": [{"text": "cross-linguistic comparison", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6743729114532471}]}, {"text": "We train exactly the same parsing model on every language, thereby allowing further comparisons.", "labels": [], "entities": []}, {"text": "In addition, the feature induction abilities of the recurrent neural network help minimise any remaining cross-lingual differences due to pre-processing or annotation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The implementation of DINN uses a parameter file to define the hidden-hidden connections, the input-hidden features, training meta-parameters, and various other parameters of the parser.", "labels": [], "entities": []}, {"text": "We use the same settings for all languages.", "labels": [], "entities": []}, {"text": "For the official submission, we used the following settings.", "labels": [], "entities": []}, {"text": "-We used a frequency cutoff for words/lemmas of 3.", "labels": [], "entities": []}, {"text": "-We did not normalise the input string to lowercase.", "labels": [], "entities": []}, {"text": "-We always used all the available training and development set.", "labels": [], "entities": []}, {"text": "-Search beam size is 10.", "labels": [], "entities": [{"text": "Search beam size", "start_pos": 1, "end_pos": 17, "type": "METRIC", "confidence": 0.9611995220184326}]}, {"text": "-Hidden layer size is 80.", "labels": [], "entities": [{"text": "Hidden layer size", "start_pos": 1, "end_pos": 18, "type": "METRIC", "confidence": 0.783421536286672}]}, {"text": "-The size of the internally calculated embeddings is 50.", "labels": [], "entities": []}, {"text": "-Word embeddings are initialised randomly.", "labels": [], "entities": []}, {"text": "-We do not apply any feature caching.", "labels": [], "entities": []}, {"text": "-Validation occurs at every iteration.", "labels": [], "entities": []}, {"text": "-The configurations of the Input-to-Hidden layer connections are as follows: + Look at 4 last elements in the stack and 4 next elements from the input (Except the treebanks fr, ko, it partut, grc proiel, cu, where we look at 5 last elements from the stack.)", "labels": [], "entities": []}, {"text": "+ For each element, use all possible features from UDPipe (except UPoS if XPoS exists).", "labels": [], "entities": []}, {"text": "-The configurations of the Hidden-to-Hidden layer connections are as follows: In this specification of the hidden-to-hidden connections, Queue refers to the front of the input queue and Top refers to the top of the stack in the parser configuration.", "labels": [], "entities": []}, {"text": "This specification uses the same simplified set of connections between hidden states used in.", "labels": [], "entities": []}, {"text": "We assume that the induced hidden features primarily relate to the word on the top of the syntactic stack and the word at the front of the queue, since these are the words used in any action.", "labels": [], "entities": []}, {"text": "To decide which previous state's hidden features are most relevant to the current decision, we look at these words in the current parser configuration.", "labels": [], "entities": []}, {"text": "For each such word, we look for previous states where the top of the stack or the front of the queue was the same word.", "labels": [], "entities": []}, {"text": "If more than one previous state matches, then the hidden vector of the most recent one is used.", "labels": [], "entities": []}, {"text": "If no state matches, then no connection is made.", "labels": [], "entities": []}, {"text": "To build a model for the surprise languages, we use simple cross-lingual techniques.", "labels": [], "entities": []}, {"text": "For the official test phase, we identified the most similar languages to the surprise language with a string-based technique, concatenated the treebanks, trained and tested on the surprise languages.", "labels": [], "entities": []}, {"text": "The string-based technique constructs a list of words for each language.", "labels": [], "entities": []}, {"text": "We used the sample data for the surprise language and the training data for the languages for which we have enough resources.", "labels": [], "entities": []}, {"text": "Call these languages with big data sets B.", "labels": [], "entities": []}, {"text": "We denote T as the set of lists of words of B, and t is a word in T . For a given surprise language, we calculate the similarity score S for each t.", "labels": [], "entities": [{"text": "similarity score S", "start_pos": 118, "end_pos": 136, "type": "METRIC", "confidence": 0.9551068544387817}]}, {"text": "We treat two words as similar if and only if the first three characters of these two words are identical and the edit distance between these two words is less than or equal to 1.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 113, "end_pos": 126, "type": "METRIC", "confidence": 0.9473593533039093}]}, {"text": "We choose the language that has the best S for training our model for the surprise language.", "labels": [], "entities": []}, {"text": "This procedure yields the following similar languages for training.", "labels": [], "entities": []}, {"text": "We call them source languages.", "labels": [], "entities": []}, {"text": "To train a parser for the surprise language, we concatenate the datasets for the source languages with three copies of the dataset for the target language.", "labels": [], "entities": []}, {"text": "Because our frequency threshold is three, this means that all words in the target language dataset are included in the vocabulary.", "labels": [], "entities": []}, {"text": "Then we trained a parser on this concatenated dataset, using the surprise language corpus also as a development set.", "labels": [], "entities": []}, {"text": "In addition to the surprise languages, there are other languages whose available data is just enough fora small training set without any development set.", "labels": [], "entities": []}, {"text": "For the submitted test run, we did   not do anything special for these datasets (other than the training schedule discussed above), training parsing models on the individual datasets.", "labels": [], "entities": [{"text": "training parsing", "start_pos": 132, "end_pos": 148, "type": "TASK", "confidence": 0.6436179578304291}]}, {"text": "But in subsequent experiments we tried treating them in the same way as surprise languages, with much improved results, discussed below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: DINN and Universal Dependencies treebanks -official results.", "labels": [], "entities": [{"text": "DINN and Universal Dependencies treebanks", "start_pos": 10, "end_pos": 51, "type": "DATASET", "confidence": 0.6723995983600617}]}, {"text": " Table 2: DINN and Universal Dependencies tree- banks -official results on surprise languages.", "labels": [], "entities": [{"text": "Universal Dependencies tree", "start_pos": 19, "end_pos": 46, "type": "DATASET", "confidence": 0.6151481668154398}]}, {"text": " Table 3: DINN and Universal Dependencies tree- banks -official results on PUD Treebanks.", "labels": [], "entities": [{"text": "Universal Dependencies tree- banks", "start_pos": 19, "end_pos": 53, "type": "DATASET", "confidence": 0.6889991164207458}, {"text": "PUD Treebanks", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9594796001911163}]}, {"text": " Table 4: Training and testing runtimes. T/s: Training time per sentence; NUI: Number of useful itera- tions; UTH: Useful training time (hours); DPT: Development parsing time (seconds); W/s: Words/sec.", "labels": [], "entities": [{"text": "NUI", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.924942135810852}, {"text": "UTH", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.8369020223617554}, {"text": "Development parsing", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.6409315168857574}]}, {"text": " Table 5: Small treebanks as surprise language, Run 5.1 UD PMor (20/05).", "labels": [], "entities": [{"text": "Run 5.1 UD PMor (20/05)", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.9215724998050265}]}]}