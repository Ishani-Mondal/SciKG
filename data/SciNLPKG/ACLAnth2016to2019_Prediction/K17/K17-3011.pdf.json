{"title": [], "abstractContent": [{"text": "This paper describes the system of the team Orange-Deski\u00f1, used for the CoNLL 2017 UD Shared Task.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 72, "end_pos": 97, "type": "DATASET", "confidence": 0.8150635480880737}]}, {"text": "We based our approach on an existing open source tool (BistParser), which we modified in order to produce the required output.", "labels": [], "entities": []}, {"text": "Additionally we added a kind of pseudo-projectivisation.", "labels": [], "entities": []}, {"text": "This was needed since some of the task's languages have a high percentage of non-projective dependency trees.", "labels": [], "entities": []}, {"text": "In most cases we also employed word embeddings.", "labels": [], "entities": []}, {"text": "For the 4 surprise languages , the data provided seemed too little to train on.", "labels": [], "entities": []}, {"text": "Thus we decided to use the training data of typologically close languages instead.", "labels": [], "entities": []}, {"text": "Our system achieved a macro-averaged LAS of 68.61% (10th in the overall ranking) which improved to 69.38% after bug fixes.", "labels": [], "entities": [{"text": "LAS", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8850871324539185}]}], "introductionContent": [{"text": "For our work in our lab (Orange-Deski\u00f1) we needed a robust dependency analysis for written French with the highest Labeled Attachment Score (LAS) 1 possible, using a wide range of dependency relations.", "labels": [], "entities": [{"text": "Labeled Attachment Score (LAS) 1", "start_pos": 115, "end_pos": 147, "type": "METRIC", "confidence": 0.8683955754552569}]}, {"text": "Having worked in the past on rule based dependency analysis, it became obvious that we need to adopt a more modern approach to dependency analysis.", "labels": [], "entities": [{"text": "dependency analysis", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7409972697496414}, {"text": "dependency analysis", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.8854545652866364}]}, {"text": "Thus during the last year we tried several freely available open source tools available (e.g. MaltParser 2 , Google's SyntaxNet 3 , Standford Dependency Tools 4 , Bist-Parser and HTParser 6 ), trained on different Treebanks (notably French Sequoia () and Universal Dependencies ().", "labels": [], "entities": [{"text": "Bist-Parser", "start_pos": 163, "end_pos": 174, "type": "METRIC", "confidence": 0.9230315685272217}, {"text": "French Sequoia", "start_pos": 233, "end_pos": 247, "type": "DATASET", "confidence": 0.9572402536869049}]}, {"text": "All combinations of tools and treebanks had some advantages and some inconveniences.", "labels": [], "entities": []}, {"text": "For instance, the underlying linguistic models of the treebanks are not the same or some tools would not accept CONLLU input but only raw text and apply their own segmentation and POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 180, "end_pos": 191, "type": "TASK", "confidence": 0.6691354513168335}]}, {"text": "Ina next step we enriched the French treebanks with additional information like lemmas, morphological features and more fine-graded XPOS in addition to the about 20 UPOS categories of the treebanks (UD-French v1.2 does not contain neither lemmas nor morphological features) and conducted anew training/test/evaluation cycle.", "labels": [], "entities": [{"text": "French treebanks", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.9679619073867798}, {"text": "XPOS", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9291334748268127}]}, {"text": "Since the initial results for French were encouraging we tried the same approaches with other languages, such as the languages proposed for.", "labels": [], "entities": []}, {"text": "However, for participation at the shared task, we relied exclusively on the data provided by Universal Dependencies (, also for French in spite of our previous work.", "labels": [], "entities": [{"text": "Universal Dependencies", "start_pos": 93, "end_pos": 115, "type": "DATASET", "confidence": 0.9201703667640686}]}, {"text": "For the shared task we have trained models separately for each language.", "labels": [], "entities": []}, {"text": "So strictly speaking, this is not a multilingual but a monolingual multimodel approach.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Development results (without surprise languages)", "labels": [], "entities": []}, {"text": " Table 2: Statistics on the data of the surprise lan- guages", "labels": [], "entities": []}, {"text": " Table 3: Weighted LAS of the surprise languages  using a model trained on 23 languages", "labels": [], "entities": [{"text": "LAS", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7723468542098999}]}, {"text": " Table 6: comparison of training results (on development corpora) and final results. Due to an error, the  models for the treebanks marked (*) were unfortunately not used for the tests.", "labels": [], "entities": []}]}