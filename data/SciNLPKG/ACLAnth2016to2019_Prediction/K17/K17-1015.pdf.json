{"title": [{"text": "An Artificial Language Evaluation of Distributional Semantic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent studies of distributional semantic models have setup a competition between word embeddings obtained from predictive neural networks and word vectors obtained from count-based models.", "labels": [], "entities": []}, {"text": "This paper is an attempt to reveal the underlying contribution of additional training data and post-processing stepson each type of model in word similarity and relatedness inference tasks.", "labels": [], "entities": [{"text": "word similarity and relatedness inference tasks", "start_pos": 141, "end_pos": 188, "type": "TASK", "confidence": 0.7004972795645396}]}, {"text": "We do so by designing an artificial language, training a predictive and a count-based model on data sampled from this grammar, and evaluating the resulting word vectors in paradigmatic and syntagmatic tasks defined with respect to the grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "The distributional tradition in linguistics (e.g., classically posits that a word's meaning can be estimated by its pattern of cooccurrence with other words.", "labels": [], "entities": []}, {"text": "Modern distributional semantic models (DSMs) formalize this process to construct vector representations for word meaning from statistical regularities in large-scale corpora.", "labels": [], "entities": []}, {"text": "A typical approach in NLP has been to apply dimensional reduction algorithms borrowed from linear algebra to a word-by-context frequency matrix representation of a text corpus.", "labels": [], "entities": []}, {"text": "Words that frequently appear in similar contexts will have similar patterns across resulting latent components, even if they never directly co-occur (for reviews, see Jones,).", "labels": [], "entities": []}, {"text": "These models dominated the literature over direct count methods for over two decades.", "labels": [], "entities": []}, {"text": "Recently, DSMs based on neural networks have rapidly grown in popularity (e.g.,.", "labels": [], "entities": []}, {"text": "Given a word, the model attempts to predict the context words that it occurs with, or vice-versa.", "labels": [], "entities": []}, {"text": "After training on a text corpus, the pattern of elements across the model's hidden layer come to reflect semantic similarities, i.e., will be similar for words that predict similar contexts even if those words do not predict each other.", "labels": [], "entities": []}, {"text": "In this sense, neural embedding models come to a distributed vector representation of word meaning that is reminiscent of traditional dimensional reduction DSMs, albeit with a considerably different learning algorithm.) have demonstrated state-of-the-art performance using a neural embedding model with an efficient objective function called word2vec.", "labels": [], "entities": [{"text": "dimensional reduction DSMs", "start_pos": 134, "end_pos": 160, "type": "TASK", "confidence": 0.7278442978858948}]}, {"text": "This model rapidly emerged as the leader of the DSM pack, outperforming other models on abroad range of lexical semantic tasks ().", "labels": [], "entities": []}, {"text": "However, since the early surge in excitement for word2vec, the literature has now become more focused on trying to understand the conditions under which embedding or traditional DSMs are optimal.", "labels": [], "entities": []}, {"text": "demonstrated analytically that word2vec is implicitly factorizing a word-by-context matrix whose cell values are shifted PMI values.", "labels": [], "entities": []}, {"text": "In other words, the objective function and the input to word2vec are formally equivalent to traditional DSMs; thus the models should behave alike in the limit.", "labels": [], "entities": []}, {"text": "The distinction is really one of process and parameterization.", "labels": [], "entities": []}, {"text": "With optimum parameterization of traditional DSMs, more recent research is finding insignificant performance differences between word2vec and SVD factorizations of a PMI matrix.", "labels": [], "entities": []}, {"text": "even found a slight advantage fora factorization of the bias shifted log-count matrix and for traditional PPMI over word2vec on some tasks when hyperparameters were optimized.", "labels": [], "entities": []}, {"text": "One general distinction between the two types of models is that neural embedding models such as word2vec seem to underperform when the training corpus is small, particularly for lowfrequency words.", "labels": [], "entities": []}, {"text": "note that there is often a benefit in word2vec of tuning a larger parameter space over using a larger training corpus.", "labels": [], "entities": []}, {"text": "With limited-data mining scenarios becoming more common, a better understanding of how model type and corpus size interact with optimal parameterization is an important topic of inquiry.", "labels": [], "entities": []}, {"text": "Secondly, interest has shifted from trying to determine the best overall model towards a better understanding of what kinds of word relations each model is best at learning, and under what parameterizations.", "labels": [], "entities": []}, {"text": "Count-based PMI models are very good at representing first-order statistical patterns that reflect syntagmatic relationships in language (aka \"relatedness\" data).", "labels": [], "entities": []}, {"text": "In contrast, the training scheme used by word2vec attempts to optimize it for detecting second-order statistical patterns that reflect paradigmatic relationships in language (aka \"similarity\" data).", "labels": [], "entities": []}, {"text": "Indeed, this was the pattern demonstrated by: After tuning hyperparameters, word2vec performed best on similarity-based tasks while PPMI performed best on relatedness tasks.", "labels": [], "entities": []}, {"text": "SVD-based models attempt to represent both statistical patterns.", "labels": [], "entities": []}, {"text": "This count-based model outperformed both word2vec and PPMI in Levy et al. on both types of relations when standard parameter sets were used; however, the advantage disappeared when hyperparameters were tuned.", "labels": [], "entities": []}, {"text": "Standard word2vec is optimized for paradigmatic tasks but architectural adaptations exist to make the model better suited for syntagmatic tasks (e.g.,).", "labels": [], "entities": []}, {"text": "Making a model better atone type of task might come at the cost of making it worse at the other if the two types of word relations are orthogonal.", "labels": [], "entities": []}, {"text": "Optimizing fora particular task is also closely tied to the issue of training data size.", "labels": [], "entities": []}, {"text": "Finally, both of these issues are intricately tied to post-processing of the embeddings.", "labels": [], "entities": []}, {"text": "inspired by pointed out an important parametrization of the word2vec model, where co-occurrence information encoded between hidden and output layers (context vectors) are used as well as weighs between the input and hidden layers (word vectors) to construct the final word embeddings (w+c representation).", "labels": [], "entities": []}, {"text": "When calculating word similarity based on this composite representation, a mixture between first-and second-order coocurrence information are considered.", "labels": [], "entities": []}, {"text": "This is remarkably similar to cognitive models that construct composite memory representations from both paradigmatic and syntagmatic information.", "labels": [], "entities": []}, {"text": "Recent empirical studies in developmental psychology have found that children learn word relations that have both sources of information before relations with either source alone (.", "labels": [], "entities": []}, {"text": "found a consistent benefit for word2vec and PPMI when the w+c post-processing combination was applied.", "labels": [], "entities": []}, {"text": "Even though, this is an efficient adaptation in that the scheme does not require retraining, most studies on word similarity and relatedness have only employed the default word2vec setting (i.e., only using word vectors) and the usefulness of context vectors has been left underexplored.", "labels": [], "entities": []}, {"text": "It is very plausible to assume that the above three issues (corpus size, relation type, postprocessing) interact: Higher-order paradigmatic word relations likely require more training data to discover, and the merging of w+c blends different relation types.", "labels": [], "entities": []}, {"text": "The goal of this paper is to elaborate on the effect of corpus size and postprocessing on the reflection of syntagmatic and paradigmatic relations between words within the resulting vector space.", "labels": [], "entities": []}, {"text": "It has proven impossible in psycholinguistics to select real words that cleanly separate paradigmatic and syntagmatic relations).", "labels": [], "entities": []}, {"text": "Hence, we opted to bring the statistical structure of the language under experimental control using an artificial language adapted from.", "labels": [], "entities": []}, {"text": "Unlike in natural language corpora, the sources are independent: e.g., dog never directly appears with cat, and hence any learned relation between them could not be due to first-order information.", "labels": [], "entities": []}, {"text": "Thus by defining crisp semantic categories and sentence frames, we investigate how first and second-order co-occurrence information sources are consumed and represented in terms of similarity between words by count-based and predictive DSMs.", "labels": [], "entities": []}, {"text": "Given current uncertainty in the literature on the role of corpus size, relation type, and w+c post-processing regarding the performance of various DSM architectures, this approach affords experimental control to evaluate relative performance as a factorial combination of information sources and parameters while controlling for the many confounding factors that exist in natural language corpora; including the ambiguity of similarity vs. relatedness of two words in evaluation datasets.", "labels": [], "entities": []}, {"text": "Section 2 describes our framework in details, and section 3 presents several experiments exploring the capacity of count vs. predict DSMs in modeling relations between words.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Vanilla setup accuracy in paradigmatic and  syntagmatic tasks with different size training corpuses.", "labels": [], "entities": [{"text": "Vanilla setup", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7950251400470734}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9467114806175232}]}]}