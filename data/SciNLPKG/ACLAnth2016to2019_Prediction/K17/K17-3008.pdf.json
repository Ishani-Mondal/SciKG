{"title": [{"text": "Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies", "labels": [], "entities": [{"text": "Multilingual Parsing from Raw Text", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.8294702887535095}]}], "abstractContent": [{"text": "We introduce context embeddings, dense vectors derived from a language model that represent the left/right context of a word instance, and demonstrate that context embeddings significantly improve the accuracy of our transition based parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9985257983207703}]}, {"text": "Our model consists of a bidirectional LSTM (BiLSTM) based language model that is pre-trained to predict words in plain text, and a multi-layer perceptron (MLP) decision model that uses features from the language model to predict the correct actions for an ArcHybrid transition based parser.", "labels": [], "entities": []}, {"text": "We participated in the CoNLL 2017 UD Shared Task as the \"Ko\u00e7 Univer-sity\" team and our system was ranked 7th out of 33 systems that parsed 81 treebanks in 49 languages.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 23, "end_pos": 48, "type": "DATASET", "confidence": 0.7714627981185913}]}], "introductionContent": [{"text": "Recent studies in parsing natural language has seen a shift from shallow models that use high dimensional, sparse, hand engineered features, e.g. (, to deeper models with dense feature vectors, e.g..", "labels": [], "entities": [{"text": "parsing natural language", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.915217657883962}]}, {"text": "Shallow linear models cannot represent feature conjunctions that maybe useful for parsing decisions, therefore designers of such models have to add specific combinations to the feature list by hand: for example Zhang and Nivre (2011) define 72 hand designed conjunctive combinations of 39 primitive features.", "labels": [], "entities": [{"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.9721254110336304}]}, {"text": "Deep models can represent and automatically learn feature combinations that are useful fora given task, so the designer only has to come up with a list of primitive features.", "labels": [], "entities": []}, {"text": "Two questions about feature representation still remain critical: what parts of the parser state to represent, and how to represent these (typically discrete) features with continuous embedding vectors.", "labels": [], "entities": [{"text": "feature representation", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.743245005607605}]}, {"text": "In this work we derive features for the parser from a bidirectional LSTM language model trained with pre-tokenized text to predict words in a sentence using both the left and the right context.", "labels": [], "entities": []}, {"text": "In particular we derive word embeddings and context embeddings from the language model.", "labels": [], "entities": []}, {"text": "Word embeddings represent the general features of a word type averaged overall its occurrences.", "labels": [], "entities": []}, {"text": "Taking advantage of word embeddings derived from language models in other applications is common practice, however, using the same embedding for every occurrence of an ambiguous word ignores polysemy and meaning shifts.", "labels": [], "entities": []}, {"text": "To mitigate this problem, we also construct and use context embeddings that represent the immediate context of a word instance.", "labels": [], "entities": []}, {"text": "Context embeddings were previously shown to improve tasks such as part-ofspeech induction () and word sense induction.", "labels": [], "entities": [{"text": "part-ofspeech induction", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7737155556678772}, {"text": "word sense induction", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.7803546587626139}]}, {"text": "In this study, we derive context embeddings from the hidden states of the forward and backward LSTMs of the language model that are generated while predicting a word.", "labels": [], "entities": []}, {"text": "These hidden states summarize the information from the left context and the right context of a word that was useful in predicting it.", "labels": [], "entities": []}, {"text": "Our main contribution is to demonstrate that using context embeddings as features leads to a significant improvement in parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 120, "end_pos": 127, "type": "TASK", "confidence": 0.9682397246360779}]}, {"text": "The rest of the paper is organized as follows: Section 2 introduces basic components of a transition based neural network parser and describes related work based on their design choices.", "labels": [], "entities": [{"text": "transition based neural network parser", "start_pos": 90, "end_pos": 128, "type": "TASK", "confidence": 0.6859827280044556}]}, {"text": "Section 3 describes the details of our model and training method.", "labels": [], "entities": []}, {"text": "Section 4 discusses our results and Section 5 summarizes our contributions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Parent models used for parsing surprise  languages and LAS scores obtained after pre-train  and finetuning.", "labels": [], "entities": [{"text": "parsing surprise  languages", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.8783632516860962}, {"text": "LAS scores", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.863568902015686}]}, {"text": " Table 3: Our official results in CoNLL 2017 UD Shared Task", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task", "start_pos": 34, "end_pos": 59, "type": "DATASET", "confidence": 0.8225708961486816}]}, {"text": " Table 4: Feature comparison results on three  languages. p=postag, v=word-vector, c=context- vector, fb=Facebook-vector.", "labels": [], "entities": []}]}