{"title": [{"text": "If you can't beat them, join them: the University of Alberta system description", "labels": [], "entities": [{"text": "University of Alberta system", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.7278119474649429}]}], "abstractContent": [{"text": "We describe our approach and experiments in the context of the CoNLL-SIGMORPHON 2017 Shared Task on Universal Morphological Reinflection.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2017 Shared Task on Universal Morphological Reinflection", "start_pos": 63, "end_pos": 136, "type": "TASK", "confidence": 0.6981889829039574}]}, {"text": "We combine a discriminative transduction system with neural models.", "labels": [], "entities": []}, {"text": "The results on five languages show that our approach works well in the low-resource setting.", "labels": [], "entities": []}, {"text": "We also investigate adaptations designed to handle small training sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we describe our system as participants in the CoNLL-SIGMORPHON 2017 Shared Task on Universal Morphological Reinflection (.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON 2017 Shared Task on Universal Morphological Reinflection", "start_pos": 61, "end_pos": 134, "type": "TASK", "confidence": 0.678598165512085}]}, {"text": "Our focus is on the sub-task of inflection generation under the lowresource scenario, in which the training data is limited to 100 labeled examples, with and without monolingual corpora.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8172665536403656}]}, {"text": "Our principal approach follows, performing discriminative string transduction with a modified version of the DIRECTL+ program (.", "labels": [], "entities": []}, {"text": "Taking into account the results of the SIG-MORPHON 2016 Shared Task on Morphological Reinflection (, we investigate ways to combine the strengths of DIRECTL+ with those of neural models.", "labels": [], "entities": []}, {"text": "In addition, we experiment with various adaptations designed to handle small training sets, such as splitting and reordering morphological tags, and synthetic training data.", "labels": [], "entities": [{"text": "splitting and reordering morphological tags", "start_pos": 100, "end_pos": 143, "type": "TASK", "confidence": 0.7347554683685302}]}, {"text": "We derive inflection models for five languages: English, German, Persian, Polish, and Spanish.", "labels": [], "entities": []}, {"text": "These languages display varying degrees of inflectional complexity, but are mostly suffixing, fusional languages.", "labels": [], "entities": []}, {"text": "We combine three systems for each language: a discriminative transduction system, an ensemble of neural encoder-decoder models, and the affix-matching baseline provided by the task organizers.", "labels": [], "entities": []}, {"text": "We test two methods of system combination: linear combination and an SVM reranker.", "labels": [], "entities": []}, {"text": "The results demonstrate that our transduction approach is strongly competitive in the low-resource setting.", "labels": [], "entities": []}, {"text": "Further gains can be obtained via tag reordering heuristics and system combination.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on five languages: English (EN), German (DE), Persian (FA), Polish (PL), and Spanish (ES).", "labels": [], "entities": []}, {"text": "The training data in the low-resource setting of the inflection generation task is limited to 100 instances.", "labels": [], "entities": [{"text": "inflection generation task", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.8098001480102539}]}, {"text": "The DIRECTL+ models are trained on the subtag sequences made consistent with the method described in Section 2.3.", "labels": [], "entities": []}, {"text": "For two languages, we identified best subtag orderings that are different from the initial orderings; the Spanish ordering was found with the alignment-based method.", "labels": [], "entities": []}, {"text": "while the Persian ordering was hand-crafted by a native speaker using linguistic analysis.", "labels": [], "entities": []}, {"text": "Our other systems take advantage of the first one million lines of the Wikipedia dumps from 2017/03/01 provided by the task organizers.", "labels": [], "entities": [{"text": "Wikipedia dumps from 2017/03/01", "start_pos": 71, "end_pos": 102, "type": "DATASET", "confidence": 0.9432892724871635}]}, {"text": "Our RNN models are trained on the original training set augmented with 16,000 synthetic instances generated by the DIRECTL+ morphological analyzers, as described in Section 2.5.", "labels": [], "entities": []}, {"text": "For the language models that inform our SVM reranker, we use the entire Persian corpus, training data only for English and Polish, and the affix-match method for German and Spanish (Section 2.6).", "labels": [], "entities": [{"text": "Persian corpus", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.7494522929191589}]}, {"text": "The reranker is trained using 2-fold cross-validation on the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the development sets.", "labels": [], "entities": []}, {"text": " Table 2.  The numerical tags of the submitted runs are  shown in the top row. In the cases of incorrect  files being mistakenly submitted, we provide the  actual results, which may differ from the official  ones. With the exception of Persian, our results  are among the best in the low-resource setting.", "labels": [], "entities": [{"text": "Persian", "start_pos": 236, "end_pos": 243, "type": "DATASET", "confidence": 0.873415470123291}]}, {"text": " Table 2: Results on the test sets. Runs corrected  after the submission deadline are in italics.", "labels": [], "entities": []}]}