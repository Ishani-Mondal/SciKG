{"title": [{"text": "Combining Global Models for Parsing Universal Dependencies", "labels": [], "entities": [{"text": "Parsing Universal Dependencies", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8213672041893005}]}], "abstractContent": [{"text": "We describe our entry, C2L2, to the CoNLL 2017 shared task on parsing Universal Dependencies from raw text.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.8607456833124161}, {"text": "parsing Universal Dependencies from raw text", "start_pos": 62, "end_pos": 106, "type": "TASK", "confidence": 0.7357621093591055}]}, {"text": "Our system features an ensemble of three global parsing paradigms, one graph-based and two transition-based.", "labels": [], "entities": []}, {"text": "Each model leverages character-level bi-directional LSTMs as lexical feature ex-tractors to encode morphological information.", "labels": [], "entities": []}, {"text": "Though relying on baseline tokeniz-ers and focusing only on parsing, our system ranked second in the official end-to-end evaluation with a macro-average of 75.00 LAS F1 score over 81 test treebanks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9620394706726074}, {"text": "LAS F1 score", "start_pos": 162, "end_pos": 174, "type": "METRIC", "confidence": 0.8664703766504923}]}, {"text": "In addition, we had the top average performance on the four surprise languages and on the small treebank subset.", "labels": [], "entities": []}], "introductionContent": [{"text": "General Parsing Approach Our submitted system to the CoNLL 2017 shared task) focuses only on the task of dependency parsing, assuming that tokenization, sentence boundary detection, part-of-speech (POS) tagging and morphological features are already handled by a baseline model.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.9252762943506241}, {"text": "dependency parsing", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8209008574485779}, {"text": "sentence boundary detection", "start_pos": 153, "end_pos": 180, "type": "TASK", "confidence": 0.6737369894981384}, {"text": "part-of-speech (POS) tagging", "start_pos": 182, "end_pos": 210, "type": "TASK", "confidence": 0.6671419620513916}]}, {"text": "In this paper, we highlight our neural-network-based feature extractors and ensemble of global parsing models, including two novel global transition-based models.", "labels": [], "entities": []}, {"text": "Bi-directional long-short term memory networks (, biLSTMs) have recently achieved state-of-the-art performance on syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.6784863770008087}]}, {"text": "Our system leverages the representational power of bi-LSTMs to generate compact features for both graph-based and transition-based parsing frameworks.", "labels": [], "entities": []}, {"text": "The latter further enables the application of dynamic programming techniques) for global training and exact decoding.", "labels": [], "entities": []}, {"text": "With just two bi-LSTM vectors as features, all three global parsing paradigms in our system have efficient Opn 3 q implementations.", "labels": [], "entities": []}, {"text": "The full system consists of 3-5 each of these unlabeled parsing models (9-15 in total, depending on the treebank), and another ensemble of arc labelers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Official UAS and LAS scores on the test  sets. Rankings are based on the macro-average  LAS F1 scores over all treebanks in the set.", "labels": [], "entities": [{"text": "UAS", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8042276501655579}, {"text": "LAS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.8897069096565247}, {"text": "LAS F1 scores", "start_pos": 98, "end_pos": 111, "type": "METRIC", "confidence": 0.8656018972396851}]}, {"text": " Table 2: Evaluation results of our system on the  surprise languages. We show the source treebanks  from which we trained the delexicalized parsers.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results of our system on PUD  treebanks. We give post-evaluation (non-official)  results\u02dawhereresults\u02daresults\u02dawhere we tested with models trained on  treebanks with canonical language codes. The ta- ble is sorted by our rankings.", "labels": [], "entities": [{"text": "PUD  treebanks", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.8860329687595367}]}, {"text": " Table 4: Ablation of our ensemble system.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8067066073417664}]}]}