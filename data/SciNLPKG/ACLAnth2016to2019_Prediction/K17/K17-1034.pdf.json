{"title": [{"text": "Zero-Shot Relation Extraction via Reading Comprehension", "labels": [], "entities": [{"text": "Zero-Shot Relation Extraction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7499909102916718}]}], "abstractContent": [{"text": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9069830179214478}]}, {"text": "This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision , and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples.", "labels": [], "entities": []}, {"text": "Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.995665967464447}, {"text": "accuracy", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.9800012707710266}]}], "introductionContent": [{"text": "Relation extraction systems populate knowledge bases with facts from an unstructured text corpus.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8841192424297333}]}, {"text": "When the type of facts (relations) are predefined, one can use crowdsourcing ( or distant supervision () to collect examples and train an extraction model for each relation type.", "labels": [], "entities": []}, {"text": "However, these approaches are incapable of extracting relations that were not specified in advance and observed during training.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative approach for relation extraction, which can potentially extract facts of new types that were neither specified nor observed a priori.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.9391888082027435}]}], "datasetContent": [{"text": "To collect reading-comprehension examples as in, we first gather labeled examples for the task of relation-slot filling.", "labels": [], "entities": [{"text": "relation-slot filling", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.8205496668815613}]}, {"text": "Slot-filling examples are similar to reading-comprehension examples, but contain a knowledge-base query R(e, ?) instead of a natural-language question; e.g. spouse(Angela Merkel, ?) instead of \"Who is Angela Merkel married to?\".", "labels": [], "entities": []}, {"text": "We collect many slot-filling examples via distant supervision, and then convert their queries into natural language.", "labels": [], "entities": []}, {"text": "Slot-Filling Data We use the WikiReading dataset () to collect labeled slot-filling examples.", "labels": [], "entities": [{"text": "WikiReading dataset", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9651011526584625}]}, {"text": "WikiReading was collected by aligning each Wikidata (Vrande\u010di\u00b4Vrande\u010di\u00b4c, 2012) relation R(e, a) with the corresponding Wikipedia article D for the entity e, under the reasonable assumption that the relation can be derived from the article's text.", "labels": [], "entities": [{"text": "Wikidata (Vrande\u010di\u00b4Vrande\u010di\u00b4c, 2012) relation R", "start_pos": 43, "end_pos": 90, "type": "DATASET", "confidence": 0.7411560267210007}]}, {"text": "Each instance in this dataset contains a relation R, an entity e, a document D, and an answer a.", "labels": [], "entities": []}, {"text": "We used distant supervision to select the specific sentences in which each R(e, a) manifests.", "labels": [], "entities": []}, {"text": "Specifically, we took the first sentence sin D to contain both e and a.", "labels": [], "entities": []}, {"text": "We then grouped instances by R, e, and s to merge all the answers for R(e, ?) given s into one answer set A.", "labels": [], "entities": []}, {"text": "This can be implemented efficiently by constraining potential entities with existing facts in the knowledge base.", "labels": [], "entities": []}, {"text": "For example, any entity x that satisfies occupation(x, cryptographer) or any entity y for which subclass of (y, cipher) holds.", "labels": [], "entities": []}, {"text": "We leave the exact implementation details of such a system for future work.", "labels": [], "entities": []}, {"text": "To understand how well our method can generalize to unseen data, we design experiments for unseen entities (Section 6.1), unseen question templates (Section 6.2), and unseen relations (Section 6.3).", "labels": [], "entities": []}, {"text": "Evaluation Metrics Each instance is evaluated by comparing the tokens in the labeled answer set with those of the predicted span.", "labels": [], "entities": []}, {"text": "Precision is the true positive count divided by the number of times the system returned a non-null answer.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9953619837760925}]}, {"text": "Recall is the true positive count divided by the number of instances that have an answer.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9862923622131348}]}, {"text": "Hyperparameters In our experiments, we initialized word embeddings with GloVe), and did not fine-tune them.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9570436477661133}]}, {"text": "The typical training set was an order of 1 million examples, for which 3 epochs were enough for convergence.", "labels": [], "entities": []}, {"text": "All training sets had a ratio of 1:1 positive and negative examples, which was chosen to match the test sets' ratio.", "labels": [], "entities": []}, {"text": "Comparison Systems We experiment with several variants of our model.", "labels": [], "entities": []}, {"text": "In KB Relation, we feed our model a relation indicator (e.g. R ) instead of a question.", "labels": [], "entities": [{"text": "KB Relation", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.681109219789505}, {"text": "R", "start_pos": 61, "end_pos": 62, "type": "METRIC", "confidence": 0.8801809549331665}]}, {"text": "We expect this variant to generalize reasonably well to unseen entities, but fail on unseen relations.", "labels": [], "entities": []}, {"text": "The second variant (NL Relation) uses the relation's name (as a naturallanguage expression) instead of a question (e.g. educated at as \"educated at\").", "labels": [], "entities": []}, {"text": "We also consider a weakened version of our querification approach (Single Template) where, during training, only one question template per relation is observed.", "labels": [], "entities": []}, {"text": "The full variant of our model, Multiple Templates, is trained on a more diverse set of questions.", "labels": [], "entities": []}, {"text": "We expect this variant to have significantly better paraphrasing abilities than Single Template.", "labels": [], "entities": []}, {"text": "We also evaluate how asking about the same relation in multiple ways improves performance (Question Ensemble).", "labels": [], "entities": []}, {"text": "We create an ensemble by sampling 3 questions per test instance and predicting the answer for each.", "labels": [], "entities": []}, {"text": "We then choose the answer with the highest sum of confidence scores.", "labels": [], "entities": []}, {"text": "In addition to our model, we compare three other systems.", "labels": [], "entities": []}, {"text": "The first is a random baseline that chooses a named entity in the sentence that does not appear in the question (Random NE).", "labels": [], "entities": [{"text": "Random NE)", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.7950365940729777}]}, {"text": "We also reimplement the RNN Labeler that was shown to have good results on the extractive portion of WikiReading.", "labels": [], "entities": [{"text": "WikiReading", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.9026620388031006}]}, {"text": "Lastly, we retrain an off-the-shelf relation extraction system (, which has shown promising results on a number of benchmarks.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7867904603481293}]}, {"text": "This system (and many like it) represents relations as indicators, and cannot extract unseen relations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on unseen entities.", "labels": [], "entities": []}, {"text": " Table 2: Performance on seen/unseen questions.", "labels": [], "entities": []}, {"text": " Table 3: Performance on unseen relations.", "labels": [], "entities": []}, {"text": " Table 4: The distribution of cues by type, based on  a sample of 60.", "labels": [], "entities": []}, {"text": " Table 5: Our method's accuracy on subsets of ex- amples pertaining to different cue types. Results  in italics are based on a sample of less than 10.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9996292591094971}]}]}