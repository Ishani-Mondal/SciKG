{"title": [{"text": "Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies", "labels": [], "entities": [{"text": "Multilingual Parsing from Raw Text", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.8294702887535095}]}], "abstractContent": [{"text": "This paper describes the IIT Kharagpur dependency parsing system in CoNLL-2017 shared task on Multilingual Parsing from Raw Text to Universal Dependencies.", "labels": [], "entities": [{"text": "IIT Kharagpur dependency parsing", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.7314301282167435}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 94, "end_pos": 128, "type": "TASK", "confidence": 0.7278281867504119}]}, {"text": "We primarily focus on the low-resource languages (surprise languages).", "labels": [], "entities": []}, {"text": "We have developed a framework to combine multiple treebanks to train parsers for low resource languages by a delexical-ization method.", "labels": [], "entities": []}, {"text": "We have applied transformation on the source language tree-banks based on syntactic features of the low-resource language to improve performance of the parser.", "labels": [], "entities": []}, {"text": "In the official evaluation , our system achieves macro-averaged LAS scores of 67.61 and 37.16 on the entire blind test data and the surprise language test data respectively.", "labels": [], "entities": [{"text": "LAS", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.8499221801757812}, {"text": "surprise language test data", "start_pos": 132, "end_pos": 159, "type": "DATASET", "confidence": 0.5639921873807907}]}], "introductionContent": [{"text": "A dependency parser analyzes the relations among the words in a sentence to determine the syntactic dependencies among them where the dependency relations are drawn from a fixed set of grammatical relations.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7242689281702042}]}, {"text": "Dependency parsing is a very important NLP task and has wide usage in different tasks such as question answering, semantic parsing, information extraction and machine translation.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.844060480594635}, {"text": "question answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8781570196151733}, {"text": "semantic parsing", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7583786845207214}, {"text": "information extraction", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.8314944803714752}, {"text": "machine translation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.8007053136825562}]}, {"text": "There has been a lot of focus recently on development of dependency parsers for low-resource languages i.e., the languages for which little or no treebanks are available by cross-lingual transfer parsing methods using knowledge derived from treebanks of other languages and the resources available for the low-resource languages).", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7262114137411118}, {"text": "cross-lingual transfer parsing", "start_pos": 173, "end_pos": 203, "type": "TASK", "confidence": 0.678200364112854}]}], "datasetContent": [{"text": "Our system comprises of 88 models.", "labels": [], "entities": []}, {"text": "70 models were trained on the individual treebanks available from http://universaldependencies.", "labels": [], "entities": [{"text": "universaldependencies", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.8946990370750427}]}, {"text": "org/, 14 models were trained for the new parallel treebanks and 4 models for the surprise language.", "labels": [], "entities": []}, {"text": "Given the language code (lcode) and treebank code (tcode), our system identifies the parser model corresponding to the input test treebank and parses the sentences in the treebank file.", "labels": [], "entities": []}, {"text": "The systems were ranked based on macroaveraged LAS.", "labels": [], "entities": []}, {"text": "The final evaluation of the parser is on blind test data sets) through TIRA platform setup by.", "labels": [], "entities": []}, {"text": "We submitted 9 systems (softwarek, where k \u2208 {2, \u00b7 \u00b7 \u00b7 , 10}).", "labels": [], "entities": []}, {"text": "The systems differ in the models trained for the surprise languages.", "labels": [], "entities": []}, {"text": "The models corresponding to the known language treebanks and the new parallel treebanks were same in all the systems.", "labels": [], "entities": []}, {"text": "Since the test set was blind, the first four systems (software 2 to 5) consisted of a combination of models for the surprise languages that were expected to perform best based on the performance on the sample treebanks.", "labels": [], "entities": []}, {"text": "The remaining 5 consisted of models corresponding to combinations of top-k (k= 1, 5, 10, 15, 20) models for each of the surprise languages.", "labels": [], "entities": []}, {"text": "Table 3.1.3 lists the treebanks combined to train the models for our primary system.", "labels": [], "entities": []}, {"text": "We summarize the macro-averaged LAS scores for the surprise languages for the 8 models in.3.", "labels": [], "entities": [{"text": "LAS", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.7188631892204285}]}, {"text": "The highest scoring system for the surprise languages (software2) consists of top-2 model for Buryat, top-10 model for Kurmanji (Kurdish) and top-6 models for North S\u00e1mi and Upper Sorbian.", "labels": [], "entities": [{"text": "Buryat", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.9054932594299316}]}, {"text": "The results using the primary system is summarized in.1 and the macroaverage overall submitted softwares are listed in.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ordering of the head-modifier pairs in the target language as derived from the universal depen- dency treebank statistics. \"pre\" indicates that the modifier precedes the head, \"post\" indicates that the  modifier succeeds the head and \"-\" indicates that the ordering cannot be decided. \"\u00d7\" shows that the  dependency does apply to the language. Some of the feature identifiers are derived from WALS: 81A - order of subject, object and verb in a sentence, 84A -order of object, oblique and verb, 87A -ordering of  ADJ and NOUN, 85A -ordering of ADP and NOUN, 37A/38A -ordering of Definite/Indefinite articles  and NOUN, 88A -ordering of Demonstrative and NOUN, 86A -ordering of genitive and NOUN, 90A - ordering of relative clause and VERB", "labels": [], "entities": []}, {"text": " Table 3: UAS and LAS on blind test as obtained by primary system run and their comparison with the  best runs. Stan. and C2L2 refer to the systems submitted by Stanford University and C2L2 (Ithaca)  respectively", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8133482336997986}, {"text": "LAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8689194917678833}]}, {"text": " Table 5: UAS and LAS scores of models trained on treebank combinations on the surprise language  sample data", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.4928053617477417}, {"text": "LAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9131311178207397}]}, {"text": " Table 6: Comparison of LAS F1 scores of the submitted systems and their macro-averages on the surprise  language test data. The system with highest macro-averaged LAS F1 score (software2) is composed of  top-2, top-10, top-6, top-6 models for bxr, kmr, sme and hsb respectively. The software4 is composed of  top-1, top-2, top-3, top-15 models for bxr, kmr, sme and hsb respectively and the software5 is composed  of top-2, top-10, top-15, top-15 models for bxr, kmr, sme and hsb respectively. For the remaining systems  (software6-10) we combined the top-5, top-10, top-15 and top-20 treebanks respectively.", "labels": [], "entities": [{"text": "LAS F1 score", "start_pos": 164, "end_pos": 176, "type": "METRIC", "confidence": 0.6575654745101929}]}]}