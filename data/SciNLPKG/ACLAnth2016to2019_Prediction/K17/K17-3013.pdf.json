{"title": [{"text": "The parse is darc and full of errors: Universal dependency parsing with transition-based and graph-based algorithms", "labels": [], "entities": [{"text": "Universal dependency parsing", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6509217619895935}]}], "abstractContent": [{"text": "We developed two simple systems for dependency parsing: darc, a transition-based parser, and mstnn, a graph-based parser.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.804399311542511}]}], "introductionContent": [{"text": "Universal Dependencies (UD) () is a cross-linguistically consistent annotation scheme for dependency-based treebanks.", "labels": [], "entities": []}, {"text": "UD version 2.0 (UD2) (,a) provided the datasets for the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies ().", "labels": [], "entities": [{"text": "UD version 2.0 (UD2)", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9031260510285696}, {"text": "CoNLL 2017 Shared Task", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.7926708310842514}]}, {"text": "In the shared task participating systems were evaluated through the TIRA platform ().", "labels": [], "entities": [{"text": "TIRA platform", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.7109998762607574}]}, {"text": "The main evaluation metric was the labeled attachment F 1 -score (LAS).", "labels": [], "entities": [{"text": "attachment F 1 -score (LAS)", "start_pos": 43, "end_pos": 70, "type": "METRIC", "confidence": 0.9229369089007378}]}, {"text": "33 systems completed the official evaluation, including the baseline UDPipe (.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.7622402310371399}]}, {"text": "We submitted a primary system darc and a secondary system mstnn, with the primary system partaking in the official evaluation.", "labels": [], "entities": []}, {"text": "Both are open sourced under the MIT license.", "labels": [], "entities": []}, {"text": "The two systems differ only in the parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9822210669517517}]}, {"text": "Darc is equipped with a transitionbased non-projective/projective parser.", "labels": [], "entities": [{"text": "Darc", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9635515809059143}]}, {"text": "Mstnn is equipped with a graph-based non-projective unlabeled parser and a standalone labeler.", "labels": [], "entities": [{"text": "Mstnn", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8964483737945557}]}, {"text": "Both sys-1 https://github.com/CoNLL-UD-2017/darc tems utilize a neural network classifier with similar input features.", "labels": [], "entities": []}, {"text": "In this paper, we start with a description of our treatments for different datasets in the shared task, and then the separate descriptions of our two parsers, followed by an analysis of the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We were tasked with producing parsed outputs for 81 test-sets, either from raw texts or from segmented and tagged inputs produced by the baseline system.", "labels": [], "entities": []}, {"text": "The outputs were required to conform to the CoNLL-U format.", "labels": [], "entities": [{"text": "CoNLL-U format", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9323708713054657}]}, {"text": "In this format, each node in a dependency-graph hasten fields named ID, FORM, LEMMA, UPOSTAG, XPOSTAG, FEATS, HEAD, DEPREL, DEPS, and MISC, where ID, HEAD, and DEPREL defines an edge.", "labels": [], "entities": [{"text": "FORM", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9964806437492371}, {"text": "LEMMA", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9761713147163391}, {"text": "XPOSTAG", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9076989889144897}, {"text": "FEATS", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.982448399066925}, {"text": "MISC", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.8059804439544678}]}, {"text": "Segmentation establishes the graph/sentence boundaries while filling in ID, FORM, and MISC.", "labels": [], "entities": [{"text": "ID", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9031070470809937}, {"text": "FORM", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9936838150024414}, {"text": "MISC", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.8084657788276672}]}, {"text": "63 test-sets have corresponding treebanks in UD2.", "labels": [], "entities": [{"text": "UD2", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8883611559867859}]}, {"text": "These treebanks were the only training resources we used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 lists the number of rows (min, max &  avg) and columns (dim) in the embedding matri- ces.", "labels": [], "entities": []}, {"text": " Table 2: Official results (LAS)", "labels": [], "entities": [{"text": "LAS)", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.8446941375732422}]}, {"text": " Table 4: Statistics for FEATS", "labels": [], "entities": [{"text": "FEATS", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.4498920440673828}]}]}