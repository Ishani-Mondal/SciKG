{"title": [{"text": "A non-DNN Feature Engineering Approach to Dependency Parsing - FBAML at CoNLL 2017 Shared Task", "labels": [], "entities": [{"text": "Dependency Parsing - FBAML", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6779757291078568}, {"text": "CoNLL 2017 Shared Task", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.8406545966863632}]}], "abstractContent": [{"text": "For this year's multilingual dependency parsing shared task, we developed a pipeline system, which uses a variety of features for each of its components.", "labels": [], "entities": [{"text": "multilingual dependency parsing shared task", "start_pos": 16, "end_pos": 59, "type": "TASK", "confidence": 0.7321911811828613}]}, {"text": "Unlike the recent popular deep learning approaches that learn low dimensional dense features using non-linear classifier, our system uses structured linear classi-fiers to learn millions of sparse features.", "labels": [], "entities": []}, {"text": "Specifically, we trained a linear classifier for sentence boundary prediction, linear chain conditional random fields (CRFs) for tokenization, part-of-speech tagging and morph analysis.", "labels": [], "entities": [{"text": "sentence boundary prediction", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7036831180254618}, {"text": "tokenization", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.9640679955482483}, {"text": "part-of-speech tagging", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7125859558582306}, {"text": "morph analysis", "start_pos": 170, "end_pos": 184, "type": "TASK", "confidence": 0.7126593887805939}]}, {"text": "A second order graph based parser learns the tree structure (without relations), and a linear tree CRF then assigns relations to the dependencies in the tree.", "labels": [], "entities": []}, {"text": "Our system achieves reasonable performance-67.87% official averaged macro F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9486984312534332}]}], "introductionContent": [{"text": "Our system for the universal dependency parsing shared task in CoNLL 2017 () follows atypical pipeline framework.", "labels": [], "entities": [{"text": "universal dependency parsing shared task", "start_pos": 19, "end_pos": 59, "type": "TASK", "confidence": 0.7305618345737457}, {"text": "CoNLL 2017", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8536155521869659}]}, {"text": "The system architecture is shown in, which consists of the following components : (1) sentence segmentor, which segments raw text into sentences, (2) tokenizer that tokenizes sentences into words, or performs word segmentation for Asian languages, (3) morphologic analyzer generates morphologic features, (4) part-of-speech (POS) tagger generates universal POS tags and language specific POS tags, (5) parser predicts tree structures without relations, (6) a relation predictor assigns relations to the dependencies in the tree.", "labels": [], "entities": [{"text": "sentence segmentor", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7385169565677643}]}, {"text": "For each component, we take anon deep learning based approach, that is the typical structured linear classifier that learns sparse features, but requires heavy feature engineering.", "labels": [], "entities": []}, {"text": "Sentence segmentation, tokenization, POS tagger and morphologic analyzer are based on linear chain CRFs (), and the relation predictor is based on linear tree CRFs.", "labels": [], "entities": [{"text": "Sentence segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9253112077713013}, {"text": "tokenization", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9629185199737549}, {"text": "POS tagger", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.75340336561203}, {"text": "relation predictor", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7571284770965576}]}, {"text": "We train the pipeline for each language independently using the training portion of the treebank and the official word embeddings for 45 languages provided by the organizers.", "labels": [], "entities": []}, {"text": "Our system components are implemented in C++ with no third party toolkits.", "labels": [], "entities": []}, {"text": "Due to the time limit, we did not optimize our system for speed or memory.", "labels": [], "entities": [{"text": "speed", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9856042861938477}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of trigger characters for EOS de- tection.", "labels": [], "entities": [{"text": "EOS de- tection", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6218396723270416}]}, {"text": " Table 7: Performance of our system on development dataset. XPOS accuracy for some languages are  quite low due to the format issue.", "labels": [], "entities": [{"text": "XPOS", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.7686657309532166}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8117895722389221}]}, {"text": " Table 8: Official performance of our system on big treebanks. For language no nynorsk, we used the  model trained on another language, thus got very poor result.", "labels": [], "entities": []}, {"text": " Table 9: Official performance of our system on small treebanks, PUD treebanks and suprise languages.", "labels": [], "entities": [{"text": "PUD treebanks", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.7442644834518433}]}]}