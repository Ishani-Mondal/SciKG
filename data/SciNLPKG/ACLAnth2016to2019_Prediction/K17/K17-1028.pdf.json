{"title": [{"text": "Making Neural QA as Simple as Possible but not Simpler", "labels": [], "entities": [{"text": "Neural QA", "start_pos": 7, "end_pos": 16, "type": "TASK", "confidence": 0.6566420644521713}, {"text": "Simpler", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9951040744781494}]}], "abstractContent": [{"text": "Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8598839521408081}]}, {"text": "Increasingly complex systems have been conceived without comparison to simpler neu-ral baseline systems that would justify their complexity.", "labels": [], "entities": []}, {"text": "In this work, we propose a simple heuristic that guides the development of neural baseline systems for the ex-tractive QA task.", "labels": [], "entities": []}, {"text": "We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks.", "labels": [], "entities": []}, {"text": "Our results show that FastQA, a system that meets these two requirements , can achieve very competitive performance compared with existing models.", "labels": [], "entities": [{"text": "FastQA", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.8602461814880371}]}, {"text": "We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective .", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.7668990790843964}]}], "introductionContent": [{"text": "Question answering is an important end-user task at the intersection of natural language processing (NLP) and information retrieval (IR).", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9351186156272888}, {"text": "information retrieval (IR)", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.849990963935852}]}, {"text": "QA systems can bridge the gap between IR-based search engines and sophisticated intelligent assistants that enable a more directed information retrieval process.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.7457485198974609}]}, {"text": "Such systems aim at finding precisely the piece of information sought by the user instead of documents or snippets containing the answer.", "labels": [], "entities": []}, {"text": "A special form of QA, namely extractive QA, deals with the extraction of a direct answer to a question from a given textual context.", "labels": [], "entities": [{"text": "extraction of a direct answer to a question from a given textual context", "start_pos": 59, "end_pos": 131, "type": "TASK", "confidence": 0.7208319925344907}]}, {"text": "The creation of large-scale, extractive QA datasets () sparked research interest into the development of end-to-end neural QA systems.", "labels": [], "entities": []}, {"text": "A typical neural architecture consists of an embedding-, encoding-, interaction-and answer layer ().", "labels": [], "entities": []}, {"text": "Most such systems describe several innovations for the different layers of the architecture with a special focus on developing powerful interaction layer that aims at modeling word-by-word interaction between question and context.", "labels": [], "entities": []}, {"text": "Although a variety of extractive QA systems have been proposed, there is no competitive neural baseline.", "labels": [], "entities": []}, {"text": "Most systems were builtin what we calla top-down process that proposes a complex architecture and validates design decisions by an ablation study.", "labels": [], "entities": []}, {"text": "Most ablation studies, however, remove only a single part of an overall complex architecture and therefore lack comparison to a reasonable neural baseline.", "labels": [], "entities": []}, {"text": "This gap raises the question whether the complexity of current systems is justified solely by their empirical results.", "labels": [], "entities": []}, {"text": "Another important observation is the fact that seemingly complex questions might be answerable by simple heuristics.", "labels": [], "entities": []}, {"text": "Let's consider the following example: When did building activity occur on St. Kazimierz Church?", "labels": [], "entities": []}, {"text": "Building activity occurred in numerous noble palaces and churches.", "labels": [], "entities": []}, {"text": "One of the best examples [..] are Krasinski Palace, Wilanow Palace (1677-1696) and Although it seems that evidence synthesis of multiple sentences is necessary to fully understand the relation between the answer and the question, answering this question is easily possible by applying a simple context/type matching heuristic.", "labels": [], "entities": []}, {"text": "The heuristic aims at selecting answer spans that a) match the expected answer type (a time as indicated by \"When\") and b) are close to important question words (\"St. Kazimierz Church\").", "labels": [], "entities": []}, {"text": "The actual answer \"1688-1692\" would easily be extracted by such a heuristic.", "labels": [], "entities": []}, {"text": "In this work, we propose to use the aforementioned context/type matching heuristic as a guideline to derive simple neural baseline architectures for the extractive QA task.", "labels": [], "entities": []}, {"text": "In particular, we develop a simple neural, bag-of-words (BoW)-and a recurrent neural network (RNN) baseline, namely FastQA.", "labels": [], "entities": [{"text": "FastQA", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.9340476393699646}]}, {"text": "Crucially, both models do not make use of a complex interaction layer but model interaction between question and context only through computable features on the word level.", "labels": [], "entities": []}, {"text": "FastQA's strong performance questions the necessity of additional complexity, especially in the interaction layer, which is exhibited by recently developed models.", "labels": [], "entities": []}, {"text": "We address this question by evaluating the impact of extending FastQA with an additional interaction layer (FastQAExt) and find that it doesn't lead to systematic improvements.", "labels": [], "entities": []}, {"text": "Finally, our contributions are the following: i) definition and evaluation of a BoW-and RNN-based neural QA baselines guided by a simple heuristic; ii) bottom-up evaluation of our FastQA system with increasing architectural complexity, revealing that the awareness of question words and the application of a RNN are enough to reach stateof-the-art results; iii) a complexity comparison between FastQA and more complex architectures as well as an in-depth discussion of usefulness of an interaction layer; iv) a qualitative analysis indicating that FastQA mostly follows our heuristic which thus constitutes a strong baseline for extractive QA.", "labels": [], "entities": [{"text": "BoW-and RNN-based neural QA baselines", "start_pos": 80, "end_pos": 117, "type": "DATASET", "confidence": 0.9025675177574157}]}], "datasetContent": [{"text": "We conduct experiments on the following datasets..", "labels": [], "entities": []}, {"text": "Performance on the SQuAD and NewsQA datasets is measured in terms of exact match (accuracy) and a mean, per answer token-based F1 measure which was originally proposed by to also account for partial matches.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.8848276138305664}, {"text": "NewsQA datasets", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.8977333605289459}, {"text": "exact match (accuracy)", "start_pos": 69, "end_pos": 91, "type": "METRIC", "confidence": 0.8599436521530152}, {"text": "F1 measure", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9499044120311737}]}], "tableCaptions": [{"text": " Table 1: SQuAD results on development set for  increasingly complex architectures. 1 Rajpurkar  et al. (2016)", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.8334433436393738}]}, {"text": " Table 2: Official SQuAD leaderboard of single- model systems on test set from 2016/12/29, the  date of submitting our model. 1 Rajpurkar et al.  (2016), 2 Wang and Jiang (2017), 3 Yu et al. (2017),  4 Yang et al. (2017), 5 Wang et al. (2017), 6 Xiong  et al. (2017), 7 Seo et al. (2017), 8 not published.  Note that systems are regularly uploaded and im- proved on SQuAD.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 366, "end_pos": 371, "type": "DATASET", "confidence": 0.8884021639823914}]}, {"text": " Table 3: Results on the NewsQA dataset.  1 Wang and Jiang (2017) was re-implemented by  2 Trischler et al. (2017).", "labels": [], "entities": [{"text": "NewsQA dataset", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9922911524772644}]}]}