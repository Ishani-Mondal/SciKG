{"title": [{"text": "RACAI's Natural Language Processing pipeline for Universal Dependencies", "labels": [], "entities": [{"text": "RACAI", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.8238645792007446}]}], "abstractContent": [{"text": "This paper presents RACAI's approach, experiments and results at CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.8733080178499222}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 89, "end_pos": 123, "type": "TASK", "confidence": 0.7607304632663727}]}, {"text": "We handle raw text and we cover tokeniza-tion, sentence splitting, word segmenta-tion, tagging, lemmatization and parsing.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7605423033237457}]}, {"text": "All results are reported under strict training , development and testing conditions, in which the corpora provided for the shared tasks is used \"as is\", without any modifications to the composition of the train and development sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes RACAI's entry for the CONLL Shared Task on Universal Dependencies parsing.", "labels": [], "entities": [{"text": "CONLL Shared Task on Universal Dependencies parsing", "start_pos": 43, "end_pos": 94, "type": "TASK", "confidence": 0.7019973056656974}]}, {"text": "We represent the Research Institute for Artificial Intelligence, in Bucharest, Romania.", "labels": [], "entities": []}, {"text": "The shared task refers to processing raw text with the goal of automatically inferring word dependencies.", "labels": [], "entities": []}, {"text": "While some approaches require only segmented (tokenized) text, parsing methods that depend on rich feature sets (which is our case), implicitly require that the text is tokenized, POS tagged and lemmatized.", "labels": [], "entities": []}, {"text": "The Universal Dependencies (UD) corpus) uses 3 distinct layers of analysis: (a) a Universal Part-of-Speech layer (UPOS); (b) a language specific part-of-speech layer (XPOS) and (c) a list of language-dependent morphological attributes.", "labels": [], "entities": []}, {"text": "Also, in the current version of UD, tokenization and wordsegmentation require different handling strategies (see section 3.2 for details).", "labels": [], "entities": [{"text": "tokenization", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9692028760910034}]}, {"text": "In what follows we will provide an overview of our system's architecture (section 2) and a detailed description of each module (section 3) used in our processing pipeline, followed by its evaluation (section 4) () on the TIRA platform ().", "labels": [], "entities": [{"text": "TIRA platform", "start_pos": 221, "end_pos": 234, "type": "DATASET", "confidence": 0.8808954060077667}]}, {"text": "Though Syntaxnet () models were also available, some discrepancies in the token and word-segmentation methodology made the comparison impossible (mainly because the Syntaxnet's output was incompatible with the evaluation script).", "labels": [], "entities": []}, {"text": "This work is focused on presenting our system's technical details and individual results.", "labels": [], "entities": []}, {"text": "The full comparison between competing systems, as well as the baseline values obtained by UDPipe v1.1 () are available in.", "labels": [], "entities": [{"text": "UDPipe v1.1", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.8203705549240112}]}], "datasetContent": [{"text": "For sentence splitting, also based on DT, we obtained an average F1 score of 87.52 versus the top score of 89.10.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8023661375045776}, {"text": "F1 score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9849035143852234}]}, {"text": "We obtained first place on a number of languages like Hebrew, Basque or Latin.", "labels": [], "entities": []}, {"text": "However on Latvian we obtained last place with an F1 of 93.30 versus 98.90.", "labels": [], "entities": [{"text": "Latvian", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9739425182342529}, {"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9995949864387512}]}, {"text": "We note that no languagedependent tuning was performed, neither for tokenization nor for sentence splitting.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.9668821692466736}, {"text": "sentence splitting", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7476674020290375}]}, {"text": "The same features were chosen for all languages.", "labels": [], "entities": []}, {"text": "While we did perform tree pruning based on the dev sets, we did not vary and choose the best feature set for each language (e.g. tokenization on some languages was better with a context of 2,2, while for others with context 4,1; we used 3,3 for every language that had punctuation and was not pre-tokenized).", "labels": [], "entities": []}, {"text": "In word segmentation we obtained a score of 98.39 vs 98.81, a difference of only 0.42 percent.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7467608451843262}]}, {"text": "Using the DT classifier brought us top places in several languages like Czech (CLTT), Danish, Norwegian (Bokmaal) or Russian.", "labels": [], "entities": []}, {"text": "Lemmatization, also based on decision trees, unfortunately worked really well on only a small number of languages.", "labels": [], "entities": []}, {"text": "For example on Farsi we were the first with a 1.5 point difference over second place.", "labels": [], "entities": [{"text": "Farsi", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.6922486424446106}]}, {"text": "Overall, we obtained an F1 score of 77.45 versus the top performer that obtained 83.74.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9880096614360809}]}, {"text": "The morphological features average F1 score of 70.8 brings us relatively close to the top score of 73.92.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9841170310974121}]}, {"text": "Again, while not a best performer, the decision tree algorithm we used has shown very good performance compared to more complex algorithms in the competition.", "labels": [], "entities": []}, {"text": "On the language independent parts of speech (UPOSes) we obtained 90.71 vs 93.09.", "labels": [], "entities": []}, {"text": "On language dependent parts of speech (XPOSes) we have an average F1 of 78.20 vs the top performer 82.27, a larger difference that for UPOSes.", "labels": [], "entities": [{"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9995189905166626}, {"text": "UPOSes", "start_pos": 135, "end_pos": 141, "type": "DATASET", "confidence": 0.8486698269844055}]}, {"text": "Finally, on parsing we obtained an UAS of 74.67 vs 81.30 (11th place), and on LAS an F1 score of 67.71 vs 76.3, placing us on the 18th place.", "labels": [], "entities": [{"text": "UAS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.992066502571106}, {"text": "LAS", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9964099526405334}, {"text": "F1 score", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9867719113826752}]}], "tableCaptions": [{"text": " Table 2: Results per language", "labels": [], "entities": []}]}