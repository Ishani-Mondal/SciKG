{"title": [{"text": "Stanford's Graph-based Neural Dependency Parser at the CoNLL 2017 Shared Task", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.942887470126152}]}], "abstractContent": [{"text": "This paper describes the neural dependency parser submitted by Stanford to the CoNLL 2017 Shared Task on parsing Universal Dependencies.", "labels": [], "entities": [{"text": "neural dependency parser", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.6635698477427164}, {"text": "CoNLL 2017 Shared Task", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.8381400108337402}, {"text": "parsing Universal Dependencies", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.6699337561925253}]}, {"text": "Our system uses relatively simple LSTM networks to produce part of speech tags and labeled dependency parses from segmented and tok-enized sequences of words.", "labels": [], "entities": [{"text": "labeled dependency parses from segmented and tok-enized sequences of words", "start_pos": 83, "end_pos": 157, "type": "TASK", "confidence": 0.7907790005207062}]}, {"text": "In order to address the rare word problem that abounds in languages with complex morphology, we include a character-based word representation that uses an LSTM to produce embeddings from sequences of characters.", "labels": [], "entities": []}, {"text": "Our system was ranked first according to all five relevant metrics for the system: UPOS tagging (93.09%), XPOS tagging (82.27%), unlabeled attachment score (81.30%), labeled attachment score (76.30%), and content word labeled attachment score (72.57%).", "labels": [], "entities": [{"text": "UPOS tagging", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.6306087374687195}, {"text": "XPOS tagging", "start_pos": 106, "end_pos": 118, "type": "TASK", "confidence": 0.6007355153560638}, {"text": "unlabeled attachment score (81.30%)", "start_pos": 129, "end_pos": 164, "type": "METRIC", "confidence": 0.7717028309901556}, {"text": "labeled attachment score (76.30%)", "start_pos": 166, "end_pos": 199, "type": "METRIC", "confidence": 0.8353302280108134}]}], "introductionContent": [{"text": "In this paper, we describe Stanford's approach to tackling the CoNLL 2017 shared task on Universal Dependency parsing (.", "labels": [], "entities": [{"text": "tackling the CoNLL 2017 shared task", "start_pos": 50, "end_pos": 85, "type": "TASK", "confidence": 0.6563142488400141}, {"text": "Universal Dependency parsing", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.7220110595226288}]}, {"text": "Our system builds on the deep biaffine neural dependency parser presented by, which uses a well-tuned LSTM network to produce vector representations for each word, then uses those vector representations in novel biaffine classifiers to predict the head token of each dependent and the class of the resulting edge.", "labels": [], "entities": []}, {"text": "In order to adapt it to the wide variety of different treebanks in Universal Dependencies, we make two noteworthy extensions to the system: first, we incorporate a word representation built up from character sequences using an LSTM, theorizing that this should improve the model's ability to adapt to rare or unknown words in languages with rich morphology; second, we train our own taggers for the treebanks using nearly identical architecture to the one used for parsing, in order to capitalize on potential improvements in part of speech tag quality over baseline or off-the-shelf taggers.", "labels": [], "entities": []}, {"text": "This approach gets state-of-the-art results on the macro average of the shared task datasets according to all five POS tagging and attachment accuracy metrics.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 115, "end_pos": 126, "type": "TASK", "confidence": 0.7405684292316437}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.4819754958152771}]}, {"text": "One noteworthy feature of our approach is its relative simplicity.", "labels": [], "entities": []}, {"text": "It uses a single tagger/parser pair per language, trained on only words and tags; thus we refrain from taking advantage of ensembling, lemmas, or morphological features, anyone of which could potentially push accuracy even higher.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9986757636070251}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on each treebank in the shared task plus the macro average over all of them. State of the  art performance by the system is in bold.", "labels": [], "entities": []}]}