{"title": [{"text": "ISI at the SIGMORPHON 2017 Shared Task on Morphological Reinflection", "labels": [], "entities": [{"text": "Morphological Reinflection", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7790960073471069}]}], "abstractContent": [{"text": "We present a system for morphological reinflection based on the LSTM model.", "labels": [], "entities": [{"text": "morphological reinflection", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7408261001110077}]}, {"text": "Given an input word and morphosyntac-tic descriptions, the problem is to classify the proper edit tree that, applied on the input word, produces the target form.", "labels": [], "entities": []}, {"text": "The proposed method does not require human defined features and it is language independent also.", "labels": [], "entities": []}, {"text": "Currently, we evaluate our system only for task 1 without using any external data.", "labels": [], "entities": []}, {"text": "From the test set results, it is found that the proposed model beats the baseline on 15 out of the 52 languages in high resource scenario.", "labels": [], "entities": []}, {"text": "But its performance is poor when the training set size is medium or low.", "labels": [], "entities": []}], "introductionContent": [{"text": "The morphological reinflection task is to generate the variant of a source word, given the morphosyntactic descriptions of the target word.", "labels": [], "entities": []}, {"text": "This year's shared task () is divided into two sub-tasks.", "labels": [], "entities": []}, {"text": "Task 1 demands to inflect the isolated word forms based on labelled training data.", "labels": [], "entities": []}, {"text": "For example, given the source form 'communicate' and the features 'V;3;SG;PRS', one has to predict the target form 'communicates'.", "labels": [], "entities": []}, {"text": "Whereas, in task 2, partially filled incomplete paradigms are provided.", "labels": [], "entities": []}, {"text": "The goal is to complete them using a restricted number of full paradigms.", "labels": [], "entities": []}, {"text": "For each of the tasks, 3 separate training files are given per language, which differ in size (low/medium/high), in order to analyze systems' generalization ability in low and high resource situations.", "labels": [], "entities": []}, {"text": "The competition is spread over 52 languages.", "labels": [], "entities": []}, {"text": "For each language, a finite set of morphological tags are provided, from which the target inflections are taken.", "labels": [], "entities": []}, {"text": "Evaluation is done separately under each of the three different training sets.", "labels": [], "entities": []}, {"text": "To make the shared task competition fair, use of external resources are forbidden for the main competition track.", "labels": [], "entities": []}, {"text": "However, for those systems which make use of external monolingual corpora, a list of approved external corpora selected from the Wikipedia text dumps are provided.", "labels": [], "entities": []}, {"text": "So far, there have been several efforts on reinflection employing statistical learning based methods ( and string transduction).", "labels": [], "entities": []}, {"text": "These methods entail feature definition which is hard to generalize for all of the world's languages.", "labels": [], "entities": [{"text": "feature definition", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7173848301172256}]}, {"text": "In this article, we introduce along short-term memory (LSTM) network architecture to handle the morphological reinflection task.", "labels": [], "entities": []}, {"text": "The proposed method is language independent and does not require features to be defined manually.", "labels": [], "entities": []}, {"text": "Our model is related to the encoder-decoder based approaches such as (, but the main difference is that the proposed network is not designed to generate sequence of characters as output.", "labels": [], "entities": []}, {"text": "Rather, we formulate the problem as to classify the transformation process required to convert a source form to its target form (.", "labels": [], "entities": []}, {"text": "Our goal is to model such a system which receives an input word and the morphological tags and returns the proper transformation that induces the target word.", "labels": [], "entities": []}, {"text": "The source-target transformation is accomplished using edit tree (.", "labels": [], "entities": []}, {"text": "Initially all edit trees are extracted from the labelled pairs in the training data and then the distinct candidates from them are marked as the class labels.", "labels": [], "entities": []}, {"text": "We feed the character sequence of the input word through the LSTM network to encode it and finally, the encoded representation is jointly trained with the input tags to classify the correct edit tree.", "labels": [], "entities": []}, {"text": "Currently, we assess our system only for task 1 on all 52 languages, though it can be used for task 2 also.", "labels": [], "entities": []}, {"text": "No external data such as the Wikipedia dumps provided by the SIGMORPHON committee has been exploited in the present work.", "labels": [], "entities": []}, {"text": "The results obtained from the test sets indicate that the proposed method is resource intensive.", "labels": [], "entities": []}, {"text": "When the training size is high, it achieves over the baseline system on 15 out of the 52 languages.", "labels": [], "entities": []}, {"text": "But on medium and low amount training data, the performance is poor beating the baseline on 5 and 4 languages only.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Our model's performance on the test  datasets for those languages where it beats the  baseline system.", "labels": [], "entities": []}, {"text": " Table 2: Our model's performance on the devel- opment datasets for those languages where it beats  the baseline system.", "labels": [], "entities": []}]}