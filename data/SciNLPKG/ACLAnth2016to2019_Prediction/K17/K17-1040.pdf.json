{"title": [{"text": "Neural Structural Correspondence Learning for Domain Adaptation", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7101089358329773}]}], "abstractContent": [{"text": "We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks (NNs).", "labels": [], "entities": []}, {"text": "Our model is a three-layer NN that learns to encode the non-pivot features of an input example into a low-dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation.", "labels": [], "entities": []}, {"text": "The low-dimensional representation is then employed in a learning algorithm for the task.", "labels": [], "entities": []}, {"text": "Moreover, we show how to inject pre-trained word embed-dings into our model in order to improve generalization across examples with similar pivot features.", "labels": [], "entities": []}, {"text": "We experiment with the task of cross-domain sentiment classification on 16 domain pairs and show substantial improvements over strong baselines.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7895875374476115}]}], "introductionContent": [{"text": "Many state-of-the-art algorithms for Natural Language Processing (NLP) tasks require labeled data.", "labels": [], "entities": []}, {"text": "Unfortunately, annotating sufficient amounts of such data is often costly and labor intensive.", "labels": [], "entities": []}, {"text": "Consequently, for many NLP applications even resource-rich languages like English have labeled data in only a handful of domains.", "labels": [], "entities": []}, {"text": "Domain adaptation, training an algorithm on labeled data taken from one domain so that it can perform properly on data from other domains, is therefore recognized as a fundamental challenge in NLP.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7972711026668549}]}, {"text": "Indeed, over the last decade domain adaptation methods have been proposed for tasks such as sentiment classification), POS tagging (, syntactic parsing) and relation extraction), if to name just a handful of applications and works.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7026232033967972}, {"text": "sentiment classification", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.9486216902732849}, {"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.8974713087081909}, {"text": "syntactic parsing", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7213629186153412}, {"text": "relation extraction", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.9075550734996796}]}, {"text": "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7590286731719971}]}, {"text": "These models are believed to extract features that are robust to crossdomain variations.", "labels": [], "entities": []}, {"text": "However, while excelling on benchmark domain adaptation tasks such as crossdomain product sentiment classification, the reasons to this success are not entirely understood.", "labels": [], "entities": [{"text": "benchmark domain adaptation", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.674385686715444}, {"text": "crossdomain product sentiment classification", "start_pos": 70, "end_pos": 114, "type": "TASK", "confidence": 0.773395836353302}]}, {"text": "In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7249572575092316}, {"text": "sentiment classification", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.9510693252086639}, {"text": "structural correspondence learning (SCL)", "start_pos": 124, "end_pos": 164, "type": "TASK", "confidence": 0.7147189031044642}]}, {"text": "Following the auxiliary problems approach to semi-supervised learning), this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task.", "labels": [], "entities": []}, {"text": "Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing abridge between the domains.", "labels": [], "entities": []}, {"text": "Elegant and well motivated as it maybe, SCL does not form the state-of-the-art since the neural approaches took over.", "labels": [], "entities": [{"text": "SCL", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9582750797271729}]}, {"text": "In this paper we marry these approaches, proposing NN models inspired by ideas from both.", "labels": [], "entities": []}, {"text": "Particularly, our basic model receives the nonpivot features of an input example, encodes them into a hidden layer and then, instead of decoding the input layer as an autoencoder would do, it aims to decode the pivot features.", "labels": [], "entities": []}, {"text": "Our more advanced model is identical to the basic one except that the decoding matrix is not learned but is rather replaced with a fixed matrix consisting of pre-trained embeddings of the pivot features.", "labels": [], "entities": []}, {"text": "Under this model the probability of the i-th pivot feature to appear in an example is a (non-linear) function of the dot product of the feature's embedding vector and the network's hidden layer vector.", "labels": [], "entities": []}, {"text": "As explained in Section 3, this approach encourages the model to learn similar hidden layers for documents that have different pivot features as long as these features have similar meaning.", "labels": [], "entities": []}, {"text": "In sentiment classification, for example, although one positive review may use the unigram pivot feature excellent while another positive review uses the pivot great, as long as the embeddings of pivot features with similar meaning are similar (as expected from high quality embeddings) the hidden layers learned for both documents are biased to be similar.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.9523556232452393}]}, {"text": "We experiment with the task of cross-domain product sentiment classification of, consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs).", "labels": [], "entities": [{"text": "cross-domain product sentiment classification", "start_pos": 31, "end_pos": 76, "type": "TASK", "confidence": 0.7101987302303314}]}, {"text": "For pivot feature embedding in our advanced model, we employ the word2vec algorithm ().", "labels": [], "entities": []}, {"text": "Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model) and the MSDA-DAN model () that combines the power of MSDA with a domain adversarial network (DAN).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experiments.", "labels": [], "entities": []}, {"text": "To facilitate clarity, some details are not given here and instead are provided in the appendices.", "labels": [], "entities": []}, {"text": "Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification (.", "labels": [], "entities": [{"text": "Cross-domain Sentiment Classification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6498530507087708}, {"text": "domain adaptation", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7603584229946136}, {"text": "crossdomain sentiment classification", "start_pos": 130, "end_pos": 166, "type": "TASK", "confidence": 0.7217086354891459}]}, {"text": "The data for this task consist of Amazon product reviews from four product domains: Books (B), DVDs (D), Electronic items (E) and Kitchen appliances (K).", "labels": [], "entities": []}, {"text": "For each domain 2000 labeled reviews are provided: 1000 are classified as positive and 1000 as negative, and these are augmented with unlabeled reviews: 6000 (B), 34741 (D), 13153 (E) and 16785 (K).", "labels": [], "entities": []}, {"text": "We also consider an additional target domain, denoted with Blog: the University of Michigan sentence level sentiment dataset, consisting of sentences taken from social media blogs.", "labels": [], "entities": [{"text": "Blog", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.8982799649238586}]}, {"text": "The dataset for the original task consists of a labeled training set (3995 positive and 3091 negative) and a 33052 sentences test set for which sentiment labels are not provided.", "labels": [], "entities": []}, {"text": "We hence used the original test set as our target domain unlabeled set and the original training set as our target domain test set.", "labels": [], "entities": []}, {"text": "Baselines Cross-domain sentiment classification has been studied in a large number of papers.", "labels": [], "entities": [{"text": "Cross-domain sentiment classification", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.7976902921994528}]}, {"text": "However, the difference in preprocessing methods, dataset splits to train/dev/test subsets and the different sentiment classifiers make it hard to directly compare between the numbers reported in past.", "labels": [], "entities": []}, {"text": "We hence compare our models to three strong baselines, running all models under the same conditions.", "labels": [], "entities": []}, {"text": "We aim to select baselines that represent the state-of-the-art in cross-domain sentiment classification in general, and in the two lines of work we focus at: pivot based and autoencoder based representation learning, in particular.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.7761810819307963}]}, {"text": "The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, ().", "labels": [], "entities": []}, {"text": "This is the SCL method where pivot features are frequent in the unlabeled data of both the source and the target domains, and among those features are the ones with the highest mutual information with the task (sentiment) label in the source domain labeled data.", "labels": [], "entities": []}, {"text": "In our implementation unigrams and bigrams should appear at least 10 times in both domains to be considered frequent.", "labels": [], "entities": []}, {"text": "For non-pivot features we consider unigrams and bigrams that appear at least 10 times in their domain.", "labels": [], "entities": []}, {"text": "The same pivot and non-pivot selection criteria are employed for our AE-SCL and AE-SCL-SR models.", "labels": [], "entities": [{"text": "AE-SCL", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.7666423916816711}]}, {"text": "Among autoencoder models, SDA has shown by to outperform SFA and SCL on cross-domain sentiment classification and later on demonstrated superior performance for MSDA over SDA and SCL on the same task.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.6805474360783895}]}, {"text": "Our second baseline is hence the MSDA method (, with code taken from the authors' web page.", "labels": [], "entities": []}, {"text": "To consider a regularization scheme on top of MSDA representations we also experiment with the MSDA-DAN model () which employs a domain adversarial network (DAN) with the MSDA vectors as input.", "labels": [], "entities": []}, {"text": "In MSDA-DAN has shown to substantially outperform the DAN model when DAN is randomly initialized.", "labels": [], "entities": []}, {"text": "The DAN code is taken from the authors' repository.", "labels": [], "entities": [{"text": "DAN code", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8053645193576813}]}, {"text": "For reference we compare to the No-DA case where the sentiment classifier is trained in the source domain and applied to the target domain without adaptation.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7616161704063416}]}, {"text": "The sentiment classifier we employ, in this case as well as with our methods and with the SCL-MI and MSDA baselines, is a standard logistic regression classifier.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8008202016353607}, {"text": "MSDA baselines", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.914462149143219}]}, {"text": "Experimental Protocol Following the unsupervised domain adaptation setup (Section 2), we have access to unlabeled data from both the source and the target domains, which we use to train the representation learning models.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7624930441379547}]}, {"text": "However, only the source domain has labeled training data for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.9653345942497253}]}, {"text": "The original feature set we start from consists of word unigrams and bigrams.", "labels": [], "entities": []}, {"text": "All methods (baselines and ours), except from MSDA-DAN, follow a two-step protocol at both training and test time.", "labels": [], "entities": [{"text": "MSDA-DAN", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.8778695464134216}]}, {"text": "In the first step, the input example is run through the representation model which generates anew feature vector for this example.", "labels": [], "entities": []}, {"text": "Then, in the second step, this vector is concatenated with the original feature vector of the ex-ample and the resulting vector is fed into the sentiment classifier (this concatenation is a standard convention in the baseline methods).", "labels": [], "entities": []}, {"text": "For MSDA-DAN all the above holds, except from one exception.", "labels": [], "entities": [{"text": "MSDA-DAN", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.870675802230835}]}, {"text": "MSDA-DAN gets an input representation that consists of a concatenation of the original and the MSDA-induced feature sets.", "labels": [], "entities": [{"text": "MSDA-DAN", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.874423623085022}]}, {"text": "As this is an end-to-end model that predicts the sentiment class jointly with the new feature representation, we do not employ any additional sentiment classifier.", "labels": [], "entities": []}, {"text": "As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time.", "labels": [], "entities": []}, {"text": "We experiment with a 5-fold cross-validation on the source domain (: 1600 reviews for training and 400 reviews for development.", "labels": [], "entities": []}, {"text": "The test set for each target domain of consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.", "labels": [], "entities": []}, {"text": "In all five folds half of the training examples and half of the development examples are randomly selected from the positive reviews and the other halves from the negative reviews.", "labels": [], "entities": []}, {"text": "We report average results across these five folds, employing the same folds for all models.", "labels": [], "entities": []}, {"text": "Hyper-parameter Tuning The details of the hyper-parameter tuning process for all models (including data splits to training, development and test sets) are described in the appendices.", "labels": [], "entities": []}, {"text": "Here we provide a summary.", "labels": [], "entities": []}, {"text": "AE-SCL and AE-SCL-SR: For the stochastic gradient descent (SGD) training algorithm we set the learning rate to 0.1, momentum to 0.9 and weightdecay regularization to 10 \u22125 . The number of pivots was chosen among {100, 200, . .", "labels": [], "entities": [{"text": "AE-SCL", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9509971737861633}, {"text": "AE-SCL-SR", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9581468105316162}, {"text": "stochastic gradient descent (SGD) training", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.7550871201923915}, {"text": "momentum", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9792349338531494}]}, {"text": ", 500} and the dimensionality of h among {100, 300, 500}.", "labels": [], "entities": []}, {"text": "For the features induced by these models we take their w h x np vector.", "labels": [], "entities": []}, {"text": "For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (.", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7348248958587646}]}, {"text": "Details about the software and the way we learn bigram representations are in the appendices.", "labels": [], "entities": []}, {"text": "Baselines: For SCL-MI, following we tuned the number of pivot features between 500 and 1000 and the SVD dimensions among 50,100 and 150.", "labels": [], "entities": []}, {"text": "For MSDA we tuned the number of reconstructed features among {500, 1000, 2000, 5000, 10000}, the number of model layers among {1, 3, 5} and the corrup-), p < 0.05) is denoted with: * (AE-SCL-SR vs. AE-SCL), + (AE-SCL-SR vs. MSDA), (AE-SCL-SR vs. MSDA-DAN), \u2021 (AE-SCL vs. MSDA) and (AE-SCL vs. MSDA-DAN).", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9094020128250122}]}, {"text": "All the differences between any model and No-DA are statistically significant.", "labels": [], "entities": []}, {"text": "tion probability among {0.1, 0.2, . .", "labels": [], "entities": []}, {"text": "For MSDA-DAN, we followed: the \u03bb adaptation parameter is chosen among 9 values between 10 \u22122 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate \u00b5 is 10 \u22123 . presents our results.", "labels": [], "entities": [{"text": "learning rate \u00b5", "start_pos": 186, "end_pos": 201, "type": "METRIC", "confidence": 0.8405524492263794}]}, {"text": "In the task (top tables), AE-SCL-SR is the best performing model in 9 of 12 setups and on a unified test set consisting of the test sets of all 12 setups (the Test-All column).", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9916270971298218}]}, {"text": "AE-SCL, MSDA and MSDA-DAN perform best in one setup each.", "labels": [], "entities": [{"text": "AE-SCL", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8951507210731506}]}, {"text": "On the unified test set, AE-SCL-SR improves over SCL-MI by 3.8% (error reduction (ER) of 14.8%) and over MSDA-DAN by 2% (ER of 8.4%), while AE-SCL improves over SCL-MI and MSDA-DAN by 2.7% (ER of 10.5%) and 0.9% (ER of 3.8%), respectively.", "labels": [], "entities": [{"text": "unified test set", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.7009113430976868}, {"text": "AE-SCL-SR", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9707630276679993}, {"text": "error reduction (ER)", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.9643126606941224}, {"text": "ER", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9683148264884949}, {"text": "AE-SCL", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.8942807912826538}, {"text": "ER", "start_pos": 190, "end_pos": 192, "type": "METRIC", "confidence": 0.9822801947593689}, {"text": "ER", "start_pos": 213, "end_pos": 215, "type": "METRIC", "confidence": 0.9910861849784851}]}, {"text": "MSDA-DAN and MSDA perform very similarly on the unified test set (0.761 and 0.759, respectively) with generally minor differences in the individual setups.", "labels": [], "entities": [{"text": "MSDA-DAN", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8697817325592041}, {"text": "MSDA", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8842650055885315}]}, {"text": "When adapting from the product review domains to the Blog domain (bottom table), AE-SCL-SR performs best in 3 of 4 setups, providing particularly large improvements when training is in the Kitchen (K) domain.", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9457525014877319}]}, {"text": "The average improvement of AE-SCL-SR over MSDA is 5.2% and over a non-adapted classifier is 11.7%.", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9965443015098572}]}, {"text": "As before, MSDA-DAN performs similarly to MSDA on the unified test set, although the differences in the individual setups are much higher.", "labels": [], "entities": []}, {"text": "The differences between AE-SCL-SR and the other models are statistically significant inmost cases.: Document pair examples from eight setups (1st column) with the same gold sentiment class.", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9111896753311157}]}, {"text": "In all cases, AE-SCL-SR correctly classifies both documents, while AE-SCL misclassifies one (5th column).", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.8610109090805054}, {"text": "AE-SCL", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.8094236254692078}]}, {"text": "The 6th column presents the difference in the ranking of the cosine scores between the representation vectors\u02dchvectors\u02dc vectors\u02dch of the documents according to both models (the rank of AE-SCL minus the rank of AE-SCL-SR), both in absolute values and as a percentage of the 1,999,000 document pairs in the test set of each setup.", "labels": [], "entities": [{"text": "AE-SCL", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.962785542011261}, {"text": "AE-SCL-SR", "start_pos": 210, "end_pos": 219, "type": "METRIC", "confidence": 0.8747632503509521}]}, {"text": "As\u02dchAs\u02dc As\u02dch is feeded to the sentiment classifer we expect documents that belong to the same class to have more similar\u02dchsimilar\u02dc similar\u02dch vectors.", "labels": [], "entities": []}, {"text": "The differences are indeed positive in all 8 cases.", "labels": [], "entities": []}, {"text": "SCL-SR, this is a weaker effect which only moderates the overall superiority of AE-SCL-SR.", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9918521046638489}]}, {"text": "The unlabeled documents from all four domains are strongly biased to convey positive opinions (Section 4).", "labels": [], "entities": []}, {"text": "This is indicated, for example, by the average score given to these reviews by their authors: 4.29 (B), 4.33 (D), 3.96 (E) and 4.16 (K), on a scale of 1 to 5.", "labels": [], "entities": []}, {"text": "This analysis suggests that AE-SCL-SR better learns from of its unlabeled data.", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9013276696205139}]}, {"text": "Variants of the Product Review Data There are two releases of the datasets of the cross-domain product review task.", "labels": [], "entities": [{"text": "Product Review Data", "start_pos": 16, "end_pos": 35, "type": "DATASET", "confidence": 0.6959063510100046}, {"text": "cross-domain product review task", "start_pos": 82, "end_pos": 114, "type": "TASK", "confidence": 0.6567702144384384}]}, {"text": "We use the one from http://www.cs.jhu.edu/ \u02dc mdredze/ datasets/sentiment/index2.html where the data is imbalanced, consisting of more positive than negative reviews.", "labels": [], "entities": []}, {"text": "We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.", "labels": [], "entities": []}, {"text": "Note that used the other release where the unlabeled data consists of the same number of positive and negative reviews.", "labels": [], "entities": []}, {"text": "Test Set Size While used only 400 target domain reviews for test, we use the entire set of 2000 reviews.", "labels": [], "entities": []}, {"text": "We believe that this decision yields more robust and statistically significant results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentiment classification accuracy for the Blitzer et al. (2007) task (top tables), and for adaptation  from the Blitzer's product review domains to the Blog domain (bottom table). Test-All presents average  results across setups. Statistical significance (with the McNemar paired test for labeling disagreements  (", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7711275517940521}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9689623117446899}, {"text": "labeling disagreements", "start_pos": 299, "end_pos": 321, "type": "TASK", "confidence": 0.8799022734165192}]}]}