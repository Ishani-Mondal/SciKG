{"title": [{"text": "The LMU System for the CoNLL-SIGMORPHON 2017 Shared Task on Universal Morphological Reinflection", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.7313701510429382}]}], "abstractContent": [{"text": "We present the LMU system for the CoNLL-SIGMORPHON 2017 shared task on universal morphological reinflection, which consists of several subtasks, all concerned with producing an inflected form of a paradigm in different settings.", "labels": [], "entities": []}, {"text": "Our solution is based on a neural sequence-to-sequence model, extended by prepro-cessing and data augmentation methods.", "labels": [], "entities": []}, {"text": "Additionally, we develop anew algorithm for selecting the most suitable source form in the case of multi-source input, outper-forming the baseline by 5.7% on average overall languages and settings.", "labels": [], "entities": []}, {"text": "Finally , we propose a fine-tuning approach for the multi-source setting, and combine this with the source form detection, increasing accuracy by a further 4.6% on average .", "labels": [], "entities": [{"text": "source form detection", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.6051686604817709}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9994290471076965}]}], "introductionContent": [{"text": "Many of the world's languages have a rich morphology, i.e., make use of surface variations of lemmata in order to express certain properties, like the tense or mood of a verb.", "labels": [], "entities": []}, {"text": "This makes a variety of natural language processing tasks more challenging, as it increases the number of words in a language drastically; a problem morphological analysis and generation help to mitigate.", "labels": [], "entities": []}, {"text": "However, a big issue when developing methods for morphological processing is that for many morphologically rich languages, there are only few or no relevant training data available, making it impossible to train state-of-the-art machine learning models (e.g.,).", "labels": [], "entities": [{"text": "morphological processing", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.7850386202335358}]}, {"text": "This is the motivation for the CoNLL-SIGMORPHON-2017 shared task on universal morphological reinflection (, which animates the development of systems for as many as 52 different languages in 6 different low-resource settings for morphological reinflection: to generate an inflected form, given a target morphological tag and either the lemma (task 1) or a partial paradigm (task 2).", "labels": [], "entities": []}, {"text": "An example is (use, V;3;SG;PRS) \u2192 uses In this paper, we describe the LMU system for the shared task.", "labels": [], "entities": []}, {"text": "Since it depends on the language and the amount of resources available for training which method performs best, our approach consists of a modular system.", "labels": [], "entities": []}, {"text": "For most medium-and high-resource, as well as some low-resource settings, we make use of the state-of-the-art encoderdecoder () network MED (, while extending the training data in several ways.", "labels": [], "entities": []}, {"text": "Whenever the given data are not sufficient, we make use of the baseline system, which can be trained on fewer instances.", "labels": [], "entities": []}, {"text": "While we submit solutions for every language and setting, our main focus is on task 2 of the shared task and the main contributions of this paper correspondingly address a multi-source input setting: (i) We develop CIS (\"choice of important sources\"), a novel algorithm for selecting the most appropriate source form fora target tag from a partially given paradigm, which is based on edit trees.", "labels": [], "entities": []}, {"text": "(ii) We propose to cast the task of multi-source morphological reinflection as a domain adaptation problem.", "labels": [], "entities": []}, {"text": "By finetuning on forms from a partial paradigm, we improve the performance of a neural sequence-tosequence model for most shared task languages.", "labels": [], "entities": []}, {"text": "Our final methods, averaged over languages, outperform the official baseline by 7.0%, 18.5%, and 16.5% for task 1 and 8.7%, 10.1%, and 10.3% for task 2 for the low-, medium-, and highresource settings, respectively.", "labels": [], "entities": []}, {"text": "Furthermore, our submitted sytem-a combination of our methods with the baseline systemsurpasses the baseline's accuracy on test for both tasks as well as all languages and settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9994707703590393}]}, {"text": "Differences in performance are between 8.69% (task 1 low) and 17.94% (task 1 medium).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our submitted system obtained average accuracies of 0.4659 (low), 0.8264 (medium) and 0.947 (high) for task 1, and 0.6776 (low), 0.8202 (medium) and 0.8852 (high) for task 2, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9400907158851624}]}, {"text": "This corresponds to place 5 of 18, 3 of 19 and 7 of 15 for the high-, medium-and lowresource settings of task 1, respectively.", "labels": [], "entities": []}, {"text": "Remarkably, the difference to the best system for the two higher settings is less than 0.01.", "labels": [], "entities": []}, {"text": "Among 3 submissions for task 2, our system comes first.", "labels": [], "entities": []}, {"text": "It beats the baseline by 17.16 (low), 15.54 (medium) and 10.84 (high).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracies for task 1, for BL, MED* and MED* ensembles. Upper part: development languages; lower part: surprise  languages.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9812590479850769}, {"text": "BL", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.8727373480796814}]}, {"text": " Table 3: Average amount of training examples per task and  resource quantity.", "labels": [], "entities": []}, {"text": " Table 4: Accuracies for task 2. All systems are described in the text. Upper part: development languages; lower part: surprise  languages.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9837718605995178}]}]}