{"title": [{"text": "An Automatic Approach for Document-level Topic Model Evaluation", "labels": [], "entities": [{"text": "Document-level Topic Model Evaluation", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7440004572272301}]}], "abstractContent": [{"text": "Topic models jointly learn topics and document-level topic distribution.", "labels": [], "entities": [{"text": "document-level topic distribution", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.5999814768632253}]}, {"text": "Extrin-sic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics.", "labels": [], "entities": []}, {"text": "We demonstrate that there can be large discrepancies between topic-and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading.", "labels": [], "entities": [{"text": "basing model evaluation", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.8840529322624207}]}, {"text": "We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models such as latent Dirichlet allocation () jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation.", "labels": [], "entities": [{"text": "latent Dirichlet allocation", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.5598839521408081}, {"text": "document collection navigation", "start_pos": 268, "end_pos": 298, "type": "TASK", "confidence": 0.7286640504995981}]}, {"text": "One property of LDA-style topic models that has contributed to their popularity is that they are highly configurable, and can be structured to capture a myriad of statistical dependencies, such as between topics (), between documents associated with the same individual, or between documents associated with individuals in different network relations ().", "labels": [], "entities": []}, {"text": "This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model over the same document collections.", "labels": [], "entities": []}, {"text": "Test data perplexity is the obvious solution, but it has been shown to correlate poorly with direct human assessment of topic model quality (, motivating the need for automatic topic model evaluation methods which emulate human assessment.", "labels": [], "entities": []}, {"text": "Research in this vein has focused primarily on evaluating the quality of individual topics and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model ().", "labels": [], "entities": []}, {"text": "We challenge this assumption, and demonstrate that topic model evaluation should operate at both the topic and document levels.", "labels": [], "entities": [{"text": "topic model evaluation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7393020391464233}]}, {"text": "Our primary contributions are as follows: (1) we empirically demonstrate that there can be large discrepancies between topic-and document-level topic model evaluation; we demonstrate that previously-proposed document-level evaluation approaches can be misleading, and propose an alternative evaluation method; and (3) we propose an automatic approach to topic model evaluation based on analysis of document-level topic distributions, which we show to correlate strongly with manual annotations.", "labels": [], "entities": [{"text": "topic model evaluation", "start_pos": 354, "end_pos": 376, "type": "TASK", "confidence": 0.6198755701382955}]}], "datasetContent": [{"text": "We use two document collections for our experiments: APNEWS and the British National Corpus (\"BNC\":).", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.922337532043457}, {"text": "British National Corpus (\"BNC\":)", "start_pos": 68, "end_pos": 100, "type": "DATASET", "confidence": 0.9522670606772105}]}, {"text": "APNEWS is a collection of Associated Press 1 news articles from 2009 to 2016, while BNC is an amalgamation of extracts from different sources such as books, journals, letters, and pamphlets.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9373980164527893}, {"text": "Associated Press 1 news articles", "start_pos": 26, "end_pos": 58, "type": "DATASET", "confidence": 0.9353993892669678}, {"text": "BNC", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.5483310222625732}]}, {"text": "We sample 50K and 15K documents from APNEWS and BNC, respectively, to create two datasets for our experiments.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9177826046943665}, {"text": "BNC", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9170469641685486}]}, {"text": "In terms of preprocessing, we use Stanford CoreNLP () to tokenise words and sentences.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9365391433238983}, {"text": "tokenise words and sentences", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8722147345542908}]}, {"text": "We additionally remove stop words, 2 lower-case all word tokens, filter word types which occur less than 10 times, and exclude the top 0.1% most frequent word types.", "labels": [], "entities": []}, {"text": "Statistics for each of the preprocessed datasets are provided in.", "labels": [], "entities": []}, {"text": "Similarly to, we base our analysis on a representative selection of topic models, each of which we train over APNEWS and BNC to generate 100 topics: \u2022 \u2022 ctm) is an extension of lda that uses a logistic normal prior over topic proportions instead of a Dirichlet prior to model correlations between different topics and reduce overlap in topic content.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.9348074793815613}, {"text": "BNC", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.7979280948638916}]}, {"text": "\u2022 hca) is an extension to LDA to capture word burstiness, based on the observation that there tends to be higher likelihood of generating a word which has already been seen recently.", "labels": [], "entities": []}, {"text": "Word generation is modelled by a Pitman-Yor process).", "labels": [], "entities": [{"text": "Word generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6932824701070786}]}, {"text": "\u2022 ntm) is a neural topic model, where topic-word multinomials are modelled as a look-up layer of words, and topic-document multinomials are modelled as a look-up layer of documents.", "labels": [], "entities": []}, {"text": "The output layer of the network is given by the dot product of the two vectors.", "labels": [], "entities": []}, {"text": "There are 2 variants of ntm: unsupervised and supervised.", "labels": [], "entities": []}, {"text": "We use only the unsupervised variant in our experiments.", "labels": [], "entities": []}, {"text": "\u2022 cluster is a baseline topic model, specifically designed to produce highly coherent topics but \"bland\" topic allocations.", "labels": [], "entities": []}, {"text": "We represent word types in the documents with pre-trained word2vec vectors (Mikolov et al., 2013a,b), pre-trained on Google News, and create word clusters using k-means clustering (k = 100) to generate the topics.", "labels": [], "entities": []}, {"text": "We derive the multinomial distribution for each topic based on the cosine distance to the cluster centroid, and  linear normalisation across all words.", "labels": [], "entities": []}, {"text": "To generate the topic allocation fora given document, we first calculate a document representation based on the mean of the word2vec vectors of its content words.", "labels": [], "entities": []}, {"text": "For each cluster, we represent them by calculating the mean word2vec vectors of its top-10 words.", "labels": [], "entities": []}, {"text": "Given the document vector and clusters/topics, we calculate the similarity of the document to each cluster based on cosine similarity, and finally (linearly) normalise the similarities to generate a probability distribution.", "labels": [], "entities": []}, {"text": "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence.", "labels": [], "entities": []}, {"text": "Although the method is successful in assessing topic quality, it tells us little about the association between documents and topics.", "labels": [], "entities": []}, {"text": "As we will see, a topic model can produce topics that are coherent -in terms of npmi association -but poor descriptor of the overall concepts in the document collection.", "labels": [], "entities": []}, {"text": "We first compute topic coherence for all 5 topic models over APNEWS and BNC using npmi () and present the results in.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.949449896812439}, {"text": "BNC", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.9141572713851929}]}, {"text": "We see that lda and cluster perform consistently well across both datasets.", "labels": [], "entities": []}, {"text": "hca performs well over APNEWS but poorly over BNC.", "labels": [], "entities": [{"text": "hca", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.906882643699646}, {"text": "APNEWS", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.8593976497650146}, {"text": "BNC", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.876535177230835}]}, {"text": "Both ctm and ntm topics appear to have low coherence over the two datasets.", "labels": [], "entities": []}, {"text": "Based on these results, one would conclude that cluster is a good topic model, as it produces very coherent topics.", "labels": [], "entities": []}, {"text": "To better understand the nature and quality of the topics, we present a random sample of lda and cluster topics in.", "labels": [], "entities": []}, {"text": "Looking at the topics, we see that cluster tends to include different inflectional forms of the same word (e.g. prohibited, probihiting) and nearsynonyms/sister words (e.g. river, lake, creeks) in a single topic.", "labels": [], "entities": []}, {"text": "This explains the strong npmi association of the cluster topics.", "labels": [], "entities": []}, {"text": "On the other hand, lda discovers related words that collectively describe concepts rather than just clustering (near) synonyms.", "labels": [], "entities": []}, {"text": "This suggests that the topic coherence metric alone may not completely capture topic model quality, leading us to also investigate the topic distribution associated with documents from our collections.", "labels": [], "entities": []}, {"text": "In this section, we describe a series of manual evaluations of document-level topic allocations, in order to get a more holistic evaluation of the true quality of the different topic models (in line with the original work of Chang et al.).", "labels": [], "entities": []}, {"text": "A limitation of the topic intrusion task is that it requires manual annotation, making it ill-suited for large-scale or automatic evaluation.", "labels": [], "entities": [{"text": "topic intrusion task", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8131585121154785}]}, {"text": "We present the first attempt to automate the prediction of the intruder topic, with the aim of developing an approach to topic model evaluation which comple- The 100 documents used for this task were different to the ones used in Section 5.1.", "labels": [], "entities": [{"text": "topic model evaluation", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.6009825666745504}]}, {"text": "ments topic coherence (as motivated in Sections 4 and 5).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the two document collections  used in our experiments", "labels": [], "entities": []}, {"text": " Table 2: Topic coherence scores (npmi)", "labels": [], "entities": []}, {"text": " Table 4: Mean model precision for human judge- ments", "labels": [], "entities": [{"text": "Mean model precision", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8453719615936279}]}, {"text": " Table 6: Top-1 document-topic rating for each  topic model", "labels": [], "entities": []}]}