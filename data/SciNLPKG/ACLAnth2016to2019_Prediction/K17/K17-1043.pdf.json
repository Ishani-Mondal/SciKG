{"title": [{"text": "Learning from Relatives: Unified Dialectal Arabic Segmentation", "labels": [], "entities": [{"text": "Unified Dialectal Arabic Segmentation", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.6434694677591324}]}], "abstractContent": [{"text": "Arabic dialects do not just share a common koin\u00e9, but there are shared pan-dialectal linguistic phenomena that allow computational models for dialects to learn from each other.", "labels": [], "entities": []}, {"text": "In this paper we build a unified segmentation model where the training data for different dialects are combined and a single model is trained.", "labels": [], "entities": []}, {"text": "The model yields higher accuracies than dialect-specific models, eliminating the need for dialect identification before seg-mentation.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7152391374111176}]}, {"text": "We also measure the degree of relatedness between four major Ara-bic dialects by testing how a segmenta-tion model trained on one dialect performs on the other dialects.", "labels": [], "entities": []}, {"text": "We found that linguistic relatedness is contingent with geographical proximity.", "labels": [], "entities": []}, {"text": "In our experiments we use SVM-based ranking and bi-LSTM-CRF sequence labeling.", "labels": [], "entities": []}], "introductionContent": [{"text": "Segmenting Arabic words into their constituent parts is important fora variety of applications such as machine translation, parsing and information retrieval.", "labels": [], "entities": [{"text": "Segmenting Arabic words", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8772693872451782}, {"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8237946927547455}, {"text": "parsing", "start_pos": 124, "end_pos": 131, "type": "TASK", "confidence": 0.9648187756538391}, {"text": "information retrieval", "start_pos": 136, "end_pos": 157, "type": "TASK", "confidence": 0.8160653710365295}]}, {"text": "Though much work has focused on segmenting Modern Standard Arabic (MSA), recent work began to examine dialectal segmentation in some Arabic dialects.", "labels": [], "entities": [{"text": "segmenting Modern Standard Arabic (MSA)", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.799390401159014}, {"text": "dialectal segmentation", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7219504565000534}]}, {"text": "Dialectal segmentation is becoming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA.", "labels": [], "entities": [{"text": "Dialectal segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8473721742630005}]}, {"text": "Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing.", "labels": [], "entities": [{"text": "word merging", "start_pos": 97, "end_pos": 109, "type": "TASK", "confidence": 0.6992221623659134}, {"text": "letter substitution or deletion", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.770566038787365}, {"text": "foreign word borrowing", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.6303949356079102}]}, {"text": "Existing work on dialectal segmentation focused on building resources and tools for each dialect separately ().", "labels": [], "entities": [{"text": "dialectal segmentation", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8139133453369141}]}, {"text": "The rational for the separation is that different dialects have different affixes, make different lexical choices, and are influenced by different foreign languages.", "labels": [], "entities": []}, {"text": "However, performing reliable dialect identification to properly route text to the appropriate segmenter maybe problematic, because conventional dialectal identification may lead to results that are lower than 90% ( ).", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.6948980838060379}, {"text": "dialectal identification", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.7245673537254333}]}, {"text": "Thus, building a segmenter that performs reliably across multiple dialects without the need for dialect identification is desirable.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7284562587738037}]}, {"text": "In this paper we examine the effectiveness of using a segmenter built for one dialect in segmenting other dialects.", "labels": [], "entities": []}, {"text": "Next, we explore combining training data for different dialects in building a joint segmentation model for all dialects.", "labels": [], "entities": []}, {"text": "We show that the joint segmentation model matches or outperforms dialect-specific segmentation models.", "labels": [], "entities": []}, {"text": "For this work, we use training data in four different dialects, namely Egyptian (EGY), Levantine (LEV), Gulf (GLF), and Maghrebi (MGR).", "labels": [], "entities": []}, {"text": "We utilize two methods for segmentation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.9795843362808228}]}, {"text": "The first poses segmentation as a ranking problem, where we use an SVM ranker.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9819926023483276}]}, {"text": "The second poses the problem as a sequence labeling problem, where we use a bidirectional Long Short-Term Memory (bi-LSTM) Recurrent Neural Network (RNN) that is coupled with Conditional Random Fields (CRF) sequence labeler.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.6891657114028931}]}], "datasetContent": [{"text": "We used datasets for four dialects, namely Egyptian (EGY), Levantine (LEV), Gulf (GLF), and Maghrebi (MGR) which are available at http://alt.qcri.org/resources/da_ resources/.", "labels": [], "entities": []}, {"text": "Each dataset consists of a sets of 350 manually segmented tweets.", "labels": [], "entities": []}, {"text": "Briefly, we obtained a large Arabic collection composed of 175 million Arabic tweets by querying the Twitter API using the query \"lang:ar\" during March 2014.", "labels": [], "entities": []}, {"text": "Then, we identified tweets whose authors identified their location in countries where the dialects of interest are spoken (e.g. Morocco, MADAMIRA release 20160516 2.1 Algeria, Tunisia, and Libya for MGR) using a large location gazetteer ) which maps each region/city to its country.", "labels": [], "entities": [{"text": "MADAMIRA release 20160516 2.1 Algeria", "start_pos": 137, "end_pos": 174, "type": "DATASET", "confidence": 0.8821167588233948}]}, {"text": "Then we filtered the tweets using a list containing 10 strong dialectal words per dialect, such as the MGR word \"kymA\" (like/as in) and the LEV word \"hyk\" (like this).", "labels": [], "entities": [{"text": "MGR", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.7627055048942566}]}, {"text": "Given the filtered tweets, we randomly selected 2,000 unique tweets for each dialect, and we asked a native speaker of each dialect to manually select 350 tweets that are heavily dialectal, i.e. contain more dialectal than MSA words.", "labels": [], "entities": []}, {"text": "lists the number of tweets that we obtained for each dialect and the number of words they contain.", "labels": [], "entities": []}, {"text": "Using the approaches described earlier, we perform several experiments, serving two main objectives.", "labels": [], "entities": []}, {"text": "First we want to see how closely related the dialects are and whether we can use one dialect for the augmentation of training data in another dialect.", "labels": [], "entities": []}, {"text": "The second objective is to find out whether we can build a one-fits-all model that does not need to know which specific dialect it is dealing with.", "labels": [], "entities": []}, {"text": "In the first set of experiments shown in, we build segmentation models for each dialect and tested them on all the other dialects.", "labels": [], "entities": []}, {"text": "We compare these cross dialect training and testing to training and testing on the same dialect, where we use 5 fold cross validation with 70/10/20 train/dev/test splits.", "labels": [], "entities": []}, {"text": "We also use the Farasa MSA segmenter as a baseline.", "labels": [], "entities": [{"text": "Farasa MSA segmenter", "start_pos": 16, "end_pos": 36, "type": "DATASET", "confidence": 0.9177011450131735}]}, {"text": "We conduct the experiments at three levels: pure system output (without lookup), with DA lookup, and with DA+MSA lookup.", "labels": [], "entities": []}, {"text": "We mean by \"lookup\" a post-processing add-on step where we feed segmentation solutions in the test files directly from the training data when a match is found.", "labels": [], "entities": []}, {"text": "This is based on the assumption that segmentation is a context-free problem and therefore the utilization of observed data can be maximized.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9723497629165649}]}, {"text": "Using both algorithms (SVM and LSTM) the results show a general trend where EGY segmentation yields better results from the LEV model than from the GLF's.", "labels": [], "entities": [{"text": "EGY segmentation", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.6770084500312805}]}, {"text": "The GLF data benefits more from the LEV model than from the EGY one.", "labels": [], "entities": [{"text": "GLF data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.801753431558609}, {"text": "EGY", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.892135500907898}]}, {"text": "For the LEV data both GLF and EGY models are equally good.", "labels": [], "entities": [{"text": "LEV data", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.7508280277252197}, {"text": "GLF", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.8970914483070374}, {"text": "EGY", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.8465077877044678}]}, {"text": "MGR seems relatively distant in that it does not contribute to or benefit from other dialects independently.", "labels": [], "entities": [{"text": "MGR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.726406455039978}]}, {"text": "This shows a trend where dialects favor geographical proximity.", "labels": [], "entities": []}, {"text": "In the case with no lookup, LSTM fairs better than SVM when training and testing is done on the same dialect.", "labels": [], "entities": []}, {"text": "However, the opposite is true when training on one dialect and testing on another.", "labels": [], "entities": []}, {"text": "This may indicate that the SVM-ranker has better cross-dialect generalization than the bi-LSTM-CRF sequence labeler.", "labels": [], "entities": []}, {"text": "When lookup is used, SVM yields better results across the board except in three cases, namely when training and testing on Egyptian with DA+MSA lookup, when training with Egyptian and testing on MGR, and when training with GLF and testing on MGR with DA+MSA lookup.", "labels": [], "entities": [{"text": "SVM", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8853950500488281}, {"text": "MGR", "start_pos": 195, "end_pos": 198, "type": "DATASET", "confidence": 0.867790699005127}, {"text": "GLF", "start_pos": 223, "end_pos": 226, "type": "DATASET", "confidence": 0.9011353850364685}, {"text": "MGR", "start_pos": 242, "end_pos": 245, "type": "DATASET", "confidence": 0.917336642742157}]}, {"text": "Lastly, the best SVM crossdialect results with lookup consistently beat the Farasa MSA baseline often by several percentage points for every dialect.", "labels": [], "entities": [{"text": "Farasa MSA baseline", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.9429768919944763}]}, {"text": "The same is true for LSTM when training with relatively related dialects (EGY, LEV, and GLF), but the performance decreases when training or testing using MGR.", "labels": [], "entities": []}, {"text": "In the second set of experiments, we wanted to see whether we can train a unified segmenter that would segment all the dialects in our datasets.", "labels": [], "entities": []}, {"text": "For the results shown in, we also used 5-fold cross validation (with the same splits generated earlier) where we trained on the combined training splits from all dialects and tested on all the test splits with no lookup, DA lookup, and MSA+DA lookup.", "labels": [], "entities": []}, {"text": "We refer to these models as \"joint\" models.", "labels": [], "entities": []}, {"text": "Using SVM, the combined model drops by 0.3% to 1.3% compared to exclusively using matching dialectal training data.", "labels": [], "entities": []}, {"text": "We also conducted another SVM experiment in which we use the joint model in conjunction with a dialect identification oracle to restrict possible affixes only to those that are possible for that dialect (last two row   in).", "labels": [], "entities": []}, {"text": "The results show improvements for all dialects, but aside for EGY, the improvements do not lead to better results than those for single dialect models.", "labels": [], "entities": [{"text": "EGY", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.7597976326942444}]}, {"text": "Conversely, the bi-LSTM-CRF joint model with DA+MSA lookup beats every other experimental setup that we tested, leading to the best segmentation results for all dialects, without doing dialect identification.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.6920870691537857}]}, {"text": "This may indicate that bi-LSTM-CRF benefited from cross-dialect data in improving segmentation for individual dialects.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset size for the different dialects", "labels": [], "entities": []}, {"text": " Table 4: Common words across dialects", "labels": [], "entities": []}, {"text": " Table 7: Cross dialect results.", "labels": [], "entities": []}, {"text": " Table 8: Joint model results.", "labels": [], "entities": []}]}