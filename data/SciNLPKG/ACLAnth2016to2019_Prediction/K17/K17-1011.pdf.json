{"title": [{"text": "Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.8552071849505106}]}], "abstractContent": [{"text": "Pairwise ranking methods are the basis of many widely used discriminative training approaches for structure prediction problems in natural language processing (NLP).", "labels": [], "entities": [{"text": "structure prediction problems in natural language processing (NLP)", "start_pos": 98, "end_pos": 164, "type": "TASK", "confidence": 0.7273319631814956}]}, {"text": "Decomposing the problem of ranking hypotheses into pairwise comparisons enables simple and efficient solutions.", "labels": [], "entities": []}, {"text": "However, neglecting the global ordering of the hypothesis list may hinder learning.", "labels": [], "entities": []}, {"text": "We propose a listwise learning framework for structure prediction problems such as machine translation.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7212721854448318}, {"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7958322167396545}]}, {"text": "Our framework directly models the entire translation list's ordering to learn parameters which may better fit the given listwise samples.", "labels": [], "entities": []}, {"text": "Furthermore , we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions.", "labels": [], "entities": []}, {"text": "Experiments on a large-scale Chinese-English translation task show that both our list-wise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7391054431597391}]}], "introductionContent": [{"text": "Discriminative training methods for structured prediction in natural language processing (NLP) aim to estimate the parameters of a model that assigns a score to each hypothesis in the (possibly very large) search space.", "labels": [], "entities": [{"text": "structured prediction in natural language processing (NLP)", "start_pos": 36, "end_pos": 94, "type": "TASK", "confidence": 0.8122453954484727}]}, {"text": "For example, in statistical machine translation (SMT), the model assigns a score to each possible translation, and in syntactic parsing, the function assigns a score to each possible syntactic tree.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.8065153062343597}, {"text": "syntactic parsing", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7517850399017334}]}, {"text": "Ideally, the model should assign scores that rank hypotheses according to their true quality.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of discriminative training for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9965761303901672}]}, {"text": "Traditional SMT systems use log-linear models with only about a dozen features, such as translation probabilities and language model probabilities ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9914008975028992}]}, {"text": "These models can be tuned by minimum error rate training (MERT), which directly optimizes BLEU using coordinate ascent combined with a global line search.", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 29, "end_pos": 63, "type": "METRIC", "confidence": 0.8295626512595585}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9969839453697205}]}, {"text": "To enable training of modern SMT systems, which can have thousands of features or more, many research efforts have been made towards scalable discriminative training methods ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9948951601982117}]}, {"text": "Most of these methods either define loss functions that push the model to correctly compare pairs of hypotheses, or use approximate optimization methods that effectively do the same.", "labels": [], "entities": []}, {"text": "For practical reasons, only a subset of the pairs are considered; these pairs are selected by either sampling () or heuristic methods (.", "labels": [], "entities": []}, {"text": "But this pairwise approach neglects the global ordering of the list of hypotheses, which may lead to problems trying to learn good parameter values.", "labels": [], "entities": []}, {"text": "Inspired by research in information retrieval (IR) (, we propose to directly model the ordering of the whole translation list, instead of decomposing it into translation pairs.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.8677538990974426}]}, {"text": "Previous research has tried to integrate listwise methods into SMT, but almost all of them focus on the reranking task, which aims to rescore the fixed translation lists generated by a baseline system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9905064702033997}]}, {"text": "They try to either use listwise approaches to training the reranking model ( or replace the pointwise ranking function, i.e. the log-linear model, with a listwise ranking function by introducing listwise features (.", "labels": [], "entities": []}, {"text": "In this paper, we focus on listwise approaches that can learn better discriminative models for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.995316743850708}]}, {"text": "We present a listwise learning framework for tuning translation systems that uses two listwise ranking objectives originally developed for IR, and.", "labels": [], "entities": [{"text": "IR", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.8771325349807739}]}, {"text": "But unlike standard IR problems, structured prediction problems usually have a huge search space, and at each training iteration, the list of search results can vary.", "labels": [], "entities": [{"text": "IR", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9813178181648254}, {"text": "structured prediction", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.681070476770401}]}, {"text": "The usual strategy is to form the union of all lists of search results, but this can lead to a \"patchy\" list that doesn't represent the full search space well.", "labels": [], "entities": []}, {"text": "The listwise approaches always based on the permutation probability distribution over the list.", "labels": [], "entities": []}, {"text": "Modeling the distribution over a \"patchy\" list, whose elements were generated by different parameters will affect listwise approaches' performance.", "labels": [], "entities": []}, {"text": "To address this issue, we design an instance-aggregating method: Instead of treating the data as a fixed-size set of lists that each grow overtime as new translations are added at each iteration, we treat the data as a growing set of lists; each time a sentence is translated, the k-best list of translations is added as anew list.", "labels": [], "entities": []}, {"text": "We also extend standard listwise training by considering the importance of different instances in the list.", "labels": [], "entities": []}, {"text": "Based on the intuition that instances at the top maybe more important for ranking, we propose top-rank enhanced loss functions, which incorporate a position-dependent cost that penalizes errors occurring at the top of the list more strongly.", "labels": [], "entities": []}, {"text": "We conduct large-scale Chinese-to-English translation experiments showing that our top-rank enhanced listwise learning methods significantly outperform other tuning methods with high dimensional feature sets.", "labels": [], "entities": []}, {"text": "Additionally, even with a small basic feature set, our methods still obtain better results than MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.5241798162460327}]}], "datasetContent": [{"text": "We first investigate the effectiveness of our instance aggregating training procedure.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "The table compare training with instance aggregating and k-best merging.", "labels": [], "entities": []}, {"text": "As the result suggested, with the instance aggregating method, the performance improves on both listwise tuning approaches.", "labels": [], "entities": []}, {"text": "For the rest of this paper, we use the instance aggregating as standard setting for listwise tuning approaches.", "labels": [], "entities": []}, {"text": "To verify the performance of our proposed listwise learning framework, we first compare systems with standard listwise losses to the baseline systems.", "labels": [], "entities": []}, {"text": "The first four rows in show the results.", "labels": [], "entities": []}, {"text": "ListNet can outperform PRO by 0.55 BLEU score and 0.26 BLEU score on extended feature set and sparse feature set, respectively.", "labels": [], "entities": [{"text": "ListNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8901463150978088}, {"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9847584068775177}, {"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9833842515945435}]}, {"text": "Its main reason is that our listwise methods can obtain structured order information when we take complete translation list as instance.", "labels": [], "entities": []}, {"text": "We also observe that ListMLE can only get a modest performance compare to ListNet.", "labels": [], "entities": [{"text": "ListMLE", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.8636165261268616}, {"text": "ListNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9621025919914246}]}, {"text": "We think the objective function of standard ListMLE which forces the whole list ranking in a correct order is too hard.", "labels": [], "entities": []}, {"text": "ListNet mainly benefits from its top one permutation probability which only concerns the permutation with the best object ranked first.", "labels": [], "entities": [{"text": "ListNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.866945743560791}]}], "tableCaptions": [{"text": " Table 2: The comparison of instances aggre- gating and k-best merging on the extended fea- ture set.(Net m and MLE m denote ListNet and  ListMLE with k-best merging respectively.)", "labels": [], "entities": [{"text": "fea- ture set.", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.6899749934673309}]}, {"text": " Table 4: Comparison of baselines and listwise ap- proaches with a larger k-best list on extended fea- ture set.", "labels": [], "entities": []}, {"text": " Table 4. With a larger size k, our tuning meth- ods also perform better than baselines. For List-", "labels": [], "entities": []}, {"text": " Table 5: Comparison of baseline and liswise ap- proaches on basic feature set.", "labels": [], "entities": []}]}