{"title": [{"text": "A Joint Model for Semantic Sequences: Frames, Entities, Sentiments", "labels": [], "entities": []}], "abstractContent": [{"text": "Understanding stories-sequences of events-is a crucial yet challenging natural language understanding task.", "labels": [], "entities": [{"text": "Understanding stories-sequences of events-is", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.9035343676805496}, {"text": "natural language understanding", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.6682067016760508}]}, {"text": "These events typically carry multiple aspects of semantics including actions, entities and emotions.", "labels": [], "entities": []}, {"text": "Not only does each individual aspect contribute to the meaning of the story, so does the interaction among these aspects.", "labels": [], "entities": []}, {"text": "Building on this intuition, we propose to jointly model important aspects of semantic knowledge-frames, entities and sentiments-via a semantic language model.", "labels": [], "entities": []}, {"text": "We achieve this by first representing these aspects' semantic units at an appropriate level of abstraction and then using the resulting vector representations for each semantic aspect to learn a joint representation via a neural language model.", "labels": [], "entities": []}, {"text": "We show that the joint semantic language model is of high quality and can generate better semantic sequences than models that operate on the word level.", "labels": [], "entities": []}, {"text": "We further demonstrate that our joint model can be applied to story cloze test and shallow discourse parsing tasks with improved performance and that each semantic aspect contributes to the model.", "labels": [], "entities": [{"text": "shallow discourse parsing tasks", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.6925501301884651}]}], "introductionContent": [{"text": "Understanding a story requires understanding sequences of events.", "labels": [], "entities": []}, {"text": "It is thus vital to model semantic sequences in text.", "labels": [], "entities": []}, {"text": "This modeling process necessitates deep semantic knowledge about what can happen next.", "labels": [], "entities": []}, {"text": "Since events involve actions, participants and emotions, semantic knowledge about these aspects must be captured and modeled.", "labels": [], "entities": []}, {"text": "In Ex.1, we observe a sequence of actions (commit, arrest, charge, try), each corresponding to a predicate Ex.1 (Actions -Frames) Steven Avery committed murder.", "labels": [], "entities": []}, {"text": "He was arrested, charged and tried.", "labels": [], "entities": []}, {"text": "Opt.1 Steven Avery was convicted of murder.", "labels": [], "entities": [{"text": "Opt.1 Steven Avery", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.8292513291041056}]}, {"text": "Opt.2 Steven went to the movies with friends.", "labels": [], "entities": []}, {"text": "Steven was held in jail during his trial.", "labels": [], "entities": []}, {"text": "Ex.2 (Participants -Entities) It was my first time ever playing football and I was so nervous.", "labels": [], "entities": []}, {"text": "During the game, I got tackled and it did not hurt at all!", "labels": [], "entities": []}, {"text": "Opt.1 I then felt more confident playing football.", "labels": [], "entities": []}, {"text": "Opt.2 I realized playing baseball was a lot of fun.", "labels": [], "entities": []}, {"text": "However, I still love baseball more.", "labels": [], "entities": []}, {"text": "Ex.3 (Emotions -Sentiments) Joe wanted to become a professional plumber.", "labels": [], "entities": [{"text": "Emotions -Sentiments)", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7255589365959167}]}, {"text": "So, he applied to a trade school.", "labels": [], "entities": []}, {"text": "Fortunately, he was accepted.", "labels": [], "entities": []}, {"text": "Opt.1 It made Joe very happy.", "labels": [], "entities": [{"text": "Opt.1", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9009097218513489}]}, {"text": "Opt.2 It made Joe very sad.", "labels": [], "entities": []}, {"text": "However, Joe decided not to enroll because he did not have enough money to pay tuition.: Examples of short stories requiring different aspects of semantic knowledge.", "labels": [], "entities": []}, {"text": "For all stories, Opt.1 is the correct follow-up, while Opt.2 is the contrastive wrong follow-up demonstrating the importance of each aspect.", "labels": [], "entities": []}, {"text": "showcases an alternative correct follow-up, which requires considering different aspects of semantics jointly. frame.", "labels": [], "entities": []}, {"text": "Clearly, \"convict\" is more likely than \"go\" to follow such sequence.", "labels": [], "entities": []}, {"text": "This semantic knowledge can be learned through modeling frame sequences observed in a large corpus.", "labels": [], "entities": []}, {"text": "This phenomena has already been studied in script learning works;.", "labels": [], "entities": []}, {"text": "However, modeling actions is not sufficient; participants in actions and their emotions are also important.", "labels": [], "entities": []}, {"text": "2, Opt.2 is not a plausible answer because the story is about \"football\", and it does not make sense to suddenly change the key en-: Comparison of generative ability for different models.", "labels": [], "entities": []}, {"text": "For each model, we provide Ex.1 as context and compare the generated ending.", "labels": [], "entities": []}, {"text": "4-gram and RNNLM models are trained on NYT news data while Seq2Seq model is trained on the story data (details see Sec. 5).", "labels": [], "entities": [{"text": "NYT news data", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9535102844238281}]}, {"text": "These are models operated on the word level.", "labels": [], "entities": []}, {"text": "We compare them with FC-SemLM (, which works on frame abstractions, i.e. \"predicate.sense\".", "labels": [], "entities": []}, {"text": "For the proposed FES-LM, we further assign the arguments (subject and object) of a predicate with NER types (\"PER, LOC, ORG, MISC\") or \"ARG\" if otherwise.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.830481231212616}, {"text": "PER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9785665273666382}, {"text": "ORG", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.7062994241714478}, {"text": "MISC", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.6793429851531982}, {"text": "ARG", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.990202784538269}]}, {"text": "Each argument is also associated with a \"[new/old]\" label indicating if it is first mentioned in the sequence (decided by entity co-reference).", "labels": [], "entities": []}, {"text": "Additionally, the sentiment of a frame is represented as positive (POS), neural (NEU) or negative (NEG).", "labels": [], "entities": []}, {"text": "FES-LM can generate better endings in terms of soundness and specificity.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.46471065282821655}]}, {"text": "The FES-LM ending can be understood as \" convict a person, who has been mentioned before (with an overall negative sentiment)\", which can be instantiated as \"Steven Avery was convicted.\" given current context.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7518710494041443}]}, {"text": "tity to \"baseball\".", "labels": [], "entities": []}, {"text": "In Ex.3, one needs understand that \"being accepted\" typically indicates a positive sentiment and that it applies to \"Joe\".", "labels": [], "entities": []}, {"text": "As importantly, we believe that modeling these semantic aspects should be done jointly; otherwise, it may not convey the complete intended meaning.", "labels": [], "entities": []}, {"text": "Consider the alternative follow-ups in: in Ex.1, the entity \"jail\" gives strong indication that it follows the storyline that mentions \"murder\"; in Ex.2, even though \"football\" is not explicitly mentioned, there is a comparison between \"baseball\" and \"football\" that makes this continuation coherent; in Ex.3, \"decided not to enroll\" is a reasonable action after \"being accepted\", although the general sentiment of the sentence is negative.", "labels": [], "entities": [{"text": "Ex.1", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.9560903906822205}, {"text": "Ex.2", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.9286980032920837}, {"text": "Ex.3", "start_pos": 304, "end_pos": 308, "type": "DATASET", "confidence": 0.9197036623954773}]}, {"text": "These examples show that in order to model semantics in a more complete way, we need to consider interactions between frames, entities and sentiments.", "labels": [], "entities": []}, {"text": "In this paper, we propose a joint semantic language model, FES-LM, for semantic sequences, which captures Frames, Entities and Sentiment information.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.8459800481796265}]}, {"text": "Just as \"standard\" language models built on top of words, we construct FES-LM by building language models on top of joint semantic representations.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.6579998731613159}]}, {"text": "This joint semantic representation is a mixture of representations corresponding to different semantic aspects.", "labels": [], "entities": []}, {"text": "For each aspect, we capture semantics via abstracting over and disambiguating text surface forms, i.e. semantic frames for predicates, entity types for semantic arguments, and sentiment labels for the overall context.", "labels": [], "entities": []}, {"text": "These abstractions provide the basic vocabulary for FES-LM and are essential for capturing the underlying semantics of a story.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 52, "end_pos": 58, "type": "TASK", "confidence": 0.5451078414916992}]}, {"text": "In Table 1, we provide Ex.1 as context input (although FC-SemLM and FES-LM automatically generate a more abstract representation of this input) and examine the ability of different models to generate an ending.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9128885865211487}]}, {"text": "4-gram, RNNLM and Seq2Seq models operate on the word level, and the generated endings are not satisfactory.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.8024159669876099}]}, {"text": "FC-SemLM () works on basic frame abstractions and the proposed FES-LM model adds abstracted entity and sentiment information into frames.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.48213520646095276}]}, {"text": "The results show that FES-LM produces the best ending among all compared models in terms of semantic soundness and specificity.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.8368024230003357}]}, {"text": "We build the joint language model from plain text corpus with automatic annotation tools, requiring no human effort.", "labels": [], "entities": []}, {"text": "In the empirical study, FES-LM is first built on news documents.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.5585305094718933}]}, {"text": "We provide perplexity analysis of different variants of FES-LM as well as for the narrative cloze test, where we test the system's ability to recover a randomly dropped frame.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.48492321372032166}]}, {"text": "We further show that FES-LM improves the performance of sense disambiguation for shallow discourse parsing.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9564869999885559}, {"text": "sense disambiguation", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7103667259216309}, {"text": "shallow discourse parsing", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.6922780871391296}]}, {"text": "We then re-train the model on short commonsense stories (with the model trained on news as initialization).", "labels": [], "entities": []}, {"text": "We perform story cloze test, i.e. given a four-sentence story, choose the fifth sentence from two provided options.", "labels": [], "entities": []}, {"text": "Our joint model achieves the best known results in the unsupervised setting.", "labels": [], "entities": []}, {"text": "In all cases, our ablation study demonstrates that each aspect of FES-LM contributes to the model.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.5341271758079529}]}, {"text": "The main contributions of our work are: 1) the design of a joint neural language model for semantic sequences built from frames, entities and sentiments; 2) showing that FES-LM trained on news is of high quality and can help to improve shallow discourse parsing; 3) achieving the state-of-the-art result on story cloze test in an unsupervised setting with the FES-LM tuned on stories.", "labels": [], "entities": [{"text": "shallow discourse parsing", "start_pos": 236, "end_pos": 261, "type": "TASK", "confidence": 0.7750176588694254}]}], "datasetContent": [{"text": "Dataset We first use the New York Times (NYT) Corpus 4 (from year 1987 to 2007) to train FES-LM.", "labels": [], "entities": [{"text": "New York Times (NYT) Corpus 4", "start_pos": 25, "end_pos": 54, "type": "DATASET", "confidence": 0.7125080302357674}, {"text": "FES-LM", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.7795628309249878}]}, {"text": "It contains over 1.8M documents in total.", "labels": [], "entities": []}, {"text": "To fine tune the model on short stories, we re-train FES-LM on the ROCStories dataset () with the model trained on NYT as initialization.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.8062679767608643}, {"text": "ROCStories dataset", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.9473673701286316}, {"text": "NYT", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9530380964279175}]}, {"text": "We use the train set of ROCStories, which contains around 100K short stories (each consists of five sentences) . Preprocessing We pre-process all documents with Semantic Role Labeling (SRL) () and Part-of-Speech (POS) tagger ().", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 161, "end_pos": 189, "type": "TASK", "confidence": 0.686443954706192}]}, {"text": "We also implement the explicit discourse connective identification module of a shallow discourse parser ().", "labels": [], "entities": [{"text": "discourse connective identification", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.6643430590629578}]}, {"text": "Additionally, we utilize within document entity co-reference () to produce co-reference chains to get the new entity Available at https://catalog.ldc.upenn.edu/LDC2008T19 5 Available at http://cs.rochester.edu/nlp/rocstories/ information.", "labels": [], "entities": []}, {"text": "To obtain all annotations, we employ the Illinois NLP tools 6 .  We first show that our proposed FES-LM is of high quality in terms of language modeling ability.", "labels": [], "entities": [{"text": "Illinois NLP tools", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9610419472058614}, {"text": "FES-LM", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.6672791242599487}]}, {"text": "We then evaluate FES-LM for shallow discourse parsing on news data as well as application for story cloze test on short commonsense stories.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9270352125167847}, {"text": "shallow discourse parsing", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.6311832169691721}]}, {"text": "In all studies, we verify that each semantic aspect contributes to the joint model.: Quality comparison of neural language models.", "labels": [], "entities": []}, {"text": "We report results for perplexity and narrative cloze test.", "labels": [], "entities": []}, {"text": "Both evaluations are done on the gold PropBank data (annotated with gold frames).", "labels": [], "entities": [{"text": "gold PropBank data", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.696070392926534}]}, {"text": "LBL outperforms CBOW and SG on both tests.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.6929028034210205}]}, {"text": "We carryout ablation studies for narrative cloze test for FES-LM without entity and sentiment aspects respectively.", "labels": [], "entities": [{"text": "FES-LM", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.48197293281555176}]}], "tableCaptions": [{"text": " Table 2: Statistics on FES-LM vocabularies and  sequences. We compare FES-LM trained on NYT  vs. ROCStories; \"FES\" stands for unique FES  representations while \"F\" for frame embeddings,  \"E\" for entity representations, and \"S\" for senti- ment representations. \"#seq\" is the number of se- quences, and \"#token\" is the total number of to- kens (FES representations) used for training.", "labels": [], "entities": [{"text": "NYT", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.8794698119163513}, {"text": "FES", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9853185415267944}]}, {"text": " Table 3: Quality comparison of neural language  models. We report results for perplexity and nar- rative cloze test. Both evaluations are done on the  gold PropBank data (annotated with gold frames).  LBL outperforms CBOW and SG on both tests.  We carry out ablation studies for narrative cloze  test for FES-LM without entity and sentiment as- pects respectively.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 157, "end_pos": 170, "type": "DATASET", "confidence": 0.8336625397205353}, {"text": "FES-LM", "start_pos": 306, "end_pos": 312, "type": "DATASET", "confidence": 0.7688566446304321}]}, {"text": " Table 4: Shallow discourse parsing results. With added FES-LM features, we get significant improve- ment (based on McNemar's Test) over the base system(*) and outperform SemLM, which only models  frame information. We also rival the top system", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.6991339325904846}, {"text": "FES-LM", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.963167130947113}, {"text": "improve- ment", "start_pos": 92, "end_pos": 105, "type": "METRIC", "confidence": 0.886477510134379}, {"text": "McNemar's Test", "start_pos": 116, "end_pos": 130, "type": "DATASET", "confidence": 0.8437809149424235}]}, {"text": " Table 5: Accuracy results for story cloze text in  the unsupervised setting. \"S.\" represents the in- ference method with the single most informative  feature while \"M.V.\" means majority voting. FES- LM outperforms the strongest baseline (Seq2Seq  with attention) by 3 points. The difference is  statistically significant based on McNemar's Test.  Additional ablation studies show that each seman- tic aspect contributes to the joint model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9858745336532593}, {"text": "FES- LM", "start_pos": 195, "end_pos": 202, "type": "METRIC", "confidence": 0.9540335337320963}, {"text": "McNemar's Test", "start_pos": 331, "end_pos": 345, "type": "DATASET", "confidence": 0.7814854184786478}]}]}