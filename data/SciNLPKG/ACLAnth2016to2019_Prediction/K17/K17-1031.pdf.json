{"title": [{"text": "Learning Stock Market Sentiment Lexicon and Sentiment-Oriented Word Vector from StockTwits", "labels": [], "entities": [{"text": "StockTwits", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.8127766847610474}]}], "abstractContent": [{"text": "Previous studies have shown that investor sentiment indicators can predict stock market change.", "labels": [], "entities": []}, {"text": "A domain-specific sentiment lexicon and sentiment-oriented word embedding model would help the sentiment analysis in financial domain and stock market.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9279379844665527}]}, {"text": "In this paper, we present anew approach to learning stock market lexicon from StockTwits, a popular financial social network for investors to share ideas.", "labels": [], "entities": []}, {"text": "It learns word polarity by predicting message sentiment, using a neural network.", "labels": [], "entities": [{"text": "word polarity", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6942199170589447}, {"text": "predicting message sentiment", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8584901491800944}]}, {"text": "The sentiment-oriented word embeddings are learned from tens of millions of StockTwits posts, and this is the first study presenting sentiment-oriented word embeddings for stock market.", "labels": [], "entities": []}, {"text": "The experiments of predicting investor sentiment show that our lexicon outperformed other lexicons built by the state-of-the-art methods, and the sentiment-oriented word vector was much better than the general word embeddings.", "labels": [], "entities": [{"text": "predicting investor sentiment", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.8741211692492167}]}], "introductionContent": [{"text": "Social media has provided a rich opinion content that is valuable for diverse decision-making processes (, and sentiment analysis is being increasingly used to predict stock market variables).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7590471506118774}]}, {"text": "In particular, social media messages area useful source for supporting stock market decisions).", "labels": [], "entities": []}, {"text": "Users of social media, such as StockTwits and Twitter, post very frequently, and this makes the real-time assessment possible, which can be exploited during the trading day.", "labels": [], "entities": []}, {"text": "The two important sentiment data that can help sentiment analysis greatly are sentiment lexicons and word embeddings learned from large amount of data.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.9631533920764923}]}, {"text": "Word embedding (word vector) has been used in many NLP tasks and noticeably improved their performance.", "labels": [], "entities": []}, {"text": "However, there has been little effort in constructing sentiment lexicon for financial domain and stock market, and in using social media as the data source.", "labels": [], "entities": []}, {"text": "Many terms in financial market have different meanings, especially sentiment polarity, from that in other domains or sources, such as the general news articles and Twitter.", "labels": [], "entities": []}, {"text": "For example, terms long, short, put and call have special meanings in stock market.", "labels": [], "entities": []}, {"text": "Another example is the term underestimate, which is a negative term in general, but it can suggest an opportunity to buy equities when is used in stock market messages.", "labels": [], "entities": []}, {"text": "Domain independent lexicons or general word embedding model may not perform well in financial domain.", "labels": [], "entities": []}, {"text": "Therefore, it is necessary and important to built sentiment lexicons and word embeddings specifically for stock market.", "labels": [], "entities": []}, {"text": "The automatic lexicon creation approaches in previous studies are mainly based on statistic measures.", "labels": [], "entities": [{"text": "automatic lexicon creation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6818480590979258}]}, {"text": "There are few studies exploiting machine learning models (.", "labels": [], "entities": []}, {"text": "In this study, we propose anew approach that is based on neural network, and our experiment shows that it outperforms the state-ofthe-art methods.", "labels": [], "entities": []}, {"text": "Most word embedding models only consider the syntactic and semantic information of a word, and the sentiment information is not coded in the embeddings.", "labels": [], "entities": []}, {"text": "In this study, we extend the word vector model from) by incorporating the sentiment information into the neural network to learn the embeddings; it captures the sentiment information of sentences as well as the syntactic contexts of words.", "labels": [], "entities": []}, {"text": "The main contributions of this study are: first, we proposed anew approach based on neural network for constructing a large scale sentiment lexicon for stock market.", "labels": [], "entities": []}, {"text": "Second, we built a sentiment-oriented word embedding (SOWE) model specifically for stock market.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first word embedding model for stock market.", "labels": [], "entities": []}, {"text": "The experiment shows that it outperforms the general embedding models.", "labels": [], "entities": []}, {"text": "The lexicons and embeddings are available at https://github.com/quanzhili/stocklexicon.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, we evaluated the lexicons built by these approaches: TF.IDF, PMI, Vo & Zhang, and our proposed approach.", "labels": [], "entities": [{"text": "TF.IDF", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.7060108780860901}, {"text": "PMI", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.8194125294685364}]}, {"text": "The same data set, which consists of 6.4 million labeled StockTwits messages, is used by these four methods.", "labels": [], "entities": []}, {"text": "The messages are preprocessed accordingly for each method.", "labels": [], "entities": []}, {"text": "If the difference between a term's learned positive and negative values is very small, then this term has a neutral sentiment.", "labels": [], "entities": []}, {"text": "If we use 0.10 as the threshold to differentiate neutral terms from positive and negative terms (i.e. terms with |positive-negative| < 0.10 are neutral), our approach generated 42K sentiment words and phrases.", "labels": [], "entities": []}, {"text": "The other three methods have slightly lower amount of sentiment terms.", "labels": [], "entities": []}, {"text": "Sentiment Classification: The lexicons built from these methods can be used in both unsupervised and supervised sentiment classifiers.", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8920443952083588}, {"text": "sentiment classifiers", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.6796850562095642}]}, {"text": "The former is implemented by summing the sentiment scores of all tokens contained in a given message.", "labels": [], "entities": []}, {"text": "If the total sentiment score is larger than 0, then the message is classified as positive.", "labels": [], "entities": []}, {"text": "Here only one positivity attribute is required to represent a lexicon, so for lexicons with both positive and negative values fora term, the value of (positive \u2212 negative) is used as the score.", "labels": [], "entities": []}, {"text": "In this experiment, we used a supervised method for performance evaluation.", "labels": [], "entities": []}, {"text": "There are different ways to generate features fora message using a lexicon.", "labels": [], "entities": []}, {"text": "In this study, we follow the method used in previous studies (.", "labels": [], "entities": []}, {"text": "If a lexicon has both positive and negative values fora term, then a unified score is first computed (i.e. positive -negative), and it is used to generate features described below.", "labels": [], "entities": []}, {"text": "Given a message m, the features are: -The number of sentiment tokens in m, where sentiment tokens are words or phrases whose sentiment scores are not zero in a lexicon.", "labels": [], "entities": []}, {"text": "-The total scores of negative and positive terms.", "labels": [], "entities": []}, {"text": "-The maximal score of all the terms in this message.", "labels": [], "entities": [{"text": "maximal score", "start_pos": 5, "end_pos": 18, "type": "METRIC", "confidence": 0.9772524237632751}]}, {"text": "-The total sentiment score of the message.", "labels": [], "entities": []}, {"text": "-The sentiment score of the last term in m.", "labels": [], "entities": [{"text": "sentiment score", "start_pos": 5, "end_pos": 20, "type": "METRIC", "confidence": 0.8272169232368469}]}, {"text": "Data Set: we selected 30K messages that were already labeled as bullish or bearish from StockTwits's 2016 data set.", "labels": [], "entities": [{"text": "StockTwits's 2016 data set", "start_pos": 88, "end_pos": 114, "type": "DATASET", "confidence": 0.9715452790260315}]}, {"text": "They were not included in the data set used for constructing the lexicons.", "labels": [], "entities": []}, {"text": "The amounts of bullish and bearish messages in the data set are roughly about 70% vs. 30%.", "labels": [], "entities": []}, {"text": "We split this data set into three parts: 70% as training data, 10% as validation data and 20% for testing.", "labels": [], "entities": []}, {"text": "Classifier and Performance Metrics: we tried several classifiers, such as LibLiner, logistic regression and SMO.", "labels": [], "entities": []}, {"text": "SMO gave the best results for most cases, and so we used it to compare the four lexicons.", "labels": [], "entities": [{"text": "SMO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8515084385871887}]}, {"text": "SMO is a sequential minimal optimization algorithm for training a support vector classifier.", "labels": [], "entities": []}, {"text": "The F1 measure and accuracy are used as the performance metrics, which have been used by many previous studies.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9827398955821991}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9991040825843811}]}, {"text": "Result: presents the results.", "labels": [], "entities": [{"text": "Result", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9621225595474243}]}, {"text": "It shows that the two methods based on neural network performed better than the two statistic measures.", "labels": [], "entities": []}, {"text": "PMI outperformed TF.IDF, which is also demonstrated by other studies ().", "labels": [], "entities": [{"text": "TF.IDF", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.8025271892547607}]}, {"text": "For the two models using neural networks, our proposed model outperformed the Vo & Zhang model, and the result was statistically significant at p=0.01 using t-test.", "labels": [], "entities": []}, {"text": "This result also shows that learning lexicon by predicting the accuracy of message is better than the approaches using statistic measures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9928931593894958}]}, {"text": "In this experiment, we evaluated the SOWE embeddings, which encode both the syntactic and sentiment information and are generated specifically for stock market.", "labels": [], "entities": []}, {"text": "We also use sentiment classification task to do the evaluation.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8914724588394165}]}, {"text": "We compare SOWE to only embedding models, not lexicons.", "labels": [], "entities": []}, {"text": "The reason is that they are the same type of data, and so we can use the same feature setting for them, and the experiment setting would not affect the performance comparison result.", "labels": [], "entities": []}, {"text": "We didn't compare the SOWE to the lexicons, because they are different types of data and we need to use different approaches to generate features for them, and this will inevitably affect their performance, and make an unfair comparison.", "labels": [], "entities": []}, {"text": "We leave this type of comparison for future research.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of StockTwits data set. Sentiment labels (bullish or bearish) are provided by mes- sage authors.", "labels": [], "entities": [{"text": "StockTwits data set", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.967600405216217}]}, {"text": " Table 2: Metadata of word embedding models", "labels": [], "entities": []}]}