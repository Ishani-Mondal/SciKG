{"title": [{"text": "Neural Sequence-to-sequence Learning of Internal Word Structure", "labels": [], "entities": [{"text": "Neural Sequence-to-sequence Learning of Internal Word Structure", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.8475676434380668}]}], "abstractContent": [{"text": "Learning internal word structure has recently been recognized as an important step in various multilingual processing tasks and in theoretical language comparison.", "labels": [], "entities": [{"text": "theoretical language comparison", "start_pos": 131, "end_pos": 162, "type": "TASK", "confidence": 0.7113487919171652}]}, {"text": "In this paper, we present a neural encoder-decoder model for learning canonical morphological segmenta-tion.", "labels": [], "entities": []}, {"text": "Our model combines character-level sequence-to-sequence transformation with a language model over canonical segments.", "labels": [], "entities": [{"text": "character-level sequence-to-sequence transformation", "start_pos": 19, "end_pos": 70, "type": "TASK", "confidence": 0.6079106430212656}]}, {"text": "We obtain up to 4% improvement over a strong character-level encoder-decoder baseline for three languages.", "labels": [], "entities": []}, {"text": "Our model outperforms the previous state-of-the-art for two languages, while eliminating the need for external resources such as large dictionaries.", "labels": [], "entities": []}, {"text": "Finally, by comparing the performance of encoder-decoder and classical statistical machine translation systems trained with and without corpus counts, we show that including corpus counts is beneficial to both approaches.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.7054427663485209}]}], "introductionContent": [{"text": "One of the most obvious structural differences between languages is the variation in the complexity of internal word structure.", "labels": [], "entities": []}, {"text": "In some languages, such as English, words are relatively short and morphologically less complex.", "labels": [], "entities": []}, {"text": "In other languages, such as Chintang in Example 1 1 , words tend to belong and encapsulate rather rich structure.", "labels": [], "entities": []}, {"text": "The Chintang verb thaptakha in Example 1 consists of a number of morphemes expressing the imperative mode, aspect and deixis.", "labels": [], "entities": []}, {"text": "The information expressed by a single Chintang verb requires several The example is adapted from ( words in English, as it can be seen in the glosses. and in the translation.", "labels": [], "entities": []}, {"text": "Example 1 a. cuwa thaptakha b. cuwa thapt -a -khag -a c. water move -IMP -see -IMP across [2sS] d.", "labels": [], "entities": []}, {"text": "Bring some water over here!", "labels": [], "entities": []}, {"text": "The variation in word structure is observed even in common categories such as plural, which is typically part of a word, but expressed using different structures.", "labels": [], "entities": []}, {"text": "The items in Example 2 show three different structural types associated with expressing plural across languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first give a description of the corpora that we employ for the experiments.", "labels": [], "entities": []}, {"text": "Then, we discuss the experimental setup for our model.", "labels": [], "entities": []}, {"text": "Finally, we discuss the different configurations of the corpus we employ to explore encoder-decoder model behavior with and without corpus frequencies.", "labels": [], "entities": []}, {"text": "Baseline and comparison As a baseline, we use the basic component of our model (cED), an ensemble of 5 character-level attention encoderdecoder models with the hyperparameters described below.", "labels": [], "entities": []}, {"text": "We also compare the encoder-decoder model to the character-level statistical machine translation (cSMT).", "labels": [], "entities": [{"text": "character-level statistical machine translation", "start_pos": 49, "end_pos": 96, "type": "TASK", "confidence": 0.5877235680818558}]}, {"text": "This approach is a natural choice in machine translation with small training sets, but no results have been reported so far for the task of canonical segmentation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7146329134702682}]}, {"text": "We used the Moses toolkit with the following settings: distortion is disallowed and build-in MERT optimization is used to optimize the translation model and language model.", "labels": [], "entities": [{"text": "distortion", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9756559729576111}, {"text": "MERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.8083215951919556}]}, {"text": "As a reference, we compare our results to the state-of-the-art neural re-ranker model of Note, however, that the results cannot be directly compared since use extra training material in the form of external dictionaries.", "labels": [], "entities": []}, {"text": "Evaluation Since our method is intended to be used for processing corpora, the evaluation is performed at the level of word tokens using accuracy of the full segmentation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9984903335571289}]}, {"text": "In addition, we evaluate the performance on subsets of test words.", "labels": [], "entities": []}, {"text": "Besides the seen words, we distinguish between two kinds of test words that are not seen in the training corpus: a) new combinations of morphemes already seen during training and b) words that contain unseen morphemes.", "labels": [], "entities": []}, {"text": "Hyperparameters The bidirectional encoder consists of a forward and a backward recurrent RNN each having 100 hidden units.", "labels": [], "entities": []}, {"text": "The decoder also has 100 hidden units.", "labels": [], "entities": []}, {"text": "The dimensionality of the character embedding is 300.", "labels": [], "entities": []}, {"text": "We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta to train each model.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.7504898210366567}]}, {"text": "Each update direction is computed using a minibatch of 20 training examples.", "labels": [], "entities": []}, {"text": "At decoding, we use abeam search with abeam size of 12 to find the segmentation that approximately maximizes the conditional probability.", "labels": [], "entities": []}, {"text": "All the described hyperparameters are the same as in the work of.", "labels": [], "entities": []}, {"text": "Initialization of all weights (encoder, decoder, embeddings) to the identity matrix and the biases to zero () results in a very fast convergence rate compared to other initializations.", "labels": [], "entities": []}, {"text": "We train a single model for 20 epochs with an early stopping based on the development set performance.", "labels": [], "entities": [{"text": "stopping", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8057434558868408}]}, {"text": "We also shuffle the training data between the epochs.", "labels": [], "entities": []}, {"text": "Finally, we use an ensemble of five encoderdecoder models with different random initializations.", "labels": [], "entities": []}, {"text": "We shuffle the training data for each of the model using different seed value.", "labels": [], "entities": []}, {"text": "The ensemble model is based on a combined score from all 5 models and is used to guide the decoding process.", "labels": [], "entities": []}, {"text": "Following earlier experiments, we use morpheme 3-gram language model and apply KneserNey smoothing.", "labels": [], "entities": []}, {"text": "As the objective for the MERT weight optimization we use accuracy on the development set.", "labels": [], "entities": [{"text": "MERT weight optimization", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.76188858350118}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9993898868560791}]}, {"text": "Tokens vs Types The Chintang corpus allows us to assess the influence of the corpus counts on the performance of the models used for canonical segmentation.", "labels": [], "entities": [{"text": "Chintang corpus", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.8055357038974762}]}, {"text": "In these experiments, we run only the two baseline models, cSMT and cED (without our language model component), in order to evaluate directly the relevance of such corpus signal to these two training paradigms.", "labels": [], "entities": []}, {"text": "We run each model, cSMT and cED, in two regimes.", "labels": [], "entities": []}, {"text": "In the first, type regime, we train the models using a parallel corpus which consists of word types, i.e. unique pairs of surface form and its canonical segmentation.", "labels": [], "entities": []}, {"text": "The size of such type corpus is around 21,000 word forms.", "labels": [], "entities": []}, {"text": "In the second regime, we train the models using a parallel corpus where each pair of surface form and its canonical segmentation appears as many time as the corresponding word appears in the corpus.", "labels": [], "entities": []}, {"text": "The size of the token-based corpus is then 100,000 tokens.", "labels": [], "entities": []}, {"text": "In the type regime, the amount of training examples is substantially smaller than in the token regime.", "labels": [], "entities": []}, {"text": "In order to make the comparison between the token regime and the type regime more fair in terms of the amount of training and testing data, we add one more experiment.", "labels": [], "entities": []}, {"text": "Specifically, we train the cED model in the type regime using the same number of iterations as in the token regime: 100,000 iterations.", "labels": [], "entities": []}, {"text": "In this way, each word type is seen multiple times both in the type and the token regime.", "labels": [], "entities": []}, {"text": "The difference is that all the types are equally frequent in the type regime, while we observe their natural text frequency in the token regime.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on the task of canonical segmentation for English, German and Indonesian. Type- based regime. cED+LM -character based encoder-decoder model fused with morpheme based language  model. Baseline models: cED -character based encoder-decoder model, cSMT -character based statis- tical machine translation model. For reference only: Joint* -model of", "labels": [], "entities": [{"text": "statis- tical machine translation", "start_pos": 288, "end_pos": 321, "type": "TASK", "confidence": 0.6741836190223693}]}, {"text": " Table 2: Performance on the task of canonical segmentation for Chintang. Type-based vs token-based  training regime. cED -character based encoder-decoder model, cSMT -character based statistical ma- chine translation model. Comparative setting for cED: training in types regime for the same number of  iterations as in the individual setting of token regime.", "labels": [], "entities": [{"text": "statistical ma- chine translation", "start_pos": 184, "end_pos": 217, "type": "TASK", "confidence": 0.6883715867996216}]}]}