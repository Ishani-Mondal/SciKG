{"title": [{"text": "Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring", "labels": [], "entities": [{"text": "Automatic Essay Scoring", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.7276500463485718}]}], "abstractContent": [{"text": "Neural network models have recently been applied to the task of automatic essay scoring , giving promising results.", "labels": [], "entities": [{"text": "automatic essay scoring", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.5925153493881226}]}, {"text": "Existing work used recurrent neural networks and convolutional neural networks to model input essays, giving grades based on a single vector representation of the essay.", "labels": [], "entities": []}, {"text": "On the other hand, the relative advantages of RNNs and CNNs have not been compared.", "labels": [], "entities": []}, {"text": "In addition, different parts of the essay can contribute differently for scoring, which is not captured by existing models.", "labels": [], "entities": []}, {"text": "We address these issues by building a hierarchical sentence-document model to represent essays, using the attention mechanism to automatically decide the relative weights of words and sentences.", "labels": [], "entities": []}, {"text": "Results show that our model outperforms the previous state-of-the-art methods, demonstrating the effectiveness of the attention mechanism.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic essay scoring (AES) is the task of automatically assigning grades to student essays.", "labels": [], "entities": [{"text": "Automatic essay scoring (AES)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6822518408298492}]}, {"text": "It can be highly challenging, requiring not only knowledge on spelling and grammars, but also on semantics, discourse and pragmatics.", "labels": [], "entities": []}, {"text": "Traditional models use sparse features such as bagof-words, part-of-speech tags, grammar complexity measures, word error rates and essay lengths, which can suffer from the drawbacks of timeconsuming feature engineering and data sparsity.", "labels": [], "entities": []}, {"text": "Recently, neural network models have been used for AES, giving better results compared to statistical models with handcrafted features.", "labels": [], "entities": [{"text": "AES", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9536007046699524}]}, {"text": "In particular, distributed word representations are used for the input, and * Corresponding author.", "labels": [], "entities": []}, {"text": "a neural network model is employed to combine word information, resulting in a single dense vector form of the whole essay.", "labels": [], "entities": []}, {"text": "A score is given based on a non-linear neural layer on the representation.", "labels": [], "entities": []}, {"text": "Without handcrafted features, neural network models have been shown to be more robust than statistical models across different domains.", "labels": [], "entities": []}, {"text": "Both recurrent neural networks and convolutional neural networks () have been used for modelling input essays.", "labels": [], "entities": []}, {"text": "In particular, and use a single-layer LSTM) over the word sequence to model the essay, and use a two-level hierarchical CNN structure to model sentences and documents separately.", "labels": [], "entities": []}, {"text": "It has been commonly understood that CNNs can capture local ngram information effectively, while LSTMs are strong in modelling long history.", "labels": [], "entities": []}, {"text": "No previous work has compared the effectiveness of LSTMs and CNNs under the same settings for AES.", "labels": [], "entities": []}, {"text": "To better understand the contrast, we adopt the two-layer structure of, comparing CNNs and LSTMs for modelling sentences and documents.", "labels": [], "entities": []}, {"text": "Not all sentences contribute equally to the scoring of a given essay, and not all words contribute equally within a sentence.", "labels": [], "entities": []}, {"text": "We adopt the neural attention model () to automatically calculate weights for convolution features of CNNs and hidden state values of LSTMs, which has been used for obtaining the most pertinent information for machine translation ( , sentiment analysis () and other tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 210, "end_pos": 229, "type": "TASK", "confidence": 0.8428079187870026}, {"text": "sentiment analysis", "start_pos": 234, "end_pos": 252, "type": "TASK", "confidence": 0.9305867552757263}]}, {"text": "In our case, the attention mechanism can intuitively select sentences and grams that are more aligned with the props or obviously incorrect.", "labels": [], "entities": []}, {"text": "To our knowledge, no prior work has investigated the effectiveness of attention models for AES.", "labels": [], "entities": [{"text": "AES", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9470803737640381}]}, {"text": "Results show that CNN is relatively more effective for modelling sentences, and LSTMs are relatively more effective for modelling documents.", "labels": [], "entities": []}, {"text": "This is likely because local ngram information are more relevant to the scoring of sentence structures, and global information is more relevant for scoring document level coherence.", "labels": [], "entities": []}, {"text": "In addition, attention gives significantly more accurate results.", "labels": [], "entities": []}, {"text": "Our final model achieves the best result reported on the ASAP 1 test set.", "labels": [], "entities": [{"text": "ASAP 1 test set", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8305213749408722}]}, {"text": "We release our code at https: //github.com/feidong1991/aes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Many measurement metrics have be adopted to assess the quality of AES systems, including Pearson's correlation, Spearman's ranking correlation, Kendall's Tau and kappa, especially quadratic weighted kappa (QWK).", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 89, "end_pos": 110, "type": "METRIC", "confidence": 0.8247701128323873}]}, {"text": "We follow the Automated Student Assessment Prize (ASAP) competition official criteria which takes QWK as evaluation metric, which is also adopted as evaluation metric in.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP) competition official criteria", "start_pos": 14, "end_pos": 85, "type": "DATASET", "confidence": 0.5427584201097488}, {"text": "QWK", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.792803943157196}]}, {"text": "Kappa measures inter-raters agreement on the qualitive items, here inter-raters refer to AES system and human rater.", "labels": [], "entities": [{"text": "AES", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.7202534079551697}]}, {"text": "QWK is modified from kappa which takes quadratic weights.", "labels": [], "entities": [{"text": "QWK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7897830009460449}]}, {"text": "The quadratic weight matrix in QWK is defined as: where i and j are the reference rating (assigned by a human rater) and the system rating (assigned by an AES system), respectively, and R is the number of possible ratings.", "labels": [], "entities": [{"text": "QWK", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8303927779197693}]}, {"text": "An observed score matrix O is calculated such that O i,j refers to the number of essays that receive a rating i by the human rater and a rating j by the AES system.", "labels": [], "entities": [{"text": "observed score matrix O", "start_pos": 3, "end_pos": 26, "type": "METRIC", "confidence": 0.6063182204961777}, {"text": "O", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9715994596481323}, {"text": "AES", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.49520614743232727}]}, {"text": "An expected score matrix E is calculated as the outer product of histogram vectors of the two (reference and system) ratings.", "labels": [], "entities": [{"text": "expected score matrix E", "start_pos": 3, "end_pos": 26, "type": "METRIC", "confidence": 0.8040352165699005}]}, {"text": "The matrix E needs to be normalized such that the sum of elements in E and the sum of elements in O keep the same.", "labels": [], "entities": []}, {"text": "Finally, given the three matrices W , O and E, the QWK value is calculated according to Equation 3: We evaluate our model using QWK as the metric, and perform one-tailed t-test to determine the significance of improvements.", "labels": [], "entities": [{"text": "QWK", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9742159247398376}]}], "tableCaptions": [{"text": " Table 2: Statistics of the ASAP dataset; Range  refers to score range and Med. refers to median  scores. For genre, ARG specifies argumentative  essays, RES means response essays and NAR de- notes narrative essays.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.8568448722362518}, {"text": "Med.", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9696637988090515}, {"text": "RES", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9864150881767273}]}, {"text": " Table 3: Comparison of quadratic weighted kappa  between different models on the test data.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of quadratic weight kappa  using different features on the test data.", "labels": [], "entities": []}, {"text": " Table 7: Attention weights of sentences coming  from one student essay in Prompt 4 (The darkness  of blue indicates the relative magnitude of atten- tion weights.", "labels": [], "entities": [{"text": "Attention weights", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9524532556533813}]}]}