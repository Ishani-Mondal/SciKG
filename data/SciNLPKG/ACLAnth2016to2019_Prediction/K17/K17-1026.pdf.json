{"title": [{"text": "A Probabilistic Generative Grammar for Semantic Parsing", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7183087468147278}]}], "abstractContent": [{"text": "We present a generative model of natural language sentences and demonstrate its application to semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7418035864830017}]}, {"text": "In the generative process, a logical form sampled from a prior, and conditioned on this logical form, a grammar probabilistically generates the output sentence.", "labels": [], "entities": []}, {"text": "Grammar induction using MCMC is applied to learn the grammar given a set of labeled sentences with corresponding logical forms.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7543079257011414}]}, {"text": "We develop a semantic parser that finds the logical form with the highest posterior probability exactly.", "labels": [], "entities": []}, {"text": "We obtain strong results on the GeoQuery dataset and achieve state-of-the-art F1 on Jobs.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.9536111354827881}, {"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9991012811660767}]}], "introductionContent": [{"text": "Accurate and efficient semantic parsing is a long-standing goal in natural language processing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7448690533638}]}, {"text": "Existing approaches are quite successful in particular domains.", "labels": [], "entities": []}, {"text": "However, they are largely domain-specific, relying on additional supervision such as a lexicon that provides the semantics or the type of each token in a set (, or a set of initial synchronous context-free grammar rules.", "labels": [], "entities": []}, {"text": "To apply the above systems to anew domain, additional supervision is necessary.", "labels": [], "entities": []}, {"text": "When beginning to read text from anew domain, humans do not need to re-learn basic English gram- our grammar is applied in this paper.", "labels": [], "entities": []}, {"text": "The dark arrows outline the generative process.", "labels": [], "entities": [{"text": "generative process", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.920033723115921}]}, {"text": "During parsing, the input is the observed sentence, and we wish to find the most probable logical form and derivation given the training data under the semantic prior. mar.", "labels": [], "entities": []}, {"text": "Rather, they may encounter novel terminology.", "labels": [], "entities": []}, {"text": "With this in mind, our approach is akin to that of ( where we provide domain-independent supervision to train a semantic parser on anew domain.", "labels": [], "entities": []}, {"text": "More specifically, we restrict the rules that maybe learned during training to a set that characterizes the general syntax of English.", "labels": [], "entities": []}, {"text": "While we do not explicitly present and evaluate an open-domain semantic parser, we hope our work provides a step in that direction.", "labels": [], "entities": []}, {"text": "Knowledge plays a critical role in natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6639524896939596}]}, {"text": "Even seemingly trivial sentences may have a large number of ambiguous interpretations.", "labels": [], "entities": []}, {"text": "Consider the sentence \"Ada started the machine with the GPU,\" for example.", "labels": [], "entities": []}, {"text": "Without additional knowledge, such as the fact that \"machine\" can refer to computing devices that contain GPUs, or that computers generally contain devices such as GPUs, the reader cannot determine whether the GPU is part of the machine or if the GPU is a device that is used to start machines.", "labels": [], "entities": []}, {"text": "Context is highly instrumental to quickly and unambiguously understand sentences.", "labels": [], "entities": []}, {"text": "In contrast to most semantic parsers, which are built on discriminative models, our model is fully generative: To generate a sentence, the logical form is first drawn from a prior.", "labels": [], "entities": []}, {"text": "A grammar then recursively constructs a derivation tree top-down, probabilistically selecting production rules from distributions that depend on the logical form (see fora high-level schematic diagram).", "labels": [], "entities": []}, {"text": "The semantic prior distribution provides a straightforward way to incorporate background knowledge, such as information about the types of entities and predicates, or the context of the utterance.", "labels": [], "entities": []}, {"text": "Additionally, our generative model presents a promising direction to jointly learn to understand and generate natural language.", "labels": [], "entities": []}, {"text": "This article describes the following contributions: \u2022 In Section 2, we present our grammar formalism in its general form.", "labels": [], "entities": []}, {"text": "\u2022 Section 2.2 discusses aspects of the model in its application to the later experiments.", "labels": [], "entities": []}, {"text": "\u2022 In Section 3, we present a method to perform grammar induction in this model.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.8116013407707214}]}, {"text": "Given a set of observed sentences and their corresponding logical forms, we apply Markov chain Monte Carlo (MCMC) to infer the posterior distributions of the production rules in the grammar.", "labels": [], "entities": [{"text": "Markov chain Monte Carlo (MCMC)", "start_pos": 82, "end_pos": 113, "type": "METRIC", "confidence": 0.7397402482373374}]}, {"text": "\u2022 Given a trained grammar, we also develop a method to perform parsing in Section 4: to find the k-best logical forms fora given sentence, leveraging the semantic prior to guide its search.", "labels": [], "entities": []}, {"text": "\u2022 Using the GeoQuery and Jobs datasets, we demonstrate in Section 6 that this framework can be applied to create natural language interfaces for semantic formalisms as complex as Datalog/lambda calculus, which contain variables, scope ambiguity, and superlatives.", "labels": [], "entities": [{"text": "GeoQuery and Jobs datasets", "start_pos": 12, "end_pos": 38, "type": "DATASET", "confidence": 0.6930148527026176}]}, {"text": "All code and datasets are available at github.", "labels": [], "entities": []}, {"text": "com/asaparov/parser.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}