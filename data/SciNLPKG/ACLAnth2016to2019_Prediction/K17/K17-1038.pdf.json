{"title": [], "abstractContent": [{"text": "Extending semantic parsing systems to new domains and languages is a highly expensive , time-consuming process, so making effective use of existing resources is critical.", "labels": [], "entities": [{"text": "Extending semantic parsing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.652518630027771}]}, {"text": "In this paper, we describe a transfer learning method using crosslin-gual word embeddings in a sequence-to-sequence model.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9312990605831146}]}, {"text": "On the NLmaps corpus, our approach achieves state-of-the-art accuracy of 85.7% for English.", "labels": [], "entities": [{"text": "NLmaps corpus", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9355991184711456}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9955347776412964}]}, {"text": "Most importantly , we observed a consistent improvement for German compared with several baseline domain adaptation techniques.", "labels": [], "entities": []}, {"text": "As a by-product of this approach, our models that are trained on a combination of English and German utterances perform reasonably well on code-switching utterances which contain a mixture of English and German, even though the training data does not contain any code-switching.", "labels": [], "entities": []}, {"text": "As far as we know, this is the first study of code-switching in semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.746810793876648}]}, {"text": "We manually constructed the set of code-switching test utterances for the NLmaps corpus and achieve 78.3% accuracy on this dataset.", "labels": [], "entities": [{"text": "NLmaps corpus", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9511175751686096}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9980447292327881}]}], "introductionContent": [{"text": "Semantic parsing is the task of mapping a natural language query to a logical form (LF) such as Prolog or lambda calculus, which can be executed directly through database query.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8264914751052856}, {"text": "Prolog", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.924079418182373}]}, {"text": "Semantic parsing needs application or domain specific training data, so the most straightforward approach is to manufacture training data for each combination of language and application domain.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8180741965770721}]}, {"text": "However, acquiring such data is an expensive, lengthy process.", "labels": [], "entities": []}, {"text": "This paper investigates ways of leveraging application domain specific training data in one language to improve performance and reduce the training data needs for the same application domain in another language.", "labels": [], "entities": []}, {"text": "This is an increasingly common commercially important scenario, where a single application must be developed for multiple languages simultaneously.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the question of transferring a semantic parser from a source language (e.g. English) to a target language (e.g. German).", "labels": [], "entities": [{"text": "transferring a semantic parser from a source language (e.g. English)", "start_pos": 46, "end_pos": 114, "type": "TASK", "confidence": 0.7608394225438436}]}, {"text": "In particular, we examine the situation where there is a large amount of training data for the source language but much less training data for the target language.", "labels": [], "entities": []}, {"text": "It is important to note that, despite surface language differences, it has long been suggested that logical forms are the same across languages, motivating transfer learning for this paper.", "labels": [], "entities": []}, {"text": "We conceptualize our work as a form of domain adaptation, where we transfer knowledge about a specific application domain (e.g. navigation queries) from one language to another.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7741973996162415}]}, {"text": "Much work has investigated feature-based domain adaptation.", "labels": [], "entities": [{"text": "feature-based domain adaptation", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.6284859677155813}]}, {"text": "However, it is a non-trivial research question to apply these methods to deep learning.", "labels": [], "entities": []}, {"text": "We experiment with several deep learning methods for supervised crosslingual domain adaptation and make two key findings.", "labels": [], "entities": [{"text": "crosslingual domain adaptation", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.6562511722246805}]}, {"text": "The first is that we can use a bilingual dictionary to build crosslingual word embeddings, serving as abridge between source and target language.", "labels": [], "entities": []}, {"text": "The second is that machine-translated training data can also be used to effectively improve performance when there is little application domain specific training data in the target language.", "labels": [], "entities": []}, {"text": "Interestingly, even when training on the full dataset of the target language, we show that it is still useful to lever-age information from the source language through crosslingual word embeddings.", "labels": [], "entities": []}, {"text": "We set new stateof-the-art results on the NLmaps corpus.", "labels": [], "entities": [{"text": "NLmaps corpus", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8973097801208496}]}, {"text": "Another benefit of joint training of the model is that a single model has the capacity to understand both languages.", "labels": [], "entities": []}, {"text": "We show this gives the model the ability to parse code-switching utterances, where the natural language query contains a mixture of two languages.", "labels": [], "entities": [{"text": "parse code-switching utterances", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.8687093655268351}]}, {"text": "Being able to handle code-switching is valuable in real-world applications that expect spoken natural language input in a variety of settings and from a variety of speakers.", "labels": [], "entities": []}, {"text": "Many people around the world are bilingual or multilingual, and even monolingual speakers are liable to use foreign expressions or phrases.", "labels": [], "entities": []}, {"text": "Real systems must be able to handle that kind of input, and the method we propose is a simple and efficient way to extend the capabilities of an existing system.", "labels": [], "entities": []}, {"text": "As far as we know, this is the first study of codeswitching in semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.756358414888382}]}, {"text": "We constructed anew set of code-switching test utterances for the NLmaps corpus.", "labels": [], "entities": [{"text": "NLmaps corpus", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9072689414024353}]}, {"text": "Our jointly trained model obtains a logical form exact match accuracy of 78.3% on this test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.717800498008728}]}, {"text": "Our contributions are: \u2022 We achieve new state-of-the-art results on the English and German versions of the NLmaps corpus (85.7% and 83.0% respectively).", "labels": [], "entities": [{"text": "NLmaps corpus", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.8616849184036255}]}, {"text": "\u2022 We propose a method to incorporate bilingual word embeddings into a sequence-tosequence model, and apply it to semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.7035310715436935}]}, {"text": "To the best of our knowledge, we are the first to apply crosslingual word embedding in a sequence-to-sequence model.", "labels": [], "entities": []}, {"text": "\u2022 Our joint model allows us to also process input with code-switching.", "labels": [], "entities": []}, {"text": "We develop anew dataset for evaluating semantic parsing on code-switching input which we make publicly available.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7275332063436508}]}], "datasetContent": [{"text": "In this section, we evaluate the methods proposed in \u00a73 for transfer learning for semantic parsing.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.9306923449039459}, {"text": "semantic parsing", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.8542827367782593}]}, {"text": "The aim is to build a parser fora target language with minimum supervision given application domain specific training data fora source language.", "labels": [], "entities": []}, {"text": "The question we want to answer is whether it is possible to share information across languages to improve the performance of semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7221367955207825}]}, {"text": "We use the NLmaps corpus (, a semantic parsing corpus for English and German.", "labels": [], "entities": [{"text": "NLmaps corpus", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8119059801101685}]}, {"text": "We evaluated our approach on this corpus because it is the only dataset which provides data in both English and German.", "labels": [], "entities": []}, {"text": "presents typical examples from this dataset, together with a constructed code-switching utterance.", "labels": [], "entities": []}, {"text": "Utterances from different languages are assigned the same logical forms, thus motivating the approach taken in this paper.", "labels": [], "entities": []}, {"text": "We tokenize in way similar to Ko\u010disk\u00b4.", "labels": [], "entities": [{"text": "tokenize", "start_pos": 3, "end_pos": 11, "type": "TASK", "confidence": 0.9707828164100647}]}, {"text": "For each language, the corpus contains 1500 pairs of natural language queries and corresponding logical forms for training and 880 pairs for testing.", "labels": [], "entities": []}, {"text": "We use 10% of the training set as development data for early stopping and hyper-parameter tuning.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.6866172254085541}, {"text": "hyper-parameter tuning", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.6831684410572052}]}, {"text": "For evaluation, we use exact match accuracy for the logical form).", "labels": [], "entities": [{"text": "exact match accuracy", "start_pos": 23, "end_pos": 43, "type": "METRIC", "confidence": 0.8413508534431458}]}], "tableCaptions": [{"text": " Table 1: Performance of the baseline attentional  model (TGT Only) on GeoQuery (Zettlemoyer  and Collins, 2005) and ATIS (Zettlemoyer and  Collins, 2007) dataset compared with prior work.  The best performance is shown in bold.", "labels": [], "entities": [{"text": "ATIS (Zettlemoyer and  Collins, 2007) dataset", "start_pos": 117, "end_pos": 162, "type": "DATASET", "confidence": 0.7744990322324965}]}, {"text": " Table 3: Results on the full datasets. The best re- sult is shown in bold.", "labels": [], "entities": [{"text": "re- sult", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9618236819903055}]}]}