{"title": [{"text": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.6077524224917094}]}], "abstractContent": [{"text": "We introduce a simple and accurate neu-ral model for dependency-based semantic role labeling.", "labels": [], "entities": [{"text": "dependency-based semantic role labeling", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.669211782515049}]}, {"text": "Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM en-coder.", "labels": [], "entities": []}, {"text": "The semantic role labeler achieves competitive performance on English, even without any kind of syntactic information and only using local inference.", "labels": [], "entities": [{"text": "semantic role labeler", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6437909801801046}]}, {"text": "However , when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the English CoNLL-2009 dataset.", "labels": [], "entities": [{"text": "English CoNLL-2009 dataset", "start_pos": 185, "end_pos": 211, "type": "DATASET", "confidence": 0.9311349391937256}]}, {"text": "We also consider Chi-nese, Czech and Spanish where our approach also achieves competitive results.", "labels": [], "entities": []}, {"text": "Syntactic parsers are unreliable on out-of-domain data, so standard (i.e., syntactically-informed) SRL models are hindered when tested in this setting.", "labels": [], "entities": [{"text": "SRL", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9484855532646179}]}, {"text": "Our syntax-agnostic model appears more robust , resulting in the best reported results on standard out-of-domain test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of semantic role labeling (SRL), pioneered by, involves the prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.8054881691932678}, {"text": "prediction of predicate argument structure", "start_pos": 69, "end_pos": 111, "type": "TASK", "confidence": 0.7665553569793702}]}, {"text": "These representations have been shown to be beneficial in many NLP applications, including question answering and information extraction).", "labels": [], "entities": [{"text": "question answering", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.9257411956787109}, {"text": "information extraction", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.8255925476551056}]}, {"text": "Semantic banks (e.g.,)) often represent arguments as syntactic constituents or, more generally, text spans (Baker Sequa makes and repairs jet engines.", "labels": [], "entities": []}, {"text": "01 01 01 A0 A1 A0 A1 A1 Figure 1: A semantic dependency graph.", "labels": [], "entities": [{"text": "A0 A1 A0 A1 A1", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.7692445278167724}]}, {"text": "In contrast, CoNLL-2008 and 2009 shared tasks () popularized dependency-based semantic role labeling where the goal is to identify syntactic heads of arguments rather than entire constituents.", "labels": [], "entities": [{"text": "dependency-based semantic role labeling", "start_pos": 61, "end_pos": 100, "type": "TASK", "confidence": 0.6072027832269669}]}, {"text": "shows an example of such a dependency-based representation: node labels are senses of predicates (e.g., \"01\" indicates that the first sense from the PropBank sense repository is used for predicate makes in this sentence) and edge labels are semantic roles (e.g., A0 is a proto-agent, 'doer').", "labels": [], "entities": [{"text": "PropBank sense repository", "start_pos": 149, "end_pos": 174, "type": "DATASET", "confidence": 0.8715177178382874}]}, {"text": "Until recently, state-of-the-art SRL systems relied on complex sets of lexico-syntactic features () as well as declarative constraints.", "labels": [], "entities": [{"text": "SRL", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9742418527603149}]}, {"text": "Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex hand-crafted features.", "labels": [], "entities": [{"text": "Neural SRL", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.5737678110599518}]}, {"text": "Initially achieving stateof-the-art results only in the multilingual setting, where careful feature engineering is not practical (, neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (.", "labels": [], "entities": []}, {"text": "Recently, it has been shown that an accurate span-based SRL model can be constructed without relying on syntactic features (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9285074472427368}]}, {"text": "Nevertheless, the situation with dependency-based SRL has not changed: even recent state-of-the-art methods for this task heavily rely on syntactic features ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.7243414521217346}]}, {"text": "In particular, argue that syntactic features are necessary for the dependency-based SRL and show that performance of their model degrades dramatically if syntactic paths between arguments and predicates are not provided as an input.", "labels": [], "entities": [{"text": "dependency-based SRL", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.5719911754131317}]}, {"text": "In this work, we are the first to show that it is possible to construct a very accurate dependency-based semantic role labeler which either does not use any kind of syntactic information or uses very little (automatically predicted part-of-speech tags).", "labels": [], "entities": [{"text": "dependency-based semantic role labeler", "start_pos": 88, "end_pos": 126, "type": "TASK", "confidence": 0.5825395435094833}]}, {"text": "This suggests that our LSTM model can largely implicitly capture syntactic information, and this information can, to a large extent, substitute treebank syntax.", "labels": [], "entities": []}, {"text": "Similarly to the span-based model of we use bidirectional LSTMs to encode sentences and rely on their states when predicting arguments of each predicate.", "labels": [], "entities": []}, {"text": "We predict semantic dependency edges between predicates and arguments relying on LSTM states corresponding to the predicate and the argument positions (i.e. both edge endpoints).", "labels": [], "entities": []}, {"text": "As semantic roles are often specific to predicates or even predicate senses (e.g., in PropBank (), instead of predicting the role label (e.g., A0 for Sequa in our example), we predict predicate-specific roles (e.g., make-A0) using a compositional model.", "labels": [], "entities": []}, {"text": "Both these aspects (predicting edges and compositional embeddings of roles) contrast our approach with that of who essentially treat the SRL task as a generic sequence labeling task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 137, "end_pos": 145, "type": "TASK", "confidence": 0.9123764932155609}]}, {"text": "We empirically show that using these two ideas is crucial for achieving competitive performance on dependency SRL (+1.0% semantic F 1 in our ablation studies on English).", "labels": [], "entities": []}, {"text": "Also, unlike the spanbased version, we observe that using automatically predicted POS tags is also important (+0.7% F 1 ).", "labels": [], "entities": [{"text": "F 1", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9874931871891022}]}, {"text": "The resulting SRL model is very simple.", "labels": [], "entities": [{"text": "SRL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8887549042701721}]}, {"text": "Not only we do not rely on syntax, our model is also local, i.e., we do not globally score or constrain sets of arguments.", "labels": [], "entities": []}, {"text": "On the standard English in-domain CoNLL-2009 benchmark we achieve 87.7 F 1 which compares favorable to the best local model (86.7% F 1 for PathLSTM) and approaches the best results overall (87.9% for an ensemble of 3 PathLSTM models with a reranker on top).", "labels": [], "entities": [{"text": "CoNLL-2009 benchmark", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.7651460468769073}, {"text": "F 1", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9886719882488251}, {"text": "F 1", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9831652641296387}]}, {"text": "When we experiment with Chinese, Czech and Spanish portions of the CoNLL-2009 dataset, we also achieve competitive results, even without any extra hyper-parameter tuning.", "labels": [], "entities": [{"text": "CoNLL-2009 dataset", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.9700112640857697}]}, {"text": "Moreover, as syntactic parsers are not reliable when used out-of-domain, standard (i.e., syntactically-informed) dependency SRL models are crippled when applied to such data.", "labels": [], "entities": []}, {"text": "In contrast, our syntax-agnostic model appears to be considerably more robust: we achieve the best result so far on the English and Czech out-of-domain test set (77.7% and 87.2% F 1 , respectively).", "labels": [], "entities": [{"text": "Czech out-of-domain test set", "start_pos": 132, "end_pos": 160, "type": "DATASET", "confidence": 0.7500628158450127}, {"text": "F 1", "start_pos": 178, "end_pos": 181, "type": "METRIC", "confidence": 0.9880608320236206}]}, {"text": "For English, this constitutes a 2.4% absolute improvement over the comparable previous model (75.3% for the local PathLSTM) and substantially outperforms any previous method (76.5% for the ensemble of 3 PathLSTMs).", "labels": [], "entities": []}, {"text": "We believe that out-of-domain performance may in fact be more important than indomain one: in practice linguistic tools are rarely, if ever, used in-domain.", "labels": [], "entities": []}, {"text": "The key contributions can be summarized as follows: \u2022 we propose the first effective syntax-agnostic model for dependency-based SRL; \u2022 it achieves the best results among local models on the English, Chinese and Czech indomain test sets; \u2022 it substantially outperforms all previous methods on the out-of-domain test set on both English and Czech.", "labels": [], "entities": [{"text": "SRL", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.7693942189216614}]}, {"text": "Despite the effectiveness of our syntax-agnostic version, we believe that both integration of treebank syntax and global inference are promising directions and leave them for future work.", "labels": [], "entities": []}, {"text": "In fact, the proposed SRL model, given its simplicity and efficiency, maybe used as a natural building block for future global and syntactically-informed SRL models.", "labels": [], "entities": [{"text": "SRL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9394094347953796}, {"text": "SRL", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.937889575958252}]}], "datasetContent": [{"text": "We; we replaced a word with the UNK token with probability \u03b1 f r(w)+\u03b1 , where \u03b1 is an hyper-parameter and f r(w) is the frequency of the word w.", "labels": [], "entities": [{"text": "UNK token", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.9238828122615814}]}, {"text": "The predicted POS tags were provided by the CoNLL-2009 shared-task organizers.", "labels": [], "entities": [{"text": "CoNLL-2009 shared-task organizers", "start_pos": 44, "end_pos": 77, "type": "DATASET", "confidence": 0.8547781308492025}]}, {"text": "We used the same predicate disambiguator as in for English, the one used in for Czech and Spanish, and the one used in for Chinese.", "labels": [], "entities": []}, {"text": "The training objective was the categorical cross-entropy, and we optimized it with Adam (.", "labels": [], "entities": []}, {"text": "The hyperparameter tuning and all model selection was performed on the English development set; the chosen values are shown in: Hyperparameter values.", "labels": [], "entities": [{"text": "English development set", "start_pos": 71, "end_pos": 94, "type": "DATASET", "confidence": 0.9219712217648824}]}], "tableCaptions": [{"text": " Table 2: Results on the English in-domain test  set.", "labels": [], "entities": [{"text": "English in-domain test  set", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.7629518806934357}]}, {"text": " Table 3: Results on the English out-of-domain  test set.", "labels": [], "entities": [{"text": "English out-of-domain  test set", "start_pos": 25, "end_pos": 56, "type": "DATASET", "confidence": 0.831506684422493}]}, {"text": " Table 4: Results on the Chinese test set.", "labels": [], "entities": [{"text": "Chinese test set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9822543660799662}]}, {"text": " Table 5: Results on the Czech test sets.", "labels": [], "entities": [{"text": "Czech test sets", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.9543337424596151}]}, {"text": " Table 6: Results on the Spanish test set.", "labels": [], "entities": [{"text": "Spanish test set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9847872257232666}]}, {"text": " Table 7: Ablation study on the English develop- ment set.", "labels": [], "entities": [{"text": "English develop- ment set", "start_pos": 32, "end_pos": 57, "type": "DATASET", "confidence": 0.70629403591156}]}, {"text": " Table 8: F 1 results on the English test set broken  down into verbal and nominal predicates.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9533353745937347}, {"text": "English test set", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.8173341155052185}]}, {"text": " Table 9: Argument recognition results broken  down into verbal and nominal predicates.", "labels": [], "entities": [{"text": "Argument recognition", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7312083095312119}]}]}