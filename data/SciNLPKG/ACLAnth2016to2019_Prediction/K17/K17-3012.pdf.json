{"title": [{"text": "TurkuNLP: Delexicalized Pre-training of Word Embeddings for Dependency Parsing", "labels": [], "entities": [{"text": "TurkuNLP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8330039381980896}]}], "abstractContent": [{"text": "We present the TurkuNLP entry in the CoNLL 2017 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies.", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task", "start_pos": 37, "end_pos": 59, "type": "DATASET", "confidence": 0.8166126161813736}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.6934393584728241}]}, {"text": "The system is based on the UDPipe parser with our focus being in exploring various techniques to pre-train the word embeddings used by the parser in order to improve its performance especially on languages with small training sets.", "labels": [], "entities": []}, {"text": "The system ranked 11th among the 33 participants overall, being 8th on the small tree-banks, 10th on the large treebanks, 12th on the parallel test sets, and 26th on the surprise languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we describe the TurkuNLP entry in the CoNLL 2017 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies ().", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.8550586104393005}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.6655166029930115}]}, {"text": "The Universal Dependencies (UD) treebank collection () has 70 treebanks for 50 languages with cross-linguistically consistent annotation.", "labels": [], "entities": [{"text": "Universal Dependencies (UD) treebank collection", "start_pos": 4, "end_pos": 51, "type": "DATASET", "confidence": 0.6176151079790932}]}, {"text": "Of these, the 63 treebanks which have at least 10,000 tokens in their test section are used for training and testing the systems.", "labels": [], "entities": []}, {"text": "Further, a parallel corpus consisting of 1,000 sentences in 14 languages was developed as an additional test set, and finally, the shared task included test sets for four \"surprise\" languages not known until a week prior to the test phase of the shared task (.", "labels": [], "entities": []}, {"text": "No training data was provided for these languagesonly a handful of sentences was given as an example.", "labels": [], "entities": []}, {"text": "As an additional novelty, participation in the shared task involved developing an end-to-end parsing system, from raw text to dependency trees, for all of the languages and treebanks.", "labels": [], "entities": []}, {"text": "The participants were provided with automatically predicted word and sentence segmentation as well as morphological tags for the test sets, which they could choose to use as an alternative to developing own segmentation and tagging.", "labels": [], "entities": [{"text": "word and sentence segmentation", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.6999102979898453}]}, {"text": "These baseline segmentations and morphological analyses were provided by UDPipe v1.1 (.", "labels": [], "entities": [{"text": "UDPipe v1.1", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.8613688945770264}]}, {"text": "In addition to the manually annotated treebanks, the shared task organizers also distributed a large collection of web-crawled text for all but one of the languages in the shared task, totaling over 90 billion tokens of fully dependency parsed data.", "labels": [], "entities": []}, {"text": "Once again, these analyses were produced by the UDPipe system.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.8525813221931458}]}, {"text": "This automatically processed large dataset was intended by the organizers to complement the manually annotated data and, for instance, support the induction of word embeddings.", "labels": [], "entities": []}, {"text": "As an overall strategy for the shared task, we chose to build on an existing parser and focus on exploring various methods of pre-training the parser and especially its embeddings, using the large, automatically analyzed corpus provided by the organizers.", "labels": [], "entities": []}, {"text": "We expected this strategy to be particularly helpful for languages with only a little training data.", "labels": [], "entities": []}, {"text": "On the other hand we put only a minimal effort into the surprise languages.", "labels": [], "entities": []}, {"text": "We also chose to use the word and sentence segmentation of the test datasets, as provided by the organizers.", "labels": [], "entities": [{"text": "word and sentence segmentation", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6528795436024666}]}, {"text": "As we will demonstrate, the results of our system correlate with the focus of our efforts.", "labels": [], "entities": []}, {"text": "Initially, we focused on the latest ParseySaurus parser (, but due to the magnitude of the task and restrictions on time, we finally used the UDPipe parsing pipeline of as the basis of our efforts.", "labels": [], "entities": [{"text": "ParseySaurus parser", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.5951485186815262}, {"text": "UDPipe parsing pipeline", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.7559319734573364}]}], "datasetContent": [], "tableCaptions": []}