{"title": [], "abstractContent": [{"text": "This paper presents the submission of the Linguistics Department of the University of Colorado at Boulder for the 2017 CoNLL-SIGMORPHON Shared Task on Universal Morphological Reinflection.", "labels": [], "entities": [{"text": "CoNLL-SIGMORPHON Shared Task on Universal Morphological Reinflection", "start_pos": 119, "end_pos": 187, "type": "TASK", "confidence": 0.46357285550662447}]}, {"text": "The system is implemented as an RNN Encoder-Decoder.", "labels": [], "entities": []}, {"text": "It is specifically geared toward a low-resource setting.", "labels": [], "entities": []}, {"text": "To this end, it employs data augmentation for counteracting overfitting and a copy symbol for processing characters unseen in the training data.", "labels": [], "entities": []}, {"text": "The system is an ensemble often models combined using a weighted voting scheme.", "labels": [], "entities": []}, {"text": "It delivers substantial improvement inaccuracy compared to a non-neural baseline system in presence of varying amounts of training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing (NLP) for English is typically word based, that is, words such as dogs, cat's and they've are treated as atomic units.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7534220566352209}]}, {"text": "In the case of English, this is a viable approach because lexemes correspond to a handful of inflected forms.", "labels": [], "entities": []}, {"text": "However, for languages with more extensive inflectional morphology, the approach fails because one lexeme can be realized by thousands of distinct word forms in the worst case.", "labels": [], "entities": []}, {"text": "Therefore, NLP systems for languages with extensive inflectional morphology often need to be able to generate new inflected word forms based on known word forms.", "labels": [], "entities": []}, {"text": "This is the task of morphological reinflection.", "labels": [], "entities": [{"text": "morphological reinflection", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.9132883250713348}]}, {"text": "The traditional approach to word form generation is rule-based.", "labels": [], "entities": [{"text": "word form generation", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7210415204366049}]}, {"text": "For example, finite-state technology has been successfully applied in constructing morphological analyzers and generators fora large variety of languages).", "labels": [], "entities": []}, {"text": "Unfortunately, the rule-based approach is labor-intensive and therefore costly.", "labels": [], "entities": []}, {"text": "Additionally, coverage can become a problem because systems need to be continually updated with new lexemes.", "labels": [], "entities": [{"text": "coverage", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.6056032776832581}]}, {"text": "For these reasons, machine learning approaches have recently gained ground.", "labels": [], "entities": []}, {"text": "Results from the 2016 SIGMORPHON Shared Task on Morphological Reinflection ( indicate that models based on recurrent neural networks can deliver high accuracies for reinflection.", "labels": [], "entities": [{"text": "SIGMORPHON Shared Task on Morphological Reinflection", "start_pos": 22, "end_pos": 74, "type": "TASK", "confidence": 0.6348330279191335}]}, {"text": "The winning system by achieved an average accuracy in excess of 95% when tested on 10 languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9991274476051331}]}, {"text": "Based on these results, morphological reinflection could be considered a solved problem.", "labels": [], "entities": [{"text": "morphological reinflection", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.9299117922782898}]}, {"text": "However, the 2016 shared task employed training sets of more than 10,000 word forms for most languages.", "labels": [], "entities": []}, {"text": "Ina setting with less training data, the reinflection task becomes much more challenging.", "labels": [], "entities": []}, {"text": "In an extreme low-resource setting of 100 training examples, a standard RNN Encoder-Decoder system like the one used by will typically perform quite poorly.", "labels": [], "entities": []}, {"text": "This paper documents the submission of the CU Boulder Linguistics Department for the 2017 CoNLL-SIGMORPHON Shared Task on Universal Morphological Reinflection.", "labels": [], "entities": [{"text": "CU Boulder Linguistics Department", "start_pos": 43, "end_pos": 76, "type": "DATASET", "confidence": 0.8300908952951431}, {"text": "CoNLL-SIGMORPHON Shared Task on Universal Morphological Reinflection", "start_pos": 90, "end_pos": 158, "type": "TASK", "confidence": 0.47887555190495085}]}, {"text": "The task covers 52 languages from different language families with a wide geographical distribution.", "labels": [], "entities": []}, {"text": "The task evaluates systems trained on varying amounts of data ranging from 100 to more than 10,000 training examples.", "labels": [], "entities": []}, {"text": "Our system is an RNN Encoder-Decoder ( ) specifically geared toward a lowresource setting.", "labels": [], "entities": []}, {"text": "The system closely resembles the system introduced by.", "labels": [], "entities": []}, {"text": "However, the novelty of our approach lies in the training procedure.", "labels": [], "entities": []}, {"text": "We augment the training data with generated training examples.", "labels": [], "entities": []}, {"text": "This is a commonly used technique in image processing but it has been employed to a lesser degree in NLP.", "labels": [], "entities": [{"text": "image processing", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.9283568859100342}]}, {"text": "Data augmentation counteracts overfitting and allows us to learn reinflection systems using small training sets.", "labels": [], "entities": []}, {"text": "We employ an ensemble of 10 models under a weighted voting scheme.", "labels": [], "entities": []}, {"text": "We also implement a mechanism, the copy symbol, which allows the system to copy unseen characters from an input lemma to the resulting word form.", "labels": [], "entities": []}, {"text": "This improves accuracy for small training sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9989622831344604}]}, {"text": "Unfortunately, due to time constraints, we were only able to use the copy symbol in Task 2 of the shared task.", "labels": [], "entities": []}, {"text": "For Task 1 of the shared task, we achieve substantial improvements over a non-neural baseline (, even in the low resource setting.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 presents related work on morphological reinflection and data augmentation for natural language processing.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the shared task and associated data sets.", "labels": [], "entities": []}, {"text": "We provide a detailed description of our system in Section 4 and present experiments and results in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we provide a discussion of results and conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "For Task 1, we train ten models for each language and setting.", "labels": [], "entities": []}, {"text": "We then apply weighted voting as explained in Section 4.", "labels": [], "entities": []}, {"text": "For most languages, a hidden layer size, embeddings size, and attention layer size of 32 gave reasonable results.", "labels": [], "entities": []}, {"text": "For 11 languages, Faroese, French, German, Haida, Hungarian, Icelandic, Latin, Lithuanian, Navajo, Bokmal, and Nynorsk, we found 32 insufficient, and set hidden layer size, embedding size and attention layer size to 100 instead.", "labels": [], "entities": [{"text": "Bokmal", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.9114350080490112}, {"text": "Nynorsk", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.8783447742462158}]}, {"text": "Setting the layer size to 100 might improve results for other languages as well.", "labels": [], "entities": []}, {"text": "Unfortunately, we did not have enough time to test this.", "labels": [], "entities": []}, {"text": "Data augmentation is used in order to improve accuracy in the low and medium training data settings for Task 1.", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6541029512882233}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9986991882324219}]}, {"text": "In the low setting, we add 4900 augmented training examples to the training set, and in the medium data setting, we add 9900 augmented training examples.", "labels": [], "entities": []}, {"text": "Given that the original low training data spans 100 and the medium training data spans 1000 examples, this means that the original training data accounts for 2% of the augmented low training set and 10% of the augmented medium training set.", "labels": [], "entities": []}, {"text": "For Task 2, we also use augmented data.", "labels": [], "entities": []}, {"text": "In the low setting, we add augmented data until the total size of the training set is 20,000 examples.", "labels": [], "entities": []}, {"text": "In the medium and high settings, we add augmented examples until the size of the training set is 25,000.", "labels": [], "entities": []}, {"text": "Time constraints prohibited us from using more generated data.", "labels": [], "entities": []}, {"text": "Because of the large variance of the sizes of training sets in Task 2 (for example the low Basque training data spans 4,750 examples, whereas the low English training data spans 50 examples), some languages use substantially more augmented data than other languages.", "labels": [], "entities": []}, {"text": "In the high setting, some languages, in fact, draw upon no augmented data at all due to the large size of the training set.", "labels": [], "entities": []}, {"text": "For Task 2, we use the copy symbol as explained in Section 4.", "labels": [], "entities": []}, {"text": "This would probably have resulted in improved accuracy for Task 1 as well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9995125532150269}]}, {"text": "Unfortunately, we were unable to run experiments using the copy symbol for Task 1 because of time constraints.", "labels": [], "entities": []}, {"text": "The test results for Task 1 and Task 2 are shown in.", "labels": [], "entities": []}, {"text": "For Task 1, the RNN system achieves average accuracy 45.74% for the low settings, 77.60% for the medium setting and 92.97% for the high setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9112765789031982}]}, {"text": "All of these figures are substantially greater than the baseline accuracies which are 37.90%, 64.70% and 77.81% for the different settings, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.8205906748771667}]}, {"text": "The RNN system fails to achieve the base- For Task2, the RNN system fails to achieve the baseline accuracy for most languages and settings.", "labels": [], "entities": [{"text": "base- For", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9119271039962769}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9144722819328308}]}], "tableCaptions": [{"text": " Table 1: Results from Task 1 and Task 2. RNN refers to the RNN Encoder-Decoder with data augmentation and weighted  voting presented in Section 4. Baseline refers to the non-neural baseline system presented in", "labels": [], "entities": []}]}