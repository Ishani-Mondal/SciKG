{"title": [{"text": "Feature Selection as Causal Inference: Experiments with Text Classification", "labels": [], "entities": [{"text": "Feature Selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7688989043235779}, {"text": "Text Classification", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.6946860551834106}]}], "abstractContent": [{"text": "This paper proposes a matching technique for learning causal associations between word features and class labels in document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.6724681109189987}]}, {"text": "The goal is to identify more meaningful and general-izable features than with only correla-tional approaches.", "labels": [], "entities": []}, {"text": "Experiments with sentiment classification show that the proposed method identifies interpretable word associations with sentiment and improves classification performance in a majority of cases.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.9214222133159637}]}, {"text": "The proposed feature selection method is particularly effective when applied to out-of-domain data.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7121375948190689}]}], "introductionContent": [{"text": "A major challenge when building classifiers for high-dimensional data like text is learning to identify features that are not just correlated with the classes in the training data, but associated with classes in a meaningful way that will generalize to new data.", "labels": [], "entities": []}, {"text": "Methods for regularization) and feature selection) are critical for obtaining good classification performance by removing or minimizing the effects of noisy features.", "labels": [], "entities": [{"text": "regularization", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.9596856236457825}, {"text": "feature selection", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7610325813293457}]}, {"text": "While empirically successful, these techniques can only identify features that are correlated with classes, and these associations can still be caused by factors other than the direct relationship that is assumed.", "labels": [], "entities": []}, {"text": "A more meaningful association is a causal one.", "labels": [], "entities": []}, {"text": "In the context of document classification using bag-of-words features, we ask the question, which word features \"cause\" documents to have the class labels that they do?", "labels": [], "entities": [{"text": "document classification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7161680161952972}]}, {"text": "For example, it might be reasonable to claim that adding the word horrible to a review would cause its sentiment to become negative, while this is less plausible fora word like said.", "labels": [], "entities": []}, {"text": "Yet, in one of our experimental datasets of doctor reviews, said has a stronger correlation with negative sentiment than horrible.", "labels": [], "entities": []}, {"text": "Inspired by methods for causal inference in other domains, we seek to learn causal associations between word features and document classes.", "labels": [], "entities": []}, {"text": "We experiment with propensity score matching, a technique attempts to mimic the random assignment of subjects to treatment and control groups in a randomized controlled trial by matching subjects with a similar \"propensity\" to receive treatment.", "labels": [], "entities": [{"text": "propensity score matching", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.6204704840977987}]}, {"text": "Translating this idea to document classification, we match documents with similar propensity to contain a word, allowing us to compare the effect a word has on the class distribution after controlling for the context in which the word appears.", "labels": [], "entities": [{"text": "document classification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7393286228179932}]}, {"text": "We propose a statistical test for measuring the importance of word features on the matched training data.", "labels": [], "entities": []}, {"text": "We experiment with binary sentiment classification on three review corpora from different domains (doctors, movies, products) using propensity score matching to test for statistical significance of features.", "labels": [], "entities": [{"text": "binary sentiment classification", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6944975753625234}]}, {"text": "Compared to a chi-squared test, the propensity score matching test for feature selection yields superior performance in a majority of comparisons, especially for domain adaptation and for identifying top word associations.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.7233627736568451}]}, {"text": "After presenting results and analysis in Sections 4-5, we discuss the implications of our findings and make suggestions for areas of language processing that would benefit from causal learning methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the ability of propensity score matching to identify meaningful word features, we use it for feature selection) in sentiment classification ().", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.9624230265617371}]}, {"text": "We used datasets of reviews from three domains: \u2022 Doctors: Doctor reviews from RateMDs.com ().", "labels": [], "entities": [{"text": "Doctors: Doctor reviews from RateMDs.com", "start_pos": 50, "end_pos": 90, "type": "DATASET", "confidence": 0.6971634328365326}]}, {"text": "Doctors are rated on a scale from 1-5 along four different dimensions (knowledgeability, staff, helpfulness, punctuality).", "labels": [], "entities": [{"text": "punctuality", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.959567129611969}]}, {"text": "We averaged the four ratings for each review and labeled a review positive if the average rating was \u2265 4 and negative if \u2264 2.", "labels": [], "entities": []}, {"text": "\u2022 Movies: Movie reviews from IMDB ().", "labels": [], "entities": [{"text": "IMDB", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8733567595481873}]}, {"text": "Movies are rated on a scale from 1-10.", "labels": [], "entities": []}, {"text": "Reviews rated \u2265 7 are labeled positive and reviews rated \u2264 4 are labeled negative.", "labels": [], "entities": []}, {"text": "\u2022 Products: Product reviews from Amazon (Jindal and Liu, 2008).", "labels": [], "entities": []}, {"text": "Products are rated on a scale from 1-5, with reviews rated \u2265 4 labeled positive and reviews rated \u2264 2 labeled negative.", "labels": [], "entities": []}, {"text": "All datasets were sampled to have an equal class balance.", "labels": [], "entities": []}, {"text": "We used unigram word features.", "labels": [], "entities": []}, {"text": "For ef-: Area under the feature selection curve (see) using F1-score as the evaluation metric.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9975608587265015}]}, {"text": "All differences between corresponding PSM and \u03c7 2 results are statistically significant with p 0.01 except for (Doctors, Doctors).", "labels": [], "entities": []}, {"text": "ficiency reasons (a limitation that is discussed in Section 7), we pruned the long tail of features, removing words appearing in less than 0.5% of each corpus.", "labels": [], "entities": []}, {"text": "The sizes of the processed corpora and their vocabularies are summarized in.", "labels": [], "entities": []}, {"text": "For each corpus, we randomly selected 50% for training, 25% for development, and 25% for testing.", "labels": [], "entities": []}, {"text": "The training set is used for training classifiers as well as calculating all feature selection metrics.", "labels": [], "entities": []}, {"text": "We used the development set to measure classification performance for different hyperparameter values.", "labels": [], "entities": []}, {"text": "Our propensity score matching method has two hyperparameters.", "labels": [], "entities": []}, {"text": "First, when building logistic regression models to estimate the propensity scores, we adjusted the 2 regularization strength.", "labels": [], "entities": []}, {"text": "Second, when matching documents, we required the difference between scores to be less than \u03c4 \u00d7SD to count as a match, where SD is the standard deviation of the propensity scores.", "labels": [], "entities": []}, {"text": "We performed a grid search over different values of \u03c4 and different regularization strengths, described more in our analysis in Section 5.2, and used the best combination of hyperparameters for each dataset.", "labels": [], "entities": []}, {"text": "We used logistic regression classifiers for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.9607855379581451}]}, {"text": "While we experimented with 2 regularization for constructing propensity scores, we used no regularization for the sentiment classifiers.", "labels": [], "entities": []}, {"text": "Since regularization and feature selection are both used to avoid overfitting, we did not want to conflate the effects of the two, so by using unregularized classifiers we can directly assess the efficacy of our feature selection methods on held-out data.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.6759119629859924}]}, {"text": "All models were implemented with scikit-learn (Pedregosa et al., 2011).", "labels": [], "entities": []}, {"text": "Baseline We compare propensity score matching with McNemar's test (PSM) to a standard chisquared test (\u03c7 2 ) for feature selection, one of the", "labels": [], "entities": [{"text": "McNemar's test (PSM)", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.6674099216858546}, {"text": "feature selection", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7124925851821899}]}], "tableCaptions": [{"text": " Table 3: Area under the feature selection curve  (see", "labels": [], "entities": []}, {"text": " Table 4: The highest scoring words from the two  feature selection methods.", "labels": [], "entities": []}, {"text": " Table 5: Area under the feature selection curve  when using only a small number of features, M .", "labels": [], "entities": [{"text": "Area", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9801070690155029}]}]}