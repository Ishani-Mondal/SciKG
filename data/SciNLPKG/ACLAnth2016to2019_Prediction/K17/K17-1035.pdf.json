{"title": [{"text": "The Covert Helps Parse the Overt", "labels": [], "entities": [{"text": "Covert", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.8743282556533813}, {"text": "Overt", "start_pos": 27, "end_pos": 32, "type": "TASK", "confidence": 0.6188124418258667}]}], "abstractContent": [{"text": "This paper is concerned with whether deep syntactic information can help surface parsing, with a particular focus on empty categories.", "labels": [], "entities": [{"text": "surface parsing", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.7010556757450104}]}, {"text": "We design new algorithms to produce dependency trees in which empty elements are allowed, and evaluate the impact of information about empty category on parsing overt elements.", "labels": [], "entities": []}, {"text": "Such information is helpful to reduce the approximation error in a structured parsing model, but increases the search space for inference and accordingly the estimation error.", "labels": [], "entities": [{"text": "estimation error", "start_pos": 158, "end_pos": 174, "type": "METRIC", "confidence": 0.931824266910553}]}, {"text": "To deal with structure-based overfitting, we propose to integrate disam-biguation models with and without empty elements, and perform structure regular-ization via joint decoding.", "labels": [], "entities": []}, {"text": "Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing.", "labels": [], "entities": [{"text": "English and Chinese TreeBanks", "start_pos": 15, "end_pos": 44, "type": "DATASET", "confidence": 0.6273349747061729}, {"text": "surface parsing", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.6695372760295868}]}], "introductionContent": [{"text": "In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis.", "labels": [], "entities": [{"text": "surface analysis", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.7100396454334259}]}, {"text": "See, among others,.", "labels": [], "entities": []}, {"text": "Such analysis, e.g. deep dependency structures (, is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar), Headdriven Phrase-Structure Grammar (HPSG; and.", "labels": [], "entities": []}, {"text": "Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not clear whether such additional information is helpful for surface syntactic analysis.", "labels": [], "entities": [{"text": "surface syntactic analysis", "start_pos": 169, "end_pos": 195, "type": "TASK", "confidence": 0.7306933800379435}]}, {"text": "This is partly because analysis grounded on different grammar formalisms, e.g. HPSG and CFG, are not directly comparable.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9155663847923279}, {"text": "CFG", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.860107421875}]}, {"text": "In the Government and Binding (GB; Chomsky, 1981) theory, empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements.", "labels": [], "entities": [{"text": "Government and Binding (GB; Chomsky, 1981)", "start_pos": 7, "end_pos": 49, "type": "TASK", "confidence": 0.7976032584905625}]}, {"text": "Following the linguistic insights underlying GB, a traditional dependency analysis can be augmented with empty elements, viz.", "labels": [], "entities": []}, {"text": "The new representation provides a considerable amount of deep syntactic information, while keeping intact all dependencies of overt words.", "labels": [], "entities": []}, {"text": "Integrating both overt and covert elements in one unified representation provides an effective yet lightweight way to achieve deeper language understanding beyond surface syntax . Even more important, this modest way to modify tree analysis makes possible fair evaluation of the influence of deep syntactic elements on surface parsing.", "labels": [], "entities": [{"text": "surface parsing", "start_pos": 319, "end_pos": 334, "type": "TASK", "confidence": 0.6904767453670502}]}, {"text": "We study graph-based parsing models for this new representation with a particular focus on the impact of information about the covert on parsing the overt.", "labels": [], "entities": [{"text": "parsing", "start_pos": 137, "end_pos": 144, "type": "TASK", "confidence": 0.9636682868003845}]}, {"text": "The major advantage of the graph-based approach to dependency parsing is that its constrained factorization enables the design of polynomial time algorithms for decoding, especially for projective structures.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7971818447113037}]}, {"text": "Following GB, an empty element can be only a dependent.", "labels": [], "entities": []}, {"text": "Furthermore, the number and distribution of empty elements in one sentence is highly constrained.", "labels": [], "entities": []}, {"text": "These properties makes polynomial time decoding for joint empty element detection and dependency parsing still plausible.", "labels": [], "entities": [{"text": "joint empty element detection", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.6027483195066452}, {"text": "dependency parsing", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7395172715187073}]}, {"text": "The dependency structure is according to Stanford Dependency (de).", "labels": [], "entities": [{"text": "Stanford Dependency (de)", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.9407267570495605}]}, {"text": "\"\u2205\" denotes an empty element.", "labels": [], "entities": []}, {"text": "\"\u2205 1 \" indicates an expletive construction; \"\u2205 2 \" indicates that the subject for fight, i.e. GM, is located in another place; \"\u2205 3 \" indicates a wh-movement.", "labels": [], "entities": []}, {"text": "The influence of incorporating empty elements is twofold.", "labels": [], "entities": []}, {"text": "On the one hand, the extra information enriches the structural information of the outputs, which is important to reduce the approximation error in a structured prediction problem.", "labels": [], "entities": []}, {"text": "On the other hand, predicting empty elements increases the search space for decoding, and thus increases the difficulty of parameter estimation for disambiguation.", "labels": [], "entities": [{"text": "predicting empty elements", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.8867508769035339}]}, {"text": "Our experiments on English Penn TreeBank (PTB; and Chinese TreeBank (CTB; shows that the second effect is prominant.", "labels": [], "entities": [{"text": "English Penn TreeBank (PTB", "start_pos": 19, "end_pos": 45, "type": "DATASET", "confidence": 0.9088229775428772}, {"text": "Chinese TreeBank (CTB", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.9279279112815857}]}, {"text": "The accuracy of predicting dependencies among overt words sometimes declines slightly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999384880065918}, {"text": "predicting dependencies among overt words", "start_pos": 16, "end_pos": 57, "type": "TASK", "confidence": 0.873207950592041}]}, {"text": "To ensure that predicting the empty elements helps parse the overt, we need to reduce the new estimation error.", "labels": [], "entities": [{"text": "estimation error", "start_pos": 94, "end_pos": 110, "type": "METRIC", "confidence": 0.9492708444595337}]}, {"text": "To this end, we propose to integrate scores from parsing models with and without empty elements and perform joint decoding.", "labels": [], "entities": []}, {"text": "The intuition is to leverage parameters estimated without empty elements as a backoff, which exhibit better generalization ability.", "labels": [], "entities": []}, {"text": "We evaluate two joint decoders: One is based on chart merging and the other is based on dual decomposition.", "labels": [], "entities": [{"text": "chart merging", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.7417392730712891}]}, {"text": "Experiments demonstrate that information about the covert improves surface analysis in this way.", "labels": [], "entities": [{"text": "surface analysis", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.8062766790390015}]}, {"text": "Accuracy evaluated using parsing models with different factorizations and on data sets from different languages is consistently improved.", "labels": [], "entities": []}, {"text": "Especially, for those sentences in which there is no empty element, accuracy is improved too.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9995177984237671}]}, {"text": "This highlights the fact that empty category can help reduce the approximation error for surface analysis.", "labels": [], "entities": [{"text": "surface analysis", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.7933372855186462}]}, {"text": "The remaining part of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 is a brief introduction to the problem.", "labels": [], "entities": []}, {"text": "Section 3 describes existing algorithms for parsing for overt words only, while Section 4 gives the details of our new algorithms for parsing with empty elements.", "labels": [], "entities": [{"text": "parsing for overt words", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.8121756911277771}]}, {"text": "Section 5 describes the details of the joint models as well as the decoding algorithms.", "labels": [], "entities": []}, {"text": "Section 6 presents experimental results and empirical analyses.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Coverage relative to the number of suc- cessive empty elements that have the same head.", "labels": [], "entities": []}, {"text": " Table 1. The length indicates the number of suc- cessive empty elements that are governed by the  same overt word. At most three empty elements  are next to each other.", "labels": [], "entities": []}, {"text": " Table 2: Numbers of sentences, overt and covert  elements in training and test sets.", "labels": [], "entities": []}, {"text": " Table 4: UAS o of different individual models on  test data. The upper and bottom blocks present  results obtained by sibling and tri-sibling models  respectively.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8895702958106995}]}, {"text": " Table 5: UAS o of different joint decoding mod- els on test data. \"CM\" and \"DD\" are short for  joint decoders based on chart merging and dual  decomposition respectively. The upper and bot- tom blocks present results obtained by sibling and  tri-sibling models respectively. All improvements  are statistically significant.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7312507629394531}]}, {"text": " Table 6: UAS o evaluated using different types of  sentences. Dual decomposition is used for joint  decoding.", "labels": [], "entities": []}, {"text": " Table 6. For those sentences  in which there is no empty element, accuracy is  improved as well. This indicates that empty cate- gory can help reduce the approximation error for  surface analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9996744394302368}, {"text": "surface analysis", "start_pos": 180, "end_pos": 196, "type": "TASK", "confidence": 0.80500727891922}]}]}