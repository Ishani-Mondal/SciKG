{"title": [{"text": "Cross-language Learning with Adversarial Neural Networks: Application to Community Question Answering", "labels": [], "entities": [{"text": "Community Question Answering", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.5975630184014639}]}], "abstractContent": [{"text": "We address the problem of cross-language adaptation for question-question similarity reranking in community question answering , with the objective to port a system trained on one input language to another input language given labeled training data for the first language and only unlabeled data for the second language.", "labels": [], "entities": [{"text": "cross-language adaptation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.8109271824359894}, {"text": "question-question similarity reranking in community question answering", "start_pos": 56, "end_pos": 126, "type": "TASK", "confidence": 0.6729094386100769}]}, {"text": "In particular, we propose to use adversarial training of neural networks to learn high-level features that are discriminative for the main learning task, and at the same time are invariant across the input languages.", "labels": [], "entities": []}, {"text": "The evaluation results show sizable improvements for our cross-language adversarial neural network (CLANN) model over a strong non-adversarial system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Developing natural language processing (NLP) systems that can work indistinctly with different input languages is a challenging task; yet, such a setup is useful for many real-world applications.", "labels": [], "entities": []}, {"text": "One expensive solution is to annotate data for each input language and then to train a separate system for each one.", "labels": [], "entities": []}, {"text": "Another option, which can be also costly, is to translate the input, e.g., using machine translation (MT), and then to work monolingually in the target language.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.8233863472938537}]}, {"text": "However, the machine-translated text can be of low quality, might lose some input signal, e.g., it can alter sentiment (), or may not be really needed (.", "labels": [], "entities": []}, {"text": "Using a unified cross-language representation of the input is a third, less costly option, which allows any combination of input languages during both training and testing.", "labels": [], "entities": []}, {"text": "In this paper, we take this last approach, i.e., combining languages during both training and testing, and we study the problem of question-question similarity reranking in community Question Answering (cQA), when the input question can be either in English or in Arabic, and the questions it is compared to are always in English.", "labels": [], "entities": [{"text": "question-question similarity reranking in community Question Answering (cQA)", "start_pos": 131, "end_pos": 207, "type": "TASK", "confidence": 0.6787340551614761}]}, {"text": "We start with a simple language-independent representation based on cross-language word embeddings, which we input into a feed-forward multilayer neural network to classify pairs of questions, (English, English) or (Arabic, English), regarding their similarity.", "labels": [], "entities": []}, {"text": "Furthermore, we explore the question of whether adversarial training can be used to improve the performance of the network when we have some unlabeled examples in the target language.", "labels": [], "entities": []}, {"text": "In particular, we adapt the Domain Adversarial Neural Network model from (, which was originally used for domain adaptation, to our crosslanguage setting.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7519417107105255}]}, {"text": "To the best of our knowledge, this is novel for cross-language question-question similarity reranking, as well as for natural language processing (NLP) in general; moreover, we are not aware of any previous work on cross-language question reranking for community Question Answering.", "labels": [], "entities": [{"text": "cross-language question-question similarity reranking", "start_pos": 48, "end_pos": 101, "type": "TASK", "confidence": 0.6814588233828545}, {"text": "natural language processing (NLP)", "start_pos": 118, "end_pos": 151, "type": "TASK", "confidence": 0.759590764840444}, {"text": "cross-language question reranking", "start_pos": 215, "end_pos": 248, "type": "TASK", "confidence": 0.6311706900596619}, {"text": "community Question Answering", "start_pos": 253, "end_pos": 281, "type": "TASK", "confidence": 0.59023450811704}]}, {"text": "In our setup, the basic task-solving network is paired with another network that shares the internal representation of the input and tries to decide whether the input example comes from the source (English) or from the target (Arabic) language.", "labels": [], "entities": []}, {"text": "The training of this language discriminator network is adversarial with respect to the shared layers by using gradient reversal during backpropagation, which makes the training to maximize the loss of the discriminator rather than to minimize it.", "labels": [], "entities": []}, {"text": "The main idea is to learn a high-level abstract representation that is discriminative for the main classification task, but is invariant across the input languages.", "labels": [], "entities": []}, {"text": "We apply this method to an extension of the SemEval-2016 Task 3, subtask B benchmark dataset for question-question similarity reranking ().", "labels": [], "entities": [{"text": "SemEval-2016 Task 3, subtask B benchmark dataset", "start_pos": 44, "end_pos": 92, "type": "DATASET", "confidence": 0.5493280217051506}, {"text": "question-question similarity reranking", "start_pos": 97, "end_pos": 135, "type": "TASK", "confidence": 0.5287987291812897}]}, {"text": "In particular, we hired professional translators to translate the original English questions to Arabic, and we further collected additional unlabeled questions in English, which we also got translated into Arabic.", "labels": [], "entities": []}, {"text": "We show that using the unlabeled data for adversarial training allows us to improve the results by a sizable margin in both directions, i.e., when training on English and adapting the system with the Arabic unlabeled data, and vice versa.", "labels": [], "entities": []}, {"text": "Moreover, the resulting performance is comparable to the best monolingual English systems at SemEval.", "labels": [], "entities": []}, {"text": "We also compare our unsupervised model to a semi-supervised model, where we have some labeled data for the target language.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 discusses some related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces our model for adversarial training for cross-language problems.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental setup.", "labels": [], "entities": []}, {"text": "Section 5 presents the evaluation results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes and points to possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the datasets we used, the generation of the input embeddings, the nature of the pairwise features, and the general training setup of our model.", "labels": [], "entities": []}, {"text": "SemEval-2016 Task 3 (Nakov et al., 2016b), provides 267 input questions for training, 50 for development, and 70 for testing, and ten times as many potentially related questions retrieved by an IR engine for each input question: 2,670, 500, and 700, respectively.", "labels": [], "entities": []}, {"text": "Based on this data, we simulated a cross-language setup for question-question similarity reranking.", "labels": [], "entities": [{"text": "question-question similarity reranking", "start_pos": 60, "end_pos": 98, "type": "TASK", "confidence": 0.6759182711442312}]}, {"text": "We first got the 387 original train+dev+test questions translated into Arabic by professional translators.", "labels": [], "entities": []}, {"text": "Then, we used these Arabic questions as an input with the goal to rerank the ten related English questions.", "labels": [], "entities": []}, {"text": "As an example, this is the Arabic translation of the original English question from: We further collected 221 additional original questions and 1,863 related questions as unlabeled data, and we got the 221 English questions translated to Arabic.", "labels": [], "entities": []}, {"text": "Below we present the experimental results for the unsupervised and semi-supervised language adaptation settings.", "labels": [], "entities": []}, {"text": "We compare our cross-language adversarial network (CLANN) to a feed forward neural network (FNN) that has no adversarial part.", "labels": [], "entities": []}, {"text": "shows the main results for our crosslanguage adaptation experiments.", "labels": [], "entities": [{"text": "crosslanguage adaptation", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8247301280498505}]}, {"text": "Rows 1-2 present the results when the target language is Arabic and the system is trained with English input.", "labels": [], "entities": []}, {"text": "Rows 3-4 show the reverse case, i.e., adaptation into English when training on Arabic.", "labels": [], "entities": []}, {"text": "FNN stands for feed-forward neural network, and it is the upper layer in, excluding the language discriminator.", "labels": [], "entities": [{"text": "FNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7625735998153687}]}, {"text": "CLANN is the full cross-language adversarial neural network, training the discriminator with English inputs paired with random Arabic related questions from the unlabeled dataset.", "labels": [], "entities": [{"text": "CLANN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8115410208702087}]}, {"text": "We show three ranking-oriented evaluation measures that are standard in the field of Information Retrieval: mean average precision (MAP), mean reciprocal rank (MRR), and average recall (AvgRec).", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7735530436038971}, {"text": "mean average precision (MAP)", "start_pos": 108, "end_pos": 136, "type": "METRIC", "confidence": 0.8920033474763235}, {"text": "mean reciprocal rank (MRR)", "start_pos": 138, "end_pos": 164, "type": "METRIC", "confidence": 0.8772367636362711}, {"text": "recall (AvgRec)", "start_pos": 178, "end_pos": 193, "type": "METRIC", "confidence": 0.854897990822792}]}, {"text": "We computed them using the official scorer from SemEval-2016 Task 3. 4 Similarly to that task, we consider Mean Average Precision (MAP) as the main evaluation metric.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 107, "end_pos": 135, "type": "METRIC", "confidence": 0.9722765584786733}]}, {"text": "The table also presents, for reproducibility, the values of the neural network hyper-parameters after tuning (in the fifth column).", "labels": [], "entities": []}, {"text": "We can see that the MAP score for FNN with Arabic target is 75.28.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9706886112689972}, {"text": "FNN", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8932883143424988}]}, {"text": "When doing the adversarial adaptation with the unlabeled Arabic examples (CLANN), the MAP score is boosted to 76.64 (+1.36 points).", "labels": [], "entities": [{"text": "MAP score", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9771830439567566}]}, {"text": "Going in the reverse direction, with English as the target, yields very comparable results: MAP goes from 75.32 to 76.70 (+1.38).", "labels": [], "entities": [{"text": "MAP", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9962542057037354}]}, {"text": "To put these results into perspective, shows the results for the top-2 best-performing systems from SemEval-2016 Task 3, which used a monolingual English setting.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.5898619294166565}]}, {"text": "We can see that our FNN approach based on cross-language input embeddings is already not far from the best systems.", "labels": [], "entities": [{"text": "FNN", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.871637225151062}]}, {"text": "Yet, when we consider the full adversarial network, in any of the two directions, we get performance that is on par with the best, in all metrics.", "labels": [], "entities": []}, {"text": "We conclude that the adversarial component in the network does the expected job, and improves the performance by focusing the languageindependent features in the representation layer.", "labels": [], "entities": []}, {"text": "The scatter plots in are computed by projecting the representation layer vectors of the first 500 test examples into two dimensions using t-SNE visualization).", "labels": [], "entities": []}, {"text": "The first 250 are taken with Arabic input (blue), the second 250 are taken with English input (red).", "labels": [], "entities": []}, {"text": "0-1 are the class labels (similar vs. non-similar).", "labels": [], "entities": []}, {"text": "The top plot corresponds to CLANN training with English and adapting with Arabic examples, while the second one covers the opposite direction.", "labels": [], "entities": [{"text": "adapting", "start_pos": 60, "end_pos": 68, "type": "TASK", "confidence": 0.9506654143333435}]}, {"text": "The plots look as expected.", "labels": [], "entities": []}, {"text": "CLANN really mixes the blue and the red examples, as the adversarial part of the network pushes for learning shared abstract features that are language-insensitive.", "labels": [], "entities": []}, {"text": "At the same time, the points form clusters with clear majorities of 0s or 1s, as the supervised part of the network learns how to classify them in these classes.", "labels": [], "entities": []}, {"text": "We now study the semi-supervised scenario when we also have some labeled data from the target language, i.e., where the original question q is in the target language.", "labels": [], "entities": []}, {"text": "This can be relevant in practical situations, as sometimes we might be able to annotate some data in the target language.", "labels": [], "entities": []}, {"text": "It is also an exploration of training with data in multiple languages all together.", "labels": [], "entities": []}, {"text": "To simulate this scenario, we split the training set in two halves.", "labels": [], "entities": []}, {"text": "We train with one half as the source language, and we use the other half with the target language as extra supervised data.", "labels": [], "entities": []}, {"text": "At the same time, we also use the unlabeled examples as before.", "labels": [], "entities": []}, {"text": "We introduced the semi-supervised model in subsection 3.2, which is a straightforward adaptation of the CLANN model.", "labels": [], "entities": []}, {"text": "shows the main results of our crosslanguage semi-supervised experiments.", "labels": [], "entities": []}, {"text": "The table is split into two blocks by source and target language (en-ar or ar-en).", "labels": [], "entities": []}, {"text": "We also use the same notation as in.", "labels": [], "entities": []}, {"text": "The suffixes -unsup and -semisup indicate whether CLANN is trained in unsupervised mode (same as in) or in semisupervised mode.", "labels": [], "entities": [{"text": "CLANN", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.834419846534729}]}, {"text": "The language discriminator in this setting is trained to discriminate between labeled source and labeled target examples, and labeled source and unlabeled target examples.", "labels": [], "entities": []}, {"text": "This is indicated in the Discrim.", "labels": [], "entities": [{"text": "Discrim.", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.853076845407486}]}, {"text": "column using asterisk and prime symbols, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on the test set for our cross-language systems, with and without adversarial  adaptation", "labels": [], "entities": []}, {"text": " Table 2: Comparison of our cross-language ap- proach (CLANN) to the best results at SemEval- 2016 Task 3, subtask B.", "labels": [], "entities": [{"text": "SemEval- 2016 Task 3", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.5841765761375427}]}, {"text": " Table 3: Semi-supervised experiments, when training on half of the training dataset, and evaluating on the  full testing dataset. Shown is the performance of our cross-language models, with and without adversarial  adaptation (i.e., using CLANN and FNN, respectively), using the unsupervised and the semi-supervised  settings, and for both language directions: English-Arabic and Arabic-English. The prime notation in the  Discrim. column represents choosing a counterpart for the discriminator from the unlabeled data. The  asterisks stand for choosing an unpaired labeled example from the other half of the training dataset.", "labels": [], "entities": []}]}