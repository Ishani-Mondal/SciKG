{"title": [{"text": "German in Flux: Detecting Metaphoric Change via Word Entropy", "labels": [], "entities": [{"text": "German in Flux", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8736667633056641}, {"text": "Detecting Metaphoric Change via Word Entropy", "start_pos": 16, "end_pos": 60, "type": "TASK", "confidence": 0.6438875099023184}]}], "abstractContent": [{"text": "This paper explores the information-theoretic measure entropy to detect metaphoric change, transferring ideas from hypernym detection to research on language change.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7674866616725922}]}, {"text": "We also build the first diachronic test set for German as a standard for metaphoric change annotation.", "labels": [], "entities": [{"text": "metaphoric change annotation", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.7729463974634806}]}, {"text": "Our model shows high performance, is unsupervised, language-independent and generalizable to other processes of semantic change.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, computational linguistics has shown an increasing interest in language change.", "labels": [], "entities": [{"text": "language change", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7457650899887085}]}, {"text": "This interest is focused on making semantic change measurable.", "labels": [], "entities": [{"text": "semantic change measurable", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8133262594540914}]}, {"text": "However, even though different types of semantic change are well-known in historical linguistics, little effort has been made to distinguish between them.", "labels": [], "entities": []}, {"text": "Avery basic distinction in historical linguistics is the one between innovative meaning change (also polysemization)-e.g., German br\u00fcten 'breed' > 'breed, brood over sth.'-and reductive meaning change-e.g., German schinden 'to skin, torture' > 'to torture.", "labels": [], "entities": []}, {"text": "Metaphoric meaning change is an important sub-process of innovative meaning change.", "labels": [], "entities": [{"text": "Metaphoric meaning change", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7162310083707174}]}, {"text": "Hence, a computational model of semantic change should be able to distinguish metaphoric change from other-typically less strong-types of change.", "labels": [], "entities": []}, {"text": "Such a model, particularly if applicable to different languages, would be beneficial fora number of areas: (i), historical linguists may test their theoretical claims about semantic change on a large-scale empirical basis going beyond the traditional corpus-based approaches; (ii), linguists and psychologists working on metaphor in language or cognition may benefit by gaining new insights into the diachronic aspects of metaphor which are not yet as central in these fields as the synchronic aspects; and, finally, (iii), the Natural Language Processing research community may benefit by applying the model presented hereto a wide range of tasks in which polysemy and non-literalness are involved.", "labels": [], "entities": []}, {"text": "Our aim is to build an unsupervised and language-independent computational model which is able to distinguish metaphoric change from semantic stability.", "labels": [], "entities": []}, {"text": "We apply entropy (a measure of uncertainty inherited from information theory) to a Distributional Semantic Model (DSM).", "labels": [], "entities": []}, {"text": "In particular, we exploit the idea of semantic generality applied in hypernym detection, to detect metaphoric change as a special process of meaning innovation.", "labels": [], "entities": [{"text": "semantic generality", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7094961255788803}, {"text": "hypernym detection", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7571221888065338}]}, {"text": "German will serve as a sample language, since there is a rich historical corpus available covering a large time period.", "labels": [], "entities": []}, {"text": "Nevertheless, our model is presumably applicable to other languages requiring only minor adjustments.", "labels": [], "entities": []}, {"text": "With the model, we introduce the first resource for evaluation of models of metaphoric change and propose a structured annotation process that is generalizable to the creation of gold standards for other types of semantic change.", "labels": [], "entities": []}, {"text": "In the next section, we give an overview of related work on semantic change and automatic detection of metaphor.", "labels": [], "entities": [{"text": "semantic change", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7528252303600311}, {"text": "automatic detection of metaphor", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.8303536474704742}]}, {"text": "In Section 3, the basic linguistic notions we focus on are introduced and connected to their distributional properties, followed by a description of the corpus used to obtain vector representations of words in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, the information-theoretic measures we apply to word vectors are described.", "labels": [], "entities": []}, {"text": "Section 6 presents the annotation study conducted to create a metaphoric change test set for German.", "labels": [], "entities": []}, {"text": "Section 7 illustrates how the measures' predictions shall be evaluated.", "labels": [], "entities": []}, {"text": "The results are presented and discussed in Section 8.", "labels": [], "entities": []}, {"text": "Section 9 will then conclude and give a short outlook to further research objectives.", "labels": [], "entities": []}], "datasetContent": [{"text": "As with  time period 1 before the starting point of change and in a time period 2 after that.", "labels": [], "entities": []}, {"text": "We then compute the difference din values between period 1 and 2 for each target word and further rank the target words according to d.", "labels": [], "entities": []}, {"text": "Next, we compute the rank correlation between each of these predicted ranks and the gold rank as a performance measure.", "labels": [], "entities": []}, {"text": "Time period 1 is usually the century before and period 2 the century after the century of change, e.g., ausstechen (1739) will be compared in 1600-1700 and 1800-1900.", "labels": [], "entities": [{"text": "ausstechen (1739)", "start_pos": 104, "end_pos": 121, "type": "DATASET", "confidence": 0.7203305959701538}]}, {"text": "(Only for targets from 1800-1900 time period 2 is different, i.e., 1850-1926, since the corpus version we use only contains texts until 1926.)", "labels": [], "entities": []}, {"text": "Stable words are compared in the same time periods as their metaphoric counterparts (see Section 6).", "labels": [], "entities": []}, {"text": "With this procedure we have the possibility to evaluate the measures (i), only on targets from the same century, fixing influential side factors such as corpus size, and (ii), on all targets, which is a much harder task.", "labels": [], "entities": []}, {"text": "(Find a list of time periods with corpus sizes in 8 Results shows Spearman's \u03c1 quantifying the correlation between the measures' predicted ranks and the gold standard rank.", "labels": [], "entities": [{"text": "Spearman's \u03c1 quantifying", "start_pos": 66, "end_pos": 90, "type": "METRIC", "confidence": 0.788162350654602}]}, {"text": "We can directly see that word entropy (H) correlates significantly with the gold rank in different conditions.", "labels": [], "entities": [{"text": "word entropy (H)", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.814426589012146}]}, {"text": "Moreover, the ranking it predicts for targets from 1700-1800 correlates much stronger (.64) with the gold rank than the other measures' predictions.", "labels": [], "entities": []}, {"text": "Note that the correlation is highly significant despite the relatively small sample size.", "labels": [], "entities": [{"text": "correlation", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9835591912269592}]}, {"text": "In the harder condition, where we look at the ranks across different time periods, H still correlates significantly and stronger than all other measures with the gold rank.", "labels": [], "entities": [{"text": "H", "start_pos": 83, "end_pos": 84, "type": "METRIC", "confidence": 0.9947209358215332}]}, {"text": "However, apart from H, the conclusions we can draw about the other measures can only be preliminary, as there is no significance for their predicted ranks.", "labels": [], "entities": []}, {"text": "At first glance, the normalized versions of entropy do not perform as expected: H MON never outperforms frequency and shows even negative correlation in onetime period.", "labels": [], "entities": [{"text": "MON", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.665916383266449}]}, {"text": "Since we reckoned that the reason for this is the low setting of the hyperparameter n = 29 (which we adopted with the intention to construct all vectors from a common number of contexts), we also tested the measure on target words from 1700-1800 with a setting of n such that the maximum number of contexts is used to construct the word vector and the number of vectors to average over k = 10.", "labels": [], "entities": []}, {"text": "In this setting H MON 's prediction has a highly significant correlation with the gold rank which is comparable in strength to H.", "labels": [], "entities": [{"text": "MON", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8649848103523254}, {"text": "gold rank", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9741254448890686}]}, {"text": "Notably, H OLS has the best performance for targets from 1800-1900.", "labels": [], "entities": [{"text": "H", "start_pos": 9, "end_pos": 10, "type": "METRIC", "confidence": 0.7080906629562378}, {"text": "OLS", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.48087453842163086}]}, {"text": "We tried out different hyperparameter settings and found that our initial choice of the data window size n = 1000 may also not have been optimal, as higher n yield better, yet non-significant, results: n = 500/10000/20000/50000 yields \u03c1 = 0.19/0.32/0.31/0.21 respectively, for targets from 1700-1800.", "labels": [], "entities": []}, {"text": "Another factor possibly biasing H OLS are different variances in different corpora or frequency areas which may also connect to our observation that the measure correlates negatively with absolute changes in frequency, i.e., decrease in frequency often leads to increase in H OLS and vice versa.", "labels": [], "entities": [{"text": "OLS", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.6865016222000122}]}, {"text": "H 2 consistently performs poorly.", "labels": [], "entities": []}, {"text": "Moreover, testing of different values for N yields a wide range of \u03c1 values between -0.29 and 0.42 for targets from 1700-1800, not allowing conclusions on the performance of the measure because the correlation is not significant.", "labels": [], "entities": []}, {"text": "Analyzing the predicted ranks reveals interesting insights.", "labels": [], "entities": []}, {"text": "H and its normalized siblings rank Donnerwetter, which is at the top of the gold rank, at the very bottom.", "labels": [], "entities": [{"text": "Donnerwetter", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.7323513031005859}]}, {"text": "This is, presumably, because in its later metaphoric sense 'blowup' the word can be used as an interjection in very short sentences as in (7).", "labels": [], "entities": []}, {"text": "This narrows down Donnerwetter's contextual distribution due to our model only considering words within a sentence as context.", "labels": [], "entities": []}, {"text": "H 2 and frequency are not sensitive to this and rank the word much higher.", "labels": [], "entities": [{"text": "H 2", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8955396413803101}, {"text": "frequency", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9733124375343323}]}, {"text": "This shows that, (i), different factors play a role in determining the contextual distribution of a word suggesting that a model of semantic change should incorporate different types of information and, (ii), that H 2 and frequency may still be helpful in detecting metaphoric change in certain settings.", "labels": [], "entities": []}, {"text": "The dominance of H may also be a hint to this direction: Word entropy combines frequency and contextual distribution as it is influenced by both.", "labels": [], "entities": []}, {"text": "Feder and Haube from the very bottom of the gold rank are not beyond the bottom-items of any measure's prediction.", "labels": [], "entities": []}, {"text": "In H's prediction, which is the best-performing measure, they rank near the middle.", "labels": [], "entities": []}, {"text": "This indicates that their position at the bottom of the gold rank may not accurately reflect the semantic change they underwent.", "labels": [], "entities": []}, {"text": "Similarly for the adjectives freundlich and fett ranking in all predictions near middle or lower (for H: 18, 10).", "labels": [], "entities": []}, {"text": "We still have to assess how these words behave in future studies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotation results divided into judgments for ear-", "labels": [], "entities": []}, {"text": " Table 2: Summary of the predictions of word entropy", "labels": [], "entities": []}, {"text": " Table 3: Time periods for evaluation and their re- spective corpus sizes after preprocessing.", "labels": [], "entities": []}, {"text": " Table 4: Example Annotation Table", "labels": [], "entities": [{"text": "Example Annotation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6486282646656036}]}, {"text": " Table 5: Historical data: sta (stable), met (metaphoric).", "labels": [], "entities": []}]}