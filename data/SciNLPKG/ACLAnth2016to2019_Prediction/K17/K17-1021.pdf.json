{"title": [{"text": "A Supervised Approach to Extractive Summarisation of Scientific Papers", "labels": [], "entities": [{"text": "Extractive Summarisation of Scientific Papers", "start_pos": 25, "end_pos": 70, "type": "TASK", "confidence": 0.8391639947891235}]}], "abstractContent": [{"text": "Automatic summarisation is a popular approach to reduce a document to its main arguments.", "labels": [], "entities": [{"text": "Automatic summarisation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.548153281211853}]}, {"text": "Recent research in the area has focused on neural approaches to summari-sation, which can be very data-hungry.", "labels": [], "entities": []}, {"text": "However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further.", "labels": [], "entities": [{"text": "summarisation of computer science publications", "start_pos": 45, "end_pos": 91, "type": "TASK", "confidence": 0.8623083472251892}]}, {"text": "We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisa-tion features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic summarisation is the task of reducing a document to its main points.", "labels": [], "entities": [{"text": "Automatic summarisation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5856267809867859}]}, {"text": "There are two streams of summarisation approaches: extractive summarisation, which copies parts of a document (often whole sentences) to form a summary, and abstractive summarisation, which reads a document and then generates a summary from it, which can contain phrases not appearing in the document.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9785229563713074}, {"text": "extractive summarisation", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.592308983206749}]}, {"text": "Abstractive summarisation is the more difficult task, but useful for domains where sentences taken out of context are not a good basis for forming a grammatical and coherent summary, like novels.", "labels": [], "entities": [{"text": "Abstractive summarisation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.770349383354187}]}, {"text": "Here, we are concerned with summarising scientific publications.", "labels": [], "entities": [{"text": "summarising scientific publications", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.9041850765546163}]}, {"text": "Since scientific publications area technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation.", "labels": [], "entities": [{"text": "extractive summarisation", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.7850868105888367}]}, {"text": "Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (.", "labels": [], "entities": [{"text": "summarisation of scientific publications", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.8988189548254013}]}, {"text": "Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.9078631401062012}, {"text": "sentence and document encoding", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.6501054689288139}]}, {"text": "In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation.", "labels": [], "entities": [{"text": "summarisation of computer science publications", "start_pos": 52, "end_pos": 98, "type": "TASK", "confidence": 0.8505797147750854}, {"text": "extractive summarisation", "start_pos": 142, "end_pos": 166, "type": "TASK", "confidence": 0.49467162787914276}]}, {"text": "It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains.", "labels": [], "entities": []}, {"text": "The dataset is created by exploiting an existing resource, ScienceDirect, 1 where many journals require authors to submit highlight statements along with their manuscripts.", "labels": [], "entities": []}, {"text": "Using such highlight statements as gold statements has been proven a good gold standard for news documents ().", "labels": [], "entities": []}, {"text": "This new dataset offers many exciting research challenges, such how best to encode very large technical documents, which are largely ignored by current research.", "labels": [], "entities": []}, {"text": "In more detail, our contributions are as follows: \u2022 We introduce anew dataset for summarisation of scientific publications consisting of over 10k documents \u2022 Following the approach of () in the news domain, we introduce a method, HighlightROUGE, which can be used to automatically extend this dataset and Paper Title Statistical estimation of the names of HTTPS servers with domain name graphs Highlights we present the domain name graph (DNG), which is a formal expression that can keep track of cname chains and characterize the dynamic and diverse nature of DNS mechanisms and deployments.", "labels": [], "entities": [{"text": "summarisation of scientific publications", "start_pos": 82, "end_pos": 122, "type": "TASK", "confidence": 0.8768421858549118}]}, {"text": "We develop a framework called service-flow map (sfmap) that works on top of the DNG.sfmap estimates the hostname of an HTTPS server when given a pair of client and server IP addresses.", "labels": [], "entities": []}, {"text": "It can statistically estimate the hostname even when associating DNS queries are unobserved due to caching mechanisms, etc through extensive analysis using real packet traces, we demonstrate that the sfmap framework establishes good estimation accuracies and can outperform the state-of-the art technique called dnhunter.", "labels": [], "entities": []}, {"text": "We also identify the optimized setting of the sfmap framework.", "labels": [], "entities": []}, {"text": "The experiment results suggest that the success of the sfmap lies in the fact that it can complement incomplete DNS information by leveraging the graph structure.", "labels": [], "entities": []}, {"text": "To cope with large-scale measurement data, we introduce techniques to make the sfmap framework scalable.", "labels": [], "entities": []}, {"text": "We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links . Summary Statements Highlighted in Context from Section of Main Text Contributions: in this work, we present a novel methodology that aims to infer the hostnames of HTTPS flows, given the three research challenges shown above.", "labels": [], "entities": []}, {"text": "The key contributions of this work are summarized as follows.", "labels": [], "entities": []}, {"text": "We present the domain name graph (DNG), which is a formal expression that can keep track of cname chains (challenge 1) and characterize the dynamic and diverse nature of DNS mechanisms and deployments (challenge 3).", "labels": [], "entities": []}, {"text": "We develop a framework called service-flow map (sfmap) that works on top of the DNG.", "labels": [], "entities": []}, {"text": "sfmap estimates the hostname of an https server when given a pair of client and server IP addresses.", "labels": [], "entities": []}, {"text": "It can statistically estimate the hostname even when associating DNS queries are unobserved due to caching mechanisms, etc (challenge 2).", "labels": [], "entities": []}, {"text": "Through extensive analysis using real packet traces , we demonstrate that the sfmap framework establishes good estimation accuracies and can outperform the state-of-the art technique called dn-hunter,.", "labels": [], "entities": []}, {"text": "We also identify the optimized setting of the sfmap framework.", "labels": [], "entities": []}, {"text": "The experiment results suggest that the success of the sfmap lies in the fact that it can complement incomplete DNS information by leveraging the graph structure.", "labels": [], "entities": []}, {"text": "To cope with large-scale measurement data, we introduce techniques to make the sfmap framework scalable.", "labels": [], "entities": []}, {"text": "We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: section2 summarizes the related work.", "labels": [], "entities": []}, {"text": "[...]: An example of a document with summary statements highlighted in context.", "labels": [], "entities": []}, {"text": "show empirically that this improves summarisation performance \u2022 Taking inspiration from previous work in summarising scientific literature (, we introduce a metric we use as a feature, AbstractROUGE, which can be used to extract summaries by exploiting the abstract of a paper \u2022 We benchmark several neural as well traditional summarisation methods on the dataset and use simple features to model the global context of a summary statement, which contribute most to the overall score \u2022 We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation", "labels": [], "entities": [{"text": "summarisation", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9921715259552002}, {"text": "summarising scientific literature", "start_pos": 105, "end_pos": 138, "type": "TASK", "confidence": 0.9210275808970133}, {"text": "AbstractROUGE", "start_pos": 185, "end_pos": 198, "type": "METRIC", "confidence": 0.9882885217666626}, {"text": "summarisation", "start_pos": 726, "end_pos": 739, "type": "TASK", "confidence": 0.7568445205688477}]}], "datasetContent": [{"text": "We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.", "labels": [], "entities": [{"text": "extractive summarisation", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.9142295122146606}]}, {"text": "Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them.", "labels": [], "entities": []}, {"text": "As such, the dataset could easily be extended to more domains.", "labels": [], "entities": []}, {"text": "An example document is shown in.", "labels": [], "entities": []}, {"text": "Each paper in this dataset is guaranteed to have a title, abstract, author written highlight statements and author defined keywords.", "labels": [], "entities": []}, {"text": "The highlight statements are sentences that should effectively convey the main takeaway of each paper and area good gold summary, while the keyphrases are the key topics of the paper.", "labels": [], "entities": []}, {"text": "Both abstract and highlights can bethought of as a summary of a paper.", "labels": [], "entities": []}, {"text": "Since highlight statements, unlike sentences in the abstract, generally do not have dependencies between them, we opt to use those as gold summary statements for developing our summarisation models, following; in their approaches to news summarisation.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 177, "end_pos": 190, "type": "TASK", "confidence": 0.9625258445739746}, {"text": "news summarisation", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.6652352660894394}]}], "tableCaptions": [{"text": " Table 2:  The CSPubSum and CSPubSumExt  datasets as described in Section 2.2. Instances are  items of training data.", "labels": [], "entities": [{"text": "CSPubSum", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.9653627276420593}, {"text": "CSPubSumExt  datasets", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.9400000274181366}]}]}