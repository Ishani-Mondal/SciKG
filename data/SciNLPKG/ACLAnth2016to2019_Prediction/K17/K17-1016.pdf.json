{"title": [{"text": "Learning Word Representations with Regularization from Prior Knowledge", "labels": [], "entities": [{"text": "Learning Word Representations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6115060746669769}]}], "abstractContent": [{"text": "Conventional word embeddings are trained with specific criteria (e.g., based on language modeling or co-occurrence) inside a single information source, disregarding the opportunity for further calibration using external knowledge.", "labels": [], "entities": []}, {"text": "This paper presents a unified framework that leverages pre-learned or external priors, in the form of a regularizer, for enhancing conventional language model-based embedding learning.", "labels": [], "entities": []}, {"text": "We consider two types of regularizers.", "labels": [], "entities": []}, {"text": "The first type is derived from topic distribution by running latent Dirich-let allocation on unlabeled data.", "labels": [], "entities": []}, {"text": "The second type is based on dictionaries that are created with human annotation efforts.", "labels": [], "entities": []}, {"text": "To effectively learn with the regularizers, we propose a novel data structure, trajectory softmax, in this paper.", "labels": [], "entities": []}, {"text": "The resulting em-beddings are evaluated byword similarity and sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.8991812765598297}]}, {"text": "Experimental results show that our learning framework with regularization from prior knowledge improves embedding quality across multiple datasets, compared to a diverse collection of baseline methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representation of words (or word embedding) has been demonstrated to be effective in many natural language processing (NLP) tasks (.", "labels": [], "entities": [{"text": "Distributed representation of words", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9007377922534943}]}, {"text": "Conventional word embeddings are trained with a single objective function (e.g., language modeling) or word co-occurrence factorization (), which restricts the capability of the learned embeddings from integrating other types of knowledge.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.701553225517273}]}, {"text": "Prior work has leveraged relevant sources to obtain embeddings that are best suited for the target tasks, such as using a sentiment lexicon to enhance embeddings for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.8856111168861389}]}, {"text": "However, learning word embeddings with a particular target makes the approach less generic, also implying that customized adaptation has to be made whenever anew knowledge source is considered.", "labels": [], "entities": []}, {"text": "Along the lines of improving embedding quality, semantic resources have been incorporated as guiding knowledge to refine objective functions in a joint learning framework (, or used for retrofitting based on word relations defined in the semantic lexicons).", "labels": [], "entities": []}, {"text": "These approaches, nonetheless, require explicit word relations defined in semantic resources, which is a difficult prerequisite for knowledge preparation.", "labels": [], "entities": [{"text": "knowledge preparation", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.6979758888483047}]}, {"text": "Given the above challenges, we propose a novel framework that extends typical context learning by integrating external knowledge sources for enhancing embedding learning.", "labels": [], "entities": []}, {"text": "Compared to a well known work by that focused on tackling the task using a retrofitting 1 framework on semantic lexicons, our method has an emphasis on joint learning where two objectives are considered for optimization simultaneously.", "labels": [], "entities": []}, {"text": "In the meantime, we design a general-purpose infrastructure which can incorporate arbitrary external sources into learning as long as the sources can be encoded into vectors of numerical values (e.g. multi-hot vector according to the topic distributions from a topic model).", "labels": [], "entities": []}, {"text": "In prior work by and, the ex-ternal knowledge has to be clustered beforehand according to their semantic relatedness (e.g., cold, icy, winter, frozen), and words of similar meanings are added as part of context for learning.", "labels": [], "entities": []}, {"text": "This may set a high bar for preparing external knowledge since finding the precise word-word relations is required.", "labels": [], "entities": []}, {"text": "Our infrastructure, on the other hand, is more flexible as knowledge that is learned elsewhere, such as from topic modeling or even a sentiment lexicon, can be easily encoded and incorporated into the framework to enrich embeddings.", "labels": [], "entities": []}, {"text": "The way we integrate external knowledge is performed by the notion of a regularizer, which is an independent component that can be connected to the two typical architectures, namely, continuous bag-of-words (CBOW) and skip-gram (SG), or used independently as a retrofitter.", "labels": [], "entities": []}, {"text": "We construct the regularizers based on the knowledge learned from both unlabeled data and manually crafted information sources.", "labels": [], "entities": []}, {"text": "As an example of the former, a topic model from latent Dirichlet allocation (LDA) () is first generated from a given corpus, based on which per-word topical distributions are then added as extra signals to aid embedding learning.", "labels": [], "entities": []}, {"text": "As an example of the latter, one can encode a dictionary into the regularizer and thus adapt the learning process with the encoded knowledge.", "labels": [], "entities": []}, {"text": "Another contribution of this paper is that we propose a novel data structure, trajectory softmax, to effectively learn prior knowledge in the regularizer.", "labels": [], "entities": []}, {"text": "Compared to conventional tree based hierarchical softmax, trajectory softmax can greatly reduce the space complexity when learning over a high-dimension vector.", "labels": [], "entities": []}, {"text": "Our experimental results on several different tasks have demonstrated the effectiveness of our approach compared to up-todate studies.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe in detail our framework and show how we learn the regularizer in section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents and analyzes our experimental results and section 5 surveys related work.", "labels": [], "entities": []}, {"text": "Finally, conclusions and directions of future work are discussed in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The resulting word embeddings based on joint learning as well as retrofitting are evaluated intrinsically and extrinsically.", "labels": [], "entities": []}, {"text": "For intrinsic evaluation, we use word similarity benchmark to directly test the quality of the learned embeddings.", "labels": [], "entities": []}, {"text": "For extrinsic evaluation, we use sentiment analysis as a downstream task with different input embeddings.", "labels": [], "entities": [{"text": "extrinsic evaluation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7431454062461853}, {"text": "sentiment analysis", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9425777792930603}]}, {"text": "Regularizers based on LDA, PPDB and WN syn are used in word similarity experiment, while SentiWordNet regularization is used in sentiment analysis.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7354220747947693}, {"text": "sentiment analysis", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.9600237309932709}]}, {"text": "The experimental results will be discussed in \u00a74.1 and \u00a74.2.", "labels": [], "entities": []}, {"text": "We experiment with three learning paradigms, namely CBOW, SG and GloVe.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.7723107933998108}, {"text": "GloVe", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.8609968423843384}]}, {"text": "GloVe is only tested in retrofitting since our regularizer is not compatible with GloVe learning objective in joint learning.", "labels": [], "entities": []}, {"text": "In all of our retrofitting experiments, we only train the regularizer with one iteration, consistent with.", "labels": [], "entities": []}, {"text": "The base corpus that we used to train initial word embeddings is from the latest articles dumped from Wikipedia and newswire , which contains approximately 8 billion words.", "labels": [], "entities": []}, {"text": "When training on this corpus, we set the dimension of word embeddings to be 200 and cutoff threshold of word frequency threshold to be 5 times of occurrence.", "labels": [], "entities": [{"text": "cutoff threshold", "start_pos": 84, "end_pos": 100, "type": "METRIC", "confidence": 0.9451799094676971}]}, {"text": "These are common setups shared across the following experiments.", "labels": [], "entities": []}, {"text": "We use the MEN-3k (), SimLex-999 (  and) datasets to perform quantitative comparisons among different approaches to generating embeddings.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.860927164554596}]}, {"text": "The cosine scores are computed between the vectors of each pair of words in the datasets 8 . The measures adopted are Pearson's coefficient of product-moment correlation (\u03b3) and Spearman's rank correlation (\u03c1), which reflect how  We perform sentiment classification on the IMDB review data set), which has 50K labeled samples with equal number of positive and negative reviews.", "labels": [], "entities": [{"text": "Pearson's coefficient of product-moment correlation (\u03b3)", "start_pos": 118, "end_pos": 173, "type": "METRIC", "confidence": 0.9033450682957967}, {"text": "Spearman's rank correlation (\u03c1)", "start_pos": 178, "end_pos": 209, "type": "METRIC", "confidence": 0.7709967706884656}, {"text": "sentiment classification", "start_pos": 241, "end_pos": 265, "type": "TASK", "confidence": 0.8904728889465332}, {"text": "IMDB review data set", "start_pos": 273, "end_pos": 293, "type": "DATASET", "confidence": 0.9535440057516098}]}, {"text": "The data set is pre-divided into training and test sets, with each set containing 25K reviews.", "labels": [], "entities": []}, {"text": "The classifier is based on a bi-directional LSTM model as described in, with one hidden layer of 1024 units.", "labels": [], "entities": []}, {"text": "Embeddings from different approaches are used as inputs for the LSTM classifier.", "labels": [], "entities": [{"text": "LSTM classifier", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.6800829768180847}]}, {"text": "For determining the hyperparameters (e.g., training epoch and learning rate), we use 15% of the training data as the validation set and we apply early stopping strategy when the error rate on the validation set starts to increase.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 145, "end_pos": 159, "type": "METRIC", "confidence": 0.9610385298728943}]}, {"text": "Note that the final model for testing is trained on the entire training set.", "labels": [], "entities": []}, {"text": "As reported in, the embeddings trained by our approach work effectively for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.9655373394489288}]}, {"text": "Both joint learning and retrofitting with our regularizer outperform other baseline approaches from previous studies, with joint learning being somewhat better than retrofitting.", "labels": [], "entities": []}, {"text": "Overall, our joint learning with CBOW achieves the best performance on this task.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.932232677936554}]}, {"text": "A ten-partition twotailed paired t-test at p < 0.05 level is performed on comparing each score with the baseline result for each embedding type.", "labels": [], "entities": []}, {"text": "Considering that sentiment is not directly related to word meaning, the results indicate that our regularizer is capable of incorporating different type of knowledge fora specific task, even if it is not aligned with the context learning.", "labels": [], "entities": []}, {"text": "This task demonstrates the potential of our framework for encoding external knowledge and using it to enrich the representa-  tions of words, without the requirement to build a task-specific, customized model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word similarity results for joint learning on three datasets in terms of Pearson's coefficient cor- relation (\u03b3) and Spearman's rank correlation (\u03c1) in percentages. Higher score indicates better correlation  of the model with respect to the gold standard. Bold indicates the highest score for each embedding type.", "labels": [], "entities": [{"text": "Pearson's coefficient cor- relation (\u03b3)", "start_pos": 83, "end_pos": 122, "type": "METRIC", "confidence": 0.8175775673654344}, {"text": "Spearman's rank correlation (\u03c1)", "start_pos": 127, "end_pos": 158, "type": "METRIC", "confidence": 0.7679059888635363}]}, {"text": " Table 2: Word similarity results for retrofitting on three datasets in terms of Pearson's coefficient corre- lation (\u03b3) and Spearman's rank correlation (\u03c1) in percentages. Higher score indicates better correlation  of the model with respect to the gold standard. Bold indicates the highest score for each embedding type.", "labels": [], "entities": [{"text": "Pearson's coefficient corre- lation (\u03b3)", "start_pos": 81, "end_pos": 120, "type": "METRIC", "confidence": 0.8861260877715217}, {"text": "Spearman's rank correlation (\u03c1)", "start_pos": 125, "end_pos": 156, "type": "METRIC", "confidence": 0.7831333790506635}]}, {"text": " Table 3: Sentiment classification results on IMDB  data set (Maas et al., 2011). Bold indicates the  highest score for each embedding type. * indi- cates t-test significance at p < 0.05 level when  compared with the baseline.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9496622085571289}, {"text": "IMDB  data set", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.9773706992467245}, {"text": "indi- cates t-test significance", "start_pos": 143, "end_pos": 174, "type": "METRIC", "confidence": 0.8330497980117798}]}]}