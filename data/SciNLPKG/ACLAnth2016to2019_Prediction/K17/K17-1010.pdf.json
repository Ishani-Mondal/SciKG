{"title": [{"text": "Learning What is Essential in Questions", "labels": [], "entities": []}], "abstractContent": [{"text": "Question answering (QA) systems are easily distracted by irrelevant or redundant words in questions, especially when faced with long or multi-sentence questions in difficult domains.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9004233717918396}]}, {"text": "This paper introduces and studies the notion of essential question terms with the goal of improving such QA solvers.", "labels": [], "entities": [{"text": "QA solvers", "start_pos": 105, "end_pos": 115, "type": "TASK", "confidence": 0.8876765668392181}]}, {"text": "We illustrate the importance of essential question terms by showing that humans' ability to answer questions drops significantly when essential terms are eliminated from questions.", "labels": [], "entities": []}, {"text": "We then develop a classifier that reliably (90% mean average precision) identifies and ranks essential terms in questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.6693642139434814}]}, {"text": "Finally , we use the classifier to demonstrate that the notion of question term essen-tiality allows state-of-the-art QA solvers for elementary-level science questions to make better and more informed decisions, improving performance by up to 5%.", "labels": [], "entities": [{"text": "QA solvers for elementary-level science questions", "start_pos": 118, "end_pos": 167, "type": "TASK", "confidence": 0.7712165613969167}]}, {"text": "We also introduce anew dataset of over 2,200 crowd-sourced essential terms annotated science questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding what a question is really about is a fundamental challenge for question answering systems that operate with a natural language interface.", "labels": [], "entities": [{"text": "Understanding what a question is really about", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8143829958779472}, {"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7831719219684601}]}, {"text": "In domains with multi-sentence questions covering a wide array of subject areas, such as standardized tests for elementary level science, the challenge is even more pronounced.", "labels": [], "entities": []}, {"text": "Many QA systems in such domains \u2020 Most of the work was done when the first and last authors were affiliated with the University of Illinois, UrbanaChampaign.", "labels": [], "entities": []}, {"text": "derive significant leverage from relatively shallow Information Retrieval (IR) and statistical correlation techniques operating on large unstructured corpora (.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.8095942497253418}]}, {"text": "Inference based QA systems operating on (semi-)structured knowledge formalisms have also demonstrated complementary strengths, by using optimization formalisms such as Semantic Parsing (), Integer Linear Program (ILP) ( , and probabilistic logic formalisms such as Markov Logic Networks (MLNs) (.", "labels": [], "entities": []}, {"text": "These QA systems, however, often struggle with seemingly simple questions because they are unable to reliably identify which question words are redundant, irrelevant, or even intentionally distracting.", "labels": [], "entities": []}, {"text": "This reduces the systems' precision and results in questionable \"reasoning\" even when the correct answer is selected among the given alternatives.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9990942478179932}]}, {"text": "The variability of subject domain and question style makes identifying essential question words challenging.", "labels": [], "entities": []}, {"text": "Further, essentiality is context dependent-a word like 'animals' can be critical for one question and distracting for another.", "labels": [], "entities": []}, {"text": "Consider the following example: One way animals usually respond to a sudden drop in temperature is by (A) sweating (B) shivering (C) blinking (D) salivating.", "labels": [], "entities": []}, {"text": "A state-of-the-art optimization based QA system called TableILP ( ), which performs reasoning by aligning the question to semi-structured knowledge, aligns only the word 'animals' when answering this question.", "labels": [], "entities": []}, {"text": "Not surprisingly, it chooses an incorrect answer.", "labels": [], "entities": []}, {"text": "The issue is that it does not recognize that \"drop in temperature\" is an essential aspect of the question.", "labels": [], "entities": []}, {"text": "Towards this goal, we propose a system that can assign an essentiality score to each term in the question.", "labels": [], "entities": []}, {"text": "For the above example, our system gen-", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected 2,223 elementary school science exam questions for the annotation of essential terms.", "labels": [], "entities": []}, {"text": "This set includes the questions used by and additional ones obtained from other public resources such as the Internet or textbooks.", "labels": [], "entities": []}, {"text": "For each of these questions, we asked crowd workers 3 to annotate essential question terms based on the above criteria as well as a few examples of essential and non-essential terms.", "labels": [], "entities": []}, {"text": "The questions were annotated by 5 crowd workers, and resulted in 19,380 annotated terms.", "labels": [], "entities": []}, {"text": "The Fleiss' kappa statistic for this task was \u03ba = 0.58, indicating a level of inter-annotator agreement very close to 'substantial'.", "labels": [], "entities": []}, {"text": "In particular, all workers agreed on 36.5% of the terms and at least 4 agreed on 69.9% of the terms.", "labels": [], "entities": []}, {"text": "We use the proportion of workers that marked a term as essential to be its annotated essentiality score.", "labels": [], "entities": []}, {"text": "On average, less than one-third (29.9%) of the terms in each question were marked as essential (i.e., score > 0.5).", "labels": [], "entities": []}, {"text": "This shows the large proportion of distractors in these science tests (as compared to traditional QA datasets), further showing the importance of this task.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.7165130078792572}]}, {"text": "Next we provide some insights into these terms.", "labels": [], "entities": []}, {"text": "We found that part-of-speech (POS) tags are not a reliable predictor of essentiality, making it difficult to hand-author POS tag based rules.", "labels": [], "entities": []}, {"text": "Among the proper nouns (NNP, NNPS) mentioned in the questions, fewer than half (47.0%) were marked as essential.", "labels": [], "entities": []}, {"text": "This is in contrast with domains such as news articles where proper nouns carry perhaps the most important information.", "labels": [], "entities": []}, {"text": "Nearly twothirds (65.3%) of the mentioned comparative adjectives (JJR) were marked as essential, whereas only a quarter of the mentioned superlative adjectives (JJS) were deemed essential.", "labels": [], "entities": []}, {"text": "Verbs were marked essential less than a third (32.4%) of the time.", "labels": [], "entities": [{"text": "Verbs were marked essential", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.8766032755374908}]}, {"text": "This differs from domains such as math word problems where verbs have been found to play a key role ().", "labels": [], "entities": []}, {"text": "The best single indicator of essential terms, not surprisingly, was being a scientific term 5 (such as precipitation and gravity).", "labels": [], "entities": []}, {"text": "76.6% of such terms occurring in questions were marked as essential.", "labels": [], "entities": []}, {"text": "In summary, we have a term essentiality annotated dataset of 2,223 questions.", "labels": [], "entities": []}, {"text": "We split this into train/development/test subsets in a 70/9/21 ratio, resulting in 483 test sentences used for perquestion evaluation.", "labels": [], "entities": []}, {"text": "We also derive from the above an annotated dataset of 19,380 terms by pooling together all terms across all questions.", "labels": [], "entities": []}, {"text": "Each term in this larger dataset is annotated with an essentiality score in the context of the question it appears in.", "labels": [], "entities": []}, {"text": "This results in 4,124 test instances (derived from the above 483 test questions).", "labels": [], "entities": []}, {"text": "We use this dataset for per-term evaluation.", "labels": [], "entities": []}, {"text": "We consider two natural evaluation metrics for essentiality detection, first treating it as a binary prediction task at the level of individual terms and then as a task of ranking terms within each question by the degree of essentiality.", "labels": [], "entities": [{"text": "essentiality detection", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.7674161195755005}]}, {"text": "We consider all question terms pooled together as described in Section 2.1, resulting in a dataset of 19,380 terms annotated (in the context of the corresponding question) independently as essential or not.", "labels": [], "entities": []}, {"text": "The ET classifier is trained on the train subset, and the threshold is tuned using the dev subset.", "labels": [], "entities": [{"text": "ET", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9175435304641724}]}, {"text": "For each term in the corresponding test set of 4,124 instances, we use various methods to predict whether the term is essential (for the corresponding question) or not.", "labels": [], "entities": []}, {"text": "For the threshold-based scores, each method was tuned to maximize the F1 score based on the dev set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9860040247440338}]}, {"text": "The ET classifier achieves an F1 score of 0.80, which is 5%-14% higher than the baselines.", "labels": [], "entities": [{"text": "ET", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9649930000305176}, {"text": "F1 score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9889953434467316}]}, {"text": "Its accuracy at 0.75 is statistically significantly better than all baselines based on the Binomial 9 exact test at p-value 0.05.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996730089187622}, {"text": "Binomial 9 exact test", "start_pos": 91, "end_pos": 112, "type": "DATASET", "confidence": 0.749753937125206}]}, {"text": "As noted earlier, each of these essentiality identification methods are parameterized by a threshold for balancing precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9991618394851685}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9963937401771545}]}, {"text": "This allows them to be tuned for end-to-end performance of the downstream task.", "labels": [], "entities": []}, {"text": "We use this feature later when incorporating the ET classifier in QA systems.", "labels": [], "entities": []}, {"text": "depicts the PR curves for various methods as the threshold is varied, highlighting that the ET classifier performs reliably at various recall points.", "labels": [], "entities": [{"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9838200807571411}]}, {"text": "Its precision, when tuned to optimize F1, is 0.91, which is very suitable for Each test term prediction is assumed to be a binomial.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990150928497314}, {"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9990999698638916}]}, {"text": "It has a 5% higher AUC (area under the curve) and outperforms baselines by roughly 5% throughout the precisionrecall spectrum.", "labels": [], "entities": [{"text": "AUC", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9979082345962524}, {"text": "precisionrecall", "start_pos": 101, "end_pos": 116, "type": "METRIC", "confidence": 0.947603166103363}]}, {"text": "As a second study, we assess how well our classifier generalizes to unseen terms.", "labels": [], "entities": []}, {"text": "For this, we consider only the 559 test terms that do not appear in the train set.", "labels": [], "entities": []}, {"text": "provides the resulting performance metrics.", "labels": [], "entities": []}, {"text": "We see that the frequency based supervised baselines, having never seen the test terms, stay close to the default precision of 0.5.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9851843118667603}]}, {"text": "The unsupervised baselines, by nature, generalize much better but are substantially dominated by our ET classifier, which achieves an F1 score of 78%.", "labels": [], "entities": [{"text": "ET", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.947851836681366}, {"text": "F1 score", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9899632334709167}]}, {"text": "This is only 2% below its own F1 across all seen and unseen terms, and 6% higher than the second best baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9986392855644226}]}], "tableCaptions": [{"text": " Table 1: Effectiveness of various methods for  identifying essential question terms in the test set,  including area under the PR curve (AUC), accu- racy (Acc), precision (P), recall (R), and F1 score.  ET classifier substantially outperforms all super- vised and unsupervised (denoted with  \u2020 ) baselines.", "labels": [], "entities": [{"text": "PR curve (AUC)", "start_pos": 128, "end_pos": 142, "type": "METRIC", "confidence": 0.874322509765625}, {"text": "accu- racy (Acc)", "start_pos": 144, "end_pos": 160, "type": "METRIC", "confidence": 0.960721621910731}, {"text": "precision (P)", "start_pos": 162, "end_pos": 175, "type": "METRIC", "confidence": 0.9609639197587967}, {"text": "recall (R)", "start_pos": 177, "end_pos": 187, "type": "METRIC", "confidence": 0.9595381319522858}, {"text": "F1 score", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9895200431346893}]}, {"text": " Table 2: Generalization to unseen terms: Effec- tiveness of various methods, using the same met- rics as in Table 1. As expected, supervised meth- ods perform poorly, similar to a random baseline.  Unsupervised methods generalize well, but the ET  classifier again substantially outperforms them.", "labels": [], "entities": [{"text": "Effec-", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9589554369449615}]}, {"text": " Table 3: Effectiveness of various methods for  ranking the terms in a question by essentiality.   \u2020 indicates unsupervised method. Mean-Average  Precision (MAP) numbers reflect the mean (across  all test set questions) of the average precision of  the term ranking for each question. ET classifier  again substantially outperforms all baselines.", "labels": [], "entities": [{"text": "Mean-Average  Precision (MAP) numbers", "start_pos": 132, "end_pos": 169, "type": "METRIC", "confidence": 0.9772904415925344}, {"text": "ET", "start_pos": 285, "end_pos": 287, "type": "METRIC", "confidence": 0.9633400440216064}]}, {"text": " Table 4: Performance of the IR solver without  (Basic IR) and with (IR + ET) essential terms. The  numbers are solver scores (%) on the test sets of  the three datasets.", "labels": [], "entities": [{"text": "IR solver", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9445035457611084}, {"text": "IR + ET)", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.7371526658535004}]}]}