{"title": [{"text": "Experiments on Morphological Reinflection: CoNLL-2017 Shared Task", "labels": [], "entities": [{"text": "Morphological Reinflection", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.810920000076294}, {"text": "CoNLL-2017", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.7726917862892151}]}], "abstractContent": [{"text": "We present two systems for the task of morphological inflection, i.e., finding a target morphological form, given a lemma and a set of target tags.", "labels": [], "entities": []}, {"text": "Both are trained on datasets of three sizes: low, medium and high.", "labels": [], "entities": []}, {"text": "The first uses a simple Long Short-Term Memory (LSTM) for low-sized dataset, while it uses an LSTM-based encoder-decoder based model for the medium and high sized datasets.", "labels": [], "entities": []}, {"text": "The second uses a simple Gated Recurrent Unit (GRU) for low-sized data, while it uses a combination of simple LSTMs, simple GRUs, stacked GRUs and encoder-decoder models, depending on the language , for medium-sized data.", "labels": [], "entities": []}, {"text": "Though the systems are not very complex, they give accuracies above baseline accuracies on high-sized datasets, around baseline accuracies for medium-sized datasets but mostly accuracies lower than baseline for low-sized datasets.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9923539161682129}]}], "introductionContent": [{"text": "The CoNLL-SIGMOPRHON 2017 shared task consists of two subtasks out of which we participate only in the first subtask, which involves generating a target inflected form from a given lemma with its part-of-speech.", "labels": [], "entities": [{"text": "CoNLL-SIGMOPRHON 2017 shared task", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.8208453059196472}]}, {"text": "For instance, the word writing is the present continuous inflected form of the lemma write.", "labels": [], "entities": []}, {"text": "The models were trained on three differently-sized datasets.", "labels": [], "entities": []}, {"text": "The low-sized datasets had around 100 training samples, the medium-sized datasets had around 1000 training samples and the high-sized datasets had around 10000 samples for most languages.", "labels": [], "entities": []}, {"text": "Datasets were provided fora total of 52 languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "For training the model on the low-sized dataset, we did not use any encoder and we used a simple LSTM with a single layer as the recurrent unit ().", "labels": [], "entities": []}, {"text": "For medium-sized dataset, we used different model configurations for different languages.", "labels": [], "entities": []}, {"text": "Four different kinds of configurations were used: 1) Bidirectional LSTM as the encoder and a simple LSTM with a single layer as the decoder 2) Bidirectional GRU as the encoder and a simple GRU with a single layer as the decoder 3) No encoder and a simple GRU with a single layer as the recurrent unit () 4) Bidirectional GRU as the encoder and a deep GRU (two GRUs stacked one above the other) as the decoder () The specific configuration used for each language has been listed in.", "labels": [], "entities": []}, {"text": "The configuration numbers indicated in the table are according to those mentioned above.", "labels": [], "entities": []}, {"text": "For high-sized data, we were unable to complete experiments for the second submission due to lack of time.", "labels": [], "entities": []}, {"text": "However, we have been able to perform      ., C n represent characters of the root word while O 1 , .., O n represent characters of the output word some ablation studies on high-size datasets, which have been discussed in the analysis section.", "labels": [], "entities": []}, {"text": "For training the model on the low-sized dataset, we did not use any encoder and we used a simple GRU, as reported by, with a single layer as the recurrent unit ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Configurations for different languages for medium-sized data for submission-2.", "labels": [], "entities": []}, {"text": " Table 2: Accuracies for top-5 languages for low  data.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9981192946434021}]}, {"text": " Table 3: Accuracies for top-5 languages for  medium data.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9973601698875427}]}, {"text": " Table 4: Accuracies for top-5 languages for high  data.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9977883100509644}]}, {"text": " Table 5: Levenshtein distances for top-5 languages  for low data.", "labels": [], "entities": []}, {"text": " Table 6: Levenshtein distances for top-5 languages  for medium data.", "labels": [], "entities": []}, {"text": " Table 7: Levenshtein distances for top-5 languages  for high data.", "labels": [], "entities": []}, {"text": " Table 8: Results for all languages for high data,  sorted by submission-1 accuracy", "labels": [], "entities": []}, {"text": " Table 9: Results for all languages for medium  data, sorted by submission-1 accuracy", "labels": [], "entities": []}]}