{"title": [{"text": "Neural Domain Adaptation for Biomedical Question Answering", "labels": [], "entities": [{"text": "Neural Domain Adaptation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7497232953707377}, {"text": "Biomedical Question Answering", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.6369510690371195}]}], "abstractContent": [{"text": "Factoid question answering (QA) has recently benefited from the development of deep learning (DL) systems.", "labels": [], "entities": [{"text": "Factoid question answering (QA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8417792816956838}]}, {"text": "Neural network models outperform traditional approaches in domains where large datasets exist, such as SQuAD (\u2248 100, 000 questions) for Wikipedia articles.", "labels": [], "entities": []}, {"text": "However, these systems have not yet been applied to QA in more specific domains, such as biomedicine, because datasets are generally too small to train a DL system from scratch.", "labels": [], "entities": [{"text": "QA", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9368637800216675}]}, {"text": "For example, the BioASQ dataset for biomedical QA comprises less then 900 factoid (single answer) and list (multiple answers) QA instances.", "labels": [], "entities": [{"text": "BioASQ dataset", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.9213789105415344}]}, {"text": "In this work, we adapt a neural QA system trained on a large open-domain dataset (SQuAD, source) to a biomedical dataset (BioASQ, target) by employing various transfer learning techniques.", "labels": [], "entities": []}, {"text": "Our network architecture is based on a state-of-the-art QA system, extended with biomedical word embeddings and a novel mechanism to answer list questions.", "labels": [], "entities": []}, {"text": "In contrast to existing biomedical QA systems, our system does not rely on domain-specific ontolo-gies, parsers or entity taggers, which are expensive to create.", "labels": [], "entities": []}, {"text": "Despite this fact, our systems achieve state-of-the-art results on factoid questions and competitive results on list questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) is the task of retrieving answers to a question given one or more contexts.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9447773218154907}]}, {"text": "It has been explored both in the opendomain setting) as well as domain-specific settings, such as BioASQ for the biomedical domain ().", "labels": [], "entities": []}, {"text": "The BioASQ challenge provides \u2248 900 factoid and list questions, i.e., questions with one and several answers, respectively.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.6046966314315796}]}, {"text": "This work focuses on answering these questions, for example: Which drugs are included in the FEC-75 regimen?", "labels": [], "entities": [{"text": "FEC-75 regimen", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.91925448179245}]}, {"text": "\u2192 fluorouracil, epirubicin, and cyclophosphamide.", "labels": [], "entities": []}, {"text": "We further restrict our focus to extractive QA, i.e., QA instances where the correct answers can be represented as spans in the contexts.", "labels": [], "entities": []}, {"text": "Contexts are relevant documents which are provided by an information retrieval (IR) system.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.7052569150924682}]}, {"text": "Traditionally, a QA pipeline consists of namedentity recognition, question classification, and answer processing steps).", "labels": [], "entities": [{"text": "QA pipeline", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.8942596912384033}, {"text": "namedentity recognition", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7313311398029327}, {"text": "question classification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.8285338580608368}, {"text": "answer processing", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7380462288856506}]}, {"text": "These methods have been applied to biomedical datasets, with moderate success (.", "labels": [], "entities": []}, {"text": "The creation of large-scale, open-domain datasets such as SQuAD ( have recently enabled the development of neural QA systems, e.g.,,,,, leading to impressive performance gains over more traditional systems.", "labels": [], "entities": []}, {"text": "However, creating large-scale QA datasets for more specific domains, such as the biomedical, would be very expensive because of the need for domain experts, and therefore not desirable.", "labels": [], "entities": []}, {"text": "The recent success of deep learning based methods on open-domain QA datasets raises the question whether the capabilities of trained models are transferable to another domain via domain adaptation techniques.", "labels": [], "entities": []}, {"text": "Although domain adaptation has been studied for traditional QA systems) and deep learning systems, it has to our knowledge not yet been applied for end-to-end neural QA systems.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7286226749420166}]}, {"text": "To bridge this gap we employ various do-main adaptation techniques to transfer knowledge from a trained, state-of-the-art neural QA system) to the biomedical domain using the much smaller BioASQ dataset.", "labels": [], "entities": [{"text": "BioASQ dataset", "start_pos": 188, "end_pos": 202, "type": "DATASET", "confidence": 0.9593561887741089}]}, {"text": "In order to answer list questions in addition to factoid questions, we extend FastQA with a novel answering mechanism.", "labels": [], "entities": []}, {"text": "We evaluate various transfer learning techniques comprehensively.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.9054572880268097}]}, {"text": "For factoid questions, we show that mere fine-tuning reaches state-of-the-art results, which can further be improved by a forgetting cost regularization ().", "labels": [], "entities": []}, {"text": "On list questions, the results are competitive to existing systems.", "labels": [], "entities": []}, {"text": "Our manual analysis of a subset of the factoid questions suggests that the results are even better than the automatic evaluation states, revealing that many of the \"incorrect\" answers are in fact synonyms to the gold-standard answer.", "labels": [], "entities": []}], "datasetContent": [{"text": "SQuAD SQuAD () is a dataset of \u2248 100, 000 questions with relevant contexts and answers that sparked research interest into the development of neural QA systems recently.", "labels": [], "entities": []}, {"text": "The contexts are excerpts of Wikipedia articles for which crowd-source workers generated questions-answer pairs.", "labels": [], "entities": []}, {"text": "Because of the large amount of training examples in SQuAD, it lends itself perfectly as our source dataset.", "labels": [], "entities": []}, {"text": "BioASQ The BioASQ challenge provides a biomedical QA dataset ( consisting of questions, relevant contexts (called snippets) from PubMed abstracts and possible answers to the question.", "labels": [], "entities": []}, {"text": "It was carefully created with the help of biomedical experts.", "labels": [], "entities": []}, {"text": "In this work, we focus on Task B, Phase B of the BioASQ challenge, in which systems must answer questions from gold-standard snippets.", "labels": [], "entities": []}, {"text": "These questions can be either yes/no questions, summary questions, factoid questions, or list questions.", "labels": [], "entities": []}, {"text": "Because we employ an extractive QA system, we restrict this study to answering factoid and list questions by extracting answer spans from the provided contexts.", "labels": [], "entities": []}, {"text": "The 2017 BioASQ training dataset contains 1, 799 questions, of which 413 are factoid and 486 are list questions.", "labels": [], "entities": [{"text": "BioASQ training dataset", "start_pos": 9, "end_pos": 32, "type": "DATASET", "confidence": 0.702478845914205}]}, {"text": "The questions have \u2248 20 snippets on average, each of which are on average \u2248 34 tokens long.", "labels": [], "entities": []}, {"text": "We found that around 65% of the factoid questions and around 92% of the list questions have at least one extractable answer.", "labels": [], "entities": []}, {"text": "For questions with extractable answers, answers spans are computed via a simple substring search in the provided snippets.", "labels": [], "entities": []}, {"text": "All other questions are ignored during training and treated as answered incorrectly during evaluation.", "labels": [], "entities": []}, {"text": "The official evaluation measures from BioASQ are mean reciprocal rank (MRR) for factoid questions and F1 score for list questions . For factoid questions, the list of ranked answers can beat most five entries long.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.7262645363807678}, {"text": "mean reciprocal rank (MRR)", "start_pos": 49, "end_pos": 75, "type": "METRIC", "confidence": 0.9580846230189005}, {"text": "F1 score", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9921523928642273}]}, {"text": "The F1 score is measured on the gold standard list elements.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9839563965797424}]}, {"text": "For both measures, case-insensitive string matches are used to check the correctness of a given answer.", "labels": [], "entities": []}, {"text": "A list of synonyms is provided for all gold-standard answers.", "labels": [], "entities": []}, {"text": "If the system's response matches one of them, the answer counts as correct.", "labels": [], "entities": []}, {"text": "For evaluation, we use two different finetuning datasets, depending on the experiment: BioASQ3B, which contains all questions of the first three BioASQ challenges, and BioASQ4B which additionally contains the test questions of the fourth challenge.", "labels": [], "entities": [{"text": "BioASQ3B", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9707359671592712}, {"text": "BioASQ4B", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.8431804776191711}]}, {"text": "BioASQ4B is used as the training dataset for the fifth BioASQ challenge whereas BioASQ3B was used for training during the fourth challenge.", "labels": [], "entities": [{"text": "BioASQ3B", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9545465707778931}]}, {"text": "Because the datasets are small, we perform 5-fold cross-validation and report the average performance across the five folds.", "labels": [], "entities": []}, {"text": "We use the larger BioASQ4B dataset except when evaluating the ensemble and when comparing to participating systems of previous BioASQ challenges.", "labels": [], "entities": [{"text": "BioASQ4B dataset", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.9234631657600403}]}, {"text": "All models were implemented using TensorFlow () with a hidden size of 100.", "labels": [], "entities": []}, {"text": "Because the context in BioASQ usually comprises multiple snippets, they are processed independently in parallel for each question.", "labels": [], "entities": []}, {"text": "Answers from all snippets belonging to a question are merged and ranked according to their individual probabilities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of various transfer learning techniques. In Experiment 1, the model was trained on  BioASQ only. In Experiment 2, the model was trained on SQuAD and tested on BioASQ. We refer to it as  the base model. In Experiment 3, the base model parameters were fine-tuned on the BioASQ training set.  Experiments 4-5 evaluate the utility of domain dependent word vectors and features. Experiments 6-8  address the problem of catastrophic forgetting. All experiments have been conducted with the BioASQ4B  dataset and 5-fold cross-validation.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 180, "end_pos": 186, "type": "DATASET", "confidence": 0.8203379511833191}, {"text": "BioASQ training set", "start_pos": 289, "end_pos": 308, "type": "DATASET", "confidence": 0.7969101866086324}, {"text": "BioASQ4B  dataset", "start_pos": 505, "end_pos": 522, "type": "DATASET", "confidence": 0.905273050069809}]}, {"text": " Table 2:  Performance of a model ensemble.  Five models have been trained on the BioASQ3B  dataset and tested on the 4B test questions. We  report the average and best single model perfor- mances, as well as the ensemble performance.", "labels": [], "entities": [{"text": "BioASQ3B  dataset", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9555548429489136}, {"text": "4B test questions", "start_pos": 118, "end_pos": 135, "type": "DATASET", "confidence": 0.7808223168055216}]}, {"text": " Table 3: Comparison to systems on last year's (fourth) BioASQ challenge for factoid and list questions.", "labels": [], "entities": []}]}