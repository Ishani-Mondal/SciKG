{"title": [{"text": "A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.7802327871322632}]}], "abstractContent": [{"text": "We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.8355019986629486}, {"text": "graph-based dependency parsing", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.6991519331932068}]}, {"text": "Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem.", "labels": [], "entities": [{"text": "POS tagging and dependency parsing tasks", "start_pos": 84, "end_pos": 124, "type": "TASK", "confidence": 0.7357408255338669}]}, {"text": "Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outper-forms the state-of-the-art neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in anew state of the art.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 193, "end_pos": 204, "type": "TASK", "confidence": 0.7896698415279388}, {"text": "dependency parsing", "start_pos": 226, "end_pos": 244, "type": "TASK", "confidence": 0.6812086850404739}]}, {"text": "Our code is open-source and available together with pre-trained models at: https://github.com/ datquocnguyen/jPTDP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the shared tasks on multilingual dependency parsing).", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8910972476005554}, {"text": "multilingual dependency parsing", "start_pos": 124, "end_pos": 155, "type": "TASK", "confidence": 0.6188788314660391}]}, {"text": "McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches and transition-based approaches.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8287786841392517}]}, {"text": "Most traditional graph-or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (.", "labels": [], "entities": []}, {"text": "Recent work shows that using deep learning in dependency parsing has obtained state-of-the-art performances.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8237805962562561}]}, {"text": "Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network-based classifiers.", "labels": [], "entities": []}, {"text": "In addition, others propose novel neural architectures for parsing to handle feature-engineering (;.", "labels": [], "entities": []}, {"text": "Part-of-speech (POS) tags are essential features used inmost dependency parsers.", "labels": [], "entities": []}, {"text": "In real-world parsing, those dependency parsers rely heavily on the use of automatically predicted POS tags, thus encountering error propagation problems., and show that parsing accuracies drop by 5+% when utilizing automatic POS tags instead of gold ones.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9622156023979187}]}, {"text": "Some attempts have been made to avoid using POS tags during dependency parsing , however, these approaches still additionally use the automatic POS tags to achieve the best accuracy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7766173481941223}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9933570027351379}]}, {"text": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. ambiguities.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.7332631051540375}, {"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7632452249526978}, {"text": "POS tagging", "start_pos": 284, "end_pos": 295, "type": "TASK", "confidence": 0.7250757217407227}, {"text": "dependency parsing", "start_pos": 312, "end_pos": 330, "type": "TASK", "confidence": 0.7602287828922272}]}, {"text": "In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.858605831861496}, {"text": "dependency parsing", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.6867574453353882}]}, {"text": "Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTMthe bidirectional LSTM ().", "labels": [], "entities": [{"text": "POS tagging and dependency parsing tasks", "start_pos": 64, "end_pos": 104, "type": "TASK", "confidence": 0.7645067075888315}]}, {"text": "Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, achieving anew state of the art.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 298, "end_pos": 309, "type": "TASK", "confidence": 0.8056983649730682}, {"text": "dependency parsing", "start_pos": 331, "end_pos": 349, "type": "TASK", "confidence": 0.6800200343132019}]}], "datasetContent": [{"text": "Following Zhang and Weiss (2016) and, we conduct multilingual experiments on 19 languages from the Universal Dependencies (UD) treebanks 1 v1.2 (, using the universal POS tagset () instead of the language specific POS tagset.", "labels": [], "entities": [{"text": "Universal Dependencies (UD) treebanks 1 v1.2", "start_pos": 99, "end_pos": 143, "type": "DATASET", "confidence": 0.6769719272851944}]}, {"text": "2 For dependency parsing, the evaluation metric is the labeled attachment score (LAS).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8468884229660034}, {"text": "labeled attachment score (LAS)", "start_pos": 55, "end_pos": 85, "type": "METRIC", "confidence": 0.8729539811611176}]}, {"text": "LAS is the percentage of words which are correctly assigned both dependency arc and relation type.", "labels": [], "entities": [{"text": "LAS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9734222292900085}]}], "tableCaptions": [{"text": " Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with \u2022 refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.7081868350505829}, {"text": "LAS", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9811767339706421}]}, {"text": " Table 2: Official macro-averaged LAS F1 scores  of MQuni and baselines from the CoNLL 2017  shared task on UD parsing (Zeman et al., 2017):  http://universaldependencies.org/  conll17/results-las.html. \"All\" refers  to the averaged score over all 81 test sets, which is  used as the main metric for ranking participating  systems. Big: the averaged score over 55/81 test  sets whose training treebanks are big and have  development data available. PUD: the averaged  score over 14/81 test sets that are additional  parallel ones, produced separately and their  domain may be different from their training data.  Sma.: the averaged score over 8/81 test sets  whose training treebanks are small, i.e., they lack  development data and some of them have very  little training data. Sur.: the averaged score over  4/81 remaining test sets for surprise languages.  Here the subscript denotes the official rank out of  33 participating systems. R -S is the system rank  where the 4 surprise language test sets are not  taken into account.", "labels": [], "entities": [{"text": "LAS F1 scores", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.8799286484718323}, {"text": "CoNLL 2017  shared task", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.8429494351148605}, {"text": "UD parsing", "start_pos": 108, "end_pos": 118, "type": "TASK", "confidence": 0.8578731119632721}, {"text": "PUD", "start_pos": 449, "end_pos": 452, "type": "METRIC", "confidence": 0.8056581020355225}]}]}