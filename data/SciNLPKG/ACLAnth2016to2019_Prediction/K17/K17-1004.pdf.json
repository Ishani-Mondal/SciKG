{"title": [{"text": "The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task", "labels": [], "entities": [{"text": "ROC Story Cloze Task", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.8219633847475052}]}], "abstractContent": [{"text": "A writer's style depends not just on personal traits but also on her intent and mental state.", "labels": [], "entities": []}, {"text": "In this paper, we show how variants of the same writing task can lead to measurable differences in writing style.", "labels": [], "entities": []}, {"text": "We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending fora given story context, and (3) adding an incoherent ending to a story.", "labels": [], "entities": []}, {"text": "We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context.", "labels": [], "entities": []}, {"text": "In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge.", "labels": [], "entities": []}, {"text": "Our results demonstrate that different task framings can dramatically affect the way people write.", "labels": [], "entities": []}], "introductionContent": [{"text": "Writing style is expressed through a range of linguistic elements such as words, sentence structure, and rhetorical devices.", "labels": [], "entities": []}, {"text": "It is influenced by personal factors such as age and gender (), by personality traits such as agreeableness and openness), as well as by mental states such as sentiment ( ), sarcasm ( , and deception.", "labels": [], "entities": []}, {"text": "In this paper, we study the extent to which writing style is affected by the nature of the writing task the writer was asked to perform,", "labels": [], "entities": []}], "datasetContent": [{"text": "We design two experiments to answer our research questions.", "labels": [], "entities": []}, {"text": "The first is an attempt to distinguish between right and wrong endings, the second between original endings and new (right) endings.", "labels": [], "entities": []}, {"text": "For completeness, we also run a third experiment, which compares between original and wrong endings.", "labels": [], "entities": []}, {"text": "Experiment 1: right/wrong endings.", "labels": [], "entities": []}, {"text": "The goal of this experiment is to measure the extent to which style features capture differences between the right and wrong endings.", "labels": [], "entities": []}, {"text": "As the story cloze task doesn't have a training corpus for the right and wrong endings (see Section 2), we use the development set as our training set, holding out 10% for development (3,366 training endings, 374 for development).", "labels": [], "entities": []}, {"text": "We keep the story cloze test set as is (3,742 endings).", "labels": [], "entities": []}, {"text": "It is worth noting that our classification task is slightly different from the story cloze task.", "labels": [], "entities": []}, {"text": "Instead of classifying pairs of endings, one which is right and another which is wrong, our classifier decides about each ending individually, whether it is right (positive instance) or wrong (negative instance).", "labels": [], "entities": []}, {"text": "By ignoring the coupling between right and wrong pairs, we are able to decrease the impact of author-specific style differences, and focus on the difference between the styles accompanied with right and wrong writings.", "labels": [], "entities": []}, {"text": "Experiment 2: original/new endings.", "labels": [], "entities": []}, {"text": "Here the goal is to measure whether writing the ending as part of a story imposes different style compared to writing anew (right) ending to an existing story.", "labels": [], "entities": []}, {"text": "We use the endings of the ROC stories as our original examples and right endings from the story cloze task as new examples.", "labels": [], "entities": [{"text": "ROC stories", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9175483882427216}]}, {"text": "As there are far more original instances than new instances, we randomly select five original sets, each with the same number of instances as we have new instances (3,366 training endings, 374 development endings, and 3,742 test endings).", "labels": [], "entities": []}, {"text": "We train five classifiers, one with each of the original training sets, and report the average classification result.", "labels": [], "entities": []}, {"text": "Experiment 3: original/wrong endings.", "labels": [], "entities": []}, {"text": "For completeness, we measure the extent to which our classifier can discriminate between original and wrong endings.", "labels": [], "entities": []}, {"text": "We replicate Experiment 2, this time replacing right endings with wrong ones.", "labels": [], "entities": []}, {"text": "In all experiments, we add a START symbol at the beginning of each sentence.", "labels": [], "entities": [{"text": "START", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9515437483787537}]}, {"text": "For computing our features, we keep ngram (character or word) features that occur at least five times in the training set.", "labels": [], "entities": []}, {"text": "All feature values are normalized to.", "labels": [], "entities": []}, {"text": "For the POS features, we tag all endings with the Spacy POS tagger.", "labels": [], "entities": []}, {"text": "We use Python's sklearn logistic regression imple-.", "labels": [], "entities": []}, {"text": "In all cases, our setup implies a 50% random baseline.", "labels": [], "entities": []}, {"text": "mentation (Pedregosa et al., 2011) with L 2 regularization, performing grid search on the development set to tune a single hyperparameter-the regularization parameter.", "labels": [], "entities": []}, {"text": "In all experiments, our model achieves performance well above what would be expected under chance (50% by design).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of experiments 1 (right  vs. wrong), 2 (original vs. right (new)) and 3 (orig- inal vs. wrong (new) endings", "labels": [], "entities": []}, {"text": " Table 3: Results on the test set of the story cloze  task. The middle block are our results. cogcomp  results and human judgement scores are taken  from Mostafazadeh et al. (2017). Methods marked  with ( \u2020) do not use the story context in order to  make a prediction.", "labels": [], "entities": []}, {"text": " Table 5: The top 5 most heavily weighted features for predicting right vs. wrong endings (5a) and original  vs. new (right) endings (5b). length is the sentence length feature (see Section 4).", "labels": [], "entities": [{"text": "predicting right vs. wrong endings", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.8515514373779297}, {"text": "length", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9938493371009827}]}]}