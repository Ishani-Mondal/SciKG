{"title": [{"text": "The ParisNLP entry at the ConLL UD Shared Task 2017: A Tale of a #ParsingTragedy\u00c9ric", "labels": [], "entities": [{"text": "ParisNLP entry at the ConLL UD Shared Task 2017", "start_pos": 4, "end_pos": 51, "type": "DATASET", "confidence": 0.6421390540070004}, {"text": "ParsingTragedy\u00c9ric", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.26712867617607117}]}], "abstractContent": [{"text": "We present the ParisNLP entry at the UD CoNLL 2017 parsing shared task.", "labels": [], "entities": [{"text": "ParisNLP entry", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.966034322977066}, {"text": "UD CoNLL 2017 parsing shared task", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.636172836025556}]}, {"text": "In addition to the UDpipe models provided, we built our own data-driven tokenization models, sentence segmenter and lexicon-based morphological analyzers.", "labels": [], "entities": [{"text": "sentence segmenter", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.713672086596489}]}, {"text": "All of these were used with a range of different parsing models (neural or not, feature-rich or not, transition or graph-based, etc.) and the best combination for each language was selected.", "labels": [], "entities": []}, {"text": "Unfortunately, a glitch in the shared task's Matrix led our model selector to run generic, weakly lexicalized models , tailored for surprise languages, instead of our dataset-specific models.", "labels": [], "entities": []}, {"text": "Because of this #ParsingTragedy, we officially ranked 27th, whereas our real models finally unofficially ranked 6th.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Universal Dependency parsing shared task () was arguably the hardest shared task the field has seen since the CoNLL 2006 shared task () where 13 languages had to be parsed in gold token, gold morphology mode, while its followup in 2007 introduced an out-of-domain track fora subset of the 2006 languages ().", "labels": [], "entities": [{"text": "Universal Dependency parsing shared task", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.7103428363800048}, {"text": "CoNLL 2006 shared task", "start_pos": 114, "end_pos": 136, "type": "DATASET", "confidence": 0.8095954954624176}]}, {"text": "The SANCL \"parsing the web\" shared task) introduced the parsing of English non-canonical data in gold token, predicted morphology mode and saw a large decrease of performance compared to what was usually reported in English parsing of the Penn Treebank . As far as we know, the SPMRL shared tasks) were first to introduce anon gold tokenization, predicted morphology, scenario for two morphologically rich languages, Arabic and Hebrew while, for other languages, complex source tokens were left untouched (Korean, German, French.", "labels": [], "entities": [{"text": "SANCL \"parsing the web\" shared task", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.6919674538075924}, {"text": "Penn Treebank", "start_pos": 239, "end_pos": 252, "type": "DATASET", "confidence": 0.9951718151569366}]}, {"text": "). Here, the Universal Dependency (hereafter \"UD\") shared task introduced an endto-end parsing evaluation protocol where none of the usual stratification layers were to be evaluated in gold mode: tokenization, sentence segmentation, morphology prediction and of course syntactic structures had to be produced 1 for 46 languages covering 81 datasets.", "labels": [], "entities": [{"text": "endto-end parsing evaluation", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.7593774199485779}, {"text": "sentence segmentation", "start_pos": 210, "end_pos": 231, "type": "TASK", "confidence": 0.74962118268013}, {"text": "morphology prediction", "start_pos": 233, "end_pos": 254, "type": "TASK", "confidence": 0.7907004952430725}]}, {"text": "Some of them are low-resource languages, with training sets containing as few as 22 sentences.", "labels": [], "entities": []}, {"text": "In addition, an out-of-domain scenario was de facto included via anew 14-language parallel test set.", "labels": [], "entities": []}, {"text": "Because of the very nature of the UD initiative, some languages are covered by several treebanks (English, French, Russian, Finnish.", "labels": [], "entities": []}, {"text": ") built by different teams, who interpreted the annotation guidelines with a certain degree of freedom when it comes to rare, or simply not covered, phenomena.", "labels": [], "entities": []}, {"text": "Let us add that our systems had to be deployed on a virtual machine and evaluated in a total blind mode with different metadata between the trial and the test runs.", "labels": [], "entities": []}, {"text": "All those parameters led to a multi-dimension shared task which can loosely be summarized by the following \"equation\": Lang.T ok.W S.Seg.M orph.DS.OOD.AS.Exp, where Lang stands for Language, T ok for tokenization, W S for word segmentation, Seg for sentence segmentation, M orph for predicted morphology, DS for data scarcity, OOD for out-ofdomainness, AS for annotation scheme, Exp for experimental environment.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 222, "end_pos": 239, "type": "TASK", "confidence": 0.7019312679767609}, {"text": "sentence segmentation", "start_pos": 249, "end_pos": 270, "type": "TASK", "confidence": 0.734904021024704}, {"text": "OOD", "start_pos": 327, "end_pos": 330, "type": "METRIC", "confidence": 0.9851147532463074}, {"text": "AS", "start_pos": 353, "end_pos": 355, "type": "METRIC", "confidence": 0.9537518620491028}]}, {"text": "In this shared task, we earnestly tried to coverall of these dimensions, ranking #3 in UPOS tagging and #5 in sentence segmentation.", "labels": [], "entities": [{"text": "UPOS tagging", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.6970654726028442}, {"text": "sentence segmentation", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7161519229412079}]}, {"text": "But we were ultimately strongly impacted by the Exp parameter (cf. Section 6.3), a parameter we could not control, resulting in a disappointing rank of #27 out of 33.", "labels": [], "entities": [{"text": "Exp", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.908450186252594}]}, {"text": "Once this variable was corrected, we reached rank #6.", "labels": [], "entities": []}, {"text": "Our system relies on a strong pre-processing pipeline, which includes lexicon-enhanced statistical taggers as well as data-driven tokenizers and sentence segmenters.", "labels": [], "entities": []}, {"text": "The parsing step proper makes use for each dataset of one of 4 parsing models: 2 non-neural ones (transition and graphbased) and extensions of these models with character and word-level neural layers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9702951312065125}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: UPOS accuracies for the UDPipe baseline and for our best alVWTagger setting.", "labels": [], "entities": [{"text": "UDPipe baseline", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9034356474876404}]}, {"text": " Table 2: Tokenization and sentence segmentation  accuracies for the UDPipe baseline and our tok- enizer (restricted to those datasets for which we  experimented the use of our own tokenization).", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7076102048158646}, {"text": "UDPipe baseline", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.9303765892982483}]}, {"text": " Table 3: Neural models & feature impact (Dev)", "labels": [], "entities": []}, {"text": " Table 5: Generic pool vs. small treebank vs. full  treebank (with srcat models (LAS, Dev)", "labels": [], "entities": []}]}