{"title": [{"text": "The HIT-SCIR System for End-to-End Parsing of Universal Dependencies", "labels": [], "entities": [{"text": "End-to-End Parsing of Universal Dependencies", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.703509658575058}]}], "abstractContent": [{"text": "This paper describes our system (HIT-SCIR) for the CoNLL 2017 shared task: Multilingual Parsing from Raw Text to Universal Dependencies.", "labels": [], "entities": [{"text": "HIT-SCIR", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.806171715259552}, {"text": "CoNLL 2017 shared task", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.8328108191490173}, {"text": "Multilingual Parsing from Raw Text", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.8171615242958069}]}, {"text": "Our system includes three pipelined components: to-kenization, Part-of-Speech (POS) tagging and dependency parsing.", "labels": [], "entities": [{"text": "Part-of-Speech (POS) tagging", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.57738698720932}, {"text": "dependency parsing", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8203911185264587}]}, {"text": "We use character-based bidirectional long short-term memory (LSTM) networks for both tokenization and POS tagging.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.9726841449737549}, {"text": "POS tagging", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.8112505674362183}]}, {"text": "After-wards, we employ a list-based transition-based algorithm for general non-projective parsing and present an improved Stack-LSTM-based architecture for representing each transition state and making predictions.", "labels": [], "entities": []}, {"text": "Furthermore, to parse low/zero-resource languages and cross-domain data, we use a model transfer approach to make effective use of existing resources.", "labels": [], "entities": [{"text": "parse low/zero-resource languages", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.8481476783752442}]}, {"text": "We demonstrate substantial gains against the UDPipe baseline, with an average improvement of 3.76% in LAS of all languages.", "labels": [], "entities": [{"text": "UDPipe baseline", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.7707625329494476}, {"text": "LAS", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.939789891242981}]}, {"text": "And finally , we rank the 4th place on the official test sets.", "labels": [], "entities": [{"text": "official test sets", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.7208943168322245}]}], "introductionContent": [{"text": "Our system for the CoNLL 2017 shared task) is a pipeline which includes three cascaded modules, tokenization, Part-of-Speech (POS) tagging and dependency parsing.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.84856316447258}, {"text": "tokenization", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.9740983843803406}, {"text": "Part-of-Speech (POS) tagging", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.5886202991008759}, {"text": "dependency parsing", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.764974057674408}]}, {"text": "This module includes two components, the sentence segmenter and the word segmenter which recognize the sentence and word boundaries respectively (Section 2.1).", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.6755430996417999}]}, {"text": "We focus mainly on universal POS tags, and don't use language-specific POS as well as other morphological features (Section 2.2).", "labels": [], "entities": []}, {"text": "To handle the nonprojective dependencies inmost of the languages (or treebanks) provided in the task, we employ the list-based transition parsing algorithm, equipped with an improved Stack-LSTMbased model for representing the transition states, i.e., configurations (Section 2.3).", "labels": [], "entities": [{"text": "list-based transition parsing", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.6332027713457743}]}, {"text": "We mainly concentrate on parsing in this task, and make use of UDPipe (v1.1) () for most of the pre-processing steps.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9865266680717468}]}, {"text": "However, our preliminary experiments showed that the UDPipe tokenizer and POS tagger perform rather poorly in some languages and specific domains.", "labels": [], "entities": [{"text": "UDPipe tokenizer", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.5349850654602051}, {"text": "POS tagger", "start_pos": 74, "end_pos": 84, "type": "TASK", "confidence": 0.6504203826189041}]}, {"text": "Therefore, we develop our own tokenizer and POS tagger fora subset of languages.", "labels": [], "entities": []}, {"text": "To deal with the parallel test sets (crossdomain) and low/zero-resource languages, we adopt the neural transfer approaches proposed in our previous studies) to encourage knowledge transfer across different but related languages or treebanks.", "labels": [], "entities": []}, {"text": "Experiments on 81 test sets demonstrate that our system (HIT-SCIR: software4) obtains an average improvement of 3.76% in LAS as compared with the UDPipe baseline, and ranks the 4th place in this task.", "labels": [], "entities": [{"text": "HIT-SCIR: software4", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.7249592145284017}, {"text": "LAS", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.928825318813324}]}], "datasetContent": [{"text": "We first describe our experiment setups and strategies for processing different languages (treebanks) in each module.", "labels": [], "entities": []}, {"text": "Then we present the results and analysis.", "labels": [], "entities": []}, {"text": "We realized a small problem in our implementation of the word segmentation models after official evaluation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7285145223140717}]}, {"text": "After revision, we re-evaluated our models on the four test treebanks: zh, vi, ja.", "labels": [], "entities": []}, {"text": "On zh, viand ja pud, we outperform the rank-1 system significantly.", "labels": [], "entities": []}, {"text": "We can see that the performance of word segmentation is crucial for the pipeline system.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7580218017101288}]}], "tableCaptions": [{"text": " Table 4: Experiment results (LAS) on devel- opment sets with different settings.  B: Bi- LSTM Subtraction, T: Incremental Tree-LSTM,  C: Brown cluster. Ensemble is produced with  models we eventually submitted.", "labels": [], "entities": []}, {"text": " Table 5: Effects of cross-domain transfer parsing  on a subset of development sets.", "labels": [], "entities": [{"text": "cross-domain transfer parsing", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.767066240310669}]}, {"text": " Table 6: Effects of cross-lingual transfer parsing  on ug uk and ga. 5-fold cross-validation is used  for evaluation.", "labels": [], "entities": [{"text": "cross-lingual transfer parsing", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.7692818442980448}]}, {"text": " Table 7: End-to-end parsing results on all test treebanks.  \u2021 indicates the test sets on which UDPipe  performs better. Among the 5 sets, es pud, ru pud and tr pud are parallel test sets on which we simply  use the model trained from the source treebank. We suggest better strategies should be explored.", "labels": [], "entities": []}]}