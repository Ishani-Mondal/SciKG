{"title": [{"text": "Learning an Executable Neural Semantic Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "This article describes a neural semantic parser that maps natural language utterances ontological forms that can be executed against a task-specific environment, such as a knowledge base or a database, to produce a response.", "labels": [], "entities": []}, {"text": "The parser generates tree-structured logical forms with a transition-based approach, combining a generic tree-generation algorithm with domain-general grammar defined by the logical language.", "labels": [], "entities": []}, {"text": "The generation process is modeled by structured recurrent neural networks, which provide a rich encoding of the sentential context and generation history for making predictions.", "labels": [], "entities": []}, {"text": "To tackle mismatches between natural language and logical form tokens, various attention mechanisms are explored.", "labels": [], "entities": []}, {"text": "Finally, we consider different training settings for the neural semantic parser, including fully supervised training where annotated logical forms are given, weakly supervised training where denotations are provided, and distant supervision where only unlabeled sentences and a knowledge base are available.", "labels": [], "entities": []}, {"text": "Experiments across a wide range of data sets demonstrate the effectiveness of our parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important task in artificial intelligence is to develop systems that understand natural language and enable interactions between computers and humans.", "labels": [], "entities": []}, {"text": "Semantic parsing has emerged as a key technology toward achieving this goal.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7762610614299774}]}, {"text": "Semantic parsers specify a mapping between natural language utterances and machine-understandable meaning In this work we propose a neural semantic parsing framework that combines recurrent neural networks and their ability to model long-range dependencies with a transition system to generate well-formed and meaningful logical forms.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.7260110974311829}]}, {"text": "The transition system combines a generic tree-generation algorithm with a small set of domain-general grammar pertaining to the logical language to guarantee correctness.", "labels": [], "entities": []}, {"text": "Our neural parser differs from conventional semantic parsers in two respects.", "labels": [], "entities": []}, {"text": "First, it does not require lexicon-level rules to specify the mapping between natural language and logical form tokens.", "labels": [], "entities": []}, {"text": "Instead, the parser is designed to handle cases where the lexicon is missing or incomplete thanks to a neural attention layer, which encodes a soft mapping between natural language and logical form tokens.", "labels": [], "entities": []}, {"text": "This modeling choice greatly reduces the number of grammar rules used during inference to those only specifying domain-general aspects.", "labels": [], "entities": []}, {"text": "Second, our parser is transition-based rather than chart-based.", "labels": [], "entities": []}, {"text": "Although chartbased inference has met with popularity in conventional semantic parsers, it has difficulty in leveraging sentence-level features because the dynamic programming algorithm requires features defined over substructures.", "labels": [], "entities": []}, {"text": "In comparison, our linear-time parser allows us to generate parse structures incrementally conditioned on the entire sentence.", "labels": [], "entities": []}, {"text": "We perform several experiments in downstream question-answering tasks and demonstrate the effectiveness of our approach across different training scenarios.", "labels": [], "entities": []}, {"text": "These include full supervision with questions paired with annotated logical forms using the GEOQUERY (Zettlemoyer and Collins 2005) data set, weak supervision with questionanswer pairs using the WEBQUESTIONS () and GRAPHQUESTIONS () data sets, and distant supervision without question-answer pairs, using the SPADES () data set.", "labels": [], "entities": [{"text": "GEOQUERY (Zettlemoyer and Collins 2005) data set", "start_pos": 92, "end_pos": 140, "type": "DATASET", "confidence": 0.8805992669529386}, {"text": "WEBQUESTIONS", "start_pos": 195, "end_pos": 207, "type": "METRIC", "confidence": 0.525977611541748}, {"text": "GRAPHQUESTIONS", "start_pos": 215, "end_pos": 229, "type": "METRIC", "confidence": 0.9754853248596191}, {"text": "SPADES () data set", "start_pos": 309, "end_pos": 327, "type": "DATASET", "confidence": 0.7514128535985947}]}, {"text": "Experimental results show that our neural semantic parser is able to generate high-quality logical forms and answer real-world questions on a wide range of domains.", "labels": [], "entities": []}, {"text": "The remainder of this article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces our neural semantic parsing framework and discusses the various training scenarios to which it can be applied.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7064643899599711}]}, {"text": "Our experiments are described in Section 4, together with detailed analysis of system output.", "labels": [], "entities": []}, {"text": "Discussion of future work concludes in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our experimental set-up for assessing the performance of the neural semantic parsing framework.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.6724896530310313}]}, {"text": "We present the data sets on which our model was trained and tested, discuss implementation details, and finally report and analyze semantic parsing results.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.6966284960508347}]}], "tableCaptions": [{"text": " Table 6  Fully supervised experimental results on the GEOQUERY data set. For Jia and Liang (2016), we  include two of their results: one is a standard neural sequence to sequence model; and the other  is the same model trained with a data augmentation algorithm on the labeled data (reported in  parentheses).", "labels": [], "entities": [{"text": "GEOQUERY data set", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.9797435204188029}]}, {"text": " Table 7  Weakly supervised experimental results on two data sets. Results with additional resources are  shown in parentheses.", "labels": [], "entities": []}, {"text": " Table 8  Breakdown of questions answered by type for the GRAPHQUESTIONS.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8650231957435608}, {"text": "GRAPHQUESTIONS", "start_pos": 58, "end_pos": 72, "type": "METRIC", "confidence": 0.9559107422828674}]}, {"text": " Table 9  Distantly supervised experimental results on the SPADES data set.", "labels": [], "entities": [{"text": "SPADES data set", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.8665053645769755}]}]}