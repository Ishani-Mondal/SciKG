{"title": [{"text": "Parsing Chinese Sentences with Grammatical Relations under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "labels": [], "entities": [{"text": "Parsing Chinese Sentences with Grammatical Relations", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8636750082174937}]}], "abstractContent": [{"text": "We report our work on building linguistic resources and data-driven parsers in the grammatical relation (GR) analysis for Mandarin Chinese.", "labels": [], "entities": [{"text": "grammatical relation (GR) analysis", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.7340107262134552}]}, {"text": "Chinese, as an analytic language, encodes grammatical information in a highly configurational rather than morphological way.", "labels": [], "entities": []}, {"text": "Accordingly, it is possible and reasonable to represent almost all grammatical relations as bilexical dependencies.", "labels": [], "entities": []}, {"text": "In this work, we propose to represent grammatical information using general directed dependency graphs.", "labels": [], "entities": []}, {"text": "Both only-local and rich long-distance dependencies are explicitly represented.", "labels": [], "entities": []}, {"text": "To create high-quality annotations, we take advantage of an existing TreeBank, namely, Chinese TreeBank (CTB), which is grounded on the Government and Binding theory.", "labels": [], "entities": [{"text": "Chinese TreeBank (CTB)", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.9553735733032227}]}, {"text": "We define a set of linguistic rules to explore CTB's implicit phrase structural information and build deep dependency graphs.", "labels": [], "entities": [{"text": "CTB's implicit phrase structural information", "start_pos": 47, "end_pos": 91, "type": "TASK", "confidence": 0.6518777708212534}]}, {"text": "The reliability of this linguistically motivated GR extraction procedure is highlighted by manual Submission Volume 45, Number 1 evaluation.", "labels": [], "entities": [{"text": "GR extraction", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.965924471616745}]}, {"text": "Based on the converted corpus, data-driven, including graph-and transition-based, models are explored for Chinese GR parsing.", "labels": [], "entities": [{"text": "GR parsing", "start_pos": 114, "end_pos": 124, "type": "TASK", "confidence": 0.7678342163562775}]}, {"text": "For graph-based parsing, anew perspective, graph merging, is proposed for building flexible dependency graphs: constructing complex graphs via constructing simple subgraphs.", "labels": [], "entities": [{"text": "graph-based parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6344835758209229}, {"text": "graph merging", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.7497261464595795}]}, {"text": "Two key problems are discussed in this perspective: (1) how to decompose a complex graph into simple subgraphs, and (2) how to combine subgraphs into a coherent complex graph.", "labels": [], "entities": []}, {"text": "For transition-based parsing, we introduce a neural parser based on a list-based transition system.", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.5264179110527039}]}, {"text": "We also discuss several other key problems, including dynamic oracle and beam search for neural transition-based parsing.", "labels": [], "entities": [{"text": "neural transition-based parsing", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.6057416399319967}]}, {"text": "Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models.", "labels": [], "entities": [{"text": "GR parsing", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.9508005976676941}]}, {"text": "The empirical analysis suggests several directions for future study.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency grammar is a longstanding tradition that determines syntactic structures on the basis of word-to-word connections.", "labels": [], "entities": [{"text": "Dependency grammar", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8344348669052124}]}, {"text": "It names a family of approaches to the linguistic analysis that all share a commitment to the typed relations between ordered pairs of words.", "labels": [], "entities": []}, {"text": "Usually, dependencies represent various grammatical relations (GRs), which are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc., and therefore encode rich syntactic information of natural language sentences in an explicit way.", "labels": [], "entities": []}, {"text": "In recent years, parsing a sentence to a dependency representation has been well studied and widely applied to many Natural Language Processing (NLP) tasks, for example, Information Extraction and Machine Translation.", "labels": [], "entities": [{"text": "parsing a sentence", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.881948709487915}, {"text": "Information Extraction", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.8658624887466431}, {"text": "Machine Translation", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.7914455235004425}]}, {"text": "In particular, the data-driven approaches have made great progress during the past two decades.", "labels": [], "entities": []}, {"text": "Various practical parsing systems have been built, not only for English but also fora large number of typologically different languages, for example, Arabic, Basque, Catalan, Chinese, Czech, Greek, Hungarian, Italian, and Turkish (.", "labels": [], "entities": []}, {"text": "Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed bilexical dependency trees.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8597118854522705}]}, {"text": "Although tree-shaped graphs have several desirable properties from the computational perspective, they do notwork well for coordinations, long-range dependencies involved in raising, control, as well as extraction, and many other complicated linguistic phenomena that go beyond the surface syntax.", "labels": [], "entities": []}, {"text": "Some well-established and leading linguistic theories, such as Word Grammar) and Meaning-Text Theory, argue that more general dependency graphs are necessary to represent a variety of syntactic or semantic phenomena.", "labels": [], "entities": [{"text": "Word Grammar", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.6754301190376282}]}, {"text": "In this article, we are concerned with parsing Chinese sentences to an enriched deep dependency representation, which marks up a rich set of GRs to specify a linguisticallyrich syntactic analysis.", "labels": [], "entities": [{"text": "parsing Chinese sentences", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.8638095657030741}]}, {"text": "Different from the popular surface 1 tree-based dependency representation, our GR annotations are represented as general directed graphs that express not only local but also various long-distance dependencies (see for example).", "labels": [], "entities": []}, {"text": "To enhance the tree-shaped dependency representation, we borrow the key ideas underlying Lexical Function Grammar (LFG), Dalrymple), a well-defined and widely-applied linguistic grammar formalism.", "labels": [], "entities": []}], "datasetContent": [{"text": "To have a precise understanding of whether our extraction algorithm works well, we have selected 20 files that contain 209 sentences in total for manual evaluation.", "labels": [], "entities": []}, {"text": "Linguistic experts carefully examine the corresponding GR graphs derived by our extraction algorithm and correct all errors.", "labels": [], "entities": []}, {"text": "In other words, a gold-standard GR annotation set is created.", "labels": [], "entities": []}, {"text": "The measure for comparing two dependency graphs is precision/recall of GR tokens, which are defined as w h , w d , l tuples, where w h is the head, w dis the dependent, and l is the relation.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9993615746498108}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9824235439300537}]}, {"text": "Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the automatic generator, while unlabeled precision/recall (UP/UR) is the ratio regardless of l.", "labels": [], "entities": [{"text": "precision/recall (LP/LR)", "start_pos": 8, "end_pos": 32, "type": "METRIC", "confidence": 0.8556649684906006}, {"text": "precision/recall (UP/UR)", "start_pos": 121, "end_pos": 145, "type": "METRIC", "confidence": 0.8490864038467407}]}, {"text": "F-score is a harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9714664816856384}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9995357990264893}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9972677230834961}]}, {"text": "These measures correspond to attachment scores (LAS/UAS) in dependency tree parsing.", "labels": [], "entities": [{"text": "attachment scores (LAS/UAS)", "start_pos": 29, "end_pos": 56, "type": "METRIC", "confidence": 0.8927062579563686}, {"text": "dependency tree parsing", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.6583877007166544}]}, {"text": "To evaluate our GR parsing models that will be introduced later, we also report these metrics.", "labels": [], "entities": [{"text": "GR parsing", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.8689206838607788}]}, {"text": "The overall performance is summarized in.", "labels": [], "entities": []}, {"text": "We can see that the automatic GR extraction achieves relatively high performance.", "labels": [], "entities": [{"text": "GR extraction", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.927190899848938}]}, {"text": "There are two sources of errors in treebank conversion: (1) inadequate conversion rules and (2) wrong or inconsistent original annotations.", "labels": [], "entities": []}, {"text": "During the creation of the gold-standard corpus, we find that rule-based errors are mainly caused by complicated unbounded dependencies and the lack of internal structure for some phrases.", "labels": [], "entities": []}, {"text": "Such problems are very hard to solve through rules only, if even possible, since original annotations do not provide sufficient information.", "labels": [], "entities": []}, {"text": "The latter problem is more scattered and unpredictable, which requires manual correction.", "labels": [], "entities": []}, {"text": "CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (, and dependency parsing ().", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7556743919849396}, {"text": "POS tagging", "start_pos": 213, "end_pos": 224, "type": "TASK", "confidence": 0.8703567683696747}, {"text": "constituent parsing", "start_pos": 251, "end_pos": 270, "type": "TASK", "confidence": 0.7601586580276489}, {"text": "dependency parsing", "start_pos": 278, "end_pos": 296, "type": "TASK", "confidence": 0.8272924423217773}]}, {"text": "This corpus was collected during different time periods from different sources with a diverse range of topics.", "labels": [], "entities": []}, {"text": "We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task.", "labels": [], "entities": [{"text": "CTB 6.0", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.9467445611953735}, {"text": "CoNLL 2009 shared task", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.9436803013086319}]}, {"text": "gives a summary of the data sets for experiments.", "labels": [], "entities": []}, {"text": "Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance.", "labels": [], "entities": []}, {"text": "The measure for comparing two dependency graphs is precision/recall of bilexical dependencies, which are defined as w h , w d , l tuples, where w h is the head, w dis the dependent and l is the relation.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.999466598033905}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9865199327468872}]}, {"text": "Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified, while unlabeled metrics (UP/UR) is the ratio regardless of l.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.8866562843322754}, {"text": "recall (LP/LR)", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.9215209384759268}, {"text": "unlabeled metrics (UP/UR)", "start_pos": 84, "end_pos": 109, "type": "METRIC", "confidence": 0.7171808481216431}]}, {"text": "F-score (UF/LF) is a harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-score (UF/LF)", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9310809075832367}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9992347955703735}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9959810972213745}]}, {"text": "These measures correspond to attachment scores (LAS/UAS) in dependency tree parsing.", "labels": [], "entities": [{"text": "attachment scores (LAS/UAS)", "start_pos": 29, "end_pos": 56, "type": "METRIC", "confidence": 0.8927062579563686}, {"text": "dependency tree parsing", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.6583877007166544}]}, {"text": "To evaluate the ability to recover nonlocal dependencies, the recall (UR NL /LR NL ) of such dependencies is reported.", "labels": [], "entities": [{"text": "recall (UR NL /LR NL )", "start_pos": 62, "end_pos": 84, "type": "METRIC", "confidence": 0.8688264489173889}]}, {"text": "We also consider the correctness with respect to the whole graph and report unlabeled and labeled complete match (UCM/LCM).", "labels": [], "entities": [{"text": "correctness", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9738752841949463}, {"text": "complete match (UCM/LCM)", "start_pos": 98, "end_pos": 122, "type": "METRIC", "confidence": 0.5877665281295776}]}, {"text": "Column \"Training,\" \"Development,\" and \"Test\" present the number of sentences/words/dependencies in the training, development, and test sets.", "labels": [], "entities": []}, {"text": "shows the results of graph decomposition on the training set.", "labels": [], "entities": []}, {"text": "If we use simple decomposition, for example, directly extracting three trees from a graph, we get three subgraphs.", "labels": [], "entities": []}, {"text": "On the training set, each of the subgraphs covers around 90% of edges and 30% of sentences.", "labels": [], "entities": []}, {"text": "When merged together, they cover nearly 97% of edges and more than 70% of sentences.", "labels": [], "entities": []}, {"text": "This indicates that the ability of a singletree is limited and that three trees can cover most of the edges.", "labels": [], "entities": []}, {"text": "To achieve the best coverage, we need to tune the weights defined in Equations).", "labels": [], "entities": [{"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9472271800041199}]}, {"text": "This can be done on the development data.", "labels": [], "entities": []}, {"text": "When we apply Lagrangian Relaxation to the decomposition process, both the edge coverage and the sentence coverage gain a great error reduction, indicating that Lagrangian Relaxation is very effective in the task of decomposition.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Manual evaluation of 209 sentences.", "labels": [], "entities": []}, {"text": " Table 3  Data sets for experiments. Column \"Training,\" \"Development,\" and \"Test\" present the number  of sentences/words/dependencies in the training, development, and test sets.", "labels": [], "entities": []}, {"text": " Table 4  Results of graph decomposition. SD is for Simple Decomposition and LR for Lagrangian  Relaxation.", "labels": [], "entities": [{"text": "graph decomposition", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7142212092876434}, {"text": "LR", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9772367477416992}]}, {"text": " Table 5  Accuracies of the graph merging-based parser on development set. SM is for Simple Merging,  and LR for Lagrangian Relaxation. A second-order graph-based parser with a global linear  disambiguation model is used for tree parsing.", "labels": [], "entities": [{"text": "tree parsing", "start_pos": 225, "end_pos": 237, "type": "TASK", "confidence": 0.7370302379131317}]}, {"text": " Table 6  Accuracies of the graph merging-based parser on development set. A first-order graph-based  parser with a neural disambiguation model is used for tree parsing.", "labels": [], "entities": [{"text": "tree parsing", "start_pos": 156, "end_pos": 168, "type": "TASK", "confidence": 0.7230023741722107}]}, {"text": " Table 7  Accuracies of the transition-based parser on development set.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9792190194129944}]}, {"text": " Table 8  Accuracies with respect to dependency relations. The results are evaluated on the development  data. Only the relation types that appear more than 500 times on the development data are  reported.", "labels": [], "entities": []}, {"text": " Table 9  Parsing accuracies with different POS taggers. The results are evaluated on the development  data. \"LGLM\" is short for the POS tagger based on a linear-chain global linear model, while  \"SC\" denotes the same POS tagger enhanced with structure compilation.", "labels": [], "entities": []}, {"text": " Table 10  Parsing accuracies with ELMo. The results are evaluated on the development data. \"NoPOS\"  means training and parsing without POS tags.", "labels": [], "entities": []}, {"text": " Table 11  Accuracies of the graph merging-based parser on development set without POSTag information.  A first-order graph-based parser with a neural scoring model is used for tree parsing. ELMo is  used.", "labels": [], "entities": [{"text": "tree parsing", "start_pos": 177, "end_pos": 189, "type": "TASK", "confidence": 0.7091411352157593}]}, {"text": " Table 12  Accuracies with respect to representative grammatical functions. The results are obtained by the  graph merging parser on the development data. The ELMo vectors are utilized but not POS tags.", "labels": [], "entities": []}, {"text": " Table 13  Recalls with respect to dependency types, that is, local or non-local dependencies. The results are  evaluated on the development data.", "labels": [], "entities": []}, {"text": " Table 15  Accuracies of the GM model on test set. No POS tags are utilized, whereas the ELMo vectors are  used.", "labels": [], "entities": []}]}