{"title": [{"text": "Unsupervised Compositionality Prediction of Nominal Compounds", "labels": [], "entities": [{"text": "Unsupervised Compositionality Prediction of Nominal Compounds", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.7049199243386587}]}], "abstractContent": [{"text": "Nominal compounds such as red wine and nutcase display a continuum of compositionality, with varying contributions from the components of the compound to its semantics.", "labels": [], "entities": []}, {"text": "This article proposes a framework for compound compositionality prediction using distributional semantic models, evaluating to what extent they capture idiomaticity compared to human judgments.", "labels": [], "entities": [{"text": "compound compositionality prediction", "start_pos": 38, "end_pos": 74, "type": "TASK", "confidence": 0.8415671388308207}]}, {"text": "For evaluation, we introduce data sets containing human judgments in three languages: English, French, and Portuguese.", "labels": [], "entities": []}, {"text": "The results obtained reveal a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idiomaticity.", "labels": [], "entities": []}, {"text": "We also present an in-depth evaluation of various factors that can affect prediction, such as model and corpus parameters and compositionality operations.", "labels": [], "entities": []}, {"text": "General crosslingual analyses reveal the impact of morphological variation and corpus size in the ability of the model to predict compositionality, and of a uniform combination of the components for best results.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is a universally acknowledged assumption that the meaning of phrases, expressions, or sentences can be determined by the meanings of their parts and by the rules used to combine them.", "labels": [], "entities": []}, {"text": "Part of the appeal of this principle of compositionality 1 is that it implies that a meaning can be assigned even to anew sentence involving an unseen combination of familiar words.", "labels": [], "entities": []}, {"text": "Indeed, for natural language processing (NLP), this is an attractive way of linearly deriving the meaning of larger units from their components, performing the semantic interpretation of any text.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8089386423428854}]}, {"text": "For representing the meaning of individual words and their combinations in computational systems, distributional semantic models (DSMs) have been widely used.", "labels": [], "entities": []}, {"text": "DSMs are based on Harris' distributional hypothesis that the meaning of a word can be inferred from the context in which it occurs.", "labels": [], "entities": []}, {"text": "In DSMs, words are usually represented as vectors that, to some extent, capture cooccurrence patterns in corpora.", "labels": [], "entities": []}, {"text": "Evaluation of DSMs has focused on obtaining accurate semantic representations for words, and state-of-the-art models are already capable of obtaining a high level of agreement with human judgments for predicting synonymy or similarity between words ( and for modeling syntactic and semantic analogies between word pairs.", "labels": [], "entities": [{"text": "predicting synonymy or similarity between words", "start_pos": 201, "end_pos": 248, "type": "TASK", "confidence": 0.8244092464447021}]}, {"text": "These representations for individual words can also be combined to create representations for larger units such as phrases, sentences, and even whole documents, using simple additive and multiplicative vector operations (, syntax-based lexical functions (), or matrix and tensor operations (Baroni and Lenci 2010; Bride, Van de Cruys, and Asher 2015).", "labels": [], "entities": []}, {"text": "However, it is not clear to what extent this approach is adequate in the case of idiomatic multiword expressions (MWEs).", "labels": [], "entities": []}, {"text": "MWEs fall into a wide spectrum of compositionality; that is, some MWEs are more compositional (e.g., olive oil) while others are more idiomatic (.", "labels": [], "entities": []}, {"text": "In the latter case, the meaning of the MWE may not be straightforwardly related to the meanings of its parts, creating a challenge for the principle of compositionality (e.g., snake oil as a product of questionable benefit, not necessarily an oil and certainly not extracted from snakes).", "labels": [], "entities": []}, {"text": "In this article, we discuss approaches for automatically detecting to what extent the meaning of an MWE can be directly computed from the meanings of its component words, represented using DSMs.", "labels": [], "entities": []}, {"text": "We evaluate how accurately DSMs can model the semantics of MWEs with various levels of compositionality compared to human judgments.", "labels": [], "entities": []}, {"text": "Since MWEs encompass a large amount of related but distinct phenomena, we focus exclusively on a subcategory of MWEs: nominal compounds.", "labels": [], "entities": []}, {"text": "They represent an ideal case study for this work, thanks to their relatively homogeneous syntax (as opposed to other categories of MWEs such as verbal idioms) and their pervasiveness in language.", "labels": [], "entities": []}, {"text": "We assume that models able to predict the compositionality of nominal compounds could be generalized to other MWE categories by addressing their variability in future work.", "labels": [], "entities": []}, {"text": "Furthermore, to determine to what extent these approaches are also adequate cross-lingually, we evaluate them in three languages: English, French, and Portuguese.", "labels": [], "entities": []}, {"text": "Given that MWEs are frequent in languages (, identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (; Jagfeld and van der Plas 2015), word sense disambiguation (), and machine translation ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 259, "end_pos": 275, "type": "TASK", "confidence": 0.719500795006752}, {"text": "word sense disambiguation", "start_pos": 311, "end_pos": 336, "type": "TASK", "confidence": 0.7101942896842957}, {"text": "machine translation", "start_pos": 345, "end_pos": 364, "type": "TASK", "confidence": 0.7868742048740387}]}, {"text": "Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.9313101768493652}]}, {"text": "The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors fora compositional nominal compound and for the combination of the vectors of its parts using some vector operation, that we refer to as composition function.", "labels": [], "entities": []}, {"text": "Conversely we can use the lack of similarity between the nominal compound vector representation and a combination of its parts to detect idiomaticity.", "labels": [], "entities": []}, {"text": "Furthermore, we hypothesize that accuracy in predicting compositionality depends both on the characteristics of the DSMs used to represent expressions and their components and on the composition function adopted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.996198832988739}, {"text": "predicting compositionality", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.8956808745861053}]}, {"text": "Therefore, we have built 684 DSMs and performed an extensive evaluation, involving over 9,072 analyses, investigating various types of DSMs, their configurations, the corpora used to train them, and the composition function used to build vectors for expressions.", "labels": [], "entities": []}, {"text": "This article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work on distributional semantics, compositionality prediction, and nominal compounds.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.9570083022117615}]}, {"text": "Section 3 presents the data sets created for our evaluation.", "labels": [], "entities": []}, {"text": "Section 4 describes the compositionality prediction framework, along with the composition functions which we evaluate.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9722023606300354}]}, {"text": "Section 5 specifies the experimental setup (corpora, DSMs, parameters, and evaluation measures).", "labels": [], "entities": []}, {"text": "Section 6 presents the overall results of the evaluated models.", "labels": [], "entities": []}, {"text": "Sections 7 and 8 evaluate the impact of DSM and corpus parameters, and of composition functions on compositionality prediction.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.936753511428833}]}, {"text": "Section 9 discusses system predictions through an error analysis.", "labels": [], "entities": []}, {"text": "Section 10 summarizes our conclusions.", "labels": [], "entities": []}, {"text": "Appendix A contains a glossary, Appendix B presents extra sanity-check experiments, Appendix C contains the questionnaire used for data collection, and Appendices D, E, and F list the compounds in the data sets.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9725217819213867}, {"text": "Appendix", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9535704851150513}, {"text": "data collection", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.7600886225700378}]}], "datasetContent": [{"text": "This section describes the common setup used for evaluating compositionality prediction, such as corpora (Section 5.1), DSMs (Section 5.2), and evaluation metrics (Section 5.3).", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.8484310805797577}]}, {"text": "To evaluate a compositionality prediction configuration, we calculate Spearman's \u03c1 rank correlation between the predicted compositionality scores (pc)s and the human compositionality scores (hc)s for the compounds that appear in the evaluation data set.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.8471969366073608}, {"text": "Spearman's \u03c1 rank correlation", "start_pos": 70, "end_pos": 99, "type": "METRIC", "confidence": 0.7122160792350769}]}, {"text": "We mostly use the rank correlation instead of linear correlation (Pearson) because we are interested in the framework's ability to order compounds from least to most compositional, regardless of the actual predicted values.", "labels": [], "entities": [{"text": "rank correlation", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.7533601522445679}, {"text": "linear correlation (Pearson)", "start_pos": 46, "end_pos": 74, "type": "METRIC", "confidence": 0.7341331005096435}]}, {"text": "For English, besides the evaluation data sets presented in Section 3, we also use Reddy and Farahmand (see Section 2.4) to enable comparison with related work.", "labels": [], "entities": []}, {"text": "For Farahmand, since it contains binary judgments 33 instead of graded compositionality scores, results are reported using the best F 1 (BF 1 ) score, which is the highest F 1 score found using the top n compounds classified as noncompositional, when n is varied.", "labels": [], "entities": [{"text": "F 1 (BF 1 ) score", "start_pos": 132, "end_pos": 149, "type": "METRIC", "confidence": 0.9658244337354388}, {"text": "F 1 score", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9639047384262085}]}, {"text": "For Reddy, we sometimes present Pearson scores to enable comparison with related work.", "labels": [], "entities": [{"text": "Pearson scores", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9762179851531982}]}, {"text": "Because of the large number of compositionality prediction configurations evaluated, we only report the best performance for each configuration overall possible DSM parameter values.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.9382169544696808}]}, {"text": "The generalization of these analyses is then ensured using cross-validation and held-out data.", "labels": [], "entities": []}, {"text": "To determine whether the difference between two prediction results are statistically different, we use nonparametric Wilcoxon's sign-rank test.", "labels": [], "entities": []}, {"text": "As an additional test of the robustness of the results obtained, we calculated the performance of the best models obtained for one of the data sets (EN-comp), on a separate held-out data set (EN-comp Ext ).", "labels": [], "entities": []}, {"text": "The latter contains 100 compounds balanced for compositionality, not included in EN-comp (that is, not used in any of the preceding experiments).", "labels": [], "entities": [{"text": "EN-comp", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.7128088474273682}]}, {"text": "The results obtained on EN-comp Ext are shown in.", "labels": [], "entities": [{"text": "EN-comp Ext", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.884340226650238}]}, {"text": "They are comparable and mostly better than those for the oracle and for cross-validation.", "labels": [], "entities": []}, {"text": "As the items are different in the two data sets, a direct comparison of the results is not possible, but the equivalent performances confirm the robustness of the models and configurations for compositionality prediction.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 193, "end_pos": 220, "type": "TASK", "confidence": 0.9622776210308075}]}, {"text": "Moreover, these results are obtained in an unsupervised manner, as the compositionality scores are not used to train any of the models.", "labels": [], "entities": []}, {"text": "The scores are used only for comparative purposes for determining the impact of various factors in the ability of these DSMs to predict compositionality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Average number of answers per compound n, average standard deviation \u03c3, proportion of high  standard deviation P \u03c3>1.5 , for the compound (HM), head (H), and modifier (M).", "labels": [], "entities": [{"text": "average standard deviation \u03c3", "start_pos": 52, "end_pos": 80, "type": "METRIC", "confidence": 0.7542711496353149}]}, {"text": " Table 2  Spearman \u03c1 correlation between compositionality, frequency, and PMI for the three data sets.", "labels": [], "entities": [{"text": "PMI", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9681201577186584}]}, {"text": " Table 3  Summary of DSMs, their parameters, and evaluated parameter values. The combination of  these DSMs and their parameter values leads to 228 DSM configurations evaluated per language  (1 \u00d7 1 \u00d7 4 \u00d7 3 = 12 for PPMI-TopK, plus 6 \u00d7 3 \u00d7 4 \u00d7 3 = 216 for the other models).", "labels": [], "entities": []}, {"text": " Table 4  Highest results for each DSM, using BF 1 for Farahmand data set, Pearson r for Reddy (r), and  Spearman \u03c1 for all the other data sets. For English, in each pair of values, the first is for the  compounds found in the corpus, and the second uses fallback for missing compounds.", "labels": [], "entities": [{"text": "BF 1", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9780354797840118}, {"text": "Farahmand data set", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.9392635822296143}, {"text": "Pearson r", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9877388775348663}, {"text": "Spearman \u03c1", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9750564098358154}, {"text": "fallback", "start_pos": 255, "end_pos": 263, "type": "METRIC", "confidence": 0.9596937298774719}]}, {"text": " Table 5  Configurations with best performances on EN-comp and on EN-comp Ext . Best performances are  measured on EN-comp and the corresponding configurations are applied to EN-comp Ext .", "labels": [], "entities": [{"text": "EN-comp", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.959447979927063}, {"text": "EN-comp Ext", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.857096791267395}, {"text": "EN-comp", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.9323845505714417}, {"text": "EN-comp Ext", "start_pos": 175, "end_pos": 186, "type": "DATASET", "confidence": 0.8947809040546417}]}, {"text": " Table 6  Spearman \u03c1 for the proposed compositionality prediction scores, using the best DSM  configuration for each score.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.9436640441417694}]}, {"text": " Table 7  DSM and Separman \u03c1 of pc maxsim , as well as the average weights for the head (\u03b2) and for the  modifier (1 \u2212 \u03b2) on each data set.", "labels": [], "entities": [{"text": "DSM", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9710134863853455}, {"text": "Separman \u03c1", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.6163100302219391}]}, {"text": " Table 8  Outlier compounds with extreme positive/negative improv maxsim values. Example identifiers  correspond to numbers/letters shown in Figure 9.", "labels": [], "entities": []}, {"text": " Table 9  Outlier compounds with extreme positive/negative improv geom values. Example identifiers  correspond to numbers/letters shown on Figure 10.", "labels": [], "entities": []}, {"text": " Table 11  Spearman's \u03c1 of best pc uniform models, separated into 3 ranges according to \u03c3 HM and according to  hc HM , all with p < 0.05.", "labels": [], "entities": []}, {"text": " Table 12  Results using a higher number of iterations.", "labels": [], "entities": []}, {"text": " Table 13  Results for a higher minimum threshold of word count.", "labels": [], "entities": []}, {"text": " Table 14  Results using a window of size 2+2.", "labels": [], "entities": []}, {"text": " Table 15  Results for higher numbers of dimensions (PPMI-thresh).", "labels": [], "entities": []}, {"text": " Table 16  Configurations with highest \u03c1 avg for nondeterministic models.", "labels": [], "entities": []}, {"text": " Table 17  Intrinsic quality measures for the raw and filtered data sets.", "labels": [], "entities": [{"text": "Intrinsic quality", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.9428239464759827}]}, {"text": " Table 18  Extrinsic quality measures for the raw and filtered data sets.", "labels": [], "entities": []}]}