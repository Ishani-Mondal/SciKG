{"title": [{"text": "SNAG: Spoken Narratives and Gaze Dataset", "labels": [], "entities": []}], "abstractContent": [{"text": "Humans rely on multiple sensory modalities when examining and reasoning over images.", "labels": [], "entities": []}, {"text": "In this paper, we describe anew multimodal dataset that consists of gaze measurements and spoken descriptions collected in parallel during an image inspection task.", "labels": [], "entities": []}, {"text": "The task was performed by multiple participants on 100 general-domain images showing everyday objects and activities.", "labels": [], "entities": []}, {"text": "We demonstrate the usefulness of the dataset by applying an existing visual-linguistic data fusion framework in order to label important image regions with appropriate linguistic labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, eye tracking has become widespread, with applications ranging from VR to assistive communication (.", "labels": [], "entities": [{"text": "eye tracking", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.8494775891304016}, {"text": "assistive communication", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.6927783936262131}]}, {"text": "Gaze data, such as fixation location and duration, can reveal crucial information about where observers look and how long they look at those locations.", "labels": [], "entities": [{"text": "duration", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.997195839881897}]}, {"text": "Researchers have used gaze measurements to understand where drivers look and to identify differences in experts' and novices' viewing behaviors in domain-specific tasks (.", "labels": [], "entities": []}, {"text": "Numerous studies highlight the potential of gaze data to shed light on how humans process information, make decisions, and vary in observer behaviors (.", "labels": [], "entities": []}, {"text": "Eye tracking has also long been an important tool in psycholinguistics.", "labels": [], "entities": [{"text": "Eye tracking", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8140096962451935}]}, {"text": "Co-collecting observers' gaze information and spoken descriptions of visual input has the potential to provide insight into how humans understand what they see.", "labels": [], "entities": []}, {"text": "There is a need for public datasets containing both modalities.", "labels": [], "entities": []}, {"text": "In this paper, we present the Spoken Narratives and Gaze dataset (SNAG), which contains gaze information and spoken narratives co-captured from observers as they view general domain images.", "labels": [], "entities": []}, {"text": "We describe the data collection procedure using a high-quality eye-tracker, summary statistics of the multimodal data, and the results of applying a visual-lingustic alignment framework to automatically annotate regions of general-domain images, inspired by work on medical images.", "labels": [], "entities": []}, {"text": "Our main contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We provide the language and vision communities with a unique multimodal dataset 1 comprised of co-captured gaze and audio data, and transcriptions.", "labels": [], "entities": []}, {"text": "This dataset was collected via an image-inspection task with 100 general-domain images and American English speakers.", "labels": [], "entities": []}, {"text": "2. We demonstrate the usefulness of this general-domain dataset by applying an existing visual-linguistic annotation framework that successfully annotates image regions by combining gaze and language data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average alignment performance across images. MSFC provides the best recall and lowest AER,  and modified k-means the best precision. In all cases, the alignment framework yields stronger results  than either of the timing-based baselines.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9994326233863831}, {"text": "AER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9993368983268738}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9973347187042236}]}]}