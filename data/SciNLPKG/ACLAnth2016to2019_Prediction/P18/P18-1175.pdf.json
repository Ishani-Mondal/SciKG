{"title": [{"text": "Training Classifiers with Natural Language Explanations", "labels": [], "entities": []}], "abstractContent": [{"text": "Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification).", "labels": [], "entities": []}, {"text": "In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision.", "labels": [], "entities": []}, {"text": "A semantic parser converts these explanations into program-matic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier.", "labels": [], "entities": []}, {"text": "On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8406025469303131}, {"text": "F1 scores", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9753221273422241}]}, {"text": "Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.", "labels": [], "entities": []}], "introductionContent": [{"text": "The standard protocol for obtaining a labeled dataset is to have a human annotator view each example, assess its relevance, and provide a label (e.g., positive or negative for binary classification).", "labels": [], "entities": []}, {"text": "However, this only provides one bit of information per example.", "labels": [], "entities": []}, {"text": "This invites the question: how can we get more information per example, given that the annotator has already spent the effort reading and understanding an example?", "labels": [], "entities": []}, {"text": "Previous works have relied on identifying relevant parts of the input such as labeling features, highlighting rationale phrases in Both cohorts showed signs of optic nerve toxicity due to ethambutol.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the accuracy of BabbleLabble on three relation extraction tasks, which we refer to as Spouse, Disease, and Protein.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9995676875114441}, {"text": "relation extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7454159557819366}]}, {"text": "The goal of each task is to train a classifier for predicting whether the two entities in an example are participating in the relationship of interest, as described below.", "labels": [], "entities": []}, {"text": "Statistics for each dataset are reported in, with one example and one explanation for each given in and additional explanations shown in Appendix B. In the Spouse task, annotators were shown a sentence with two highlighted names and asked to label whether the sentence suggests that the two people are spouses.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9762669801712036}]}, {"text": "Sentences were pulled from the Signal Media dataset of news articles (Corney F1 scores obtained by a classifier trained with BabbleLabble (BL) using 30 explanations or with traditional supervision (TS) using the specified number of individually labeled examples.", "labels": [], "entities": [{"text": "Signal Media dataset", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.9562593102455139}, {"text": "Corney F1 scores", "start_pos": 70, "end_pos": 86, "type": "METRIC", "confidence": 0.6280149817466736}]}, {"text": "BabbleLabble achieves the same F1 score as traditional supervision while using fewer user inputs by a factor of over 5 (Protein) to over 100 (Spouse).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9886605739593506}]}, {"text": "et al., 2016).", "labels": [], "entities": []}, {"text": "Ground truth data was collected from Amazon Mechanical Turk workers, accepting the majority label over three annotations.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk workers", "start_pos": 37, "end_pos": 67, "type": "DATASET", "confidence": 0.9443760365247726}]}, {"text": "The 30 explanations we report on were sampled randomly from a pool of 200 that were generated by 10 graduate students unfamiliar with BabbleLabble.", "labels": [], "entities": []}, {"text": "In the Disease task, annotators were shown a sentence with highlighted names of a chemical and a disease and asked to label whether the sentence suggests that the chemical causes the disease.", "labels": [], "entities": []}, {"text": "Sentences and ground truth labels came from a portion of the 2015 BioCreative chemical-disease relation dataset, which contains abstracts from PubMed.", "labels": [], "entities": [{"text": "BioCreative chemical-disease relation dataset", "start_pos": 66, "end_pos": 111, "type": "DATASET", "confidence": 0.5861791893839836}, {"text": "PubMed", "start_pos": 143, "end_pos": 149, "type": "DATASET", "confidence": 0.967634916305542}]}, {"text": "Because this task requires specialized domain expertise, we obtained explanations by having someone unfamiliar with BabbleLabble translate from Python to natural language labeling functions from an existing publication that explored applying weak supervision to this task (.", "labels": [], "entities": [{"text": "BabbleLabble translate from Python to natural language labeling", "start_pos": 116, "end_pos": 179, "type": "TASK", "confidence": 0.5260406248271465}]}, {"text": "The Protein task was completed in conjunction with OccamzRazor, a neuroscience company targeting biological pathways of Parkinson's disease.", "labels": [], "entities": [{"text": "Protein task", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.8036424517631531}, {"text": "OccamzRazor", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9087939262390137}]}, {"text": "For this task, annotators were shown a sentence from the relevant biomedical literature with highlighted names of a protein and a kinase and asked to label whether or not the kinase influences the protein in terms of a physical interaction or phosphorylation.", "labels": [], "entities": []}, {"text": "The annotators had domain expertise but minimal programming experience, making BabbleLabble a natural fit for their use case.", "labels": [], "entities": []}, {"text": "Text documents are tokenized with spaCy.", "labels": [], "entities": []}, {"text": "The semantic parser is built on top of the Python-based https://github.com/explosion/spaCy implementation SippyCup.", "labels": [], "entities": []}, {"text": "On a single core, parsing 360 explanations takes approximately two seconds.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9617202877998352}]}, {"text": "We use existing implementations of the label aggregator, feature library, and discriminative classifier described in Sections 2.4-2.5 provided by the open-source project.", "labels": [], "entities": []}, {"text": "Hyperparameters for all methods we report were selected via random search over thirty configurations on the same held-out development set.", "labels": [], "entities": []}, {"text": "We searched over learning rate, batch size, L 2 regularization, and the subsampling rate (for improving balance between classes).", "labels": [], "entities": []}, {"text": "6 All reported F1 scores are the average value of 40 runs with random seeds and otherwise identical settings.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9992984533309937}]}, {"text": "We evaluate the performance of BabbleLabble with respect to its rate of improvement by number of user inputs, its dependence on correctly parsed logical forms, and the mechanism by which it utilizes logical forms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: F1 scores obtained by a classifier trained with BabbleLabble (BL) using 30 explanations  or with traditional supervision (TS) using the specified number of individually labeled examples.  BabbleLabble achieves the same F1 score as traditional supervision while using fewer user inputs  by a factor of over 5 (Protein) to over 100 (Spouse).", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993791580200195}, {"text": "F1 score", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9827743172645569}]}, {"text": " Table 4: The number of LFs generated from 30  explanations (pre-filters), discarded by the filter  bank, and remaining (post-filters), along with the  percentage of LFs that were correctly parsed from  their corresponding explanations.", "labels": [], "entities": []}, {"text": " Table 5: F1 scores obtained using BabbleLabble  with no filter bank (BL-FB), as normal (BL), and  with a perfect parser (BL+PP) simulated by hand.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9990025162696838}, {"text": "BL-FB", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.9416373372077942}]}]}