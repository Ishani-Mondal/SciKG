{"title": [], "abstractContent": [{"text": "An abugida is a writing system where the consonant letters represent syllables with a default vowel and other vowels are denoted by diacritics.", "labels": [], "entities": []}, {"text": "We investigate the feasibility of recovering the original text written in an abugida after omitting subordinate diacritics and merging consonant letters with similar phonetic values.", "labels": [], "entities": []}, {"text": "This is crucial for developing more efficient input methods by reducing the complexity in abugidas.", "labels": [], "entities": []}, {"text": "Four abugidas in the southern Brahmic family, i.e., Thai, Burmese, Khmer, and Lao, were studied using a newswire 20, 000-sentence dataset.", "labels": [], "entities": []}, {"text": "We compared the recovery performance of a support vector machine and an LSTM-based recurrent neural network, finding that the abugida graphemes could be recovered with 94%-97% accuracy at the top-1 level and 98%-99% at the top-4 level, even after omitting most diacritics (10-30 types) and merging the remaining 30-50 characters into 21 graphemes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9977439641952515}]}], "introductionContent": [{"text": "Writing systems are used to record utterances in a wide range of languages and can be organized into the hierarchy shown in.", "labels": [], "entities": []}, {"text": "The symbols in a writing system generally represent either speech sounds (phonograms) or semantic units (logograms).", "labels": [], "entities": []}, {"text": "Phonograms can be either segmental or syllabic, with segmental systems being more phonetic because they use separate symbols (i.e., letters) to represent consonants and vowels.", "labels": [], "entities": []}, {"text": "Segmental systems can be further subdivided depending on their representation of vowels.", "labels": [], "entities": []}, {"text": "Alphabets (e.g., the Latin, Cyrillic, and Greek scripts) are the most common and treat vowel and consonant let- ters equally.", "labels": [], "entities": []}, {"text": "In contrast, abjads (e.g., the Arabic and Hebrew scripts) do not write most vowels explicitly.", "labels": [], "entities": []}, {"text": "The third type, abugidas, also called alphasyllabary, includes features from both segmental and syllabic systems.", "labels": [], "entities": []}, {"text": "In abugidas, consonant letters represent syllables with a default vowel, and other vowels are denoted by diacritics.", "labels": [], "entities": []}, {"text": "Abugidas thus denote vowels less explicitly than alphabets but more explicitly than abjads, while being less phonetic than alphabets, but more phonetic than syllabaries.", "labels": [], "entities": []}, {"text": "Since abugidas combine segmental and syllabic systems, they typically have more symbols than conventional alphabets.", "labels": [], "entities": []}, {"text": "In this study, we investigate how to simplify and recover abugidas, with the aim of developing a more efficient method of encoding abugidas for input.", "labels": [], "entities": []}, {"text": "Alphabets generally do not have a large set of symbols, making them easy to map to a traditional keyboard, and logogram and syllabic systems need specially designed input methods because of their large variety of symbols.", "labels": [], "entities": []}, {"text": "Traditional input methods for abugidas are similar to those for alphabets, mapping two or three different symbols onto each key and requiring users to type each character and diacritic exactly.", "labels": [], "entities": []}, {"text": "In contrast, we are able to substantially simplify inputting abugidas by encoding them in a lossy (or \"fuzzy\") way.", "labels": [], "entities": []}, {"text": "Figure 3: Merging and omission for Thai (TH), Burmese (MY), Khmer (KM), and Lao (LO) scripts.", "labels": [], "entities": [{"text": "Merging", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9115636348724365}]}, {"text": "The MN row lists the mnemonics assigned to graphemes in our experiment.", "labels": [], "entities": []}, {"text": "In this study, the mnemonics can be assigned arbitrarily, and we selected Latin letters related to the real pronunciation wherever possible.", "labels": [], "entities": []}, {"text": "gives an overview of this study, showing examples in Khmer.", "labels": [], "entities": []}, {"text": "We simplify abugidas by omitting vowel diacritics and merging consonant letters with identical or similar phonetic values, as shown in (a).", "labels": [], "entities": []}, {"text": "This simplification is intuitive, both orthographically and phonetically.", "labels": [], "entities": []}, {"text": "To resolve the ambiguities introduced by the simplification, we use data-driven methods to recover the original texts, as shown in (b).", "labels": [], "entities": []}, {"text": "We conducted experiments on four southern Brahmic scripts, i.e., Thai, Burmese, Khmer, and Lao scripts, with a unified framework, using data from the Asian Language Treebank (ALT) (.", "labels": [], "entities": [{"text": "Asian Language Treebank (ALT)", "start_pos": 150, "end_pos": 179, "type": "DATASET", "confidence": 0.7699699997901917}]}, {"text": "The experiments show that the abugidas can be recovered satisfactorily by a recurrent neural network (RNN) using long short-term memory (LSTM) units, even when nearly all of the diacritics (10 -30 types) have been omitted and the remaining 30 -50 characters have been merged into 21 graphemes.", "labels": [], "entities": []}, {"text": "Thai gave the best performance, with 97% top-1 accuracy for graphemes and over 99% top-4 accuracy.", "labels": [], "entities": [{"text": "Thai", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9612500667572021}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9925187826156616}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9923025369644165}]}, {"text": "Lao, which gave the worst performance, still achieved the top-1 and top-4 accuracies of around 94% and 98%, respectively.", "labels": [], "entities": [{"text": "Lao", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9563488364219666}, {"text": "accuracies", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9912124276161194}]}, {"text": "The Burmese and Khmer results, which lay in-between the other two, were also investigated by manual evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used raw textual data from the ALT, 4 comprising around 20, 000 sentences translated from English.", "labels": [], "entities": [{"text": "ALT", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7929628491401672}]}, {"text": "The data were divided into training, development, and test sets as specified by the project.", "labels": [], "entities": []}, {"text": "For the SVM experiments, we used the offthe-shelf LIBLINEAR library ( wrapped by the KyTea toolkit.", "labels": [], "entities": [{"text": "SVM", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9155398011207581}, {"text": "KyTea toolkit", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9070684909820557}]}, {"text": "gives the recovery accuracies, demonstrating that recovery is not a difficult classification task, given well represented contextual features.", "labels": [], "entities": [{"text": "recovery accuracies", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8572492897510529}]}, {"text": "In general, using up to 5-gram features before/after the simplified grapheme yielded the best results for the baseline, except with Burmese, where 7-gram features brought a small additional improvement.", "labels": [], "entities": []}, {"text": "Because Burmese texts use relatively more spaces than the other three scripts, longer contexts help more.", "labels": [], "entities": []}, {"text": "Meanwhile, Lao produced the worst results, possibly because the omission and merging process was harsh: Lao is the most phonetic of the four scripts, with the least redundant spellings.", "labels": [], "entities": []}, {"text": "The LSTM-based RNN was implemented using DyNet (, and it was trained using Adam () with an initial learning rate of 10 \u22123 . If the accuracy decreased on the development set, the learning rate was halved, and learning was terminated when there was no improvement on the development set for three iterations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9975118637084961}]}, {"text": "We did not use dropout () but instead a voting ensemble over a set of differently initialized models trained in parallel, which is both more effective and faster.", "labels": [], "entities": []}, {"text": "As shown in, the RNN outperformed SVM on all scripts in terms of top-1 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9888285994529724}]}, {"text": "A more lenient evaluation, i.e., top-n accuracy, showed a satisfactory coverage of around 98% (Khmer and Lao) to 99% (Thai and Burmese) considering only the top four results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9755531549453735}, {"text": "coverage", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9907330274581909}]}, {"text": "shows the effect of changing the size of the training dataset by repeatedly halving it until it was one-eighth of its original size, demonstrating that the RNN outperformed SVM regardless of training data size.", "labels": [], "entities": []}, {"text": "The LSTM-based RNN should thus be a substantially better solution than the SVM for this task.", "labels": [], "entities": []}, {"text": "We also investigated Burmese and Khmer further using manual evaluation.", "labels": [], "entities": []}, {"text": "The results of RNN @1 \u229516 in were evaluated by native speakers, who examined the output writing units corresponding to each input simplified grapheme and classified the errors using four levels: 0) acceptable, i.e., alternative spelling, 1) clear and easy to identify the correct result, 2) confusing but possible to identify the correct result, and 3) incomprehensible.", "labels": [], "entities": []}, {"text": "For Burmese, most of the errors are at levels 1 and 2, and Khmer has a wider distribution.", "labels": [], "entities": [{"text": "errors", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9789182543754578}]}, {"text": "For both scripts, around 50% of the errors are serious (level 2 or 3), but the distributions suggest that they have different characteristics.", "labels": [], "entities": []}, {"text": "We are currently conducting a case study on these errors for further language-specific improvements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top-1 recovery accuracy for the SVM.  Here, \"Dev \u00b1m \" represents the results for the de- velopment set when using N -gram (N \u2208 [1, m])  features within m-grapheme windows of the sim- plified encodings, and \"Test\" represents the test set  results when using the feature set that gave the best  development set results. \"Leng.\" shows the ratio  of the number of characters in the simplified en- codings compared with the original strings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.969054639339447}]}, {"text": " Table 2: Top-n accuracy on the test set for the  LSTM-based RNN with an m-model ensemble  (RNN @n  \u2295m ). Here,  \u2020 and  \u2021 mean the RNN outper- formed the SVM with statistical significance at  p < 10 \u22122 and p < 10 \u22123 level, respectively, mea- sured by bootstrap re-sampling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9753124713897705}]}, {"text": " Table 3: Recovery error distribution.", "labels": [], "entities": [{"text": "Recovery error distribution", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.5751335819562277}]}]}