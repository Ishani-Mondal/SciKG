{"title": [{"text": "Improving Event Coreference Resolution by Modeling Correlations between Event Coreference Chains and Document Topic Structures", "labels": [], "entities": [{"text": "Improving Event Coreference Resolution", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9094705879688263}]}], "abstractContent": [{"text": "This paper proposes a novel approach for event coreference resolution that models correlations between event coreference chains and document topical structures through an Integer Linear Programming formulation.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.7652098337809244}]}, {"text": "We explicitly model correlations between the main event chains of a document with topic transition sentences , inter-coreference chain correlations , event mention distributional characteristics and sub-event structure, and use them with scores obtained from a local coreference relation classifier for jointly resolving multiple event chains in a document.", "labels": [], "entities": []}, {"text": "Our experiments across KBP 2016 and 2017 datasets suggest that each of the structures contribute to improving event coreference resolution performance.", "labels": [], "entities": [{"text": "KBP 2016 and 2017 datasets", "start_pos": 23, "end_pos": 49, "type": "DATASET", "confidence": 0.8879471540451049}, {"text": "event coreference resolution", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.8382546703020731}]}], "introductionContent": [{"text": "Event coreference resolution aims to identify and link event mentions in a document that refer to the same real-world event, which is vital for identifying the skeleton of a story and text understanding and is beneficial to numerous other NLP applications such as question answering and summarization.", "labels": [], "entities": [{"text": "Event coreference resolution", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7892305254936218}, {"text": "text understanding", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.6116359382867813}, {"text": "question answering", "start_pos": 264, "end_pos": 282, "type": "TASK", "confidence": 0.9188626706600189}, {"text": "summarization", "start_pos": 287, "end_pos": 300, "type": "TASK", "confidence": 0.9818288087844849}]}, {"text": "In spite of its importance, compared to considerable research for resolving coreferential entity mentions, far less attention has been devoted to event coreference resolution.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 146, "end_pos": 174, "type": "TASK", "confidence": 0.7465897997220358}]}, {"text": "Event coreference resolution thus remained a challenging task and the best performance remained low.", "labels": [], "entities": [{"text": "Event coreference resolution", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7940918604532877}]}, {"text": "Event coreference resolution presents unique challenges.", "labels": [], "entities": [{"text": "Event coreference resolution", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8457010388374329}]}, {"text": "Compared to entities, coreferential event mentions are fewer in a document and much more sparsely scattered across sentences.", "labels": [], "entities": []}, {"text": "Here, the main entity, \"President Chen\", appears frequently in ev- ery sentence, while the main event \"hearing\" and its accompanying event \"detention\" are mentioned much less frequently.", "labels": [], "entities": []}, {"text": "If we look more closely, referring back to the same entity serves a different purpose than referring to the same event.", "labels": [], "entities": []}, {"text": "The protagonist entity of a story is involved in many events and relations; thus, the entity is referred back each time such an event or relation is described.", "labels": [], "entities": []}, {"text": "In this example, the entity was mentioned when describing various events he participated or was involved in, including \"detention\", \"said\", \"pointed out\", \"remitted\", \"have a chance\", \"release\", \"cheating\", \"asked\" and \"returned\", as well as when describing several relations involving him, including \"former president\", \"his family\" and \"his wife\".", "labels": [], "entities": []}, {"text": "In contrast, most events only appear once in a text, and there is less motivation to repeat them: a story is mainly formed by a se-  ries of related but different events.", "labels": [], "entities": []}, {"text": "Essentially, (1) the same event is referred back only when anew aspect or further information of the event has to be described, and (2) repetitions of the same events are mainly used for content organization purposes and, consequently, correlate well with topic structures.", "labels": [], "entities": []}, {"text": "further shows the comparisons of positional patterns between event coreference and entity coreference chains, based on two benchmark datasets, ERE ( ) and ACE05 (), where we paired each event (entity) mention with its nearest antecedent event (entity) mention and calculated the percentage of (event vs. entity) coreferent mention pairs based on the number of sentences between two mentions.", "labels": [], "entities": [{"text": "ERE", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.7405207753181458}, {"text": "ACE05", "start_pos": 155, "end_pos": 160, "type": "DATASET", "confidence": 0.7979298830032349}]}, {"text": "Indeed, for entity coreference resolution, centering and nearness are striking properties (, and the nearest antecedent of an entity mention is mostly in the same sentence or in the immediately preceding sentence ( 70%).", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.7712697188059489}]}, {"text": "This is especially true for nominals and pronouns, two common types of entity mentions, where the nearest preceding mention that is also compatible in basic properties (e.g., gender, person and number) is likely to co-refer with the current mention.", "labels": [], "entities": []}, {"text": "In contrast, coreferential event mentions are rarely from the same sentence ( 10%) and are often sentences apart.", "labels": [], "entities": []}, {"text": "The sparse distribution of coreferent event mentions also applies to the three KBP corpora used in this work.", "labels": [], "entities": []}, {"text": "To address severe sparsity of event coreference relations in a document, we propose a holistic approach to identify coreference relations between event mentions by considering their correlations with document topic structures.", "labels": [], "entities": []}, {"text": "Our key observation is that event mentions make the backbone of a document and coreferent mentions of the same event play a key role in achieving a coherent content structure.", "labels": [], "entities": []}, {"text": "For example, in figure 1, the events \"hearing\" and \"detention\" were mentioned in the headline (H), in the first sentence (S1) as a story overview, in the second sentence (S2) for transitioning to the body section of the story describing what happened during the hearing, and then in the fifth sentence (S5) for transitioning to the ending section of the story describing what happened after the hearing.", "labels": [], "entities": []}, {"text": "By attaching individual event mentions to a coherent story and its topic structures, our approach recognizes event coreference relations that are otherwise not easily seen due to a mismatch of two event mentions' local contexts or long distances between event mentions.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7032192200422287}]}, {"text": "We model several aspects of correlations between event coreference chains and document level topic structures, in an Integer Linear Programming (ILP) joint inference framework.", "labels": [], "entities": []}, {"text": "Experimental results on the benchmark event coreference resolution dataset and show that the ILP system greatly improves event coreference resolution performance by modeling different aspects of correlations between event coreferences and document topic structures, which outperforms the previous best system on the same dataset consistently across several event coreference evaluation metrics.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6368237237135569}, {"text": "event coreference resolution", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.7181778947512308}]}], "datasetContent": [{"text": "We trained our ILP system on the KBP 2015 ( ) English dataset and evaluated the system on KBP 2016 and KBP 2017 English datasets . All the KBP corpora include documents from both discussion forum and news articles.", "labels": [], "entities": [{"text": "KBP 2015 ( ) English dataset", "start_pos": 33, "end_pos": 61, "type": "DATASET", "confidence": 0.902904619773229}, {"text": "KBP 2016 and KBP 2017 English datasets", "start_pos": 90, "end_pos": 128, "type": "DATASET", "confidence": 0.8339031849588666}]}, {"text": "But as the goal of this study is to leverage discourse level topic structure in a document for improving event coreference resolution performance, we only evaluate the ILP system using regular documents (news articles) in the KBP corpora.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.8150098323822021}, {"text": "KBP corpora", "start_pos": 226, "end_pos": 237, "type": "DATASET", "confidence": 0.9160361588001251}]}, {"text": "Specifically, we train our event extraction system and local coreference resolution classifier on 310 documents from the KBP 2015 corpus that consists of both discussion forum documents and news articles, tune the hyper-parameters corresponding to ILP using 50 news articles 6 from the KBP 2015 corpus and evaluate our system on The ECB+ ( corpus is another commonly used dataset for evaluating event coreference resolution performance.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7103263884782791}, {"text": "coreference resolution classifier", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.8370483915011088}, {"text": "KBP 2015 corpus", "start_pos": 121, "end_pos": 136, "type": "DATASET", "confidence": 0.9542883038520813}, {"text": "KBP 2015 corpus", "start_pos": 286, "end_pos": 301, "type": "DATASET", "confidence": 0.9207034309705099}, {"text": "ECB+ ( corpus", "start_pos": 333, "end_pos": 346, "type": "DATASET", "confidence": 0.9471453428268433}, {"text": "event coreference resolution", "start_pos": 395, "end_pos": 423, "type": "TASK", "confidence": 0.6430971821149191}]}, {"text": "But we determined that this corpus is not appropriate for evaluating our ILP model that explicitly focuses on using discourse level topic structures for event coreference resolution.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 153, "end_pos": 181, "type": "TASK", "confidence": 0.8482823570569357}]}, {"text": "Particularly, the ECB+ corpus was created to facilitate both cross-document and indocument event coreference resolution research.", "labels": [], "entities": [{"text": "ECB+ corpus", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.897161602973938}, {"text": "event coreference resolution research", "start_pos": 91, "end_pos": 128, "type": "TASK", "confidence": 0.731370821595192}]}, {"text": "Thus, the documents in the corpus were grouped based on several common topics and in each document, event mentions and coreference relations were only annotated selectively in sentences that are on a common topic.", "labels": [], "entities": []}, {"text": "When the annotated sentences in each document are stitched together, they do not well reveal the original document structure, which makes the ECB+ corpus a bad choice for evaluating our approach.", "labels": [], "entities": [{"text": "ECB+ corpus", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.9091923634211222}]}, {"text": "In addition, due to the selective annotation issue, in-document event coreference resolution with the ECB+ corpus is somewhat easier than with the KBP corpus, which partly explained the significant differences of published in-document event coreference resolution results on the two corpora.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.626154621442159}, {"text": "ECB+ corpus", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9344052076339722}, {"text": "KBP corpus", "start_pos": 147, "end_pos": 157, "type": "DATASET", "confidence": 0.9179053902626038}, {"text": "event coreference resolution", "start_pos": 235, "end_pos": 263, "type": "TASK", "confidence": 0.703268309434255}]}, {"text": "Each discussion forum document consists of a series of posts in an online discussion thread, which lacks coherent discourse structures as a regular document.", "labels": [], "entities": []}, {"text": "Therefore, only news articles in the KBP corpora are appropriate for evaluating our approach.", "labels": [], "entities": [{"text": "KBP corpora", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9395072758197784}]}, {"text": "6 KBP 2015 dataset consists of 181 and 179 documents from discussion forum and news articles respectively.", "labels": [], "entities": [{"text": "6 KBP 2015 dataset", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9051218330860138}]}, {"text": "We randomly picked 50 documents from news articles for tuning ILP hyper-parameters and remaining 310 documents for training classifiers.", "labels": [], "entities": []}, {"text": "news articles from the official KBP 2016 and 2017 evaluation corpora 7 respectively.", "labels": [], "entities": [{"text": "KBP 2016 and 2017 evaluation corpora 7", "start_pos": 32, "end_pos": 70, "type": "DATASET", "confidence": 0.8959922279630389}]}, {"text": "For direct comparisons, the results reported for the baselines, including the previous state-of-the-art model, were based on news articles in the test datasets as well.", "labels": [], "entities": []}, {"text": "We report the event coreference resolution results based on the version 1.8 of the official KBP 2017 scorer.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6952504515647888}, {"text": "KBP 2017 scorer", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9378821651140848}]}, {"text": "The scorer employs four coreference scoring measures, namely B 3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), MUC ( and BLANC (Recasens and Hovy, 2011) and the unweighted average of their F1 scores (AV G F 1 ).", "labels": [], "entities": [{"text": "B 3", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.977657675743103}, {"text": "CEAF e", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9708604216575623}, {"text": "MUC", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9724476933479309}, {"text": "BLANC", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9985664486885071}, {"text": "F1 scores", "start_pos": 190, "end_pos": 199, "type": "METRIC", "confidence": 0.979804664850235}, {"text": "AV G F 1 )", "start_pos": 201, "end_pos": 211, "type": "METRIC", "confidence": 0.9314931750297546}]}], "tableCaptions": [{"text": " Table 1: Percentages of adjacent (event vs. entity)  mention pairs based on the number of sentences  between two mentions.", "labels": [], "entities": []}, {"text": " Table 2: F1 scores for event mention extraction on  the KBP 2016 and 2017 corpus", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993370175361633}, {"text": "the KBP 2016 and 2017", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.8964937329292297}]}, {"text": " Table 3: Results for event coreference resolution systems on the KBP 2016 and 2017 corpus. Joint learning", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.8042670488357544}, {"text": "KBP 2016 and 2017 corpus", "start_pos": 66, "end_pos": 90, "type": "DATASET", "confidence": 0.9646009564399719}]}]}