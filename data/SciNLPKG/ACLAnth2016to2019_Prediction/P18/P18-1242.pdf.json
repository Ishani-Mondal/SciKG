{"title": [{"text": "Think Visually: Question Answering through Virtual Imagery", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8217040300369263}]}], "abstractContent": [{"text": "In this paper, we study the problem of geometric reasoning in the context of question-answering.", "labels": [], "entities": []}, {"text": "We introduce Dynamic Spatial Memory Network (DSMN), anew deep network architecture designed for answering questions that admit latent visual representations.", "labels": [], "entities": []}, {"text": "DSMN learns to generate and reason over such representations.", "labels": [], "entities": [{"text": "DSMN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8983890414237976}]}, {"text": "Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIn-tersection, to evaluate the geometric reasoning capability of QA systems.", "labels": [], "entities": []}, {"text": "Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to reason is a hallmark of intelligence and a requirement for building question-answering (QA) systems.", "labels": [], "entities": []}, {"text": "In AI research, reasoning has been strongly associated with logic and symbol manipulation, as epitomized by work in automated theorem proving.", "labels": [], "entities": [{"text": "symbol manipulation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7216725647449493}, {"text": "automated theorem proving", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.679516593615214}]}, {"text": "But for humans, reasoning involves not only symbols and logic, but also images and shapes.", "labels": [], "entities": []}, {"text": "Einstein famously wrote: \"The psychical entities which seem to serve as elements in thought are certain signs and more or less clear images which can be 'voluntarily' reproduced and combined...", "labels": [], "entities": []}, {"text": "Conventional words or other signs have to besought for laboriously only in a secondary state...\"", "labels": [], "entities": []}, {"text": "And the history of science abounds with discoveries from visual thinking, from the Benzene ring to the structure of DNA.", "labels": [], "entities": []}, {"text": "There are also plenty of ordinary examples of human visual thinking.", "labels": [], "entities": []}, {"text": "Consider a square room with a door in the middle of its southern wall.", "labels": [], "entities": []}, {"text": "Suppose you are standing in the room such that the eastern wall of the room is behind you.", "labels": [], "entities": []}, {"text": "Where is the door with respect to you?", "labels": [], "entities": []}, {"text": "The answer is 'to your left.'", "labels": [], "entities": []}, {"text": "Note that in this case both the question and answer are just text.", "labels": [], "entities": []}, {"text": "But in order to answer the question, it is natural to construct a mental picture of the room and use it in the process of reasoning.", "labels": [], "entities": []}, {"text": "Similar to humans, the ability to 'think visually' is desirable for AI agents like household robots.", "labels": [], "entities": []}, {"text": "An example could be to construct a rough map and navigation plan for an unknown environment from verbal descriptions and instructions.", "labels": [], "entities": []}, {"text": "In this paper, we investigate how to model geometric reasoning (a form of visual reasoning) using deep neural networks (DNN).", "labels": [], "entities": [{"text": "model geometric reasoning (a form of visual reasoning)", "start_pos": 37, "end_pos": 91, "type": "TASK", "confidence": 0.661389422416687}]}, {"text": "Specifically, we address the task of answering questions through geometric reasoning-both the question and answer are expressed in symbols or words, but a geometric representation is created and used as part of the reasoning process.", "labels": [], "entities": []}, {"text": "In order to focus on geometric reasoning, we do away with natural language by designing two synthetic QA datasets, FloorPlanQA and ShapeIntersection.", "labels": [], "entities": []}, {"text": "In FloorPlanQA, we provide the blueprint of a house in words and ask questions about location and orientation of objects in it.", "labels": [], "entities": []}, {"text": "For ShapeIntersection, we give a symbolic representation of various shapes and ask how many places they intersect.", "labels": [], "entities": []}, {"text": "In both datasets, a reference visual representation is provided for each sample.", "labels": [], "entities": []}, {"text": "Further, we propose Dynamic Spatial Memory Network (DSMN), a novel DNN that uses virtual imagery for QA.", "labels": [], "entities": []}, {"text": "DSMN is similar to existing memory networks () in that it uses vector embeddings of questions and memory modules to perform reasoning.", "labels": [], "entities": [{"text": "DSMN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7707277536392212}]}, {"text": "The main novelty of DSMN is that it creates virtual images for the input question and uses a spatial memory to aid the reasoning process.", "labels": [], "entities": []}, {"text": "We show through experiments that with the aid of an internal visual representation and a spatial memory, DSMN outperforms strong baselines on both FloorPlanQA and ShapeIntersection.", "labels": [], "entities": []}, {"text": "We also demonstrate that explicitly learning to create visual representations further improves performance.", "labels": [], "entities": []}, {"text": "Finally, we show that DSMN is substantially better than the baselines even when visual supervision is provided for only a small proportion of the samples.", "labels": [], "entities": [{"text": "DSMN", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.847949743270874}]}, {"text": "It's important to note that our proposed datasets consist of synthetic questions as opposed to natural texts.", "labels": [], "entities": []}, {"text": "Such a setup allows us to sidestep difficulties in parsing natural language and instead focus on geometric reasoning.", "labels": [], "entities": [{"text": "parsing natural language", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.8731169104576111}]}, {"text": "However, synthetic data lacks the complexity and diversity of natural text.", "labels": [], "entities": []}, {"text": "For example, spatial terms used in natural language have various ambiguities that need to resolved by context (e.g. how far is \"far\" and whether \"to the left\" is relative to the speaker or the listener)), but our synthetic data lacks such complexities.", "labels": [], "entities": []}, {"text": "Therefore, our method and results do not automatically generalize to real-life tasks involving natural language.", "labels": [], "entities": []}, {"text": "Additional research is needed to extend and validate our approach on natural data.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: First, we present Dynamic Spatial Memory Network (DSMN), a novel DNN that performs geometric reasoning for QA.", "labels": [], "entities": []}, {"text": "Second, we introduce two synthetic datasets that evaluate a system's visual thinking ability.", "labels": [], "entities": []}, {"text": "Third, we demonstrate that on synthetic data, DSMN achieves superior performance for answering questions that require visual thinking.", "labels": [], "entities": []}], "datasetContent": [{"text": "We introduce two synthetically-generated QA datasets to evaluate a system's goemetrical reasoning ability: FloorPlanQA and ShapeIntersection.", "labels": [], "entities": []}, {"text": "These datasets are not meant to test natural language understanding, but instead focus on geometrical reasoning.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.7295992771784464}]}, {"text": "Owing to their synthetic nature, they are easy to parse, but nevertheless they are still challenging for DNNs like DMN+ (Xiong et al., 2016) and) that achieved SOTA results on existing QA datasets (see).", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 185, "end_pos": 196, "type": "DATASET", "confidence": 0.7052564769983292}]}, {"text": "The proposed datasets are similar in spirit to bAbI , which is also synthetic.", "labels": [], "entities": []}, {"text": "In spite of its synthetic nature, bAbI has proved to be a crucial benchmark for the development of new models like MemN2N, DMN+, variants of which have proved successful in various natural domains (.", "labels": [], "entities": []}, {"text": "Our proposed dataset is first to explicitly test 'visual thinking', and its synthetic nature helps us avoid the expensive and tedious task of collecting human annotations.", "labels": [], "entities": []}, {"text": "Meanwhile, it is important to note that conclusions drawn from synthetic data do not automatically translate to natural data, and methods developed on synthetic benchmarks need additional validation on natural domains.", "labels": [], "entities": []}, {"text": "The proposed datasets also contain visual representations of the questions.", "labels": [], "entities": []}, {"text": "Each of them has 38,400 questions, evenly split into a training set, a validation set and a test set (12,800 each).", "labels": [], "entities": []}, {"text": "Baselines: LSTM (Hochreiter and Schmidhuber, 1997) is a popular neural network for sequence processing tasks.", "labels": [], "entities": [{"text": "sequence processing tasks", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.7495299875736237}]}, {"text": "We use two versions of LSTM-based baselines.", "labels": [], "entities": []}, {"text": "LSTM-1 is a common version that is used as a baseline for textual QA (.", "labels": [], "entities": [{"text": "LSTM-1", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7902124524116516}, {"text": "textual QA", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.5117056965827942}]}, {"text": "In LSTM-1, we concatenate all the sentences and the question to a single string.", "labels": [], "entities": []}, {"text": "For FloorPlanQA, we do word embedding look-up, while for ShapeIntersection, we project each real number into higher dimension via a series of FC layers.", "labels": [], "entities": []}, {"text": "The sequence of vectors is fed into an LSTM.", "labels": [], "entities": []}, {"text": "The final output vector of the LSTM is then used for prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 53, "end_pos": 63, "type": "TASK", "confidence": 0.9685768485069275}]}, {"text": "We develop another version of LSTM that we call LSTM-2, in which the question is concatenated to the description.", "labels": [], "entities": []}, {"text": "We use a two-level hierarchy to embed the description.", "labels": [], "entities": []}, {"text": "We first extract an embedding for each sentence.", "labels": [], "entities": []}, {"text": "For FloorPlanQA, we use an LSTM to get the sentence embeddings, and for ShapeIntersection, we use a series of FC layers.", "labels": [], "entities": []}, {"text": "We then feed the sentence embeddings into an LSTM, whose output is used for prediction.", "labels": [], "entities": []}, {"text": "Further, we compare our model to DMN+) and, which achieved state-of-the-art results on bAbI ).", "labels": [], "entities": []}, {"text": "In particular, we compare the 3-hop versions of DSMN, DMN+, and MemN2N.", "labels": [], "entities": [{"text": "DSMN", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.945246696472168}, {"text": "MemN2N", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9414581060409546}]}, {"text": "Training Details: We used ADAM ( to train all models, and the learning rate and the LSTM baselines on both datasets.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9109718799591064}, {"text": "LSTM baselines", "start_pos": 84, "end_pos": 98, "type": "METRIC", "confidence": 0.7738056182861328}]}, {"text": "However, we consider DSMN to be only slightly better than DMN+ because both are observed to be unstable across multiple runs and so the gap between the two has a large variance.", "labels": [], "entities": [{"text": "DSMN", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.9259961843490601}]}, {"text": "Finally, DSMN* outperforms all other approaches by a large margin on both datasets, which demonstrates the utility of visual supervision in proposed tasks.", "labels": [], "entities": []}, {"text": "While the variation can be significant across runs, if we run each model 10 times and choose the best run, we observe consistent results.", "labels": [], "entities": []}, {"text": "We visualized the intermediate visual representations, but when no visual supervision is pro- vided, they were not interpretable (sometimes they looked like random noise, sometimes blank).", "labels": [], "entities": []}, {"text": "In the case when visual supervision is provided, the intermediate visual representation is well-formed and similar to the ground-truth.", "labels": [], "entities": []}, {"text": "We further investigate how DSMN* performs when intermediate visual supervision is available for only a portion of training samples.", "labels": [], "entities": []}, {"text": "As shown in, DSMN* outperforms DMN+ by a large margin, even when intermediate visual supervision is provided for only 1% of the training samples.", "labels": [], "entities": []}, {"text": "This can be useful when obtaining visual representations is expensive and time-consuming.", "labels": [], "entities": []}, {"text": "One possible justification for why visual supervision (even in a small amount) helps a lot is that it constrains the high-dimensional space of possible intermediate visual representations.", "labels": [], "entities": []}, {"text": "With limited data and no explicit supervision, automatically learning these high-dimensional representations can be difficult.", "labels": [], "entities": []}, {"text": "Additonally, we performed ablation study (see) on the usefulness of final memory tag vector (m (T ) ) and 2D spatial memory (M ) in the answer feature vector f (see Eqn. 4).", "labels": [], "entities": []}, {"text": "We removed each of them one at a time, and retrained (with hyperparameter tuning) the DSMN and DSMN* models.", "labels": [], "entities": [{"text": "DSMN", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9762483239173889}, {"text": "DSMN", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.7713157534599304}]}, {"text": "Note that they are removed only from the final feature vector f , and both of them are still coupled.", "labels": [], "entities": []}, {"text": "The model with both tag and 2D spatial memory (f = En f (M (T ) ); m (T ) ; q ) performs slightly better than the only tag vector model (f = m (T ) ; q ).", "labels": [], "entities": []}, {"text": "Also, as expected the only 2D spatial memory model (f = En f (M (T ) ); q ) performs much better for DSMN* than DSMN becuase of the intermdiate supervision.", "labels": [], "entities": []}, {"text": "Further, shows the effect of varying the number of memory 'hops' for DSMN and DSMN*", "labels": [], "entities": [{"text": "DSMN", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9256717562675476}, {"text": "DSMN", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.925215482711792}]}], "tableCaptions": [{"text": " Table 2: Experimental results showing compari- son with baselines, and ablation study of DSMN", "labels": [], "entities": [{"text": "ablation", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9909172654151917}, {"text": "DSMN", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.9151923060417175}]}]}