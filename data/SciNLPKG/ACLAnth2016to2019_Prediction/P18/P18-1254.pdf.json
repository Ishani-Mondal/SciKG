{"title": [{"text": "Finding Syntax in Human Encephalography with Beam Search", "labels": [], "entities": []}], "abstractContent": [{"text": "Recurrent neural network grammars (RNNGs) are generative models of (tree, string) pairs that rely on neural networks to evaluate derivational choices.", "labels": [], "entities": [{"text": "Recurrent neural network grammars (RNNGs)", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7575845164912087}]}, {"text": "Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count.", "labels": [], "entities": []}, {"text": "When used as regressors against human electrophys-iological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak.", "labels": [], "entities": []}, {"text": "By contrast, a non-syntactic neural language model yields no reliable effects.", "labels": [], "entities": []}, {"text": "Model comparisons attribute the early peak to syntactic composition within the RNNG.", "labels": [], "entities": [{"text": "RNNG", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.8310723900794983}]}, {"text": "This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.", "labels": [], "entities": [{"text": "RNNG+beam search", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.6118012219667435}]}], "introductionContent": [{"text": "Computational psycholinguistics has \"always been...the thing that computational linguistics stood the greatest chance of providing to humanity\".", "labels": [], "entities": []}, {"text": "Within this broad area, cognitively-plausible parsing models are of particular interest.", "labels": [], "entities": []}, {"text": "They are mechanistic computational models that, at some level, do the same task people do in the course of ordinary language comprehension.", "labels": [], "entities": []}, {"text": "As such, they offer away to gain insight into the operation of the human sentence processing mechanism (for a review see.", "labels": [], "entities": []}, {"text": "As suggests, a promising place to look for such insights is at the intersection of (a) incremental processing, (b) broad coverage, and (c) neural signals from the human brain.", "labels": [], "entities": []}, {"text": "The contribution of the present paper is situated precisely at this intersection.", "labels": [], "entities": []}, {"text": "It combines a probabilistic generative grammar (RNNG;) with a parsing procedure that uses this grammar to manage a collection of syntactic derivations as it advances from one word to the next ().", "labels": [], "entities": []}, {"text": "Via well-known complexity metrics, the intermediate states of this procedure yield quantitative predictions about language comprehension difficulty.", "labels": [], "entities": []}, {"text": "Juxtaposing these predictions against data from human encephalography (EEG), we find that they reliably derive several amplitude effects including the P600, which is known to be associated with syntactic processing (e.g..", "labels": [], "entities": []}, {"text": "Comparison with language models based on long short term memory networks (LSTM, e.g. shows that these effects are specific to the RNNG.", "labels": [], "entities": []}, {"text": "A further analysis pinpoints one of these effects to RNNGs' syntactic composition mechanism.", "labels": [], "entities": [{"text": "RNNGs' syntactic composition", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.6993365287780762}]}, {"text": "These positive findings reframe earlier null results regarding the syntaxsensitivity of human processing.", "labels": [], "entities": []}, {"text": "They extend work with eyetracking (e.g.) and neuroimaging ( to higher temporal resolution.", "labels": [], "entities": []}, {"text": "1 Perhaps most significantly, they establish a general correspondence between a computational model and electrophysiological responses to naturalistic language.", "labels": [], "entities": []}, {"text": "Following this Introduction, section 2 presents recurrent neural network grammars, emphasizing their suitability for incremental parsing.", "labels": [], "entities": []}, {"text": "Sections 3 then reviews a previously-proposed beam search procedure for them.", "labels": [], "entities": [{"text": "beam search", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9022301733493805}]}, {"text": "Section 4 goes onto introduce the novel application of this procedure to human data via incremental complexity metrics.", "labels": [], "entities": []}, {"text": "Section 5 explains how these theoretical predictions are specifically brought to bear on EEG data using regression.", "labels": [], "entities": []}, {"text": "Sections 6 and 7 elaborate on the model comparison mentioned above and report the results in away that isolates the operative element.", "labels": [], "entities": []}, {"text": "Section 8 discusses these results in relation to established computational models.", "labels": [], "entities": []}, {"text": "The conclusion, to anticipate section 9, is that syntactic processing can be found in naturalistic speech stimuli if ambiguity resolution is modeled as beam search.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Penn Treebank development section bracketing accuracies (F1) under Word-Synchronous  beam search. These figures show that an incremental parser for RNNG can perform well on a stan- dard benchmark. \"ppl\" indicates the perplexity of over both trees and strings for the trained model on  the development set, averaged over words In all cases the word beam is set to a tenth of the action beam,  i.e. k word = k/10.", "labels": [], "entities": [{"text": "Penn Treebank development", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9768651525179545}, {"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9421654939651489}]}, {"text": " Table 4: Likelihood-ratio tests indicate that regression models with predictors derived from RNNGs with  syntactic composition (see", "labels": [], "entities": []}]}