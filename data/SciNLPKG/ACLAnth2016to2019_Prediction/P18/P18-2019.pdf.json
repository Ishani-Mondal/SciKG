{"title": [{"text": "Some of Them Can be Guessed! Exploring the Effect of Linguistic Context in Predicting Quantifiers", "labels": [], "entities": []}], "abstractContent": [{"text": "We study the role of linguistic context in predicting quantifiers ('few', 'all').", "labels": [], "entities": []}, {"text": "We collect crowdsourced data from human participants and test various models in a local (single-sentence) and a global context (multi-sentence) condition.", "labels": [], "entities": []}, {"text": "Models significantly out-perform humans in the former setting and are only slightly better in the latter.", "labels": [], "entities": []}, {"text": "While human performance improves with more linguistic context (especially on proportional quanti-fiers), model performance suffers.", "labels": [], "entities": []}, {"text": "Models are very effective in exploiting lexical and morpho-syntactic patterns; humans are better at genuinely understanding the meaning of the (global) context.", "labels": [], "entities": []}], "introductionContent": [{"text": "A typical exercise used to evaluate a language learner is the cloze deletion test.", "labels": [], "entities": []}, {"text": "In it, a word is removed and the learner must replace it.", "labels": [], "entities": []}, {"text": "This requires the ability to understand the context and the vocabulary in order to identify the correct word.", "labels": [], "entities": []}, {"text": "Therefore, the larger the linguistic context, the easier the test becomes.", "labels": [], "entities": []}, {"text": "It has been recently shown that higher-ability test takers rely more on global information, with lower-ability test takers focusing more on the local context, i.e. information contained in the words immediately surrounding the gap.", "labels": [], "entities": []}, {"text": "In this study, we explore the role of linguistic context in predicting generalized quantifiers ('few', 'some', 'most') in a cloze-test task (see).", "labels": [], "entities": []}, {"text": "Both human and model performance is evaluated in a local (single-sentence) and a global context (multi-sentence) condition to study the role of context and assess the cognitive plausibility of the models.", "labels": [], "entities": []}, {"text": "The reasons we are inter-: Given a target sentence st , or st with the preceding and following sentence, the task is to predict the target quantifier replaced by <qnt>.", "labels": [], "entities": []}, {"text": "ested in quantifiers are myriad.", "labels": [], "entities": []}, {"text": "First, quantifiers are of central importance in linguistic semantics and its interface with cognitive science.", "labels": [], "entities": [{"text": "linguistic semantics", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.709304928779602}]}, {"text": "Second, the choice of quantifier depends both on local context (e.g., positive and negative quantifiers license different patterns of anaphoric reference) and global context (the degree of positivity/negativity is modulated by discourse specificity) ().", "labels": [], "entities": []}, {"text": "Third and more generally, the ability of predicting function words in the cloze test represents a benchmark test for human linguistic competence.", "labels": [], "entities": [{"text": "predicting function words", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.8980706135431925}]}, {"text": "We conjecture that human performance will be boosted by more context and that this effect will be stronger for proportional quantifiers (e.g. 'few', 'many', 'most') than for logical quantifiers (e.g. 'none', 'some', 'all') because the former are more dependent on discourse context.", "labels": [], "entities": []}, {"text": "In contrast, we expect models to be very effective in exploiting the local context () but to suffer with a broader context, due to their reported inability to handle longer sequences (.", "labels": [], "entities": []}, {"text": "The best mod-els are very effective in the local context condition, where they significantly outperform humans.", "labels": [], "entities": []}, {"text": "Moreover, model performance declines with more context, whereas human performance is boosted by the higher accuracy with proportional quantifiers like 'many' and 'most'.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.998290479183197}]}, {"text": "Finally, we show that best-performing models and humans make similar errors.", "labels": [], "entities": []}, {"text": "In particuar, they tend to confound quantifiers that denote a similar 'magnitude' (.", "labels": [], "entities": []}, {"text": "First, we present anew task and results for training models to learn semantically-rich function words.", "labels": [], "entities": []}, {"text": "1 Second, we analyze the role of linguistic context in both humans and the models, with implications for cognitive plausibility and future modeling work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test our hypotheses, we need linguistic contexts containing quantifiers.", "labels": [], "entities": []}, {"text": "To ensure similarity in the syntactic environment of the quantifiers, we focus on partitive uses: where the quantifier is followed by the preposition 'of'.", "labels": [], "entities": []}, {"text": "To avoid any effect of intensifiers like 'very' and 'so' and adverbs like 'only' and 'incredibly', we study only sentences in which the quantifier occurs at the beginning (see).", "labels": [], "entities": []}, {"text": "We experiment with a set of 9 quantifiers: 'a few', 'all', 'almost all', 'few', 'many', 'more than half', 'most', 'none', 'some'.", "labels": [], "entities": []}, {"text": "This set strikes the best trade-off between number of quantifiers and their frequency in our source corpus, a large collection of written English including around 3B tokens.", "labels": [], "entities": []}, {"text": "One dataset -1-Sentcontains datapoints that only include the sentence with the quantifier (the target sentence, st ).", "labels": [], "entities": []}, {"text": "The second -3-Sent -contains datapoints that are 3-sentence long: the target sentence (s t ) together with both the preceding (s p ) and following one (s f ).", "labels": [], "entities": []}, {"text": "To directly analyze the effect of the linguistic context in the task, the target sentences are exactly the same in both settings.", "labels": [], "entities": []}, {"text": "Indeed, 1-Sent is obtained by simply extracting all target sentences <s t > from 3-Sent (<s p , st , sf >).", "labels": [], "entities": []}, {"text": "The 3-Sent dataset is built as follows: (1) We split our source corpus into sentences and select those starting with a 'quantifier of' construction.", "labels": [], "entities": []}, {"text": "Around 391K sentences of this type are found.", "labels": [], "entities": []}, {"text": "We tokenize the sentences and replace the quantifier at the beginning of the sentence (the target quantifier) with the string <qnt>, to treat all target quantifiers as a single token.", "labels": [], "entities": []}, {"text": "(3) We filter out sentences longer than 50 tokens (less than 6% of the total), yielding around 369K sentences.", "labels": [], "entities": []}, {"text": "(4) We select all cases for which both the preceding and the following sentence are at most 50-tokens long.", "labels": [], "entities": []}, {"text": "We also ensure that the target quantifier does not occur again in the target sentence.", "labels": [], "entities": []}, {"text": "(5) We ensure that each datapoint <s p , st , sf > is unique.", "labels": [], "entities": []}, {"text": "The distribution of target quantifiers across the resulting 309K datapoints ranges from 1152 cases ('more than half') to 93801 cases ('some').", "labels": [], "entities": [{"text": "309K datapoints", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.7325809597969055}]}, {"text": "To keep the dataset balanced, we randomly select 1150 points for each quantifier, resulting in a dataset of 10350 datapoints.", "labels": [], "entities": []}, {"text": "This was split into train (80%), validation (10%), and test (10%) sets while keeping the balancing.", "labels": [], "entities": []}, {"text": "Then, 1-Sent is obtained by extracting the target sentences <s t > from <s p , st , sf >.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy of models and humans. Values  in bold are the highest in the column. *Note that  due to an imperfect balancing of data, chance level  for humans (computed as majority class) is 0.124.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9924530386924744}, {"text": "chance level", "start_pos": 139, "end_pos": 151, "type": "METRIC", "confidence": 0.9582357704639435}]}, {"text": " Table 3: Responses by humans (top) and  AttCon-LSTM (bottom) in 3-Sent (val). Val- ues in bold are the highest in the row.", "labels": [], "entities": [{"text": "AttCon-LSTM", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.7805134654045105}]}]}