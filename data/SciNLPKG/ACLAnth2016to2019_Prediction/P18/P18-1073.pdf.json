{"title": [{"text": "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training.", "labels": [], "entities": []}, {"text": "However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios.", "labels": [], "entities": []}, {"text": "This work proposes an alternative approach based on a fully un-supervised initialization that explicitly exploits the structural similarity of the em-beddings, and a robust self-learning algorithm that iteratively improves this solution.", "labels": [], "entities": []}, {"text": "Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems.", "labels": [], "entities": []}, {"text": "Our implementation is released as an open source project at https://github.", "labels": [], "entities": []}, {"text": "com/artetxem/vecmap.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-lingual embedding mappings have shown to bean effective way to learn bilingual word embeddings (.", "labels": [], "entities": []}, {"text": "The underlying idea is to independently train the embeddings in different languages using monolingual corpora, and then map them to a shared space through a linear transformation.", "labels": [], "entities": []}, {"text": "This allows to learn high-quality cross-lingual representations without expensive supervision, opening new research avenues like unsupervised neural machine translation (.", "labels": [], "entities": [{"text": "unsupervised neural machine translation", "start_pos": 129, "end_pos": 168, "type": "TASK", "confidence": 0.6858137995004654}]}, {"text": "While most embedding mapping methods rely on a small seed dictionary, adversarial training has recently produced exciting results in fully unsupervised settings ().", "labels": [], "entities": []}, {"text": "However, their evaluation has focused on particularly favorable conditions, limited to closely-related languages or comparable Wikipedia corpora.", "labels": [], "entities": []}, {"text": "When tested on more realistic scenarios, we find that they often fail to produce meaningful results.", "labels": [], "entities": []}, {"text": "For instance, none of the existing methods works in the standard EnglishFinnish dataset from, obtaining translation accuracies below 2% in all cases (see Section 5).", "labels": [], "entities": [{"text": "EnglishFinnish dataset", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.975793868303299}, {"text": "accuracies", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.6495332717895508}]}, {"text": "On another strand of work, showed that an iterative self-learning method is able to bootstrap a high quality mapping from very small seed dictionaries (as little as 25 pairs of words).", "labels": [], "entities": []}, {"text": "However, their analysis reveals that the self-learning method gets stuck in poor local optima when the initial solution is not good enough, thus failing for smaller training dictionaries.", "labels": [], "entities": []}, {"text": "In this paper, we follow this second approach and propose anew unsupervised method to build an initial solution without the need of a seed dictionary, based on the observation that, given the similarity matrix of all words in the vocabulary, each word has a different distribution of similarity values.", "labels": [], "entities": []}, {"text": "Two equivalent words in different languages should have a similar distribution, and we can use this fact to induce the initial set of word pairings (see).", "labels": [], "entities": []}, {"text": "We combine this initialization with a more robust self-learning method, which is able to start from the weak initial solution and iteratively improve the mapping.", "labels": [], "entities": []}, {"text": "Coupled together, we provide a fully unsupervised crosslingual mapping method that is effective in realistic settings, converges to a good solution in all cases tested, and sets anew state-of-the-art in bilingual lexicon extraction, even surpassing previous supervised methods.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 203, "end_pos": 231, "type": "TASK", "confidence": 0.7095820705095927}]}, {"text": "Figure 1: Motivating example for our unsupervised initialization method, showing the similarity distributions of three words (corresponding to the smoothed density estimates from the normalized square root of the similarity matrices as defined in Section 3.2).", "labels": [], "entities": []}, {"text": "Equivalent translations (two and due) have more similar distributions than non-related words (two and cane -meaning dog).", "labels": [], "entities": []}, {"text": "This observation is used to build an initial solution that is later improved through self-learning.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following common practice, we evaluate our method on bilingual lexicon extraction, which measures the accuracy of the induced dictionary in comparison to a gold standard.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.7407099803288778}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9991625547409058}]}, {"text": "As discussed before, previous evaluation has focused on favorable conditions.", "labels": [], "entities": []}, {"text": "In particular, existing unsupervised methods have almost exclusively been tested on Wikipedia corpora, which is comparable rather than monolingual, exposing a strong cross-lingual signal that is not available in strictly unsupervised settings.", "labels": [], "entities": []}, {"text": "In addition to that, some datasets comprise unusually small embeddings, with only 50 dimensions and around 5,000-10,000 vocabulary items (.", "labels": [], "entities": []}, {"text": "As the only exception,  report positive results on the English-Italian dataset of  in addition to their main experiments, which are carried out in Wikipedia.", "labels": [], "entities": [{"text": "English-Italian dataset", "start_pos": 55, "end_pos": 78, "type": "DATASET", "confidence": 0.7270843684673309}]}, {"text": "While this dataset does use strictly monolingual corpora, it still corresponds to a pair of two relatively close indo-european languages.", "labels": [], "entities": []}, {"text": "In order to get a wider picture of how our method compares to previous work in different conditions, including more challenging settings, we carryout our experiments in the widely used dataset of  and the subsequent extensions of, which together comprise English-Italian, English-German, English-Finnish and EnglishSpanish.", "labels": [], "entities": [{"text": "EnglishSpanish", "start_pos": 308, "end_pos": 322, "type": "DATASET", "confidence": 0.950785219669342}]}, {"text": "More concretely, the dataset consists of 300-dimensional CBOW embeddings trained on WacKy crawling corpora (English, Italian, German), Common Crawl (Finnish) and WMT News Crawl (Spanish).", "labels": [], "entities": [{"text": "WacKy crawling corpora", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.890623410542806}, {"text": "Common", "start_pos": 135, "end_pos": 141, "type": "DATASET", "confidence": 0.9212818741798401}, {"text": "WMT News Crawl", "start_pos": 162, "end_pos": 176, "type": "DATASET", "confidence": 0.9231396118799845}]}, {"text": "The gold standards were derived from dictionaries built from Europarl word alignments and available at OPUS, split in a test set of 1,500 entries and a training set of 5,000 that we do not use in our experiments.", "labels": [], "entities": [{"text": "Europarl word alignments", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.9098669290542603}, {"text": "OPUS", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.9463385343551636}]}, {"text": "The datasets are freely available.", "labels": [], "entities": []}, {"text": "As a non-european agglutinative language, the English-Finnish pair is particularly challeng-: Results of unsupervised methods on the dataset of.", "labels": [], "entities": []}, {"text": "We perform 10 runs for each method and report the best and average accuracies (%), the number of successful runs (those with >5% accuracy) and the average runtime (minutes).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.8821116089820862}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9967719912528992}]}], "tableCaptions": [{"text": " Table 2: Results of unsupervised methods on the dataset of Dinu et al. (2015) and the extensions of  Artetxe et al. (", "labels": [], "entities": []}, {"text": " Table 3: Accuracy (%) of the proposed method in comparison with previous work. * Results obtained  with the official implementation from the authors.  \u2020 Results obtained with the framework from Artetxe", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989722967147827}, {"text": "Artetxe", "start_pos": 195, "end_pos": 202, "type": "DATASET", "confidence": 0.8988160490989685}]}]}