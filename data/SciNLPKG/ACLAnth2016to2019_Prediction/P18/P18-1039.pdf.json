{"title": [{"text": "Using Intermediate Representations to Solve Math Word Problems", "labels": [], "entities": [{"text": "Solve Math Word Problems", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8675686717033386}]}], "abstractContent": [{"text": "To solve math word problems, previous statistical approaches attempt at learning a direct mapping from a problem description to its corresponding equation system.", "labels": [], "entities": []}, {"text": "However, such mappings do not include the information of a few higher-order operations that cannot be explicitly represented in equations but are required to solve the problem.", "labels": [], "entities": []}, {"text": "The gap between natural language and equations makes it difficult fora learned model to generalize from limited data.", "labels": [], "entities": []}, {"text": "In this work we present an intermediate meaning representation scheme that tries to reduce this gap.", "labels": [], "entities": []}, {"text": "We use a sequence-to-sequence model with a novel attention regularization term to generate the intermediate forms, then execute them to obtain the final answers.", "labels": [], "entities": []}, {"text": "Since the intermediate forms are latent, we propose an iterative labeling framework for learning by leveraging supervision signals from both equations and answers.", "labels": [], "entities": []}, {"text": "Our experiments show using intermediate forms out-performs directly predicting equations.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is a growing interest in math word problem solving (.", "labels": [], "entities": [{"text": "math word problem solving", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7646468132734299}]}, {"text": "It requires reasoning with respect to sets of numbers or variables, which is an essential capability in many other natural language understanding tasks.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.6522889236609141}]}, {"text": "Consider the math problems shown in.", "labels": [], "entities": []}, {"text": "To solve the problems, one needs to know how many numbers to be summed up (e.g. \"2 numbers/3 numbers\"), and the relation between * Work done while this author was an intern at Microsoft Research.", "labels": [], "entities": []}, {"text": "1) The sum of 2 numbers is 18.", "labels": [], "entities": []}, {"text": "The first number is 4 more than the second number.", "labels": [], "entities": []}, {"text": "Equations: x + y = 18, x = y + 4 2) The sum of 3 numbers is 15.", "labels": [], "entities": []}, {"text": "The larger number is 4 times the smallest and the middle number is 5.", "labels": [], "entities": []}, {"text": "Equations: x + y + z = 15, x = 4 * z, y = 5: Math word problems.", "labels": [], "entities": []}, {"text": "Equations have lost the information of count, max, ordinal operations.", "labels": [], "entities": [{"text": "count", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9808371663093567}]}, {"text": "variables (\"the first/second number\").", "labels": [], "entities": []}, {"text": "However, an equation system does not encode these information explicitly.", "labels": [], "entities": []}, {"text": "For example, an equation represents \"the sum of 2 numbers\" as (x + y) and \"the sum of 3 numbers\" as (x + y + z).", "labels": [], "entities": []}, {"text": "This makes it difficult to generalize to cases unseen from data (e.g. \"the sum of 100 numbers\").", "labels": [], "entities": []}, {"text": "This paper presents anew intermediate meaning representation scheme for solving math problems, aiming at closing the semantic gap between natural language and equations.", "labels": [], "entities": []}, {"text": "To generate the intermediate forms, we adapt a sequence-to-sequence (seq2seq) network following recent work that tries to generate equations from problem descriptions for this task.", "labels": [], "entities": []}, {"text": "have shown that seq2seq models have the power to generate equations of which problem types do not exist in training data.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew method which adds an extra meaning representation and generate an intermediate form as output.", "labels": [], "entities": []}, {"text": "Additionally, we observe that the attention weights of the seq2seq model repetitively concentrates on numbers in the problem description.", "labels": [], "entities": []}, {"text": "To address the issue, we further propose to use a form of attention regularization.", "labels": [], "entities": [{"text": "attention regularization", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.6915617138147354}]}, {"text": "To train the model without explicit annotations of intermediate forms, we propose an iterative la-beling framework to leverage signals from both equations and their solutions.", "labels": [], "entities": []}, {"text": "We first derive possible intermediate forms with ambiguity using the gold-standard equation systems, and use these forms for training to get a pre-trained model.", "labels": [], "entities": []}, {"text": "Then we iteratively refine the intermediate forms using the learned model and the signals from the goldstandard answers.", "labels": [], "entities": []}, {"text": "We conduct experiments on two publicly available math problem datasets.", "labels": [], "entities": []}, {"text": "Our experimental results show that using the intermediate forms for training performs significantly better than directly mapping problems to equation systems.", "labels": [], "entities": []}, {"text": "Furthermore, our iterative labeling framework creates better labeled data with intermediate forms for training, which leads to improved performance.", "labels": [], "entities": []}, {"text": "To summarize, our contributions include: \u2022 We present anew intermediate meaning representation scheme for solving math problems.", "labels": [], "entities": []}, {"text": "\u2022 We design an iterative labeling framework to automatically augment training data with intermediate meaning representation.", "labels": [], "entities": []}, {"text": "\u2022 We propose using attention regularization in training to address the issue of incorrect attention in the seq2seq model.", "labels": [], "entities": []}, {"text": "\u2022 We verify the effectiveness of our proposed solutions by conducting experiments and analysis on real-world datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare our method against several strong baseline systems.", "labels": [], "entities": []}, {"text": "Following previous work, experiments are done in 5-fold cross validation: in each run, 20% is used for testing, 70% for training and 10% for validation.", "labels": [], "entities": []}, {"text": "Representation To make the task easier with less auxiliary nuisances (e.g. bracket pairs), we represent the intermediate forms in Polish notation.", "labels": [], "entities": []}, {"text": "Implementation details The dimension of encoder hidden state, decoder hidden state and embeddings are 100 in NumWord, 512 in Dolphin18K.", "labels": [], "entities": [{"text": "NumWord", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9215593934059143}, {"text": "Dolphin18K", "start_pos": 125, "end_pos": 135, "type": "DATASET", "confidence": 0.9596918821334839}]}, {"text": "All model parameters are initialized randomly with Gaussian distribution.", "labels": [], "entities": []}, {"text": "The hyperparameter \u03bb for the weight of attention regularization is set to 1.0 on NumWord and 0.4 on Dolphin18K.", "labels": [], "entities": [{"text": "NumWord", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9244031310081482}, {"text": "Dolphin18K", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.9778359532356262}]}, {"text": "We use SGD optimizer with decaying learning rate initialized as 0.5.", "labels": [], "entities": []}, {"text": "Dropout rate is set to 0.5.", "labels": [], "entities": [{"text": "Dropout rate", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.6330609321594238}]}, {"text": "The stopping criterion for training is validation accuracy with the maximum number of iterations no more than 150.", "labels": [], "entities": [{"text": "stopping", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9560824632644653}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9860803484916687}]}, {"text": "The vocabulary consists of words observed no less than N times in training set.", "labels": [], "entities": []}, {"text": "We set N = 1 for NumWord and N = 5 for Dolphin18K.", "labels": [], "entities": [{"text": "Dolphin18K", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.981074869632721}]}, {"text": "The beam size is set to 20 in the decoding stage.", "labels": [], "entities": []}, {"text": "For iterative training, we first train a model for N pre = 50 iterations for pre-training.", "labels": [], "entities": []}, {"text": "We tune the hyper-parameters on a separate dev set.", "labels": [], "entities": []}, {"text": "We consider the following models for comparisons: \u2022: a seq2seq model with attention mechanism.", "labels": [], "entities": []}, {"text": "As preprocessing, it replaces numbers in the math problem with tokens {n 1 , n 2 , ...}.", "labels": [], "entities": []}, {"text": "It generates equation as output and recovers {n 1 , n 2 , ...} to corresponding numbers in the post-processing.", "labels": [], "entities": []}, {"text": "\u2022 Seq2Seq Equ: we implement a seq2seq model with attention and copy mechanism.", "labels": [], "entities": []}, {"text": "Different from, it has the ability to copy numbers from problem description.", "labels": [], "entities": []}, {"text": "\u2022 Shi et al.: a rule-based system.", "labels": [], "entities": []}, {"text": "It parses math problems into Dolphin language trees with predefined grammars and reasons across trees to get the equations with rules.", "labels": [], "entities": []}, {"text": "We report numbers from their paper as the Dolphin language is not publicly available.", "labels": [], "entities": [{"text": "Dolphin language", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9550114274024963}]}, {"text": "\u2022 Huang et al.: the current state-of-theart model on Dolphin18K.", "labels": [], "entities": [{"text": "Dolphin18K", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9664738178253174}]}, {"text": "It is a featurebased model.", "labels": [], "entities": []}, {"text": "It generates candidate equations and find the most probable equation by ranking with predefined features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Statistics of latent forms on two datasets.  The percentage of problems with operator and  alignment ambiguity is shown in the 2nd and 3rd  columns respectively. We also show the average  number of intermediate forms of problems with  derivation ambiguity in the rightmost column.", "labels": [], "entities": []}, {"text": " Table 6: Performances on two datasets. \"LF\" means that the model generates latent intermediate forms  instead of equation systems. \"AttReg\" means attention regularization. \"Iter\" means iterative labeling.  \"n/a\" means that the model does not run on the dataset.", "labels": [], "entities": []}, {"text": " Table 7: Performance with and without pre- training in iterative labeling.", "labels": [], "entities": []}]}