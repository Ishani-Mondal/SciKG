{"title": [{"text": "Analogical Reasoning on Chinese Morphological and Semantic Relations", "labels": [], "entities": [{"text": "Analogical Reasoning on Chinese Morphological and Semantic Relations", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.6904310323297977}]}], "abstractContent": [{"text": "Analogical reasoning is effective in capturing linguistic regularities.", "labels": [], "entities": [{"text": "Analogical reasoning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8484126627445221}]}, {"text": "This paper proposes an analogical reasoning task on Chinese.", "labels": [], "entities": [{"text": "analogical reasoning task", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.912224272886912}]}, {"text": "After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations.", "labels": [], "entities": []}, {"text": "A big and balanced dataset CA8 is then built for this task, including 17813 questions.", "labels": [], "entities": []}, {"text": "Furthermore, we systematically explore the influences of vector representations, context features, and corpora on analogical reasoning.", "labels": [], "entities": []}, {"text": "With the experiments, CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, the boom of word embedding draws our attention to analogical reasoning on linguistic regularities.", "labels": [], "entities": [{"text": "analogical reasoning", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8307810425758362}]}, {"text": "Given the word representations, analogy questions can be automatically solved via vector computation, e.g. \"apples -apple + car \u2248 cars\" for morphological regularities and \"kingman + woman \u2248 queen\" for semantic regularities.", "labels": [], "entities": []}, {"text": "Analogical reasoning has become a reliable evaluation method for word embeddings.", "labels": [], "entities": [{"text": "Analogical reasoning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8535837531089783}]}, {"text": "In addition, It can be used in inducing morphological transformations, detecting semantic relations, and translating unknown words.", "labels": [], "entities": [{"text": "translating unknown words", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.9052722851435343}]}, {"text": "It is well known that linguistic regularities vary a lot among different languages.", "labels": [], "entities": []}, {"text": "For example, Chinese is atypical analytic language which lacks inflection.", "labels": [], "entities": []}, {"text": "shows that function words and reduplication are used to denote grammatical and semantic information.", "labels": [], "entities": []}, {"text": "In addition, many semantic \u2020 Corresponding author.", "labels": [], "entities": []}, {"text": "relations are closely related with social and cultural factors, e.g. in Chinese \"sh\u00af \u0131-xi\u00af an\" (god of poetry) refers to the poet Li-bai and \"sh\u00af \u0131-sh\u00e8ng\" (saint of poetry) refers to the poet Du-fu.", "labels": [], "entities": []}, {"text": "However, few attempts have been made in Chinese analogical reasoning.", "labels": [], "entities": [{"text": "Chinese analogical reasoning", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6885359485944113}]}, {"text": "The only Chinese analogy dataset is translated from part of an English dataset () (denote as CA_translated).", "labels": [], "entities": []}, {"text": "Although it has been widely used in evaluation of word embeddings, it could not serve as a reliable benchmark since it includes only 134 unique Chinese words in three semantic relations (capital, state, and family), and morphological knowledge is not even considered.", "labels": [], "entities": []}, {"text": "Therefore, we would like to investigate linguistic regularities beneath Chinese.", "labels": [], "entities": []}, {"text": "By modeling them as an analogical reasoning task, we could further examine the effects of vector offset methods in detecting Chinese morphological and semantic relations.", "labels": [], "entities": [{"text": "detecting Chinese morphological and semantic relations", "start_pos": 115, "end_pos": 169, "type": "TASK", "confidence": 0.8077210386594137}]}, {"text": "As far as we know, this is the first study focusing on Chinese analogical reasoning.", "labels": [], "entities": [{"text": "Chinese analogical reasoning", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.7006117105484009}]}, {"text": "Moreover, we release a standard benchmark for evaluation of Chinese word embedding, together with 36 open-source pre-trained embeddings at GitHub 1 , which could serve as a solid basis for Chinese NLP tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In Chinese analogical reasoning task, we aim at investigating to what extent word vectors capture the linguistic relations, and how it is affected by three important factors: vector representations (sparse and dense), context features (character, word, and ngram), and training corpora (size and domain).", "labels": [], "entities": [{"text": "Chinese analogical reasoning task", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.6745457649230957}]}, {"text": "shows the hyper-parameters used in this work.", "labels": [], "entities": []}, {"text": "All the text data used in our experiments (as shown in) are preprocessed via the following steps: \u2022 Remove the html and xml tags from the texts and set the encoding as utf-8.", "labels": [], "entities": []}, {"text": "Digits and punctuations are remained.", "labels": [], "entities": []}, {"text": "\u2022 Convert traditional Chinese characters into simplified characters with Open Chinese Convert (OpenCC) 2 . \u2022 Conduct Chinese word segmentation with HanLP(v_1.5.3) 3 .", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.5776485006014506}, {"text": "HanLP", "start_pos": 148, "end_pos": 153, "type": "DATASET", "confidence": 0.8887500762939453}]}], "tableCaptions": [{"text": " Table 1: Comparisons of CA_translated and CA8 benchmarks. More details about the relations in CA8  can be seen in GitHub.", "labels": [], "entities": []}, {"text": " Table 2: Hyper-parameter details. Levy and Goldberg (2014b) unifies SGNS and PPMI in a framework,  which share the same hyper-parameter settings. We exploit 3COSMUL to solve the analogical questions  suggested by Levy and Goldberg (2014a).", "labels": [], "entities": []}, {"text": " Table 3: Detailed information of the corpora. #tokens denotes the number of tokens in corpus. |V |  denotes the vocabulary size.", "labels": [], "entities": []}, {"text": " Table 4: Performance of word representations learned under different configurations. Baidubaike is used  as the training corpus. The top 1 results are in bold.", "labels": [], "entities": [{"text": "Baidubaike", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.8959120512008667}]}, {"text": " Table 5: Performance of word representations learned upon different training corpora by SGNS with  context feature of word. The top 2 results are in bold.", "labels": [], "entities": []}]}