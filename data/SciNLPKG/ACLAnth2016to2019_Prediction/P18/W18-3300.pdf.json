{"title": [{"text": "Table of Contents Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acous- tic modalities Recognizing Emotions in Video Using Multimodal DNN Feature Fusion Multimodal Relational Tensor Network for Sentiment and Emotion Classification Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data Sentiment Analysis using Imperfect Views from Spoken Language and Acoustic Modalities Polarity and Intensity: the Two Aspects of Sentiment Analysis ASR-based Features for Emotion Recognition: A Transfer Learning Approach Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis DNN Multimodal Fusion Techniques for Predicting Video Sentiment Grand Challenge and Workshop Program", "labels": [], "entities": [{"text": "Scalable multimodal sentiment classification", "start_pos": 56, "end_pos": 100, "type": "TASK", "confidence": 0.6684752553701401}, {"text": "Multimodal Emotion Recognition from Speech and Text Data Sentiment Analysis", "start_pos": 320, "end_pos": 395, "type": "TASK", "confidence": 0.7455814987421036}, {"text": "Sentiment Analysis ASR-based", "start_pos": 506, "end_pos": 534, "type": "TASK", "confidence": 0.7177173693974813}, {"text": "Emotion Recognition", "start_pos": 548, "end_pos": 567, "type": "TASK", "confidence": 0.7350093424320221}, {"text": "Predicting Video Sentiment Grand Challenge", "start_pos": 716, "end_pos": 758, "type": "TASK", "confidence": 0.8212209105491638}]}], "abstractContent": [], "introductionContent": [{"text": "Welcome to the First Grand Challenge and Workshop on Human Multimodal Language.", "labels": [], "entities": []}, {"text": "This grand challenge is co-located with ACL 2018 in Melbourne, Australia.", "labels": [], "entities": []}, {"text": "During this grand challenge, we aim to gauge the performance of current natural language processing models in understanding the complete form of human language: from language, vision and acoustic modalities all used in a coordinated manner to convey intentions.", "labels": [], "entities": []}, {"text": "Communicating using multimodal language (verbal and nonverbal) shares a significant portion of our communication including face-to-face communication, video chatting, and social multimedia opinion sharing.", "labels": [], "entities": []}, {"text": "Hence, it's computational analysis is centric to NLP research.", "labels": [], "entities": [{"text": "computational analysis", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7219974994659424}]}, {"text": "The challenges of modeling human multimodal language can be split into two major categories: 1) studying each modality individually and modeling each in a manner that can be linked to other modalities (also known as intramodal dynamics) 2) linking the modalities by modeling the interactions between them (also known as intermodal dynamics).", "labels": [], "entities": []}, {"text": "Common forms of these interactions include complementary or correlated information across modes.", "labels": [], "entities": []}, {"text": "Intrinsic to each modality, modeling human multimodal language is complex due to factors such as idiosyncrasy in communicative styles, non-trivial alignment between modalities and unreliable or contradictory information across modalities.", "labels": [], "entities": []}, {"text": "Therefore computational analysis becomes a challenging research area.", "labels": [], "entities": [{"text": "computational analysis", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8693566620349884}]}], "datasetContent": [], "tableCaptions": []}