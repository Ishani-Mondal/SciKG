{"title": [{"text": "The OSU Realizer for SRST '18: Neural Sequence-to-Sequence Inflection and Incremental Locality-Based Linearization", "labels": [], "entities": [{"text": "OSU Realizer", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7007511854171753}, {"text": "SRST '18", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.9237716794013977}, {"text": "Neural Sequence-to-Sequence Inflection", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.6622505883375803}]}], "abstractContent": [{"text": "Surface realization is a nontrivial task as it involves taking structured data and producing grammatically and semantically correct utterances.", "labels": [], "entities": [{"text": "Surface realization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7431083917617798}]}, {"text": "Many competing grammar-based and statistical models for realization still struggle with relatively simple sentences.", "labels": [], "entities": []}, {"text": "For our submission to the 2018 Surface Realization Shared Task, we tackle the shallow task by first generating inflected wordforms with a neural sequence-to-sequence model before incre-mentally linearizing them.", "labels": [], "entities": [{"text": "Surface Realization Shared Task", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.802925281226635}]}, {"text": "For lineariza-tion, we use a global linear model trained using early update that makes use of features that take into account the dependency structure and dependency locality.", "labels": [], "entities": []}, {"text": "Using this pipeline sufficed to produce surprisingly strong results in the shared task.", "labels": [], "entities": []}, {"text": "In future work, we intend to pursue joint approaches to linearization and morphological inflection and incorporating a neu-ral language model into the linearization choices.", "labels": [], "entities": []}], "introductionContent": [{"text": "We participated in the surface track of the 2018 Surface Realization Shared Task ().", "labels": [], "entities": [{"text": "Surface Realization Shared Task", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7830334603786469}]}, {"text": "In the surface track, task inputs were created by extracting sentences in 10 languages from the Universal Dependency treebanks corpus, scrambling the words and converting them to their citation form.", "labels": [], "entities": [{"text": "Universal Dependency treebanks corpus", "start_pos": 96, "end_pos": 133, "type": "DATASET", "confidence": 0.77691850066185}]}, {"text": "The task was then to generate a natural and semantically adequate sentence by inflecting and ordering the words.", "labels": [], "entities": []}, {"text": "Our aims in participating in the shared task were twofold.", "labels": [], "entities": []}, {"text": "First, we aimed to investigate the extent to which neural sequence-to-sequence models developed for the 2016 and 2017 SIGMOR-PHON shared tasks on morphological reinflection ( could be adapted to the more realistic setting for generation of SRST '18.", "labels": [], "entities": [{"text": "SRST '18", "start_pos": 240, "end_pos": 248, "type": "TASK", "confidence": 0.8639137546221415}]}, {"text": "Second, we aimed to investigate the extent to which dependency locality) features previously shown to be important for grammar-based generation in English () and in corpus-based studies of syntactic choice would also prove effective with incremental, dependency-based linearization () across languages.", "labels": [], "entities": [{"text": "grammar-based generation", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.7351702749729156}]}, {"text": "At an overview level, our system treats the task of surface realization as a simple two-stage process.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7681461870670319}]}, {"text": "First, we convert uninflected lexemes to fully inflected wordforms using the grammatical features supplied by the UD corpus; and second, we incrementally linearize the inflected words using the supplied syntactic dependencies, grammatical features and locality-based features that take dependency length and phrase size into account.", "labels": [], "entities": [{"text": "UD corpus", "start_pos": 114, "end_pos": 123, "type": "DATASET", "confidence": 0.854334831237793}]}, {"text": "A simple rule-based detokenizer attaches punctuation to adjacent words in a final step.", "labels": [], "entities": []}, {"text": "The system was trained using only the supplied data.", "labels": [], "entities": []}, {"text": "We leave for future work investigating ways to jointly make inflection and linearization choices and to incorporate a neural language model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Morphological inflection results on the development set compared to baseline results of simply  copying the lemma or using the most frequent inflected wordform.", "labels": [], "entities": []}, {"text": " Table 5: Automatic metric results for combined system on development and test sets, along with ablation  results with no locality features (NoLoc) for the dev set.", "labels": [], "entities": [{"text": "ablation", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.951651394367218}]}, {"text": " Table 6: Non-projective dependency prevalence, recall and precision in the development set.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9995481371879578}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9995081424713135}]}]}