{"title": [], "abstractContent": [{"text": "To be informative, an evaluation must measure how well systems generalize to realistic unseen data.", "labels": [], "entities": []}, {"text": "We identify limitations of and propose improvements to current evaluations of text-to-SQL systems.", "labels": [], "entities": []}, {"text": "First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications.", "labels": [], "entities": []}, {"text": "To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset.", "labels": [], "entities": []}, {"text": "Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate how the common practice of anonymiz-ing variables during evaluation removes an important challenge of the task.", "labels": [], "entities": []}, {"text": "Our observations highlight key difficulties, and our methodology enables effective measurement of future development.", "labels": [], "entities": []}], "introductionContent": [{"text": "Effective natural language interfaces to databases (NLIDB) would give laypeople access to vast amounts of data stored in relational databases.", "labels": [], "entities": []}, {"text": "This paper identifies key oversights in current evaluation methodology for this task.", "labels": [], "entities": []}, {"text": "In the process, we (1) introduce anew, challenging dataset, (2) standardize and fix many errors in existing datasets, and (3) propose a simple yet effective baseline system.", "labels": [], "entities": []}, {"text": "1 * The first two authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "Code and data is available at https://github.", "labels": [], "entities": []}, {"text": "com/jkkummerfeld/text2sql-data/: Traditional question-based splits allow queries to appear in both train and test.", "labels": [], "entities": []}, {"text": "Our querybased split ensures each query is in only one.", "labels": [], "entities": []}, {"text": "First, we consider query complexity, showing that human-written questions require more complex queries than automatically generated ones.", "labels": [], "entities": [{"text": "query complexity", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.8333903849124908}]}, {"text": "To illustrate this challenge, we introduce Advising, a dataset of questions from university students about courses that lead to particularly complex queries.", "labels": [], "entities": []}, {"text": "Second, we identify an issue in the way examples are divided into training and test sets.", "labels": [], "entities": []}, {"text": "The standard approach, shown at the top of, divides examples based on the text of each question.", "labels": [], "entities": []}, {"text": "As a result, many of the queries in the test set are seen in training, albeit with different entity names and with the question phrased differently.", "labels": [], "entities": []}, {"text": "This means metrics are mainly measuring robustness to the way a set of known SQL queries can be expressed in English-still a difficult problem, but not a complete test of ability to compose new queries in a familiar domain.", "labels": [], "entities": []}, {"text": "We introduce a template-based slot-filling baseline that cannot generalize to new queries, and yet is competitive with prior work on multiple datasets.", "labels": [], "entities": []}, {"text": "To measure robustness to new queries, we propose splitting based on the SQL query.", "labels": [], "entities": []}, {"text": "We show that stateof-the-art systems with excellent performance on traditional question-based splits struggle on querybased splits.", "labels": [], "entities": []}, {"text": "We also consider the common practice of variable anonymization, which removes a challenging form of ambiguity from the task.", "labels": [], "entities": []}, {"text": "In the process, we apply extensive effort to standardize datasets and fix a range of errors.", "labels": [], "entities": []}, {"text": "Previous NLIDB work has led to impressive systems, but current evaluations provide an incomplete picture of their strengths and weaknesses.", "labels": [], "entities": []}, {"text": "In this paper, we provide new and improved data, anew baseline, and guidelines that complement existing metrics, supporting future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation to be informative it must use data that is representative of real-world queries.", "labels": [], "entities": []}, {"text": "If datasets have biases, robust comparisons of models will require evaluation on multiple datasets.", "labels": [], "entities": []}, {"text": "For example, some datasets, such as ATIS and Advising, were collected from users and are taskoriented, while others, such as WikiSQL, were produced by automatically generating queries and engaging people to express the query in language.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.795630156993866}]}, {"text": "If these two types of datasets differ systematically, evaluation on one may not reflect performance on the other.", "labels": [], "entities": []}, {"text": "In this section, we provide descriptive statistics aimed at understanding how several datasets differ, especially with respect to query redundancy and complexity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Manually identified duplicate queries  (different SQL for equivalent questions).", "labels": [], "entities": []}, {"text": " Table 2: Descriptive statistics for text-to-SQL datasets. Datasets in the first group are human-generated  from the NLP community, in the second are human-generated from the DB community, and in the third  are automatically-generated. [1]/[2] is Question count / Unique query count.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of neural text-to-SQL systems on English question splits ('?' columns) and SQL query  splits ('Q' columns). The vertical line separates datasets from the NLP (left) and DB (right) communities.  Results for Iyer et al. (2017) are slightly lower here than in the original paper because we evaluate on  SQL output, not the database response.", "labels": [], "entities": [{"text": "SQL query  splits", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.6220987637837728}]}, {"text": " Table 4: Types of errors by the attention-based copying model for question and query splits, with  (Count)s of queries in each category, and the (\u00b5 Length) of gold queries in the category.", "labels": [], "entities": [{"text": "question and query splits", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6550454869866371}, {"text": "Count)s", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9363131125768026}, {"text": "\u00b5 Length)", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.7898320158322653}]}]}