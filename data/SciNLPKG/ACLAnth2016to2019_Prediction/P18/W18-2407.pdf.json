{"title": [{"text": "Named-Entity Tagging and Domain adaptation for Better Customized Translation", "labels": [], "entities": [{"text": "Named-Entity Tagging", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6052162349224091}, {"text": "Domain adaptation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7080744653940201}, {"text": "Customized Translation", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7395196855068207}]}], "abstractContent": [{"text": "Customized translation need pay special attention to the target domain terminology especially the named-entities for the domain.", "labels": [], "entities": []}, {"text": "Adding linguistic features to neural machine translation (NMT) has been shown to benefit translation in many studies.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.8512275815010071}, {"text": "translation", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.9692341089248657}]}, {"text": "In this paper, we further demonstrate that adding named-entity (NE) feature with named-entity recognition (NER) into the source language produces better translation with NMT.", "labels": [], "entities": [{"text": "translation", "start_pos": 153, "end_pos": 164, "type": "TASK", "confidence": 0.9573691487312317}]}, {"text": "Our experiments show that by just including the different NE classes and boundary tags, we can increase the BLEU score by around 1 to 2 points using the standard test sets from WMT2017.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9861653745174408}, {"text": "WMT2017", "start_pos": 177, "end_pos": 184, "type": "DATASET", "confidence": 0.95844566822052}]}, {"text": "We also show that adding NE tags using NER and applying in-domain adaptation can be combined to further improve customized machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7086620628833771}]}], "introductionContent": [{"text": "As generic machine translation cannot deal well with the translation with local or specific domain context, customized machine translation is adopted to focus on the terminology of local or domain context especially for named-entities translation.", "labels": [], "entities": [{"text": "generic machine translation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.63748366634051}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7843864858150482}, {"text": "named-entities translation", "start_pos": 220, "end_pos": 246, "type": "TASK", "confidence": 0.7600347101688385}]}, {"text": "Neural machine translation (NMT) (Sutskever et al.,;) is a more recent and effective approach than the traditional statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7820012122392654}, {"text": "statistical machine translation (SMT)", "start_pos": 115, "end_pos": 152, "type": "TASK", "confidence": 0.7983704606691996}]}, {"text": "It uses a large recurrent neural network (RNN) to encode a source sentence into a vector, and uses another large network to generate sentence in the target language one word at a time using the source sentence embedding and the attention mechanism.", "labels": [], "entities": []}, {"text": "NMT has achieved impressive result by learning the translation as an end-to-end model ().", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8711024522781372}]}, {"text": "Conventional NMT systems do not use linguistic features explicitly.", "labels": [], "entities": []}, {"text": "They expect the NMT model to learn these complex sentence structures and linguistic features from big data as word embedding vectors.", "labels": [], "entities": []}, {"text": "However, because of uneven data distribution and high linguistic complexity, there is no guarantee that NMT can capture this information and produce proper translation in all cases, especially for those terms which do not occur very often.", "labels": [], "entities": []}, {"text": "Recently, researchers have shown the potential benefit of explicitly encoding the linguistic features into NMT.", "labels": [], "entities": []}, {"text": "proposed to include linguistic features (part-ofspeech tag, lemmatized form and dependency label, morphology) at NMT source encoder side.", "labels": [], "entities": []}, {"text": "instead incorporated syntactic information of target language as linearized, lexicalized constituency trees into NMT target decoder side.", "labels": [], "entities": []}, {"text": "Their experiments showed adding linguistic information at both the source and target side can be beneficial for NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9579774141311646}]}, {"text": "Based on these findings, in this paper, we propose to incorporate named-entity (NE) features to further improve neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.6206573247909546}]}, {"text": "Named entities play a crucial role in many monolingual and multilingual Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Proper NE identification will enhance the sentence structure understanding for NMT, and thus give better translation of the named entities as well as the whole sentence.", "labels": [], "entities": [{"text": "NE identification", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.9195655882358551}]}, {"text": "In general, named entities are more difficult to translate for NMT than SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9784213304519653}]}, {"text": "This is because and NMT is weaker in translating less frequent words as compared to SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9633687734603882}]}, {"text": "In addition, since there are different types of named entities, e.g. Person, Place, Organization, etc., so linguistically and logically speaking, the translation mechanisms for different types of named entities are also different.", "labels": [], "entities": []}, {"text": "Unlike other words or phrases which occur more frequent in the training corpus, NE expressions are quite flexible, they can be composed of any character or word; moreover, in real-world applications, new named entities can emerge everyday.", "labels": [], "entities": []}, {"text": "Thus, NMT need to pay special attention to named entities to enhance the overall translation quality.", "labels": [], "entities": []}, {"text": "Without NE context information, it is difficult to know the meaning of the words or entities with different meaning under ambiguous situation (\u6211 \u559c\u6b22 \u79cb\u6708\u3002\u79cb\u6708 can be interpreted as person name or natural phenomenon.", "labels": [], "entities": []}, {"text": "\u4e09\u5341\u516d\u884c can be interpreted as a number entity or an idiomatic expression).", "labels": [], "entities": []}, {"text": "It is also very difficult to translate number entities under never seen or rare situation.", "labels": [], "entities": []}, {"text": "There are many domain-based or locationbased named entities.", "labels": [], "entities": []}, {"text": "These named entities are often rare words in the document, and generally NMT cannot produce good translation for these local contexts with local named entities.", "labels": [], "entities": []}, {"text": "Identifying local named entities and generating their translation with local context is also a challenging task which we will address in this paper.", "labels": [], "entities": [{"text": "Identifying local named entities", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8759859204292297}, {"text": "generating their translation", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.5840098758538564}]}, {"text": "(e.g., the English name for \u5f20\u5fd7\u8d24 is 'Teo Chee Hean' in Singapore while it's pinyin translation is 'Zhang Zhi Xian' in China) To address the NE translation issue, some researchers work on separate models or methods while others incorporate these separate models/methods with the main NMT models (.", "labels": [], "entities": [{"text": "NE translation", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.9716384708881378}]}, {"text": "They use NER to identify and align the NE pairs at both of source and target sentences, then NE pairs are replaced with NE tags for training the model; at reference stage the NE tags at target are replaced by the separate NE translation model or bilingual NE dictionary.", "labels": [], "entities": []}, {"text": "The disadvantages of the replacement methods include NE information loss and NE alignment errors.", "labels": [], "entities": [{"text": "NE alignment errors", "start_pos": 77, "end_pos": 96, "type": "METRIC", "confidence": 0.5743085443973541}]}, {"text": "To avoid the complexity and disadvantages of separate model training and integration, in this paper, we add the NE type information and boundary information directly to the source sentence by a NER tool, we hope NMT will learn and understand the sentence better with this additional NE information.", "labels": [], "entities": []}, {"text": "NE classification based on context information is important for NMT to reduce translation error under various ambiguous situations.", "labels": [], "entities": [{"text": "NE classification", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9242525398731232}, {"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.959934651851654}]}, {"text": "A named entity can consist of a single word or several words, the boundary tag feature of the named entity will inform NMT model to treat these words as a single entity during translation.", "labels": [], "entities": []}, {"text": "Since named entities often contain local names or domain-specific names, however, the amount of local or domain-specific training data is often small.", "labels": [], "entities": []}, {"text": "Thus, in this paper we apply domain adaptation together with named entity features to make further improvement for local context or domainspecific translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7676529586315155}, {"text": "domainspecific translation", "start_pos": 132, "end_pos": 158, "type": "TASK", "confidence": 0.6842090934514999}]}], "datasetContent": [{"text": "We have conducted our experiments with bidirection translation between Chinese/English languages pair.", "labels": [], "entities": [{"text": "bidirection translation", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.6396099627017975}]}, {"text": "We select the first 7 million Chinese-English sentence pairs from United Nations Parallel Corpus v1.0 (, and data from LDC for the training corpus, we also select some indomain data from local context for domain adaption training.", "labels": [], "entities": [{"text": "United Nations Parallel Corpus v1.0", "start_pos": 66, "end_pos": 101, "type": "DATASET", "confidence": 0.9274527430534363}, {"text": "domain adaption training", "start_pos": 205, "end_pos": 229, "type": "TASK", "confidence": 0.7639307777086893}]}, {"text": "After filtering out the long sentences (Chinese character length > 60 or number of English words > 60), the total number of sentence pairs for training is around 7 million.", "labels": [], "entities": []}, {"text": "shows the corpus sources for training.", "labels": [], "entities": []}, {"text": "We use the tuning sets with in-domain content for the model tuning.", "labels": [], "entities": []}, {"text": "We use the standard test set from WMT 17 (http://www.statmt.org/wmt17/) to evaluate our model performance and compare with other models using same test set.", "labels": [], "entities": [{"text": "WMT 17", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8388921618461609}]}], "tableCaptions": [{"text": " Table 1: Training Data Corpus Selection.", "labels": [], "entities": [{"text": "Training Data Corpus Selection", "start_pos": 10, "end_pos": 40, "type": "DATASET", "confidence": 0.871688574552536}]}, {"text": " Table 3: BLEU scores for in-domain test sets", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988471269607544}]}]}