{"title": [{"text": "Tackling Adversarial Examples in QA via Answer Sentence Selection", "labels": [], "entities": [{"text": "Tackling Adversarial Examples in QA", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7918463110923767}, {"text": "Answer Sentence", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7444696128368378}]}], "abstractContent": [{"text": "Question answering systems deteriorate dramatically in the presence of adversar-ial sentences in articles.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8807520866394043}]}, {"text": "According to Jia and Liang (2017), the single BiDAF system (Seo et al., 2016) only achieves an F1 score of 4.8 on the ADDANY adver-sarial dataset.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9317656755447388}, {"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.999665379524231}, {"text": "ADDANY adver-sarial dataset", "start_pos": 118, "end_pos": 145, "type": "DATASET", "confidence": 0.8438093066215515}]}, {"text": "In this paper, we present a method to tackle this problem via answer sentence selection.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.792768140633901}]}, {"text": "Given a paragraph of an article and a corresponding query, instead of directly feeding the whole paragraph to the single BiDAF system, a sentence that most likely contains the answer to the query is first selected, which is done via a deep neural network based on Tree-LSTM (Tai et al., 2015).", "labels": [], "entities": []}, {"text": "Experiments on ADDANY adversarial dataset validate the effectiveness of our method.", "labels": [], "entities": [{"text": "ADDANY adversarial dataset", "start_pos": 15, "end_pos": 41, "type": "DATASET", "confidence": 0.7257266839345297}]}, {"text": "The F1 score has been improved to 52.3.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9636822938919067}]}], "introductionContent": [{"text": "Question answering is an important task in evaluating the ability of language understanding of machines.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9155528545379639}, {"text": "language understanding", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7393800318241119}]}, {"text": "Usually, given a paragraph and a corresponding question, a question answering system is supposed to generate the answer of this question from the paragraph.", "labels": [], "entities": [{"text": "question answering", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7165949493646622}]}, {"text": "By comparing the predicted answer with human-approved answers, the performance of the system can be assessed.", "labels": [], "entities": []}, {"text": "Recently, many systems have achieved great results on this task;.", "labels": [], "entities": []}, {"text": "However, show that these systems are very vulnerable to paragraphs with adversarial sentences.", "labels": [], "entities": []}, {"text": "For instance, the single BiDAF system (, which achieves an F1 of 75.5 on Standford Question Answering Dataset (SQuAD), deteriorates significantly to an F1 of 4.8 on the ADDANY adversarial dataset.", "labels": [], "entities": [{"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9993919134140015}, {"text": "Standford Question Answering Dataset (SQuAD)", "start_pos": 73, "end_pos": 117, "type": "DATASET", "confidence": 0.8543852482523236}, {"text": "F1", "start_pos": 152, "end_pos": 154, "type": "METRIC", "confidence": 0.9953800439834595}, {"text": "ADDANY adversarial dataset", "start_pos": 169, "end_pos": 195, "type": "DATASET", "confidence": 0.9071172078450521}]}, {"text": "Besides the single BiDAF, the single Match LSTM, the ensemble Match LSTM, and the ensemble BiDAF achieve an F1 of 7.6, 11.7, and 2.7 respectively in question answering on ADDANY adversarial dataset.", "labels": [], "entities": [{"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9995306730270386}, {"text": "ADDANY adversarial dataset", "start_pos": 171, "end_pos": 197, "type": "DATASET", "confidence": 0.8884085615475973}]}, {"text": "Therefore, question answering with adversarial sentences in paragraphs is a prominent issue and is the focus of this study.", "labels": [], "entities": [{"text": "question answering with adversarial sentences in paragraphs", "start_pos": 11, "end_pos": 70, "type": "TASK", "confidence": 0.8953462157930646}]}, {"text": "In this paper, we propose a method to improve the performance of the single BiDAF system 1 on ADDANY adversarial dataset.", "labels": [], "entities": [{"text": "ADDANY adversarial dataset", "start_pos": 94, "end_pos": 120, "type": "DATASET", "confidence": 0.8414602478345236}]}, {"text": "Given a paragraph and a corresponding question, our method works in two steps to generate an answer.", "labels": [], "entities": []}, {"text": "In the first step, a deep neural network named the QA Likelihood neural network is deployed to predict the likelihood of each sentence in the paragraph to bean answer sentence, i.e., the sentence that contains the answer.", "labels": [], "entities": []}, {"text": "The architecture and the loss of the QA Likelihood neural network follow the neural network for semantic relatedness proposed by.", "labels": [], "entities": [{"text": "QA Likelihood neural network", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.6322972178459167}]}, {"text": "Its main ingredient is the Tree-LSTM model.", "labels": [], "entities": []}, {"text": "While the neural network for semantic relatedness is used to predict the similarity between sentence A and B, the QA Likelihood neural network is used to predict if sentence A contains the answer to query B.", "labels": [], "entities": []}, {"text": "In the second step, only the sentence with the highest likelihood is paired with the question and passed to the single BiDAF to further output an answer.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.8034371733665466}]}, {"text": "In summary, compared to the original BiDAF that is an end-to-end question answering system, our method first selects a sentence that is most likely to bean answer sentence.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8231833577156067}, {"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7298545837402344}]}, {"text": "Since adversarial sentences are not supposed to contain the answer, they can be screened out.", "labels": [], "entities": []}, {"text": "Therefore, the distractions of adversarial sentences are reduced.", "labels": [], "entities": []}, {"text": "Experiments on ADDANY adversarial dataset demonstrates the effectiveness The contributions of this study are in three folds.", "labels": [], "entities": [{"text": "ADDANY adversarial dataset", "start_pos": 15, "end_pos": 41, "type": "DATASET", "confidence": 0.7301673889160156}]}, {"text": "First, to the best of our knowledge, it's the first work that tries to address the problem of Question Answering with Adversarial Examples.", "labels": [], "entities": [{"text": "Question Answering with Adversarial Examples", "start_pos": 94, "end_pos": 138, "type": "TASK", "confidence": 0.8320388078689576}]}, {"text": "Our results show the effectiveness of answer sentence selection to tackle adverserial sentences in ADDANY dataset.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7074615955352783}, {"text": "ADDANY dataset", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9527860879898071}]}, {"text": "Second, the power of sentence representation of Tree-LSTM has been demonstrated in different NLP tasks, such as semantic relatedness computation, machine translation evaluation and natural language inference; meanwhile, multiple methods have been proposed for answer sentence selection (.", "labels": [], "entities": [{"text": "semantic relatedness computation", "start_pos": 112, "end_pos": 144, "type": "TASK", "confidence": 0.7449309428532919}, {"text": "machine translation evaluation", "start_pos": 146, "end_pos": 176, "type": "TASK", "confidence": 0.8604329228401184}, {"text": "answer sentence selection", "start_pos": 260, "end_pos": 285, "type": "TASK", "confidence": 0.8216391801834106}]}, {"text": "We are the first to design a framework that illustrates the effectiveness of Tree-LSTM in answer sentence selection.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.8098233739535013}]}, {"text": "Third, two sampling methods are implemented to build the training set for the QA Likelihood neural network.", "labels": [], "entities": [{"text": "QA Likelihood neural network", "start_pos": 78, "end_pos": 106, "type": "DATASET", "confidence": 0.5688727647066116}]}, {"text": "We show that different sampling methods do influence the performance of question answering in this scenario.", "labels": [], "entities": [{"text": "question answering", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8058450222015381}]}], "datasetContent": [{"text": "As shows, the input of our system is a pair of sentences.", "labels": [], "entities": []}, {"text": "Thus, the training instances for the QA Likelihood neural network are in the form of sentence pairs.", "labels": [], "entities": []}, {"text": "They are sampled from the training set of SQuAD v1.1) that contains no adversarial sentences.", "labels": [], "entities": []}, {"text": "Specifically, there are 87,599 queries of 18,896 paragraphs in the training set of SQuAD v1.1.", "labels": [], "entities": []}, {"text": "While each query refers to one paragraph, a paragraph may refer to multiple queries.", "labels": [], "entities": []}, {"text": "For the k-th query Q k , by splitting its corresponding paragraph C k into separate sentences and combining them with the query, a set of sentence pairs is obtained, where D k represents the set of sentence pairs for the k-th query, m k is the number of sentences in the paragraph C k , S k i is the i-th sentence in C k . A sentence pair (S k i , Q k ) is called a positive instance if S k i contains the answer to Q k ; otherwise, it is called a negative instance.", "labels": [], "entities": []}, {"text": "Then, the union of the sets D k for all the 87,599 queries in SQuDA is In order to train our model properly and efficiently, both downsampling of D and undersampling of negative instances must be done.", "labels": [], "entities": []}, {"text": "In this paper, we implement two different sampling methods: pair-level sampling and paragraph-level sampling.", "labels": [], "entities": []}, {"text": "In pair-level sampling, 45,000 positive instances and 45,000 negative instances are randomly selected from D as the training set.", "labels": [], "entities": []}, {"text": "By contrast, in paragraph-level sampling, we first randomly select a query Q k without replacement, then one positive instance and one negative instance are randomly sampled from the set of sentence pairs D k . This operation is repeated until we get 45,000 positive instances and 45,000 negative instances.", "labels": [], "entities": []}, {"text": "Finally, two different training sets are generated by pair-level sampling and paragraphlevel sampling.", "labels": [], "entities": []}, {"text": "Each set has 90,000 instances.", "labels": [], "entities": []}, {"text": "The validation set with 3,000 instances are sampled through these two methods as well.", "labels": [], "entities": []}, {"text": "Our test set is Jia and Liang (2017)'s ADDANY adversarial dataset.", "labels": [], "entities": [{"text": "ADDANY adversarial dataset", "start_pos": 39, "end_pos": 65, "type": "DATASET", "confidence": 0.8154071966807047}]}, {"text": "It includes 1,000 paragraphs and each paragraph refers to only one query, i.e., 1,000 (C, Q) pairs.", "labels": [], "entities": []}, {"text": "By splitting and combining, 6,154 sentence pairs are obtained.", "labels": [], "entities": []}, {"text": "The dimension of GloVe word vectors () is set as 300.", "labels": [], "entities": []}, {"text": "The sentence scoring neural network is trained by) with a learning rate of 0.01 and a batch size of 25.", "labels": [], "entities": [{"text": "sentence scoring", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7682968080043793}, {"text": "learning rate", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9700252115726471}]}, {"text": "Model parameters are regularized by a 10 \u22124 strength of per-minibatch L 2 regularization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of QA with Adversarial Examples", "labels": [], "entities": [{"text": "QA", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.8899549841880798}]}, {"text": " Table 2: Results of Answer Sentence Selection", "labels": [], "entities": [{"text": "Answer Sentence Selection", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.9518568913141886}]}]}