{"title": [{"text": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "labels": [], "entities": [{"text": "Ruminating Reader", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8598637282848358}]}], "abstractContent": [{"text": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context.", "labels": [], "entities": []}, {"text": "To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader.", "labels": [], "entities": []}, {"text": "Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BIDAF).", "labels": [], "entities": []}, {"text": "We propose novel layer structures that construct a query aware context vector representation and fuse encoding representation with intermediate representation on top of BIDAF model.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 169, "end_pos": 174, "type": "METRIC", "confidence": 0.6065624356269836}]}, {"text": "We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure.", "labels": [], "entities": []}, {"text": "In experiments on SQuAD, we find that the Reader outper-forms the BIDAF baseline by 2.1 F1 score and 2.7 EM score.", "labels": [], "entities": [{"text": "BIDAF baseline", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.9638456106185913}, {"text": "F1 score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.98749840259552}, {"text": "EM score", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9861133098602295}]}, {"text": "Our analysis shows that different hops of the attention have different responsibilities in selecting answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The majority of recorded human knowledge is circulated in unstructured natural language.", "labels": [], "entities": []}, {"text": "It is tremendously valuable to allow machines to read and comprehend the text knowledge.", "labels": [], "entities": []}, {"text": "Machine comprehension (MC)-especially in the form of question answering (QA)-is therefore attracting a significant amount of attention from the machine learning community.", "labels": [], "entities": [{"text": "Machine comprehension (MC)-", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8550590634346008}, {"text": "question answering (QA)-", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8320883095264435}]}, {"text": "Recently introduced large-scale datasets like CNN/Daily Mail, the Stanford Question Answering Dataset (SQuAD;) and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO;) have  allow data-driven methods, including deep learning, to become viable.", "labels": [], "entities": [{"text": "CNN/Daily Mail", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.8790259510278702}, {"text": "Stanford Question Answering Dataset (SQuAD", "start_pos": 66, "end_pos": 108, "type": "DATASET", "confidence": 0.8233347435792288}, {"text": "Microsoft MAchine Reading COmprehension Dataset (MS-MARCO", "start_pos": 119, "end_pos": 176, "type": "DATASET", "confidence": 0.6036276859896523}]}, {"text": "Recent approaches toward solving machine comprehension tasks using neural networks can be viewed as falling into two broad categories: single-pass reasoners and multiple-pass reasoners.", "labels": [], "entities": []}, {"text": "Single-pass models read a question and a source text once and often adopt the differentiable attention mechanism that emphasizes important parts of the context related to the question.", "labels": [], "entities": []}, {"text": "BIDAF () represents one of the competitive single-pass models in Machine Comprehension.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9379045367240906}]}, {"text": "BIDAF uses a bi-directional attention matrix which calculates the correlations between each word pair in context and query to build queryaware context representation.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7706031799316406}]}, {"text": "However, BIDAF and some similar models miss some questions because they don't have the capacity to reflect on problematic candidate answers and revise their decisions.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.8679715394973755}]}, {"text": "When humans are reading a text with the goal of answering a question, they tend to read it multiple times to get a better understanding of the context and question, and to give a better response.", "labels": [], "entities": []}, {"text": "With this intuition, recent multi-pass models revisit the question and the context passage (or ruminate) to infer the relations between the context, the question and the answer.", "labels": [], "entities": []}, {"text": "We propose an extension of BIDAF, called Ruminating Reader, which uses a second pass of reading and reasoning to allow it to learn to avoid mistakes and to ensure that it is able to effectively use the full context when selecting an answer.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.712939977645874}, {"text": "Ruminating Reader", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7581821084022522}]}, {"text": "In addition to adding a second pass, we also introduce two novel layer types, the ruminate layers, which use gating mechanisms to fuse the obtained from the first and second passes.", "labels": [], "entities": []}, {"text": "We observe a surprising phenomenon that when an LSTM layer in the context ruminate layer takes same input in each timestep, it can produce useful representation for the gates.", "labels": [], "entities": []}, {"text": "In addition, we introduce an answerquestion similarity loss to penalize overlap between question and predicted answer, a common feature in the errors of our base model.", "labels": [], "entities": [{"text": "answerquestion similarity loss", "start_pos": 29, "end_pos": 59, "type": "METRIC", "confidence": 0.7730011741320292}]}, {"text": "This allows us to achieve an F1 score of 79.5 and Exact Match (EM) score of 70.6 on hidden test set, 1 an improvement of 2.2 F1 score and 2.9 EM on BIDAF.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9859816133975983}, {"text": "Exact Match (EM) score", "start_pos": 50, "end_pos": 72, "type": "METRIC", "confidence": 0.975217322508494}, {"text": "F1 score", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.986373096704483}, {"text": "EM", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9972080588340759}, {"text": "BIDAF", "start_pos": 148, "end_pos": 153, "type": "DATASET", "confidence": 0.5688152313232422}]}, {"text": "shows a high-level comparison between BIDAF and Ruminating Reader.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.5571209192276001}]}, {"text": "Our analysis shows that the first hop attention is responsible for identifying key informative word while the second hop is responsible for finding candidate answers.", "labels": [], "entities": []}, {"text": "The gates are effective in selecting useful information from different representations.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In Section 2 we define the problem to be solved and introduce the SQuAD task.", "labels": [], "entities": []}, {"text": "In Section 3 we introduce Ruminating Reader, focusing on the informationextracting and information-digesting components and how they integrate.", "labels": [], "entities": [{"text": "Ruminating Reader", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6804991960525513}]}, {"text": "Section 4 discusses related The latest results are listed at https://rajpurkar.", "labels": [], "entities": []}, {"text": "github.io/SQuAD-explorer/ work.", "labels": [], "entities": []}, {"text": "Section 5 presents the experimental setting, results and analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "Rajpurkar et al. provide an official evaluation script that allows us to measure F1 score and EM score by comparing the prediction and ground truth answers.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9896687865257263}, {"text": "EM score", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9909029603004456}]}, {"text": "Three answers are provided for each question.", "labels": [], "entities": []}, {"text": "The prediction is compared to each of the answer and best score is selected.", "labels": [], "entities": []}, {"text": "F1 score is defined by recall and precision of words and EM score, as Exact Match score, is defined as the score of 100% accuracy in prediction.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9854987561702728}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9991574287414551}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9962959885597229}, {"text": "EM score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9751810729503632}, {"text": "Exact Match score", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.7192065119743347}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9924383759498596}]}, {"text": "We do not use any kind of ensembling, and compare our results primarily with other single-model (non-ensemble) results.", "labels": [], "entities": []}, {"text": "The test set performance is evaluated using the CodaLab platform by a task administrator.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Selected results on SQuAD leader- board.", "labels": [], "entities": [{"text": "SQuAD leader- board", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.857432633638382}]}, {"text": " Table 3: Layer ablation results. The order of  the listing corresponds to the description in Ap- pendix A.1. CRL refers to context ruminate layer  and QRL refers to query ruminate layer.", "labels": [], "entities": []}]}