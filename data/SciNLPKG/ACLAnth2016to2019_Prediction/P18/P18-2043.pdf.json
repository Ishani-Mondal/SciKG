{"title": [], "abstractContent": [{"text": "We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words.", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8777149617671967}]}, {"text": "In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms.", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.9421372711658478}]}, {"text": "Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words ambiguity and polysemy leads to performance improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Continuous representations of words are used in many natural language processing (NLP) applications.", "labels": [], "entities": []}, {"text": "Using pre-trained high-quality word embeddings are most effective if not millions of training examples are available, which is true for most tasks in NLP (.", "labels": [], "entities": []}, {"text": "Recently, several unsupervised methods were introduced to learn word vectors from large corpora of texts (.", "labels": [], "entities": [{"text": "learn word vectors from large corpora of texts", "start_pos": 58, "end_pos": 104, "type": "TASK", "confidence": 0.7901701703667641}]}, {"text": "Learned vector representations have been shown to have useful and interesting properties.", "labels": [], "entities": []}, {"text": "For example, showed that vector operations such as subtraction or addition reflect semantic relations between words.", "labels": [], "entities": []}, {"text": "Despite all these properties it is hard to precisely evaluate embeddings because analogy relation or word similarity tasks measure learned information indirectly.", "labels": [], "entities": []}, {"text": "Quite recently introduced a more direct way for word embeddings evaluation.", "labels": [], "entities": [{"text": "word embeddings evaluation", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.7259140113989512}]}, {"text": "Authors suggested considering definition modeling as the evaluation task.", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.9835360944271088}]}, {"text": "In definition modeling vector representations of words are used for conditional generation of corresponding word definitions.", "labels": [], "entities": [{"text": "conditional generation of corresponding word definitions", "start_pos": 68, "end_pos": 124, "type": "TASK", "confidence": 0.7804659406344095}]}, {"text": "The primary motivation is that highquality word embedding should contain all useful information to reconstruct the definition.", "labels": [], "entities": []}, {"text": "The important drawback of definition models is that they cannot take into account words with several different meanings.", "labels": [], "entities": []}, {"text": "These problems are related to word disambiguation task, which is a common problem in natural language processing.", "labels": [], "entities": [{"text": "word disambiguation task", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.7748096386591593}, {"text": "natural language processing", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.6392074426015218}]}, {"text": "Such examples of polysemantic words as \"bank\" or \"spring\" whose meanings can only be disambiguated using their contexts.", "labels": [], "entities": []}, {"text": "In such cases, proposed models tend to generate definitions based on the most frequent meaning of the corresponding word.", "labels": [], "entities": []}, {"text": "Therefore, building models that incorporate word sense disambiguation is an important research direction in natural language processing.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6805978814760844}, {"text": "natural language processing", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.6514840026696523}]}, {"text": "In this work, we study the problem of word ambiguity in definition modeling task.", "labels": [], "entities": [{"text": "definition modeling task", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.9307676752408346}]}, {"text": "We propose several models which can be possible solutions to it.", "labels": [], "entities": []}, {"text": "One of them is based on recently proposed Adaptive Skip Gram model (, the generalized version of the original SkipGram Word2Vec, which can differ word meanings using word context.", "labels": [], "entities": [{"text": "SkipGram Word2Vec", "start_pos": 110, "end_pos": 127, "type": "DATASET", "confidence": 0.8155911564826965}]}, {"text": "The second one is the attention-based model that uses the context of a word being defined to determine components of embedding referring to relevant word meaning.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: (1) we introduce two models based on recurrent neural network (RNN) language models, (2) we collect new dataset of definitions, which is larger in number of unique words than proposed in and also supplement it with examples of the word usage (3) finally, in the experiment section we show that our models outperform previously proposed models and have the ability to generate definitions depending on the meaning of words.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of new dataset", "labels": [], "entities": []}, {"text": " Table 2: Examples of definitions generated by S + I-Attention model for the words and contexts from the  test set.", "labels": [], "entities": []}, {"text": " Table 3: Performance comparison between best  model proposed by Noraset et al. (2017) and our  models on the test set. Number in brackets means  number of LSTM layers. BLEU is averaged across  3 trials.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9989915490150452}]}, {"text": " Table 3. We see that both  models that utilize knowledge about meaning of  the word have better performance than the com- peting one. We generated definitions using S + I- Attention model with simple temperature sampling", "labels": [], "entities": []}]}