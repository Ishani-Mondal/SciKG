{"title": [{"text": "Context-Aware Neural Machine Translation Learns Anaphora Resolution", "labels": [], "entities": [{"text": "Context-Aware Neural Machine Translation Learns Anaphora", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7033964494864146}]}], "abstractContent": [{"text": "Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7218239307403564}]}, {"text": "We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed.", "labels": [], "entities": [{"text": "context-aware neural machine translation", "start_pos": 15, "end_pos": 55, "type": "TASK", "confidence": 0.6296666786074638}]}, {"text": "We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7529287934303284}]}, {"text": "We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora.", "labels": [], "entities": []}, {"text": "It is consistent with gains for sentences where pronouns need to be gen-dered in translation.", "labels": [], "entities": []}, {"text": "Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and oversimple concatenation of the context and source sentences (+0.6).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9983003735542297}]}], "introductionContent": [{"text": "It has long been argued that handling discourse phenomena is important in translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9755005240440369}]}, {"text": "Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent.", "labels": [], "entities": []}, {"text": "Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7460337281227112}]}, {"text": "Earlier research on this topic focused on handling specific phenomena, such as translating pronouns, discourse connectives), verb tense (, increasing lexical consistency), or topic adaptation (, with special-purpose features engineered to model these phenomena.", "labels": [], "entities": [{"text": "topic adaptation", "start_pos": 175, "end_pos": 191, "type": "TASK", "confidence": 0.8260675370693207}]}, {"text": "However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6328869958718618}]}, {"text": "While the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled.", "labels": [], "entities": []}, {"text": "Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited.", "labels": [], "entities": []}, {"text": "In our work we aim to enhance our understanding of the modelling of selected discourse phenomena in NMT.", "labels": [], "entities": []}, {"text": "To this end, we construct a simple discourse-aware model, demonstrate that it achieves improvements over the discourse-agnostic baseline on an English-Russian subtitles dataset () and study which context information is being captured in the model.", "labels": [], "entities": []}, {"text": "Specifically, we start with the Trans-former (, a state-of-the-art model for context-agnostic NMT, and modify it in such way that it can handle additional context.", "labels": [], "entities": []}, {"text": "In our model, a source sentence and a context sentence are first encoded independently, and then a single attention layer, in a combination with a gating function, is used to produce a context-aware representation of the source sentence.", "labels": [], "entities": []}, {"text": "The information from context can only flow through this attention layer.", "labels": [], "entities": []}, {"text": "When compared to simply concatenating input sentences, as proposed by, our architecture appears both more accurate (+0.6 BLEU) and also guarantees that the contextual information cannot bypass the attention layer and hence remain undetected in our analysis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9982288479804993}]}, {"text": "We analyze what types of contextual information are exploited by the translation model.", "labels": [], "entities": []}, {"text": "While studying the attention weights, we observe that much of the information captured by the model has to do with pronoun translation.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7209922522306442}]}, {"text": "It is not entirely surprising, as we consider translation from a language without grammatical gender (English) to a language with grammatical gender (Russian).", "labels": [], "entities": []}, {"text": "For Russian, translated pronouns need to agree in gender with their antecedents.", "labels": [], "entities": []}, {"text": "Moreover, since in Russian verbs agree with subjects in gender and adjectives also agree in gender with pronouns in certain frequent constructions, mistakes in translating pronouns have a major effect on the words in the produced sentences.", "labels": [], "entities": []}, {"text": "Consequently, the standard cross-entropy training objective sufficiently rewards the model for improving pronoun translation and extracting relevant information from the context.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7505351901054382}]}, {"text": "We use automatic co-reference systems and human annotation to isolate anaphoric cases.", "labels": [], "entities": []}, {"text": "We observe even more substantial improvements in performance on these subsets.", "labels": [], "entities": []}, {"text": "By comparing attention distributions induced by our model against co-reference links, we conclude that the model implicitly captures coreference phenomena, even without having any kind of specialized features which could help it in this subtask.", "labels": [], "entities": []}, {"text": "These observations also suggest potential directions for future work.", "labels": [], "entities": []}, {"text": "For example, effective co-reference systems go beyond relying simply on embeddings of contexts.", "labels": [], "entities": []}, {"text": "One option would be to integrate 'global' features summarizing properties of groups of mentions predicted as linked in a document, or to use latent relations to trace entities across documents (.", "labels": [], "entities": []}, {"text": "Our key contributions can be summarized as follows: \u2022 we introduce a context-aware neural model, which is effective and has a sufficiently simple and interpretable interface between the context and the rest of the translation model; \u2022 we analyze the flow of information from the context and identify pronoun translation as the key phenomenon captured by the model; \u2022 by comparing to automatically predicted or human-annotated coreference relations, we observe that the model implicitly captures anaphora.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 300, "end_pos": 319, "type": "TASK", "confidence": 0.7713097035884857}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Automatic evaluation: BLEU. Signifi- cant differences at p < 0.01 are in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9987221360206604}]}, {"text": " Table 2: Top-10 words with the highest average  attention to context words. attn gives an average  attention to context words, pos gives an average  position of the source word. Left part is for words  on all positions, right -for words on positions  higher than first.", "labels": [], "entities": []}, {"text": " Table 3: BLEU for test sets with coreference between pronoun and a word in context sentence. We show  both N, the total number of instances in a particular test set, and number of instances with pronominal  antecedent. Significant BLEU differences are in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982263445854187}, {"text": "coreference between pronoun and a word in context sentence", "start_pos": 34, "end_pos": 92, "type": "TASK", "confidence": 0.7055261267556084}, {"text": "BLEU", "start_pos": 232, "end_pos": 236, "type": "METRIC", "confidence": 0.9941060543060303}]}, {"text": " Table 4: BLEU for test sets of pronouns having a  nominal antecedent in context sentence. N: num- ber of examples in the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988681077957153}]}, {"text": " Table 5: BLEU for test sets of pronoun \"it\" hav- ing a nominal antecedent in context sentence. N:  number of examples in the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989181756973267}]}, {"text": " Table 6: Agreement with CoreNLP for test sets of  pronouns having a nominal antecedent in context  sentence (%).", "labels": [], "entities": [{"text": "CoreNLP", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.878889262676239}]}, {"text": " Table 7: Agreement with CoreNLP for test sets of  pronouns having a nominal antecedent in context  sentence (%). Examples with \u22651 noun in context  sentence.", "labels": [], "entities": []}]}