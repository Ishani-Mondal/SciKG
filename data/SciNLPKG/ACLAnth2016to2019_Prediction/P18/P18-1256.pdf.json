{"title": [{"text": "Let's do it \"again\": A First Computational Approach to Detecting Adverbial Presupposition Triggers", "labels": [], "entities": [{"text": "Detecting Adverbial Presupposition Triggers", "start_pos": 55, "end_pos": 98, "type": "TASK", "confidence": 0.8377822637557983}]}], "abstractContent": [{"text": "We introduce the task of predicting adverbial presupposition triggers such as also and again.", "labels": [], "entities": [{"text": "predicting adverbial presupposition triggers", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.8793140947818756}]}, {"text": "Solving such a task requires detecting recurring or similar events in the discourse context, and has applications in natural language generation tasks such as summarization and dialogue systems.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.674970010916392}, {"text": "summarization", "start_pos": 159, "end_pos": 172, "type": "TASK", "confidence": 0.9704816937446594}]}, {"text": "We create two new datasets for the task, derived from the Penn Treebank and the Annotated English Gigaword corpora, as well as a novel attention mechanism tailored to this task.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.9958565831184387}, {"text": "Annotated English Gigaword corpora", "start_pos": 80, "end_pos": 114, "type": "DATASET", "confidence": 0.843023344874382}]}, {"text": "Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our mechanism.", "labels": [], "entities": []}, {"text": "We demonstrate that our model statistically outperforms a number of baselines, including an LSTM-based language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "In pragmatics, presuppositions are assumptions or beliefs in the common ground between discourse participants when an utterance is made, and are ubiquitous in naturally occurring discourses).", "labels": [], "entities": []}, {"text": "Presuppositions underly spoken statements and written sentences and understanding them facilitates smooth communication.", "labels": [], "entities": []}, {"text": "We refer to expressions that indicate the presence of presuppositions as presupposition triggers.", "labels": [], "entities": []}, {"text": "These include definite descriptions, factive verbs and certain adverbs, among others.", "labels": [], "entities": []}, {"text": "For example, consider the following statements: (1) John is going to the restaurant again.", "labels": [], "entities": []}, {"text": "* Authors (listed in alphabetical order) contributed equally.", "labels": [], "entities": []}, {"text": "(2) John has been to the restaurant.", "labels": [], "entities": []}, {"text": "(1) is only appropriate in the context where (2) is held to be true because of the presence of the presupposition trigger again.", "labels": [], "entities": []}, {"text": "One distinguishing characteristic of presupposition is that it is unaffected by negation of the presupposing context, unlike other semantic phenomena such as entailment and implicature.", "labels": [], "entities": []}, {"text": "The negation of (1), John is not going to the restaurant again., also presupposes.", "labels": [], "entities": []}, {"text": "Our focus in this paper is on adverbial presupposition triggers such as again, also and still.", "labels": [], "entities": []}, {"text": "Adverbial presupposition triggers indicate the recurrence, continuation, or termination of an event in the discourse context, or the presence of a similar event.", "labels": [], "entities": []}, {"text": "In one study of presuppositional triggers in English journalistic texts, adverbial triggers were found to be the most commonly occurring presupposition triggers after existential triggers.", "labels": [], "entities": []}, {"text": "Despite their frequency, there has been little work on these triggers in the computational literature from a statistical, corpus-driven perspective.", "labels": [], "entities": []}, {"text": "As a first step towards language technology systems capable of understanding and using presuppositions, we propose to investigate the detection of contexts in which these triggers can be used.", "labels": [], "entities": []}, {"text": "This task constitutes an interesting testing ground for pragmatic reasoning, because the cues that are indicative of contexts containing recurring or similar events are complex and often span more than one sentence, as illustrated in Sentences (1) and.", "labels": [], "entities": [{"text": "pragmatic reasoning", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7156491577625275}]}, {"text": "Moreover, such a task has immediate practical consequences.", "labels": [], "entities": []}, {"text": "For example, in language generation applications such as summarization and dialogue systems, adding presuppositional triggers in contextually appropriate loca-tions can improve the readability and coherence of the generated output.", "labels": [], "entities": [{"text": "summarization", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9877848625183105}]}, {"text": "We create two datasets based on the Penn Treebank corpus) and the English Gigaword corpus (, extracting contexts that include presupposition triggers as well as other similar contexts that do not, in order to form a binary classification task.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.9949808120727539}, {"text": "English Gigaword corpus", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.8255656560262045}]}, {"text": "In creating our datasets, we consider a set of five target adverbs: too, again, also, still, and yet.", "labels": [], "entities": []}, {"text": "We focus on these adverbs in our investigation because these triggers are well known in the existing linguistic literature and commonly triggering presuppositions.", "labels": [], "entities": []}, {"text": "We control fora number of potential confounding factors, such as class balance, and the syntactic governor of the triggering adverb, so that models cannot exploit these correlating factors without any actual understanding of the presuppositional properties of the context.", "labels": [], "entities": []}, {"text": "We test a number of standard baseline classifiers on these datasets, including a logistic regression model and deep learning methods based on recurrent neural networks (RNN) and convolutional neural networks (CNN).", "labels": [], "entities": []}, {"text": "In addition, we investigate the potential of attention-based deep learning models for detecting adverbial triggers.", "labels": [], "entities": []}, {"text": "Attention is a promising approach to this task because it allows a model to weigh information from multiple points in the previous context and infer long-range dependencies in the data ().", "labels": [], "entities": []}, {"text": "For example, the model could learn to detect multiple instances involving John and restaurants, which would be a good indication that again is appropriate in that context.", "labels": [], "entities": []}, {"text": "Also, an attention-based RNN has achieved success in predicting article definiteness, which involves another class of presupposition triggers (.", "labels": [], "entities": [{"text": "predicting article definiteness", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.8467204769452413}]}, {"text": "As another contribution, we introduce anew weighted pooling attention mechanism designed for predicting adverbial presupposition triggers.", "labels": [], "entities": [{"text": "predicting adverbial presupposition triggers", "start_pos": 93, "end_pos": 137, "type": "TASK", "confidence": 0.8572791218757629}]}, {"text": "Our attention mechanism allows fora weighted averaging of our RNN hidden states where the weights are informed by the inputs, as opposed to a simple unweighted averaging.", "labels": [], "entities": []}, {"text": "Our model uses a form of self-attention (, where the input sequence acts as both the attention mechanism's query and key/value.", "labels": [], "entities": []}, {"text": "Unlike other attention models, instead of simply averaging the scores to be weighted, our approach aggregates (learned) attention scores by learning a reweighting scheme of those scores through another level (dimension) of attention.", "labels": [], "entities": []}, {"text": "Additionally, our mechanism does not introduce any new parameters when compared to our LSTM baseline, reducing its computational impact.", "labels": [], "entities": []}, {"text": "We compare our model using the novel attention mechanism against the baseline classifiers in terms of prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.926285982131958}]}, {"text": "Our model outperforms these baselines for most of the triggers on the two datasets, achieving 82.42% accuracy on predicting the adverb \"also\" on the Gigaword dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9993796348571777}, {"text": "Gigaword dataset", "start_pos": 149, "end_pos": 165, "type": "DATASET", "confidence": 0.9804416298866272}]}, {"text": "The contributions of this work are as follows: 1.", "labels": [], "entities": []}, {"text": "We introduce the task of predicting adverbial presupposition triggers.", "labels": [], "entities": [{"text": "predicting adverbial presupposition triggers", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.9050300270318985}]}, {"text": "2. We present new datasets for the task of detecting adverbial presupposition triggers, with a data extraction method that can be applied to other similar pre-processing tasks.", "labels": [], "entities": [{"text": "detecting adverbial presupposition triggers", "start_pos": 43, "end_pos": 86, "type": "TASK", "confidence": 0.8021133840084076}]}, {"text": "3. We develop anew attention mechanism in an RNN architecture that is appropriate for the prediction of adverbial presupposition triggers, and show that its use results in better prediction performance over a number of baselines without introducing additional parameters.", "labels": [], "entities": [{"text": "prediction of adverbial presupposition triggers", "start_pos": 90, "end_pos": 137, "type": "TASK", "confidence": 0.8498134255409241}]}], "datasetContent": [{"text": "We compare the performance of our WP model against several models which we describe in this section.", "labels": [], "entities": []}, {"text": "We carryout the experiments on both datasets described in Section 3.", "labels": [], "entities": []}, {"text": "We also investigate the impact of POS tags and attention mechanism on the models' prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9617791771888733}]}], "tableCaptions": [{"text": " Table 1: Number of training samples in each dataset.", "labels": [], "entities": []}, {"text": " Table 2: Performance of various models, including our weighted-pooled LSTM (WP). MFC refers to the  most-frequent-class baseline, LogReg is the logistic regression baseline. LSTM and CNN correspond  to strong neural network baselines. Note that we bold the performance numbers for the best performing  model for each of the \"+ POS\" case and the \"-POS\" case.", "labels": [], "entities": []}, {"text": " Table 3: Confusion matrix for the best performing  model, predicting the presence of a presupposition  trigger or the absence of such as trigger.", "labels": [], "entities": []}, {"text": " Table 4: Contingency table for correct (cor.) and  incorrect (inc.) predictions between the LSTM  baseline and the attention model (WP) on the  Giga_also dataset.", "labels": [], "entities": [{"text": "Giga_also dataset", "start_pos": 145, "end_pos": 162, "type": "DATASET", "confidence": 0.8723523914813995}]}]}