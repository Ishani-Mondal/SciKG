{"title": [{"text": "Building Language Models for Text with Named Entities", "labels": [], "entities": []}], "abstractContent": [{"text": "Text in many domains involves a significant amount of named entities.", "labels": [], "entities": []}, {"text": "Predicting the entity names is often challenging fora language model as they appear less frequent on the training corpus.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel and effective approach to building a discriminative language model which can learn the entity names by leveraging their entity type information.", "labels": [], "entities": []}, {"text": "We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model.", "labels": [], "entities": []}, {"text": "Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than the state-of-the-art language models.", "labels": [], "entities": [{"text": "recipe generation", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.8121575117111206}]}], "introductionContent": [{"text": "Language model is a fundamental component in Natural Language Processing (NLP) and it supports various applications, including document generation (), text autocompletion (, spelling correction), and many others.", "labels": [], "entities": [{"text": "document generation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7697775661945343}, {"text": "spelling correction", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.8023402690887451}]}, {"text": "Recently, language models are also successfully used to generate software source code written in programming languages like Java, C, etc.", "labels": [], "entities": []}, {"text": "These models have improved the language generation tasks to a great extent, e.g.,.", "labels": [], "entities": [{"text": "language generation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7503005862236023}]}, {"text": "However, while generating text or code with a large number of named entities (e.g., different variable names in source code), these models often fail to predict the entity names properly due to their wide variations.", "labels": [], "entities": []}, {"text": "For instance, consider building a language model for generating recipes.", "labels": [], "entities": []}, {"text": "There are numerous similar, yet slightly different cooking ingredients (e.g., olive oil, canola oil, grape oil, etc.-all are different varieties of oil).", "labels": [], "entities": []}, {"text": "Such diverse vocabularies of the ingredient names hinder the language model from predicting them properly.", "labels": [], "entities": []}, {"text": "To address this problem, we propose a novel language model for texts with many entity names.", "labels": [], "entities": []}, {"text": "Our model learns the probability distribution overall the candidate words by leveraging the entity type information.", "labels": [], "entities": []}, {"text": "For example, oil is the type for named entities like olive oil, canola oil, grape oil, etc.", "labels": [], "entities": []}, {"text": "1 Such type information is even more prevalent for source code corpus written in statically typed programming languages, since all the variables are by construct associated with types like integer, float, string, etc.", "labels": [], "entities": []}, {"text": "Our model exploits such deterministic type information of the named entities and learns the probability distribution over the candidate words by decomposing it into two sub-components: (i) Type Model.", "labels": [], "entities": []}, {"text": "Instead of distinguishing the individual names of the same type of entities, we first consider all of them equal and represent them by their type information.", "labels": [], "entities": []}, {"text": "This reduces the vocab size to a great extent and enables to predict the type of each entity more accurately.", "labels": [], "entities": []}, {"text": "(ii) Entity Composite Model.", "labels": [], "entities": []}, {"text": "Using the entity type as a prior, we learn the conditional probability distribution of the actual entity names at inference time.", "labels": [], "entities": []}, {"text": "We depict our model in.", "labels": [], "entities": []}, {"text": "To evaluate our model, we create two benchmark datasets that involve many named entities.", "labels": [], "entities": []}, {"text": "One is a cooking recipe corpus 2 where each recipe contains a number of ingredients which are cate-place proteins in center of a dish with vegetables on each side . place chicken in center of a dish with broccoli on each side .   gorized into 8 super-ingredients (i.e., type); e.g., \"proteins\", \"vegetables\", \"fruits\", \"seasonings\", \"grains\", etc.", "labels": [], "entities": []}, {"text": "Our second dataset comprises a source code corpus of 500 open-source Android projects collected from GitHub.", "labels": [], "entities": []}, {"text": "We use an Abstract Syntax Tree (AST) based approach to collect the type information of the code identifiers.", "labels": [], "entities": []}, {"text": "Our experiments show that although state-ofthe-art language models are, in general, good to learn the frequent words with enough training instances, they perform poorly on the entity names.", "labels": [], "entities": []}, {"text": "A simple addition of type information as an extra feature to a neural network does not guarantee to improve the performance because more features may overfit or need more model parameters on the same data.", "labels": [], "entities": []}, {"text": "In contrast, our proposed method significantly outperforms state-of-the-art neural network based language models and also the models with type information added as an extra feature.", "labels": [], "entities": []}, {"text": "Overall, followings are our contributions: \u2022 We analyze two benchmark language corpora where each consists of a reasonable number of entity names.", "labels": [], "entities": []}, {"text": "While we leverage an existing corpus for recipe, we curated the code corpus.", "labels": [], "entities": []}, {"text": "For both datasets, we created auxiliary corpora with entity type information.", "labels": [], "entities": []}, {"text": "All the code and datasets are released.", "labels": [], "entities": []}, {"text": "3 \u2022 We design a language model for text consisting of many entity names.", "labels": [], "entities": []}, {"text": "The model learns to mention entities names by leveraging the entity type information.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate our model on our benchmark datasets and establish anew baseline performance which significantly outperforms stateof-the-art language models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed model on two different language generation tasks where there exist a lot of entity names in the text.", "labels": [], "entities": [{"text": "language generation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.760644942522049}]}, {"text": "In this paper, we release all the codes and datasets.", "labels": [], "entities": []}, {"text": "The first task is recipe generation.", "labels": [], "entities": [{"text": "recipe generation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.9247086048126221}]}, {"text": "For this task, we analyze a cooking recipe corpus.", "labels": [], "entities": []}, {"text": "Each instance in this corpus is an individual recipe and consists of many ingredi-6 While calculating the final probability distribution overall candidate words, with our joint inference schema, a strong state-of-art language model, without the type information, itself can work sufficiently well and replace the entity composite model.", "labels": [], "entities": []}, {"text": "Our experiments using () in Section 4.1 validate this claim.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparing the performance of recipe gen-", "labels": [], "entities": []}, {"text": " Table 2: Comparing the performance of code genera-", "labels": [], "entities": []}, {"text": " Table 3: Performance of fill in the blank task.", "labels": [], "entities": []}]}