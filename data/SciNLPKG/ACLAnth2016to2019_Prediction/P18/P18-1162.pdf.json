{"title": [{"text": "Question Condensing Networks for Answer Selection in Community Question Answering", "labels": [], "entities": [{"text": "Question Condensing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7819335460662842}, {"text": "Answer Selection in Community Question Answering", "start_pos": 33, "end_pos": 81, "type": "TASK", "confidence": 0.6942484229803085}]}], "abstractContent": [{"text": "Answer selection is an important subtask of community question answering (CQA).", "labels": [], "entities": [{"text": "Answer selection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9609202444553375}, {"text": "community question answering (CQA)", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.7768913954496384}]}, {"text": "Ina real-world CQA forum, a question is often represented as two parts: a subject that summarizes the main points of the question, and a body that elaborates on the subject in detail.", "labels": [], "entities": []}, {"text": "Previous researches on answer selection usually ignored the difference between these two parts and concatenated them as the question representation.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.929471343755722}]}, {"text": "In this paper, we propose the Question Condensing Networks (QCN) to make use of the subject-body relationship of community questions.", "labels": [], "entities": []}, {"text": "In this model, the question subject is the primary part of the question representation, and the question body information is aggregated based on similarity and disparity with the question subject.", "labels": [], "entities": []}, {"text": "Experimental results show that QCN outperforms all existing models on two CQA datasets.", "labels": [], "entities": [{"text": "CQA datasets", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9748102128505707}]}], "introductionContent": [{"text": "Community question answering (CQA) has seen a spectacular increase in popularity in recent years.", "labels": [], "entities": [{"text": "Community question answering (CQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.787981371084849}]}, {"text": "With the advent of sites like Stack Overflow 1 and Quora 2 , more and more people can freely ask any question and expect a variety of answers.", "labels": [], "entities": []}, {"text": "With the influx of new questions and the varied quality of provided answers, it is very time-consuming fora user to inspect them all.", "labels": [], "entities": []}, {"text": "Therefore, developing automated tools to identify good answers fora question is of practical importance.", "labels": [], "entities": []}, {"text": "A typical example for CQA is shown in.", "labels": [], "entities": []}, {"text": "In this example, Answer 1 is a good answer, because it provides helpful information, e.g., \"check it to the traffic dept\".", "labels": [], "entities": []}, {"text": "Although Answer 2 is relevant to the question, it does not contain any useful information so that it should be regarded as a bad answer.", "labels": [], "entities": []}, {"text": "From this example, we can observe two characteristics of CQA that ordinary QA does not possess.", "labels": [], "entities": []}, {"text": "First, a question includes both a subject that gives a brief summary of the question and a body that describes the question in detail.", "labels": [], "entities": []}, {"text": "The questioners usually convey their main concern and key information in the question subject.", "labels": [], "entities": []}, {"text": "Then, they provide more extensive details about the subject, seek help, or express gratitude in the question body.", "labels": [], "entities": []}, {"text": "Second, the problem of redundancy and noise is prevalent in CQA (.", "labels": [], "entities": [{"text": "CQA", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.8962421417236328}]}, {"text": "Both questions and answers contain auxiliary sentences that do not provide meaningful information.", "labels": [], "entities": []}, {"text": "Previous researches () usually treat each word equally in the question and answer representation.", "labels": [], "entities": []}, {"text": "However, due to the redundancy and noise problem, only part of text from questions and answers is useful to determine the answer quality.", "labels": [], "entities": []}, {"text": "To make things worse, they ignored the difference between question subject and body, and simply concatenated them as the question representation.", "labels": [], "entities": []}, {"text": "Due to the subject-body relationship described above, this simple concatenation can aggravate the redundancy problem in the question.", "labels": [], "entities": []}, {"text": "In this paper, we propose the Question Condensing Networks (QCN) to address these problems.", "labels": [], "entities": []}, {"text": "In order to utilize the subject-body relationship in community questions, we propose to treat the question subject as the primary part of the question, and aggregate the question body information based on similarity and disparity with the question subject.", "labels": [], "entities": []}, {"text": "The similarity part corresponds to the information that exists in both question subject and body, and the disparity part corresponds to the additional information provided by the ques-Question Subject Checking the history of the car.", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9847638010978699}]}, {"text": "Question body How can one check the history of the car like maintenance, accident or service history.", "labels": [], "entities": []}, {"text": "In every advertisement of the car, people used to write \"Accident Free\", but inmost cases, car have at least one or two accident, which is not easily detectable through Car Inspection Company.", "labels": [], "entities": [{"text": "Accident Free\"", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.9049479762713114}]}, {"text": "Share your opinion in this regard.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two community question answering datasets from SemEval () to evaluate our model.", "labels": [], "entities": []}, {"text": "The statistics of these datasets are listed in: Statistics of two CQA datasets.", "labels": [], "entities": [{"text": "CQA datasets", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9885007441043854}]}, {"text": "We can see from the statistics that the question body is much lengthier than the question subject.", "labels": [], "entities": []}, {"text": "Thus, it is necessary to condense the question representation.", "labels": [], "entities": []}, {"text": "consists of questions and a list of answers for each question, and each question consists of a short title and a more detailed description.", "labels": [], "entities": []}, {"text": "There are also some metadata associated with them, e.g., user ID, date of posting, and the question category.", "labels": [], "entities": []}, {"text": "We do not use the metadata because they failed to boost performance in our model.", "labels": [], "entities": []}, {"text": "Since the SemEval 2017 dataset is an updated version of SemEval 2016 6 , and shares the same evaluation metrics with SemEval 2016, we choose to use the SemEval 2017 dataset for evaluation.", "labels": [], "entities": [{"text": "SemEval 2017 dataset", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.6444498598575592}, {"text": "SemEval 2016 6", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.7425006230672201}, {"text": "SemEval 2017 dataset", "start_pos": 152, "end_pos": 172, "type": "DATASET", "confidence": 0.7518279353777567}]}, {"text": "In order to facilitate comparison, we adopt the evaluation metrics used in the official task or prior work.", "labels": [], "entities": []}, {"text": "For the SemEval 2015 dataset, the official scores are macro-averaged F1 and accuracy over three categories.", "labels": [], "entities": [{"text": "SemEval 2015 dataset", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.7171211640040079}, {"text": "F1", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9460582733154297}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.999671459197998}]}, {"text": "However, many recent researches () switched to a binary classification setting, i.e., identifying Good vs. Bad answers.", "labels": [], "entities": []}, {"text": "Because binary classification is much closer to a realworld CQA application.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.7431646883487701}]}, {"text": "Besides, the PotentiallyUseful class is both the smallest and the noisiest class, making it the hardest to predict.", "labels": [], "entities": []}, {"text": "To make it worse, its impact is magnified by the macroaveraged F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9412604570388794}]}, {"text": "Therefore, we adopt the F1 score and accuracy on two categories for evaluation.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.989469438791275}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9994423985481262}]}, {"text": "SemEval 2017 regards answer selection as a ranking task, which is closer to the application scenario.", "labels": [], "entities": [{"text": "SemEval 2017", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8541476726531982}, {"text": "answer selection", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.9539553821086884}]}, {"text": "As a result, mean average precision (MAP) is used as an evaluation measure.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 13, "end_pos": 41, "type": "METRIC", "confidence": 0.9364861845970154}]}, {"text": "For a perfect ranking, a system has to place all Good answers above the PotentiallyUseful and Bad answers.", "labels": [], "entities": []}, {"text": "The latter two are not actually distinguished and are considered Bad in terms of evaluation.", "labels": [], "entities": []}, {"text": "Addition- The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but it does not include a development set.", "labels": [], "entities": [{"text": "SemEval 2017 dataset", "start_pos": 14, "end_pos": 34, "type": "DATASET", "confidence": 0.7262657781442007}]}, {"text": "Following previous work, we use the 2016 official test set as the development set.", "labels": [], "entities": [{"text": "2016 official test set", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.7302279472351074}]}, {"text": "ally, standard classification measures like accuracy and F1 score are also reported.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.999808132648468}, {"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9815787076950073}]}, {"text": "In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks.", "labels": [], "entities": [{"text": "SemEval shared tasks", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.7651121218999227}]}, {"text": "\u2022 HITSZ-ICRC (Hou et al., 2015): It proposed ensemble learning and hierarchical classification method to classify answers.", "labels": [], "entities": [{"text": "HITSZ-ICRC (Hou et al., 2015)", "start_pos": 2, "end_pos": 31, "type": "DATASET", "confidence": 0.8739763498306274}]}], "tableCaptions": [{"text": " Table 2: Statistics of two CQA datasets. We can see from the statistics that the question body is much  lengthier than the question subject. Thus, it is necessary to condense the question representation.", "labels": [], "entities": [{"text": "CQA datasets", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9552948176860809}]}, {"text": " Table 3: Comparisons on the SemEval 2015  dataset.", "labels": [], "entities": [{"text": "SemEval 2015  dataset", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.7614050904909769}]}, {"text": " Table 4: Comparisons on the SemEval 2017  dataset.", "labels": [], "entities": [{"text": "SemEval 2017  dataset", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.7799484928448995}]}, {"text": " Table 5: Ablation studies on the SemEval 2017  dataset.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.955078661441803}, {"text": "SemEval 2017  dataset", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.7763462464014689}]}]}