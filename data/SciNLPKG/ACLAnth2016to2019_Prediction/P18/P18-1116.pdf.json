{"title": [], "abstractContent": [{"text": "Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors.", "labels": [], "entities": [{"text": "Tree-based neural machine translation (NMT)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.764346318585532}]}, {"text": "For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.8476922114690145}]}, {"text": "This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model).", "labels": [], "entities": []}, {"text": "The BLEU score of the proposed method is higher than that of the string-to-string NMT, tree-based NMT, and forest-based SMT systems .", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9756520688533783}, {"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9304731488227844}]}], "introductionContent": [{"text": "NMT has witnessed promising improvements recently.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7819087505340576}]}, {"text": "Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems ( ; tree-to-string systems (; and string-totree systems ().", "labels": [], "entities": []}, {"text": "Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features.", "labels": [], "entities": []}, {"text": "They can use more syntactic information ( , and can conveniently incorporate prior knowledge ( . * Contribution during internship at National Institute of Information and Communications Technology.", "labels": [], "entities": []}, {"text": "\u2020 Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays.", "labels": [], "entities": []}, {"text": "Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (, representing trees by linearization ().", "labels": [], "entities": []}, {"text": "Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation.", "labels": [], "entities": []}, {"text": "Therefore we focus on this kind of methods in this paper.", "labels": [], "entities": []}, {"text": "In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors).", "labels": [], "entities": []}, {"text": "For SMT, forest-based methods have employed a packed forest to address this problem, which represents exponentially many parse trees rather than just the 1-best one ( . But for NMT, (computationally efficient) forestbased methods are still being explored . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9940343499183655}]}, {"text": "This hinders the development of forest-based NMT to some extent.", "labels": [], "entities": [{"text": "NMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8334459662437439}]}, {"text": "Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT approach (Section 3), which can en-code the syntactic information of a packed forest on the basis of a novel weighted linearization method fora packed forest (Section 3.1), and can decode the linearized packed forest under the simple sequence-to-sequence framework.", "labels": [], "entities": []}, {"text": "Experiments demonstrate the effectiveness of our method (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "We can see that for both English-Chinese and English-Japanese, compared with the s2s baseline system, both the 1-best and forest-based configurations yield better results.", "labels": [], "entities": []}, {"text": "This indicates syntactic information contained in the constituent trees or forests is indeed useful for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7492523193359375}]}, {"text": "Specifically, we observe the following facts.", "labels": [], "entities": []}, {"text": "First, among the three different frameworks SoE, SoA, and No-score, the SoA framework performs the best, while the No-score framework per- In the Czech Republic , which was ravaged by serious floods last summer , the temperatures in its border region adjacent to neighboring Slovakia plunged to minus 18 degrees Celsius . [  forms the worst.", "labels": [], "entities": []}, {"text": "This indicates that the scores of the edges in constituent trees or packed forests, which reflect the confidence of the correctness of the edges, are indeed useful.", "labels": [], "entities": []}, {"text": "In fact, for the 1-best constituent parsing tree, the score of the edge reflects the confidence of the parser.", "labels": [], "entities": []}, {"text": "By using this information, the NMT system succeed to learn a better attention, paying much attention to the confident structure and not paying attention to the unconfident structure, which improved the translation performance.", "labels": [], "entities": []}, {"text": "This fact is ignored by previous studies on tree-based NMT.", "labels": [], "entities": []}, {"text": "Furthermore, it is better to use the scores to modify the values of attention instead of rescaling the word embeddings, because modifying word embeddings carelessly may change the semantic meanings of words.", "labels": [], "entities": []}, {"text": "Second, compared with the cases that only using the 1-best constituent trees, using packed forests yields statistical significantly better results for the SoE and SoA frameworks.", "labels": [], "entities": []}, {"text": "This shows the effectiveness of using more syntactic information.", "labels": [], "entities": []}, {"text": "Compared with one constituent tree, the packed forest, which contains multiple different trees, describes the syntactic structure of the sentence in different aspects, which together increase the accuracy of machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9984159469604492}, {"text": "machine translation", "start_pos": 208, "end_pos": 227, "type": "TASK", "confidence": 0.706137016415596}]}, {"text": "However, without using the scores, the 1-best constituent tree is preferred.", "labels": [], "entities": []}, {"text": "This is because without using the scores, all trees in the packed forest are treated equally, which makes it easy to import noise into the encoder.", "labels": [], "entities": []}, {"text": "Compared with other types of state-of-the-art systems, our systems using only the 1-best tree (1-best(SoE, SoA)) are better than the other treebased systems.", "labels": [], "entities": []}, {"text": "Moreover, our NMT systems using the packed forests achieve the best performance.", "labels": [], "entities": []}, {"text": "These results also support the usefulness of the scores of the edges and packed forests in NMT.", "labels": [], "entities": []}, {"text": "As for the efficiency, the training time of the SoA system was slightly longer than that of the SoE system, which was about twice of the s2s baseline.", "labels": [], "entities": []}, {"text": "The training time of the tree-based system was about 1.5 times of the baseline.", "labels": [], "entities": []}, {"text": "For the case of Forest (SoA), with 1 core of Tesla P100 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed was about 10 sentences per second.", "labels": [], "entities": []}, {"text": "The reason for the relatively low efficiency is that the linearized sequences of packed forests were much longer than word sequences, enlarging the scale of the inputs.", "labels": [], "entities": []}, {"text": "Despite this, the training process ended within reasonable time.", "labels": [], "entities": []}, {"text": "illustrates the translation results of an English sentence using several different configurations: the s2s baseline, using only the 1-best tree (SoE), and using the packed forest (SoE).", "labels": [], "entities": []}, {"text": "This is a sentence from NIST MT 03, and the training corpus is the LDC corpus.", "labels": [], "entities": [{"text": "NIST MT 03", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.8004099527994791}, {"text": "LDC corpus", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.8570594489574432}]}], "tableCaptions": [{"text": " Table 1: Statistics of the corpora.", "labels": [], "entities": []}, {"text": " Table 2: English-Chinese experimental results (character-level BLEU). \"FS,\" \"TN,\" and \"FN\" denote  forest-based SMT, tree-based NMT, and forest-based NMT systems, respectively. We performed the  paired bootstrap resampling significance test", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9638742208480835}, {"text": "FS", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9876810312271118}, {"text": "FN", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.8961256742477417}]}, {"text": " Table 3: English-Japanese experimental results (character-level BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.957567036151886}]}]}