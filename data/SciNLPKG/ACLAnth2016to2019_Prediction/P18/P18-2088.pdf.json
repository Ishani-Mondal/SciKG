{"title": [{"text": "A Rank-Based Similarity Metric for Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Word Embeddings (WE) have recently imposed themselves as a standard for representing word meaning in NLP.", "labels": [], "entities": []}, {"text": "Semantic similarity between word pairs has become the most common evaluation benchmark for these representations, with vector cosine being typically used as the only similarity metric.", "labels": [], "entities": []}, {"text": "In this paper, we report experiments with a rank-based metric for WE, which performs comparably to vector cosine in similarity estimation and out-performs it in the recently-introduced and challenging task of outlier detection, thus suggesting that rank-based measures can improve clustering quality.", "labels": [], "entities": [{"text": "similarity estimation", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.6682705134153366}, {"text": "outlier detection", "start_pos": 209, "end_pos": 226, "type": "TASK", "confidence": 0.7676889896392822}]}], "introductionContent": [{"text": "\"All happy families resemble one another, but each unhappy family is unhappy in its own way.\"", "labels": [], "entities": []}, {"text": "Anna Karenina, Leo Tolstoy Distributional Semantic Models (DSMs) have received an increasing attention in the NLP community, as they constitute an efficient data-driven method for creating word representations and measuring their semantic similarity by computing their distance in the vector space.", "labels": [], "entities": [{"text": "Leo Tolstoy Distributional Semantic Models (DSMs)", "start_pos": 15, "end_pos": 64, "type": "TASK", "confidence": 0.7060998752713203}]}, {"text": "The most popular similarity metric in DSMs is the vector cosine.", "labels": [], "entities": []}, {"text": "Compared to Euclidean distances, vector cosine scores are normalized on each dimension and hence are robust to the scaling effect.", "labels": [], "entities": []}, {"text": "On the other hand, one limitation of this metric is that it regards each dimension equally, without taking into account the fact that some dimensions might be more relevant for characteriz-ing the semantic content of a word.", "labels": [], "entities": [{"text": "characteriz-ing the semantic content of a word", "start_pos": 177, "end_pos": 223, "type": "TASK", "confidence": 0.7530090894017901}]}, {"text": "Such a limitation led to the introduction of alternative metrics based on feature ranking, which have been reported to outperform vector cosine in several similarity tasks (.", "labels": [], "entities": []}, {"text": "Recently, the focus of the research on word representations has been shifting onto the so-called word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (.", "labels": [], "entities": []}, {"text": "Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ ().", "labels": [], "entities": []}, {"text": "Most research works involving WE still adopt vector cosine for similarity estimation, yet little experimentation has been done on alternative metrics for comparing dense representations (exceptions include).", "labels": [], "entities": [{"text": "WE", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8734458684921265}, {"text": "similarity estimation", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.6844307482242584}]}, {"text": "Some attempts to directly transfer rank-based measures from traditional DSMs to WE have faced difficulties (see, for example,).", "labels": [], "entities": []}, {"text": "In this paper, we suggest a possible solution to this problem by adapting AP Syn, a rank-based similarity metric originally proposed for sparse vectors, to low-dimensional word embeddings.", "labels": [], "entities": []}, {"text": "This goal is achieved by removing the parameter N (the extent of the feature overlap to betaken into account) and adding a smoothing parameter that is proven to be constant under multiple settings, therefore making the measure unsupervised.", "labels": [], "entities": []}, {"text": "Our experiments show performance improvements both in similarity estimation and in the more challenging outlier detection task, which consists in cluster and outlier identification.", "labels": [], "entities": [{"text": "similarity estimation", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7259722054004669}, {"text": "outlier detection", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7387103140354156}, {"text": "cluster and outlier identification", "start_pos": 146, "end_pos": 180, "type": "TASK", "confidence": 0.6653071939945221}]}], "datasetContent": [{"text": "As for the similarity estimation task, we evaluate the Spearman correlation between systemgenerated scores and human judgments.", "labels": [], "entities": [{"text": "similarity estimation task", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.7717316746711731}]}, {"text": "We used three popular benchmark datasets: WordSim-353 (), MEN () and SimLex-999 (.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9661842584609985}]}, {"text": "It is important to point out that SimLex-999 is the only one specifically built for targeting genuine semantic similarity, while the others tend to mix similarity and relatedness scores.", "labels": [], "entities": []}, {"text": "As for outlier detection, we evaluate our DSMs on the 8-8-8 dataset).", "labels": [], "entities": [{"text": "outlier detection", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.6894921958446503}]}, {"text": "The dataset consists of eight clusters, each one with a different topic and consisting in turn of eight lexical items belonging to the cluster and eight outliers (with four degrees of relatedness to the cluster members: C1, C2, C3, C4).", "labels": [], "entities": []}, {"text": "In total, the dataset includes 64 sets of 8 words + 1 outlier for the evaluation.", "labels": [], "entities": []}, {"text": "For each word w of a cluster W of n words, the authors defined a compactness score c(w) corresponding to the average of all pairwise similarities of the words in W \\ {w}.", "labels": [], "entities": []}, {"text": "On the basis of the compactness score, they proposed two evaluation metrics: Outlier Position (OP) and Outlier Detection (OD).", "labels": [], "entities": [{"text": "Outlier Detection (OD)", "start_pos": 103, "end_pos": 125, "type": "METRIC", "confidence": 0.6956989467144012}]}, {"text": "Given a set W of n + 1 words, OP is the rank of the outlier w n + 1 according to the compactness score.", "labels": [], "entities": [{"text": "OP", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9946867227554321}]}, {"text": "Ideally, the rank of the outlier should be n, mean-'s pairwise method, while PT-Cos refers to the prototype-based one.", "labels": [], "entities": [{"text": "PT-Cos", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.8937947750091553}]}, {"text": "In bold, best scores per method; in bold and underlined, best scores per corpus-embedding combination.", "labels": [], "entities": []}, {"text": "ing that it has the lowest average similarity with the other cluster elements.", "labels": [], "entities": []}, {"text": "The second metric, Outlier Detection (OD), is indeed defined as 1 iff OP (w n + 1) = n, 0 otherwise.", "labels": [], "entities": [{"text": "Outlier Detection (OD)", "start_pos": 19, "end_pos": 41, "type": "METRIC", "confidence": 0.8265085041522979}]}, {"text": "Finally, the performance on a dataset composed of |D| sets of words was estimated in terms of Outlier Position Percentage (OP P , Eq. 3) and Accuracy (Eq. 4):", "labels": [], "entities": [{"text": "Outlier Position Percentage (OP P", "start_pos": 94, "end_pos": 127, "type": "METRIC", "confidence": 0.9137307703495026}, {"text": "Accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9994077682495117}]}], "tableCaptions": [{"text": " Table 1: Similarity Estimation, Spearman Correlation by Setting. Embeddings trained on Wikipedia.", "labels": [], "entities": [{"text": "Similarity Estimation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9518839716911316}]}, {"text": " Table 2: Outlier Detection, Performance by Setting. CC-Cos refers to Camacho-Collados and Navigli", "labels": [], "entities": [{"text": "Outlier Detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7624348104000092}, {"text": "Navigli", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.5474809408187866}]}]}