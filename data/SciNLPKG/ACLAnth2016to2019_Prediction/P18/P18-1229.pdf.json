{"title": [{"text": "End-to-End Reinforcement Learning for Automatic Taxonomy Induction", "labels": [], "entities": [{"text": "Automatic Taxonomy Induction", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6223760445912679}]}], "abstractContent": [{"text": "We present a novel end-to-end reinforcement learning approach to automatic tax-onomy induction from a set of terms.", "labels": [], "entities": [{"text": "tax-onomy induction", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.6360720545053482}]}, {"text": "While prior methods treat the problem as a two-phase task (i.e., detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy.", "labels": [], "entities": []}, {"text": "In our approach , the representations of term pairs are learned using multiple sources of information and used to determine which term to select and whereto place it on the taxonomy via a policy network.", "labels": [], "entities": []}, {"text": "All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies.", "labels": [], "entities": []}, {"text": "Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many tasks in natural language understanding (e.g., information extraction, question answering (, and textual entailment) rely on lexical resources in the form of term taxonomies (cf. rightmost column in).", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.6604490478833517}, {"text": "information extraction", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.8267536461353302}, {"text": "question answering", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.8779173791408539}, {"text": "textual entailment", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7173800617456436}]}, {"text": "However, most existing taxonomies, such as WordNet and Cyc, are manually curated and thus may have limited coverage or become unavailable in some domains and languages.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9599520564079285}]}, {"text": "Therefore, recent efforts have been focusing on automatic taxonomy induction, which aims to organize a set of terms into a taxonomy based on relevant resources such as text corpora.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8008321523666382}]}, {"text": "Prior studies on automatic taxonomy induction ( often divide the problem into two sequential subtasks: (1) hypernymy detection (i.e., extracting term pairs of \"is-a\" relation); and (2) hypernymy organization (i.e., organizing is-a term pairs into a tree-structured hierarchy).", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8111684918403625}, {"text": "hypernymy detection", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.6842518895864487}]}, {"text": "Methods developed for hypernymy detection either harvest new terms ( or presume a vocabulary is given and study term semantics ().", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8881983458995819}]}, {"text": "The hypernymy pairs extracted in the first subtask form a noisy hypernym graph, which is then transformed into a tree-structured taxonomy in the hypernymy organization subtask, using different graph pruning methods including maximum spanning tree (MST) (, minimum-cost flow (MCF) ( and other pruning heuristics (.", "labels": [], "entities": []}, {"text": "However, these two-phase methods encounter two major limitations.", "labels": [], "entities": []}, {"text": "First, most of them ignore the taxonomy structure when estimating the probability that a term pair holds the hypernymy relation.", "labels": [], "entities": []}, {"text": "They estimate the probability of different term pairs independently and the learned term pair representations are fixed during hypernymy organization.", "labels": [], "entities": []}, {"text": "In consequence, there is no feedback from the second phase to the first phase and possibly wrong representations cannot be rectified based on the results of hypernymy organization, which causes the error propagation problem.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 198, "end_pos": 215, "type": "TASK", "confidence": 0.6907254159450531}]}, {"text": "Secondly, some methods () do explore the taxonomy space by regarding the induction of taxonomy structure as inferring the conditional distribution of edges.", "labels": [], "entities": []}, {"text": "In other words, they use the product of edge proba- Figure 1: An illustrative example showing the process of taxonomy induction.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.8923346102237701}]}, {"text": "The input vocabulary V 0 is {\"working dog\", \"pinscher\", \"shepherd dog\", ...}, and the initial taxonomy T 0 is empty.", "labels": [], "entities": []}, {"text": "We use a virtual \"root\" node to represent T 0 at t = 0.", "labels": [], "entities": []}, {"text": "At time t = 5, there are 5 terms on the taxonomy T 5 and 3 terms left to be attached: Vt = {\"shepherd dog\", \"collie\", \"affenpinscher\"}.", "labels": [], "entities": [{"text": "Vt", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9634089469909668}]}, {"text": "Suppose the term \"affenpinscher\" is selected and put under \"pinscher\", then the remaining vocabulary V t+1 at next time step becomes {\"shepherd dog\", \"collie\"}.", "labels": [], "entities": []}, {"text": "Finally, after |V 0 | time steps, all the terms are attached to the taxonomy and V |V 0 | = V 8 = {}.", "labels": [], "entities": []}, {"text": "A full taxonomy is then constructed from scratch.", "labels": [], "entities": []}, {"text": "bilities to represent the taxonomy quality.", "labels": [], "entities": []}, {"text": "However, the edges are treated equally, while in reality, they contribute to the taxonomy differently.", "labels": [], "entities": []}, {"text": "For example, a high-level edge is likely to be more important than a bottom-out edge because it has much more influence on its descendants.", "labels": [], "entities": []}, {"text": "In addition, these methods cannot explicitly capture the holistic taxonomy structure by optimizing global metrics.", "labels": [], "entities": []}, {"text": "To address the above issues, we propose to jointly conduct hypernymy detection and organization by learning term pair representations and constructing the taxonomy simultaneously.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8348224759101868}]}, {"text": "Since it is infeasible to estimate the quality of all possible taxonomies, we design an end-to-end reinforcement learning (RL) model to combine the two phases.", "labels": [], "entities": []}, {"text": "Specifically, we train an RL agent that employs the term pair representations using multiple sources of information and determines which term to select and whereto place it on the taxonomy via a policy network.", "labels": [], "entities": []}, {"text": "The feedback from hypernymy organization is propagated back to the hypernymy detection phase, based on which the term pair representations are adjusted.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.6591393202543259}]}, {"text": "All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies.", "labels": [], "entities": []}, {"text": "The probability of a full taxonomy is no longer a simple aggregated probability of its edges.", "labels": [], "entities": []}, {"text": "Instead, we assess an edge based on how much it can contribute to the whole quality of the taxonomy.", "labels": [], "entities": []}, {"text": "We perform two sets of experiments to evaluate the effectiveness of our proposed approach.", "labels": [], "entities": []}, {"text": "First, we test the end-to-end taxonomy induction performance by comparing our approach with the state-of-the-art two-phase methods, and show that our approach outperforms them significantly on the quality of constructed taxonomies.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7828162014484406}]}, {"text": "Second, we use the same (noisy) hypernym graph as the input of all compared methods, and demonstrate that our RL approach does better hypernymy organization through optimizing metrics that can capture holistic taxonomy structure.", "labels": [], "entities": []}, {"text": "In summary, we have made the following contributions: (1) We propose a deep reinforcement learning approach to unify hypernymy detection and organization so as to induct taxonomies in an end-to-end manner.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7072540074586868}]}, {"text": "(2) We design a policy network to incorporate semantic information of term pairs and use cumulative rewards to measure the quality of constructed taxonomies holistically.", "labels": [], "entities": []}, {"text": "(3) Experiments on two public datasets from different domains demonstrate the superior performance of our approach compared with state-of-the-art methods.", "labels": [], "entities": []}, {"text": "We also show that our method can effectively reduce error propagation and capture global taxonomy structure.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.6923942416906357}]}], "datasetContent": [{"text": "We design two experiments to demonstrate the effectiveness of our proposed RL approach for taxonomy induction.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.918848067522049}]}, {"text": "First, we compare our end-toend approach with two-phase methods and show that our approach yields taxonomies with higher quality through reducing error propagation and optimizing towards holistic metrics.", "labels": [], "entities": []}, {"text": "Second, we conduct a controlled experiment on hypernymy organization, where the same hypernym graph is used as the input of both our approach and the compared methods.", "labels": [], "entities": [{"text": "hypernymy organization", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8022997379302979}]}, {"text": "We show that our RL method is more effective at hypernymy organization.", "labels": [], "entities": [{"text": "RL", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9165785908699036}]}, {"text": "Here we introduce the details of our two experiments on validating that (1) the proposed approach can effectively reduce error propagation; and (2) our approach yields better taxonomies via optimizing metrics on holistic taxonomy structure.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.6989844143390656}]}, {"text": "Performance Study on End-to-End Taxonomy Induction.", "labels": [], "entities": [{"text": "End-to-End Taxonomy Induction", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.5624802708625793}]}, {"text": "In the first experiment, we show that our joint learning approach is superior to twophase methods.", "labels": [], "entities": []}, {"text": "Towards this goal, we compare with TAXI (), atypical two-phase approach, two-phase HypeNET, implemented by pairwise hypernymy detection and hypernymy organization using MST, and.", "labels": [], "entities": [{"text": "TAXI", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9516709446907043}, {"text": "pairwise hypernymy detection", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.6513792673746744}]}, {"text": "The dataset we use in this experiment is from, which is a set of medium-sized full-domain taxonomies consisting of bottom-out full subtrees sampled from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.9689604043960571}]}, {"text": "Terms in different taxonomies are from various domains such as animals, general concepts, daily necessities.", "labels": [], "entities": []}, {"text": "Each taxonomy is of height four (i.e., 4 nodes from root to leaf) and contains nodes.", "labels": [], "entities": []}, {"text": "The dataset contains 761 nonoverlapped taxonomies in total and is partitioned by 70/15/15% (533/114/114) as training, validation, and test set, respectively.", "labels": [], "entities": []}, {"text": "In the second experiment, we show that our approach is better at hypernymy organization by leveraging the global taxonomy structure.", "labels": [], "entities": []}, {"text": "For a fair comparison, we reuse the hypernym graph as in TAXI ( and so that the inputs of each model are the same.", "labels": [], "entities": [{"text": "TAXI", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.815865159034729}]}, {"text": "Specifically, we restrict the action space to be the same as the baselines by considering only term pairs in the hypernym graph, rather than all |V | \u00d7 |T | possible term pairs.", "labels": [], "entities": []}, {"text": "As a result, it is possible that at some point no more hypernym candidates can be found but the remaining vocabulary is still not empty.", "labels": [], "entities": []}, {"text": "If the induction terminates at this point, we call it a partial induction.", "labels": [], "entities": []}, {"text": "We can also continue the induction by restoring the original action space at this moment so that all the terms in V are eventually attached to the taxonomy.", "labels": [], "entities": []}, {"text": "We call this setting a full induction.", "labels": [], "entities": []}, {"text": "In this experiment, we use the English environment and science taxonomies in the SemEval-2016 task 13 (TExEval-2) (.", "labels": [], "entities": []}, {"text": "Each taxonomy is composed of hundreds of terms, which is much larger than the WordNet taxonomies.", "labels": [], "entities": [{"text": "WordNet taxonomies", "start_pos": 78, "end_pos": 96, "type": "DATASET", "confidence": 0.9646999537944794}]}, {"text": "The taxonomies are aggregated from existing resources such as WordNet, Eurovoc 6 , and the Wikipedia Bitaxonomy (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9802520871162415}, {"text": "Eurovoc 6", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.9481691718101501}, {"text": "Wikipedia Bitaxonomy", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.8845147490501404}]}, {"text": "Since this dataset provides no training data, we train our model using the WordNet dataset in the first experiment.", "labels": [], "entities": [{"text": "WordNet dataset", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9874384999275208}]}, {"text": "To avoid possible overlap between these two sources, we exclude those taxonomies constructed from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9597580432891846}]}, {"text": "In both experiments, we combine three public corpora -the latest Wikipedia dump, the UMBC web-based corpus () and the One Billion Word Language Modeling Benchmark (: Results of the end-to-end taxonomy induction experiment.", "labels": [], "entities": [{"text": "UMBC web-based corpus", "start_pos": 85, "end_pos": 106, "type": "DATASET", "confidence": 0.9213063915570577}]}, {"text": "Our approach significantly outperforms two-phase methods (). and TaxoRL (NR) + FG are listed separately because they use extra resources.", "labels": [], "entities": [{"text": "FG", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9201776385307312}]}, {"text": "a corpus with size 2.6 GB for the WordNet dataset and 810 MB for the TExEval-2 dataset.", "labels": [], "entities": [{"text": "WordNet dataset", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9889020919799805}, {"text": "TExEval-2 dataset", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9513774216175079}]}, {"text": "Dependency paths between term pairs are extracted from the corpus via spaCy 7 .  Ancestor-F1.", "labels": [], "entities": []}, {"text": "It compares the ancestors (\"is-a\" pairs) on the predicted taxonomy with those on the gold taxonomy.", "labels": [], "entities": []}, {"text": "We use Pa , Ra , F 1 a to denote the precision, recall, and F1-score, respectively: Edge-F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9997316002845764}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9983709454536438}, {"text": "F1-score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9997202754020691}]}, {"text": "It is more strict than Ancestor-F1 since it only compares predicted edges with gold edges.", "labels": [], "entities": []}, {"text": "Similarly, we denote edge-based metrics as P e , Re , and F 1 e , respectively.", "labels": [], "entities": [{"text": "F 1 e", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9602203766504923}]}, {"text": "Note that P e = Re = F 1 e if the number of predicted edges is the same as gold edges.", "labels": [], "entities": [{"text": "F 1 e", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9142767389615377}]}], "tableCaptions": [{"text": " Table 1: Results of the end-to-end taxonomy in- duction experiment. Our approach significantly  outperforms two-phase methods (", "labels": [], "entities": []}, {"text": " Table 3: Ablation study on the WordNet  dataset (", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9683548212051392}, {"text": "WordNet  dataset", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.9848325848579407}]}]}