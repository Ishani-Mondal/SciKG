{"title": [{"text": "Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation", "labels": [], "entities": [{"text": "Objective", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.8421884775161743}, {"text": "Domain Adaptation in Neural Machine Translation", "start_pos": 58, "end_pos": 105, "type": "TASK", "confidence": 0.6426664888858795}]}], "abstractContent": [{"text": "Supervised domain adaptation-where a large generic corpus and a smaller in-domain corpus are both available for training-is a challenge for neural machine translation (NMT).", "labels": [], "entities": [{"text": "Supervised domain adaptation-where", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.686031699180603}, {"text": "neural machine translation (NMT)", "start_pos": 140, "end_pos": 172, "type": "TASK", "confidence": 0.8248352607091268}]}, {"text": "Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model.", "labels": [], "entities": []}, {"text": "We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the in-domain model's output word distribution and that of the out-of-domain model to prevent the model's output from differing too much from the original out-of-domain model.", "labels": [], "entities": []}, {"text": "We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model.", "labels": [], "entities": [{"text": "TED", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.7401649951934814}]}, {"text": "Our method shows improvements over standard continued training by up to 1.5 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9991700649261475}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) () is currently the state-of-the art paradigm for machine translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.788754920164744}, {"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7931409478187561}]}, {"text": "It dominated the recent WMT shared task (, and is used commercially (.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.849609355131785}]}, {"text": "Despite their successes, NMT systems require a large amount of training data and do not perform well in low resource and domain adaptation scenarios (.", "labels": [], "entities": []}, {"text": "Domain adaptation is required when there is sufficient data to train an NMT system in the desired language pair, but the domain (the topic, genre, style or level of formality) of this large corpus differs from that of the data that the system will need to translate attest time.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7555027902126312}]}, {"text": "In this paper, we focus on the supervised domain adaptation problem, wherein addition to a large out-of-domain corpus, we also have a smaller in-domain parallel corpus available for training.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6486570835113525}]}, {"text": "A technique commonly applied in this situation is continued training (, where a model is first trained on the out-of-domain corpus, and then that model is used to initialize anew model that is trained on the in-domain corpus.", "labels": [], "entities": []}, {"text": "This simple method leads to empirical improvements on in-domain test sets.", "labels": [], "entities": []}, {"text": "However, we hypothesize that some knowledge available in the out-of-domain data-which is not observed in the smaller in-domain data but would be useful attest time-is being forgotten during continued training, due to overfitting.", "labels": [], "entities": []}, {"text": "(This phenomena can be viewed as aversion of catastrophic forgetting ().", "labels": [], "entities": []}, {"text": "For this reason, we add an additional term to the loss function of the NMT training objective during continued training.", "labels": [], "entities": []}, {"text": "In addition to minimizing the cross entropy between the model's output word distribution and the reference translation, the additional term in the loss function minimizes the cross entropy between the model's output word distribution and that of the out-of-domain model.", "labels": [], "entities": []}, {"text": "This prevents the distribution of words produced from differing too much from the original distribution.", "labels": [], "entities": []}, {"text": "We show that this method improves upon standard continued training by as much as 1.5 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9995749592781067}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Tokenized training set sizes.", "labels": [], "entities": []}, {"text": " Table 2: Tokenized development set sizes.", "labels": [], "entities": []}, {"text": " Table 3: Tokenized test set sizes.", "labels": [], "entities": []}, {"text": " Table 4: BLEU score improvements over continued training. We compare to the out-of-domain baseline  and the in-domain baseline. We also compare to continued training without the additional regularization  term.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998778760433197}]}, {"text": " Table 5: BLEU score improvements over continued training using the 2, 000 sentence subsets as the in- domain corpus. We compare to the out-of-domain baseline and continued training without the additional  regularization term.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984161853790283}]}, {"text": " Table 6: Analysis of BLEU score improvements without continued training. We compare to the out-of- domain baseline and the in-domain baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.970796674489975}]}, {"text": " Table 7: Analysis of the sensitivity of BLEU scores on the domain-specific sets and newstest2016", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9943990111351013}, {"text": "newstest2016", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.8603198528289795}]}]}