{"title": [{"text": "Probabilistic FastText for Multi-Sense Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce Probabilistic FastText, anew model for word embeddings that can capture multiple word senses, sub-word structure , and uncertainty information.", "labels": [], "entities": []}, {"text": "In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams.", "labels": [], "entities": []}, {"text": "This representation allows the model to share statistical strength across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words.", "labels": [], "entities": []}, {"text": "Moreover, each component of the mixture can capture a different word sense.", "labels": [], "entities": []}, {"text": "Probabilistic FastText out-performs both FASTTEXT, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.7612713575363159}, {"text": "English RareWord and foreign language datasets", "start_pos": 226, "end_pos": 272, "type": "DATASET", "confidence": 0.6535638074080149}]}, {"text": "We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings.", "labels": [], "entities": []}, {"text": "Thus, the proposed model is the first to achieve multi-sense representations while having enriched semantics on rare words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings are foundational to natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6649011671543121}]}, {"text": "In order to model language, we need word representations to contain as much semantic information as possible.", "labels": [], "entities": []}, {"text": "Most research has focused on vector word embeddings, such as WORD2VEC (, where words with similar meanings are mapped to nearby points in a vector space.", "labels": [], "entities": []}, {"text": "Following the * Work done partly during internship at Amazon.", "labels": [], "entities": []}, {"text": "seminal work of, there have been numerous works looking to learn efficient word embeddings.", "labels": [], "entities": []}, {"text": "One shortcoming with the above approaches to word embedding that are based on a predefined dictionary (termed as dictionary-based embeddings) is their inability to learn representations of rare words.", "labels": [], "entities": []}, {"text": "To overcome this limitation, character-level word embeddings have been proposed.", "labels": [], "entities": []}, {"text": "FASTTEXT () is the state-of-the-art character-level approach to embeddings.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.49938979744911194}]}, {"text": "In FASTTEXT, each word is modeled by a sum of vectors, with each vector representing an n-gram.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.5562905073165894}]}, {"text": "The benefit of this approach is that the training process can then share strength across words composed of common roots.", "labels": [], "entities": []}, {"text": "For example, with individual representations for \"circum\" and \"navigation\", we can construct an informative representation for \"circumnavigation\", which would otherwise appear too infrequently to learn a dictionary-level embedding.", "labels": [], "entities": []}, {"text": "In addition to effectively modelling rare words, character-level embeddings can also represent slang or misspelled words, such as \"dogz\", and can share strength across different languages that share roots, e.g. Romance languages share latent roots.", "labels": [], "entities": []}, {"text": "A different promising direction involves representing words with probability distributions, instead of point vectors.", "labels": [], "entities": []}, {"text": "For example, represents words with Gaussian distributions, which can capture uncertainty information.", "labels": [], "entities": []}, {"text": "generalizes this approach to multimodal probability distributions, which can naturally represent words with different meanings.", "labels": [], "entities": []}, {"text": "For example, the distribution for \"rock\" could have mass near the word \"jazz\" and \"pop\", but also \"stone\" and \"basalt\".", "labels": [], "entities": []}, {"text": "further developed this approach to learn hierarchical word representations: for example, the word \"music\" can be learned to have abroad distribution, which encapsulates the distributions for \"jazz\" and \"rock\".", "labels": [], "entities": []}, {"text": "In this paper, we propose Probabilistic FastText (PFT), which provides probabilistic characterlevel representations of words.", "labels": [], "entities": []}, {"text": "The resulting word embeddings are highly expressive, yet straightforward and interpretable, with simple, efficient, and intuitive training procedures.", "labels": [], "entities": []}, {"text": "PFT can model rare words, uncertainty information, hierarchical representations, and multiple word senses.", "labels": [], "entities": []}, {"text": "In particular, we represent each word with a Gaussian or a Gaussian mixture density, which we name PFT-G and PFT-GM respectively.", "labels": [], "entities": []}, {"text": "Each component of the mixture can represent different word senses, and the mean vectors of each component decompose into vectors of n-grams, to capture character-level information.", "labels": [], "entities": []}, {"text": "We also derive an efficient energybased max-margin training procedure for PFT.", "labels": [], "entities": []}, {"text": "We perform comparison with FASTTEXT as well as existing density word embeddings W2G (Gaussian) and W2GM (Gaussian mixture).", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.5556458830833435}]}, {"text": "Our models extract high-quality semantics based on multiple word-similarity benchmarks, including the rare word dataset.", "labels": [], "entities": []}, {"text": "We obtain an average weighted improvement of 3.7% over FASTTEXT () and 3.1% over the dictionary-level density-based models.", "labels": [], "entities": []}, {"text": "We also observe meaningful nearest neighbors, particularly in the multimodal density case, where each mode captures a distinct meaning.", "labels": [], "entities": []}, {"text": "Our models are also directly portable to foreign languages without any hyperparameter modification, where we observe strong performance, outperforming FAST-TEXT on many foreign word similarity datasets.", "labels": [], "entities": [{"text": "FAST-TEXT", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9025442600250244}]}, {"text": "Our multimodal word representation can also disentangle meanings, and is able to separate different senses in foreign polysemies.", "labels": [], "entities": []}, {"text": "In particular, our models attain state-of-the-art performance on SCWS, a benchmark to measure the ability to separate different word meanings, achieving 1.0% improvement over a recent density embedding model W2GM.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to develop multi-sense embeddings with high semantic quality for rare words.", "labels": [], "entities": []}, {"text": "Our code and embeddings are publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have proposed a probabilistic FASTTEXT model which combines the flexibility of subword structure with the density embedding approach.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.6260326504707336}]}, {"text": "In this section, we show that our probabilistic representation with subword mean vectors with the simplified energy function outperforms many word similarity baselines and provides disentangled meanings for polysemies.", "labels": [], "entities": []}, {"text": "First, we describe the training details in Section 4.1.", "labels": [], "entities": []}, {"text": "We provide qualitative evaluation in Section 4.2, showing meaningful nearest neighbors for the Gaussian embeddings, as well as the ability to capture multiple meanings by Gaussian mixtures.", "labels": [], "entities": []}, {"text": "Our quantitative evaluation in Section 4.3 demonstrates strong performance against the baseline models FASTTEXT ( and the dictionary-level Gaussian (W2G)) and Gaussian mixture embeddings (Athiwaratkun and Wilson, 2017) (W2GM).", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.6549504399299622}, {"text": "W2GM)", "start_pos": 220, "end_pos": 225, "type": "DATASET", "confidence": 0.9050063788890839}]}, {"text": "We train our models on foreign language corpuses and show competitive results on foreign word similarity benchmarks in Section 4.4.", "labels": [], "entities": []}, {"text": "Finally, we explain the importance of the n-gram structures for semantic sharing in Section 4.5.", "labels": [], "entities": [{"text": "semantic sharing", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.8809130787849426}]}, {"text": "We show that our embeddings learn the word semantics well by demonstrating meaningful nearest neighbors.", "labels": [], "entities": []}, {"text": "shows examples of polysemous words such as rock, star, and cell.", "labels": [], "entities": []}, {"text": "shows the nearest neighbors of polysemous words.", "labels": [], "entities": []}, {"text": "We note that subword embeddings prefer words with overlapping characters as nearest neighbors.", "labels": [], "entities": []}, {"text": "For instance, \"rock-y\", \"rockn\", and \"rock\" are both close to the word \"rock\".", "labels": [], "entities": []}, {"text": "For the purpose of demonstration, we only show words with meaningful variations and omit words with small character-based variations previously mentioned.", "labels": [], "entities": []}, {"text": "However, all words shown are in the top-100 nearest words.", "labels": [], "entities": []}, {"text": "We observe the separation in meanings for the multi-component case; for instance, one component of the word \"bank\" corresponds to a financial bank whereas the other component corresponds to a riverbank.", "labels": [], "entities": []}, {"text": "The single-component case also has interesting behavior.", "labels": [], "entities": []}, {"text": "We observe that the subword embeddings of polysemous words can represent both meanings.", "labels": [], "entities": []}, {"text": "For instance, both \"lava-rock\" and \"rock-pop\" are among the closest words to \"rock\".", "labels": [], "entities": []}, {"text": "We evaluate our embeddings on several standard word similarity datasets, namely, SL-999 (), WS-353 (), MEN-3k (), MC-30, RG-65, YP-130 (), MTurk(-287,-771), and RW-2k (.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8455832600593567}, {"text": "MTurk", "start_pos": 139, "end_pos": 144, "type": "DATASET", "confidence": 0.7470150589942932}, {"text": "RW-2k", "start_pos": 161, "end_pos": 166, "type": "DATASET", "confidence": 0.8107182383537292}]}, {"text": "Each dataset contains a list of word pairs with a human score of how related or similar the two words are.", "labels": [], "entities": []}, {"text": "We use the notation DATASET-NUM to denote the number of word pairs NUM in each evaluation set.", "labels": [], "entities": [{"text": "DATASET-NUM", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.7491264343261719}]}, {"text": "We note that the dataset RW focuses more on infrequent words and SimLex-999 focuses on the similarity of words rather than relatedness.", "labels": [], "entities": []}, {"text": "We also compare PFT-GM with other multi-prototype embeddings in the literature using SCWS (), a word similarity dataset that is aimed to measure the ability of embeddings to discern multiple meanings.", "labels": [], "entities": []}, {"text": "We calculate the Spearman correlation) between the labels and our scores gen-   erated by the embeddings.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 17, "end_pos": 37, "type": "METRIC", "confidence": 0.6683629751205444}]}, {"text": "The Spearman correlation is a rank-based correlation measure that assesses how well the scores describe the true labels.", "labels": [], "entities": []}, {"text": "The scores we use are cosine-similarity scores between the mean vectors.", "labels": [], "entities": []}, {"text": "In the case of Gaussian mixtures, we use the pairwise maximum score: The pair (i, j) that achieves the maximum cosine similarity corresponds to the Gaussian component pair that is the closest in meanings.", "labels": [], "entities": []}, {"text": "Therefore, this similarity score yields the most related senses of a given word pair.", "labels": [], "entities": []}, {"text": "This score reduces to a cosine similarity in the Gaussian case (K = 1).", "labels": [], "entities": []}, {"text": "We evaluate the foreign-language embeddings on word similarity datasets in respective languages.", "labels": [], "entities": []}, {"text": "We use Italian WORDSIM353 and Italian SIMLEX-999 () for Italian models, GUR350 and GUR65) for German models, and French WORD-SIM353 () for French models.", "labels": [], "entities": [{"text": "SIMLEX-999", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.8235151171684265}, {"text": "GUR350", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.5782729983329773}, {"text": "GUR65", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.4523094594478607}]}, {"text": "For datasets GUR350 and GUR65, we use the results reported in the FASTTEXT publication (  public code 5 on our text corpuses.", "labels": [], "entities": [{"text": "GUR350", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9406963586807251}, {"text": "GUR65", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.9491788148880005}, {"text": "FASTTEXT publication", "start_pos": 66, "end_pos": 86, "type": "DATASET", "confidence": 0.8685925304889679}]}, {"text": "We also train dictionary-level models W2G, and W2GM for comparison.", "labels": [], "entities": []}, {"text": "shows the Spearman's correlation results of our models.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.5281272828578949}]}, {"text": "We outperform FASTTEXT on many word similarity benchmarks.", "labels": [], "entities": [{"text": "FASTTEXT", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.49942120909690857}]}, {"text": "Our results are also significantly better than the dictionary-based models, W2G and W2GM.", "labels": [], "entities": [{"text": "W2G", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.9057563543319702}, {"text": "W2GM", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8738958835601807}]}, {"text": "We hypothesize that W2G and W2GM can perform better than the current reported results given proper pre-processing of words due to special characters such as accents.", "labels": [], "entities": [{"text": "W2G", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.8635982871055603}, {"text": "W2GM", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.8706878423690796}]}, {"text": "We investigate the nearest neighbors of polysemies in foreign languages and also observe clear sense separation.", "labels": [], "entities": []}, {"text": "For example, piano in Italian can mean \"floor\" or \"slow\".", "labels": [], "entities": []}, {"text": "These two meanings are reflected in the nearest neighbors where one component is close to piano-piano, pianod which mean \"slowly\" whereas the other component is close to piani (floors), istrutturazione (renovation) or infrastruttre (infrastructure).", "labels": [], "entities": []}, {"text": "shows additional results, demonstrating that the disentangled semantics can be observed in multiple languages.", "labels": [], "entities": []}, {"text": "One of the motivations for using subword information is the ability to handle out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "Another benefit is the ability to help improve the semantics of rare words via subword sharing.", "labels": [], "entities": [{"text": "subword sharing", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.7301900386810303}]}, {"text": "Due to an observation that text corpuses follow Zipf's power law, words at the tail of the occurrence distribution appears much   less frequently.", "labels": [], "entities": []}, {"text": "Training these words to have a good semantic representation is challenging if done at the word level alone.", "labels": [], "entities": []}, {"text": "However, an ngram such as 'abnorm' is trained during both occurrences of \"abnormal\" and \"abnormality\" in the corpus, hence further augments both words's semantics.", "labels": [], "entities": []}, {"text": "shows the contribution of n-grams to the final representation.", "labels": [], "entities": []}, {"text": "We filter out to show only the n-grams with the top-5 and bottom-5 similarity scores.", "labels": [], "entities": [{"text": "similarity", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9475112557411194}]}, {"text": "We observe that the final representations of both words align with n-grams \"abno\", \"bnor\", \"abnorm\", \"anbnor\", \"<abn\".", "labels": [], "entities": []}, {"text": "In fact, both \"abnormal\" and \"abnormality\" share the same top-5 n-grams.", "labels": [], "entities": []}, {"text": "Due to the fact that many rare words such as \"autobiographer\", \"circumnavigations\", or \"hypersensitivity\" are composed from many common sub-words, the n-gram structure can help improve the representation quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Nearest neighbors of PFT-GM (top) and PFT-G (bottom). The notation w:i denotes the i th  mixture component of the word w.", "labels": [], "entities": [{"text": "PFT-G", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8770758509635925}]}, {"text": " Table 2: Spearman's Correlation \u03c1 \u00d7 100 on Word Similarity Datasets.", "labels": [], "entities": [{"text": "Word Similarity Datasets", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.6293507814407349}]}, {"text": " Table 3: Spearman's Correlation \u03c1 \u00d7 100 on word  similarity dataset SCWS.", "labels": [], "entities": [{"text": "word  similarity dataset SCWS", "start_pos": 44, "end_pos": 73, "type": "DATASET", "confidence": 0.7004077062010765}]}, {"text": " Table 4: Word similarity evaluation on foreign languages.", "labels": [], "entities": [{"text": "Word similarity evaluation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7627985179424286}]}]}