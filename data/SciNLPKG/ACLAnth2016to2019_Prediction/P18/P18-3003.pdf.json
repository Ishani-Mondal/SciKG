{"title": [{"text": "Learning-based Composite Metrics for Improved Caption Evaluation", "labels": [], "entities": [{"text": "Improved Caption Evaluation", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.8624567190806071}]}], "abstractContent": [{"text": "The evaluation of image caption quality is a challenging task, which requires the assessment of two main aspects in a caption: adequacy and fluency.", "labels": [], "entities": [{"text": "image caption quality", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.6888245145479838}]}, {"text": "These quality aspects can be judged using a combination of several linguistic features.", "labels": [], "entities": []}, {"text": "However, most of the current image captioning metrics focus only on specific linguistic facets, such as the lexical or semantic, and fail to meet a satisfactory level of correlation with human judgements at the sentence-level.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7388695031404495}]}, {"text": "We propose a learning-based framework to incorporate the scores of a set of lexical and semantic metrics as features, to capture the adequacy and fluency of captions at different linguistic levels.", "labels": [], "entities": []}, {"text": "Our experimental results demonstrate that composite metrics draw upon the strengths of stand-alone measures to yield improved correlation and accuracy.", "labels": [], "entities": [{"text": "correlation", "start_pos": 126, "end_pos": 137, "type": "METRIC", "confidence": 0.9849390983581543}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9959348440170288}]}], "introductionContent": [{"text": "Automatic image captioning requires the understanding of the visual aspects of images to generate human-like descriptions (.", "labels": [], "entities": [{"text": "Automatic image captioning", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6661092539628347}]}, {"text": "The evaluation of the generated captions is crucial for the development and fine-grained analysis of image captioning systems ( . Automatic evaluation metrics aim at providing efficient, cost-effective and objective assessments of the caption quality.", "labels": [], "entities": []}, {"text": "Since these automatic measures serve as an alternative to the manual evaluation, the major concern is that such measures should correlate well with human assessments.", "labels": [], "entities": []}, {"text": "In other words, automatic metrics are expected to mimic the human judgement process by taking into account various aspects that humans consider when they assess the captions.", "labels": [], "entities": []}, {"text": "The evaluation of image captions can be characterized as having two major aspects: adequacy and fluency.", "labels": [], "entities": [{"text": "evaluation of image captions", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6322424337267876}]}, {"text": "Adequacy is how well the caption reflects the source image, and fluency is how well the caption conforms to the norms and conventions of human language.", "labels": [], "entities": []}, {"text": "In the case of manual evaluation, both adequacy and fluency tend to shape the human perception of the overall caption quality.", "labels": [], "entities": []}, {"text": "Most of the automatic evaluation metrics tend to capture these aspects of quality based on the idea that \"the closer the candidate description to the professional human caption, the better it is in quality\" ().", "labels": [], "entities": []}, {"text": "The output in such case is a score (the higher the better) reflecting the similarity.", "labels": [], "entities": []}, {"text": "The majority of the commonly used metrics for image captioning such as BLEU () and METEOR () are based on the lexical similarity.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.715673953294754}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9986338019371033}, {"text": "METEOR", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.977264404296875}]}, {"text": "Lexical measures (n-gram based) work by rewarding the ngram overlaps between the candidate and the reference captions.", "labels": [], "entities": []}, {"text": "Thus, measuring the adequacy by counting the n-gram matches and assessing the fluency by implicitly using the reference n-grams as a language model (.", "labels": [], "entities": []}, {"text": "However, a high number of n-gram matches cannot always be indicative of a high caption quality, nor a low number of n-gram matches can always be reflective of a low caption quality (.", "labels": [], "entities": []}, {"text": "A recently proposed semantic metric SPICE (, overcomes this deficiency of lexical measures by measuring the semantic similarity of candidate and reference captions using Scene Graphs.", "labels": [], "entities": []}, {"text": "However, the major drawback of SPICE is that it ignores the fluency of the output caption.", "labels": [], "entities": [{"text": "SPICE", "start_pos": 31, "end_pos": 36, "type": "TASK", "confidence": 0.9299929738044739}]}, {"text": "Integrating assessment scores of different measures is an intuitive and reasonable way to improve the current image captioning evaluation methods.", "labels": [], "entities": [{"text": "image captioning evaluation", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.8011618951956431}]}, {"text": "Through this methodology, each metric plays the role of a judge, assessing the quality of captions in terms of lexical, grammatical or semantic accuracy.", "labels": [], "entities": []}, {"text": "For this research, we use the scores conferred by a set of measures that are commonly used for captioning and combine them through a learningbased framework.", "labels": [], "entities": []}, {"text": "In this work: 1.", "labels": [], "entities": []}, {"text": "We evaluate various combinations of a chosen set of metrics and show that the proposed composite metrics correlate better with human judgements.", "labels": [], "entities": []}, {"text": "2. We analyse the accuracy of composite metrics in terms of differentiating between pairs of captions in reference to the ground truth captions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9984630346298218}]}], "datasetContent": [{"text": "To train our composite metrics, we source data from.", "labels": [], "entities": []}, {"text": "For each image in Flicker30k, we randomly select three of the human generated captions as positive training examples, and three machine generated (one from each image captioning model) captions as negative training examples.", "labels": [], "entities": [{"text": "Flicker30k", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.9499829411506653}]}, {"text": "We combined the Microsoft COCO ( training and validation set (containing 123,287 images in total, each paired with 5 or more captions), to train the image captioning models using their official codes.", "labels": [], "entities": [{"text": "Microsoft COCO", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.6988276988267899}]}, {"text": "These image captioning models achieved state-of-the-art performance when they were published.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7292155474424362}]}, {"text": "In order to obtain reference captions for each training example, we again use the human written descriptions of Flicker30k.", "labels": [], "entities": [{"text": "Flicker30k", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.9553883671760559}]}, {"text": "For each negative training example (machine-generated caption), we randomly choose 4 out of 5 human written captions originally associated with each image.", "labels": [], "entities": []}, {"text": "Whereas, for each positive training example (human-generated caption), we use the 5 human written captions associated with each image, selecting one of these as a human candidate caption (positive example) and the remaining 4 as references.", "labels": [], "entities": []}, {"text": "In, a possible pairing scenario is shown for further clarification.", "labels": [], "entities": []}, {"text": "For our validation set, we source data from Flicker8k ().", "labels": [], "entities": [{"text": "Flicker8k", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.8771018981933594}]}, {"text": "This dataset contains 5,822 captions assessed by three expert judges on a scale of 1 (the caption is unrelated to the image) to 4 (the caption describes the im- age without any errors).", "labels": [], "entities": []}, {"text": "From our training set, we remove the captions of images which overlap with the captions in the validation and test sets (discussed in Sec.", "labels": [], "entities": []}, {"text": "5), leaving us with a total of 132,984 non-overlapping captions for the training of the composite metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Kendall's correlation co-efficient of au- tomatic evaluation metrics and proposed compos- ite metrics against human quality judgements. All  correlations are significant at p<0.001", "labels": [], "entities": []}, {"text": " Table 2: Comparative accuracy results (in percent- age) on four kinds of pairs tested on PASCAL-50s", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9401335716247559}, {"text": "PASCAL-50s", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.7785057425498962}]}]}