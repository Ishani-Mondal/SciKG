{"title": [{"text": "Incorporating Glosses into Neural Word Sense Disambiguation", "labels": [], "entities": [{"text": "Neural Word Sense Disambiguation", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.6195837631821632}]}], "abstractContent": [{"text": "Word Sense Disambiguation (WSD) aims to identify the correct meaning of poly-semous words in the particular context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7769330888986588}]}, {"text": "Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9448772668838501}, {"text": "WSD", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.7680330872535706}]}, {"text": "However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions).", "labels": [], "entities": [{"text": "WSD", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.971853494644165}]}, {"text": "In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge.", "labels": [], "entities": []}, {"text": "Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word.", "labels": [], "entities": []}, {"text": "GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods.", "labels": [], "entities": []}, {"text": "We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9700697064399719}]}, {"text": "The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets.", "labels": [], "entities": [{"text": "WSD datasets", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.6937825828790665}]}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7929964611927668}]}, {"text": "There are several lines of research on WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9785950779914856}]}, {"text": "Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context.", "labels": [], "entities": []}, {"text": "Supervised methods usually train multiple classifiers with manual designed features.", "labels": [], "entities": []}, {"text": "Although supervised methods can achieve the state-of-the-art performance (, there are still two major challenges.", "labels": [], "entities": []}, {"text": "Firstly, supervised methods () usually train a dedicated classifier for each word individually (often called word expert).", "labels": [], "entities": []}, {"text": "So it cannot easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts . Recent neural-based methods) solve this problem by building a unified model for all the polysemous words, but they still can't beat the best word expert system.", "labels": [], "entities": [{"text": "WSD", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9367033243179321}]}, {"text": "Secondly, all the neural-based methods always only consider the local context of the target word, ignoring the lexical resources like WordNet which are widely used in the knowledge-based methods.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9608879089355469}]}, {"text": "The gloss, which extensionally defines a word sense meaning, plays a key role in the well-known Lesk algorithm.", "labels": [], "entities": []}, {"text": "Recent studies () have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of Lesk algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9991247057914734}]}, {"text": "To this end, our goal is to incorporate the gloss information into a unified neural network for all of the polysemous words.", "labels": [], "entities": []}, {"text": "We further consider extending the original gloss through its semantic relations in our framework.", "labels": [], "entities": []}, {"text": "As shown in, the glosses of hypernyms and hyponyms can enrich the original gloss information as well as help to build better a sense representation.", "labels": [], "entities": []}, {"text": "Therefore, we integrate not only the original gloss but also the related glosses of hypernyms and hyponyms into the neural network.", "labels": [], "entities": []}, {"text": "Figure 1: The hypernym (green node) and hyponyms (blue nodes) for the 2nd sense bed 2 of bed, which means a plot of ground in which plants are growing, rather than the bed for sleeping in.", "labels": [], "entities": []}, {"text": "The figure shows that bed 2 is a kind of plot 2 , and bed 2 includes flowerbed 1 , seedbed 1 , etc.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel model GAS: a gloss-augmented WSD neural network which is a variant of the memory network (.", "labels": [], "entities": [{"text": "GAS", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9407882690429688}]}, {"text": "GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module.", "labels": [], "entities": []}, {"text": "In order to measure the inner relationship between glosses and context more accurately, we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are listed as follows: \u2022 To the best of our knowledge, our model is the first to incorporate the glosses into an end-to-end neural WSD model.", "labels": [], "entities": []}, {"text": "In this way, our model can benefit from not only massive labeled data but also rich lexical knowledge.", "labels": [], "entities": []}, {"text": "\u2022 In order to model semantic relationship of context and glosses, we propose a glossaugmented neural network (GAS) in an improved memory network paradigm.", "labels": [], "entities": []}, {"text": "\u2022 We further expand the gloss through its semantic relations to enrich the gloss information and better infer the context.", "labels": [], "entities": []}, {"text": "We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.9391831159591675}]}, {"text": "\u2022 The experimental results on several English all-words WSD benchmark datasets show that our model outperforms the state-of-theart systems.", "labels": [], "entities": [{"text": "WSD benchmark datasets", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.7180588245391846}]}], "datasetContent": [{"text": "Evaluation Dataset: we evaluate our model on several English all-words WSD datasets.", "labels": [], "entities": [{"text": "Evaluation Dataset", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.6663489788770676}, {"text": "WSD datasets", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.7014847099781036}]}, {"text": "For fair comparison, we use the benchmark datasets proposed by which includes five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions.", "labels": [], "entities": [{"text": "WSD datasets", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.7805987298488617}]}, {"text": "They are Senseval-2 (SE2), Senseval-3 task 1 (SE3), SemEval-07 task 17 (SE7), SemEval-13 task 12 (SE13), and SemEval-15 task 13 (SE15).", "labels": [], "entities": []}, {"text": "Following by , we choose SE7, the smallest test set as the development (validation) set, which consists of 455 labeled instances.", "labels": [], "entities": []}, {"text": "The last four test sets consist of 6798 labeled instances with four types of target words, namely nouns, verbs, adverbs and adjectives.", "labels": [], "entities": []}, {"text": "We extract word sense glosses from WordNet3.0 because maps all the sense annotations 8 from its original version to 3.0.", "labels": [], "entities": [{"text": "WordNet3.0", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.957996666431427}]}, {"text": "Training Dataset: We choose SemCor 3.0 as the training set, which was also used by ,,, , etc.", "labels": [], "entities": [{"text": "Training Dataset", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9239427149295807}]}, {"text": "It consists of 226,036 sense annotations from 352 documents, which is the largest manually annotated corpus for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.516035795211792}]}, {"text": "Note that all the systems listed in are trained on SemCor 3.0.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: An example of attention weights in the memory module within 5 passes. Darker colors mean that  the attention weight is higher. Case studies show that the proposed multi-pass operation can recognize  the correct sense by enlarging the attention gap between correct senses and incorrect ones.", "labels": [], "entities": []}, {"text": " Table 3: F1-score (%) of different passes from 1  to 5 on the test data sets. It shows that appropri- ate number of passes can boost the performance as  well as avoid over-fitting of the model.  .  fourth block, we can find that our best model out- performs the previous best neural network models  (", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994370341300964}]}, {"text": " Table 2. Consider the situ- ation that we meet an unknown word x 11 , we look", "labels": [], "entities": []}]}