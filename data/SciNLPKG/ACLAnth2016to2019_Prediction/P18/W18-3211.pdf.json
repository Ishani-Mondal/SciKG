{"title": [{"text": "Language Informed Modeling of Code-Switched Text", "labels": [], "entities": [{"text": "Language Informed Modeling of Code-Switched Text", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7380275577306747}]}], "abstractContent": [{"text": "Code-switching (CS), the practice of alternating between two or more languages in conversations, is pervasive inmost multilingual communities.", "labels": [], "entities": [{"text": "Code-switching (CS)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6409673690795898}]}, {"text": "CS texts have a complex interplay between languages and occur in informal contexts that make them harder to collect and construct NLP tools for.", "labels": [], "entities": []}, {"text": "We approach this problem through Language Modeling (LM) on anew Hindi-English mixed corpus containing 59,189 unique sentences collected from blogging websites.", "labels": [], "entities": [{"text": "Language Modeling (LM", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7627899050712585}]}, {"text": "We implement and discuss different Language Models derived from a multi-layered LSTM architecture.", "labels": [], "entities": []}, {"text": "We hypothesize that encoding language information strengthens a language model by helping to learn code-switching points.", "labels": [], "entities": []}, {"text": "We show that our highest performing model achieves a test perplexity of 19.52 on the CS corpus that we collected and processed.", "labels": [], "entities": [{"text": "CS corpus", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.8580872416496277}]}, {"text": "On this data we demonstrate that our performance is an improvement over AWD-LSTM LM (a recent state of the art on monolingual English).", "labels": [], "entities": [{"text": "AWD-LSTM", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.6813119649887085}]}], "introductionContent": [{"text": "Code-switching (CS) is a widely studied linguistic phenomenon where two different languages are interleaved.", "labels": [], "entities": [{"text": "Code-switching (CS)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6095763966441154}]}, {"text": "This occurs within multilingual communities.", "labels": [], "entities": []}, {"text": "Typically one language (the matrix language) provides the grammatical structure for CS text and words from another language (the embedded language) are inserted.", "labels": [], "entities": []}, {"text": "However, CS data is challenging to obtain because this phenomenon is usually observed in informal settings.", "labels": [], "entities": []}, {"text": "Data obtained from online sources is often noisy because of spelling, script, morphological, and grammatical variations.", "labels": [], "entities": []}, {"text": "* These authors contributed equally These sources of noise make it quite challenging to build robust NLP tools (C \u00b8 etino\u02d8.", "labels": [], "entities": []}, {"text": "Our goal is to improve LM for HindiEnglish code-mixed data (Hinglish) where similar challenges are apparent.", "labels": [], "entities": [{"text": "LM", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9837774038314819}, {"text": "HindiEnglish code-mixed data (Hinglish)", "start_pos": 30, "end_pos": 69, "type": "DATASET", "confidence": 0.8833606938521067}]}, {"text": "The task of language modeling is very important to several downstream applications in NLP including speech recognition, machine translation, etc.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7293370068073273}, {"text": "speech recognition", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8116647899150848}, {"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8170158863067627}]}, {"text": "This is particularly important in domains that lack annotated data, such as code-switching, where the need to leverage unsupervised techniques is a must.", "labels": [], "entities": []}, {"text": "We address the task of language modeling in CS text with a dual objective: (1) predicting the next word, and (2) predicting the language of the next word.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7261018455028534}, {"text": "predicting the next word", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.8402714431285858}, {"text": "predicting the language of the next word", "start_pos": 113, "end_pos": 153, "type": "TASK", "confidence": 0.8371328711509705}]}, {"text": "In addition to the techniques used for monolingual language modeling, providing information about the language is a key component in CS domain.", "labels": [], "entities": [{"text": "monolingual language modeling", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.6780790487925211}]}, {"text": "Our main goal in this paper is to examine the effect of language information in modeling CS text.", "labels": [], "entities": []}, {"text": "We approach this systematically by experimenting with ablations of encoding and decoding language IDs along with the word itself.", "labels": [], "entities": []}, {"text": "In this way, the model implicitly learns the switch points between the languages.", "labels": [], "entities": []}, {"text": "We achieve the least perplexity score using a combination of a language informed encoder and a language informed decoder.", "labels": [], "entities": []}, {"text": "The current material begins with a review of LM techniques for CS text in section 2.", "labels": [], "entities": [{"text": "LM", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9603445529937744}]}, {"text": "Then we describe our data collection and processing steps in Section 3 and model architecture in Section 4.", "labels": [], "entities": [{"text": "data collection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7542987167835236}]}, {"text": "Section 5 contains a brief quantitative and qualitative discussion of our observations and promising directions for future work.", "labels": [], "entities": []}, {"text": "We then conclude in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "There area number of ways to frame the desire for humans to switch between languages), however, we view the human desire as out of scope for this work.", "labels": [], "entities": []}, {"text": "Instead, our focus is on how we can incorporate linguistic information while training a statistical model for code-switched text.", "labels": [], "entities": []}, {"text": "We discuss two main choices as to where we can introduce this information: either at the encoding stage or at the decoding stage of an RNN language model.", "labels": [], "entities": []}, {"text": "Given a CS sentence X cs = (x 1 , x 2 ... , x n ) which has lexical level language sequence L cs = (l 1 , l 2 ...", "labels": [], "entities": []}, {"text": ", l n ), our model has to predict the word at the next time step.", "labels": [], "entities": []}, {"text": "Note that this vector l i is the language of the ith lexical item trained in concert with the model.", "labels": [], "entities": []}, {"text": "This allows our model to encode the distributional properties of the language switching.", "labels": [], "entities": []}, {"text": "We experimented with encoding and decoding the word and language embeddings for this task.", "labels": [], "entities": []}, {"text": "\u03b8 E X , \u03b8 E L , \u03b8 D X and \u03b8 D L are the parameters for the word encoder, language encoder, word decoder and language decoder respectively.", "labels": [], "entities": []}, {"text": "We identify four different model architectures () that could be useful in training codeswitched language models.", "labels": [], "entities": []}, {"text": "In the first model, our baseline, we have a sequence of words and we are trying to predict the following word.", "labels": [], "entities": []}, {"text": "This model is identical to running a traditional RNN language model on CS text.", "labels": [], "entities": []}, {"text": "For our baseline model we adapt the state-ofthe-art language model, the AWD-LSTM, for this domain.", "labels": [], "entities": [{"text": "AWD-LSTM", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.6469161510467529}]}, {"text": "This model is a 3 layered stacked LSTM trained via Averaged SGD with tied weights between the embedding and the softmax layer.", "labels": [], "entities": [{"text": "Averaged", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9563474059104919}]}, {"text": "There are several other important elements of this model, all of which are detailed in ().", "labels": [], "entities": []}, {"text": "The next word in this model is given by: In our second model we extend our baseline such that we have a sequence of words and their language IDs and we try to predict the following word.", "labels": [], "entities": []}, {"text": "In this and all the subsequent models, language ID is represented as a vector of length sixteen.", "labels": [], "entities": []}, {"text": "This model can be seen as a factored language model operating with code-switched data.", "labels": [], "entities": []}, {"text": "So, the next word in this model is given by: In our third model we take a sequence of words as an input and attempt to predict both the language and the value of the following word.", "labels": [], "entities": []}, {"text": "The next word in this model is given by:  In our fourth model we take a sequence of words and their corresponding language IDs as input and attempt to predict both the language and value of the subsequent word.", "labels": [], "entities": []}, {"text": "In our third and fourth models we operate with two loss values being calculated for (one for the word error, and one for the language error multiplied by 0.1) and gradients for both losses are propagated through the network and are used to update the weights.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hinglish Data Statistics", "labels": [], "entities": [{"text": "Hinglish Data Statistics", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.9827055136362711}]}, {"text": " Table 2: Perplexity scores of different models", "labels": [], "entities": [{"text": "Perplexity scores", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9391946196556091}]}]}