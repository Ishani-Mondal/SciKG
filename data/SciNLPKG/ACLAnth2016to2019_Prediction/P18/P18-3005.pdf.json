{"title": [{"text": "Pushing the Limits of Radiology with Joint Modeling of Visual and Textual Information", "labels": [], "entities": []}], "abstractContent": [{"text": "Recently, there has been increasing interest in the intersection of computer vision and natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6667564908663431}]}, {"text": "Researchers have studied several interesting tasks, including generating text descriptions from images and videos and language embedding of images.", "labels": [], "entities": []}, {"text": "More recent work has further extended the scope of this area to combine videos and language , learning to solve non-visual tasks using visual cues, visual question answering , and visual dialog.", "labels": [], "entities": [{"text": "question answering", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7130392640829086}]}, {"text": "Despite a large body of research on the intersection of vision-language technology, its adaption to the medical domain is not fully explored.", "labels": [], "entities": []}, {"text": "To address this research gap, we aim to develop machine learning models that can reason jointly on medical images and clinical text for advanced search, retrieval , annotation and description of medical images.", "labels": [], "entities": [{"text": "description of medical images", "start_pos": 180, "end_pos": 209, "type": "TASK", "confidence": 0.742502972483635}]}], "introductionContent": [{"text": "Integrating information from various modalities is deeply rooted inhuman lives.", "labels": [], "entities": []}, {"text": "Humans combine vision, language, speech and touch to acquire knowledge about the world and comprehend the world (.", "labels": [], "entities": []}, {"text": "Vision and Language are the most common ways of expressing our knowledge about the world.", "labels": [], "entities": []}, {"text": "Both Computer Vision (CV) and Natural Language Processing (NLP) demonstrated successful results on various general purpose tasks such as image classification, object detection, semantic segmentation, and machine translation.", "labels": [], "entities": [{"text": "image classification", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.7362309247255325}, {"text": "object detection", "start_pos": 159, "end_pos": 175, "type": "TASK", "confidence": 0.7861434519290924}, {"text": "semantic segmentation", "start_pos": 177, "end_pos": 198, "type": "TASK", "confidence": 0.7063163965940475}, {"text": "machine translation", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.8067372143268585}]}, {"text": "Although research at the intersection of CV and NLP is gaining pace, its applications to healthcare are still under-explored.", "labels": [], "entities": []}, {"text": "The success of Artificial Intelligence (AI) technologies in general purpose tasks is mainly attributed to publicly available large-scale datasets, enhanced compute power due to rise of Graphics Processing Units (GPUs), and due to advancements in Machine Learning (ML) algorithms and its various architectures.", "labels": [], "entities": []}, {"text": "One of the biggest hurdles in deploying ML (especially Deep Learning) models in healthcare is alack of annotated data.", "labels": [], "entities": []}, {"text": "Although it is easy to get annotated data for general purpose tasks by crowdsourcing, it is almost impossible for medical data because of limited expertise, privacy and ethical issues.", "labels": [], "entities": []}, {"text": "On the positive side, a lot of medical data in the form of medical images and accompanying text reports is stored in hospitals' Picture Archival and Communication Systems (PACS).", "labels": [], "entities": []}, {"text": "For instance, Beth Israel Deaconnes Medical Center (Harvard) generates approximately 20 terabytes of image data and one terabyte of text data per year.", "labels": [], "entities": [{"text": "Deaconnes Medical Center", "start_pos": 26, "end_pos": 50, "type": "DATASET", "confidence": 0.8832056522369385}]}, {"text": "Also, the drive toward structured reporting in radiology definitely enhance NLP accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9668994545936584}]}, {"text": "Interpreting medical images and summarising them in natural text is a challenging, complex and tedious task.", "labels": [], "entities": [{"text": "Interpreting medical images", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8981922070185343}, {"text": "summarising them in natural text", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.8410219073295593}]}, {"text": "Various research studies show that the general rate of missed radiological findings can be as much as 30%).", "labels": [], "entities": [{"text": "general rate of missed radiological findings", "start_pos": 39, "end_pos": 83, "type": "METRIC", "confidence": 0.7140463491280874}]}, {"text": "These errors are mainly due to limited expertise, increasing patient volumes, the subjectivity of human perception, fatigue, and inability to locate critical and subtle findings.", "labels": [], "entities": []}, {"text": "Based on a recent estimate one billion radiology examinations are performed worldwide annually.", "labels": [], "entities": []}, {"text": "This equates to about 40 million radiologist errors per annum.", "labels": [], "entities": []}, {"text": "In order to reduce these errors, there is a need to develop automated clinical decision support systems (CDSS) () that can interpret medical images and generate written reports to augment radiologist's work.", "labels": [], "entities": []}, {"text": "Our research aims to develop machine learning models that reason jointly on medical images and clinical text for advanced search, retrieval, annotation and description of medical images.", "labels": [], "entities": [{"text": "description of medical images", "start_pos": 156, "end_pos": 185, "type": "TASK", "confidence": 0.6981601417064667}]}, {"text": "Specifically, we aim to automatically generate description of medical images, to develop medical visual question answering system and to develop medical dialog agents that interact with patients to answer their queries based on their medical data.", "labels": [], "entities": [{"text": "medical visual question answering", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.5857575461268425}]}], "datasetContent": [{"text": "The proposed research work has approval from Macquarie University Human Research Ethics Committee to use medical data from Macquarie University Hospital.", "labels": [], "entities": [{"text": "Macquarie University Human Research Ethics Committee", "start_pos": 45, "end_pos": 97, "type": "DATASET", "confidence": 0.9581796328226725}, {"text": "Macquarie University Hospital", "start_pos": 123, "end_pos": 152, "type": "DATASET", "confidence": 0.9441200494766235}]}, {"text": "We will also use datasets that are publicly available such as ChestX-Ray8, Open-i 4 , and ImageCLEF 5 challenge datasets.", "labels": [], "entities": [{"text": "ImageCLEF 5 challenge datasets", "start_pos": 90, "end_pos": 120, "type": "DATASET", "confidence": 0.7960066199302673}]}, {"text": "These datasets comprise of medical images and their accompanied text in the form of disease labels or caption, mined from open source biomedical literature and image collections.", "labels": [], "entities": []}, {"text": "For medical captioning task, we will use standard image captioning metrics such as BLEU (), ROUGE), METEOR (Banerjee and), CIDEr (, and SPICE).", "labels": [], "entities": [{"text": "medical captioning task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8078246514002482}, {"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9991129040718079}, {"text": "ROUGE", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9939612150192261}, {"text": "METEOR", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9864161014556885}]}, {"text": "For VQA in the medical domain, we will use accuracy for multiple-choice questions, but to measure how much a predicted answer differs from ground truth based on differences in their semantic meaning, Wu-Palmer Similarity ( will be used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9990692734718323}]}, {"text": "For the Visual-Dialog task in the medical domain, an algorithm has to return candidate answers fora given medical image, dialog history, question, and a list of candidate answers.", "labels": [], "entities": []}, {"text": "We will use two standard retrieval metrics namely, recall@k and mean reciprocal rank (MRR) ().", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.998216450214386}, {"text": "mean reciprocal rank (MRR)", "start_pos": 64, "end_pos": 90, "type": "METRIC", "confidence": 0.9452391266822815}]}, {"text": "In the task of medical retrieval system, the evaluation task is to measure how effectively an algorithm is able to produce search results to satisfy the user's query in the form of sample image or complex textual query.", "labels": [], "entities": [{"text": "medical retrieval", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.684333547949791}]}, {"text": "For this task, standard information retrieval metrics such as Precision, Recall, and F-score will be used.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7535233497619629}, {"text": "Precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9989224672317505}, {"text": "Recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9668083190917969}, {"text": "F-score", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9981077909469604}]}], "tableCaptions": []}