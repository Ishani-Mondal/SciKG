{"title": [{"text": "Batch IS NOT Heavy: Learning Word Representations From All Samples", "labels": [], "entities": [{"text": "Batch", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8395819067955017}, {"text": "NOT", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8721608519554138}, {"text": "Learning Word Representations From All Samples", "start_pos": 20, "end_pos": 66, "type": "TASK", "confidence": 0.7179953356583914}]}], "abstractContent": [{"text": "Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations.", "labels": [], "entities": [{"text": "Stochastic Gradient Descent (SGD)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7829720675945282}, {"text": "learn word representations", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.6456506450970968}]}, {"text": "However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution.", "labels": [], "entities": []}, {"text": "Besides, SGD suffers from dramatic fluctuation due to the one-sample learning scheme.", "labels": [], "entities": [{"text": "SGD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9630652070045471}]}, {"text": "In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples.", "labels": [], "entities": []}, {"text": "Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples.", "labels": [], "entities": []}, {"text": "We evaluate AllVec on several benchmark tasks.", "labels": [], "entities": [{"text": "AllVec", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.8792260885238647}]}, {"text": "Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency , especially for small training corpora .", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing words using dense and real-valued vectors, aka word embeddings, has become the cornerstone for many natural language processing (NLP) tasks, such as document classification), parsing (, discourse relation recognition ( and named entity recognition (.", "labels": [], "entities": [{"text": "document classification", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.7630153596401215}, {"text": "parsing", "start_pos": 188, "end_pos": 195, "type": "TASK", "confidence": 0.9791816473007202}, {"text": "discourse relation recognition", "start_pos": 199, "end_pos": 229, "type": "TASK", "confidence": 0.6746228734652201}, {"text": "named entity recognition", "start_pos": 236, "end_pos": 260, "type": "TASK", "confidence": 0.6499295135339102}]}, {"text": "Word embeddings can be learned by optimizing that words occurring in similar contexts have similar embeddings, i.e. the well-known distributional hypothesis.", "labels": [], "entities": []}, {"text": "A representative method is skip-gram (SG) (, which realizes the hypothesis using a * The first two authors contributed equally to this paper and share the first-authorship.", "labels": [], "entities": []}, {"text": "(a) (b): Impact of different settings of negative sampling on skip-gram for the word analogy task on Text8.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.834071675936381}, {"text": "Text8", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.872150719165802}]}, {"text": "Clearly, the accuracy depends largely on (a) the sampling size of negative words, and (b) the sampling distribution (\u03b2 = 0 means the uniform distribution and \u03b2 = 1 means the word frequency distribution).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9995787739753723}]}, {"text": "The other family of methods is count-based, such as GloVe) and, which exploit low-rank models such as matrix factorization (MF) to learn embeddings by reconstructing the word co-occurrence statistics.", "labels": [], "entities": []}, {"text": "By far, most state-of-the-art embedding methods rely on SGD and negative sampling for optimization.", "labels": [], "entities": []}, {"text": "However, the performance of SGD is highly sensitive to the sampling distribution and the number of negative samples, as shown in.", "labels": [], "entities": [{"text": "SGD", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.966918408870697}]}, {"text": "Essentially, sampling is biased, making it difficult to converge to the same loss with all examples, regardless of how many update steps have been taken.", "labels": [], "entities": []}, {"text": "Moreover, SGD exhibits dramatic fluctuation and suffers from overshooting on local minimums.", "labels": [], "entities": [{"text": "SGD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9466691017150879}]}, {"text": "These drawbacks of SGD can be attributed to its one-sample learning scheme, which updates parameters based on one training sample in each step.", "labels": [], "entities": [{"text": "SGD", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9361551403999329}]}, {"text": "To address the above-mentioned limitations of SGD, a natural solution is to perform exact (full) batch learning.", "labels": [], "entities": [{"text": "SGD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9722332954406738}]}, {"text": "In contrast to SGD, batch learning does not involve any sampling procedure and computes the gradient overall training samples.", "labels": [], "entities": [{"text": "SGD", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9354857206344604}]}, {"text": "As such, it can easily converge to a better optimum in a more stable way.", "labels": [], "entities": []}, {"text": "Nevertheless, a well-known difficulty in applying full batch learning lies in the expensive computational cost for large-scale data.", "labels": [], "entities": []}, {"text": "Taking the word embedding learning as an example, if the vocabulary size is |V |, then evaluating the loss function and computing the full gradient takes O(|V | 2 k) time, where k is the embedding size.", "labels": [], "entities": [{"text": "O", "start_pos": 154, "end_pos": 155, "type": "METRIC", "confidence": 0.987343430519104}]}, {"text": "This high complexity is unaffordable in practice, since |V | 2 can easily reach billion level or even higher.", "labels": [], "entities": []}, {"text": "In this paper, we introduce AllVec, an exact and efficient word embedding method based on full batch learning.", "labels": [], "entities": []}, {"text": "To address the efficiency challenge in learning from all training samples, we devise a regression-based loss function for word embedding, which allows fast optimization with memorization strategies.", "labels": [], "entities": []}, {"text": "Specifically, the acceleration is achieved by reformulating the expensive loss overall negative samples using a partition and a decouple operation.", "labels": [], "entities": []}, {"text": "By decoupling and caching the bottleneck terms, we succeed to use all samples for each parameter update in a manageable time complexity which is mainly determined by the positive samples.", "labels": [], "entities": []}, {"text": "The main contributions of this work are summarized as follows: \u2022 We present a fine-grained weighted least square loss for learning word embeddings.", "labels": [], "entities": []}, {"text": "Unlike GloVe, it explicitly accounts for all negative samples and reweights them with a frequency-aware strategy.", "labels": [], "entities": []}, {"text": "\u2022 We propose an efficient and exact optimization algorithm based on full batch gradient optimization.", "labels": [], "entities": []}, {"text": "It has a comparable time complexity with SGD, but being more effective and stable due to the consideration of all samples in each parameter update.", "labels": [], "entities": []}, {"text": "\u2022 We perform extensive experiments on several benchmark datasets and tasks to demonstrate the effectiveness, efficiency, and convergence property of our AllVec method.", "labels": [], "entities": [{"text": "convergence", "start_pos": 125, "end_pos": 136, "type": "METRIC", "confidence": 0.9811171889305115}]}], "datasetContent": [{"text": "We conduct experiments on three popular evaluation tasks, namely word analogy (), word similarity) and QVEC (.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.7458192408084869}, {"text": "QVEC", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9051960110664368}]}, {"text": "The task aims to answer questions like, \"a is to b as c is to ?\".", "labels": [], "entities": []}, {"text": "We adopt the Google testbed 2 which contains 19, 544 such questions in two categories: semantic and syntactic.", "labels": [], "entities": [{"text": "Google testbed 2", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.9099138776461283}]}, {"text": "The semantic questions are usually analogies about people or locations, like \"king is to man as queen is to ?\", while the syntactic questions focus on forms or tenses, e.g., \"swimming is to swim as running to ?\".", "labels": [], "entities": []}, {"text": "We perform evaluation on six datasets, including MEN (), MC, RW (Luong et al., 2013), RG, WS-353 Similarity (WSim) and Relatedness (WRel) ().", "labels": [], "entities": [{"text": "MEN", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8138342499732971}, {"text": "Similarity (WSim) and Relatedness (WRel)", "start_pos": 97, "end_pos": 137, "type": "METRIC", "confidence": 0.8301879465579987}]}, {"text": "We compute the spearman rank correlation between the similarity scores calculated based on the trained embeddings and human labeled scores.", "labels": [], "entities": []}, {"text": "QVEC is an intrinsic evaluation metric of word embeddings based on the alignment to features extracted from manually crafted lexical resources.", "labels": [], "entities": []}, {"text": "QVEC has shown strong correlation with the performance of embeddings in several semantic tasks (.", "labels": [], "entities": [{"text": "QVEC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8534392714500427}]}, {"text": "We compare AllVec with the following word embedding methods.", "labels": [], "entities": []}, {"text": "\u2022 SG: This is the original skip-gram model with  For all baselines, we use the original implementation released by the authors.", "labels": [], "entities": []}, {"text": "We evaluate the performance of AllVec on four real-world corpora, namely Text8 3 , NewsIR 4 , Wiki-sub and Wiki-all.", "labels": [], "entities": [{"text": "Wiki-all", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.9060231447219849}]}, {"text": "Wiki-sub is a subset of 2017 Wikipedia dump 5 . All corpora have been pre-processed by a standard pipeline (i.e. removing non-textual elements, lowercasing and tokenization).", "labels": [], "entities": [{"text": "Wiki-sub", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.918300211429596}]}, {"text": "summarizes the statistics of these corpora.", "labels": [], "entities": []}, {"text": "To obtain M wc for positive (w, c) pairs, we follow GloVe where word pairs that are x words apart contribute 1/x to M wc . The window size is set as win = 8.", "labels": [], "entities": []}, {"text": "Regarding \u03b1 + wc , we set xmax = 100 and \u03c1 = 0.75.", "labels": [], "entities": []}, {"text": "For a fair comparison, the embedding size k is set as 200 for all models and corpora.", "labels": [], "entities": []}, {"text": "AllVec can be easily trained by AdaGrad (Zeiler, 2012) like GloVe or Newton-like () second order methods.", "labels": [], "entities": [{"text": "AdaGrad (Zeiler, 2012)", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.8533502121766409}]}, {"text": "For models based on negative sampling (i.e. SG, SGA and LexVec), the sample size is set as n = 25 for Text8, n = 10 for NewsIR and n = 5 for Wiki-sub and Wiki-all.", "labels": [], "entities": [{"text": "LexVec", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.9444355964660645}, {"text": "Text8", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.9143736958503723}, {"text": "NewsIR", "start_pos": 120, "end_pos": 126, "type": "DATASET", "confidence": 0.9453020095825195}, {"text": "Wiki-all", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.9116001129150391}]}, {"text": "The setting is also suggested by.", "labels": [], "entities": []}, {"text": "Other detailed hyper-parameters are reported in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. As shown, AllVec achieves the high- est total accuracy (Tot.) in all corpora, particu-larly in smaller corpora (Text8 and NewsIR). The  reason is that in smaller corpora the number of  positive (w, c) pairs is very limited, thus making  use of negative examples will bring more benefits.  Similar reason also explains the poor accuracy of  GloVe in Text8, because GloVe does not consider  negative samples. Even in the very large corpus  (Wiki-all), ignoring negative samples still results  in sub-optimal performance.", "labels": [], "entities": [{"text": "accuracy (Tot.)", "start_pos": 56, "end_pos": 71, "type": "METRIC", "confidence": 0.7711475789546967}, {"text": "Text8", "start_pos": 122, "end_pos": 127, "type": "DATASET", "confidence": 0.9452879428863525}, {"text": "NewsIR", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.8626644611358643}, {"text": "accuracy", "start_pos": 337, "end_pos": 345, "type": "METRIC", "confidence": 0.9978554844856262}]}, {"text": " Table 2: Results (\"Tot.\" denotes total accuracy) on the word analogy task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9456173181533813}, {"text": "word analogy task", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8253141840298971}]}, {"text": " Table 3: Results on the word similarity task.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8313255111376444}]}, {"text": " Table 4: Results on QVEC.", "labels": [], "entities": [{"text": "QVEC", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.8878502249717712}]}, {"text": " Table 5: Comparison of runtime.", "labels": [], "entities": []}]}