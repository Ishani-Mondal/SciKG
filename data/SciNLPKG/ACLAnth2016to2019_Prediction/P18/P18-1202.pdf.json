{"title": [{"text": "Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction", "labels": [], "entities": []}], "abstractContent": [{"text": "Fine-grained opinion analysis aims to extract aspect and opinion terms from each sentence for opinion summarization.", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.748298704624176}, {"text": "opinion summarization", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.6554833203554153}]}, {"text": "Supervised learning methods have proven to be effective for this task.", "labels": [], "entities": []}, {"text": "However, in many domains, the lack of labeled data hinders the learning of a precise extraction model.", "labels": [], "entities": []}, {"text": "In this case, unsupervised domain adaptation methods are desired to transfer knowledge from the source domain to any un-labeled target domain.", "labels": [], "entities": []}, {"text": "In this paper, we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations.", "labels": [], "entities": []}, {"text": "We treat these relations as invariant \"pivot information\" across domains to build structural correspondences and generate an auxiliary task to predict the relation between any two adjacent words in the dependency tree.", "labels": [], "entities": []}, {"text": "In the end, we demonstrate state-of-the-art results on three benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of fine-grained opinion analysis involves extraction of opinion targets (or aspect terms) and opinion expressions (or opinion terms) from each review sentence.", "labels": [], "entities": [{"text": "fine-grained opinion analysis", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6373690863450369}]}, {"text": "For example, in the sentence: \"They offer good appetizers\", the aspect and opinion terms are appetizers and good correspondingly.", "labels": [], "entities": []}, {"text": "Many supervised deep models have been proposed for this problem (, and obtained promising results.", "labels": [], "entities": []}, {"text": "However, these methods fail to adapt well across domains, because the aspect terms from two different domains are usually disjoint, e.g., laptop v.s. restaurant, leading to large domain shift in the feature vector space.", "labels": [], "entities": []}, {"text": "Though unsupervised methods () can deal with data with few labels, their performance is unsatisfactory compared with supervised ones.", "labels": [], "entities": []}, {"text": "There have been a number of domain adaptation methods for coarse-grained sentiment classification problems across domains, where an overall sentiment polarity of a sentence or document is being predicted.", "labels": [], "entities": [{"text": "coarse-grained sentiment classification", "start_pos": 58, "end_pos": 97, "type": "TASK", "confidence": 0.6807945072650909}]}, {"text": "Nevertheless, very few approaches exist for cross-domain fine-grained opinion analysis due to the difficulties in fine-grained adaptation, which is more challenging than coarse-grained problems.", "labels": [], "entities": [{"text": "cross-domain fine-grained opinion analysis", "start_pos": 44, "end_pos": 86, "type": "TASK", "confidence": 0.6744731217622757}]}, {"text": "proposed a bootstrap method based on the TrAdaBoost algorithm ( to iteratively expand opinion and aspect lexicons in the target domain by exploiting source-domain labeled data and cross-domain common relations between aspect terms and opinion terms.", "labels": [], "entities": []}, {"text": "However, their model requires a seed opinion lexicon in the target domain and pre-mined syntactic patterns as abridge.", "labels": [], "entities": []}, {"text": "proposed to use rules to generate auxiliary supervision on top of a recurrent neural network to learn domain-invariant hidden representation for each word.", "labels": [], "entities": []}, {"text": "The performance highly depends on the quality of the manually defined rules and the prior knowledge of a sentiment lexicon.", "labels": [], "entities": []}, {"text": "In addition, the recurrent structure fails to capture the syntactic interactions among words intrinsically for opinion extraction.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.8736631274223328}]}, {"text": "The requirement for rules makes the above methods non-flexible.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel cross-domain Recursive Neural Network (RNN) 1 for aspect and opinion terms co-extraction across domains.", "labels": [], "entities": []}, {"text": "Our motivations are twofold: 1) The dependency relations capture the interactions among different words.", "labels": [], "entities": []}, {"text": "These relations are especially important for identifying aspect terms and opinion terms (Qiu et al., 2011;, which are also domain-invariant within the same language.", "labels": [], "entities": []}, {"text": "Therefore, they can be used as \"pivot\" information to bridge the gap between different domains.", "labels": [], "entities": []}, {"text": "2) Inspired by the idea of structural learning (Ando and, the success of target task depends on the ability of finding good predictive structures learned from other related tasks, e.g., structural correspondence learning (SCL)) for coarse-grained cross-domain sentiment classification.", "labels": [], "entities": [{"text": "structural correspondence learning (SCL))", "start_pos": 186, "end_pos": 227, "type": "TASK", "confidence": 0.7460972468058268}, {"text": "cross-domain sentiment classification", "start_pos": 247, "end_pos": 284, "type": "TASK", "confidence": 0.6441788673400879}]}, {"text": "Here, we aim to generate an auxiliary task on dependency relation classification.", "labels": [], "entities": [{"text": "dependency relation classification", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.7823196053504944}]}, {"text": "Different from previous approaches, our auxiliary task and the target extraction task are of heterogeneous label spaces.", "labels": [], "entities": [{"text": "target extraction", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.6972844302654266}]}, {"text": "We aim to integrate this auxiliary task with distributed relation representation learning into a recursive neural network.", "labels": [], "entities": [{"text": "distributed relation representation learning", "start_pos": 45, "end_pos": 89, "type": "TASK", "confidence": 0.6675492972135544}]}, {"text": "Specifically, we generate a dependency tree for each sentence from the dependency parser and construct a unified RNN that integrates an auxiliary task into the computation of each node.", "labels": [], "entities": []}, {"text": "The auxiliary task is to classify the dependency relation for each direct edge in the dependency tree by learning a relation feature vector.", "labels": [], "entities": []}, {"text": "To reduce label noise brought by inaccurate parsing trees, we further propose to incorporate an autoencoder into the auxiliary task to group the relations into different clusters.", "labels": [], "entities": []}, {"text": "Finally, to model the sequential context interaction, we develop a joint architecture that combines RNN with a sequential labeling model for aspect and opinion terms extraction.", "labels": [], "entities": [{"text": "aspect and opinion terms extraction", "start_pos": 141, "end_pos": 176, "type": "TASK", "confidence": 0.5845997571945191}]}, {"text": "Extensive experiments are conducted to demonstrate the advantage of our proposed model.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data is taken from the benchmark customer reviews in three different domains, namely restaurant, laptop and digital devices.", "labels": [], "entities": []}, {"text": "The restaurant domain contains a combination of restaurant reviews from SemEval 2014 task 4 subtask 1 ( and SemEval 2015 task 12 subtask 1 (.", "labels": [], "entities": [{"text": "SemEval 2014 task 4 subtask", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.6481769919395447}, {"text": "SemEval 2015 task 12 subtask", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.6728205561637879}]}, {"text": "The laptop domain consists of laptop reviews from SemEval 2014 task 4 subtask 1.", "labels": [], "entities": [{"text": "laptop domain consists of laptop reviews from SemEval 2014 task 4 subtask 1", "start_pos": 4, "end_pos": 79, "type": "DATASET", "confidence": 0.6750457401459034}]}, {"text": "For digital device, we take reviews from () containing sentences from 5 digital devices.", "labels": [], "entities": []}, {"text": "The statistics for each domain are shown in.", "labels": [], "entities": []}, {"text": "In our experiments, we randomly split the data in each domain into training set and testing set with the proportion being 3:1.", "labels": [], "entities": []}, {"text": "To obtain more rigorous result, we make three random splits for each domain and test the learned model on each split.", "labels": [], "entities": []}, {"text": "The number of sentences for training and testing after each split is also shown in.", "labels": [], "entities": []}, {"text": "Each sentence is labeled with aspect terms and opinion terms.", "labels": [], "entities": []}, {"text": "For each cross-domain task, we conduct both inductive and transductive experiments.", "labels": [], "entities": []}, {"text": "Specifically, we train our model only on the training sets from both (labeled) source and (unlabeled) target domains.", "labels": [], "entities": []}, {"text": "For testing, the inductive results are obtained using the test data from the target domain, and the transductive results are obtained using the (unlabeled) training data from the target domain.", "labels": [], "entities": []}, {"text": "The evaluation metric we used is F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9859070181846619}]}, {"text": "Following the setting from existing work, only exact match could be counted as correct.", "labels": [], "entities": []}, {"text": "For experimental setup, we use Stanford Dependency Parser ( to generate dependency trees.", "labels": [], "entities": []}, {"text": "There are in total 43 different dependency relations, i.e. 43 classes for the auxiliary task.", "labels": [], "entities": []}, {"text": "We set the number of latent relation groups as 20.", "labels": [], "entities": []}, {"text": "The input word features for RNSCN are pre-trained word embeddings using word2vec) which is trained on 3M reviews from the Yelp dataset 2 and electronics dataset in Amazon reviews.", "labels": [], "entities": [{"text": "Yelp dataset 2", "start_pos": 122, "end_pos": 136, "type": "DATASET", "confidence": 0.9571953415870667}, {"text": "electronics dataset in Amazon reviews", "start_pos": 141, "end_pos": 178, "type": "DATASET", "confidence": 0.7601382434368134}]}, {"text": "The dimension of word embeddings is 100.", "labels": [], "entities": []}, {"text": "Because of the relatively small size of the training data compared with the number of parameters, we firstly pre-train RNSCN for 5 epochs with minibatch size 30 and rmsprop initialized at 0.01.", "labels": [], "entities": [{"text": "RNSCN", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.78680419921875}, {"text": "rmsprop", "start_pos": 165, "end_pos": 172, "type": "METRIC", "confidence": 0.9826124906539917}]}, {"text": "The joint model of RNSCN + -GRU is then trained with rmsprop initialized at 0.001 and mini-batch size 30.", "labels": [], "entities": [{"text": "RNSCN + -GRU", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.5929318517446518}, {"text": "rmsprop", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9487354755401611}]}, {"text": "The trade-off parameter \u03b1, \u03b2 and \u03b3 are set to be 1, 0.001 and 0.1, respectively.", "labels": [], "entities": []}, {"text": "The hidden-layer dimension for GRU is 50, and the context window size is 3 for input feature vectors of GRU.", "labels": [], "entities": [{"text": "GRU", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.9100261926651001}]}, {"text": "For the joint model of RNSCN-CRF, we implement SGD with a decaying learning rate initialized at 0.02.", "labels": [], "entities": []}, {"text": "The context window size is also 3 in this case.", "labels": [], "entities": []}, {"text": "Both joint models are trained for 10 epochs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics with number of sentences.", "labels": [], "entities": []}, {"text": " Table 2: Comparisons with different baselines.", "labels": [], "entities": []}, {"text": " Table 3: Comparisons with different variants of the proposed model.", "labels": [], "entities": []}, {"text": " Table 4: Comparisons with different transfer setting.", "labels": [], "entities": []}]}