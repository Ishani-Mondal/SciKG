{"title": [{"text": "Taylor's Law for Human Linguistic Sequences", "labels": [], "entities": [{"text": "Human Linguistic Sequences", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6222674349943796}]}], "abstractContent": [{"text": "Taylor's law describes the fluctuation characteristics underlying a system in which the variance of an event within a time span grows by a power law with respect to the mean.", "labels": [], "entities": []}, {"text": "Although Taylor's law has been applied in many natural and social systems, its application for language has been scarce.", "labels": [], "entities": []}, {"text": "This article describes anew quantification of Taylor's law in natural language and reports an analysis of over 1100 texts across 14 languages.", "labels": [], "entities": []}, {"text": "The Tay-lor exponents of written natural language texts were found to exhibit almost the same value.", "labels": [], "entities": []}, {"text": "The exponent was also compared for other language-related data, such as the child-directed speech, music, and programming language code.", "labels": [], "entities": []}, {"text": "The results show how the Taylor exponent serves to quantify the fundamental structural complexity underlying linguistic time series.", "labels": [], "entities": []}, {"text": "The article also shows the applicability of these findings in evaluating language models .", "labels": [], "entities": []}], "introductionContent": [{"text": "Taylor's law characterizes how the variance of the number of events fora given time and space grows with respect to the mean, forming a power law.", "labels": [], "entities": []}, {"text": "It is a quantification method for the clustering behavior of a system.", "labels": [], "entities": []}, {"text": "Since the pioneering studies of this concept, a substantial number of studies have been conducted across various domains, including ecology, life science, physics, finance, and human dynamics, as well summarized in reported Taylor's law in wind energy data using a non-parametric regression.", "labels": [], "entities": []}, {"text": "Those two papers also refer to research about Taylor's law in a wide range of fields.", "labels": [], "entities": [{"text": "Taylor's law", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.6043504575888315}]}, {"text": "Despite such diverse application across domains, there has been little analysis based on Taylor's law in studying natural language.", "labels": [], "entities": []}, {"text": "The only such report, to the best of our knowledge, is Gerlach and, but they measured the mean and variance by means of the vocabulary size within a document.", "labels": [], "entities": []}, {"text": "This approach essentially differs from the original concept of Taylor analysis, which fundamentally counts the number of events, and thus the theoretical background of Taylor's law as presented in Eisler, cannot be applied to interpret the results.", "labels": [], "entities": [{"text": "Taylor analysis", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.8763562738895416}]}, {"text": "For the work described in this article, we applied Taylor's law for texts, in a manner close to the original concept.", "labels": [], "entities": []}, {"text": "We considered lexical fluctuation within texts, which involves the cooccurrence and burstiness of word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.7036869078874588}]}, {"text": "The results can thus be interpreted according to the analytical results of Taylor's law, as described later.", "labels": [], "entities": []}, {"text": "We found that the Taylor exponent is indeed a characteristic of texts and is universal across various kinds of texts and languages.", "labels": [], "entities": []}, {"text": "These results are shown here for data including over 1100 singleauthor texts across 14 languages and large-scale newspaper data.", "labels": [], "entities": []}, {"text": "Moreover, we found that the Taylor exponents for other symbolic sequential data, including child-directed speech, programming language code, and music, differ from those for written natural language texts, thus distinguishing different kinds of data sources.", "labels": [], "entities": []}, {"text": "The Taylor exponent in this sense could categorize and quantify the structural complexity of language.", "labels": [], "entities": []}, {"text": "The Chomsky hierarchy) is, of course, the most important framework for such categorization.", "labels": [], "entities": []}, {"text": "The Taylor exponent is another way to quantify the complexity of natural language: it allows for continuous quantification based on lexical fluctuation.", "labels": [], "entities": []}, {"text": "Since the Taylor exponent can quantify and characterize one aspect of natural language, our findings are applicable in computational linguistics to assess language models.", "labels": [], "entities": []}, {"text": "At the end of this article, in \u00a75, we report how the most basic character-based long short-term memory (LSTM) unit produces texts with a Taylor exponent of 0.50, equal to that of a sequence of independent and identically distributed random variables (an i.i.d. sequence).", "labels": [], "entities": []}, {"text": "This shows how such models are limited in producing consistent co-occurrence among words, as compared with areal text.", "labels": [], "entities": []}, {"text": "Taylor analysis thus provides a possible direction to reconsider the limitations of language models.", "labels": [], "entities": [{"text": "Taylor analysis", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8148708045482635}]}], "datasetContent": [{"text": "The main contribution of this paper is the findings of Taylor's law behavior for real texts as presented thus far.", "labels": [], "entities": []}, {"text": "This section explains the applicability of these findings, through results obtained with baseline language models.", "labels": [], "entities": []}, {"text": "As mentioned previously, i.i.d. mathematical processes have a Taylor exponent of 0.50.", "labels": [], "entities": [{"text": "Taylor exponent", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.90944504737854}]}, {"text": "We show here that, even if a process is not trivially i.i.d., the exponent often takes a value of 0.50: Growth of\u02c6\u03b1of\u02c6 of\u02c6\u03b1 with respect to \u2206t, averaged across data sets within each data kind.", "labels": [], "entities": []}, {"text": "The plot labeled \"random\" shows the average for the two datasets of randomized text from Moby Dick (shuffled and bigrams, as explained in \u00a75).", "labels": [], "entities": []}, {"text": "Since this analysis required a large amount of computation, for the large data sets (such as newspaper and programming language data), 4 million words were taken from each kind of data and used here.", "labels": [], "entities": [{"text": "newspaper and programming language data", "start_pos": 93, "end_pos": 132, "type": "DATASET", "confidence": 0.7054105758666992}]}, {"text": "When \u2206t was small, the Taylor exponent was close to 0.5, as theoretically described in the main text.", "labels": [], "entities": [{"text": "Taylor exponent", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.970768541097641}]}, {"text": "As \u2206t was increased, the value of\u02c6\u03b1of\u02c6 of\u02c6\u03b1 grew.", "labels": [], "entities": []}, {"text": "The maximum \u2206t was about 10,000, or about one-tenth of the length of one long literary text.", "labels": [], "entities": []}, {"text": "For the kinds of data investigated here, \u02c6 \u03b1 grew almost linearly.", "labels": [], "entities": []}, {"text": "The results show that, at a given \u2206t, the Taylor exponent has some capability to distinguish different kinds of text data.", "labels": [], "entities": []}, {"text": "for random processes, including texts produced by standard language models such as n-gram based models.", "labels": [], "entities": []}, {"text": "A more complete work in this direction is reported in).", "labels": [], "entities": []}, {"text": "shows samples from each of two simple random processes.", "labels": [], "entities": []}, {"text": "shows the behavior of a shuffled text of Moby Dick.", "labels": [], "entities": []}, {"text": "Obviously, Given that the Taylor exponent becomes larger fora sequence with words dependent on each other, as explained in \u00a73, we would expect that a sequence generated by an n-gram model would exhibit an exponent larger than 0.50.", "labels": [], "entities": []}, {"text": "The simplest such model is the bigram model, so a sequence of 300,000 words was probabilistically generated using a bigram model of Moby Dick.", "labels": [], "entities": []}, {"text": "shows the Taylor analysis, revealing that the exponent remained 0.50.", "labels": [], "entities": [{"text": "exponent", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9842343926429749}]}, {"text": "This result does not depend much on the quality of the individual samples.", "labels": [], "entities": []}, {"text": "The first and second box plots in show the distribution of exponents for 10 different samples for the shuffled and bigram-generated texts, respectively.", "labels": [], "entities": []}, {"text": "The exponents were all around 0.50, with small variance.", "labels": [], "entities": []}, {"text": "State-of-the-art language models are based on neural models, and they are mainly evaluated by perplexity and in terms of the performance of individual applications.", "labels": [], "entities": []}, {"text": "Since their architecture is complex, quality evaluation has become an issue.", "labels": [], "entities": []}, {"text": "One possible improvement would be to use an evaluation method that qualitatively differs from judging application performance.", "labels": [], "entities": []}, {"text": "One such method is to verify whether the properties underlying natural language hold for texts generated by language models.", "labels": [], "entities": []}, {"text": "The Taylor exponent is one such possibility, among various properties of natural language texts.", "labels": [], "entities": []}, {"text": "As a step toward this approach, shows two results produced by neural language models.", "labels": [], "entities": []}, {"text": "shows the result fora sample of 2 million characters produced by a stan-dard (three-layer) stacked character-based LSTM unit that learned the complete works of Shakespeare.", "labels": [], "entities": []}, {"text": "The model was optimized to minimize the cross-entropy with a stochastic gradient algorithm to predict the next character from the previous 128 characters.", "labels": [], "entities": []}, {"text": "See for the details of the experimental settings.", "labels": [], "entities": []}, {"text": "The Taylor exponent of the generated text was 0.50.", "labels": [], "entities": [{"text": "Taylor exponent", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9667651057243347}]}, {"text": "This indicates that the character-level language model could not capture or reproduce the word-level clustering behavior in text.", "labels": [], "entities": []}, {"text": "This analysis sheds light on the quality of the language model, separate from the prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9005087018013}]}, {"text": "The application of Taylor's law fora wider range of language models appears in.", "labels": [], "entities": []}, {"text": "Briefly, state-of-theart word-level language models can generate text whose Taylor exponent is larger than 0.50 but smaller than that of the dataset used for training.", "labels": [], "entities": []}, {"text": "This indicates both the capability of modeling burstiness in text and the room for improvement.", "labels": [], "entities": []}, {"text": "Also, the perplexity values correlate well with the Taylor exponents.", "labels": [], "entities": []}, {"text": "Therefore, Taylor exponent can reasonably serve for evaluating machinegenerated text.", "labels": [], "entities": []}, {"text": "In contrast to character-level neural language models, neural-network-based machine translation (NMT) models are, in fact, capable of maintaining the burstiness of the original text.", "labels": [], "entities": [{"text": "machine translation (NMT)", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.8036851406097412}]}, {"text": "shows the Taylor analysis fora machinetranslated text of Les Mis\u00e9rables (from French to English), obtained from Google NMT (.", "labels": [], "entities": [{"text": "machinetranslated text of Les Mis\u00e9rables (from French to English", "start_pos": 31, "end_pos": 95, "type": "TASK", "confidence": 0.6775827258825302}, {"text": "Google NMT", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.8699520528316498}]}, {"text": "We split the text into 5000-character portions because of the API's limitation (See) for the details).", "labels": [], "entities": []}, {"text": "As is expected and desirable, the translated text retains the clustering behavior of the original text, as the Taylor exponent of 0.57 is equivalent to that of the original text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data we used in this article. For each dataset, length is the number of words, vocabulary is the  number of different words. For detail of the data kind, see  \u00a73.2.", "labels": [], "entities": [{"text": "length", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9600779414176941}]}]}