{"title": [{"text": "Supervised Treebank Conversion: Data and Approaches", "labels": [], "entities": []}], "abstractContent": [{"text": "Treebank conversion is a straightforward and effective way to exploit various heterogeneous treebanks for boosting parsing accuracy.", "labels": [], "entities": [{"text": "Treebank conversion", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.708893358707428}, {"text": "parsing", "start_pos": 115, "end_pos": 122, "type": "TASK", "confidence": 0.9740954637527466}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9103551506996155}]}, {"text": "However, previous work mainly focuses on unsupervised treebank conversion and makes little progress due to the lack of manually labeled data where each sentence has two syntactic trees complying with two different guidelines at the same time, referred as bi-tree aligned data.", "labels": [], "entities": [{"text": "treebank conversion", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7285956740379333}]}, {"text": "In this work, we for the first time propose the task of supervised treebank conversion.", "labels": [], "entities": [{"text": "supervised treebank conversion", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.6157266497612}]}, {"text": "First, we manually construct a bi-tree aligned dataset containing over ten thousand sentences.", "labels": [], "entities": []}, {"text": "Then, we propose two simple yet effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser.", "labels": [], "entities": []}, {"text": "Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9747593998908997}, {"text": "parsing", "start_pos": 249, "end_pos": 256, "type": "TASK", "confidence": 0.9659457206726074}, {"text": "accuracy", "start_pos": 257, "end_pos": 265, "type": "METRIC", "confidence": 0.726354718208313}]}], "introductionContent": [{"text": "During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing; Zhou * The first two (student) authors make equal contributions to this work.", "labels": [], "entities": [{"text": "neural network based dependency parsing", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.6680093824863433}, {"text": "discrete-feature based parsing", "start_pos": 134, "end_pos": 164, "type": "TASK", "confidence": 0.6991705099741617}]}, {"text": "Zhenghua is the correspondence author.", "labels": [], "entities": [{"text": "correspondence author", "start_pos": 16, "end_pos": 37, "type": "DATASET", "confidence": 0.7448162138462067}]}], "datasetContent": [{"text": "We randomly select 1, 000/2, 000 sentences from our newly annotated data as the dev/test datasets, and the remaining as train.", "labels": [], "entities": []}, {"text": "shows the data statistics after removing some broken sentences (ungrammatical or wrongly segmented) discovered during annotation.", "labels": [], "entities": []}, {"text": "The \"#tok (our)\" column shows the number of tokens annotated according to our guideline.", "labels": [], "entities": []}, {"text": "Train-HIT contains all sentences in HIT-CDT except those in dev/test, among which most sentences only have the HIT-CDT annotations.", "labels": [], "entities": []}, {"text": "We use the standard labeled attachment score (LAS, UAS for unlabeled) to measure the parsing and conversion accuracy.", "labels": [], "entities": [{"text": "labeled attachment score (LAS", "start_pos": 20, "end_pos": 49, "type": "METRIC", "confidence": 0.7276602625846863}, {"text": "UAS", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8773370981216431}, {"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9726582169532776}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9200195074081421}]}, {"text": "In order to more flexibly realize our ideas, we re-implement the baseline biaffine parser in C++ based on the lightweight neural network library of . On the Chinese CoNLL-2009 data, our parser achieves 85.80% in LAS, whereas the original tensorflow-based parser 6 achieves 85.54% (85.38% reported in their paper) under the same parameter settings and external word embedding.", "labels": [], "entities": [{"text": "Chinese CoNLL-2009 data", "start_pos": 157, "end_pos": 180, "type": "DATASET", "confidence": 0.8573411901791891}]}, {"text": "We follow most parameter settings of.", "labels": [], "entities": []}, {"text": "The external word embedding dictionary is trained on Chinese Gigaword (LDC2003T09) with GloVe ().", "labels": [], "entities": [{"text": "Chinese Gigaword", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.8093017935752869}]}, {"text": "For   efficiency, we use two biSeqLSTM layers instead of three, and reduce the biSeqLSTM output dimension (300) and the MLP output dimension (200).", "labels": [], "entities": [{"text": "MLP output dimension", "start_pos": 120, "end_pos": 140, "type": "METRIC", "confidence": 0.7606024344762167}]}, {"text": "For the conversion approaches, the sourceside pattern/label embedding dimensions are 50 (thus |r pat i\u2190j | = 200), and the treeLSTM output dimension is 100 (thus |r tree i\u2190j | = 300).", "labels": [], "entities": []}, {"text": "During training, we use 200 sentences as a data batch, and evaluate the model on the dev data every 50 batches (as an epoch).", "labels": [], "entities": []}, {"text": "Training stops after the peak LAS on dev does not increase in 50 consecutive epochs.", "labels": [], "entities": [{"text": "LAS", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9863708019256592}]}, {"text": "For the multi-task learning approach, we randomly sample 100 train sentences and 100 train-HIT sentences to compose a data batch, for the purpose of corpus weighting.", "labels": [], "entities": []}, {"text": "To fully utilize train-HIT for the conversion task, the conversion models are built upon multi-task learning, and directly reuse the embeddings and biSeqLSTMs of the multitask trained model without fine-tuning.", "labels": [], "entities": []}, {"text": "shows the conversion accuracy on the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.7630053162574768}]}, {"text": "As a strong baseline for the conversion task, the multi-task trained target-side parser (\"multi-task\") does not used src during both training and evaluation.", "labels": [], "entities": [{"text": "conversion task", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.919554740190506}]}, {"text": "In contrast, the conversion approaches use both the sentence x and d src as inputs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data statistics. Kindly note that  sentences in train are also in train-HIT.", "labels": [], "entities": []}, {"text": " Table 3: Conversion accuracy on test data.", "labels": [], "entities": [{"text": "Conversion", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.757066547870636}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.7424452900886536}]}, {"text": " Table 4: Feature ablation for the conversion  approaches.", "labels": [], "entities": []}, {"text": " Table 5: Parsing accuracy on test data.  LAS difference between any two systems is  statistically significant (p < 0.005) according  to Dan Bikel s randomized parsing evaluation  comparer for significance test", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9805721044540405}, {"text": "LAS difference", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.9751853942871094}]}, {"text": " Table 6: Results on the fully annotated 372  sentences of the test data.", "labels": [], "entities": []}]}