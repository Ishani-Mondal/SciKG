{"title": [{"text": "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality", "labels": [], "entities": [{"text": "Restricted Recurrent Neural Tensor Networks", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6738304257392883}]}], "abstractContent": [{"text": "Increasing the capacity of recurrent neu-ral networks (RNN) usually involves augmenting the size of the hidden layer, with significant increase of computational cost.", "labels": [], "entities": []}, {"text": "Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage.", "labels": [], "entities": []}, {"text": "In this paper , we introduce restricted recurrent neu-ral tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words.", "labels": [], "entities": []}, {"text": "Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve language model performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs.", "labels": [], "entities": []}, {"text": "These results hold for r-RNTNs using Gated Recurrent Units and Long Short-Term Memory.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural networks (RNN), which compute their next output conditioned on a previously stored hidden state, area natural solution to sequence modeling.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.7270107865333557}]}, {"text": "applied RNNs to word-level language modeling (we refer to this model as s-RNN), outperforming traditional n-gram methods.", "labels": [], "entities": [{"text": "word-level language modeling", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6269172728061676}]}, {"text": "However, increasing capacity (number of tunable parameters) by augmenting the size H of the hidden (or recurrent) layer -to model more complex distributions -results in a significant increase in computational cost, which is O(H 2 ).", "labels": [], "entities": [{"text": "O", "start_pos": 224, "end_pos": 225, "type": "METRIC", "confidence": 0.9894313812255859}]}, {"text": "(2011) increased the performance of a character-level language model with a multiplicative RNN (m-RNN), the factored approximation of a recurrent neural tensor network (RNTN), which maps each symbol to separate hidden layer weights (referred to as recurrence matrices from hereon).", "labels": [], "entities": []}, {"text": "Besides increasing model capacity while keeping computation constant, this approach has another motivation: viewing the RNN's hidden state as being transformed by each new symbol in the sequence, it is intuitive that different symbols will transform the network's hidden state in different ways).", "labels": [], "entities": []}, {"text": "Various studies on compositionality similarly argue that some words are better modeled by matrices than by vectors (.", "labels": [], "entities": []}, {"text": "Unfortunately, having separate recurrence matrices for each symbol requires memory that is linear in the symbol vocabulary size (|V |).", "labels": [], "entities": []}, {"text": "This is not an issue for character-level models, which have small vocabularies, but is prohibitive for word-level models which can have vocabulary size in the millions if we consider surface forms.", "labels": [], "entities": []}, {"text": "In this paper, we propose the Restricted RNTN (r-RNTN) which uses only K < |V | recurrence matrices.", "labels": [], "entities": [{"text": "Restricted RNTN", "start_pos": 30, "end_pos": 45, "type": "METRIC", "confidence": 0.8373092114925385}]}, {"text": "Given that |V | words must be assigned K matrices, we map the most frequent K \u2212 1 words to the first K \u2212 1 matrices, and share the K-th matrix among the remaining words.", "labels": [], "entities": []}, {"text": "This mapping is driven by the statistical intuition that frequent words are more likely to appear in diverse contexts and so require richer modeling, and by the greater presence of predicates and function words among the most frequent words in standard corpora like COCA.", "labels": [], "entities": []}, {"text": "As a result, adding K matrices to the s-RNN both increases model capacity and satisfies the idea that some words are better represented by matrices.", "labels": [], "entities": []}, {"text": "Results show that r-RNTNs improve language model performance over s-RNNs even for small K with no computational overhead, and even for small K approximate the performance of RNTNs using a fraction of the parameters.", "labels": [], "entities": []}, {"text": "We also exper-iment with r-RNTNs using Gated Recurrent Units (GRU) (  and Long Short-Term Memory (LSTM), obtaining lower perplexity for fixed hidden layer sizes.", "labels": [], "entities": []}, {"text": "This paper discusses related work ( \u00a72), and presents r-RNTNs ( \u00a73) along with the evaluation method ( \u00a74).", "labels": [], "entities": []}, {"text": "We conclude with results ( \u00a75), and suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of validation and test set perplexity for r-RNTNs with f mapping (K = 100 for  PTB, K = 376 for text8) versus s-RNNs and m-RNN. r-RNTNs with the same H as corresponding  s-RNNs significantly increase model capacity and performance with no computational cost. The RNTN  was not run on text8 due to the number of parameters required.", "labels": [], "entities": []}]}