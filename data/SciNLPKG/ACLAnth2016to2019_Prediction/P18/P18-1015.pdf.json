{"title": [{"text": "Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization", "labels": [], "entities": [{"text": "Soft Template Based Neural Summarization", "start_pos": 30, "end_pos": 70, "type": "TASK", "confidence": 0.5554304361343384}]}], "abstractContent": [{"text": "Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends to work unstably.", "labels": [], "entities": []}, {"text": "Inspired by the traditional template-based summarization approaches , this paper proposes to use existing summaries as soft templates to guide the seq2seq model.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9051047563552856}]}, {"text": "To this end, we use a popular IR platform to Retrieve proper summaries as candidate templates.", "labels": [], "entities": [{"text": "Retrieve proper summaries", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.8535887400309244}]}, {"text": "Then, we extend the seq2seq framework to jointly conduct template Reranking and template-aware summary generation (Rewriting).", "labels": [], "entities": [{"text": "template-aware summary generation", "start_pos": 80, "end_pos": 113, "type": "TASK", "confidence": 0.5793578426043192}]}, {"text": "Experiments show that, in terms of infor-mativeness, our model significantly out-performs the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness.", "labels": [], "entities": []}, {"text": "In addition, the import of high-quality external summaries improves the stability and readabi-lity of generated summaries.", "labels": [], "entities": []}], "introductionContent": [{"text": "The exponentially growing online information has necessitated the development of effective automatic summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.8828237652778625}]}, {"text": "In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (), which generates a shorter version of a given sentence while attempting to preserve its original meaning.", "labels": [], "entities": [{"text": "abstractive sentence summarization", "start_pos": 66, "end_pos": 100, "type": "TASK", "confidence": 0.5992291470368704}]}, {"text": "It can be used to design or refine appealing headlines.", "labels": [], "entities": [{"text": "design or refine appealing headlines", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.6663967072963715}]}, {"text": "Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task ().", "labels": [], "entities": []}, {"text": "Most previous seq2seq models purely depend on the source text to generate summaries.", "labels": [], "entities": []}, {"text": "However, as reported in many studies, the performance of a seq2seq model deteriorates quickly with the increase of the length of generation.", "labels": [], "entities": []}, {"text": "Our experiments also show that seq2seq models tend to \"lose control\" sometimes.", "labels": [], "entities": []}, {"text": "For example, 3% of summaries contain less than 3 words, while there are 4 summaries repeating a word for even 99 times.", "labels": [], "entities": [{"text": "summaries", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.9733729362487793}]}, {"text": "These results largely reduce the informativeness and readability of the generated summaries.", "labels": [], "entities": []}, {"text": "In addition, we find seq2seq models usually focus on copying source words in order, without any actual \"summarization\".", "labels": [], "entities": []}, {"text": "Therefore, we argue that, the free generation based on the source sentence is not enough fora seq2seq model.", "labels": [], "entities": []}, {"text": "Template based summarization (e.g.,) is a traditional approach to abstractive summarization.", "labels": [], "entities": [{"text": "Template based summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7517409125963846}]}, {"text": "In general, a template is an incomplete sentence which can be filled with the input text using the manually defined rules.", "labels": [], "entities": []}, {"text": "For instance, a concise template to conclude the stock market quotation is: shares [NUMBER] percent [lower/higher], e.g., \"hong kong shares close #.# percent lower\".", "labels": [], "entities": []}, {"text": "Since the templates are written by humans, the produced summaries are usually fluent and informative.", "labels": [], "entities": []}, {"text": "However, the construction of templates is extremely time-consuming and requires aplenty of domain knowledge.", "labels": [], "entities": []}, {"text": "Moreover, it is impossible to develop all templates for summaries in various domains.", "labels": [], "entities": [{"text": "summaries", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.9718865156173706}]}, {"text": "Inspired by retrieve-based conversation systems (), we assume the golden summaries of the similar sentences can provide a reference point to guide the input sentence summarization process.", "labels": [], "entities": []}, {"text": "We call these existing summaries soft templates since no actual rules are nee-ded to build new summaries from them.", "labels": [], "entities": []}, {"text": "Due to the strong rewriting ability of the seq2seq framework (, in this paper, we propose to combine the seq2seq and template based summarization approaches.", "labels": [], "entities": []}, {"text": "We call our summarization system Re 3 Sum, which consists of three modules: Retrieve, Rerank and Rewrite.", "labels": [], "entities": [{"text": "Retrieve", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.8978936076164246}]}, {"text": "We utilize a widely-used Information Retrieval (IR) platform to find out candidate soft templates from the training corpus.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7922110080718994}]}, {"text": "Then, we extend the seq2seq model to jointly learn template saliency measurement (Rerank) and final summary generation (Rewrite).", "labels": [], "entities": []}, {"text": "Specifically, a Recurrent Neural Network (RNN) encoder is applied to convert the input sentence and each candidate template into hidden states.", "labels": [], "entities": []}, {"text": "In Rerank, we measure the informativeness of a candidate template according to its hidden state relevance to the input sentence.", "labels": [], "entities": []}, {"text": "The candidate template with the highest predicted informativeness is regarded as the actual soft template.", "labels": [], "entities": []}, {"text": "In Rewrite, the summary is generated according to the hidden states of both the sentence and template.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments on the popular Gigaword dataset ().", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.9723927676677704}]}, {"text": "Experiments show that, in terms of informativeness, Re 3 Sum significantly outperforms the state-ofthe-art seq2seq models, and even soft templates themselves demonstrate high competitiveness.", "labels": [], "entities": []}, {"text": "In addition, the import of high-quality external summaries improves the stability and readability of generated summaries.", "labels": [], "entities": []}, {"text": "The contributions of this work are summarized as follows: \u2022 We propose to introduce soft templates as additional input to improve the readability and stability of seq2seq summarization systems.", "labels": [], "entities": []}, {"text": "Code and results can be found at http://www4.comp.polyu.", "labels": [], "entities": []}, {"text": "edu.hk/ \u02dc cszqcao/ \u2022 We extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously.", "labels": [], "entities": [{"text": "template-aware summary generation", "start_pos": 87, "end_pos": 120, "type": "TASK", "confidence": 0.5907832582791647}]}, {"text": "\u2022 We fuse the popular IR-based and seq2seq-based summarization systems, which fully utilize the supervisions from both sides.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on the Annotated English Gigaword corpus, as with (.", "labels": [], "entities": [{"text": "Annotated English Gigaword corpus", "start_pos": 30, "end_pos": 63, "type": "DATASET", "confidence": 0.8997589647769928}]}, {"text": "This parallel corpus is produced by pairing the first sentence in the news article and its headline as the summary with heuristic rules.", "labels": [], "entities": []}, {"text": "All the training, development and test datasets can be downloaded at https://github.", "labels": [], "entities": []}, {"text": "com/harvardnlp/sent-summary.", "labels": [], "entities": []}, {"text": "The statistics of the Gigaword corpus is presented in Table 1.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.934457540512085}]}, {"text": "Train AvgSourceLen is the average input sentence length and AvgTargetLen is the average summary length.", "labels": [], "entities": [{"text": "AvgTargetLen", "start_pos": 60, "end_pos": 72, "type": "METRIC", "confidence": 0.9573307633399963}]}, {"text": "COPY means the copy ratio in the summaries (without stopwords).", "labels": [], "entities": [{"text": "COPY", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9462159276008606}]}, {"text": "We adopt ROUGE) for automatic evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9967977404594421}]}, {"text": "ROUGE has been the standard evaluation metric for DUC shared tasks since 2004.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9757218956947327}, {"text": "DUC shared tasks", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7279575069745382}]}, {"text": "It measures the quality of summary by computing the overlapping lexical units between the candidate summary and actual summaries, such as unigram, bi-gram and longest common subsequence (LCS).", "labels": [], "entities": [{"text": "longest common subsequence (LCS)", "start_pos": 159, "end_pos": 191, "type": "METRIC", "confidence": 0.7773067255814871}]}, {"text": "Following the common practice, we report ROUGE-1 (uni-gram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) F1 scores 4 in the following experiments.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9922264814376831}, {"text": "ROUGE-2", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9843259453773499}, {"text": "ROUGE-L (LCS) F1 scores 4", "start_pos": 83, "end_pos": 108, "type": "METRIC", "confidence": 0.9032891392707825}]}, {"text": "We also measure the actual saliency of a candidate template r with its combined ROUGE scores given the actual summary y * : where \"RG\" stands for ROUGE for short.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9894883036613464}, {"text": "RG", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.9517076015472412}]}, {"text": "We also introduce a series of metrics to measure the summary quality from the following aspects: LEN DIF The absolute value of the length difference between the generated summaries and the actual summaries.", "labels": [], "entities": [{"text": "LEN DIF", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.7954916059970856}]}, {"text": "We use mean value \u00b1 standard deviation to illustrate this item.", "labels": [], "entities": []}, {"text": "The average value partially reflects the readability and informativeness, while the standard deviation links to stability.", "labels": [], "entities": []}, {"text": "We also measure the linguistic quality of generated summaries from various aspects, and the results are present in.", "labels": [], "entities": []}, {"text": "As can be seen from the rows \"LEN DIF\" and \"LESS 3\", the performance of Re 3 Sum is almost the same as that of soft templates.", "labels": [], "entities": [{"text": "LEN DIF", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.8148621618747711}, {"text": "LESS 3", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9829159080982208}]}, {"text": "The soft templates indeed well guide the summary generation.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.889664888381958}]}, {"text": "Compared with Source grid positions after the final qualifying session in the indonesian motorcycle grand prix at the sentul circuit , west java , saturday : UNK grid positions for british grand prix OpenNMT circuit Re 3 Sum grid positions for indonesian grand prix Source india 's children are getting increasingly overweight and unhealthy and the government is asking schools to ban junk food , officials said thursday . Target indian government asks schools to ban junk food Template skorean schools to ban soda junk food OpenNMT india 's children getting fatter Re 3 Sum indian schools to ban junk food: Examples of generated summaries.", "labels": [], "entities": []}, {"text": "We use Bold font to indicate the crucial rewriting behavior from the templates to generated summaries.", "labels": [], "entities": []}, {"text": "Re 3 Sum, the standard deviation of LEN DF is 0.7 times larger in OpenNMT, indicating that Open-NMT works quite unstably.", "labels": [], "entities": [{"text": "LEN DF", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.7372073829174042}]}, {"text": "Moreover, OpenNMT generates 53 extreme short summaries, which seriously reduces readability.", "labels": [], "entities": []}, {"text": "Meanwhile, the copy ratio of actual summaries is 36%.", "labels": [], "entities": [{"text": "copy ratio", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9651796221733093}]}, {"text": "Therefore, the copy mechanism is severely overweighted in OpenNMT.", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9168533682823181}]}, {"text": "Our model is encouraged to generate according to human-written soft templates, which relatively diminishes copying from the source sentences.", "labels": [], "entities": []}, {"text": "Look at the last row \"NEW NE\".", "labels": [], "entities": [{"text": "NEW NE\"", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8796681761741638}]}, {"text": "A number of new named entities appear in the soft templates, which makes them quite unfaithful to source sentences.", "labels": [], "entities": []}, {"text": "By contrast, this index in Re 3 Sum is close to the OpenNMT's.", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9377545118331909}]}, {"text": "It highlights the rewriting ability of our seq2seq framework.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics for English Gigaword.", "labels": [], "entities": []}, {"text": " Table 2: Final perplexity on the development set.  \u2020  indicates the value is cited from the corresponding  paper. ABS+, Featseq2seq and Luong-NMT do  not provide this value.", "labels": [], "entities": [{"text": "ABS+", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.9044328331947327}]}, {"text": " Table 3: ROUGE F1 (%) performance. \"RG\" re- presents \"ROUGE\" for short. \"  *  \" indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8045830726623535}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.5088841319084167}, {"text": "RG", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9083046913146973}]}, {"text": " Table 4: ROUGE F1 (%) performance of different  types of soft templates.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9963390827178955}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.876130223274231}]}, {"text": " Table 5: Statistics of different types of summaries.", "labels": [], "entities": []}, {"text": " Table 6: ROUGE F1 (%) performance of Re 3 Sum  generated with different soft templates.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9948099851608276}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8269878029823303}]}]}