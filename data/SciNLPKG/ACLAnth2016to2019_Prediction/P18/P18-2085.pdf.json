{"title": [{"text": "Learning with Structured Representations for Negation Scope Extraction", "labels": [], "entities": [{"text": "Negation Scope Extraction", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7824339667956034}]}], "abstractContent": [{"text": "We report an empirical study on the task of negation scope extraction given the negation cue.", "labels": [], "entities": [{"text": "negation scope extraction", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.9540314674377441}]}, {"text": "Our key observation is that certain useful information such as features related to negation cue, long distance dependencies as well as some latent structural information can be exploited for such a task.", "labels": [], "entities": []}, {"text": "We design approaches based on conditional random fields (CRF), semi-Markov CRF, as well as latent-variable CRF models to capture such information.", "labels": [], "entities": []}, {"text": "Extensive experiments on several standard datasets demonstrate that our approaches are able to achieve better results than existing approaches reported in the literature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Negation is an important linguistic phenomenon, which reverts the assertion associated with a proposition.", "labels": [], "entities": []}, {"text": "Broadly speaking, the part of the sentence being negated is called negation scope ().", "labels": [], "entities": []}, {"text": "Automatic negation scope detection is a vital but challenging task that has various applications in areas such as text mining (, and sentiment analysis ().", "labels": [], "entities": [{"text": "Automatic negation scope detection", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7965507954359055}, {"text": "text mining", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.8678333163261414}, {"text": "sentiment analysis", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.9686021208763123}]}, {"text": "Negation scope detection task commonly involves a negation cue which can be one of the following 3 types -either a single word (e.g., not), affixes (e.g., im-, -less) or multiple words (e.g., no longer) expressing negation.", "labels": [], "entities": [{"text": "Negation scope detection task", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9348408430814743}]}, {"text": "presents two real examples for such a task, where the first example involves discontinuous negation scope of an affix cue.", "labels": [], "entities": []}, {"text": "The second example shows a discontinuous negation cue and its corresponding discontinuous negation scope.", "labels": [], "entities": []}, {"text": "Most existing approaches tackled the negation scope detection problem from a boundary detecHe declares that he heard cries but is unable to state from what direction they came . There is neither money nor credit in it , and yet one would wish to tidy it up . tion perspective, aiming to identify whether each word token in the sentence belongs to the negation scope or not.", "labels": [], "entities": [{"text": "negation scope detection", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.9298359553019205}]}, {"text": "To perform sequence labeling, various approaches have been proposed based on models such as support vector machines (SVMs) (with heuristic rules) (), conditional random fields (CRF) () and neural networks ().", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7247129380702972}]}, {"text": "These models typically either make use of external resources for extracting complex syntax and grammar features, or are based on neural architectures such as long shortterm memory networks (LSTM) and convolutional neural networks (CNNs) to extract automatic features.", "labels": [], "entities": []}, {"text": "We observe that there are some useful features that can be explicitly and implicitly captured and modelled in the learning process for negation scope extraction.", "labels": [], "entities": [{"text": "negation scope extraction", "start_pos": 135, "end_pos": 160, "type": "TASK", "confidence": 0.9325217604637146}]}, {"text": "We use the term partial scope to refer to a continuous text span that is part of discontinuous scope, and use the term gap to refer to the text span between two pieces of partial scope.", "labels": [], "entities": []}, {"text": "From the first example in we can observe that, with the negation cue as a prefix in a word, the partial scope before, after and in the middle of the negation cue differ in terms of composition of words and their associated syntactic roles in the sentence.", "labels": [], "entities": []}, {"text": "Furthermore, the type of cue Figure 2: Label assignments of model variants for the first example mentioned in. as we mentioned earlier may also reveal crucial information for this task.", "labels": [], "entities": []}, {"text": "Moreover, two pieces of partial scope separated by a gap might have some long distance dependencies.", "labels": [], "entities": []}, {"text": "For instance, in the first sentence of, \"He\" as the first partial scope is the subject phrase of the token \"is\" which is the first word of the second partial scope with along gap in between.", "labels": [], "entities": []}, {"text": "Similarly, the second example shows that a discontinuous negation cue involves multiple words, neither and nor, which shows the importance of cue features and some long distance dependencies among text spans.", "labels": [], "entities": []}, {"text": "Furthermore, besides explicit features that we are able to define, we believe there exist some implicit linguistic patterns within the scope fora given negation cue.", "labels": [], "entities": []}, {"text": "While it is possible to manually design linguistic features to extract such patterns, approaches that can automatically capture such implicit patterns in a domain and language independent manner can be more attractive.", "labels": [], "entities": []}, {"text": "How to design models that can effectively capture such features mentioned above remains a research question to be answered.", "labels": [], "entities": []}, {"text": "In the paper, we design different models to capture such useful features based on the above motivations, and report our empirical findings through extensive experiments.", "labels": [], "entities": []}, {"text": "We release our code at http://statnlp.org/research/st.", "labels": [], "entities": []}], "datasetContent": [{"text": "To understand the robustness of our model, we additionally conducted two sets of experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the CDS-CO corpus.", "labels": [], "entities": [{"text": "CDS-CO corpus", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9097127616405487}]}, {"text": " Table 2: Main results on CDS-CO data", "labels": [], "entities": [{"text": "CDS-CO data", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9538757801055908}]}, {"text": " Table 3: Results on BioScope datasets.", "labels": [], "entities": [{"text": "BioScope datasets", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.8517171740531921}]}, {"text": " Table 4: Results on Product Review from CNeSP.", "labels": [], "entities": [{"text": "CNeSP", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.8213731646537781}]}]}