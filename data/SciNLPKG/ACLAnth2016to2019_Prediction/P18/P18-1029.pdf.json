{"title": [{"text": "Zero-shot Learning of Classifiers from Natural Language Quantification", "labels": [], "entities": [{"text": "Zero-shot Learning of Classifiers from Natural Language Quantification", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.5755361281335354}]}], "abstractContent": [{"text": "Humans can efficiently learn new concepts using language.", "labels": [], "entities": []}, {"text": "We present a framework through which a set of explanations of a concept can be used to learn a classifier without access to any labeled examples.", "labels": [], "entities": []}, {"text": "We use semantic parsing to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data, and leverage the differential semantics of linguistic quantifiers (e.g., 'usually' vs 'always') to drive model training.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7560502290725708}]}, {"text": "Experiments on three domains show that the learned classifiers outperform previous approaches for learning with limited data, and are comparable with fully supervised classifiers trained from a small number of labeled examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "As computer systems that interact with us in natural language become pervasive (e.g., Siri, Alexa, Google Home), they suggest the possibility of letting users teach machines in language.", "labels": [], "entities": []}, {"text": "The ability to learn from language can enable a paradigm of ubiquitous machine learning, allowing users to teach personalized concepts (e.g., identifying 'important emails' or 'project-related emails') when limited or no training data is available.", "labels": [], "entities": []}, {"text": "In this paper, we take a step towards solving this problem by exploring the use of quantifiers to train classifiers from declarative language.", "labels": [], "entities": []}, {"text": "For illustration, consider the hypothetical example of a user explaining the concept of an \"important email\" through natural language statements.", "labels": [], "entities": [{"text": "explaining the concept of an \"important email\" through natural language statements", "start_pos": 62, "end_pos": 144, "type": "TASK", "confidence": 0.6617540671275213}]}, {"text": "Our framework takes a set of such natural language explanations describing a concept (e.g., \"emails that I reply to are usually important\") and a set of unlabeled instances as input, and produces: Supervision from language can enable concept learning from limited or even no labeled examples.", "labels": [], "entities": []}, {"text": "Our approach assumes the learner has sensors that can extract attributes from data, such as those listed in the table, and language that can refer to these sensors and their values.", "labels": [], "entities": []}, {"text": "a binary classifier (for important emails) as output.", "labels": [], "entities": []}, {"text": "Our hypothesis is that language describing concepts encodes key properties that can aid statistical learning.", "labels": [], "entities": []}, {"text": "These include specification of relevant attributes (e.g., whether an email was replied to), relationships between such attributes and concept labels (e.g., if a reply implies the class label of that email is 'important'), as well as the strength of these relationships (e.g., via quantifiers like 'often', 'sometimes', 'rarely').", "labels": [], "entities": []}, {"text": "We infer these properties automatically, and use the semantics of linguistic quantifiers to drive the training of classifiers without labeled examples for any concept.", "labels": [], "entities": []}, {"text": "This is a novel scenario, where previous approaches in semi-supervised and constraint-based learning are not directly applicable.", "labels": [], "entities": []}, {"text": "Those approaches require manual pre-specification of expert knowledge for model training.", "labels": [], "entities": []}, {"text": "In our approach, this knowledge is automatically inferred from noisy natural language explanations from a user.", "labels": [], "entities": []}, {"text": "Our approach is summarized in the schematic in.", "labels": [], "entities": []}, {"text": "First, we map the set of natural language explanations of a concept to logical forms that identify the attributes mentioned in the explanation, and describe the information conveyed about the attribute and the concept label as a quantitative constraint.", "labels": [], "entities": []}, {"text": "This mapping is done through semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7624363899230957}]}, {"text": "The logical forms denote quantitative constraints, which are probabilistic assertions about observable attributes of the data and unobserved concept labels.", "labels": [], "entities": []}, {"text": "Here the strength of a constraint is assumed to be specified by a linguistic quantifier (such as 'all', 'some', 'few', etc., which reflect degrees of generality of propositions).", "labels": [], "entities": []}, {"text": "Next, we train a classification model that can assimilate these constraints by adapting the posterior regularization framework (.", "labels": [], "entities": []}, {"text": "Intuitively, this can be seen as defining an optimization problem, where the objective is to find parameter estimates for the classifier that do not simply fit the data, but also agree with the human provided natural language advice to the greatest extent possible.", "labels": [], "entities": []}, {"text": "Since logical forms can be grounded in a variety of sensors and external resources, an explicit model of semantic interpretation conceptually allows the framework to subsume a flexible range of grounding behaviors.", "labels": [], "entities": []}, {"text": "The main contributions of this work are: 1.", "labels": [], "entities": []}, {"text": "We introduce the problem of zero-shot learning of classifiers from language, and present an approach towards this.", "labels": [], "entities": []}, {"text": "2. We develop datasets for zero-shot classification from natural descriptions, exhibiting tasks with various levels of difficulty.", "labels": [], "entities": [{"text": "zero-shot classification", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8606334030628204}]}, {"text": "3. We empirically show that coarse probability estimates to model linguistic quantifiers can effectively supervise model training across three domains of classification tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluating our approach, we created datasets of classification tasks paired with descriptions of the classes, as well as used some existing resources.", "labels": [], "entities": []}, {"text": "In this section, we summarize these steps.", "labels": [], "entities": []}, {"text": "Shapes data: To experiment with our approach in a wider range of controlled settings, part of our evaluation focuses on synthetic concepts.", "labels": [], "entities": []}, {"text": "For this, we created a set of 50 shape classification tasks that exhibit a range of difficulty, and elicited language descriptions spanning a variety of quantifier expressions.", "labels": [], "entities": [{"text": "shape classification tasks", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.7822715044021606}]}, {"text": "The tasks require classifying geometric shapes with a set of predefined attributes (fill color, border, color, shape, size) into two concept-labels (abstractly named 'selected shape', and 'other').", "labels": [], "entities": []}, {"text": "The datasets were created through a generative process, where features xi are conditionally independent given the concept-label.", "labels": [], "entities": []}, {"text": "Each feature's conditional distribution is sampled from asymmetric We sample a total of 50 such datasets, consisting of 100 training and 100 test examples each, where each example is a shape and its assigned label.", "labels": [], "entities": []}, {"text": "For each dataset, we then collected statements from Mechanical Turk workers that describe the concept.", "labels": [], "entities": []}, {"text": "The task required turkers to study a sample of shapes presented on the screen for each of the two concept-labels (see).", "labels": [], "entities": []}, {"text": "They were then asked to write a set of statements that would help others classify these shapes without seeing the data.", "labels": [], "entities": []}, {"text": "In total, 30 workers participated in this task, generating a mean of 4.3 statements per dataset.", "labels": [], "entities": []}, {"text": "Email data: provide a dataset of language explanations from human users describing 7 categories of emails, as well as 1030 examples of emails belonging to those categories.", "labels": [], "entities": [{"text": "Email data", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.819348156452179}]}, {"text": "While this work uses labeled examples, and focuses Shapes: If a shape doesn't have a blue border, it is probably not a selected shape.", "labels": [], "entities": []}, {"text": "Selected shapes occasionally have a yellow fill.", "labels": [], "entities": []}, {"text": "Emails: Emails that mention the word 'meet' in the subject are usually meeting requests Personal reminders almost always have the same recipient and sender Birds: A specimen that has a striped crown is likely to be a selected bird.", "labels": [], "entities": []}, {"text": "Birds in the other category rarely ever have dagger-shaped beaks: Examples of explanations for each domain: Statement generation task for Birds data on mapping natural language explanations (\u223c30 explanations per email category) to compositional feature functions, we can also use statements in their data for evaluating our approach.", "labels": [], "entities": [{"text": "Statement generation", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.8634624481201172}]}, {"text": "While language quantifiers were not studied in the original work, we found about a third of the statements in this data to mention a quantifier.", "labels": [], "entities": []}, {"text": "Birds data: The CUB-200 dataset () contains images of birds annotated with observable attributes such assize, primary color, wing-patterns, etc.", "labels": [], "entities": [{"text": "CUB-200 dataset", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9815912544727325}]}, {"text": "We selected a subset of the data consisting of 10 species of birds and 53 attributes (60 examples per species).", "labels": [], "entities": []}, {"text": "Turkers were shown examples of birds from a species, and negative examples consisting of a mix of birds from other: Classification performance on Shapes datasets (averaged over 50 classification tasks).", "labels": [], "entities": [{"text": "Classification", "start_pos": 116, "end_pos": 130, "type": "TASK", "confidence": 0.966566801071167}]}, {"text": "species, and were asked to describe the classes (similar to the Shapes data, see).", "labels": [], "entities": [{"text": "Shapes data", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.7491549551486969}]}, {"text": "During the task, users also had access to a table enumerating groundable attributes they could refer to.", "labels": [], "entities": []}, {"text": "In all, 60 workers participated, generating 6.1 statements on average.", "labels": [], "entities": []}, {"text": "Incorporating constraints from language has not been addressed before, and hence previous approaches for learning from limited data such as; would not directly work for this setting.", "labels": [], "entities": []}, {"text": "Our baselines hence consist of extended versions of previous approaches that incorporate output from the parser, as well as fully supervised classifiers trained from a small number of labeled examples.", "labels": [], "entities": []}, {"text": "Classification performance: The top section in summarizes performance of various classifiers on the Shape datasets, averaged overall 50 classification tasks.", "labels": [], "entities": [{"text": "Shape datasets", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.8006649613380432}]}, {"text": "FLGE+ refers to a baseline that uses the Feature Labeling through Generalized Expectation criterion, following the approach in;.", "labels": [], "entities": [{"text": "FLGE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8708736896514893}, {"text": "Feature Labeling through Generalized Expectation", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.6993749797344208}]}, {"text": "The approach is based on labeling features are indicating specific class-labels, which corresponds to specifiying constraints of type P (y|x) . While the original approach () sets this value to 0.9, we provide the method the quantitative probabilities used by LNQ.", "labels": [], "entities": [{"text": "LNQ", "start_pos": 260, "end_pos": 263, "type": "DATASET", "confidence": 0.9230251908302307}]}, {"text": "Since the original method cannot handle language descriptions, we also provide the approach the concept label y and feature x as identified by the parser.", "labels": [], "entities": []}, {"text": "FLGE represents the version that is not provided quantifier probabilities.", "labels": [], "entities": [{"text": "FLGE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8909711837768555}]}, {"text": "LR refers to a supervised logistic regression model trained on n = 8 randomly chosen labeled instances.", "labels": [], "entities": []}, {"text": "We note that LNQ performs substantially better than both FLGE+ and LR on average.", "labels": [], "entities": []}, {"text": "This validates our modeling principle for learning classifiers from explanations alone, and also suggests value in our PR-based formulation, which can handle multiple constraint types.", "labels": [], "entities": []}, {"text": "We further note that not using quantifier probabilities significantly deteriorates FLGE's performance.", "labels": [], "entities": [{"text": "FLGE", "start_pos": 83, "end_pos": 87, "type": "TASK", "confidence": 0.5647048354148865}]}, {"text": "provides a more detailed characterization of LNQ's performance.", "labels": [], "entities": []}, {"text": "Each blue dot represents performance on a shape classification task.", "labels": [], "entities": [{"text": "shape classification task", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.8539973894755045}]}, {"text": "The horizontal axis represents the accuracy of the Bayes Optimal classifier, and the vertical represents accuracy of the LNQ approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9994587302207947}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9994446635246277}]}, {"text": "The blue line represents the trajectory for x = y, representing a perfect statistical classifier in the asymptotic case of infinite samples.", "labels": [], "entities": []}, {"text": "We note that LNQ is effective in learning competent classifiers for all levels of hardness.", "labels": [], "entities": []}, {"text": "Secondly, except fora small number of outliers, the approach works especially well for learning easy concepts (towards the right).", "labels": [], "entities": []}, {"text": "From an error-analysis, we found that a majority of these errors are due to problems in parsing (e.g., missed negation, incorrect constraint type) or due to poor explanations from the teacher (bad grammar, or simply incorrect information).", "labels": [], "entities": []}, {"text": "shows results for email classification tasks.", "labels": [], "entities": [{"text": "email classification tasks", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.8279310464859009}]}, {"text": "In the figure, LN* refers to the approach in, which uses natural language descriptions to define compositional features for email classification, but does not incorporate In general, Generalized Expectation can also handle broader constraint types, similar to Posterior Regularization 6 LNQ models are indistinct from LR w.r.t. parametrization, but trained to maximize a different objective.", "labels": [], "entities": [{"text": "email classification", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7224936187267303}]}, {"text": "The choice of n here is arbitrary, but is roughly twice the number of explanations for each task in this domain supervision from quantification.", "labels": [], "entities": []}, {"text": "For this task, we found very few of the natural language descriptions to contain quantifiers for some of the individual email categories, making a direct comparison impractical.", "labels": [], "entities": []}, {"text": "Thus in this case, we evaluate methods by combining supervision from descriptions in addition to 10 labeled examples (also inline with evaluation in the original paper).", "labels": [], "entities": []}, {"text": "We note that additionally incorporating quantification (LNQ) consistently improves classification performance across email categories.", "labels": [], "entities": []}, {"text": "On this task, LNQ improves upon FLGE+ and LN* for 6 of the 7 email categories.", "labels": [], "entities": [{"text": "FLGE", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.5074849128723145}]}, {"text": "shows classification results on the Birds data.", "labels": [], "entities": [{"text": "Birds data", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9873016476631165}]}, {"text": "Here, LR refers to a logistic regression model trained on n=10 examples.", "labels": [], "entities": [{"text": "LR", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.8442061543464661}]}, {"text": "The trends in this case are similar, where LNQ consistently outperforms FLGE+, and is competitive with LR.", "labels": [], "entities": [{"text": "LNQ", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.7201622128486633}, {"text": "FLGE", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.5679539442062378}]}, {"text": "Ablating quantification: From, we further observe that the differential associative strengths of linguistic quantifiers are crucial for our method's classification performance.", "labels": [], "entities": []}, {"text": "LNQ (no quant) refers to a variant that assigns the same probability value (average of values in), irrespective of quantifier.", "labels": [], "entities": []}, {"text": "This yields a near random performance, which is what we'd expect if the learning is being driven by the differential strengths of quantifiers.", "labels": [], "entities": []}, {"text": "LNQ (coarse quant) refers to a variant that rounds assigned quantifier probabilities in to 0 or 1.", "labels": [], "entities": []}, {"text": "(i.e., quantifiers such are rarely get mapped to 0, while always gets mapped to a probability of 1).", "labels": [], "entities": []}, {"text": "While its performance (0.679) suggests that simple binary feedback is a substantial signal, the difference from the full model indicates value in using soft probabilities.", "labels": [], "entities": []}, {"text": "On the other hand, in a sensitivity study, we found the performance of the approach to be robust to small changes in the probability values of quantifiers.", "labels": [], "entities": []}, {"text": "Comparison with human performance: For the Shapes data, we evaluated human teachers' own understanding of concepts they teach by evaluating: Classification performance on Birds data them on a quiz based on predicting labels for examples from the test set (see).", "labels": [], "entities": []}, {"text": "Second, we solicit additional workers that were not exposed to examples from the dataset, and present them only with the statements describing that data (created by a teacher), which is comparable supervision to what LNQ receives.", "labels": [], "entities": []}, {"text": "We then evaluate their performance at the same task.", "labels": [], "entities": []}, {"text": "From, we note that a human teacher's average performance is significantly worse (p < 0.05, Wilcoxon signed-rank test) than the Bayes Optimal classifier indicating that the teacher's own synthesis of concepts is noisy.", "labels": [], "entities": []}, {"text": "The human learner performance is expectedly lower, but interestingly is also significantly worse than LNQ.", "labels": [], "entities": []}, {"text": "While this might be potentially be caused by factors such as user fatigue, this might also suggest that automated methods can be better at reasoning with constraints than humans in certain scenarios.", "labels": [], "entities": []}, {"text": "These results need to be validated through comprehensive experiments in more domains.", "labels": [], "entities": []}, {"text": "Empirical semantics of quantifiers: We can estimate the distributions of probability values for different quantifiers from our labeled data.", "labels": [], "entities": []}, {"text": "For this, we aggregate sentences mentioning a quantifier, and calculate the empirical value of the (conditional) probability associated with the statement, leading to a set of probability values for each quantifier.", "labels": [], "entities": []}, {"text": "shows empirical distributions of probability values for six quantifiers.", "labels": [], "entities": []}, {"text": "We note that while a few estimates (e.g., 'rarely' and 'often') roughly align with pre-registered beliefs, others are somewhat off (e.g., 'likely' shows a much higher value), and yet others (e.g., 'sometimes') show a large spread of values to be meaningfully modeled as point values.", "labels": [], "entities": []}, {"text": "LNQ's performance, inspite of this, shows strong stability in the approach.", "labels": [], "entities": [{"text": "LNQ", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9670203924179077}]}, {"text": "We don't use these empirical probabilities in experiments, (instead of pre-registered values), so as not to tune the hyperparameters to a specific dataset.", "labels": [], "entities": []}, {"text": "Such estimates would not be available fora new task without labeled data.", "labels": [], "entities": []}, {"text": "Further, using labeled data for estimating these probabilities, and then using the learned model for predicting labels would constitute overfitting, biasing evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Probability values we assign to com- mon linguistic quantifiers (hyper-parameters for  method)", "labels": [], "entities": []}]}