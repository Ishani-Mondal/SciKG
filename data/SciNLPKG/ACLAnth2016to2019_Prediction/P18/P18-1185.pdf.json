{"title": [{"text": "Visual Attention Model for Name Tagging in Multimodal Social Media", "labels": [], "entities": [{"text": "Name Tagging", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.8551787436008453}]}], "abstractContent": [{"text": "Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram.", "labels": [], "entities": []}, {"text": "This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites.", "labels": [], "entities": []}, {"text": "This new paradigm brings new challenges for natural language understanding, as the tex-tual component tends to be shorter, more informal, and often is only understood if combined with the visual context.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6525983512401581}]}, {"text": "In this paper, we explore the task of name tagging in multimodal social media posts.", "labels": [], "entities": [{"text": "name tagging in multimodal social media posts", "start_pos": 38, "end_pos": 83, "type": "TASK", "confidence": 0.7486491969653538}]}, {"text": "We start by creating two new multimodal datasets: one based on Twitter posts 1 and the other based on Snapchat captions (ex-clusively submitted to public and crowd-sourced stories).", "labels": [], "entities": []}, {"text": "We then propose a novel model based on Visual Attention that not only provides deeper visual understanding on the decisions of the model, but also significantly outperforms other state-of-the-art baseline methods for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social platforms, like Snapchat, Twitter, Instagram and Pinterest, have become part of our lives and play an important role in making communication easier and accessible.", "labels": [], "entities": []}, {"text": "Once textcentric, social media platforms are becoming in- * * This work was mostly done during the first author's internship at Snap Research.", "labels": [], "entities": [{"text": "Snap Research", "start_pos": 128, "end_pos": 141, "type": "DATASET", "confidence": 0.7590068578720093}]}, {"text": "The Twitter data and associated images presented in this paper were downloaded from https://archive.org/ details/twitterstream 2 We will make the annotations on Twitter data available for research purpose upon request.", "labels": [], "entities": []}, {"text": "creasingly multimodal, with users combining images, videos, audios, and texts for better expressiveness.", "labels": [], "entities": []}, {"text": "As social media posts become more multimodal, the natural language understanding of the textual components of these messages becomes increasingly challenging.", "labels": [], "entities": []}, {"text": "In fact, it is often the case that the textual component can only be understood in combination with the visual context of the message.", "labels": [], "entities": []}, {"text": "In this context, here we study the task of Name Tagging for social media containing both image and textual contents.", "labels": [], "entities": [{"text": "Name Tagging", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.8042930066585541}]}, {"text": "Name tagging is a key task for language understanding, and provides input to several other tasks such as Question Answering, Summarization, Searching and Recommendation.", "labels": [], "entities": [{"text": "Name tagging", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8509831726551056}, {"text": "language understanding", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.808429479598999}, {"text": "Question Answering", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8462485373020172}, {"text": "Summarization", "start_pos": 125, "end_pos": 138, "type": "TASK", "confidence": 0.9622032046318054}]}, {"text": "Despite its importance, most of the research in name tagging has focused on news articles and longer text documents, and not as much in multimodal social media data (.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.8291109800338745}]}, {"text": "However, multimodality is not the only challenge to perform name tagging on such data.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.8103109002113342}]}, {"text": "The textual components of these messages are often very short, which limits context around names.", "labels": [], "entities": []}, {"text": "Moreover, there linguistic variations, slangs, typos and colloquial language are extremely common, such as using 'looooove' for 'love', 'LosAngeles' for 'Los Angeles', and '#Chicago #Bull' for 'Chicago Bulls'.", "labels": [], "entities": []}, {"text": "These characteristics of social media data clearly illustrate the higher difficulty of this task, if compared to traditional newswire name tagging.", "labels": [], "entities": []}, {"text": "In this work, we modify and extend the current state-of-the-art model () in name tagging to incorporate the visual information of social media posts using an Attention mechanism.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.7203182280063629}]}, {"text": "Although the usually short textual components of social media posts provide limited contextual information, the accompanying images often provide rich information that can be useful for name tagging.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 186, "end_pos": 198, "type": "TASK", "confidence": 0.8623305857181549}]}, {"text": "For ex- ample, as shown in, both captions include the phrase 'Modern Baseball'.", "labels": [], "entities": [{"text": "ample", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.6632804274559021}, {"text": "Modern Baseball'", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9609075983365377}]}, {"text": "It is not easy to tell if each Modern Baseball refers to a name or not from the textual evidence only.", "labels": [], "entities": [{"text": "Modern Baseball", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.9625096917152405}]}, {"text": "However using the associated images as reference, we can easily infer that Modern Baseball in the first sentence should be the name of a band because of the implicit features from the objects like instruments and stage, and the Modern Baseball in the second sentence refers to the sport of baseball because of the pitcher in the image.", "labels": [], "entities": []}, {"text": "In this paper, given an image-sentence pair as input, we explore anew approach to leverage visual context for name tagging in text.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 110, "end_pos": 122, "type": "TASK", "confidence": 0.7487994134426117}]}, {"text": "First, we propose an attention-based model to extract visual features from the regions in the image that are most related to the text.", "labels": [], "entities": []}, {"text": "It can ignore irrelevant visual information.", "labels": [], "entities": []}, {"text": "Secondly, we propose to use agate to combine textual features extracted by a Bidirectional Long Short Term Memory (BLSTM) and extracted visual features, before feed them into a Conditional Random Fields(CRF) layer for tag predication.", "labels": [], "entities": [{"text": "tag predication", "start_pos": 218, "end_pos": 233, "type": "TASK", "confidence": 0.7830172777175903}]}, {"text": "The proposed gate architecture plays the role to modulate word-level multimodal features.", "labels": [], "entities": []}, {"text": "We evaluate our model on two labeled datasets collected from Snapchat and Twitter respectively.", "labels": [], "entities": []}, {"text": "Our experimental results show that the proposed model outperforms state-for-the-art name tagger in multimodal social media.", "labels": [], "entities": []}, {"text": "The main contributions of this work are as follows: \u2022 We create two new datasets for name tagging in multimedia data, one using Twitter and the other using crowd-sourced Snapchat posts.", "labels": [], "entities": [{"text": "name tagging in multimedia data", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.7995524227619171}]}, {"text": "These new datasets effectively constitute new benchmarks for the task.", "labels": [], "entities": []}, {"text": "\u2022 We propose a visual attention model specifically for name tagging in multimodal social media data.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 55, "end_pos": 67, "type": "TASK", "confidence": 0.8299594223499298}]}, {"text": "The proposed end-to-end model only uses image-sentence pairs as input without any human designed features, and a Visual Attention component that helps understand the decision making of the model.", "labels": [], "entities": []}, {"text": "shows the overall architecture of our model.", "labels": [], "entities": []}, {"text": "We describe three main components of our model in this section: BLSTM-CRF sequence labeling model (Section 2.1), Visual Attention Model (Section 2.3) and Modulation Gate (Section 2.4).", "labels": [], "entities": [{"text": "BLSTM-CRF sequence labeling", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.64923428495725}]}, {"text": "Given a pair of sentence and image as input, the Visual Attention Model extracts regional visual features from the image and computes the weighted sum of the regional visual features as the visual context vector, based on their relatedness with the sentence.", "labels": [], "entities": []}, {"text": "The BLSTM-CRF sequence labeling model predicts the label for each word in the sentence based on both the visual context vector and the textual information of the words.", "labels": [], "entities": [{"text": "BLSTM-CRF sequence labeling", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6340775887171427}]}, {"text": "The modulation gate controls the combination of the visual context vector and the word representations for each word before the CRF layer.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on two multimodal datasets, which are collected from Twitter and Snapchat respectively.", "labels": [], "entities": []}, {"text": "Both Twitter and Snapchat are social media with plenty of multimodal posts, but they have obvious differences with sentence length and image styles.", "labels": [], "entities": []}, {"text": "In Twitter, text plays a more important role, and the sentences in the Twitter dataset are much longer than those in the Snap dataset (16.0 tokens vs 8.1 tokens).", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.8620537519454956}, {"text": "Snap dataset", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.9426548182964325}]}, {"text": "The image is often more related to the content of the text and added with the purpose of illustrating or giving more context.", "labels": [], "entities": []}, {"text": "On the other hand, as users of Snapchat use cameras to communicate, the roles of text and image are switched.", "labels": [], "entities": []}, {"text": "Captions are often added to complement what is being portrayed by the snap.", "labels": [], "entities": []}, {"text": "On our experiment section we will show that our proposed model outperforms baseline on both datasets.", "labels": [], "entities": []}, {"text": "We believe the Twitter dataset can bean important step towards more research in multimodal name tagging and we plan to provide it as a benchmark upon request.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.8527401387691498}, {"text": "multimodal name tagging", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.6342443327109019}]}], "tableCaptions": [{"text": " Table 1: Sizes of the datasets in numbers of sentence and token.", "labels": [], "entities": []}, {"text": " Table 3: Results of our models on noisy social media data.", "labels": [], "entities": []}]}