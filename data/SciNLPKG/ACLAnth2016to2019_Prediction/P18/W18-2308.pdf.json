{"title": [{"text": "PICO Element Detection in Medical Text via Long Short-Term Memory Neural Networks", "labels": [], "entities": [{"text": "PICO Element Detection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7885363499323527}]}], "abstractContent": [{"text": "Successful evidence-based medicine (EBM) applications rely on answering clinical questions by analyzing large medical literature databases.", "labels": [], "entities": [{"text": "evidence-based medicine (EBM)", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.736495316028595}]}, {"text": "In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O).", "labels": [], "entities": [{"text": "Outcome (O)", "start_pos": 259, "end_pos": 270, "type": "METRIC", "confidence": 0.9450004994869232}]}, {"text": "In this work, we present a Long Short-Term Memory (LSTM) neural network based model to automatically detect PICO elements.", "labels": [], "entities": []}, {"text": "By jointly classifying subsequent sentences in the given text, we achieve state-of-the-art results on PICO element classification compared to several strong baseline models.", "labels": [], "entities": [{"text": "PICO element classification", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.7297480503718058}]}, {"text": "We also make our curated data public as a benchmarking dataset so that the community can benefit from it.", "labels": [], "entities": []}], "introductionContent": [{"text": "The paradigm of evidence-based medicine (EBM) involves the incorporation of current best evidence, such as the reports of randomized controlled trials (RCTs), into decision making for patient care.", "labels": [], "entities": [{"text": "evidence-based medicine (EBM)", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.7578632950782775}]}, {"text": "Such evidence, integrated with the physician's own expertise and patient-specific factors, can lead to better patient outcomes and higher quality healthcare.", "labels": [], "entities": []}, {"text": "In practice, successful EBM applications rely on answering clinical questions via analysis of large medical literature databases such as PubMed.", "labels": [], "entities": [{"text": "EBM", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9713441729545593}, {"text": "PubMed", "start_pos": 137, "end_pos": 143, "type": "DATASET", "confidence": 0.9350686073303223}]}, {"text": "And most often, a PICO framework is used to formulate a well-defined, focused clinical question, which decomposes the question into four parts: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O) (.", "labels": [], "entities": [{"text": "Outcome (O)", "start_pos": 207, "end_pos": 218, "type": "METRIC", "confidence": 0.9274110794067383}]}, {"text": "Typically the analyses that underlie EBM begin by selecting a set of potentially relevant papers, which are then further refined by human judgment to form the evidence base on which the answer to a specific question depends.", "labels": [], "entities": [{"text": "EBM", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.960413932800293}]}, {"text": "To facilitate this selection process, it would be advantageous that all papers (or at least their abstracts) can be organized according to the PICO foci.", "labels": [], "entities": [{"text": "PICO foci", "start_pos": 143, "end_pos": 152, "type": "DATASET", "confidence": 0.8989301025867462}]}, {"text": "Unfortunately, a significant portion of the medical literature contains either unstructured or sub-optimally structured abstracts, without specifically identified PICO elements.", "labels": [], "entities": []}, {"text": "Therefore, we would like to introduce a method to automate the identification of PICO elements in medical abstracts in order to make possible the automated selection of possibly relevant articles fora proposed study.", "labels": [], "entities": [{"text": "identification of PICO elements in medical abstracts", "start_pos": 63, "end_pos": 115, "type": "TASK", "confidence": 0.7115042294774737}]}, {"text": "In this paper, we present a system based on artificial neural networks (ANN) to tackle the issue of extracting PICO elements in medical abstracts as a classification task at the sentence level.", "labels": [], "entities": []}, {"text": "Our key contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "Previous methods for PICO elements extraction focused on shallow models such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF), which are limited in modeling capacity.", "labels": [], "entities": [{"text": "PICO elements extraction", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.9036897023518881}]}, {"text": "To significantly boost the performance, we propose a Long Short-Term Memory (LSTM) based ANN model to solve this task.", "labels": [], "entities": []}, {"text": "2. Most previous systems detected the PICO elements one by one; thus several classifiers needed to be built and trained separately, which is sub-optimal in efficiency.", "labels": [], "entities": []}, {"text": "That approach also cannot take advantage of shared structure among the individual classifiers.", "labels": [], "entities": []}, {"text": "In this work we extract PICO components simultaneously from any given medical abstract.", "labels": [], "entities": []}, {"text": "3. In all previous works, the only dataset used for training and test and made public is from).", "labels": [], "entities": []}, {"text": "However, this dataset contains only 1000 abstracts, which is not enough fora ANN based deep learning model to obtain good generalization results.", "labels": [], "entities": []}, {"text": "Therefore, we curate a dataset comprising of over tens of thousands of abstracts and make it public as a benchmark dataset so that everyone else can use it.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset used in this study  Types: Randomized Controlled Trial (Search conducted on 2017/08/28).", "labels": [], "entities": []}, {"text": "Among them, abstracts with structured section headings were selected for automatic annotation of sentence category.", "labels": [], "entities": []}, {"text": "Although P, I and O headings were our detection targets, we also annotated the other types of sentences into one of the AIM (A), METHOD (M), RESULTS (R) and CONCLUSION (C) labels to facilitate the use of our CRF label sequence optimization method.", "labels": [], "entities": [{"text": "METHOD", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.8601462244987488}, {"text": "RESULTS", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.8542996048927307}, {"text": "CRF label sequence optimization", "start_pos": 208, "end_pos": 239, "type": "TASK", "confidence": 0.5548472478985786}]}, {"text": "Note that, although we have 7 labels in total, we only care about the detection accuracy of the P, I and O labels and thus mainly discuss their performance in the following sections.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.5682517886161804}]}, {"text": "In this study, the C component was incorporated into the I category since the \"COMPARI-SON\" section also refers to a kind of intervention in an RCT.", "labels": [], "entities": []}, {"text": "And in fact, there are very few abstracts with comparison labels found in PubMed.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.9537672400474548}]}, {"text": "We annotated a certain section heading into one of the 7 labels based on whether it contains the key words that belong to the assigned label as shown in (section headings are only used to generate gold labels and not used for model training and inference).", "labels": [], "entities": []}, {"text": "In very rare cases, the section heading of a certain sentence may contain the key words of more than one category, in which case that sentence will be assigned into multi-labels according to presents atypical abstract example with section headings annotated into the 7 labels.", "labels": [], "entities": []}, {"text": "A total of 24,668 abstracts contain at least one of the P/I/O labels.", "labels": [], "entities": []}, {"text": "There are abstracts with P-labels, 13,712 with I-labels and 20,473 with O-labels ().", "labels": [], "entities": []}, {"text": "Note that, the abstracts in PubMed follow a diversity of rhetorical structure and only a small fraction of them contain PICO elements based on their section headings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: A typical abstract example with section headings and their corresponding annotated labels. The  PMID of this abstract is 28449281.", "labels": [], "entities": [{"text": "PMID", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9964321851730347}]}, {"text": " Table 3: Number of times each of the categories P,  I and O appear in abstracts and in sentences in the  data.", "labels": [], "entities": []}, {"text": " Table 4: Hyperparameters. Batch size refers to the  number of abstracts in one batch.", "labels": [], "entities": []}, {"text": " Table 5: Results in terms of precision (p), recall (r)  and F-measure (F1) on the test set for each class  obtained by our model for one of the ten folds.", "labels": [], "entities": [{"text": "precision (p)", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9273952394723892}, {"text": "recall (r)", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9555597603321075}, {"text": "F-measure (F1)", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.9407829791307449}]}, {"text": " Table 6: Confusion matrix obtained by our model  for one of the ten folds. Rows correspond to pre- dicted labels, and columns correspond to true la- bels.", "labels": [], "entities": []}, {"text": " Table 7: Performance in terms of precision (p), recall (r) and F-measure (F1) on the test set with several  baselines and our proposed model (average value based on 10 fold cross validation). Since the dataset  used here was introduced in this work, there is no previously published method for reference.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9989109039306641}, {"text": "recall (r)", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9495308250188828}, {"text": "F-measure (F1)", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.9410616010427475}]}]}