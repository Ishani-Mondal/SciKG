{"title": [{"text": "Leveraging distributed representations and lexico-syntactic fixedness for token-level prediction of the idiomaticity of English verb-noun combinations", "labels": [], "entities": [{"text": "token-level prediction of the idiomaticity of English verb-noun combinations", "start_pos": 74, "end_pos": 150, "type": "TASK", "confidence": 0.7722495396931967}]}], "abstractContent": [{"text": "Verb-noun combinations (VNCs)-e.g., blow the whistle, hit the roof, and see stars-are a common type of English idiom that are ambiguous with literal usages.", "labels": [], "entities": [{"text": "Verb-noun combinations (VNCs)-", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7276827752590179}]}, {"text": "In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming distributed representations.", "labels": [], "entities": []}, {"text": "Our results show that a model based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoughts.", "labels": [], "entities": []}, {"text": "Idiomatic usages of VNCs are known to exhibit lexico-syntactic fixedness.", "labels": [], "entities": []}, {"text": "We further incorporate this information into our models, demonstrating that this rich linguistic knowledge is complementary to the information carried by distributed representations .", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword expressions (MWEs) are combinations of multiple words that exhibit some degree of idiomaticity (.", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7028767049312592}]}, {"text": "Verb-noun combinations (VNCs), consisting of a verb with a noun in its direct object position, area common type of semantically-idiomatic MWE in English and cross-lingually).", "labels": [], "entities": [{"text": "Verb-noun combinations (VNCs)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7062119841575623}]}, {"text": "Many VNCs are ambiguous between MWEs and literal combinations, as in the following examples of see stars, in which 1 is an idiomatic usage (i.e., an MWE), while 2 is a literal combination.", "labels": [], "entities": []}, {"text": "1. Hereford United were seeing stars at Gillingham after letting in 2 early goals 2.", "labels": [], "entities": [{"text": "Gillingham", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8980647325515747}]}, {"text": "Look into the night sky to seethe stars MWE identification is the task of automatically determining which word combinations at the token-level form MWEs (, and must be able to make such distinctions.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9901758134365082}]}, {"text": "This is particularly important for applications such as machine translation (), where the appropriate meaning of word combinations in context must be preserved for accurate translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8345822095870972}]}, {"text": "In this paper, following prior work (e.g.,), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal.", "labels": [], "entities": [{"text": "token-level identification of VNCs", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.6962922215461731}]}, {"text": "We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (, word embeddings tailored to representing sentences (, and skip-thoughts sentence embeddings (.", "labels": [], "entities": []}, {"text": "We then train a support vector machine (SVM) on these representations to classify unseen VNC instances.", "labels": [], "entities": []}, {"text": "Surprisingly, we find that an approach based on representing sentences as the average of their word embeddings performs comparably to, or better than, the skip-thoughts based approach previously proposed by.", "labels": [], "entities": []}, {"text": "For example, the idiomatic interpretation in example 1 above is typically only accessible when the verb see has active voice, the determiner is null, and the noun star is in plural form, as in see stars or seeing stars.", "labels": [], "entities": []}, {"text": "Usages with a determiner (as in example 2), a singular noun (e.g., see a star), or passive voice (e.g., stars were seen) typically only have the literal interpretation.", "labels": [], "entities": []}, {"text": "In this paper we further incorporate knowledge of the lexico-syntactic fixedness of VNCs -automatically acquired from corpora using the method of -into our various embedding-based approaches.", "labels": [], "entities": []}, {"text": "Our experimental results show that this leads to substantial improve-ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss the dataset used in our experiments, and the evaluation of our models.", "labels": [], "entities": []}, {"text": "We use the VNC-Tokens dataset -the same dataset used by and -to train and evaluate our models.", "labels": [], "entities": [{"text": "VNC-Tokens dataset", "start_pos": 11, "end_pos": 29, "type": "DATASET", "confidence": 0.9642975330352783}]}, {"text": "This dataset consists of sentences containing VNC usages drawn from the British National Corpus (Burnard, 2000), along with a label indicating whether the VNC is an idiomatic or literal usage (or whether this cannot be determined, in which case it is labelled \"unknown\").", "labels": [], "entities": [{"text": "British National Corpus (Burnard, 2000)", "start_pos": 72, "end_pos": 111, "type": "DATASET", "confidence": 0.9180251210927963}]}, {"text": "VNC-Tokens is divided into DEV and TEST sets that each include fourteen VNC types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.", "labels": [], "entities": []}, {"text": "Following, we use DEV and TEST, and ignore all token instances annotated as \"unknown\".", "labels": [], "entities": [{"text": "DEV", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8314790725708008}, {"text": "TEST", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.911823570728302}]}, {"text": "Fazly We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as.", "labels": [], "entities": []}, {"text": "This allows us to develop and tune a model on DEV, and then determine whether, when retrained on instances of unseen VNCs in (the training portion of) TEST, that model is able to generalize to new VNCs without further tuning to the specific expressions in TEST.", "labels": [], "entities": [{"text": "DEV", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.8998440504074097}]}, {"text": "The proportion of idiomatic usages in the testing portions of both DEV and TEST is 63%.", "labels": [], "entities": [{"text": "DEV", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9035696983337402}, {"text": "TEST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.4537101984024048}]}, {"text": "We therefore use accuracy to evaluate our models following because the classes are roughly balanced.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994297623634338}]}, {"text": "We randomly divide both DEV and TEST into training and testing portions ten times, following.", "labels": [], "entities": [{"text": "TEST", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.7851644158363342}]}, {"text": "For each of the ten runs, we compute the accuracy for each expression, and then compute the average accuracy over the expressions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9993587136268616}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9963980913162231}]}, {"text": "We then report the average accuracy over the ten runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9816315770149231}]}, {"text": "In this section we first consider the effect of tuning the cost parameter of the SVM for each model on DEV, and then report results on DEV and TEST using the tuned models.", "labels": [], "entities": [{"text": "DEV", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.9527575373649597}, {"text": "DEV", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.941395103931427}, {"text": "TEST", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.8235154747962952}]}], "tableCaptions": [{"text": " Table 1: Accuracy on DEV while tuning the  penalty cost for the SVM for each model. The  highest accuracy for each model is shown in bold- face.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9943022727966309}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9989144802093506}]}, {"text": " Table 1. These results highlight the importance  of choosing an appropriate setting for the penalty  cost. For example, the accuracy of the word2vec  model ranges from 0.619-0.830 depending on the  cost setting. In subsequent experiments, for each", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9994657635688782}]}, {"text": " Table 2: Accuracy on DEV and TEST for each  model, without (\u2212CF) and with (+CF) the canon- ical form feature. The highest accuracy for each  setting on each dataset is shown in boldface.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9903960227966309}, {"text": "DEV", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.7794819474220276}, {"text": "TEST", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.7486495971679688}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9990346431732178}]}, {"text": " Table 3: Precision (P), recall (R), and F1 score (F), for the idiomatic and literal classes, as well as average  F1 score (Ave. F), for TEST.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9541750252246857}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9497708231210709}, {"text": "F1 score (F)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9833795189857483}, {"text": "F1 score (Ave. F)", "start_pos": 114, "end_pos": 131, "type": "METRIC", "confidence": 0.950864334901174}]}]}