{"title": [{"text": "Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8099762499332428}]}], "abstractContent": [{"text": "One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on.", "labels": [], "entities": [{"text": "fine-grained sentiment analysis of product reviews", "start_pos": 16, "end_pos": 66, "type": "TASK", "confidence": 0.8123910327752432}]}, {"text": "This paper fo-cuses on supervised aspect extraction using deep learning.", "labels": [], "entities": [{"text": "supervised aspect extraction", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6152538557847341}]}, {"text": "Unlike other highly sophisticated supervised deep learning models , this paper proposes a novel and yet simple CNN model 1 employing two types of pre-trained embeddings for aspect extraction: general-purpose embeddings and domain-specific embeddings.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.7404784113168716}]}, {"text": "Without using any additional supervision, this model achieves surprisingly good results, outper-forming state-of-the-art sophisticated existing methods.", "labels": [], "entities": []}, {"text": "To our knowledge, this paper is the first to report such double em-beddings based CNN model for aspect extraction and achieve very good results.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8774969279766083}]}], "introductionContent": [{"text": "Aspect extraction is an important task in sentiment analysis () and has many applications (.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9144643843173981}, {"text": "sentiment analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9682415723800659}]}, {"text": "It aims to extract opinion targets (or aspects) from opinion text.", "labels": [], "entities": []}, {"text": "In product reviews, aspects are product attributes or features.", "labels": [], "entities": []}, {"text": "For example, from \"Its speed is incredible\" in a laptop review, it aims to extract \"speed\".", "labels": [], "entities": [{"text": "speed", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.965470016002655}, {"text": "speed", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9881520867347717}]}, {"text": "Aspect extraction has been performed using supervised () and unsupervised approaches (.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8958046734333038}]}, {"text": "Recently, supervised deep learning models achieved state-of-the-art performances (.", "labels": [], "entities": []}, {"text": "Many of these models use handcrafted features, lexicons, and complicated neural network architectures (.", "labels": [], "entities": []}, {"text": "Although these approaches can achieve better performances than their prior works, there are two other considerations that are also important.", "labels": [], "entities": []}, {"text": "(1) Automated feature (representation) learning is always preferred.", "labels": [], "entities": [{"text": "Automated feature (representation) learning", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.576777438322703}]}, {"text": "How to achieve competitive performances without manually crafting features is an important question.", "labels": [], "entities": []}, {"text": "(2) According to Occam's razor principle, a simple model is always preferred over a complex model.", "labels": [], "entities": []}, {"text": "This is especially important when the model is deployed in a real-life application (e.g., chatbot), where a complex model will slowdown the speed of inference.", "labels": [], "entities": []}, {"text": "Thus, to achieve competitive performance whereas keeping the model as simple as possible is important.", "labels": [], "entities": []}, {"text": "This paper proposes such a model.", "labels": [], "entities": []}, {"text": "To address the first consideration, we propose a double embeddings mechanism that is shown crucial for aspect extraction.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.8895711004734039}]}, {"text": "The embedding layer is the very first layer, where all the information about each word is encoded.", "labels": [], "entities": []}, {"text": "The quality of the embeddings determines how easily later layers (e.g., LSTM, CNN or attention) can decode useful information.", "labels": [], "entities": []}, {"text": "Existing deep learning models for aspect extraction use either a pre-trained general-purpose embedding, e.g.,), or a general review embedding (.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8654448986053467}]}, {"text": "However, aspect extraction is a complex task that also requires fine-grained domain embeddings.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8869200050830841}]}, {"text": "For example, in the previous example, detecting \"speed\" may require embeddings of both \"Its\" and \"speed\".", "labels": [], "entities": []}, {"text": "However, the criteria for good embeddings for \"Its\" and \"speed\" can be totally different.", "labels": [], "entities": [{"text": "speed", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.994335949420929}]}, {"text": "\"Its\" is a general word and the general embedding (trained from a large corpus) is likely to have a better representation for \"Its\".", "labels": [], "entities": []}, {"text": "But, \"speed\" has a very fine-grained meaning (e.g., how many instructions per second) in the laptop domain, whereas \"speed\" in general embeddings or general review embeddings may mean how many miles per second.", "labels": [], "entities": [{"text": "speed", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9597440361976624}]}, {"text": "So using in-domain embeddings is important even when the in-domain embedding corpus is not large.", "labels": [], "entities": []}, {"text": "Thus, we leverage both general embeddings and domain embeddings and let the rest of the network to decide which embeddings have more useful information.", "labels": [], "entities": []}, {"text": "To address the second consideration, we use a pure Convolutional Neural Network (CNN) () model for sequence labeling.", "labels": [], "entities": []}, {"text": "Although most existing models use LSTM (Hochreiter and Schmidhuber, 1997) as the core building block to model sequences (, we noticed that CNN is also successful in many NLP tasks.", "labels": [], "entities": []}, {"text": "One major drawback of LSTM is that LSTM cells are sequentially dependent.", "labels": [], "entities": []}, {"text": "The forward pass and backpropagation must serially go through the whole sequence, which slows down the training/testing process 2 . One challenge of applying CNN on sequence labeling is that convolution and max-pooling operations are usually used for summarizing sequential inputs and the outputs are not well-aligned with the inputs.", "labels": [], "entities": [{"text": "summarizing sequential inputs", "start_pos": 251, "end_pos": 280, "type": "TASK", "confidence": 0.8678521513938904}]}, {"text": "We discuss the solutions in Section 3.", "labels": [], "entities": []}, {"text": "We call the proposed model Dual Embeddings CNN (DE-CNN).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first paper that reports a double embedding mechanism and a pure CNN-based sequence labeling model for aspect extraction.", "labels": [], "entities": [{"text": "CNN-based sequence labeling", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.5144673784573873}, {"text": "aspect extraction", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.853585958480835}]}], "datasetContent": [{"text": "Following the experiments of a recent aspect extraction paper (, we conduct experiments on two benchmark datasets from SemEval challenges ( as shown in.1.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7726771533489227}]}, {"text": "The first dataset is from the laptop domain on subtask 1 of SemEval-2014 Task 4.", "labels": [], "entities": []}, {"text": "The second dataset is from the restaurant domain on subtask 1 (slot 2) of SemEval-2016 Task 5.", "labels": [], "entities": [{"text": "SemEval-2016 Task 5", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.5748668909072876}]}, {"text": "These two datasets consist of review sentences with aspect terms labeled as spans of characters.", "labels": [], "entities": []}, {"text": "We use NLTK 3 to tokenize each sentence into a sequence of words.", "labels": [], "entities": []}, {"text": "For the general-purpose embeddings, we use the glove.840B.300d embeddings (), which are pre-trained from a corpus of 840 billion tokens that cover almost all web pages.", "labels": [], "entities": []}, {"text": "We only use reviews from restaurant categories that the second dataset is selected from 5 . We set the embedding dimensions to 100 and the number of iterations to 30 (for a small embedding corpus, embeddings tend to be under-fitted), and keep the rest hyper-parameters as the defaults in fastText.", "labels": [], "entities": []}, {"text": "We further use fastText to compose out-of-vocabulary word embeddings via subword N-gram embeddings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset description with the number of  sentences(#S.) and number of aspect terms(#A.)", "labels": [], "entities": []}, {"text": " Table 2: Comparison results in F 1 score: numbers  in the third group are averaged scores of 5 runs as  in (Li and Lam, 2017). * indicates the result is  statistical significant at the level of 0.05.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9496548175811768}]}, {"text": " Table 4.3 shows that DE-CNN performs the best.  The double embedding mechanism improves the  performance and in-domain embeddings are im- portant. We can see that using general embeddings  (GloVe-CNN) or domain embeddings (Domain- CNN) alone gives inferior performance. We fur- ther notice that the performance on Laptops and  Restaurant domains are quite different. Lap- tops has many domain-specific aspects, such as  \"adapter\". So the domain embeddings for Lap- tops are better than the general embeddings. The  Restaurant domain has many very general aspects  like \"staff\", \"service\" that do not deviate much  from their general meanings. So general embed-", "labels": [], "entities": []}]}