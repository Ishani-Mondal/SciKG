{"title": [{"text": "CRUISE: Cold-Start New Skill Development via Iterative Utterance Generation", "labels": [], "entities": [{"text": "Cold-Start New Skill Development", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.5428773462772369}]}], "abstractContent": [{"text": "We present a system, CRUISE, that guides ordinary software developers to build a high quality natural language understanding (NLU) engine from scratch.", "labels": [], "entities": []}, {"text": "This is the fundamental step of building anew skill for personal assistants.", "labels": [], "entities": []}, {"text": "Unlike existing solutions that require either developers or crowdsourcing to manually generate and annotate a large number of utterances, we design a hybrid rule-based and data-driven approach with the capability to iteratively generate more and more utterances.", "labels": [], "entities": []}, {"text": "Our system only requires light human workload to iteratively prune incorrect utterances.", "labels": [], "entities": []}, {"text": "CRUISE outputs a well trained NLU engine and a large scale annotated utterance corpus that third parties can use to develop their custom skills.", "labels": [], "entities": [{"text": "CRUISE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9061151146888733}]}, {"text": "Using both benchmark dataset and custom datasets we collected in real-world settings, we validate the high quality of CRUISE generated utterances via both competitive NLU performance and human evaluation.", "labels": [], "entities": []}, {"text": "We also show the largely reduced human workload in terms of both cognitive load and human pruning time consumption.", "labels": [], "entities": []}], "introductionContent": [{"text": "Artificially intelligent voice-enabled personal assistants have been emerging in our daily life, such as Alexa, Google Assistant, Siri, Bixby, etc.", "labels": [], "entities": []}, {"text": "Existing off-the-shelf personal assistants provide a large number of capabilities, referred to as skills, and the number of skills keeps growing rapidly.", "labels": [], "entities": []}, {"text": "Thus, it is critically desirable to design an easy to use system that facilitates developers to quickly build high quality new skills.", "labels": [], "entities": []}, {"text": "The key of developing anew skill is to understand all varieties of user utterances and carryout the intent of users, referred to as natural language understanding (NLU) engine.", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 132, "end_pos": 168, "type": "TASK", "confidence": 0.7815932035446167}]}, {"text": "Existing industrial personal assistant products or open source tools (e.g., API.ai, WIT.ai) require software developers themselves or via crowdsourcing to manually input various natural utterances and annotate the slots for each utterance.", "labels": [], "entities": []}, {"text": "Recently, researches have been made to bootstrap the utterance generations.", "labels": [], "entities": []}, {"text": "These approaches first generate canonical utterances based on either lexicon/grammar ( or language/SQL templates; then utilize crowdsourcing to create paraphrases and correct labels.", "labels": [], "entities": []}, {"text": "Unfortunately, they require software developers to have natural language expertise and still heavily rely on costly crowdsourcing.", "labels": [], "entities": []}, {"text": "Thus, it is significantly and crucially desirable to develop a system for helping ordinary developers quickly build a high quality skill for personal assistants.", "labels": [], "entities": []}, {"text": "In this paper, we present a system, called Cold-start iteRative Utterance generatIon for Skill dEvelopment (CRUISE).", "labels": [], "entities": [{"text": "Cold-start iteRative Utterance generatIon", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.5931390449404716}]}, {"text": "As the name suggests, CRUISE aims to guide software developers to build anew skill from scratch, a.k.a., cold-start.", "labels": [], "entities": []}, {"text": "It is defined from two aspects: cold-start software developers which refer to the ordinary developers who do not have either linguistic expertise or complete functionalities of the new skill in mind; and cold-start dataset which means that there is zero or very few training samples available.", "labels": [], "entities": []}, {"text": "Specifically, CRUISE initially takes the list of intents in anew skill as inputs from software developers and runs a hybrid rule-based and datadriven algorithm to automatically generate more and more new utterances for each intent.", "labels": [], "entities": []}, {"text": "During the whole process, software developers only need to iteratively prune the incorrect samples.", "labels": [], "entities": []}, {"text": "As such, CRUISE does not depend on crowdsourcing to conduct the heavy task of manually generating utterances and annotating slots.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implement the CRUISE system with an easyto-use user interface) with the thumb up/down mechanism for efficient human pruning.", "labels": [], "entities": []}, {"text": "We have internal developers to use and evaluate this real system in terms of both utterance quality and human workload.", "labels": [], "entities": []}, {"text": "We validate our CRUISE system by first evaluating the performance of existing NLU engines trained using our generated utterances compared with using benchmark or manually.", "labels": [], "entities": []}, {"text": "Both NLU engines target on classifying the intent of each whole utterance and identifying tags/entities (a.k.a. slot tagging).", "labels": [], "entities": []}, {"text": "Thus, we use the accuracy and F-1 score as the metrics for intent classifier and slot tagging respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9996863603591919}, {"text": "F-1 score", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.985152542591095}, {"text": "slot tagging", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.7124618887901306}]}, {"text": "We run RASA NLU engine using their default parameters.", "labels": [], "entities": [{"text": "RASA NLU engine", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.7973074714342753}]}, {"text": "Benchmark Dataset Evaluation: Although the benchmark NLU trained on crowdsourced data is expected to perform much better than CRUISE NLU trained on machine generated dataset from cold start, we show that CRUISE NLU still achieves a high accuracy and efficiently trades off NLU performance and human workload.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9930896162986755}]}, {"text": "We evaluate our system on the ATIS (Airline Travel Information Systems) dataset (Hemphill unique tagged utterances using CRUISE system.", "labels": [], "entities": [{"text": "ATIS (Airline Travel Information Systems) dataset", "start_pos": 30, "end_pos": 79, "type": "DATASET", "confidence": 0.7435558959841728}]}, {"text": "For a fair comparison, we randomly sample 5,000 utterances from CRUISE dataset as training set.", "labels": [], "entities": [{"text": "CRUISE dataset", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9637719392776489}]}, {"text": "Since ATIS is relatively larger, we select both word embedding and LSTM hidden dimension as 128 with 1 hidden layer in RNN NLU.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.5444068312644958}, {"text": "RNN NLU", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.8469730317592621}]}, {"text": "reports the result of NLU performance comparison.", "labels": [], "entities": []}, {"text": "As one can see, the performance of CRUISE NLU engine is roughly around 10-15% worse than benchmark NLU engine trained on crowdsourced benchmark data for both intent classification and slot tagging.", "labels": [], "entities": [{"text": "intent classification", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.7580508887767792}, {"text": "slot tagging", "start_pos": 184, "end_pos": 196, "type": "TASK", "confidence": 0.7995778322219849}]}, {"text": "After a detailed analysis, we find that CRUISE data has smaller vocabulary size (301 words) than the crowdsourced benchmark data (949 words) due to the selection of high likelihood words in beam search.", "labels": [], "entities": [{"text": "CRUISE data", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.7173255681991577}]}, {"text": "Hence, we attribute a significant cause of errors because of the out-of-vocabulary words in test set.", "labels": [], "entities": []}, {"text": "We further test CRUISE NLU on the subset of test set without out-ofvocabulary words and observe 5-6% improvement of NLU performance.", "labels": [], "entities": []}, {"text": "Importantly, we observe that CRUISE NLU performs much better on more complex utterances, e.g., \"show me fares for round trip flights with first class of delta from miami into houston\", where the benchmark NLU fail for both intent classification and slot tagging.", "labels": [], "entities": [{"text": "intent classification", "start_pos": 223, "end_pos": 244, "type": "TASK", "confidence": 0.7166489213705063}, {"text": "slot tagging", "start_pos": 249, "end_pos": 261, "type": "TASK", "confidence": 0.7580306828022003}]}, {"text": "In addition to CRUISE NLU, we further test the performance of NLU engines which are trained by mixed CRUISE and benchmark datasets, named Mixed NLU.", "labels": [], "entities": []}, {"text": "The benchmark data is treated as the manual entry data from developers such that we can better study another trade-off between additional human workload and NLU engine performance.", "labels": [], "entities": []}, {"text": "reports the result of mixed NLU engine performance with half CRUISE data and half benchmark data.", "labels": [], "entities": [{"text": "CRUISE", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9368665814399719}]}, {"text": "Both the mixed and benchmark NLU engines achieve similar performance for different sizes of training set on the costly crowdsourced ATIS dataset.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.9208878576755524}]}, {"text": "This implies that we can reduce nearly half human workload for developing a skill, given the negligible pruning effort (Section 5.2).", "labels": [], "entities": []}, {"text": "Real-World Setting Evaluation: We further evaluate CRUISE in a simulated real-world scenario when a software developer starts to develop anew skill.", "labels": [], "entities": [{"text": "Real-World Setting Evaluation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8102256457010905}]}, {"text": "In order to do so, we create two custom datasets: (a) Food and (b) Hotel.", "labels": [], "entities": []}, {"text": "Food data has three intents and hotel data has only one intent.", "labels": [], "entities": [{"text": "Food data", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7714226543903351}]}, {"text": "Each intent is associated with six to eight different attributes/tags selected from InfoBox template or provided by internal developers.", "labels": [], "entities": []}, {"text": "For each intent, we ask two developers to generate a list of tagged utterances manually and using our CRUISE system respectively.", "labels": [], "entities": []}, {"text": "The total sizes of human and CRUISE generated utterances are 5,352 and 21,429 in food and hotel datasets respectively.", "labels": [], "entities": []}, {"text": "For fairness, we randomly select a subset from human dataset as a standard test data to test both NLU engines.", "labels": [], "entities": []}, {"text": "shows that CRUISE NLU outperforms human NLU inmost cases.", "labels": [], "entities": []}, {"text": "This is because CRUISE dataset has a larger number and varieties of high quality utterances than human dataset.", "labels": [], "entities": [{"text": "CRUISE dataset", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.8774813413619995}]}, {"text": "We further evaluate the CRUISE dataset subjectively by soliciting judgments from Amazon Mechanical Turkers.", "labels": [], "entities": [{"text": "CRUISE dataset", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.7385284453630447}, {"text": "Amazon Mechanical Turkers", "start_pos": 81, "end_pos": 106, "type": "DATASET", "confidence": 0.9169409076372782}]}, {"text": "Each turker was presented a task of rating utterances sampled from mixed CRUISE and human generated datasets.", "labels": [], "entities": []}, {"text": "Turkers rate each question on a 5 point Likert scale as to whether the utterance is natural and grammatically correct.", "labels": [], "entities": []}, {"text": "Ratings range from 1 (worst) to 5 (best).", "labels": [], "entities": []}, {"text": "Thus, our evaluation provides more detailed rating than what automatic metrics such as BLEU can provide ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9961883425712585}]}, {"text": "In order to control the evaluation quality, we further judge the trustworthiness of each turker by scoring their performance on 20-30 gold-standard utterances that were internally rated by experts.", "labels": [], "entities": []}, {"text": "Based on this trustworthiness score, we establish a group of trusted turkers.", "labels": [], "entities": []}, {"text": "Then, we collect 10 ratings for each utterance from these trusted turkers.", "labels": [], "entities": []}, {"text": "Finally, we compute the average score overall trusted ratings on 300-500 randomly sampled utterances in each dataset.", "labels": [], "entities": []}, {"text": "reports human evaluation results between CRUISE and human generated data.", "labels": [], "entities": []}, {"text": "We observe that CRUISE generated dataset achieves close performance in terms of both metrics in ATIS, which is collected via costly crowdsourcing.", "labels": [], "entities": [{"text": "CRUISE generated dataset", "start_pos": 16, "end_pos": 40, "type": "DATASET", "confidence": 0.6138523916403452}, {"text": "ATIS", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.5139249563217163}]}, {"text": "More importantly, for human data generated by only a single developer in custom datasets, the results show that CRUISE data has better quality than human data in terms of both metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Human NLU engine vs. CRUISE NLU engine  results in benchmark and custom datasets", "labels": [], "entities": []}, {"text": " Table 2: Human Evaluation Results", "labels": [], "entities": []}]}