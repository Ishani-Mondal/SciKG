{"title": [{"text": "Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder", "labels": [], "entities": [{"text": "Interpretable and Compositional Relation Learning", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7552046954631806}]}], "abstractContent": [{"text": "Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base.", "labels": [], "entities": []}, {"text": "Intuitively , a relation can be modeled by a matrix mapping entity vectors.", "labels": [], "entities": []}, {"text": "However , relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices-for one reason, composition of two relations M 1 , M 2 may match a third M 3 (e.g. composition of relations currency of country and country of film usually matches currency of film budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M 1 \u00b7M 2 \u2248 M 3).", "labels": [], "entities": []}, {"text": "In this paper we investigate a dimension reduction technique by training relations jointly with an autoencoder, which is expected to better capture compositional constraints.", "labels": [], "entities": [{"text": "dimension reduction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7477117776870728}]}, {"text": "We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an autoencoder leads to interpretable sparse codings of relations , helps discovering compositional constraints and benefits from compositional training.", "labels": [], "entities": [{"text": "Mean Rank", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9575484097003937}]}, {"text": "Our source code is released at github.com/tianran/glimvec.", "labels": [], "entities": []}], "introductionContent": [{"text": "Broad-coverage knowledge bases (KBs) such as Freebase ( and DBPedia () store a large amount of facts in the form of head entity, relation, tail entity triples (e.g. The Matrix, country of film, Australia), which could support a wide range of reasoning and question answering applications.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9739676713943481}, {"text": "question answering", "start_pos": 256, "end_pos": 274, "type": "TASK", "confidence": 0.7304364889860153}]}, {"text": "The Knowledge Base Completion (KBC) task aims: In joint training, relation parameters (e.g. M 1 ) receive updates from both a KB-learning objective, trying to predict entities in the KB; and a reconstruction objective from an autoencoder, trying to recover relations from low dimension codings.", "labels": [], "entities": []}, {"text": "to predict the missing part of an incomplete triple, such as Finding Nemo, country of film, ?, by reasoning from known facts stored in the KB.", "labels": [], "entities": [{"text": "KB", "start_pos": 139, "end_pos": 141, "type": "DATASET", "confidence": 0.9386026263237}]}, {"text": "As a most common approach (, modeling entities and relations to operate in a low dimension vector space helps KBC, for three conceivable reasons.", "labels": [], "entities": [{"text": "KBC", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.638346254825592}]}, {"text": "First, when dimension is low, entities modeled as vectors are forced to share parameters, so \"similar\" entities which participate in many relations in common get close to each other (e.g. Australia close to US).", "labels": [], "entities": []}, {"text": "This could imply that an entity (e.g. US) \"type matches\" a relation such as country of film.", "labels": [], "entities": []}, {"text": "Second, relations may share parameters as well, which could transfer facts from one relation to other similar relations, for example from x, award winner, y to x, award nominated, y.", "labels": [], "entities": []}, {"text": "Third, spatial positions might be used to implement composition of relations, as relations can be regarded as mappings from head to tail entities, and the composition of two maps can match a third (e.g. the composition of currency of country and country of film matches the relation currency of film budget), which could be captured by modeling composition in a space.", "labels": [], "entities": []}, {"text": "However, modeling relations as mappings naturally requires more parameters -a general linear map between d-dimension vectors is represented by a matrix of d 2 parameters -which are less likely to be shared, impeding transfers of facts between similar relations.", "labels": [], "entities": []}, {"text": "Thus, it is desired to reduce dimensionality of relations; furthermore, the existence of a composition of two relations (assumed to be modeled by matrices M 1 , M 2 ) matching a third (M 3 ) also justifies dimension reduction, because it implies a compositional constraint M 1 \u00b7 M 2 \u2248 M 3 that can be satisfied only by a lower dimension sub-manifold in the parameter space . Previous approaches reduce dimensionality of relations by imposing pre-designed hard constraints on the parameter space, such as constraining that relations are translations () or diagonal matrices ( , or assuming they are linear combinations of a small number of prototypes (.", "labels": [], "entities": []}, {"text": "However, pre-designed hard constraints do not seem to cope well with compositional constraints, because it is difficult to know a priori which two relations compose to which third relation, hence difficult to choose a pre-design; and compositional constraints are not always exact (e.g. the composition of currency of country and headquarter location usually matches business operation currency but not always), so hard constraints are less suited.", "labels": [], "entities": []}, {"text": "In this paper, we investigate an alternative approach by training relation parameters jointly with an autoencoder).", "labels": [], "entities": []}, {"text": "During training, the autoencoder tries to reconstruct relations from low dimension codings, with the reconstruction objective back-propagating to relation parameters as well.", "labels": [], "entities": []}, {"text": "We show this novel technique promotes parameter sharing between different relations, and drives them toward low dimension manifolds (Sec.6.2).", "labels": [], "entities": []}, {"text": "Besides, we expect the technique to cope better with compositional constraints, because it discovers low dimension manifolds posteriorly from data, and it does not impose any explicit hard constraints.", "labels": [], "entities": []}, {"text": "1 It is noteworthy that similar compositional constraints apply to most modeling schemes of relations, not just matrices.", "labels": [], "entities": []}, {"text": "Yet, joint training with an autoencoder is not simple; one has to keep a subtle balance between gradients of the reconstruction and KB-learning objectives throughout the training process.", "labels": [], "entities": []}, {"text": "We are not aware of any theoretical principles directly addressing this problem; but we found some important settings after extensive pre-experiments (Sec.4).", "labels": [], "entities": []}, {"text": "We evaluate our system using standard KBC datasets, achieving state-of-the-art on several of them (Sec.6.1), with strongly improved Mean Rank.", "labels": [], "entities": [{"text": "KBC datasets", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9276960790157318}, {"text": "Mean Rank", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9882549345493317}]}, {"text": "We discuss detailed settings that lead to the performance (Sec.4.1), and we show that joint training with an autoencoder indeed helps discovering compositional constraints (Sec.6.2) and benefits from compositional training (Sec.6.3).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate on standard KBC datasets, including WN18 and FB15k (), WN18RR (Dettmers et al., 2018) and.", "labels": [], "entities": [{"text": "KBC datasets", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9142372012138367}, {"text": "WN18", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.9524276256561279}, {"text": "FB15k", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.38638439774513245}, {"text": "WN18RR", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.9500740766525269}]}, {"text": "The statistical information of these datasets are shown in WN18 collects word relations from WordNet, and FB15k is taken from Freebase (; both have filtered out low frequency entities.", "labels": [], "entities": [{"text": "WN18", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9549344182014465}, {"text": "WordNet", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9711481928825378}, {"text": "FB15k", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9200208187103271}, {"text": "Freebase", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.9628600478172302}]}, {"text": "However, it is reported in  For all datasets, we set the dimension d = 256 and c = 16, the SGD hyper-parameters \u03b7 1 = 1/64, \u03b7 2 = 2 \u221214 and \u03bb 1 = \u03bb 2 = 2 \u221214 . The training batch size is 32 and the triples in each batch share the same head entity.", "labels": [], "entities": []}, {"text": "We compare the base model (BASE) to our joint training with an autoencoder model, and the base model with compositional training (BASE+COMP) to our joint model with compositional training (JOINT+COMP).", "labels": [], "entities": [{"text": "BASE", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9929941296577454}, {"text": "BASE", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9653245806694031}]}, {"text": "When compositional training is enabled (BASE+COMP, JOINT+COMP), we use random walk to sample paths of length 1 + X, where X is drawn from a Poisson distribution of mean \u03bb = 1.0.", "labels": [], "entities": [{"text": "BASE", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9607644081115723}]}, {"text": "For any incomplete triple h, r, ? in KBC test, we calculate a score s(h, r, e) from (1), for every entity e \u2208 E such that h, r, e does not appear in any of the training, validation, or test sets (.", "labels": [], "entities": [{"text": "KBC test", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8410737216472626}]}, {"text": "Then, the calculated scores together with s(h, r, t) for the gold triple is converted to ranks, and the rank of the gold entity t is used for evaluation.", "labels": [], "entities": []}, {"text": "Evaluation metrics include Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits at 10 (H10).", "labels": [], "entities": [{"text": "Mean Rank (MR)", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.9761880755424499}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 43, "end_pos": 69, "type": "METRIC", "confidence": 0.9734785159428915}, {"text": "Hits at 10 (H10)", "start_pos": 75, "end_pos": 91, "type": "METRIC", "confidence": 0.9212637742360433}]}, {"text": "Lower MR, higher MRR, and higher H10 indicate better performance.", "labels": [], "entities": [{"text": "MR", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9996737241744995}, {"text": "MRR", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9987767338752747}, {"text": "H10", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9944288730621338}]}, {"text": "We consult MR and MRR on validation sets to determine training epochs; we stop training when both MR and MRR have stopped improving.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistical information of the KBC datasets.  |E| and |R| denote the number of entities and rela- tion types, respectively; #Train, #Valid, and #Test  are the numbers of triples in the training, validation,  and test sets, respectively.", "labels": [], "entities": [{"text": "KBC datasets", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.9040712714195251}, {"text": "Test", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9241433143615723}]}, {"text": " Table 2: KBC results on the WN18, FB15k, WN18RR, and FB15k-237 datasets. The first and second  sectors compare our joint to the base models with and without compositional training, respectively; the  third sector shows our re-experiments and the fourth shows previous published results. Bold numbers are  the best in each sector, and (  *  ) indicates the best of all.", "labels": [], "entities": [{"text": "WN18", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9722694754600525}, {"text": "FB15k", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.8438155055046082}, {"text": "WN18RR", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.8562328815460205}, {"text": "FB15k-237 datasets", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.9879975616931915}]}, {"text": " Table 3: Performance at discovering compositional  constraints extracted from FB15k-237", "labels": [], "entities": [{"text": "FB15k-237", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.9548425674438477}]}, {"text": " Table 4: Ablation of the four settings of the base  model as described in Sec.4.1", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9918695688247681}, {"text": "Sec.4.1", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.4253925681114197}]}, {"text": " Table 5: Evaluation of BASE and gains by JOINT,  on FB15k-237 with different strengths of composi- tional training. Bold numbers are improvements.", "labels": [], "entities": [{"text": "BASE", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9985824823379517}, {"text": "JOINT", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.7403354644775391}, {"text": "FB15k-237", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.9788110256195068}]}]}