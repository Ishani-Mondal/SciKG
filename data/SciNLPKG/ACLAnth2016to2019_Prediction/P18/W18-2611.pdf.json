{"title": [{"text": "Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9860625863075256}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9985597729682922}]}], "abstractContent": [{"text": "Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU.", "labels": [], "entities": [{"text": "question answering based machine reading comprehension (MRC)", "start_pos": 30, "end_pos": 90, "type": "TASK", "confidence": 0.8288567033078935}, {"text": "ROUGE", "start_pos": 187, "end_pos": 192, "type": "METRIC", "confidence": 0.9900113940238953}, {"text": "BLEU", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9847575426101685}]}, {"text": "However, bias may appear when these metrics are used for specific question types, especially questions inquiring yes-no opinions and entity lists.", "labels": [], "entities": []}, {"text": "In this paper, we make adaptations on the metrics to better correlate n-gram overlap with the human judgment for answers to these two question types.", "labels": [], "entities": []}, {"text": "Statistical analysis proves the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "Our adaptations may provide positive guidance for the development of real-scene MRC systems.", "labels": [], "entities": [{"text": "MRC", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9317348003387451}]}], "introductionContent": [{"text": "The goal of current MRC tasks is to develop agents which are able to comprehend passages automatically and answer open-domain questions correctly.", "labels": [], "entities": [{"text": "MRC tasks", "start_pos": 20, "end_pos": 29, "type": "TASK", "confidence": 0.9255855083465576}]}, {"text": "With the release of several large-scale datasets like SQuAD (, MS-MARCO () and DuReader (, many MRC models have been proposed in previous works ().", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.7621583938598633}, {"text": "DuReader", "start_pos": 79, "end_pos": 87, "type": "DATASET", "confidence": 0.8397761583328247}, {"text": "MRC", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9364394545555115}]}, {"text": "Although MRC model architectures have been intensively studied, the evaluation metrics for them are rarely discussed.", "labels": [], "entities": [{"text": "MRC", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9471240043640137}]}, {"text": "For early cloze-style and multiple choice datasets (), this may not be problematic.", "labels": [], "entities": []}, {"text": "However, considering the trend that the model is required to generate answers and question type is becoming more variable and closer to real cases, we believe the design * This work was done while the first author was doing internship at of evaluation metric is indeed an issue to be focused on.", "labels": [], "entities": []}, {"text": "Currently, the criterion for comparing generated and gold answers is mostly based on lexical overlap.", "labels": [], "entities": []}, {"text": "For example, SQuAD uses exact-match ratio and word-level F1-score, while MS-MARCO and DuReader employ ROUGE-L () and BLEU () which measure ngram consistency or longest common sequence (LCS) length.", "labels": [], "entities": [{"text": "exact-match ratio", "start_pos": 24, "end_pos": 41, "type": "METRIC", "confidence": 0.9800817370414734}, {"text": "F1-score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.7404339909553528}, {"text": "DuReader", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9106977581977844}, {"text": "ROUGE-L", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9858567714691162}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9975097179412842}, {"text": "ngram consistency", "start_pos": 139, "end_pos": 156, "type": "METRIC", "confidence": 0.6696193516254425}]}, {"text": "For some question types, we notice these metrics may not correlate with semantic correspondence well in some cases.", "labels": [], "entities": []}, {"text": "In this paper, we mainly tackle the issue of yes-no and entity questions.", "labels": [], "entities": []}, {"text": "For yes-no questions, overlapbased metrics may ignore the yes-or-no opinion which is more crucial in determining agreement between answers.", "labels": [], "entities": []}, {"text": "Answers with contrary opinions may have high lexical overlap, such as \"The radiation of wireless routers has an impact on people\" and \"The radiation of wireless routers has no impact on people\".", "labels": [], "entities": []}, {"text": "Similarly, for entity questions, we think the agreement should be more reflected by the correctness of entity listing.", "labels": [], "entities": []}, {"text": "Answers which lack or mispredict entities should be in distinction from correct answers, but the mistakes actually affect little in BLEU and ROUGE, especially when the entity is a number.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9989340901374817}, {"text": "ROUGE", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.9937600493431091}]}, {"text": "These two question types are quite common in MRC datasets and real scenario.", "labels": [], "entities": [{"text": "MRC datasets", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8686676323413849}]}, {"text": "As is shown in, 36.2% queries in DuReader and 47.5% in Baidu real search data are classified into these two categories.", "labels": [], "entities": [{"text": "DuReader", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9824824333190918}, {"text": "Baidu real search data", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.8886818587779999}]}, {"text": "For the reasons above, developing an automatic evaluation system which takes consideration of the inherent characteristics of these question types is of great necessity.", "labels": [], "entities": []}, {"text": "In previous work, employed type-specific metrics for evaluating candidate answers in TREC 2007 QA track.", "labels": [], "entities": [{"text": "TREC 2007 QA track", "start_pos": 85, "end_pos": 103, "type": "DATASET", "confidence": 0.9263180643320084}]}, {"text": "Setting the accuracy of yes-no opinion type and F1-score of entity list as extra metrics may solve the problem to some extent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992765784263611}, {"text": "F1-score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9984935522079468}]}, {"text": "However, from the perspective of simplicity and scalability to growing question type category, we hope to design a unified and end-toend evaluation metric which is calculated automatically.", "labels": [], "entities": []}, {"text": "We propose some adaptations for ROUGE and BLEU which provide them awareness to yesno opinion and entity agreement.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.947501003742218}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9958106279373169}]}, {"text": "Compared with original metrics, our modified ROUGE and BLEU achieve higher correlation to human judgment on DuReader samples in both type-specific and overall analysis.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9983514547348022}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9990530610084534}]}, {"text": "Our work is a preliminary exploration of better automatic evaluation systems for MRC model in the real application.", "labels": [], "entities": [{"text": "MRC", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9500187039375305}]}, {"text": "In the remainder of this paper, related work is discussed in section 2.", "labels": [], "entities": []}, {"text": "Then we give details about our adaptation on ROUGE and BLEU in section 3.", "labels": [], "entities": [{"text": "adaptation", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.9363613128662109}, {"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9199085235595703}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9925058484077454}]}, {"text": "Statistical analysis is given in section 4.", "labels": [], "entities": [{"text": "Statistical analysis", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7888568937778473}]}, {"text": "In section 5, we conclude the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pearson correlation coefficients (PCC)  between annotators.", "labels": [], "entities": [{"text": "Pearson correlation coefficients (PCC)", "start_pos": 10, "end_pos": 48, "type": "METRIC", "confidence": 0.9051946103572845}]}, {"text": " Table 2. The adapted  ROUGE-L achieves best performance on correla- tion to human judgment, both on single yes-no or  entity question type and on overall.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.8758201599121094}]}, {"text": " Table 2: PCCs between various automatic metrics  and human judgment for different question types  on single question level.", "labels": [], "entities": []}, {"text": " Table 3: PCCs between various automatic metrics  and human judgment for different question types  on overall score level.", "labels": [], "entities": [{"text": "PCCs", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.8810544013977051}]}]}