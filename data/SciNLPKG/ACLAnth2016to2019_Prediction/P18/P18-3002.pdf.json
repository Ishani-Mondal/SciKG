{"title": [{"text": "Sampling Informative Training Data for RNN Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose an unsupervised importance sampling approach to selecting training data for recurrent neural network (RNN) language models.", "labels": [], "entities": []}, {"text": "To increase the information content of the training set, our approach preferentially samples high per-plexity sentences, as determined by an easily queryable n-gram language model.", "labels": [], "entities": []}, {"text": "We experimentally evaluate the heldout per-plexity of models trained with our various importance sampling distributions.", "labels": [], "entities": []}, {"text": "We show that language models trained on data sampled using our proposed approach outperform models trained over randomly sampled subsets of both the Billion Word (Chelba et al., 2014) and Wikitext-103 benchmark corpora (Merity et al., 2016).", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of statistical language modeling seeks to learn a joint probability distribution over sequences of natural language words.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.7860738635063171}]}, {"text": "In recent work, recurrent neural network (RNN) language models () have produced stateof-the-art perplexities in sentence-level language modeling, far below those of traditional n-gram models (.", "labels": [], "entities": [{"text": "sentence-level language modeling", "start_pos": 112, "end_pos": 144, "type": "TASK", "confidence": 0.6116323173046112}]}, {"text": "Models trained on large, diverse benchmark corpora such as the Billion Word Corpus and Wikitext-103 have seen reported perplexities as low as 23.7 and 37.2, respectively (.", "labels": [], "entities": [{"text": "Billion Word Corpus", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.6618670523166656}]}, {"text": "However, building models on large corpora is limited by prohibitive computational costs, as the number of training steps scales linearly with the number of tokens in the training corpus.", "labels": [], "entities": []}, {"text": "Sentencelevel language models for these large corpora can be learned by training on a set of sentences subsampled from the original corpus.", "labels": [], "entities": []}, {"text": "We seek to determine whether it is possible to select a set of training sentences that is significantly more informative than a randomly drawn training set.", "labels": [], "entities": []}, {"text": "We hypothesize that by training on higher information and more difficult training sentences, RNN language models can learn the language distribution more accurately and produce lower perplexities than models trained on similar-sized randomly sampled training sets.", "labels": [], "entities": []}, {"text": "We propose an unsupervised importance sampling technique for selecting training data for sentence-level RNN language models.", "labels": [], "entities": []}, {"text": "We leverage n-gram language models' rapid training and query time, which often requires just a single pass over the training data.", "labels": [], "entities": []}, {"text": "We determine a preliminary heuristic for each sentence's importance and information content by calculating its average perword perplexity.", "labels": [], "entities": []}, {"text": "Our technique uses an offline ngram model to score sentences and then samples higher perplexity sentences with increased probability.", "labels": [], "entities": []}, {"text": "Selected sentences are then used for training with corrective weights to remove the sampling bias.", "labels": [], "entities": []}, {"text": "As entropy and perplexity have a monotonic relationship, selecting sentences with higher average n-gram perplexity also increases the average entropy and information content.", "labels": [], "entities": []}, {"text": "We experimentally evaluate the effectiveness of multiple importance sampling distributions at selecting training data for RNN language models.", "labels": [], "entities": []}, {"text": "We compare the heldout perplexities of models trained with randomly sampled and importance sampled training data on both the One Billion Word and Wikitext-103 corpora.", "labels": [], "entities": [{"text": "Wikitext-103 corpora", "start_pos": 146, "end_pos": 166, "type": "DATASET", "confidence": 0.8087000548839569}]}, {"text": "We show that our importance sampling techniques yield lower perplexities than models trained on similarly sized random samples.", "labels": [], "entities": []}, {"text": "By using an n-gram model to determine the sampling distribution, we limit added computational costs of our importance sampling approach.", "labels": [], "entities": []}, {"text": "We also find that applying perplexitybased importance sampling requires maintaining a relatively high weight on low perplexity sentences.", "labels": [], "entities": []}, {"text": "We hypothesize that this is because low perplexity sentences frequently contain common subsequences that are useful in modeling other sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimentally evaluate the effectiveness of the Z full and Z 2 sampling methods, as well as the Z \u03b1 method for various values of parameter \u03b1.", "labels": [], "entities": []}, {"text": "Sentence-level models were trained and evaluated on samples from Wikitext-103 and the One Billion Word Benchmark corpus.", "labels": [], "entities": [{"text": "One Billion Word Benchmark corpus", "start_pos": 86, "end_pos": 119, "type": "DATASET", "confidence": 0.5621935427188873}]}, {"text": "To create a dataset of independent sentences, the Wikitext-103 corpus was parsed to remove headers and to create individual sentences.", "labels": [], "entities": [{"text": "Wikitext-103 corpus", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.9390395283699036}]}, {"text": "The training and heldout sets were combined, shuffled, and then split to create new 250k token test and validation sets.", "labels": [], "entities": []}, {"text": "The remaining sequences were set as anew training set of approximately 99 million tokens.", "labels": [], "entities": []}, {"text": "In Billion Word experiments, training sequences were sampled from a 500 million subset of the released training split.", "labels": [], "entities": []}, {"text": "Billion Word models were evaluated on 250k token test and validation sets randomly sampled from the released heldout split.", "labels": [], "entities": []}, {"text": "Models were trained on 500 thousand, 1 million, and 2 million token training sets sampled from each training split.", "labels": [], "entities": []}, {"text": "Rare words were replaced with <unk> tokens, resulting in vocabularies of 267K and 250K for the Wikitext and Billion Word corpora, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexities for Wikitext models. All pro- posed models outperform the random and n-gram  baselines as number of training tokens increases.", "labels": [], "entities": []}, {"text": " Table 2: Perplexities for Billion Word models. Z \u03b1  and Z 2 both outperform the random baseline and  are comparable to the n-gram baseline.", "labels": [], "entities": []}]}