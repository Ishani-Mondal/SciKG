{"title": [{"text": "Characterizing Departures from Linearity in Word Translation", "labels": [], "entities": [{"text": "Word Translation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6894367784261703}]}], "abstractContent": [{"text": "We investigate the behavior of maps learned by machine translation methods.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7005802392959595}]}, {"text": "The maps translate words by projecting between word embedding spaces of different languages.", "labels": [], "entities": []}, {"text": "We locally approximate these maps using linear maps, and find that they vary across the word embedding space.", "labels": [], "entities": []}, {"text": "This demonstrates that the underlying maps are non-linear.", "labels": [], "entities": []}, {"text": "Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained.", "labels": [], "entities": []}, {"text": "Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.", "labels": [], "entities": [{"text": "word translation", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.8232851326465607}]}], "introductionContent": [{"text": "Following the success of monolingual word embeddings), a number of studies have recently explored multilingual word embeddings.", "labels": [], "entities": []}, {"text": "The goal is to learn word vectors such that similar words have similar vector representations regardless of their language (.", "labels": [], "entities": []}, {"text": "Multilingual word embeddings have applications in machine translation, and hold promise for cross-lingual model transfer in NLP tasks such as parsing or part-ofspeech tagging.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7696152329444885}, {"text": "cross-lingual model transfer", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.6460471053918203}, {"text": "part-ofspeech tagging", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.6860724240541458}]}, {"text": "A class of methods has emerged whose core technique is to learn linear maps between vector spaces of different languages ().", "labels": [], "entities": []}, {"text": "These methods work as follows: For a given pair of languages, first, monolingual word vectors are learned independently for each language, and second, under the assumption that word vector spaces exhibit comparable structure across languages, a linear mapping function is learned to connect the two monolingual vector spaces.", "labels": [], "entities": []}, {"text": "The map can then be used to translate words between the language pair.", "labels": [], "entities": []}, {"text": "Both seminal (, and stateof-the-art methods ( found linear maps to substantially outperform specific non-linear maps generated by feedforward neural networks.", "labels": [], "entities": []}, {"text": "Advantages of linear maps include: 1) In settings with limited training data, accurate linear maps can still be learned (.", "labels": [], "entities": []}, {"text": "For example, in unsupervised learning, ( found that using non-linear mapping functions made adversarial training unstable 1 . 2) One can easily impose constraints on the linear maps at training time to ensure that the quality of the monolingual em-beddings is preserved after mapping (.", "labels": [], "entities": []}, {"text": "However, it is not well understood to what extent the assumption of linearity holds and how it affects performance.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the behavior of word translation maps, and show that there is clear evidence of departure from linearity.", "labels": [], "entities": [{"text": "word translation maps", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.8055512011051178}]}, {"text": "Non-linear maps beyond those generated by feedforward neural networks have also been explored for this task (.", "labels": [], "entities": []}, {"text": "However, no attempt was made to characterize the resulting maps.", "labels": [], "entities": []}, {"text": "In this paper, we allow for an underlying mapping function that is non-linear, but assume that it can be approximated by linear maps at least in small enough neighborhoods.", "labels": [], "entities": []}, {"text": "If the underlying map is linear, all local approximations should be identical, or, given the finite size of the training data, similar.", "labels": [], "entities": []}, {"text": "In contrast, if the underlying map is non-linear, the locally linear approximations will depend on the neighborhood.", "labels": [], "entities": []}, {"text": "illustrates the difference between the assumption of a single linear map, and our working hypothesis of locally linear approximations to a non-linear map.", "labels": [], "entities": []}, {"text": "The variation of the linear approximations provides a characterization of the nonlinear map.", "labels": [], "entities": []}, {"text": "We show that the local linear approximations vary across neighborhoods in the embedding space by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained.", "labels": [], "entities": []}, {"text": "The functional form of this variation can be used to test non-linear methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our first experiment we translate from English (en) to German (de).", "labels": [], "entities": []}, {"text": "We obtained pretrained word embeddings from FastText ().", "labels": [], "entities": []}, {"text": "In the first experiment, we follow common practice (, and used the Google Translate API to obtain training and test data.", "labels": [], "entities": []}, {"text": "We make our data available for reproducibility 2 . For the second experiment, we repeat the first experiment, but instead of using Google Translate, we 2 nakashole.com/mt.html use the recently released Facebook AI Research dictionaries 3 for training and test data.", "labels": [], "entities": [{"text": "Facebook AI Research dictionaries 3", "start_pos": 202, "end_pos": 237, "type": "DATASET", "confidence": 0.7829074382781982}]}, {"text": "To test the generality of the findings of the first experiment, the second experiment was performed on a different language pair: English (en) to Swedish (sv).", "labels": [], "entities": []}, {"text": "In our all experiments, the cross-lingual maps are learned using the max-margin loss, which has been shown to perform competitively, while having fast run-times.", "labels": [], "entities": [{"text": "max-margin loss", "start_pos": 69, "end_pos": 84, "type": "METRIC", "confidence": 0.9451951384544373}]}, {"text": "(. The max-margin loss aims to rank correct training data pairs (x i , y i ) higher than incorrect pairs (x i , y j ) with a margin of at least \u03b3.", "labels": [], "entities": [{"text": "max-margin loss", "start_pos": 7, "end_pos": 22, "type": "METRIC", "confidence": 0.953441321849823}]}, {"text": "The margin \u03b3 is a hyper-parameter and the incorrect labels, y j can be selected randomly such that j = i or in a more application specific manner.", "labels": [], "entities": []}, {"text": "In our experiments, we set \u03b3 = 0.4 and randomly selected negative examples, one negative example for each training data point.", "labels": [], "entities": []}, {"text": "Given a seed dictionary as training data of the form , the mapping function is\u02c6W wher\u00ea y i = Wx i is the prediction, k is the number of incorrect examples per training instance, and d(x, y) = (x \u2212 y) 2 is the distance measure.", "labels": [], "entities": []}, {"text": "For the first experiment, we picked the following words as anchor words and obtained maps associated with each of their neighborhoods: For each anchor word, we set s = 0.5, thus the neighborhoods are N (x i , 0.5) where xi is the vector of the anchor word.", "labels": [], "entities": []}, {"text": "The training data for learning each neighborhood-specific linear map consists of vectors in N (x i , 0.5) and their translations.", "labels": [], "entities": []}, {"text": "shows details of the training and test data for each neighborhood.", "labels": [], "entities": []}, {"text": "The words shown in were picked as follows: first we picked the word multivitamins, then we picked the other words to have varying degrees of similarity to it.", "labels": [], "entities": []}, {"text": "The cosine similarity of these words to the word 'multivitamins' are shown in column 3 of.", "labels": [], "entities": []}, {"text": "It is also worth noting that there is nothing special about these words.", "labels": [], "entities": []}, {"text": "In fact, the second experiment: The behavior of word translation maps trained on different neighborhoods ( en \u2192 de translation).", "labels": [], "entities": [{"text": "word translation maps", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7716680467128754}]}, {"text": "Highlighted columns illustrate variations in maps.", "labels": [], "entities": []}, {"text": "Accuracy refers to precision at 10. was carried out on different set of words, and on a different language pair.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9947943091392517}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9991371035575867}]}, {"text": "The main results of our analysis are shown in.", "labels": [], "entities": []}, {"text": "We now analyze the results of in detail.", "labels": [], "entities": []}, {"text": "The 0th column contains the anchor word, xi , around which the neighborhood is formed.", "labels": [], "entities": []}, {"text": "The 1st, and 2nd columns contain the size of the training and test data from N (x i , s = 0.5) where xi is the word vector for the anchor word.", "labels": [], "entities": []}, {"text": "The 3rd column contains the cosine similarity between x 0 , multivitamins, and xi . For example, x 1 (antibiotic) is the most similar to x 0 (0.6), and x 6 , copenhagen, is the least similar to x 0 (0.11).", "labels": [], "entities": []}, {"text": "The 4th column is the translation accuracy of the single global map M , training on data from all xi neighborhoods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9694902300834656}]}, {"text": "The 5th column is the translation accuracy of the map M x 0 , trained on the train- ing data of x 0 , and tested on the test data in xi . We use precision at top-10 as a measure of translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9276688694953918}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8128584623336792}, {"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9951979517936707}, {"text": "translation", "start_pos": 181, "end_pos": 192, "type": "TASK", "confidence": 0.942828893661499}, {"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.7204344272613525}]}, {"text": "Going down this column we can see that accuracy is highest on the test data from the neighborhood anchored at x 0 itself, and lowest on the test data from the neighborhood anchored at x 6 , copenhagen, which is also the furthest word from x 0 . The 6th column is translation accuracy of the map M xi , trained on the training data of the neighborhood anchored at xi , and tested on the test data in xi . We can see that compared to the 5th column, in all cases performance is higher when we apply the map trained on data from the neighborhood, M xi instead of M x 0 . The 7th column shows the difference in translation accuracy of the map M xi and M x 0 . This shows that the more dissimilar the neighborhood anchor word xi is from x 0 according to the cosine similarity shown in the 4rd column, the larger this difference is.  outperform the global map 4th column.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.998916745185852}, {"text": "copenhagen", "start_pos": 190, "end_pos": 200, "type": "METRIC", "confidence": 0.9369598627090454}, {"text": "accuracy", "start_pos": 275, "end_pos": 283, "type": "METRIC", "confidence": 0.7712689638137817}, {"text": "accuracy", "start_pos": 619, "end_pos": 627, "type": "METRIC", "confidence": 0.8654691576957703}]}, {"text": "The 8th column shows the similarity between maps M xi and M x 0 as computed by Equation 2.", "labels": [], "entities": []}, {"text": "This column shows that the similarity between these learned maps is highly correlated with the cosine similarity or distance between the words in 3rd column.", "labels": [], "entities": []}, {"text": "We also see a correlation with the translation accuracy in the 5th column.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9127398729324341}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9166572093963623}]}, {"text": "This correlation is visualized in.", "labels": [], "entities": []}, {"text": "Finally, the 9th column shows the magnitudes of the maps.", "labels": [], "entities": []}, {"text": "The magnitudes vary somewhat between the maps trained on the different neighborhoods, and are significantly different from the magnitude expected for an orthogonal matrix.", "labels": [], "entities": []}, {"text": "(For an orthogonal 300 \u00d7 300 matrix O the norm is ||O|| = \u221a 300 \u2248 17).", "labels": [], "entities": []}, {"text": "In order to determine the generality of our results, we carried out the same experiment on a different language pair, as shown in.", "labels": [], "entities": []}, {"text": "Crucially, we seethe same trends as those observed in.", "labels": [], "entities": []}, {"text": "This supports the generality of our findings.", "labels": [], "entities": []}, {"text": "Our experimental study suggests the following: i) linear maps vary across neighborhoods, implying that the assumption of a linear map does not to hold.", "labels": [], "entities": []}, {"text": "ii) the difference between maps is tightly correlated with the distance between neighborhoods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The behavior of word translation maps trained on different neighborhoods ( en \u2192 de translation).", "labels": [], "entities": [{"text": "word translation maps", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7778268456459045}]}, {"text": " Table 2: Different language pair ( en \u2192 sv) English to Swedish, and different sets of neighborhoods.  Train and test is from the FAIR/MUSE word translation lexicons. Accuracy refers to precision at 10.", "labels": [], "entities": [{"text": "FAIR/MUSE word translation lexicons", "start_pos": 130, "end_pos": 165, "type": "DATASET", "confidence": 0.5171636914213499}, {"text": "Accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9989563226699829}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9992576241493225}]}]}