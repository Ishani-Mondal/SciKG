{"title": [{"text": "Transformation Networks for Target-Oriented Sentiment Classification *", "labels": [], "entities": [{"text": "Target-Oriented Sentiment Classification", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.6791058083375295}]}], "abstractContent": [{"text": "Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence.", "labels": [], "entities": [{"text": "Target-oriented sentiment classification", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7639977733294169}]}, {"text": "RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance.", "labels": [], "entities": []}, {"text": "After reexamining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose anew model to overcome these issues.", "labels": [], "entities": []}, {"text": "Instead of attention, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer.", "labels": [], "entities": []}, {"text": "Between the two layers, we propose a component to generate target-specific representations of words in the sentence, meanwhile incorporate a mechanism for preserving the original contextual information from the RNN layer.", "labels": [], "entities": []}, {"text": "Experiments show that our model achieves anew state-of-the-art performance on a few benchmarks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Target-oriented (also mentioned as \"target-level\" or \"aspect-level\" in some works) sentiment classification aims to determine sentiment polarities over \"opinion targets\" that explicitly appear in the sentences (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.8893868327140808}]}, {"text": "For example, in the sentence \"I am pleased with the fast logon, and the long battery life\", the user mentions two targets * The work was done when Xin Li was an intern at Tencent AI Lab.", "labels": [], "entities": []}, {"text": "This project is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China.", "labels": [], "entities": [{"text": "Hong Kong Special Administrative Region", "start_pos": 90, "end_pos": 129, "type": "DATASET", "confidence": 0.7521740198135376}]}, {"text": "Our code is open-source and available at https:// github.com/lixin4ever/TNet \"log on\" and \"better life\", and expresses positive sentiments over them.", "labels": [], "entities": []}, {"text": "The task is usually formulated as predicting a sentiment category fora (target, sentence) pair.", "labels": [], "entities": [{"text": "predicting a sentiment category fora (target, sentence) pair", "start_pos": 34, "end_pos": 94, "type": "TASK", "confidence": 0.7770429416136309}]}, {"text": "Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (, is the most commonly-used technique for this task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7923408150672913}]}, {"text": "For example,;; ;; and  employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction.", "labels": [], "entities": []}, {"text": "In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9253525733947754}]}, {"text": "For example, in \"This dish is my favorite and I always get it and never get tired of it.\", these approaches tend to involve irrelevant words such as \"never\" and \"tired\" when they highlight the opinion modifier \"favorite\".", "labels": [], "entities": []}, {"text": "To some extent, this drawback is rooted in the attention mechanism, as also observed in machine translation ( and image captioning ( . Another observation is that the sentiment of a target is usually determined by key phrases such as \"is my favorite\".", "labels": [], "entities": [{"text": "machine translation", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7456634938716888}, {"text": "image captioning", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7224480509757996}]}, {"text": "By this token, Convolutional Neural Networks (CNNs)-whose capability for extracting the informative n-gram features (also called \"active local features\") as sentence representations has been verified in)-should be a suitable model for this classification problem.", "labels": [], "entities": []}, {"text": "However, CNN likely fails in cases where a sentence expresses different sentiments over multiple targets, such as \"great food but the service was dreadful!\".", "labels": [], "entities": []}, {"text": "One reason is that CNN cannot fully explore the target information as done by RNN-based meth-ods ().", "labels": [], "entities": []}, {"text": "Moreover, it is hard for vanilla CNN to differentiate opinion words of multiple targets.", "labels": [], "entities": []}, {"text": "Precisely, multiple active local features holding different sentiments (e.g., \"great food\" and \"service was dreadful\") maybe captured fora single target, thus it will hinder the prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 178, "end_pos": 188, "type": "TASK", "confidence": 0.9691389799118042}]}, {"text": "We propose anew architecture, named TargetSpecific Transformation Networks (TNet), to solve the above issues in the task of target sentiment classification.", "labels": [], "entities": [{"text": "target sentiment classification", "start_pos": 124, "end_pos": 155, "type": "TASK", "confidence": 0.718894362449646}]}, {"text": "TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs.", "labels": [], "entities": [{"text": "TNet", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8638848662376404}]}, {"text": "To integrate the target information into the word representations, TNet introduces a novel Target-Specific Transformation (TST) component for generating the target-specific word representations.", "labels": [], "entities": []}, {"text": "Contrary to the previous attention-based approaches which apply the same target representation to determine the attention scores of individual context words, TST firstly generates different representations of the target conditioned on individual context words, then it consolidates each context word with its tailor-made target representation to obtain the transformed word representation.", "labels": [], "entities": [{"text": "TST", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.9207696914672852}]}, {"text": "Considering the context word \"long\" and the target \"battery life\" in the above example, TST firstly measures the associations between \"long\" and individual target words.", "labels": [], "entities": [{"text": "TST", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.800115704536438}]}, {"text": "Then it uses the association scores to generate the target representation conditioned on \"long\".", "labels": [], "entities": []}, {"text": "After that, TST transforms the representation of \"long\" into its target-specific version with the new target representation.", "labels": [], "entities": [{"text": "TST", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7002827525138855}]}, {"text": "Note that \"long\" could also indicate a negative sentiment (say for \"startup time\"), and the above TST is able to differentiate them.", "labels": [], "entities": [{"text": "TST", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.7662097811698914}]}, {"text": "As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST, we design a contextpreserving mechanism to contextualize the generated target-specific word representations.", "labels": [], "entities": []}, {"text": "Such mechanism also allows deep transformation structure to learn abstract features 3 . To help the CNN feature extractor locate sentiment indicators more accurately, we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target.", "labels": [], "entities": [{"text": "CNN feature extractor locate sentiment indicators", "start_pos": 100, "end_pos": 149, "type": "TASK", "confidence": 0.7097478359937668}]}, {"text": "One method could be concatenating the target representation with each word representation, but the effect as shown in () is limited.", "labels": [], "entities": []}, {"text": "3 Abstract features usually refer to the features ultimately useful for the task ( In summary, our contributions are as follows: \u2022 TNet adapts CNN to handle target-level sentiment classification, and its performance dominates the state-of-the-art models on benchmark datasets.", "labels": [], "entities": [{"text": "Abstract", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.978339433670044}, {"text": "target-level sentiment classification", "start_pos": 157, "end_pos": 194, "type": "TASK", "confidence": 0.6311869323253632}]}, {"text": "\u2022 A novel Target-Specific Transformation component is proposed to better integrate target information into the word representations.", "labels": [], "entities": []}, {"text": "\u2022 A context-preserving mechanism is designed to forward the context information into a deep transformation architecture, thus, the model can learn more abstract contextualized word features from deeper networks.", "labels": [], "entities": []}], "datasetContent": [{"text": "As shown in, we evaluate the proposed TNet on three benchmark datasets: LAPTOP and REST are from SemEval ABSA challenge), containing user reviews in laptop domain and restaurant domain respectively.", "labels": [], "entities": [{"text": "LAPTOP", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.987708330154419}, {"text": "REST", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9941325783729553}, {"text": "SemEval ABSA challenge", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.5333941876888275}]}, {"text": "We also remove a few examples having the \"conflict label\" as done in ); TWITTER is built by, containing twitter posts.", "labels": [], "entities": []}, {"text": "All tokens are lowercased without removal of stop words, symbols or digits, and sentences are zero-padded to the length of the longest sentence in the dataset.", "labels": [], "entities": []}, {"text": "Evaluation metrics are Accuracy and Macro-Averaged F1 where the latter is more appropriate for datasets with unbalanced classes.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9993188381195068}, {"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.8007203936576843}]}, {"text": "We also conduct pairwise t-test on both Accuracy and Macro-Averaged F1 to verify if the improvements over the compared models are reliable.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9975225329399109}, {"text": "F1", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.8024460673332214}]}, {"text": "TNet is compared with the following methods.", "labels": [], "entities": [{"text": "TNet", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6739822030067444}]}, {"text": "\u2022 SVM (): It is a traditional support vector machine based model with extensive feature engineering; \u2022 AdaRNN (): It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree; \u2022 AE-LSTM, and ATAE-LSTM (: AE-LSTM is a simple LSTM model incorporating the target embedding as input, while ATAE-LSTM extends AE-LSTM with attention; \u2022 IAN (Ma et al., 2017): IAN employs two LSTMs to learn the representations of the context and the target phrase interactively; \u2022 CNN-ASP: It is a CNN-based model implemented by us which directly concatenates target representation to each word embedding; \u2022 based on the top-most sentence representations; \u2022 BILSTM-ATT-G (Liu and Zhang, 2017): It models left and right contexts using two attention-based LSTMs and introduces gates to measure the importance of left context, right context, and the entire sentence for the prediction; \u2022 RAM (Chen et al., 2017): RAM is a multilayer architecture where each layer consists of attention-based aggregation of word features and a GRU cell to learn the sentence representation.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 170, "end_pos": 190, "type": "TASK", "confidence": 0.8790761828422546}, {"text": "AE-LSTM", "start_pos": 240, "end_pos": 247, "type": "METRIC", "confidence": 0.901306688785553}, {"text": "BILSTM-ATT-G", "start_pos": 697, "end_pos": 709, "type": "METRIC", "confidence": 0.9829440712928772}]}, {"text": "We run the released codes of TD-LSTM and BILSTM-ATT-G to generate results, since their papers only reported results on TWITTER.", "labels": [], "entities": [{"text": "BILSTM-ATT-G", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9967904686927795}]}, {"text": "We also rerun MemNet on our datasets and evaluate it with both accuracy and Macro-Averaged F1.", "labels": [], "entities": [{"text": "MemNet", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.9169758558273315}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9997649788856506}, {"text": "F1", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.798584520816803}]}, {"text": "We use pre-trained GloVe vectors) to initialize the word embeddings and the dimension is 300 (i.e., dim w = 300).", "labels": [], "entities": []}, {"text": "For out-of-vocabulary words, we randomly sample their embeddings from the uniform distribution U(\u22120.25, 0.25), as done in.", "labels": [], "entities": []}, {"text": "We only use one convolutional kernel size because it was observed that CNN with single optimal kernel size is comparable with CNN having multiple kernel sizes on small datasets (.", "labels": [], "entities": []}, {"text": "To alleviate overfitting, we apply dropout on the input word embeddings of the LSTM and the ultimate sentence representation z.", "labels": [], "entities": []}, {"text": "All weight matrices are initialized with the uniform distribution U(\u22120.01, 0.01) and the biases are initialized as zeros.", "labels": [], "entities": []}, {"text": "The training objective is cross-entropy, and Adam () is adopted as the optimizer by following the learning rate and the decay rates in the original paper.", "labels": [], "entities": []}, {"text": "The hyper-parameters of TNet-LF and TNet-AS are listed in.", "labels": [], "entities": [{"text": "TNet-LF", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.8909221887588501}, {"text": "TNet-AS", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8930251598358154}]}, {"text": "Specifically, all hyperparameters are tuned on 20% randomly held-out training data and the hyper-parameter collection producing the highest accuracy score is used for testing.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 140, "end_pos": 154, "type": "METRIC", "confidence": 0.9779903888702393}]}, {"text": "Our model has comparable number of parameters compared to traditional LSTM-based models as we reuse parameters in the transformation layers and BiLSTM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of datasets.", "labels": [], "entities": []}, {"text": " Table 2: Settings of hyper-parameters.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results (%). The results with symbol\"\" are retrieved from the original papers, and  those starred ( * ) one are from", "labels": [], "entities": []}]}