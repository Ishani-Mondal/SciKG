{"title": [{"text": "Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network", "labels": [], "entities": []}], "abstractContent": [{"text": "Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context.", "labels": [], "entities": []}, {"text": "In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention.", "labels": [], "entities": []}, {"text": "Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7309209108352661}]}, {"text": "First, we construct representations of text segments at different granularities solely with stacked self-attention.", "labels": [], "entities": []}, {"text": "Second, we try to extract the truly matched segment pairs with attention across the context and response.", "labels": [], "entities": []}, {"text": "We jointly introduce those two kinds of attention in one uniform neural network.", "labels": [], "entities": []}, {"text": "Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.", "labels": [], "entities": [{"text": "multi-turn response selection tasks", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.6956004649400711}]}], "introductionContent": [{"text": "Building a chatbot that can naturally and consistently converse with human-beings on opendomain topics draws increasing research interests in past years.", "labels": [], "entities": []}, {"text": "One important task in chatbots is response selection, which aims to select the bestmatched response from a set of candidates given the context of a conversation.", "labels": [], "entities": [{"text": "response selection", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7809019982814789}]}, {"text": "Besides playing a critical role in retrieval-based chatbots (, response selection models have been used in automatic evaluation of dialogue generation * Equally contributed.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7638347744941711}]}, {"text": "\u2020 Work done as a visiting scholar at Baidu.", "labels": [], "entities": [{"text": "Baidu", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8936808705329895}]}, {"text": "Wayne Xin Zhao is an associate professor of Renmin University of China and can be reached at batmanfly@ruc.edu.cn.", "labels": [], "entities": []}, {"text": "( and the discriminator of GANbased (Generative Adversarial Networks) neural dialogue generation ( ).", "labels": [], "entities": [{"text": "GANbased (Generative Adversarial Networks) neural dialogue generation", "start_pos": 27, "end_pos": 96, "type": "TASK", "confidence": 0.5452947550349765}]}], "datasetContent": [{"text": "We use the same evaluation metrics as in previous works (.", "labels": [], "entities": []}, {"text": "Each comparison model is asked to select k best-matched response from n available candidates for the given conversation context c, and we calculate the recall of the true positive replies among the k selected ones as the main evaluation metric, denoted as , where y i is the binary label for each candidate.", "labels": [], "entities": [{"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9985548853874207}]}, {"text": "In addition to Rn @k, we use MAP (Mean Average Precision) (Baeza-3 https://lucenent.apache.org/), MRR (Mean Reciprocal Rank) (), and Precision-at-one P @1 especially for Douban corpus, following the setting of previous works (Wu et al., 2017).", "labels": [], "entities": [{"text": "MAP (Mean Average Precision)", "start_pos": 29, "end_pos": 57, "type": "METRIC", "confidence": 0.8189621567726135}, {"text": "MRR (Mean Reciprocal Rank)", "start_pos": 98, "end_pos": 124, "type": "METRIC", "confidence": 0.8053259054819742}, {"text": "Precision-at-one P @1", "start_pos": 133, "end_pos": 154, "type": "METRIC", "confidence": 0.8812766373157501}, {"text": "Douban corpus", "start_pos": 170, "end_pos": 183, "type": "DATASET", "confidence": 0.9105395376682281}]}, {"text": "Ablation : To verify the effects of multi-grained representation, we setup two comparison models, i.e., DAM first and DAM last , which dispense with the multi-grained representations in DAM, and use representation results from the 0 th layer and L th layer of self-attention instead.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9742952585220337}]}, {"text": "Moreover, we setup DAM self and DAM cross , which only use self-attention-match or cross-attention-match respectively, in order to examine the effectiveness of both self-attention-match and cross-attention-match.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results of DAM and other comparison approaches on Ubuntu Corpus V1 and  Douban Conversation Corpus.", "labels": [], "entities": [{"text": "Ubuntu Corpus V1", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.9384589791297913}, {"text": "Douban Conversation Corpus", "start_pos": 95, "end_pos": 121, "type": "DATASET", "confidence": 0.8191472093264262}]}]}