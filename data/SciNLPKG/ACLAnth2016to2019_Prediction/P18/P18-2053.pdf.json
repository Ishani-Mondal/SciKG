{"title": [{"text": "Bag-of-Words as Target for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.731523871421814}]}], "abstractContent": [{"text": "A sentence can be translated into more than one correct sentences.", "labels": [], "entities": []}, {"text": "However, most of the existing neural machine translation models only use one of the correct translations as the targets, and the other correct sentences are punished as the incorrect sentences in the training stage.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.7954960266749064}]}, {"text": "Since most of the correct translations for one sentence share the similar bag-of-words, it is possible to distinguish the correct translations from the incorrect ones by the bag-of-words.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach that uses both the sentences and the bag-of-words as targets in the training stage, in order to encourage the model to generate the potentially correct sentences that are not appeared in the training set.", "labels": [], "entities": []}, {"text": "We evaluate our model on a Chinese-English translation dataset, and experiments show our model outperforms the strong baselines by the BLEU score of 4.55.", "labels": [], "entities": [{"text": "Chinese-English translation dataset", "start_pos": 27, "end_pos": 62, "type": "DATASET", "confidence": 0.5876321792602539}, {"text": "BLEU score", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.976556658744812}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has achieve success in generating coherent and reasonable translations.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7846718192100525}]}, {"text": "Most of the existing neural machine translation systems are based on the sequenceto-sequence model).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.7940273880958557}]}, {"text": "The sequence-to-sequence model (Seq2Seq) regards the translation problem as the mapping from the source sequences to the target sequences.", "labels": [], "entities": []}, {"text": "The encoder of Seq2Seq compresses the source sentences into the latent representation, and the decoder of Seq2Seq generates the target sentences from the source representations.", "labels": [], "entities": []}, {"text": "The cross-entropy loss, Source: 37.6 Reference: Export of high -tech products in guangdong in first two months this year reached 3.76 billion us dollars . Translation 1: Guangdong 's export of new high technology products amounts to us $3.76 billion in first two months of this year . Translation 2: Export of high -tech products has frequently been in the spotlight , making a significant contribution to the growth of foreign trade in guangdong .: An example of two generated translations.", "labels": [], "entities": []}, {"text": "Although Translation 1 is much more reasonable, it is punished more severely than Translation 2 by Seq2Seq.", "labels": [], "entities": []}, {"text": "which measures the distance of the generated distribution and the target distribution, is minimized in the training stage, so that the generated sentences are as similar as the target sentences.", "labels": [], "entities": []}, {"text": "Due to the limitation of the training set, most of the existing neural machine translation models only have one reference sentences as the targets.", "labels": [], "entities": []}, {"text": "However, a sentence can be translated into more than one correct sentences, which have different syntax structures and expressions but share the same meaning.", "labels": [], "entities": []}, {"text": "The correct translations that are not appeared in the training set will be punished as the incorrect translation by Seq2Seq, which is a potential harm to the model.", "labels": [], "entities": []}, {"text": "shows an example of two generated translations from Chinese to English.", "labels": [], "entities": []}, {"text": "Translation 1 is apparently more proper as the translation of the source sentence than Translation 2, but it is punished even more severely than Translation 2 by Seq2Seq.", "labels": [], "entities": []}, {"text": "Because most of the correct translations for one source sentence share the similar bag-of-words, it is possible to distinguish the correct translations from the incorrect ones by the bag-of-words inmost cases.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach that uses both sentences and bag-of-words as the targets.", "labels": [], "entities": []}, {"text": "In this way, the generated sentences which cover more words in the bag-of-words (e.g. Translation 1 in) are encouraged, while the incorrect sentences (e.g. Translation 2) are punished more severely.", "labels": [], "entities": []}, {"text": "We perform experiments on a popular Chinese-English translation dataset.", "labels": [], "entities": []}, {"text": "Experiments show our model outperforms the strong baselines by the BLEU score of 4.55.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9830910563468933}]}], "datasetContent": [{"text": "This section introduces the details of our experiments, including datasets, setups, baseline models as well as results.", "labels": [], "entities": []}, {"text": "We evaluated our proposed model on the NIST translation task for Chinese-English translation and provided the analysis on the same task.", "labels": [], "entities": [{"text": "NIST translation task", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7530381679534912}, {"text": "Chinese-English translation", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.6574411690235138}]}, {"text": "We trained our model on 1.25M sentence pairs extracted from LDC corpora 2 , with 27.9M Chinese words and 34.5M English words.", "labels": [], "entities": []}, {"text": "We validated our model on the dataset for the NIST 2002 translation task and tested our model on that for the, 2008 translation tasks.", "labels": [], "entities": [{"text": "NIST 2002 translation task", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6877885982394218}, {"text": "translation tasks", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7914034128189087}]}, {"text": "We used the most frequent 50,000 words for both the Chinese vocabulary and the English vocabulary.", "labels": [], "entities": []}, {"text": "The evaluation metric is BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.999068558216095}]}], "tableCaptions": [{"text": " Table 2: Results of our model and the baselines (directly reported in the referred articles) on the Chinese- English translation. \"-\" means that the studies did not test the models on the corresponding datasets.", "labels": [], "entities": []}]}