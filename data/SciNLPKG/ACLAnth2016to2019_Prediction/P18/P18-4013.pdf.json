{"title": [{"text": "NCRF++: An Open-source Neural Sequence Labeling Toolkit", "labels": [], "entities": [{"text": "Neural Sequence Labeling Toolkit", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.7452540695667267}]}], "abstractContent": [{"text": "This paper describes NCRF++, a toolkit for neural sequence labeling.", "labels": [], "entities": [{"text": "neural sequence labeling", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6138500571250916}]}, {"text": "NCRF++ is designed for quick implementation of different neural sequence labeling models with a CRF inference layer.", "labels": [], "entities": [{"text": "NCRF++", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9190785884857178}]}, {"text": "It provides users with an inference for building the custom model structure through configuration file with flexible neural feature design and utilization.", "labels": [], "entities": []}, {"text": "Built on PyTorch 1 , the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU.", "labels": [], "entities": []}, {"text": "It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence labeling is one of the most fundamental NLP models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9112400114536285}, {"text": "named entity recognition (NER)", "start_pos": 98, "end_pos": 128, "type": "TASK", "confidence": 0.7979967445135117}, {"text": "word segmentation", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.7633284330368042}, {"text": "part-of-speech (POS) tagging", "start_pos": 162, "end_pos": 190, "type": "TASK", "confidence": 0.6578383088111878}]}, {"text": "It has been traditionally investigated using statistical approaches (, where conditional random fields (CRF) () has been proven as an effective framework, by taking discrete features as the representation of input sequence.", "labels": [], "entities": []}, {"text": "With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (.", "labels": [], "entities": [{"text": "neural sequence labeling", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6585373878479004}]}, {"text": "Features are extracted automatically through network structures including long short-term memory (LSTM)) and convolution neural network (CNN) (),  with distributed word representations.", "labels": [], "entities": []}, {"text": "Similar to discrete models, a CRF layer is used in many state-of-the-art neural sequence labeling models for capturing label dependencies.", "labels": [], "entities": []}, {"text": "There exist several open-source statistical CRF sequence labeling toolkits, such as CRF++ 2 , CRFSuite ( and), which provide users with flexible means of feature extraction, various training settings and decoding formats, facilitating quick implementation and extension on state-of-the-art models.", "labels": [], "entities": [{"text": "CRF sequence labeling", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7179149786631266}, {"text": "feature extraction", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7448433935642242}]}, {"text": "On the other hand, there is limited choice for neural sequence labeling toolkits.", "labels": [], "entities": [{"text": "neural sequence labeling", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.6033804913361868}]}, {"text": "Although many authors released their code along with their sequence labeling papers (, the implementations are mostly focused on specific model structures and specific tasks.", "labels": [], "entities": []}, {"text": "Modifying or extending can need enormous coding.", "labels": [], "entities": []}, {"text": "In this paper, we present Neural CRF++ (NCRF++) 3 , a neural sequence labeling toolkit based on PyTorch, which is designed for solving general sequence labeling tasks with effective and efficient neural models.", "labels": [], "entities": []}, {"text": "It can be regarded as the neural version of CRF++, with both take the CoNLL data format as input and can add hand-  crafted features to CRF framework conveniently.", "labels": [], "entities": [{"text": "CoNLL data format", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.9375013907750448}]}, {"text": "We take the layerwise implementation, which includes character sequence layer, word sequence layer and inference layer.", "labels": [], "entities": []}, {"text": "NCRF++ is: \u2022 Fully configurable: users can design their neural models only through a configuration file without any code work.", "labels": [], "entities": [{"text": "NCRF", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9249491691589355}]}, {"text": "shows a segment of the configuration file.", "labels": [], "entities": []}, {"text": "It builds a LSTM-CRF framework with CNN to encode character sequence (the same structure as), plus POS and Cap features, within 10 lines.", "labels": [], "entities": [{"text": "Cap", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.6729691624641418}]}, {"text": "This demonstrates the convenience of designing neural models using NCRF++.", "labels": [], "entities": []}, {"text": "\u2022 Flexible with features: human-defined features have been proved useful in neural sequence labeling.", "labels": [], "entities": [{"text": "neural sequence labeling", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.6404038866360983}]}, {"text": "Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in).", "labels": [], "entities": []}, {"text": "In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (.", "labels": [], "entities": [{"text": "CNN", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.722287118434906}]}, {"text": "\u2022 Effective and efficient: we reimplement several state-of-the-art neural models () using NCRF++.", "labels": [], "entities": []}, {"text": "Experiments show models builtin NCRF++ give comparable performance with reported results in the literature.", "labels": [], "entities": []}, {"text": "Besides, NCRF++ is implemented using batch calculation, which can be accelerated using GPU.", "labels": [], "entities": [{"text": "NCRF++", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.8475874662399292}]}, {"text": "Our experiments demonstrate that NCRF++ as an effective and efficient toolkit.", "labels": [], "entities": []}, {"text": "\u2022 Function enriched: NCRF++ extends the Viterbi algorithm to enable decoding n best sequence labels with their probabilities.", "labels": [], "entities": []}, {"text": "Taking NER, Chunking and POS tagging as typical examples, we investigate the performance of models builtin NCRF++, the influence of humandefined and automatic features, the performance of nbest decoding and the running speed with the batch size.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.639630138874054}]}, {"text": "Detail results are shown in Section 3.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on three benchmarks.", "labels": [], "entities": []}, {"text": " Table 2: Results using different features.", "labels": [], "entities": []}]}