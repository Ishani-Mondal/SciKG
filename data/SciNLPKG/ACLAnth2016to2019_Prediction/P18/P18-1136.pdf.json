{"title": [{"text": "Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems", "labels": [], "entities": [{"text": "Mem2Seq", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.936962902545929}]}], "abstractContent": [{"text": "End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases.", "labels": [], "entities": []}, {"text": "In this paper , we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue.", "labels": [], "entities": []}, {"text": "Mem2Seq is the first neural gen-erative model that combines the multi-hop attention over memories with the idea of pointer network.", "labels": [], "entities": [{"text": "Mem2Seq", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9314630627632141}]}, {"text": "We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories.", "labels": [], "entities": []}, {"text": "In addition, our model is quite general without complicated task-specific designs.", "labels": [], "entities": []}, {"text": "As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Task-oriented dialog systems help users to achieve specific goals with natural language such as restaurant reservation and schedule arrangement.", "labels": [], "entities": [{"text": "restaurant reservation and schedule arrangement", "start_pos": 96, "end_pos": 143, "type": "TASK", "confidence": 0.6326077580451965}]}, {"text": "Traditionally, they have been built with several pipelined modules: language understanding, dialog management, knowledge query, and language generation (;.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7601735293865204}, {"text": "dialog management", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.8226823806762695}, {"text": "language generation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7485319674015045}]}, {"text": "Moreover, the ability to query external Knowledge Bases (KBs) is essential in taskoriented dialog systems, since the responses are guided not only by the dialog history but also by the query results (e.g.).", "labels": [], "entities": []}, {"text": "However, despite the stability of such pipelined systems via combining domain-specific knowledge and slot-filling * * These two authors contributed equally.", "labels": [], "entities": []}, {"text": "DRIVER Where can I get tea?", "labels": [], "entities": []}, {"text": "Seq2Seq I have a away from away would you like the address +Attn I have a listing fora place that serves tea that is 5 miles away Ptr-Unk There is a away you like would you like more info", "labels": [], "entities": []}], "datasetContent": [{"text": "We use three public multi-turn task-oriented dialog datasets to evaluate our model: the bAbI dialog (Bordes and Weston, 2017), DSTC2) and In-Car Assistant ( . The train/validation/test sets of these three datasets are split in advance by the providers.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.8984648585319519}]}, {"text": "The dataset statistics are reported in.", "labels": [], "entities": []}, {"text": "The bAbI dialog includes five end-to-end dialog learning tasks in the restaurant domain, which are simulated dialog data.", "labels": [], "entities": []}, {"text": "Task 1 to 4 are about API calls, refining API calls, recommending options, and providing additional information, respectively.", "labels": [], "entities": []}, {"text": "Task 5 is the union of tasks 1-4.", "labels": [], "entities": []}, {"text": "There are two test sets for each task: one follows the same distribution as the training set and the other has out-of-vocabulary (OOV) entity values that does not exist in the training set.", "labels": [], "entities": []}, {"text": "We also used dialogs extracted from the Dialog State Tracking Challenge 2 (DSTC2) with the refined version from, which ignores the dialog state annotations.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge 2 (DSTC2)", "start_pos": 40, "end_pos": 81, "type": "TASK", "confidence": 0.5914889648556709}]}, {"text": "The main difference with bAbI dialog is that this dataset is extracted from real human-bot dialogs, which is noisier and harder since the bots made mistakes due to speech recognition errors or misinterpretations.", "labels": [], "entities": []}, {"text": "Recently, In-Car Assistant dataset has been released.", "labels": [], "entities": [{"text": "In-Car Assistant dataset", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.6093343098958334}]}, {"text": "which is a human-human, multi-domain dialog dataset collected from Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.8772408962249756}]}, {"text": "It has three distinct domains: calendar scheduling, weather information retrieval, and point-of-interest navigation.", "labels": [], "entities": [{"text": "calendar scheduling", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7618695199489594}, {"text": "weather information retrieval", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.6565760175387064}, {"text": "point-of-interest navigation", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.7404674589633942}]}, {"text": "This dataset has shorter conversation turns, but the user and system behaviors are more diverse.", "labels": [], "entities": []}, {"text": "In addition, the system responses are variant and the KB information is much more complicated.", "labels": [], "entities": []}, {"text": "Hence, this dataset requires stronger ability to interact with KBs, rather than dialog state tracking.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.7472568154335022}]}, {"text": "Per-response/dialog Accuracy: A generative response is correct only if it is exactly the same as the gold response.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9775941371917725}, {"text": "generative response", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8958835601806641}]}, {"text": "A dialog is correct only if every generated responses of the dialog are correct, which can be considered as the task-completion rate.", "labels": [], "entities": []}, {"text": "Note that Bordes and Weston (2017) tests their model by selecting the system response from predefined response candidates, that is, their system solves a multi-class classification task.", "labels": [], "entities": []}, {"text": "Since Mem2Seq generates each token individually, evaluating with this metric is much more challenging for our model.", "labels": [], "entities": []}, {"text": "BLEU: It is a measure commonly used for machine translation systems (), but it has also been used in evaluating dialog systems (  and chat-bots (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9854623079299927}, {"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7750608325004578}]}, {"text": "Moreover, BLEU score is a relevant measure in task-oriented dialog as there is not a large variance between the generated answers, unlike open domain generation ( ).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9828335046768188}]}, {"text": "Hence, we include BLEU score in our evaluation (i.e. using Moses multi-bleu.perl script).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9793786108493805}]}, {"text": "Entity F1: We micro-average over the entire set of system responses and compare the entities in plain text.", "labels": [], "entities": []}, {"text": "The entities in each gold system response are selected by a predefined entity list.", "labels": [], "entities": []}, {"text": "This metric evaluates the ability to generate relevant entities from the provided KBs and to capture the semantics of the dialog ( . Note that the original In-Car Assis- Ptr-Unk Mem2Seq H1 Mem2Seq H3 Mem2Seq H6 T1 99.4 (-) 99.9 (99.6) 100 100 100 100 100 100 100 (100) T2 99.5 (-) 100 100 100 100 100 100 100  We mainly compare Mem2Seq with hop 1,3,6 with several existing models: query-reduction networks (QRN,), end-toend memory networks (MemNN, Sukhbaatar et al.), and gated end-to-end memory networks (GMemNN,).", "labels": [], "entities": [{"text": "GMemNN", "start_pos": 506, "end_pos": 512, "type": "DATASET", "confidence": 0.8189960718154907}]}, {"text": "We also implemented the following baseline models: standard sequence-to-sequence (Seq2Seq) models with and without attention (, and pointer to unknown (Ptr-Unk,).", "labels": [], "entities": []}, {"text": "Note that the results we listed in and for QRN are different from the original paper, because based on their released code, we discovered that the per-response accuracy was not correctly computed.", "labels": [], "entities": [{"text": "QRN", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.7563121318817139}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9162402153015137}]}, {"text": "bAbI Dialog: In, we follow Bordes We simply modified the evaluation part and reported the results.", "labels": [], "entities": []}, {"text": "(https://github.com/uwnlp/qrn) and Weston (2017) to compare the performance based on per-response and per-dialog accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9248393177986145}]}, {"text": "Mem2Seq with 6 hops can achieve per-response 97.9% and per-dialog 69.6% accuracy in T5, and 84.5% and 2.3% for T5-OOV, which surpass existing methods by far.", "labels": [], "entities": [{"text": "Mem2Seq", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9135487079620361}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9995391368865967}]}, {"text": "One can find that in T3 especially, which is the task to recommend restaurant based on their ranks, our model can achieve promising results due to the memory pointer.", "labels": [], "entities": []}, {"text": "In terms of per-response accuracy, this indicates that our model can generalize well with few performance loss for test OOV data, while others have around 15-20% drop.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9634403586387634}, {"text": "OOV", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9468589425086975}]}, {"text": "The performance gain in OOV data is also mainly attributed to the use of copy mechanism.", "labels": [], "entities": [{"text": "OOV data", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.6969194710254669}]}, {"text": "In addition, the effectiveness of hops is demonstrated in tasks 3-5, since they require reasoning ability over the KB information.", "labels": [], "entities": []}, {"text": "Note that QRN, MemNN and GMemNN viewed bAbI dialog tasks as classification problems.", "labels": [], "entities": [{"text": "MemNN", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.8943203687667847}, {"text": "GMemNN", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8886747360229492}]}, {"text": "Although their tasks are easier compared to our generative methods, Mem2Seq models can still overpass the performance.", "labels": [], "entities": []}, {"text": "Finally, one can find that Seq2Seq and Ptr-Unk models are also strong baselines, which further confirms that generative methods can also achieve good performance in taskoriented dialog systems ( )., the Seq2Seq models from  and the rule-based from Bordes and Weston (2017) are reported.", "labels": [], "entities": []}, {"text": "Mem2Seq has the highest 75.3% entity F1 score and an high of 55.3 BLEU score.", "labels": [], "entities": [{"text": "Mem2Seq", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9187463521957397}, {"text": "F1 score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9276496171951294}, {"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9804289937019348}]}, {"text": "This further confirms that Mem2Seq can perform well in retrieving the correct entity, using the multiple hop mechanism without losing language modeling.", "labels": [], "entities": []}, {"text": "Here, we do not report the results using match type or entity type) feature, since this meta-information are not commonly available and we want to have an evaluation on plain input output couples.", "labels": [], "entities": []}, {"text": "One can also find out that, Mem2Seq comparable perresponse accuracy (i.e. 2% margin) among other existing solution.", "labels": [], "entities": [{"text": "Mem2Seq", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.838113009929657}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.7572195529937744}, {"text": "margin", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9076235294342041}]}, {"text": "Note that the per-response accuracy for every model is less than 50% since the dataset is quite noisy and it is hard to generate a response that is exactly the same as the gold one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8948002457618713}]}], "tableCaptions": [{"text": " Table 2: Dataset statistics for 3 different datasets.", "labels": [], "entities": []}, {"text": " Table 3: Per-response and per-dialog (in the parentheses) accuracy on bAbI dialogs. Mem2Seq achieves  the highest average per-response accuracy and has the least out-of-vocabulary performance drop.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9972930550575256}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9473081827163696}]}, {"text": " Table 4:  Evaluation on DSTC2.  Seq2Seq (+attn and +copy) is reported  from Eric and Manning (2017).", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.7332878112792969}]}, {"text": " Table 5: Evaluation on In-Car Assistant. Human, rule- based and KV Retrieval Net evaluation (with *) are reported  from (Eric et al., 2017), which are not directly comparable.  Mem2Seq achieves highest BLEU and entity F1 score over  baselines.", "labels": [], "entities": [{"text": "KV Retrieval Net evaluation", "start_pos": 65, "end_pos": 92, "type": "METRIC", "confidence": 0.6426030844449997}, {"text": "BLEU", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9987388253211975}, {"text": "entity F1 score", "start_pos": 212, "end_pos": 227, "type": "METRIC", "confidence": 0.7942384878794352}]}, {"text": " Table 6: Example of generated responses for the  In-Car Assistant on the scheduling domain.", "labels": [], "entities": []}]}