{"title": [{"text": "YEDDA: A Lightweight Collaborative Text Span Annotation Tool", "labels": [], "entities": [{"text": "YEDDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5881257653236389}]}], "abstractContent": [{"text": "In this paper, we introduce YEDDA, a lightweight but efficient and comprehensive open-source tool for text span annotation.", "labels": [], "entities": [{"text": "YEDDA", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.7182773351669312}, {"text": "text span annotation", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8148068785667419}]}, {"text": "YEDDA provides a systematic solution for text span annotation, ranging from collaborative user annotation to administrator evaluation and analysis.", "labels": [], "entities": [{"text": "YEDDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8464466333389282}, {"text": "text span annotation", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7151060501734415}]}, {"text": "It overcomes the low efficiency of traditional text annotation tools by annotating entities through both command line and shortcut keys, which are configurable with custom labels.", "labels": [], "entities": []}, {"text": "YEDDA also gives intelligent recommendations by learning the up-to-date annotated text.", "labels": [], "entities": [{"text": "YEDDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9202561974525452}]}, {"text": "An administrator client is developed to evaluate annotation quality of multiple annotators and generate detailed comparison report for each annota-tor pair.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed system can reduce the annotation time by half compared with existing annotation tools.", "labels": [], "entities": [{"text": "annotation time", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.8376682698726654}]}, {"text": "And the annotation time can be further compressed by 16.47% through intelligent recommendation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Processing (NLP) systems rely on large-scale training data ( for supervised training.", "labels": [], "entities": []}, {"text": "However, manual annotation can be time-consuming and expensive.", "labels": [], "entities": []}, {"text": "Despite detailed annotation standards and rules, interannotator disagreement is inevitable because of human mistakes, language phenomena which are not covered by the annotation rules and the ambiguity of language itself).", "labels": [], "entities": []}, {"text": "Existing annotation tools () mainly focus on providing a visual interface for user annotation process but rarely consider the post-annotation quality analysis, which is necessary due to the inter-annotator disagreement.", "labels": [], "entities": []}, {"text": "In addition to the annotation quality, efficiency is also critical in large-scale annotation task, while being relatively less addressed in existing annotation tools).", "labels": [], "entities": []}, {"text": "Besides, many tools) require a complex system configuration on either local device or server, which is not friendly to new users.", "labels": [], "entities": []}, {"text": "To address the challenges above, we propose YEDDA 1 , a lightweight and efficient annotation tool for text span annotation.", "labels": [], "entities": [{"text": "text span annotation", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.7661421100298563}]}, {"text": "A snapshot is shown in.", "labels": [], "entities": []}, {"text": "Here text span boundaries are selected and assigned with a label, which can be useful for Named Entity Recognition (NER), word segmentation, chunking) ,etc.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.7927808364232382}, {"text": "word segmentation", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7514427602291107}]}, {"text": "To keep annotation efficient and accurate, YEDDA provides systematic solutions across the whole annotation process, which includes the shortcut annotation, batch annotation with a command line, intelligent recommendation, format exporting and  administrator evaluation/analysis.", "labels": [], "entities": [{"text": "YEDDA", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.6828708052635193}, {"text": "format exporting", "start_pos": 222, "end_pos": 238, "type": "TASK", "confidence": 0.8189541399478912}]}, {"text": "shows the general framework of YEDDA.", "labels": [], "entities": [{"text": "YEDDA", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.6066375970840454}]}, {"text": "It offers annotators with a simple and efficient Graphical User Interface (GUI) to annotate raw text.", "labels": [], "entities": []}, {"text": "For the administrator, it provides two useful toolkits to evaluate multi-annotated text and generate detailed comparison report for annotator pair.", "labels": [], "entities": []}, {"text": "YEDDA has the advantages of being: \u2022 Convenient: it is lightweight with an intuitive interface and does not rely on specific operating systems or pre-installed packages.", "labels": [], "entities": [{"text": "YEDDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.753454327583313}]}, {"text": "\u2022 Efficient: it supports both shortcut and command line annotation models to accelerate the annotating process.", "labels": [], "entities": [{"text": "Efficient", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9556060433387756}]}, {"text": "\u2022 Intelligent: it offers user with real-time system suggestions to avoid duplicated annotation.", "labels": [], "entities": []}, {"text": "\u2022 Comprehensive: it integrates useful toolkits to give the statistical index of analyzing multi-user annotation results and generate detailed content comparison for annotation pairs.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 gives an overview of previous text annotation tools and the comparison with ours.", "labels": [], "entities": []}, {"text": "Section 3 describes the architecture of YEDDA and its detail functions.", "labels": [], "entities": [{"text": "YEDDA", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.6380281448364258}]}, {"text": "Section 4 shows the efficiency comparison results of different annotation tools.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes this paper and give the future plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we compare the efficiency of our system with four widely used annotation tools.", "labels": [], "entities": []}, {"text": "We extract 100 sentences from) training data, with each sentence containing at least 4 entities.", "labels": [], "entities": []}, {"text": "Two undergraduate students without any experience on those tools are invited to annotate those sentences . Their average annotation time is shown in, where \"YEDDA+R\" suggests annotation using YEDDA with the help of system recommendation.", "labels": [], "entities": [{"text": "YEDDA+R", "start_pos": 157, "end_pos": 164, "type": "METRIC", "confidence": 0.8790752092997233}]}, {"text": "The inter-annotator agreements for those tools are closed, which around 96.1% F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9992138147354126}]}, {"text": "As we can see from the figure, our YEDDA system can greatly reduce the annotation time.", "labels": [], "entities": []}, {"text": "With the help of system recommendation, the annotation time can be further reduced.", "labels": [], "entities": [{"text": "annotation time", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.9193152785301208}]}, {"text": "We notice that \"YEDDA+R\" has larger advantage with the increasing numbers of annotated sentences, this is because the system recommendation gives better suggestions when it learns larger annotated sentences.", "labels": [], "entities": [{"text": "YEDDA+R", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.7351575295130411}]}, {"text": "The \"YEDDA+R\" gives 16.47% time reduction in annotating 100 sentences 11 .", "labels": [], "entities": [{"text": "YEDDA+R\"", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.8674406558275223}, {"text": "time reduction", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.9361333847045898}]}], "tableCaptions": [{"text": " Table 1. Statistics for two annotations, assume File1 as gold standard", "labels": [], "entities": [{"text": "File1", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7906160354614258}]}]}