{"title": [{"text": "DialSQL: Dialogue Based Structured Query Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7495793998241425}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8983412384986877}]}, {"text": "However, further improvement of the existing approaches turns out to be quite challenging.", "labels": [], "entities": []}, {"text": "Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction.", "labels": [], "entities": []}, {"text": "DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions.", "labels": [], "entities": []}, {"text": "User feedback is then leveraged to revise the query.", "labels": [], "entities": []}, {"text": "We design a generic sim-ulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 120, "end_pos": 135, "type": "DATASET", "confidence": 0.9339251518249512}]}, {"text": "Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3% to 69.0% using only 2.4 validation questions per dialogue.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building natural language interfaces to databases (NLIDB) is a long-standing open problem and has significant implications for many application domains.", "labels": [], "entities": []}, {"text": "It can enable users without SQL programming background to freely query the data they have.", "labels": [], "entities": []}, {"text": "For this reason, generating SQL queries from natural language questions has gained a renewed interest due to the recent advance in deep learning and semantic parsing).", "labels": [], "entities": [{"text": "SQL queries from natural language questions", "start_pos": 28, "end_pos": 71, "type": "TASK", "confidence": 0.7810070912043253}, {"text": "semantic parsing", "start_pos": 149, "end_pos": 165, "type": "TASK", "confidence": 0.7513256669044495}]}, {"text": "While new methods race to achieve the stateof-the-art performance on NLIDB datasets such as WikiSQL (), the accuracy is still not high enough for real use.", "labels": [], "entities": [{"text": "NLIDB datasets", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9036296904087067}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9994114637374878}]}, {"text": "For example, SQLNet () achieves 61.3% accuracy on WikiSQL.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9983906745910645}]}, {"text": "After analyzing the error cases of Seq2SQL ( and SQLNet, we recognized that many wrong translations cannot be easily corrected due to the lack of external knowledge and semantic understanding.", "labels": [], "entities": []}, {"text": "In this paper, we aim to alleviate the aforementioned problem by putting human users in the loop.", "labels": [], "entities": []}, {"text": "Previous human-in-the-loop NLIDBs () rely on users to carefully go through a generated SQL query and revise it accordingly, which is not feasible for users who do not know the SQL language.", "labels": [], "entities": []}, {"text": "Instead, we resort to a different approach by introducing a goal-oriented dialogue model, DialSQL, that interacts with users to extract and correct potential errors in the generated queries.", "labels": [], "entities": []}, {"text": "Given a SQL query generated from a natural language question, we assume any segment, or span, of the generated query such as a WHERE clause can be potentially erroneous.", "labels": [], "entities": [{"text": "SQL query generated from a natural language question", "start_pos": 8, "end_pos": 60, "type": "TASK", "confidence": 0.7133576422929764}]}, {"text": "The goal of DialSQL is to extract the erroneous spans and ask users multi-choice questions to validate and correct these errors.", "labels": [], "entities": []}, {"text": "DialSQL is based on a hierarchical encoder-decoder architecture with attention and pointer mechanisms.", "labels": [], "entities": []}, {"text": "The model first encodes each turn of interaction and runs a dialogue level RNN network on the dialogue history.", "labels": [], "entities": []}, {"text": "The output of the network is then used to predict the error category, i.e., whether it is a selection, projection, or aggregation error.", "labels": [], "entities": []}, {"text": "Conditioned on the error category, the output of a second RNN is used to predict the start and end positions of the error span by pointing to the query tokens.", "labels": [], "entities": []}, {"text": "Finally, candidate choices are decoded from the error category and span representations.", "labels": [], "entities": []}, {"text": "Following previous Natural Language Question : What are the countries that joined the NATO before 2004 ? Ground Truth SQL Query : SELECT country WHERE date of join < 2004 Initial SQL Query : SELECT count ( country ) WHERE date of join = 2004 How to train and evaluate DialSQL become two challenging issues due to the lack of error data and interaction data.", "labels": [], "entities": [{"text": "SELECT country WHERE date", "start_pos": 130, "end_pos": 155, "type": "METRIC", "confidence": 0.872796967625618}, {"text": "SELECT count", "start_pos": 191, "end_pos": 203, "type": "METRIC", "confidence": 0.9649372696876526}]}, {"text": "In this work, we construct a simulator to generate simulated dialogues, a general approach practiced by many dialogue studies.", "labels": [], "entities": []}, {"text": "Inspired by the agenda-based methods for user simulation (, we keep an agenda of pending actions that are needed to induce the ground truth query.", "labels": [], "entities": []}, {"text": "At the start of the dialogue, anew query is carefully synthesized by randomly altering the ground truth query and the agenda is populated by the sequence of altering actions.", "labels": [], "entities": []}, {"text": "Each action consists of three sub-actions: (i) Pick an error category and extract a span; (ii) Raise a question; (iii) Update the query by randomly altering the span and remove the action from the agenda.", "labels": [], "entities": [{"text": "Raise a question", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.8972288966178894}]}, {"text": "Consider the example in Step-1 synthesizes the initial query by randomly altering the WHERE clause and AGGREGATION; Step-2 generates the simulated dialogue by validating the altered spans and offering the correct choice.", "labels": [], "entities": [{"text": "WHERE", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.9686020612716675}, {"text": "AGGREGATION", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9154811501502991}]}, {"text": "To evaluate our model, we first train DialSQL on the simulated dialogues.", "labels": [], "entities": []}, {"text": "Initial queries for new questions are manufactured by running a black box SQL generation system on the new questions.", "labels": [], "entities": []}, {"text": "When tested on the WikiSQL () dataset, our model increases the query match accuracy of SQLNet () from 61.3% to 69.0% using on average 2.4 validation questions per query.", "labels": [], "entities": [{"text": "WikiSQL () dataset", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.8298364480336508}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8389795422554016}]}], "datasetContent": [{"text": "We study the problem of building an interactive natural language interface to databases (INLIDB) for synthesizing SQL queries from natural language questions.", "labels": [], "entities": []}, {"text": "In particular, our goal is to design a dialogue system to extract and validate potential errors in generated queries by asking users multi-choice questions over multiple turns.", "labels": [], "entities": []}, {"text": "We will first define the problem formally and then explain our simulation strategy.", "labels": [], "entities": []}, {"text": "In this section, we evaluate DialSQL on WikiSQL using several evaluation metrics by comparing with previous literature.", "labels": [], "entities": []}, {"text": "We measure the query generation accuracy as well as the complexity of the questions and the length of the user interactions.", "labels": [], "entities": [{"text": "query generation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.6879848837852478}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9213832020759583}]}, {"text": "We evaluate DialSQL on WikiSQL using query-match accuracy (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9273073673248291}]}, {"text": "Query-match accuracy is the proportion of testing examples for which the generated query is exactly the same as the ground truth, except the ordering of the WHERE clauses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9727866649627686}]}, {"text": "We count the number of turns to analyze whether DialSQL generates any redundant validation questions.", "labels": [], "entities": []}, {"text": "We use the average number of tokens in the generated validation questions to evaluate if DialSQL can generate simple questions without overwhelming users.", "labels": [], "entities": []}, {"text": "Since SQLNet and Seq2SQL are single-step models, we cannot analyze DialSQL's performance by comparing against these on the last two metrics.", "labels": [], "entities": []}, {"text": "We overcome this issue by generating simulated dialogues using an oracle system that has access to the ground truth query.", "labels": [], "entities": []}, {"text": "The system compares SELECT and AGGREGATION clauses of the predicted query and the ground truth; asks a validation question if they differ.", "labels": [], "entities": [{"text": "SELECT", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9930608868598938}, {"text": "AGGREGATION", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.84654301404953}]}, {"text": "For each WHERE clause pairs of generated query and the ground truth, the system counts the number of matching segments namely COLUMN, OP, and VALUE.", "labels": [], "entities": [{"text": "COLUMN", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9967222809791565}, {"text": "OP", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9393559098243713}, {"text": "VALUE", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.9969792366027832}]}, {"text": "The system takes all the pairs with the highest matching scores and asks a validation question until one of the queries has no remaining WHERE clause.", "labels": [], "entities": [{"text": "WHERE", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.9216413497924805}]}, {"text": "If both queries have no remaining clauses, the dialogue terminates.", "labels": [], "entities": []}, {"text": "Otherwise, the system asks a validate where added (validate where removed) question when the generated query (ground truth query) has more remaining clauses.", "labels": [], "entities": []}, {"text": "We call this strategy OracleMatching (OM).", "labels": [], "entities": []}, {"text": "OM ensures that the generated dialogues have the minimum number of turns possible.", "labels": [], "entities": [{"text": "OM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.37964582443237305}]}, {"text": "users are allowed to revisit their previous answers and with more informative user responses; instead the model only validates the error span and the user directly gives the correct choice.", "labels": [], "entities": []}, {"text": "In this scenario, the performance further improves on both development and test sets.", "labels": [], "entities": []}, {"text": "It seems decoding candidate choices is a hard task and has room for improvement.", "labels": [], "entities": []}, {"text": "For the rest of the evaluation, we present results with multi-choice questions.", "labels": [], "entities": []}, {"text": "We extend our evaluation of DialSQL using human subject experiment so that real users interact with the system instead of our simulated user.", "labels": [], "entities": []}, {"text": "We randomly pick 100 questions from WikiSQL development set and run SQLNet to generate initial candidate queries.", "labels": [], "entities": [{"text": "WikiSQL development set", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.7869552373886108}]}, {"text": "Next, we run DialSQL using these candidate queries to generate 100 dialogues, each of which is evaluated: QM accuracies of SQLNet, DialSQL with user simulation, and DialSQL with real users (value in paranthesis is standard deviation).", "labels": [], "entities": []}, {"text": "Figure 4: Distribution of user preference for Dial-SQL ranking (scaled to 1-6 with 6 is None of the above.). by 3 different users.", "labels": [], "entities": []}, {"text": "At each turn, we show users the headers of the corresponding table, original question, system response, and list of candidate choices for users to pick.", "labels": [], "entities": []}, {"text": "For each error category, we generate 5 choices except for the validate where added category for which we only show 2 choices (YES or NO).", "labels": [], "entities": [{"text": "YES", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9960755705833435}, {"text": "NO", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.6832060217857361}]}, {"text": "Also, we add an additional choice of None of the above so that users can keep the previous prediction unchanged.", "labels": [], "entities": [{"text": "None", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9558274149894714}]}, {"text": "At the end of each turn, we also ask users to give an overall score between 1 and 3 to evaluate whether they had a successful interaction with the DialSQL for the current turn.", "labels": [], "entities": []}, {"text": "On average, the length of the generated dialogues is 5.6.", "labels": [], "entities": []}, {"text": "In, we compare the performance of SQLNet, DialSQL with user simulation, and Dial-SQL with real users using QM metric.", "labels": [], "entities": []}, {"text": "We present the average performance across 3 different users with the standard deviation estimated overall dialogues.", "labels": [], "entities": []}, {"text": "We observe that when real users interact with our system, the overall performance of the generated queries are better than SQLNet model showing that DialSQL can improve the performance of a strong NLIDB system in areal setting.", "labels": [], "entities": []}, {"text": "However, there is still a large room for improvement between simulated dialogues and real users.", "labels": [], "entities": []}, {"text": "In, we present the correlation between DialSQL ranking of the candidate choices and user preferences.", "labels": [], "entities": []}, {"text": "We observe that, user answers and DialSQL rankings are positively correlated; most of the time users prefer the top-1 choice.", "labels": [], "entities": []}, {"text": "Interestingly, 15% of the user answers is None of the above.", "labels": [], "entities": []}, {"text": "This commonly happens in the scenario where DialSQL response asks to replace a correct condition and users prefer to keep the original prediction unchanged.", "labels": [], "entities": []}, {"text": "Another scenario where users commonly select None of the above is when table headers without the content remain insufficient for users to correctly disambiguate condition values from questions.", "labels": [], "entities": []}, {"text": "We also compute the Mean Reciprocal Rank (MMR) for each user to measure the correlation between real users and DialSQL.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MMR)", "start_pos": 20, "end_pos": 46, "type": "METRIC", "confidence": 0.9630860785643259}]}, {"text": "Average MMR is 0.69 with standard deviation of 0.004 which also shows that users generally prefer the choices ranked higher by DialSQL.", "labels": [], "entities": [{"text": "MMR", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.6994528770446777}]}, {"text": "The overall score of each turn also suggests that users had a reasonable conversation with DialSQL.", "labels": [], "entities": []}, {"text": "The average score is 2.86 with standard deviation of 0.14, showing users can understand DialSQL responses and can pick a choice confidently.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Query-match accuracy on the WikiSQL  development and test sets. The first two scores  of our model are generated using 5 candidate  choices, ( + ) denotes a variant where users can re- visit their previous answers, and (*) denotes a vari- ant with more informative user responses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9966006875038147}, {"text": "WikiSQL  development and test sets", "start_pos": 38, "end_pos": 72, "type": "DATASET", "confidence": 0.8625942945480347}]}]}