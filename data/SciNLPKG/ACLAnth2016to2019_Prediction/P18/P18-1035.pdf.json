{"title": [], "abstractContent": [{"text": "The ability to consolidate information of different types is at the core of intelligence , and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others.", "labels": [], "entities": []}, {"text": "In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.8448374271392822}, {"text": "UCCA parsing", "start_pos": 95, "end_pos": 107, "type": "TASK", "confidence": 0.8320087194442749}, {"text": "Universal Dependencies (UD) parsing", "start_pos": 141, "end_pos": 176, "type": "TASK", "confidence": 0.6814288198947906}]}, {"text": "We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks.", "labels": [], "entities": []}, {"text": "Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.", "labels": [], "entities": [{"text": "UCCA parsing", "start_pos": 114, "end_pos": 126, "type": "TASK", "confidence": 0.9280864000320435}]}, {"text": "Our code is publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.859062135219574}]}, {"text": "This shortage is more pronounced in languages other than English, and less researched domains.", "labels": [], "entities": []}, {"text": "Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR;, bilexical Semantic Dependencies (SDP; and Universal Conceptual Cognitive Annotation (UCCA;.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.8186749219894409}, {"text": "Abstract Meaning Representation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6557578444480896}, {"text": "Universal Conceptual Cognitive Annotation (UCCA", "start_pos": 149, "end_pos": 196, "type": "TASK", "confidence": 0.6407051831483841}]}, {"text": "While these schemes are formally different and focus on different distinctions, much of their semantic content is shared.", "labels": [], "entities": []}, {"text": "Multitask learning) allows exploiting the overlap between tasks to ef-1 http://github.com/danielhers/tupa fectively extend the training data, and has greatly advanced with neural networks and representation learning (see \u00a72).", "labels": [], "entities": [{"text": "representation learning", "start_pos": 192, "end_pos": 215, "type": "TASK", "confidence": 0.880418062210083}]}, {"text": "We build on these ideas and propose a general transition-based DAG parser, able to parse UCCA, AMR, SDP and UD (.", "labels": [], "entities": [{"text": "parse UCCA", "start_pos": 83, "end_pos": 93, "type": "TASK", "confidence": 0.6938853114843369}]}, {"text": "We train the parser using MTL to obtain significant improvements on UCCA parsing over single-task training in (1) in-domain and (2) outof-domain settings in English; (3) an in-domain setting in German; and (4) an in-domain setting in French, where training data is scarce.", "labels": [], "entities": [{"text": "UCCA parsing", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.7200464010238647}]}, {"text": "The novelty of this work is in proposing a general parsing and learning architecture, able to accommodate such widely different parsing tasks, and in leveraging it to show benefits from learning them jointly.", "labels": [], "entities": []}], "datasetContent": [{"text": "We here detail a range of experiments to assess the value of MTL to UCCA parsing, training the parser in single-task and multitask settings, and evaluating its performance on the UCCA test sets in both in-domain and out-of-domain settings.", "labels": [], "entities": [{"text": "MTL", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9625357985496521}, {"text": "UCCA parsing", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.7419905364513397}, {"text": "UCCA test sets", "start_pos": 179, "end_pos": 193, "type": "DATASET", "confidence": 0.9402675032615662}]}, {"text": "For UCCA, we use v1.2 of the English Wikipedia corpus, with the standard train/dev/test split (see), and the Twenty Thousand Leagues Under the Sea corpora (20K;, annotated in English, French and German.", "labels": [], "entities": [{"text": "UCCA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8971633315086365}, {"text": "English Wikipedia corpus", "start_pos": 29, "end_pos": 53, "type": "DATASET", "confidence": 0.9171012838681539}, {"text": "Twenty Thousand Leagues Under the Sea corpora", "start_pos": 109, "end_pos": 154, "type": "DATASET", "confidence": 0.5272652591977801}]}, {"text": "8 For English and French we use 20K v1.0, a small parallel corpus comprising the first five chapters of the book.", "labels": [], "entities": []}, {"text": "As in previous work, we use the English part only as an out-of-domain test set.", "labels": [], "entities": []}, {"text": "We train and test on the French part using the standard split, as well as the German corpus (v0.9), which is a pre-release and still contains a considerable amount of noisy annotation.", "labels": [], "entities": [{"text": "German corpus (v0.9)", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.8777648687362671}]}, {"text": "Tuning is performed on the respective development sets.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9537136554718018}]}, {"text": "For AMR, we use LDC2017T10, identical to the dataset targeted in) in our English experiments, henceforth referred to as UD ++ . We use only the AMR, DM and UD training sets from standard splits.", "labels": [], "entities": [{"text": "AMR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9113433957099915}, {"text": "AMR", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.8545876145362854}, {"text": "UD training sets", "start_pos": 156, "end_pos": 172, "type": "DATASET", "confidence": 0.7007519900798798}]}, {"text": "While UCCA is annotated over Wikipedia and over a literary corpus, the domains for AMR, DM and UD are blogs, news, emails, reviews, and Q&A.", "labels": [], "entities": []}, {"text": "This domain difference between training and testis particularly challenging (see \u00a79).", "labels": [], "entities": []}, {"text": "Unfortunately, none of the other schemes have available annotation over Wikipedia text.", "labels": [], "entities": []}, {"text": "We explore the following settings: (1) in-domain setting in English, training and testing on Wiki; (2) out-of-domain setting in English, training on Wiki and testing on 20K; (3) French indomain setting, where available training dataset is small, training and testing on 20K; (4) German indomain setting on 20K, with somewhat noisy annotation.", "labels": [], "entities": [{"text": "Wiki", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.9530584812164307}, {"text": "Wiki", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.9608150124549866}]}, {"text": "For MTL experiments, we use unlabeled AMR, DM and UD ++ parsing as auxiliary tasks in English, and unlabeled UD parsing in French and German.", "labels": [], "entities": [{"text": "MTL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9785327315330505}]}, {"text": "We also report baseline results training only the UCCA training sets.", "labels": [], "entities": [{"text": "UCCA training sets", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.9617825945218405}]}, {"text": "We create a unified corpus for each setting, shuffling all sentences from relevant datasets together, but using only the UCCA development set F 1 score as the early stopping criterion.", "labels": [], "entities": [{"text": "UCCA development set F 1 score", "start_pos": 121, "end_pos": 151, "type": "DATASET", "confidence": 0.7851951917012533}, {"text": "early stopping criterion", "start_pos": 159, "end_pos": 183, "type": "METRIC", "confidence": 0.9086879293123881}]}, {"text": "In each training epoch, we use the same number of examples from each task-the UCCA training set size.", "labels": [], "entities": [{"text": "UCCA training set size", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.8682742416858673}]}, {"text": "Since training sets differ in size, we sample this many sentences from each one.", "labels": [], "entities": []}, {"text": "The model is implemented using DyNet (: Hyperparameter settings.", "labels": [], "entities": []}, {"text": "Middle column shows hyperparameters used for the single-task architecture, described in \u00a74.2, and right column for the multitask architecture, described in \u00a76.", "labels": [], "entities": []}, {"text": "Main refers to parameters specific to the main task-UCCA parsing (task-specific MLP and BiLSTM, and edge label embedding), Aux to parameters specific to each auxiliary task (task-specific MLP, but no edge label embedding since the tasks are unlabeled), and Shared to parameters shared among all tasks (shared BiLSTM and embeddings).", "labels": [], "entities": [{"text": "Aux", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9880870580673218}, {"text": "Shared", "start_pos": 257, "end_pos": 263, "type": "METRIC", "confidence": 0.9799536466598511}]}, {"text": "We use dropout () between MLP layers, and recurrent dropout ( between BiLSTM layers, both with p = 0.4.", "labels": [], "entities": []}, {"text": "We also use word (\u03b1 = 0.2), tag (\u03b1 = 0.2) and dependency relation (\u03b1 = 0.5) dropout.", "labels": [], "entities": []}, {"text": "In addition, we use a novel form of dropout, node dropout: with a probability of 0.1 at each step, all features associated with a single node in the parser state are replaced with zero vectors.", "labels": [], "entities": []}, {"text": "For optimization we use a minibatch size of 100, decaying all weights by 10 \u22125 at each update, and train with stochastic gradient descent for N epochs with a learning rate of 0.1, followed by AMSGrad (Sashank J. Reddi, 2018) for N epochs with \u03b1 = 0.001, \u03b2 1 = 0.9 and \u03b2 2 = 0.999.", "labels": [], "entities": [{"text": "AMSGrad", "start_pos": 192, "end_pos": 199, "type": "METRIC", "confidence": 0.5867490172386169}]}, {"text": "We use N = 50 for English and German, and N = 400 for French.", "labels": [], "entities": []}, {"text": "We found this training strategy better than using only one of the optimization methods, In training, the embedding fora feature value w is replaced with a zero vector with a probability of    similar to findings by.", "labels": [], "entities": []}, {"text": "We select the epoch with the best average labeled F 1 score on the UCCA development set.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9671427210172018}, {"text": "UCCA development set", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.9627321163813273}]}, {"text": "Other hyperparameter settings are listed in.", "labels": [], "entities": []}, {"text": "We evaluate on UCCA using labeled precision, recall and F 1 on primary and remote edges, following previous work.", "labels": [], "entities": [{"text": "UCCA", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.9520416259765625}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.8859198093414307}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9996047616004944}, {"text": "F 1", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9940325319766998}]}, {"text": "Edges in predicted and gold graphs are matched by terminal yield and label.", "labels": [], "entities": []}, {"text": "Significance testing of improvements over the single-task model is done by the bootstrap test), with p < 0.05.", "labels": [], "entities": []}, {"text": "presents our results on the English indomain Wiki test set.", "labels": [], "entities": [{"text": "English indomain Wiki test set", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.8840268135070801}]}, {"text": "MTL with all auxiliary tasks and their combinations improves the primary F 1 score over the single task baseline.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.98172394434611}]}, {"text": "In most settings the improvement is statistically significant.", "labels": [], "entities": []}, {"text": "Using all auxiliary tasks contributed less than just DM and UD ++ , the combination of which yielded the best scores yet in in-domain UCCA parsing, with 74.9% F 1 on primary edges.", "labels": [], "entities": [{"text": "UCCA parsing", "start_pos": 134, "end_pos": 146, "type": "TASK", "confidence": 0.697298139333725}, {"text": "F 1", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.9935155808925629}]}, {"text": "Remote F 1 is improved in some settings, but due to the relatively small number of remote edges (about 2% of all edges), none of the differences is significant.", "labels": [], "entities": [{"text": "Remote F 1", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.6031011243661245}]}, {"text": "Note that our baseline single-task model (Single) is slightly better than the current state-of-the-art (HAR17;, due to the incorporation of additional features (see \u00a74.2).", "labels": [], "entities": [{"text": "HAR17", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.48646169900894165}]}, {"text": "presents our experimental results on the 20K corpora in the three languages.", "labels": [], "entities": []}, {"text": "For English out-of-domain, improvements from using MTL are even more marked.", "labels": [], "entities": [{"text": "MTL", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.6159082651138306}]}, {"text": "Moreover, the improvement is largely additive: the best model, using all three auxiliary tasks (All), yields an error reduction of 2.9%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 112, "end_pos": 127, "type": "METRIC", "confidence": 0.9854377210140228}]}, {"text": "Again, the single-task baseline is slightly better than HAR17.", "labels": [], "entities": [{"text": "HAR17", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.7307835817337036}]}], "tableCaptions": [{"text": " Table 1: Number of tokens and sentences in the training, development and test sets we use for each corpus and language.", "labels": [], "entities": []}, {"text": " Table 2: Hyperparameter settings. Middle column shows hy- perparameters used for the single-task architecture, described  in  \u00a74.2, and right column for the multitask architecture, de- scribed in  \u00a76. Main refers to parameters specific to the main  task-UCCA parsing (task-specific MLP and BiLSTM, and  edge label embedding), Aux to parameters specific to each  auxiliary task (task-specific MLP, but no edge label embed- ding since the tasks are unlabeled), and Shared to parameters  shared among all tasks (shared BiLSTM and embeddings).", "labels": [], "entities": [{"text": "Main", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9619781374931335}, {"text": "Aux", "start_pos": 327, "end_pos": 330, "type": "METRIC", "confidence": 0.9901159405708313}, {"text": "Shared", "start_pos": 464, "end_pos": 470, "type": "METRIC", "confidence": 0.9954074025154114}]}, {"text": " Table 3: Labeled precision, recall and F1 (in %) for primary  and remote edges, on the Wiki test set. indicates signifi- cantly better than Single. HAR17: Hershcovich et al. (2017).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9614385366439819}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996213912963867}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9991133809089661}, {"text": "Wiki test set", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9581233461697897}, {"text": "HAR17", "start_pos": 149, "end_pos": 154, "type": "DATASET", "confidence": 0.7123656868934631}]}, {"text": " Table 4: Labeled precision, recall and F1 (in %) for primary  and remote edges, on the 20K test sets. indicates signifi- cantly better than Single. HAR17: Hershcovich et al. (2017).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9551121592521667}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996551275253296}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9991021156311035}, {"text": "20K test sets", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.8717240492502848}, {"text": "HAR17", "start_pos": 149, "end_pos": 154, "type": "DATASET", "confidence": 0.6394157409667969}]}, {"text": " Table 5: L1 distance between dataset word distributions,  quantifying domain differences in English (low is similar).", "labels": [], "entities": []}, {"text": " Table 6: Unlabeled F1 scores between the representations of  the same English sentences (from PTB WSJ), converted to  the unified DAG format, and annotated UCCA graphs.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.951390415430069}, {"text": "PTB WSJ)", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.9539635380109152}]}]}