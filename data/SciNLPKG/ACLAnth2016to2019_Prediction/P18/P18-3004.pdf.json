{"title": [{"text": "Recursive Neural Network Based Preordering for English-to-Japanese Machine Translation", "labels": [], "entities": [{"text": "Recursive Neural Network Based Preordering", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7486026883125305}, {"text": "English-to-Japanese Machine Translation", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.6408045987288157}]}], "abstractContent": [{"text": "The word order between source and target languages significantly influences the translation quality in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.746401458978653}]}, {"text": "Preordering can effectively address this problem.", "labels": [], "entities": []}, {"text": "Previous preordering methods require a manual feature design, making language dependent design costly.", "labels": [], "entities": []}, {"text": "In this paper, we propose a preordering method with a recursive neural network that learns features from raw inputs.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed method achieves comparable gain in translation quality to the state-of-the-art method but without a manual feature design.", "labels": [], "entities": [{"text": "translation", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9641494154930115}]}], "introductionContent": [{"text": "The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 103, "end_pos": 140, "type": "TASK", "confidence": 0.7708776791890463}]}, {"text": "Models that adjust orders of translated phrases in decoding have been proposed to solve this problem).", "labels": [], "entities": []}, {"text": "However, such reordering models do not perform well for long-distance reordering.", "labels": [], "entities": []}, {"text": "In addition, their computational costs are expensive.", "labels": [], "entities": []}, {"text": "To address these problems, preordering ( and post-ordering () models have been proposed.", "labels": [], "entities": []}, {"text": "Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation.", "labels": [], "entities": []}, {"text": "In particular, preordering effectively improves the translation quality because it solves long-distance reordering and computational complexity issues (.", "labels": [], "entities": []}, {"text": "Rule-based preordering methods either manually create reordering rules ( or extract reordering rules from a corpus (.", "labels": [], "entities": []}, {"text": "On the other hand, studies in ( apply machine learning to the preordering problem.", "labels": [], "entities": []}, {"text": "proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. and proposed methods that construct a binary tree and reordering simultaneously from a source sentence.", "labels": [], "entities": []}, {"text": "These methods require a manual feature design for every language pair, which makes language dependent design costly.", "labels": [], "entities": []}, {"text": "To overcome this challenge, methods based on feed forward neural networks that do not require a manual feature design have been proposed (de.", "labels": [], "entities": []}, {"text": "However, these methods decide whether to reorder child nodes without considering the sub-trees, which contains important information for reordering.", "labels": [], "entities": []}, {"text": "As a preordering method that is free of manual feature design and makes use of information in sub-trees, we propose a preordering method with a recursive neural network (RvNN).", "labels": [], "entities": []}, {"text": "RvNN calculates reordering in a bottom-up manner (from the leaf nodes to the root) on a source syntax tree.", "labels": [], "entities": []}, {"text": "Thus, preordering is performed considering the entire sub-trees.", "labels": [], "entities": [{"text": "preordering", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.9588045477867126}]}, {"text": "Specifically, RvNN learns whether to reorder nodes of a syntax tree 1 with a vector representation of sub-trees and syntactic categories.", "labels": [], "entities": []}, {"text": "We evaluate the proposed method for English-to-Japanese translations using both phrase-based SMT (PBSMT) and neural MT (NMT).", "labels": [], "entities": []}, {"text": "The results confirm that the proposed method achieves comparable translation quality to the state-of-the-art preordering method) that requires a manual feature design.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU scores with preordering by our  model and without preordering under different \u03bb  settings (trained on a 500k subset of the training  data).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983831644058228}, {"text": "preordering", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9711827039718628}]}, {"text": " Table 2: BLEU and RIBES scores on the test set.  (All models are trained on the entire training cor- pus of 1.8M sentence pairs.) Numbers in bold in- dicate the best systems and the systems that are  statistically insignificant at p < 0.05 from the best  systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990785121917725}, {"text": "RIBES", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9826075434684753}]}]}