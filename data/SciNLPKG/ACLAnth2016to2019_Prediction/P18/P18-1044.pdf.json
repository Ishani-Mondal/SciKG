{"title": [{"text": "Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis", "labels": [], "entities": [{"text": "Predicate-argument Structure Analysis", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.8491599559783936}]}], "abstractContent": [{"text": "Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult.", "labels": [], "entities": [{"text": "Japanese predicate-argument structure (PAS) analysis", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7767929179327828}, {"text": "anaphora resolution", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7425837814807892}]}, {"text": "To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.8928461968898773}]}, {"text": "However, since it is prohibitively expensive , it is promising to take advantage of a large amount of raw corpora.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.9279453158378601}]}, {"text": "In our experiments, our model outper-forms existing state-of-the-art models for Japanese PAS analysis.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.9129711091518402}]}], "introductionContent": [{"text": "In pro-drop languages, such as Japanese and Chinese, pronouns are frequently omitted when they are inferable from their contexts and background knowledge.", "labels": [], "entities": []}, {"text": "The natural language processing (NLP) task for detecting such omitted pronouns and searching for their antecedents is called zero anaphora resolution.", "labels": [], "entities": []}, {"text": "This task is essential for downstream NLP tasks, such as information extraction and summarization.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8845790326595306}, {"text": "summarization", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.9754982590675354}]}, {"text": "For Japanese, zero anaphora resolution is usually conducted within predicate-argument structure (PAS) analysis as a task of finding an omitted argument fora predicate.", "labels": [], "entities": [{"text": "zero anaphora resolution", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.6671611766020457}, {"text": "predicate-argument structure (PAS) analysis", "start_pos": 67, "end_pos": 110, "type": "TASK", "confidence": 0.6635658393303553}]}, {"text": "PAS analysis is a task to find an argument for each case of a predicate.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9039484560489655}]}, {"text": "For Japanese PAS analysis, the ga (nominative, NOM), wo (accusative, ACC) and ni (dative, DAT) cases are generally handled.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9286514818668365}]}, {"text": "To develop models for Japanese PAS analysis, supervised learning methods using annotated corpora have been applied on the basis of morpho-syntactic clues.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.8811612129211426}]}, {"text": "However, omitted pronouns have few clues and thus these models try to learn relations between a predicate and its (omitted) argument from the annotated corpora.", "labels": [], "entities": []}, {"text": "The annotated corpora consist of several tens of thousands sentences, and it is difficult to learn predicate-argument relations or selectional preferences from such small-scale corpora.", "labels": [], "entities": []}, {"text": "The state-of-the-art models for Japanese PAS analysis achieve an accuracy of around 50% for zero pronouns (.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.8698395788669586}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9994586110115051}]}, {"text": "A promising way to solve this data scarcity problem is enhancing models with a large amount of raw corpora.", "labels": [], "entities": []}, {"text": "There are two major approaches to using raw corpora: extracting knowledge from raw corpora beforehand () and using raw corpora for data augmentation ().", "labels": [], "entities": []}, {"text": "In traditional studies on Japanese PAS analysis, selectional preferences are extracted from raw corpora beforehand and are used in PAS analysis models.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.9316913485527039}, {"text": "PAS analysis", "start_pos": 131, "end_pos": 143, "type": "TASK", "confidence": 0.9397542476654053}]}, {"text": "For example, propose a supervised model for Japanese PAS analysis based on case frames, which are automatically acquired from a raw corpus by clustering predicate-argument structures.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.9095150530338287}]}, {"text": "However, case frames are not based on distributed representations of words and have a data sparseness problem even if a large raw corpus is employed.", "labels": [], "entities": []}, {"text": "Some recent approaches to Japanese PAS analysis combines neural network models with knowledge extraction from raw corpora.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.8930096328258514}]}, {"text": "extract selectional preferences by an unsupervised method that is similar to negative sampling ().", "labels": [], "entities": []}, {"text": "They then use the pre-extracted selectional preferences as one of the features to their PAS analysis model.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 88, "end_pos": 100, "type": "TASK", "confidence": 0.8369674980640411}]}, {"text": "The PAS analysis model is trained by a supervised method and the selectional preference representations are fixed during training.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9194932878017426}]}, {"text": "Us-: Examples of Japanese sentences and their PAS analysis.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8295507431030273}]}, {"text": "In sentence (1), case markers ( \u304c(ga), \u3092(wo), and \u306b(ni) ) correspond to NOM, ACC, and DAT.", "labels": [], "entities": [{"text": "NOM", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.5070530772209167}, {"text": "ACC", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9757988452911377}, {"text": "DAT", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9698025584220886}]}, {"text": "In example (2), the correct case marker is hidden by the topic marker \u306f (wa).", "labels": [], "entities": []}, {"text": "In sentence (3), the NOM argument of the second predicate \u5dfb\u304d\u8fbc\u307e\u308c\u305f (was involved), is dropped.", "labels": [], "entities": [{"text": "NOM argument", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.8273608684539795}]}, {"text": "NULL indicates that the predicate does not have the corresponding case argument or that the case argument is not written in the sentence.", "labels": [], "entities": [{"text": "NULL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.863791286945343}]}, {"text": "ing pre-trained external knowledge in the form of word embeddings has also been ubiquitous.", "labels": [], "entities": []}, {"text": "However, such external knowledge is overwritten in the task-specific training.", "labels": [], "entities": []}, {"text": "The other approach to using raw corpora for PAS analysis is data augmentation.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9728909730911255}]}, {"text": "generate pseudo training data from a raw corpus and use them for their zero pronoun resolution model.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.6674897869427999}]}, {"text": "They generate the pseudo training data by dropping certain words or pronouns in a raw corpus and assuming them as correct antecedents.", "labels": [], "entities": []}, {"text": "After generating the pseudo training data, they rely on ordinary supervised training based on neural networks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural semisupervised model for Japanese PAS analysis.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 69, "end_pos": 81, "type": "TASK", "confidence": 0.7954628169536591}]}, {"text": "We adopt neural adversarial training to directly exploit the advantage of using a raw corpus.", "labels": [], "entities": []}, {"text": "Our model consists of two neural network models: a generator model of Japanese PAS analysis and a so-called \"validator\" model of the generator prediction.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.4390343576669693}]}, {"text": "The generator neural network is a model that predicts probabilities of candidate arguments of each predicate using RNN-based features and a head-selection model ().", "labels": [], "entities": []}, {"text": "The validator neural network gets inputs from the generator and scores them.", "labels": [], "entities": []}, {"text": "This validator can score the generator prediction even when PAS gold labels are not available.", "labels": [], "entities": []}, {"text": "We apply supervised learning to the generator and unsupervised learning to the entire network using a raw corpus.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows: (1) a novel adversarial training model for PAS analysis; (2) learning from a raw corpus as a source of external knowledge; and (3) as a result, we achieve state-of-the-art performance on Japanese PAS analysis.", "labels": [], "entities": [{"text": "PAS analysis", "start_pos": 88, "end_pos": 100, "type": "TASK", "confidence": 0.9826713502407074}, {"text": "PAS analysis", "start_pos": 241, "end_pos": 253, "type": "TASK", "confidence": 0.6209708452224731}]}], "datasetContent": [{"text": "We use the exophora entities, i.e., an author and a reader, following the annotations in KWDLC.", "labels": [], "entities": [{"text": "KWDLC", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9366902112960815}]}, {"text": "We also assign author/reader labels to the following expressions in the same way as;: author \"\u79c1\" (I), \"\u50d5\" (I), \"\u6211\u3005\" (we), \"\u5f0a\u793e\" (our company) The KWDLC corpus is available at http://nlp.", "labels": [], "entities": [{"text": "KWDLC corpus", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.9083890020847321}]}, {"text": "ist.i.kyoto-u.ac.jp/EN/index.php?KWDLC reader \"\u3042\u306a\u305f\" (you), \"\u541b\" (you), \"\u5ba2\" (customer), \"\u7686\u69d8\" (you all) Following and, we conduct two kinds of analysis: (1) case analysis and (2) zero anaphora resolution.", "labels": [], "entities": [{"text": "case analysis", "start_pos": 154, "end_pos": 167, "type": "TASK", "confidence": 0.8622742891311646}, {"text": "anaphora resolution", "start_pos": 181, "end_pos": 200, "type": "TASK", "confidence": 0.6367350816726685}]}, {"text": "Case analysis is the task to determine the correct case labels when predicates and their arguments have direct dependencies but their case markers are hidden by surface markers, such as topic markers.", "labels": [], "entities": [{"text": "Case analysis", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9325729310512543}]}, {"text": "Zero anaphora resolution is a task to find certain case arguments that do not have direct dependencies to their predicates in the sentence.", "labels": [], "entities": [{"text": "Zero anaphora resolution", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6922455330689748}]}, {"text": "Following, we exclude predicates that the same arguments are filled in multiple cases of a predicate.", "labels": [], "entities": []}, {"text": "This is relatively uncommon and 1.5 % of the whole corpus are excluded.", "labels": [], "entities": []}, {"text": "Predicates are marked in the gold dependency parses.", "labels": [], "entities": []}, {"text": "Candidate arguments are just other tokens than predicates.", "labels": [], "entities": []}, {"text": "This setting is also the same as.", "labels": [], "entities": []}, {"text": "All performances are evaluated with microaveraged F-measure ().", "labels": [], "entities": [{"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.906711757183075}]}, {"text": "We compare two models: the supervised generator model (Gen) and the proposed semi-supervised model with adversarial training (Gen+Adv).", "labels": [], "entities": []}, {"text": "We also compare our models with two previous models: and, whose performance on the KWDLC corpus is reported.", "labels": [], "entities": [{"text": "KWDLC corpus", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.9659929871559143}]}, {"text": "Our models (Gen and Gen+Adv) outperformed the previous models.", "labels": [], "entities": []}, {"text": "Furthermore, the proposed model with adversarial training (Gen+Adv) was significantly better than the supervised model (Gen).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parameters for neural network structure  and training.", "labels": [], "entities": []}, {"text": " Table 3: KWDLC data statistics.", "labels": [], "entities": [{"text": "KWDLC data statistics", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.9501648942629496}]}, {"text": " Table 4: KWDLC training data statistics for each  case.", "labels": [], "entities": [{"text": "KWDLC training data", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.788282553354899}]}, {"text": " Table 6: The detailed results of case analysis and zero anaphora resolution for the NOM, ACC and DAT  cases. Our models outperform the existing models in all cases. All values are evaluated with F-measure.", "labels": [], "entities": [{"text": "NOM", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.6444649696350098}, {"text": "F-measure", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9961334466934204}]}, {"text": " Table 7: The comparisons of Gen+Adv with Gen  and the data augmentation model (Gen+Aug).  \u2021  denotes that the improvement is statistically sig- nificant at p < 0.05, compared with Gen+Aug.", "labels": [], "entities": []}, {"text": " Table 6. Among the three cases, zero anaphora  resolution of the ACC and DAT cases is notori- ously difficult. This is attributed to the fact that  these ACC and DAT cases are fewer than the NOM", "labels": [], "entities": [{"text": "NOM", "start_pos": 192, "end_pos": 195, "type": "DATASET", "confidence": 0.6631582379341125}]}]}