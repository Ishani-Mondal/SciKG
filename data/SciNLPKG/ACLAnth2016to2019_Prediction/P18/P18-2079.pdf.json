{"title": [{"text": "Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network", "labels": [], "entities": []}], "abstractContent": [{"text": "As more and more academic papers are being submitted to conferences and journals , evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers.", "labels": [], "entities": []}, {"text": "In this paper, in order to assist professionals in evaluating academic papers, we propose a novel task: automatic academic paper rating (AAPR), which automatically determine whether to accept academic papers.", "labels": [], "entities": [{"text": "automatic academic paper rating (AAPR)", "start_pos": 104, "end_pos": 142, "type": "METRIC", "confidence": 0.8875569956643241}]}, {"text": "We build anew dataset for this task and propose a novel modularized hierarchical convolu-tional neural network to achieve automatic academic paper rating.", "labels": [], "entities": []}, {"text": "Evaluation results show that the proposed model outperforms the baselines by a large margin.", "labels": [], "entities": []}, {"text": "The dataset and code are available at https: //github.com/lancopku/AAPR", "labels": [], "entities": [{"text": "AAPR", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9173696637153625}]}], "introductionContent": [{"text": "Every year there are thousands of academic papers submitted to conferences and journals.", "labels": [], "entities": []}, {"text": "Rating all these papers can be exhausting, and sometimes rating scores can be affected by the personal factors of the reviewers, leading to inequality problem.", "labels": [], "entities": [{"text": "inequality", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.9768476486206055}]}, {"text": "Therefore, there is a great need for rating academic papers automatically.", "labels": [], "entities": []}, {"text": "In this paper, we explore how to automatically rate the academic papers based on their LA T E X source file and meta information, which we call the task of automatic academic paper rating (AAPR).", "labels": [], "entities": [{"text": "LA T E X source file", "start_pos": 87, "end_pos": 107, "type": "DATASET", "confidence": 0.6459649453560511}, {"text": "automatic academic paper rating (AAPR", "start_pos": 156, "end_pos": 193, "type": "METRIC", "confidence": 0.6936153521140417}]}, {"text": "A task that is similar to the AAPR is automatic essay scoring (AES).", "labels": [], "entities": [{"text": "automatic essay scoring (AES)", "start_pos": 38, "end_pos": 67, "type": "METRIC", "confidence": 0.7448403735955557}]}, {"text": "AES has been studied fora longtime.", "labels": [], "entities": [{"text": "AES", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8882680535316467}]}, {"text": "Project Essay Grade) is one of the earliest attempts to solve the AES task by predicting the score using linear regression over expert crafted textual features.", "labels": [], "entities": [{"text": "AES task", "start_pos": 66, "end_pos": 74, "type": "TASK", "confidence": 0.6465725004673004}]}, {"text": "Much of the following work applied similar methods by using various classifiers with more sophisticated features including grammar, vocabulary and style).", "labels": [], "entities": []}, {"text": "These traditional methods can work almost as well as human raters.", "labels": [], "entities": [{"text": "raters", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.8524115681648254}]}, {"text": "However, they all demand a large amount of feature engineering, which requires a lot of expertise.", "labels": [], "entities": []}, {"text": "Recent studies turn to use deep neural networks, claiming that deep learning models can relieve the system from heavy feature engineering.", "labels": [], "entities": []}, {"text": "proposed to use long short term memory network) with a linear regression output layer to predict the score.", "labels": [], "entities": []}, {"text": "They added a score prediction loss to the original C&W embedding, so that the word embeddings are related to the quality of the essay.", "labels": [], "entities": [{"text": "C&W embedding", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9139128774404526}]}, {"text": "also applied recurrent neural networks to process the essay, except that they put a convolutional layer ahead of the recurrent layer to extract local features.", "labels": [], "entities": []}, {"text": "proposed to apply a two-layer convolutional neural network (CNN) to model the essay.", "labels": [], "entities": []}, {"text": "The first layer is responsible for encoding the sentence and the second layer is to encode the whole essay.", "labels": [], "entities": []}, {"text": "further proposed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay.", "labels": [], "entities": []}, {"text": "Although there has been a lot of work dealing with AES task, researchers have not attempted the AAPR task.", "labels": [], "entities": [{"text": "AES task", "start_pos": 51, "end_pos": 59, "type": "TASK", "confidence": 0.45743459463119507}, {"text": "AAPR task", "start_pos": 96, "end_pos": 105, "type": "TASK", "confidence": 0.6422486305236816}]}, {"text": "Different from the essay in language capability tests, academic papers are much longer with much more information, and the overall quality is affected by a variety of factors besides the writing.", "labels": [], "entities": []}, {"text": "Therefore, we propose a model that considers the overall information of one academic paper, including the title, authors, abstract and the main content of the LA T E X source file of the paper.", "labels": [], "entities": [{"text": "LA T E X source file of the paper", "start_pos": 159, "end_pos": 192, "type": "DATASET", "confidence": 0.8250781496365865}]}, {"text": "Our main contributions are listed as follows: \u2022 We propose the task of automatically rating academic papers and build anew dataset for this task.", "labels": [], "entities": []}, {"text": "\u2022 We propose a modularized hierarchical convolutional neural network model that considers the overall information of the source paper.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed method outperforms the baselines by a large margin.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our model on the dataset we build for this task.", "labels": [], "entities": []}, {"text": "We first introduce the dataset, evaluation metric, and experimental details.", "labels": [], "entities": []}, {"text": "Then, we compare our model with baselines.", "labels": [], "entities": []}, {"text": "Finally, we provide the analysis and the discussion of experimental results.", "labels": [], "entities": []}, {"text": "Arxiv Academic Paper Dataset: As there is no existing dataset that can be used directly, we create a dataset by collecting data on academic papers in the field of artificial intelligence from the website 2 . The dataset consists of 19,218 academic papers.", "labels": [], "entities": [{"text": "Arxiv Academic Paper Dataset", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.9039247632026672}]}, {"text": "The information of each source paper consists of the venue which marks whether the paper is accepted, and the source LA T E X file.", "labels": [], "entities": [{"text": "LA T E X file", "start_pos": 117, "end_pos": 130, "type": "METRIC", "confidence": 0.8804336309432983}]}, {"text": "We divide the dataset into training, validation, and test parts.", "labels": [], "entities": []}, {"text": "The details are shown in: Statistical information of Arxiv academic paper dataset.", "labels": [], "entities": [{"text": "Arxiv academic paper dataset", "start_pos": 53, "end_pos": 81, "type": "DATASET", "confidence": 0.9157813042402267}]}, {"text": "Positive and Negative denote whether the source paper is accepted.", "labels": [], "entities": [{"text": "Negative", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9873156547546387}]}, {"text": "We use accuracy as our evaluation metric instead of the F-score, precision, and recall because the positive and negative examples in our dataset are well balanced.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9994238615036011}, {"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9959967136383057}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.999680757522583}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9996545314788818}]}, {"text": "Since the author names are different from the common scientific words in the paper, we separately buildup vocabulary for authors and text words of source papers with the size of 20,000 and 50,000, respectively.", "labels": [], "entities": []}, {"text": "We use the training strategies mentioned in for CNN classifier to tune the hyper-parameters based on the accuracy on the validation set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9988151788711548}]}, {"text": "The word or author embedding is randomly initialized and can be learned during training.", "labels": [], "entities": []}, {"text": "The size of word embedding or author embedding is 128 and the batch size is 32.", "labels": [], "entities": []}, {"text": "Adam optimizer () is used to minimize cross entropy loss function.", "labels": [], "entities": []}, {"text": "We apply dropout regularization () to avoid overfitting and clip the gradients () to the maximum norm of 5.0.", "labels": [], "entities": []}, {"text": "During training, we train the model fora fixed number of epochs and monitor its performance on the validation set after every 50 updates.", "labels": [], "entities": []}, {"text": "Once training is finished, we select the model with the highest accuracy on the validation set as our final model and evaluate its performance on the testing set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9965826869010925}]}], "tableCaptions": [{"text": " Table 1: Statistical information of Arxiv aca- demic paper dataset. Positive and Negative de- note whether the source paper is accepted.", "labels": [], "entities": [{"text": "Arxiv aca- demic paper dataset", "start_pos": 37, "end_pos": 67, "type": "DATASET", "confidence": 0.7924523254235586}]}, {"text": " Table 2: Comparison between our proposed model  and the baselines on the test set. Our proposed  model is denoted as MHCNN.", "labels": [], "entities": [{"text": "MHCNN", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.7403194904327393}]}]}