{"title": [{"text": "AMR Parsing as Graph Prediction with Latent Alignment", "labels": [], "entities": [{"text": "AMR Parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7151835262775421}]}], "abstractContent": [{"text": "meaning representations (AMRs) are broad-coverage sentence-level semantic representations.", "labels": [], "entities": [{"text": "meaning representations (AMRs)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6496452867984772}]}, {"text": "AMRs represent sentences as rooted labeled directed acyclic graphs.", "labels": [], "entities": []}, {"text": "AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9371976256370544}]}, {"text": "We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments.", "labels": [], "entities": []}, {"text": "As exact inference requires marginalizing over alignments and is infeasible, we use the variational auto-encoding framework and a continuous relaxation of the discrete alignments.", "labels": [], "entities": []}, {"text": "We show that joint modeling is preferable to using a pipeline of align and parse.", "labels": [], "entities": [{"text": "joint modeling", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7381644248962402}]}, {"text": "The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).", "labels": [], "entities": [{"text": "LDC2016E25", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.8023068904876709}]}], "introductionContent": [{"text": "Abstract meaning representations (AMRs) () are broad-coverage sentencelevel semantic representations.", "labels": [], "entities": [{"text": "Abstract meaning representations (AMRs)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6597945143779119}]}, {"text": "AMR encodes, among others, information about semantic relations, named entities, co-reference, negation and modality.", "labels": [], "entities": []}, {"text": "The semantic representations can be regarded as rooted labeled directed acyclic graphs (see).", "labels": [], "entities": []}, {"text": "As AMR abstracts away from details of surface realization, it is potentially beneficial in many semantic related NLP tasks, including text summarization (, machine translation () and question answering.", "labels": [], "entities": [{"text": "AMR abstracts away from details of surface realization", "start_pos": 3, "end_pos": 57, "type": "TASK", "confidence": 0.6546881422400475}, {"text": "text summarization", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.7616592645645142}, {"text": "machine translation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7802562117576599}, {"text": "question answering", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.8905599415302277}]}, {"text": "Figure 1: An example of AMR, the dashed lines denote latent alignments, obligate-01 is the root.", "labels": [], "entities": [{"text": "AMR", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.7202302813529968}]}, {"text": "Numbers indicate depth-first traversal order.", "labels": [], "entities": []}, {"text": "AMR parsing has recently received a lot of attention (e.g.,).", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8773918449878693}]}, {"text": "One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph (concepts) and words in the sentences.", "labels": [], "entities": [{"text": "AMR annotation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.933552473783493}]}, {"text": "Though this arguably simplified the annotation process (), it is not straightforward to produce an effective parser without relying on an alignment.", "labels": [], "entities": []}, {"text": "Most AMR parsers ( use a pipeline where the aligner training stage precedes training a parser.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.8553908467292786}]}, {"text": "The aligners are not directly informed by the AMR parsing objective and may produce alignments suboptimal for this task.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.8831887543201447}]}, {"text": "In this work, we demonstrate that the alignments can be treated as latent variables in a joint probabilistic model and induced in such away as to be beneficial for AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 164, "end_pos": 175, "type": "TASK", "confidence": 0.9713367521762848}]}, {"text": "Intuitively, in our probabilistic model, every node in a graph is assumed to be aligned to a word in a sentence: each concept is predicted based on the corresponding RNN state.", "labels": [], "entities": []}, {"text": "Similarly, graph edges (i.e. relations) are predicted based on representations of concepts and aligned words (see).", "labels": [], "entities": []}, {"text": "As alignments are latent, exact inference requires marginalizing over latent alignments, which is in-feasible.", "labels": [], "entities": []}, {"text": "Instead we use variational inference, specifically the variational autoencoding framework of.", "labels": [], "entities": []}, {"text": "Using discrete latent variables in deep learning has proven to be challenging.", "labels": [], "entities": []}, {"text": "We use a continuous relaxation of the alignment problem, relying on the recently introduced Gumbel-Sinkhorn construction (.", "labels": [], "entities": [{"text": "alignment", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.9726526737213135}]}, {"text": "This yields a computationally-efficient approximate method for estimating our joint probabilistic model of concepts, relations and alignments.", "labels": [], "entities": []}, {"text": "We assume injective alignments from concepts to words: every node in the graph is aligned to a single word in the sentence and every word is aligned to at most one node in the graph.", "labels": [], "entities": []}, {"text": "This is necessary for two reasons.", "labels": [], "entities": []}, {"text": "First, it lets us treat concept identification as sequence tagging attest time.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7276235073804855}, {"text": "sequence tagging", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.6263671368360519}]}, {"text": "For every word we would simply predict the corresponding concept or predict NULL to signify that no concept should be generated at this position.", "labels": [], "entities": [{"text": "NULL", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8777149319648743}]}, {"text": "Secondly, Gumbel-Sinkhorn can only work under this assumption.", "labels": [], "entities": []}, {"text": "This constraint, though often appropriate, is problematic for certain AMR constructions (e.g., named entities).", "labels": [], "entities": [{"text": "AMR constructions", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.9099433124065399}]}, {"text": "In order to deal with these cases, we re-categorized AMR concepts.", "labels": [], "entities": [{"text": "AMR", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8707547783851624}]}, {"text": "Similar recategorization strategies have been used in previous work.", "labels": [], "entities": []}, {"text": "The resulting parser achieves 74.4% Smatch score on the standard test set when using LDC2016E25 training set, 1 an improvement of 3.4% over the previous best result).", "labels": [], "entities": [{"text": "Smatch score", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9804480373859406}, {"text": "LDC2016E25 training set", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.9206549127896627}]}, {"text": "We also demonstrate that inducing alignments within the joint model is indeed beneficial.", "labels": [], "entities": []}, {"text": "When, instead of inducing alignments, we follow the standard approach and produce them on preprocessing, the performance drops by 0.9% Smatch.", "labels": [], "entities": [{"text": "Smatch", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.996229350566864}]}, {"text": "Our main contributions can be summarized as follows: \u2022 we introduce a joint probabilistic model for alignment, concept and relation identification; \u2022 we demonstrate that a continuous relaxation can be used to effectively estimate the model; \u2022 the model achieves the best reported results.", "labels": [], "entities": [{"text": "alignment, concept and relation identification", "start_pos": 100, "end_pos": 146, "type": "TASK", "confidence": 0.6551103591918945}]}, {"text": "2 The standard deviation across multiple training runs was 0.16%.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 6, "end_pos": 24, "type": "METRIC", "confidence": 0.907085508108139}]}, {"text": "The code can be accessed from https://github.", "labels": [], "entities": []}, {"text": "com/ChunchuanLv/AMR_AS_GRAPH_PREDICTION", "labels": [], "entities": [{"text": "ChunchuanLv", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9574195146560669}, {"text": "AMR_AS_GRAPH_PREDICTION", "start_pos": 16, "end_pos": 39, "type": "METRIC", "confidence": 0.6497154491288322}]}], "datasetContent": [{"text": "We start by comparing our parser to previous work (see).", "labels": [], "entities": []}, {"text": "Our model substantially outperforms all the previous models on both datasets.", "labels": [], "entities": []}, {"text": "Specifically, it achieves 74.4% Smatch score on LDC2016E25 (R2), which is an improvement of 3.4% over character seq2seq model relying on silver data.", "labels": [], "entities": [{"text": "Smatch score", "start_pos": 32, "end_pos": 44, "type": "METRIC", "confidence": 0.9804198741912842}, {"text": "LDC2016E25 (R2)", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.6282738670706749}]}, {"text": "For LDC2015E86 (R1), we obtain 73.7% Smatch score, which is an improvement of 3.0% over Annotation in R2 has also been slightly revised.", "labels": [], "entities": [{"text": "Smatch score", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.9758620858192444}, {"text": "Annotation", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9841572642326355}]}], "tableCaptions": [{"text": " Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.", "labels": [], "entities": [{"text": "LDC2016E25 dataset", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.9159399271011353}, {"text": "LDC2015E86  dataset", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.9434328973293304}]}, {"text": " Table 2:  F1 scores on individual phenom- ena. A'17 is AMREager, C'16 is CAMR, J'16 is  JAMR, Ch'17 is ChSeq+100K. Ours are marked  with standard deviation.", "labels": [], "entities": [{"text": "F1", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.999346911907196}, {"text": "AMREager", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.935671329498291}]}, {"text": " Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9673939347267151}, {"text": "LDC2015E86", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.9647461771965027}, {"text": "LDC2016E25", "start_pos": 154, "end_pos": 164, "type": "DATASET", "confidence": 0.9816179871559143}]}, {"text": " Table 4: Ablation studies: effect of joint model- ing (all on R2). Scores on ablations are averaged  over 2 runs. The first two models load the same  concept and alignment model before the second  stage.", "labels": [], "entities": []}, {"text": " Table 5: Ablation studies: alignment modeling  and relaxation (all on R2). Scores on ablations are  averaged over 2 runs.", "labels": [], "entities": [{"text": "alignment modeling", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9834636449813843}]}]}