{"title": [{"text": "A Hybrid Learning Scheme for Chinese Word Embedding", "labels": [], "entities": []}], "abstractContent": [{"text": "To improve word embedding, subword information has been widely employed in state-of-the-art methods.", "labels": [], "entities": [{"text": "word embedding", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7557347118854523}]}, {"text": "These methods can be classified to either compositional or predictive models.", "labels": [], "entities": []}, {"text": "In this paper, we propose a hybrid learning scheme, which integrates compositional and predictive model for word embedding.", "labels": [], "entities": []}, {"text": "Such a scheme can take advantage of both models , thus effectively learning word embedding.", "labels": [], "entities": []}, {"text": "The proposed scheme has been applied to learn word representation on Chi-nese.", "labels": [], "entities": [{"text": "learn word representation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6379635433355967}]}, {"text": "Our results show that the proposed scheme can significantly improve the performance of word embedding in terms of analogical reasoning and is robust to the size of training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embedding, also known as distributed word representation, represents a word as a real-valued low-dimensional vector and encodes its semantic meaning into the vector.", "labels": [], "entities": [{"text": "distributed word representation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6683263878027598}]}, {"text": "It is a fundamental task of natural language processing (NLP), such as language modeling (, machine translation (), caption generation () and question answering (.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.814276377360026}, {"text": "language modeling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7108578532934189}, {"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7416508346796036}, {"text": "caption generation", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8966002464294434}, {"text": "question answering", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.8965331315994263}]}, {"text": "Most previous word embedding methods suffer from high computational complexity and have difficulty to be applied to large-scale corpora.", "labels": [], "entities": []}, {"text": "Recently, Continuous Bag-Of-Words (CBOW) and Skip-Gram (SG) models), which can alleviate the above issue, have received much attention.", "labels": [], "entities": []}, {"text": "However, these models take a word as a basic unit but ignore rich subword information, which could significantly limit their performance.", "labels": [], "entities": []}, {"text": "To improve the performance of word embedding, subword information, such as morphemes and character n-grams, has been employed (.", "labels": [], "entities": []}, {"text": "While these methods are effective, they are originally developed for alphabetic writing systems and can't be applied directly to other writing systems, like Chinese.", "labels": [], "entities": []}, {"text": "In Chinese, each word typically consists of less characters than in English 1 , while each character can have a complicated structure of its meaning.", "labels": [], "entities": []}, {"text": "Typically, a Chinese character can be decomposed into components (\u90e8), where each component has its own meaning.", "labels": [], "entities": []}, {"text": "The internal semantic meaning of a Chinese word emerges from such a structure.", "labels": [], "entities": []}, {"text": "For example, the Chinese word \"\u6d77\u6c34 (seawater)\" is composed by \"\u6d77 (sea)\" and \"\u6c34 (water)\".", "labels": [], "entities": []}, {"text": "The semantic component of \"\u6d77 (sea)\" is \"\u6c35\", which is the transformation of \"\u6c34 (water)\" and indicates it is related to \"\u6c34 (water)\".", "labels": [], "entities": []}, {"text": "Therefore, the word \"\u6d77\u6c34 (seawater)\" has the meaning of \"water from the sea\".", "labels": [], "entities": []}, {"text": "Based on the linguistic feature of Chinese, recent methods have used subword information to improve Chinese word embedding.", "labels": [], "entities": []}, {"text": "For example, proposed a character-enhanced word embedding (CWE) model, which departed from CBOW of representing context words with both character embeddings and word embeddings.", "labels": [], "entities": []}, {"text": "proposed a radical embedding method, which used the CBOW framework but replacing word embeddings with radical embeddings. and extended the CWE model in different ways: the former presented a multi-granularity embedding (MGE) model, additionally using the embeddings associated with radicals detected in the target word; the latter proposed a similarity-based character-enhanced word embedding (SCWE) model, considering the similarity between a word and its component characters.", "labels": [], "entities": []}, {"text": "introduced a joint learning word embedding (JWE) model, which jointly learned embeddings for words, characters and components, and predicted the target word, respectively., on the other hand, represented Chinese words as sequences of strokes 2 and learned word embedding with stroke n-grams information.", "labels": [], "entities": []}, {"text": "The above methods can be divided into two types: compositional and predictive model.", "labels": [], "entities": []}, {"text": "The compositional model composes rich information into one vector to predict the target word.", "labels": [], "entities": []}, {"text": "In this type of model, information works in a cooperative manner for word embedding.", "labels": [], "entities": []}, {"text": "By contrast, the predictive model decouples various information to predict the target word.", "labels": [], "entities": []}, {"text": "The information in this type of model works competitively for word embedding.", "labels": [], "entities": []}, {"text": "Both models can effectively learn word embedding and give good estimation for rare and unseen words.", "labels": [], "entities": []}, {"text": "By combining richer information, the compositional model can more accurately represent the target word.", "labels": [], "entities": []}, {"text": "However, information is usually composed in a sophisticated way.", "labels": [], "entities": []}, {"text": "The predictive model, on the other hand, is simple and can directly capture the interaction between words and their internal information.", "labels": [], "entities": []}, {"text": "This type of model, however, typically ignores the interrelationship between various information.", "labels": [], "entities": []}, {"text": "To take advantage of both models, in this paper, we propose a hybrid learning scheme for word embedding.", "labels": [], "entities": []}, {"text": "The proposed scheme learns word embedding in a competitive and cooperative manner.", "labels": [], "entities": []}, {"text": "Specifically, in our scheme, the decoupled representations are used to capture the semantic meaning of target word respectively while making their composition semantically consistent with the target word.", "labels": [], "entities": []}, {"text": "The performance of proposed scheme has been evaluated on Chinese in terms of word similarity and analogy tasks.", "labels": [], "entities": []}, {"text": "The results show that our proposed scheme can effectively learn word representation and is robust to the size of training data.", "labels": [], "entities": [{"text": "word representation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7236933708190918}]}], "datasetContent": [{"text": "In this section, we evaluate COWE on Chinese in terms of word similarity computation and analogical reasoning.", "labels": [], "entities": [{"text": "word similarity computation", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.6865060826142629}, {"text": "analogical reasoning", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.8532141447067261}]}, {"text": "We . To further evaluate the effect of consistency loss and components, we create two variants of COWE, denoted as COWE-c2 and COWE-p.", "labels": [], "entities": [{"text": "COWE", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8513631820678711}, {"text": "COWE-c2", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.8765266537666321}, {"text": "COWE-p", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.9365209341049194}]}, {"text": "The former is indeed the JWE model with an additional consistency loss, while the latter is COWE without using component information.", "labels": [], "entities": [{"text": "consistency", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.9777082800865173}, {"text": "COWE", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.5826178193092346}]}, {"text": "The same parameter settings are used for all models.", "labels": [], "entities": []}, {"text": "Specifically, the vector dimension is set to 200, the training iteration is set to 100, both the size of context window and number of negative samples are set to 5, the initial learning rate is set to 0.025, and the subsampling threshold is set to 10 -4 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on word similarity evalua- tion.", "labels": [], "entities": []}, {"text": " Table 2: Results on word analogy evaluation.", "labels": [], "entities": [{"text": "word analogy evaluation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8861521085103353}]}, {"text": " Table 3: Results on word analogy evaluation,  trained on 5%/10%20% Wikipedia articles.", "labels": [], "entities": [{"text": "word analogy evaluation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8835194905598959}]}]}