{"title": [{"text": "Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia", "labels": [], "entities": [{"text": "Harvesting Paragraph-Level Question-Answer Pairs", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.809346005320549}, {"text": "Wikipedia", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.5764907002449036}]}], "abstractContent": [{"text": "We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence.", "labels": [], "entities": []}, {"text": "We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism.", "labels": [], "entities": []}, {"text": "Compared to models that only take into account sentence-level information (Heil-man and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coref-erence representation aids question generation significantly, producing models that outperform the current state-of-the-art.", "labels": [], "entities": [{"text": "question generation", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.7493296563625336}]}, {"text": "We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top-ranking Wikipedia articles and create a corpus of over one million question-answer pairs.", "labels": [], "entities": [{"text": "answer span extraction", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.5899942020575205}]}, {"text": "We also provide a qualitative analysis for this large-scale generated corpus from Wikipedia.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, there has been a resurgence of work in NLP on reading comprehension ( with the goal of developing systems that can answer questions about the content of a given passage or document.", "labels": [], "entities": [{"text": "answer questions about the content of a given passage or document", "start_pos": 125, "end_pos": 190, "type": "TASK", "confidence": 0.6897211183201183}]}, {"text": "Large-scale QA datasets are indispensable for training expressive statistical models for this task and play a critical role in advancing the field.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.7573947608470917}]}, {"text": "And there have been a number of efforts in this direction.", "labels": [], "entities": []}, {"text": "We show in italics the set of mentions that refer to Nikola, for the related task of answering cloze questions).", "labels": [], "entities": []}, {"text": "To create these datasets, either crowdsourcing or (semi-)synthetic approaches are used.", "labels": [], "entities": []}, {"text": "The (semi-)synthetic datasets (e.g.,) are large in size and cheap to obtain; however, they do not share the same characteristics as explicit QA/RC questions (.", "labels": [], "entities": []}, {"text": "In comparison, high-quality crowdsourced datasets are much smaller in size, and the annotation process is quite expensive because the labeled examples require expertise and careful design ().", "labels": [], "entities": []}, {"text": "Thus, there is a need for methods that can automatically generate high-quality question-answer pairs.", "labels": [], "entities": []}, {"text": "propose the use of recurrent neural networks to generate QA pairs from structured knowledge resources such as Freebase.", "labels": [], "entities": []}, {"text": "Their work relies on the existence of automatically acquired KBs, which are known to have errors and suffer from incompleteness.", "labels": [], "entities": []}, {"text": "They are also nontrivial to obtain.", "labels": [], "entities": []}, {"text": "In addition, the questions in the resulting dataset are limited to queries regarding a single fact (i.e., tuple) in the KB.", "labels": [], "entities": []}, {"text": "Motivated by the need for large scale QA pairs and the limitations of recent work, we investigate methods that can automatically \"harvest\" (generate) question-answer pairs from raw text/unstructured documents, such as Wikipediatype articles.", "labels": [], "entities": [{"text": "QA pairs", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.828489363193512}]}, {"text": "Recent work along these lines ( ) (see Section 2) has proposed the use of attention-based recurrent neural models trained on the crowdsourced SQuAD dataset) for question generation.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 142, "end_pos": 155, "type": "DATASET", "confidence": 0.8179841041564941}, {"text": "question generation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.8473647832870483}]}, {"text": "While successful, the resulting QA pairs are based on information from a single sentence.", "labels": [], "entities": []}, {"text": "As described in , however, nearly 30% of the questions in the human-generated questions of SQuAD rely on information beyond a single sentence.", "labels": [], "entities": []}, {"text": "For example, in, the second and third questions require coreference information (i.e., recognizing that \"His\" in sentence 2 and \"He\" in sentence 3 both corefer with \"Tesla\" in sentence 1) to answer them.", "labels": [], "entities": []}, {"text": "Thus, our research studies methods for incorporating coreference information into the training of a question generation system.", "labels": [], "entities": [{"text": "question generation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.714012011885643}]}, {"text": "In particular, we propose gated Coreference knowledge for Neural Question Generation (CorefNQG), a neural sequence model with a novel gating mechanism that leverages continuous representations of coreference clusters -the set of mentions used to refer to each entity -to better encode linguistic knowledge introduced by coreference, for paragraph-level question generation.", "labels": [], "entities": [{"text": "Neural Question Generation", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.7431900302569071}, {"text": "paragraph-level question generation", "start_pos": 337, "end_pos": 372, "type": "TASK", "confidence": 0.714932938416799}]}, {"text": "In an evaluation using the SQuAD dataset, we find that CorefNQG enables better question generation.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.8815520405769348}, {"text": "question generation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7618675231933594}]}, {"text": "It outperforms significantly the baseline neural sequence models that encode information from a single sentence, and a model that encodes all preceding context and the input sentence itself.", "labels": [], "entities": []}, {"text": "When evaluated on only the portion of SQuAD that requires coreference resolution, the gap between our system and the baseline systems is even larger.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.9083947539329529}]}, {"text": "By applying our approach to the 10,000 topranking Wikipedia articles, we obtain a question answering/reading comprehension dataset with over one million QA pairs; we provide a qualitative analysis in Section 6.", "labels": [], "entities": [{"text": "question answering/reading comprehension", "start_pos": 82, "end_pos": 122, "type": "TASK", "confidence": 0.8028181552886963}]}, {"text": "The dataset and the source code for the system are available at https://github.com/xinyadu/ HarvestingQA.", "labels": [], "entities": []}], "datasetContent": [{"text": "Recently there has been an increasing interest in question answering with the creation of many datasets.", "labels": [], "entities": [{"text": "question answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9147687554359436}]}, {"text": "Most are built using crowdsourcing; they are generally comprised of fewer than 100,000 QA pairs and are time-consuming to create.", "labels": [], "entities": []}, {"text": "WebQuestions ( (Semi-)synthetic generated datasets are easier to build to large-scale (.", "labels": [], "entities": []}, {"text": "They usually come in the form of cloze-style questions.", "labels": [], "entities": []}, {"text": "For example, created over a million examples by pairing CNN and Daily Mail news articles with their summarized bullet points.", "labels": [], "entities": [{"text": "Daily Mail news articles", "start_pos": 64, "end_pos": 88, "type": "DATASET", "confidence": 0.8008296340703964}]}, {"text": "showed that this dataset is quite noisy due to the method of data creation and concluded that performance of QA systems on the dataset is almost saturated.", "labels": [], "entities": []}, {"text": "Closest to our work is that of.", "labels": [], "entities": []}, {"text": "They train a neural triple-to-sequence model on SimpleQuestions, and apply their system to Freebase to produce a large collection of human-like question-answer pairs.", "labels": [], "entities": [{"text": "SimpleQuestions", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9376851320266724}, {"text": "Freebase", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9607360363006592}]}, {"text": "For generation of our one million QA pair corpus, we apply our systems to the 10,000 topranking articles of Wikipedia.", "labels": [], "entities": []}, {"text": "For question generation evaluation, we use BLEU () and ME-TEOR).", "labels": [], "entities": [{"text": "question generation evaluation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.9002611438433329}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9991214871406555}, {"text": "ME-TEOR", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9666572213172913}]}, {"text": "1 BLEU measures average n-gram precision vs. a set of reference questions and penalizes for overly short sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9984197616577148}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9399612545967102}]}, {"text": "METEOR is a recall-oriented metric that takes into account synonyms, stemming, and paraphrases.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.46467357873916626}, {"text": "recall-oriented", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.9677526950836182}]}, {"text": "For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers.", "labels": [], "entities": [{"text": "answer candidate extraction evaluation", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7990919798612595}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9996479749679565}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9992110729217529}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9988664388656616}]}, {"text": "Since answer boundaries are sometimes ambiguous, we compute Binary Overlap and Proportional Overlap metrics in addition to Exact Match.", "labels": [], "entities": [{"text": "Proportional Overlap metrics", "start_pos": 79, "end_pos": 107, "type": "METRIC", "confidence": 0.917209525903066}, {"text": "Exact Match", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.7405691742897034}]}, {"text": "Binary Overlap counts every predicted answer that overlaps with a gold answer span as correct, and Proportional Overlap give partial credit proportional to the amount of overlap ().", "labels": [], "entities": [{"text": "Proportional Overlap", "start_pos": 99, "end_pos": 119, "type": "METRIC", "confidence": 0.9258969128131866}]}, {"text": "To better understand the effect of coreference resolution, we also evaluate our model and the baseline models on just that portion of the test set that requires pronoun resolution (36.42% of the examples) and show the results in.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.9641889035701752}]}, {"text": "The gaps of performance between our model and the baseline models are still significant.", "labels": [], "entities": []}, {"text": "Besides, we see that all three systems' performance drop on this partial test set, which demonstrates the hardness of generating questions for the cases that require pronoun resolution (passage context).", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.763575404882431}]}, {"text": "We also show in the results of the QG models trained on the training set augmented with noisy examples with predicted answer spans.", "labels": [], "entities": []}, {"text": "There is a consistent but acceptable drop for each model on this new training set, given the inaccuracy of predicted answer spans.", "labels": [], "entities": []}, {"text": "We see that CorefNQG still outperforms the baseline models across all metrics.", "labels": [], "entities": []}, {"text": "provides sample output for input sentences that require contextual coreference knowledge.", "labels": [], "entities": []}, {"text": "We see that ContextNQG fails in all cases; our model misses only the third example due to an error introduced by coreference resolution -the \"city\" and \"it\" are considered coreferent.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.9033284485340118}]}, {"text": "We can also see that human-generated questions are more natural and varied inform with better paraphrasing.", "labels": [], "entities": []}, {"text": "In, we show the evaluation results for different answer extraction models.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7815741002559662}]}, {"text": "First we see that all variants of BiLSTM models outperform the off-the-shelf NER system (that proposes all NEs as answer spans), though the NER system has a higher recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9988799691200256}]}, {"text": "The BiLSTM-CRF that encodes the character-level and NER features for each token performs best in terms of F-measure.", "labels": [], "entities": [{"text": "BiLSTM-CRF", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.6669977307319641}, {"text": "F-measure", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9810099005699158}]}], "tableCaptions": [{"text": " Table 2: Evaluation results for question generation.", "labels": [], "entities": [{"text": "question generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7903922498226166}]}, {"text": " Table 3: Evaluation results of answer extraction systems.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8863465785980225}]}, {"text": " Table 4: Evaluation results for question generation  on the portion that requires coreference knowledge  (36.42% examples of the original test set).", "labels": [], "entities": [{"text": "question generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7308869957923889}]}, {"text": " Table 5: Human evaluation results for question  generation. \"Grammaticality\", \"Making Sense\" and \"An-", "labels": [], "entities": [{"text": "question  generation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7603704035282135}]}, {"text": " Table 6: Performance of the neural machine read- ing comprehension model (no initialization with  pretrained embeddings) on our generated corpus.", "labels": [], "entities": []}]}