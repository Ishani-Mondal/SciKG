{"title": [{"text": "Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.962913990020752}]}], "abstractContent": [{"text": "Counterfactual learning from human bandit feedback describes a scenario where user feedback on the quality of outputs of a historic system is logged and used to improve a target system.", "labels": [], "entities": [{"text": "Counterfactual learning from human bandit feedback", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8694007098674774}]}, {"text": "We show how to apply this learning framework to neural semantic parsing.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6849002242088318}]}, {"text": "From a machine learning perspective, the key challenge lies in a proper reweighting of the estimator so as to avoid known degeneracies in coun-terfactual learning, while still being applicable to stochastic gradient optimization.", "labels": [], "entities": []}, {"text": "To conduct experiments with human users, we devise an easy-to-use interface to collect human feedback on semantic parses.", "labels": [], "entities": []}, {"text": "Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data.", "labels": [], "entities": [{"text": "semantic parsers", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7276791334152222}]}], "introductionContent": [{"text": "In semantic parsing, natural language utterances are mapped to machine readable parses which are complex and often tailored specifically to the underlying task.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.72871233522892}]}, {"text": "The cost and difficulty of manually preparing large amounts of such parses thus is a bottleneck for supervised learning in semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.8063573241233826}]}, {"text": "Recent work (;;; inter alia) has applied reinforcement learning to address the annotation bottleneck as follows: Given a question, the existence of a corresponding gold answer is assumed.", "labels": [], "entities": []}, {"text": "A semantic parser produces multiple parses per question and corresponding answers are obtained.", "labels": [], "entities": []}, {"text": "These answers are then compared against the gold answer and a positive reward is recorded if there is an overlap.", "labels": [], "entities": []}, {"text": "The parser is then guided towards correct parses using the REIN-FORCE algorithm which scales the gradient for the various parses by their obtained reward (see the left half of).", "labels": [], "entities": [{"text": "REIN-FORCE", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9957612156867981}]}, {"text": "However, learning from question-answer pairs is only efficient if gold answers are cheap to obtain.", "labels": [], "entities": []}, {"text": "For complex open-domain question-answering tasks, correct answers are not unique factoids, but openended lists, counts in large ranges, or fuzzily defined objects.", "labels": [], "entities": []}, {"text": "For example, geographical queries against databases such as OpenStreetMap (OSM) can involve fuzzy operators such as \"near\" or \"in walking distance\" and thus need to allow for fuzziness in the answers as well.", "labels": [], "entities": []}, {"text": "A possible solution lies in machine learning from even weaker supervision signals inform of human bandit feedback 1 where the semantic parsing system suggests exactly one parse for which feedback is collected from a human user.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 126, "end_pos": 142, "type": "TASK", "confidence": 0.7457999289035797}]}, {"text": "In this setup neither gold parse nor gold answer are known and feedback is obtained for only one system output per question.", "labels": [], "entities": []}, {"text": "The goal of our paper is to exploit this scenario of learning from human bandit feedback to train semantic parsers.", "labels": [], "entities": []}, {"text": "This learning scenario perfectly fits commercial setups such as virtual personal assistants that embed a semantic parser.", "labels": [], "entities": []}, {"text": "Commercial systems can easily log large amounts of interaction data between users and system.", "labels": [], "entities": []}, {"text": "Once sufficient data has been collected, the log can then be used to improve the parser.", "labels": [], "entities": []}, {"text": "This leads to a counterfactual learning scenario () where we have to solve the counterfactual problem of how to improve a target system from logged feedback that was given to the outputs of a different historic system (see the right half of).", "labels": [], "entities": []}, {"text": "In order to achieve our goal of counterfactual learning of semantic parsers from human bandit feedback, the following contributions are required:: Left: Online reinforcement learning setup for semantic parsing setup where both questions and gold answers are available.", "labels": [], "entities": [{"text": "counterfactual learning of semantic parsers from human bandit feedback", "start_pos": 32, "end_pos": 102, "type": "TASK", "confidence": 0.7843601968553331}, {"text": "semantic parsing", "start_pos": 193, "end_pos": 209, "type": "TASK", "confidence": 0.7766132950782776}]}, {"text": "The parser attempts to find correct machine readable parses (MRPs) by producing multiple parses, obtaining corresponding answers, and comparing them against the gold answer.", "labels": [], "entities": []}, {"text": "Right: In our setup, a question does not have an associated gold answer.", "labels": [], "entities": []}, {"text": "The parser outputs a single MRP and the corresponding answer is shown to a user who provides some feedback.", "labels": [], "entities": []}, {"text": "Such triplets are collected in a log which can be used for offline training of a semantic parser.", "labels": [], "entities": []}, {"text": "This scenario is called counterfactual since the feedback was logged for outputs from a system different from the target system to be optimized.", "labels": [], "entities": []}, {"text": "First, we need to construct an easy-to-use user interface that allows to collect feedback based on the parse rather than the answer.", "labels": [], "entities": []}, {"text": "To this aim, we automatically convert the parse to a set of statements that can be judged as corrector incorrect by a human.", "labels": [], "entities": []}, {"text": "This approach allows us to assign rewards at the token level, which in turn enables us to perform blame assignment in bandit learning and to learn from partially correct queries where tokens are reinforced individually.", "labels": [], "entities": [{"text": "blame assignment", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7023202180862427}]}, {"text": "We show that users can provide such feedback for one question-parse pair in 16.4 seconds on average.", "labels": [], "entities": []}, {"text": "This exemplifies that our approach is more efficient and cheaper than recruiting experts to annotate parses or asking workers to compile large answer sets.", "labels": [], "entities": []}, {"text": "Next, we demonstrate experimentally that counterfactual learning can be applied to neural sequence-to-sequence learning for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.8506387770175934}]}, {"text": "A baseline neural semantic parser is trained in fully supervised fashion, human bandit feedback from human users is collected in a log and subsequently used to improve the parser.", "labels": [], "entities": []}, {"text": "The resulting parser significantly outperforms the baseline model as well as a simple bandit-to-supervised approach (B2S) where the subset of completely correct parses is treated as a supervised dataset.", "labels": [], "entities": []}, {"text": "Finally, we repeat our experiments on a larger but simulated log to show that our gains can scale: the baseline system is improved by 7.45% in answer F1 score without ever seeing a gold standard parse.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.935050904750824}]}, {"text": "Lastly, from a machine learning perspective, we have to solve problems of degenerate behavior in counterfactual learning by lifting the multiplicative control variate technique,a) to stochastic learning for neural models.", "labels": [], "entities": []}, {"text": "This is done by reweighting target model probabilities over the logged data under a one-step-late model that decouples the normalization from gradient estimation and is thus applicable in stochastic (minibatch) gradient optimization.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we use the sequence-to-sequence neural network package NEMATUS (.", "labels": [], "entities": [{"text": "NEMATUS", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.8686794638633728}]}, {"text": "Following the method used by, we split the queries into individual tokens by taking a pre-order traversal of the original tree-like structure.", "labels": [], "entities": []}, {"text": "For example, \"query(west(area(keyval('name','Paris')), nwr(keyval('railway','station'))),qtype(count))\" becomes \"query@2 west@2 area@1 keyval@2 name@0 Paris@s nwr@1 keyval@2 railway@0 station@s qtype@1 count@0\".", "labels": [], "entities": []}, {"text": "The SGD optimizer used is ADADELTA.", "labels": [], "entities": [{"text": "ADADELTA", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8549515008926392}]}, {"text": "The model employs 1,024 hidden units and word embeddings of size 1,000.", "labels": [], "entities": []}, {"text": "The maximum sentence length is 200 and gradients are clipped if they exceed a value of 1.0.", "labels": [], "entities": []}, {"text": "The stopping point is determined by validation on the development set and selecting the point at which the highest evaluation score is obtained.", "labels": [], "entities": [{"text": "stopping point", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9590665102005005}]}, {"text": "F1 validation is run after every 100 updates, and each update is made on the basis of a minibatch of size 80.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.708735466003418}]}, {"text": "The evaluation of all models is based on the answers obtained by executing the most likely query obtained after abeam search with abeam of size 12.", "labels": [], "entities": []}, {"text": "We report the F1 score which is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.990686297416687}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9996216297149658}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.998712420463562}]}, {"text": "Recall is defined as the percentage of fully correct answers divided by the set size.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9580851197242737}]}, {"text": "Precision is the percentage of correct answers out of the set of answers with non-empty strings.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9923784732818604}]}, {"text": "Statistical significance between models is measured using an approximate randomization test.", "labels": [], "entities": []}, {"text": "Baseline Parser & Log Creation.", "labels": [], "entities": [{"text": "Parser & Log Creation", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.74911168217659}]}, {"text": "Our experiment design assumes a baseline neural semantic parser that is trained in fully supervised fashion, and is to be improved by bandit feedback obtained for system outputs from the baseline system forgiven questions.", "labels": [], "entities": []}, {"text": "For this purpose, we select 2,000 question-query pairs randomly from the full extended NLMAPS V2 corpus.", "labels": [], "entities": [{"text": "NLMAPS V2 corpus", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.9096932212511698}]}, {"text": "We will call this dataset D sup . Using this dataset, a baseline semantic parser is trained in supervised fashion under a cross-entropy objective.", "labels": [], "entities": []}, {"text": "It obtains an F1 score of 57.45% and serves as the logging policy \u03c0 0 . Furthermore we randomly split off 1,843 and 2,000 pairs fora development and test set, respectively.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9866597354412079}]}, {"text": "This leaves a set of 22,765 question-query pairs.", "labels": [], "entities": []}, {"text": "The questions can be used as input and bandit feedback can be collected for the most likely output of the semantic parser.", "labels": [], "entities": []}, {"text": "We refer to this dataset as D log . To collect human feedback, we take the first 1,000 questions from D log and use \u03c0 0 to parse these questions to obtain one output query for each.", "labels": [], "entities": []}, {"text": "5 question-query pairs are discarded because the suggested query is invalid.", "labels": [], "entities": []}, {"text": "For the remaining question-query pairs, the queries are each transformed into a block of human-understandable statements and embedded into the user interface described in Section 5.", "labels": [], "entities": []}, {"text": "We recruited 9 users to provide feedback for these question-query pairs.", "labels": [], "entities": []}, {"text": "The resulting log is referred to as D human . Every question-query pair is purposely evaluated only once to mimic a realistic real-world scenario where user logs are collected as users use the system.", "labels": [], "entities": []}, {"text": "In this scenario, it is also not possible to explicitly obtain several evaluations for the same question-query pair.", "labels": [], "entities": []}, {"text": "Some examples of the received feedback can be found in the supplementary material, section C.", "labels": [], "entities": []}, {"text": "To verify that the feedback collection is efficient, we measured the time each user took from loading a form to submitting it.", "labels": [], "entities": []}, {"text": "To provide feedback for one question-query pair, users took 16.4 seconds on average with a standard deviation of 33.2 seconds.", "labels": [], "entities": []}, {"text": "The vast majority (728 instances) are completed in less than 10 seconds.", "labels": [], "entities": []}, {"text": "Learning from Human Bandit Feedback.", "labels": [], "entities": []}, {"text": "An analysis of D human shows that for 531 queries all corresponding statements were marked as correct.", "labels": [], "entities": []}, {"text": "We consider a simple baseline that treats completely correct logged data as a supervised data set with which training continues using the crossentropy objective.", "labels": [], "entities": []}, {"text": "We call this baseline banditto-supervised conversion (B2S).", "labels": [], "entities": []}, {"text": "Furthermore, we present experimental results using the log D human for stochastic (minibatch) gradient descent optimization of the counterfactual objectives introduced in equations 4, 6, 7 and 8.", "labels": [], "entities": [{"text": "gradient descent optimization", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.7792723774909973}]}, {"text": "For the tokenlevel feedback, we map the evaluated statements back to the corresponding tokens in the original query and assign these tokens a feedback of 0 if the corresponding statement was marked as wrong and 1 otherwise.", "labels": [], "entities": []}, {"text": "In the case of sequence-level feedback, the query receives a feedback of 1 if all statements are marked correct, 0 otherwise.", "labels": [], "entities": []}, {"text": "For the OSL objectives, a separate experiment (see below) showed that updating the reweighting constant after every validation step promises the best trade-off between performance and speed.", "labels": [], "entities": [{"text": "OSL", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8644458651542664}]}, {"text": "Results, averaged over 3 runs, are reported in.", "labels": [], "entities": []}, {"text": "The B2S model can slightly improve upon the baseline but not significantly.", "labels": [], "entities": []}, {"text": "DPM improves further, significantly beating the baseline.", "labels": [], "entities": [{"text": "DPM", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7282703518867493}]}, {"text": "Using the multiplicative control variate modified for SGD by OSL updates does not seem to help in this setup.", "labels": [], "entities": []}, {"text": "By moving to token-level rewards, it is possible to learn from partially correct queries.", "labels": [], "entities": []}, {"text": "These partially correct queries provide valuable information that is not present in the subset of correct answers employed by the previous models.", "labels": [], "entities": []}, {"text": "Optimizing DPM+T leads to a slight improvement and combined with the multiplicative control variate, DPM+T+OSL yields an improvement of about 1.0 in F1 score upon the baseline.", "labels": [], "entities": [{"text": "T", "start_pos": 15, "end_pos": 16, "type": "METRIC", "confidence": 0.5628586411476135}, {"text": "OSL", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.8446751832962036}, {"text": "F1 score", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9846022725105286}]}, {"text": "It beats both the baseline and the B2S model by a significant margin.", "labels": [], "entities": []}, {"text": "Learning from Large-Scale Simulated Feedback.", "labels": [], "entities": []}, {"text": "We want to investigate whether the results scale if a larger log is used.", "labels": [], "entities": []}, {"text": "Thus, we use \u03c0 0 to parse all 22,765 questions from D log and obtain for each an output query.", "labels": [], "entities": []}, {"text": "For sequence level rewards, we assign feedback of 1 fora query if it is identical to the true target query, 0 otherwise.", "labels": [], "entities": []}, {"text": "We also simulate token-level rewards by iterating over the indices of the output and assigning a feedback of 1 if the same token appears at the current index for the true target query, 0 otherwise.", "labels": [], "entities": []}, {"text": "This subset is used to train a bandit-to-supervised (B2S) model using the cross-entropy objective.", "labels": [], "entities": []}, {"text": "Experimental results for the various optimization setups, averaged over 3 runs, are reported in.", "labels": [], "entities": []}, {"text": "We see that the B2S model outperforms the baseline model by a large margin, yielding an increase in F1 score by 6.24 points.", "labels": [], "entities": [{"text": "B2S", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9117990732192993}, {"text": "F1 score", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9794754683971405}]}, {"text": "Optimizing the DPM objective also yields a significant increase over the baseline, but its performance falls short of the stronger B2S baseline.", "labels": [], "entities": [{"text": "DPM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8430824279785156}]}, {"text": "Optimizing the DPM+OSL objective leads to a substantial improvement in F1 score over optimizing DPM but still falls slightly short of the strong B2S baseline.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9846518337726593}]}, {"text": "Token-level rewards are again crucial to beat the B2S baseline significantly.", "labels": [], "entities": [{"text": "B2S baseline", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.6482757180929184}]}, {"text": "DPM+T is already able to significantly outperform B2S in this setup and DPM+T+OSL can improve upon this further.: Analysis of which type of errors DPM+T+OSL corrected on the test set compared to the baseline system for both human and simulated feedback experiments.", "labels": [], "entities": []}, {"text": "tained the correct answer and the baseline system did not (see).", "labels": [], "entities": []}, {"text": "The analysis showed that the vast majority of previously wrong queries were fixed by correcting an OSM tag in the query.", "labels": [], "entities": []}, {"text": "For example, for the question \"closest Florist from Manchester in walking distance\" the baseline system chose the tag \"landuse : retail\" in the query, whereas DPM+T+OSL learnt that the correct tag is \"shop : florist\".", "labels": [], "entities": []}, {"text": "In some cases, the question type had to be corrected, e.g. the baseline's suggested query returned the location of a point of interest but DPM+T+OSL correctly returns the phone number.", "labels": [], "entities": [{"text": "DPM+T+OSL", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.4656642317771912}]}, {"text": "Finally, in a few cases DPM+T+OSL corrected the structure fora query, e.g. by searching fora point of interest in the east of an area rather than the south.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Corpus statistics of the question- answering corpora NLMAPS and our extension  NLMAPS V2 which additionally contains the  search engine style queries (Lawrence and Riezler,  2016) and the automatic extensions of the most  common OSM tags.", "labels": [], "entities": [{"text": "question- answering corpora NLMAPS", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7005760908126831}]}, {"text": " Table 3: Human Feedback: Answer F1 scores on  the test set for the various setups, averaged over 3  runs. Statistical significance of system differences  at p < 0.05 are indicated by experiment number  in superscript.", "labels": [], "entities": [{"text": "Answer", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9769842028617859}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.5520555377006531}]}, {"text": " Table 4: Simulated Feedback: Answer F1 scores  on the test set for the various setups, averaged over  3 runs. Statistical significance of system differ- ences at p < 0.05 are indicated by experiment  number in superscript.", "labels": [], "entities": [{"text": "Answer F1", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.8075112700462341}]}, {"text": " Table 5: Analysis of which type of errors  DPM+T+OSL corrected on the test set compared  to the baseline system for both human and simu- lated feedback experiments.", "labels": [], "entities": [{"text": "T", "start_pos": 48, "end_pos": 49, "type": "METRIC", "confidence": 0.7830156087875366}, {"text": "OSL", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.5288854837417603}]}, {"text": " Table 6: Simulated Feedback: Answer F1 scores  on the test set for DPM+T and DPM+T+OSL with  varying OSL update strategies, averaged over 3  runs. Updating after every minibatch is infeasible  as it significantly slows down learning. Statistical  significance of system differences at p < 0.05 oc- cur for experiment 4 over experiment 2.", "labels": [], "entities": [{"text": "Answer F1", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.8445633351802826}]}]}