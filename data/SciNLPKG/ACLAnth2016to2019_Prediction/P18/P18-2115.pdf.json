{"title": [], "abstractContent": [{"text": "Most of the current abstractive text sum-marization models are based on the sequence-to-sequence model (Seq2Seq).", "labels": [], "entities": []}, {"text": "The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation.", "labels": [], "entities": []}, {"text": "Compared with the source content, the annotated summary is short and well written.", "labels": [], "entities": []}, {"text": "Moreover, it shares the same meaning as the source content.", "labels": [], "entities": []}, {"text": "In this work, we supervise the learning of the representation of the source content with that of the summary.", "labels": [], "entities": []}, {"text": "In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq.", "labels": [], "entities": []}, {"text": "Following previous work, we evaluate our model on a popular Chinese social media dataset.", "labels": [], "entities": [{"text": "Chinese social media dataset", "start_pos": 60, "end_pos": 88, "type": "DATASET", "confidence": 0.6336004361510277}]}, {"text": "Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text summarization is to produce a brief summary of the main ideas of the text.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7371154427528381}]}, {"text": "Unlike extractive text summarization (, which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries.", "labels": [], "entities": [{"text": "extractive text summarization", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.6433571378389994}, {"text": "abstractive text summarization", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.6577763557434082}]}, {"text": "Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.605800857146581}]}, {"text": "The contents on the social media are long, and contain many errors, which come from spelling mistakes, informal expressions, and grammatical mistakes (.", "labels": [], "entities": []}, {"text": "Large amount of errors in the contents cause great difficulties for text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7545771598815918}]}, {"text": "As for RNN-based Seq2Seq, it is difficult to compress along sequence into an accurate representation (), because of the gradient vanishing and exploding problem.", "labels": [], "entities": []}, {"text": "Compared with the source content, it is easier to encode the representations of the summaries, which are short and manually selected.", "labels": [], "entities": []}, {"text": "Since the source content and the summary share the same points, it is possible to supervise the learning of the semantic representation of the source content with that of the summary.", "labels": [], "entities": []}, {"text": "In this paper, we regard a summary autoencoder as an assistant supervisor of Seq2Seq.", "labels": [], "entities": []}, {"text": "First, we train an autoencoder, which inputs and reconstructs the summaries, to obtain a better representation to generate the summaries.", "labels": [], "entities": []}, {"text": "Then, we supervise the internal representation of Seq2Seq with that of autoencoder by minimizing the distance between two representations.", "labels": [], "entities": []}, {"text": "Finally, we use adversarial learning to enhance the supervision.", "labels": [], "entities": []}, {"text": "Following the previous work (, We evaluate our proposed model on a Chinese social media dataset.", "labels": [], "entities": [{"text": "Chinese social media dataset", "start_pos": 67, "end_pos": 95, "type": "DATASET", "confidence": 0.672183483839035}]}, {"text": "Experimental results show that our model outperforms the state-of-theart baseline models.", "labels": [], "entities": []}, {"text": "More specifically, our model outperforms the Seq2Seq baseline by the score of 7.1 ROUGE-1, 6.1 ROUGE-2, and 7.0 ROUGE-L.", "labels": [], "entities": [{"text": "Seq2Seq baseline", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.8283810615539551}, {"text": "ROUGE-1", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.984767735004425}, {"text": "ROUGE-2", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9375284314155579}]}], "datasetContent": [{"text": "Following the previous work (, we evaluate our model on a popular Chinese social media dataset.", "labels": [], "entities": [{"text": "Chinese social media dataset", "start_pos": 66, "end_pos": 94, "type": "DATASET", "confidence": 0.6074896827340126}]}, {"text": "We first introduce the datasets, evaluation metrics, and experimental details.", "labels": [], "entities": []}, {"text": "Then, we compare our model with several state-of-the-art systems.", "labels": [], "entities": []}, {"text": "Large Scale Chinese Social Media Text Summarization Dataset (LCSTS) is constructed by.", "labels": [], "entities": [{"text": "Chinese Social Media Text Summarization", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.6281320214271545}]}, {"text": "The dataset consists of more than 2,400,000 text-summary pairs, constructed from a famous Chinese social media website called Sina Weibo.", "labels": [], "entities": []}, {"text": "It is split into three parts, with 2,400,591 2 http://weibo.com pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III.", "labels": [], "entities": []}, {"text": "All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5.", "labels": [], "entities": []}, {"text": "We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III.", "labels": [], "entities": [{"text": "PART", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.8878659605979919}, {"text": "PART", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8773482441902161}]}, {"text": "Following the previous work, we use PART I as training set, PART II as validation set, and PART III as test set.", "labels": [], "entities": []}, {"text": "Our evaluation metric is ROUGE score (, which is popular for summarization evaluation.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9832657873630524}, {"text": "summarization evaluation", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.942190557718277}]}, {"text": "The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS).", "labels": [], "entities": [{"text": "longest common subsequence (LCS)", "start_pos": 164, "end_pos": 196, "type": "METRIC", "confidence": 0.7868958314259847}]}, {"text": "Following previous work, we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9647943377494812}, {"text": "ROUGE-2", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9640916585922241}, {"text": "ROUGE-L (LCS)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9417876750230789}]}, {"text": "The vocabularies are extracted from the training sets, and the source contents and the summaries share the same vocabularies.", "labels": [], "entities": []}, {"text": "In order to alleviate the risk of word segmentation mistakes, we split the Chinese sentences into characters.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7142898887395859}]}, {"text": "We prune the vocabulary size to 4,000, which covers most of the common characters.", "labels": [], "entities": []}, {"text": "We tune the hyper-parameters based on the ROUGE scores on the validation sets.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9952967762947083}]}, {"text": "We set the word embedding size and the hidden size to 512, and the number of LSTM layers is 2.", "labels": [], "entities": []}, {"text": "The batch size is 64, and we do not use dropout) on this dataset.", "labels": [], "entities": []}, {"text": "Following the previous work ( , we implement the beam search, and set the beam size to 10.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison with state-of-the-art models  on the LCSTS test set. R-1, R-2, and R-L de- note ROUGE-1, ROUGE-2, and ROUGE-L, re- spectively. The models with a suffix of 'W' in the  table are word-based, while the rest of models are  character-based.", "labels": [], "entities": [{"text": "LCSTS test set", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.9637005925178528}]}]}