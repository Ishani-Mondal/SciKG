{"title": [{"text": "Constituency Parsing with a Self-Attentive Encoder", "labels": [], "entities": [{"text": "Constituency Parsing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.787248969078064}]}], "abstractContent": [{"text": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser.", "labels": [], "entities": []}, {"text": "The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements.", "labels": [], "entities": []}, {"text": "For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 108, "end_pos": 115, "type": "TASK", "confidence": 0.9742196798324585}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.8810162544250488}]}, {"text": "Additionally, we evaluate different approaches for lexical representation.", "labels": [], "entities": [{"text": "lexical representation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7202290743589401}]}, {"text": "Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9950017631053925}, {"text": "F1", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9976968169212341}, {"text": "F1", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9981688261032104}]}, {"text": "Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.998011589050293}, {"text": "SPMRL dataset", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.8688165545463562}]}], "introductionContent": [{"text": "In recent years, neural network approaches have led to improvements in constituency parsing).", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.8999027609825134}]}, {"text": "Many of these parsers can broadly be characterized as following an encoder-decoder design: an encoder reads the input sentence and summarizes it into a vector or set of vectors (e.g. one for each word or span in the sentence), and then a decoder uses these vector summaries to incrementally buildup a labeled parse tree.", "labels": [], "entities": []}, {"text": "In contrast to the large variety of decoder architectures investigated in recent work, the encoders in recent parsers have predominantly been built using recurrent neural networks (RNNs), and in particular Long Short-Term Memory networks (LSTMs).", "labels": [], "entities": []}, {"text": "RNNs have largely replaced approaches such as the fixed-window-size feed-forward networks of in part due to their ability to capture global context.", "labels": [], "entities": []}, {"text": "However, RNNs are not the only architecture capable of summarizing large global contexts: recent work by presented anew state-of-the-art approach to machine translation with an architecture that entirely eliminates recurrent connections and relies instead on a repeated neural attention mechanism.", "labels": [], "entities": [{"text": "summarizing large global contexts", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.9065007269382477}, {"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.7735898494720459}]}, {"text": "In this paper, we introduce a parser that combines an encoder built using this kind of self-attentive architecture with a decoder customized for parsing ().", "labels": [], "entities": []}, {"text": "In Section 2 of this paper, we describe the architecture and present our finding that self-attention can outperform an LSTM-based approach.", "labels": [], "entities": []}, {"text": "A neural attention mechanism makes explicit the manner in which information is transferred between different locations in the sentence, which we can use to study the relative importance of different kinds of context to the parsing task.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 223, "end_pos": 235, "type": "TASK", "confidence": 0.9075210392475128}]}, {"text": "Different locations in the sentence can attend to each other based on their positions, but also based on their contents (i.e. based on the words at or around those positions).", "labels": [], "entities": []}, {"text": "In Section 3 we present our find-ing that when our parser learns to make an implicit trade-off between these two types of attention, it predominantly makes use of position-based attention, and show that explicitly factoring the two types of attention can noticeably improve parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 274, "end_pos": 281, "type": "TASK", "confidence": 0.9714764952659607}, {"text": "accuracy", "start_pos": 282, "end_pos": 290, "type": "METRIC", "confidence": 0.8870627284049988}]}, {"text": "In Section 4, we study our model's use of attention and reaffirm the conventional wisdom that sentence-wide global context is important for parsing decisions.", "labels": [], "entities": [{"text": "parsing decisions", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.8991646468639374}]}, {"text": "Like inmost neural parsers, we find morphological (or at least sub-word) features to be important to achieving good results, particularly on unseen words or inflections.", "labels": [], "entities": []}, {"text": "In Section 5.1, we demonstrate that a simple scheme based on concatenating character embeddings of word prefixes/suffixes can outperform using part-of-speech tags from an external system.", "labels": [], "entities": []}, {"text": "We also present aversion of our model that uses a character LSTM, which performs better than other lexical representationseven if word embeddings are removed from the model.", "labels": [], "entities": []}, {"text": "In Section 5.2, we explore an alternative approach for lexical representations that makes use of pre-training on a large unsupervised corpus.", "labels": [], "entities": []}, {"text": "We find that using the deep contextualized representations proposed by can boost parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.9719429016113281}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9742685556411743}]}, {"text": "Our parser achieves 93.55 F1 on the Penn Treebank WSJ test set when not using external word representations, outperforming all previous singlesystem constituency parsers trained only on the WSJ training set.", "labels": [], "entities": [{"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.98064124584198}, {"text": "Penn Treebank WSJ test set", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.9628422617912292}, {"text": "WSJ training set", "start_pos": 190, "end_pos": 206, "type": "DATASET", "confidence": 0.973137617111206}]}, {"text": "The addition of pre-trained word representations following increases parsing accuracy to 95.13 F1, anew stateof-the-art for this dataset.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9498468041419983}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9778283834457397}, {"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9843674302101135}]}, {"text": "Our model also outperforms previous best published results on 8 of the 9 languages in the SPMRL 2013/2014 shared tasks.", "labels": [], "entities": [{"text": "SPMRL 2013/2014 shared tasks", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.5981595714886984}]}, {"text": "Code and trained English models are publicly available.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Development-set F1 scores when atten- tion is constrained to not exceed a particular dis- tance in the sentence at test time only. In the re- laxed setting, the first and last two tokens of the  sentence can attend to any word and be attended  to by any word, to allow for sentence-wide pool- ing of information.", "labels": [], "entities": [{"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.8050811290740967}, {"text": "sentence-wide pool- ing of information", "start_pos": 283, "end_pos": 321, "type": "TASK", "confidence": 0.6696695983409882}]}, {"text": " Table 3: Development-set F1 scores when atten- tion is constrained to not exceed a particular dis- tance in the sentence during training and at test  time. In the relaxed setting, the first and last two  tokens of the sentence can attend to any word and  be attended to by any word, to allow for sentence- wide pooling of information.", "labels": [], "entities": [{"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.8123753070831299}]}, {"text": " Table 4: Development-set F1 scores for differ- ent approaches to handling morphology, with and  without the addition of learned word embeddings.", "labels": [], "entities": [{"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9524822235107422}]}, {"text": " Table 5: A comparison of different encoder ar- chitectures and their development-set performance  relative to our base self-attentive model.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of F1 scores on the WSJ test  set.", "labels": [], "entities": [{"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9884783625602722}, {"text": "WSJ test  set", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9874526460965475}]}, {"text": " Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Bj\u00f6rkelund et al. (2013) b Uses character LSTM, whereas other results  from", "labels": [], "entities": [{"text": "SPMRL dataset", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8296625018119812}, {"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9983413219451904}]}]}