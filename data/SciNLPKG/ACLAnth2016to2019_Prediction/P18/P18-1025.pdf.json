{"title": [{"text": "Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures", "labels": [], "entities": []}], "abstractContent": [{"text": "Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE) (Vendrov et al., 2016), area natural way to model transitive relational data (e.g. entailment graphs).", "labels": [], "entities": []}, {"text": "However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations).", "labels": [], "entities": [{"text": "OE", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.7294482588768005}]}, {"text": "Probabilistic extensions of OE (Lai and Hockenmaier, 2017) have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge.", "labels": [], "entities": [{"text": "consistency", "start_pos": 160, "end_pos": 171, "type": "METRIC", "confidence": 0.9753459095954895}]}, {"text": "In this work we show that abroad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anticorrelation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts , and both learning from and predicting calibrated uncertainty.", "labels": [], "entities": []}, {"text": "We show improvements over previous approaches in modeling the Flickr and WordNet entail-ment graphs, and investigate the power of the model.", "labels": [], "entities": [{"text": "Flickr and WordNet entail-ment graphs", "start_pos": 62, "end_pos": 99, "type": "DATASET", "confidence": 0.7828113913536072}]}], "introductionContent": [{"text": "Structured embeddings based on regions, densities, and orderings have gained popularity in recent years for their inductive bias towards the essential asymmetries inherent in problems such as image captioning, lexical and textual entailment, and knowledge graph completion and reasoning.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 192, "end_pos": 208, "type": "TASK", "confidence": 0.737635999917984}, {"text": "knowledge graph completion", "start_pos": 246, "end_pos": 272, "type": "TASK", "confidence": 0.6386131942272186}]}, {"text": "Models that easily encode asymmetry, and related properties such as transitivity (the two components of commonplace relations such as partially ordered sets and lattices), have great utility in these applications, leaving less to be learned from the data than arbitrary relational models.", "labels": [], "entities": []}, {"text": "At their best, they resemble a hybrid between embedding models and structured prediction.", "labels": [], "entities": []}, {"text": "As noted by and, while the models learn sets of embeddings, these parameters obey rich structural constraints.", "labels": [], "entities": []}, {"text": "The entire set can bethought of as one, sometimes provably consistent, structured prediction, such as an ontology in the form of a single directed acyclic graph.", "labels": [], "entities": []}, {"text": "While the structured prediction analogy applies best to Order Embeddings (OE), which embeds consistent partial orders, other region-and density-based representations have been proposed for the express purpose of inducing a bias towards asymmetric relationships.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6679639965295792}]}, {"text": "For example, the Gaussian Embedding (GE) model) aims to represent the asymmetry and uncertainty in an object's relations and attributes by means of uncertainty in the representation.", "labels": [], "entities": []}, {"text": "However, while the space of representations is a manifold of probability distributions, the model is not truly probabilistic in that it does not model asymmetries and relations in terms of prob-abilities, but in terms of asymmetric comparison functions such as the originally proposed KL divergence and the recently proposed thresholded divergences).", "labels": [], "entities": []}, {"text": "Probabilistic models are especially compelling for modeling ontologies, entailment graphs, and knowledge graphs.", "labels": [], "entities": []}, {"text": "Their desirable properties include an ability to remain consistent in the presence of noisy data, suitability towards semisupervised training using the expectations and uncertain labels present in these large-scale applications, the naturality of representing the inherent uncertainty of knowledge they store, and the ability to answer complex queries involving more than 2 variables.", "labels": [], "entities": []}, {"text": "Note that the final one requires a true joint probabilistic model with a tractable inference procedure, not something provided by e.g. matrix factorization.", "labels": [], "entities": []}, {"text": "We take the dual approach to density-based embeddings and model uncertainty about relationships and attributes as explicitly probabilistic, while basing the probability on a latent space of geometric objects that obey natural structural biases for modeling transitive, asymmetric relations.", "labels": [], "entities": []}, {"text": "The most similar work are the probabilistic order embeddings (POE) of, which apply a probability measure to each order embedding's forward cone (the set of points greater than the embedding in each dimension), assigning a finite and normalized volume to the unbounded space.", "labels": [], "entities": []}, {"text": "However, POE suffers severe limitations as a probabilistic model, including an inability to model negative correlations between concepts, which motivates the construction of our box lattice model.", "labels": [], "entities": []}, {"text": "Our model represents objects, concepts, and events as high-dimensional products-of-intervals (hyperrectangles or boxes), with an event's unary probability coming from the box volume and joint probabilities coming from overlaps.", "labels": [], "entities": []}, {"text": "This contrasts with POE's approach of defining events as the forward cones of vectors, extending to infinity, integrated under a probability measure that assigns them finite volume.", "labels": [], "entities": [{"text": "POE", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.7861571311950684}]}, {"text": "One desirable property of a structured representation for ordered data, originally noted in) is a \"slackness\" shared by OE, POE, and our model: when the model predicts an \"edge\" or lack thereof (i.e. P (a|b) = 0 or 1, or a zero constraint violation in the case of OE), being exposed to that fact again will not update the model.", "labels": [], "entities": [{"text": "OE", "start_pos": 120, "end_pos": 122, "type": "DATASET", "confidence": 0.7380089163780212}]}, {"text": "Moreover, there are large degrees of freedom in parameter space that exhibit this slackness, giving the model the ability to embed complex structure with 0 loss when compared to models based on symmetric inner products or distances between embeddings, e.g. bilinear), Trans-E (, and other embedding models which must always be pushing and pulling parameters towards and away from each other.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate the power of our approach to probabilistic ordering-biased relational modeling.", "labels": [], "entities": []}, {"text": "First, we investigate an instructive 2-dimensional toy dataset that both demonstrates the way the model self organizes its box event space, and enables sensible answers to queries involving arbitrary numbers of variables, despite being trained on only pairwise data.", "labels": [], "entities": []}, {"text": "We achieve anew state of the art in denotational probability modeling on the Flickr entailment dataset, and a matching state-of-the-art on WordNet hypernymy ( with the concurrent work on thresholded Gaussian embedding of, achieving our best results by training on additional co-occurrence expectations aggregated from leaf types.", "labels": [], "entities": [{"text": "denotational probability modeling", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.6400349040826162}, {"text": "Flickr entailment dataset", "start_pos": 77, "end_pos": 102, "type": "DATASET", "confidence": 0.9605573813120524}, {"text": "WordNet hypernymy", "start_pos": 139, "end_pos": 156, "type": "DATASET", "confidence": 0.9425672292709351}]}, {"text": "We find that the strong empirical performance of probabilistic ordering models, and our box lattice model in particular, and their endowment of new forms of training and querying, make them a promising avenue for future research in representing structured knowledge.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Negatively correlated variables produced  by the model.", "labels": [], "entities": []}, {"text": " Table 3: Classification accuracy on WordNet test  set.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.955075204372406}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9702189564704895}, {"text": "WordNet test  set", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.9855336745580038}]}, {"text": " Table 4: KL and Pearson correlation between  model and gold probability.", "labels": [], "entities": [{"text": "KL", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.6651579141616821}, {"text": "Pearson correlation", "start_pos": 17, "end_pos": 36, "type": "METRIC", "confidence": 0.9259639978408813}]}]}