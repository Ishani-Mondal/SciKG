{"title": [{"text": "Learning Translations via Images with a Massively Multilingual Image Dataset", "labels": [], "entities": [{"text": "Learning Translations", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.697188600897789}]}], "abstractContent": [{"text": "We conduct the most comprehensive study to date into translating words via images.", "labels": [], "entities": [{"text": "translating words via images", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.8798820823431015}]}, {"text": "To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents.", "labels": [], "entities": []}, {"text": "Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings.", "labels": [], "entities": []}, {"text": "In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages.", "labels": [], "entities": []}, {"text": "We run experiments on a dozen high resource languages and 20 low resources languages , demonstrating the effect of word concreteness and part-of-speech on translation quality.", "labels": [], "entities": []}, {"text": "To improve image-based translation, we introduce a novel method of predicting word concreteness from images , which improves on a previous state-of-the-art unsupervised technique.", "labels": [], "entities": [{"text": "image-based translation", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.6641968637704849}, {"text": "predicting word concreteness from images", "start_pos": 67, "end_pos": 107, "type": "TASK", "confidence": 0.880009138584137}]}, {"text": "This allows us to predict when image-based translation maybe effective, enabling consistent improvements to a state-of-the-art text-based word translation system.", "labels": [], "entities": [{"text": "image-based translation", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7360173761844635}, {"text": "word translation", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.7097640633583069}]}, {"text": "Our code and the Massively Multilingual Image Dataset (MMID) are available at http: //multilingual-images.org/.", "labels": [], "entities": [{"text": "Massively Multilingual Image Dataset (MMID)", "start_pos": 17, "end_pos": 60, "type": "DATASET", "confidence": 0.7033125971044812}]}], "introductionContent": [{"text": "Learning the translations of words is important for machine translation and other tasks in natural language processing.", "labels": [], "entities": [{"text": "Learning the translations of words", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7710847496986389}, {"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8189277648925781}]}, {"text": "Typically this learning is done using sentence-aligned bilingual parallel texts.", "labels": [], "entities": []}, {"text": "However, for many languages, there are not * These authors contributed equally; listed alphabetically.", "labels": [], "entities": []}, {"text": "sufficiently large parallel texts to effectively learn translations.", "labels": [], "entities": []}, {"text": "In this paper, we explore the question of whether it is possible to learn translations with images.", "labels": [], "entities": []}, {"text": "We systematically explore an idea originally proposed by Bergsma and Van Durme (2011): translations can be identified via images associated with words in different languages that have a high degree of visual similarity.", "labels": [], "entities": []}, {"text": "Most previous image datasets compiled for the task of learning translations were limited to the translation of nouns in a few high-resource languages.", "labels": [], "entities": []}, {"text": "In this work, we present anew large-scale dataset that contains images for 100 languages, and is not restricted by part-of-speech.", "labels": [], "entities": []}, {"text": "We collected images using Google Image Search for up to 10,000 words in each of 100 foreign languages, and their English translations.", "labels": [], "entities": []}, {"text": "For each word, we collected up to 100 images and the text on images' corresponding web pages.", "labels": [], "entities": []}, {"text": "We conduct abroad range of experiments to evaluate the utility of image features across a number of factors: \u2022 We evaluate on 12 high-resource and 20 lowresource languages.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate translation quality stratified by part-of-speech, finding that nouns and adjectives are translated with much higher accuracy than adverbs and verbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9924361109733582}]}, {"text": "\u2022 We present a novel method for predicting word concreteness from image features that better correlates with human perception than existing methods.", "labels": [], "entities": [{"text": "predicting word concreteness", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8595773975054423}]}, {"text": "We show that choosing concrete subsets of words to translate results in higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9989708662033081}]}, {"text": "\u2022 We augment a state-of-the-art text-based word translation system with image feature scores and find consistent improvements to the textonly system, ranging from 3.12% absolute top-1 accuracy improvement at 10% recall to 1.30% absolute improvement at 100% recall.", "labels": [], "entities": [{"text": "word translation", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.6992230713367462}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.766825795173645}, {"text": "recall", "start_pos": 212, "end_pos": 218, "type": "METRIC", "confidence": 0.985421895980835}, {"text": "recall", "start_pos": 257, "end_pos": 263, "type": "METRIC", "confidence": 0.9849686026573181}]}, {"text": "A further contribution of this paper is our dataset, which is the largest of its kind and should be a standard for future work in learning translations from images.", "labels": [], "entities": [{"text": "learning translations from images", "start_pos": 130, "end_pos": 163, "type": "TASK", "confidence": 0.7687443941831589}]}, {"text": "The dataset may facilitate research into multilingual, multimodal models, and translation of low-resource languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "By using a dataset scraped from the web, we expect some fraction of the images for each word to be incorrectly labeled.", "labels": [], "entities": []}, {"text": "To confirm the overall quality of our dataset, we asked human evaluators on Amazon Mechanical Turk to label a subset of the images returned by queries in four languages: our target language, English; a representative highresource language, French; and two low-resource languages, Indonesian and Uzbek.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.9307613571484884}]}, {"text": "In total, we collected 36,050 judgments of whether the images returned by Google Image Search were a good match for the keyword.", "labels": [], "entities": []}, {"text": "Details on the experimental setup can be found in Section 1 of the Supplemental Materials.", "labels": [], "entities": [{"text": "Section 1 of the Supplemental Materials", "start_pos": 50, "end_pos": 89, "type": "DATASET", "confidence": 0.608893096446991}]}, {"text": "shows the fraction of images that were judged to be good representations of the search word.", "labels": [], "entities": []}, {"text": "It also demonstrates that as the concreteness of a word increases, the proportion of good images associated with that word increases as well.", "labels": [], "entities": []}, {"text": "We further discuss the role of concreteness in Section 6.1.", "labels": [], "entities": []}, {"text": "Overall, 85% of the English images, 72% of French, 66% of Indonesian, and 60% of Uzbek were judged to be good.", "labels": [], "entities": []}, {"text": "Can images be used to translate words other than nouns?", "labels": [], "entities": []}, {"text": "This section presents our methods for de- across the same 10 languages.", "labels": [], "entities": [{"text": "de- across", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.9489919145901998}]}, {"text": "termining part-of-speech for foreign words even in low-resource languages, and presents our imagebased translation results across part-of-speech.", "labels": [], "entities": []}, {"text": "Can we effectively predict the concreteness of words in a variety of languages?", "labels": [], "entities": []}, {"text": "If so, can these predictions be used to determine when translation via images is helpful?", "labels": [], "entities": []}, {"text": "In this section, we answer both of these questions in the affirmative.", "labels": [], "entities": []}, {"text": "We evaluate our text-based and image-based combination method by translating Bosnian, Dutch, French, Indonesian, Italian, and Spanish into English.", "labels": [], "entities": []}, {"text": "For each language, we split our bilingual dictionary (of 8,673 entries, on average) into 2,000 entries fora testing set, 20% for training the textbased BPR system, 35% for training the reranking MLP, and the rest fora development set.", "labels": [], "entities": []}, {"text": "We filtered out multi-word phrases, and translations where w f and we are string identical.", "labels": [], "entities": []}, {"text": "We compare three models: TXT is's text-based state-of-the-art model.", "labels": [], "entities": []}, {"text": "TXT+IMG is our MLP-learned combination of the two features.", "labels": [], "entities": [{"text": "TXT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7722315192222595}, {"text": "IMG", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8969021439552307}]}, {"text": "TXT+IMG+CNC uses our predicted concreteness of the foreign word as well.", "labels": [], "entities": [{"text": "TXT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8821820020675659}]}, {"text": "We evaluate all models on varying percents of testing data sorted by predicted concreteness, as in Section 6.2.", "labels": [], "entities": []}, {"text": "As shown in, both imageaugmented methods beat TXT across concreteness thresholds on the top-1 accuracy metric.", "labels": [], "entities": [{"text": "TXT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.6942635774612427}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9842079281806946}]}, {"text": "Results across the 6 languages are reported in.", "labels": [], "entities": []}, {"text": "Confirming our intuition, images are useful at high concreteness, improving the SOA textbased method 3.21% at 10% recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9971254467964172}]}, {"text": "At 100% recall our method with images still improves the SOA by 1.3%.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9989957213401794}, {"text": "SOA", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.985539972782135}]}, {"text": "For example, the text-only system translates the Bosnian word ko\u0161arka\u0161ki incorrectly as football, whereas the image+text system produces the correct basketball.", "labels": [], "entities": []}, {"text": "Further, gains are more pronounced for lowresource languages than for high-resource languages.", "labels": [], "entities": []}, {"text": "Concreteness scores are useful for highresource languages, for example Spanish, where TXT+IMG falls below TXT alone on more abstract words, but TXT+IMG+CNC remains an improvement.", "labels": [], "entities": []}, {"text": "Finally, we note that the text-only system also performs better on concrete words than abstract words, indicating a general trend of ease in translating concrete words regardless of method.", "labels": [], "entities": []}, {"text": "The MMID will be distributed both in raw form and fora subset of languages in memory compact featurized versions from http: //multilingual-images.org along with code we used in our experiments.", "labels": [], "entities": []}, {"text": "Additional details are given in our Supplemental Materials document, which also describes our manual image annotation setup, and gives numerous illustrative examples of our system's predictions.", "labels": [], "entities": [{"text": "Supplemental Materials document", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.7704201440016428}]}], "tableCaptions": [{"text": " Table 1: The proportion of images determined to be good  representations of their corresponding word. In columns 2-5,  we bucket the results by the word's ground-truth concreteness,  while column 6 shows the results over all words. The last row  shows the number of words in each bucket of concreteness,  and the number of words overall for each language.", "labels": [], "entities": []}, {"text": " Table 2: Our results are consistently better than those reported  by Kiela et al. (2015), averaged over Dutch, French, German,  Italian, and Spanish on a similar set of 500 concrete nouns.  The rightmost column shows the added challenge with our  larger, more realistic dataset.", "labels": [], "entities": []}, {"text": " Table 3: Top-10 accuracy on 12 high-resource languages  and 20 low-resource languages. The parts of speech Noun,  Adjective, Adverb, and Verb are referred to as NN, JJ, RB, VB,  respectively. The \"all\" column reports accuracy on the entire  dictionary. The \"#\" column reports the size of the English  vocabulary used for each experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9949806332588196}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9990278482437134}]}, {"text": " Table 4: Top-10 accuracy on the expanded English dictionary  task. For each experiment, 263,102 English words were used  as candidate translations for each foreign word. The SMALL  average is given for reference, averaging the results from", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9995599389076233}, {"text": "SMALL  average", "start_pos": 175, "end_pos": 189, "type": "METRIC", "confidence": 0.6873511672019958}]}, {"text": " Table 5: Top-1 accuracy results across high-resource (Dutch,  French, Italian, Spanish) and low-resource (Bosnian, Indone- sian) languages. Words evaluated on are again sorted by con- creteness for the sake of analysis. The best result on each %  of test data is bolded.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9790913462638855}]}]}