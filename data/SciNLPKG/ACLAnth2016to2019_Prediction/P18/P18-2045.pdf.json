{"title": [{"text": "Narrative Modeling with Memory Chains and Semantic Supervision", "labels": [], "entities": [{"text": "Narrative Modeling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9123435914516449}]}], "abstractContent": [{"text": "Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task.", "labels": [], "entities": [{"text": "Story comprehension", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8405061066150665}]}, {"text": "Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect.", "labels": [], "entities": [{"text": "ROC Story Cloze Test", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.5788507089018822}]}, {"text": "Evaluated on the task of story ending prediction , our model demonstrates superior performance to a collection of competitive baselines, setting anew state of the art.", "labels": [], "entities": [{"text": "story ending prediction", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8319424589474996}]}], "introductionContent": [{"text": "Story narrative comprehension has been a longstanding challenge in artificial intelligence).", "labels": [], "entities": [{"text": "Story narrative comprehension", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8455344835917155}]}, {"text": "The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour.", "labels": [], "entities": []}, {"text": "Of particular interest in this paper is the work by on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see).", "labels": [], "entities": []}, {"text": "Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (, or representation learning via deep learning models.", "labels": [], "entities": []}, {"text": "Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain.", "labels": [], "entities": []}, {"text": "The current state-of-the-art approach of is based on understanding the context from three perspectives: (1) event sequence, (2) sentiment trajectory, and (3) topic consistency.", "labels": [], "entities": [{"text": "sentiment trajectory", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.7992807030677795}]}, {"text": "adopt external tools to recognise relevant aspect-triggering words, and manually design features to incorporate them into the classifier.", "labels": [], "entities": []}, {"text": "While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation.", "labels": [], "entities": []}, {"text": "Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible fora single aspect.", "labels": [], "entities": []}, {"text": "Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstrated by for reasoning-focused question answering and clozestyle reading comprehension, we introduce a novel multi-task learning objective, encouraging each chain to focus on a particular aspect.", "labels": [], "entities": [{"text": "reasoning-focused question answering", "start_pos": 99, "end_pos": 135, "type": "TASK", "confidence": 0.6017346978187561}]}, {"text": "While still making use of external linguistic resources, we do not extractor design features from them but rather utilise such tools to generate labels.", "labels": [], "entities": []}, {"text": "The generated labels are then used to guide training so that each chain focuses on tracking a particular aspect.", "labels": [], "entities": []}, {"text": "At test time, our model is free of feature engineering such that, once trained, it can be easily deployed to unseen data without preprocessing.", "labels": [], "entities": []}, {"text": "Moreover, our approach also differs in the lack of a ROC Stories language model component, eliminating the need for large, domain-specific training corpora.", "labels": [], "entities": []}, {"text": "Evaluated on the task of Story Cloze Test, our model outperforms a collection of competitive baselines, achieving state-of-the-art performance under more modest data requirements.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the effectiveness of our model, we employ the Story Cloze Test dataset of.", "labels": [], "entities": [{"text": "Story Cloze Test dataset", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.8501827418804169}]}, {"text": "The development and test set each consist of 1,871 4-sentence stories, each with a pair of ending options.", "labels": [], "entities": []}, {"text": "Consistent with previous studies, we split the development set into a training and validation set (for early stopping), resulting in 1,683 and 188 in each set, resp.", "labels": [], "entities": []}, {"text": "Note that while most current approaches make use of the much larger training set, comprised of 100K 5-sentence ROC stories (with coherent endings only, also released as part of the dataset) to train a language model, we make no use of this data.", "labels": [], "entities": []}, {"text": "We initialise our model with word2vec embeddings (300-D, pre-trained on 100B Google News articles, not updated during training:).", "labels": [], "entities": [{"text": "Google News articles", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.8745418190956116}]}, {"text": "In addition to the three supervised chains, we also add a \"free\" chain, unconstrained to any semantic aspect.", "labels": [], "entities": []}, {"text": "Training is carried out over 200 epochs with the FTRL optimiser () and a batch size of 128 and learning rate of 0.1.", "labels": [], "entities": [{"text": "FTRL", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.6625166535377502}, {"text": "learning rate", "start_pos": 95, "end_pos": 108, "type": "METRIC", "confidence": 0.9676112234592438}]}, {"text": "We use the following hyper-parameters for weight matrices in both directions: R \u2208 R 300\u00d71 , H, U, V, Ware all matrices of size R 300\u00d7300 , and hidden size of the Bi-GRU is 300.", "labels": [], "entities": []}, {"text": "Dropout is applied to the output of \u03c6 in the final classifier (Equation 7) with a rate of 0.2.", "labels": [], "entities": []}, {"text": "Moreover, we employ the technique introduced by where the same dropout mask is applied at every step to the input w i to the Bi-GRU and the input hi to the memory chains with rates of 0.5 and 0.2 respectively.", "labels": [], "entities": []}, {"text": "Lastly, to curb overfitting, we regularise the last layer (Equation 7) with an L 2 penalty on its weights: \u03bbR where \u03bb = 0.001.", "labels": [], "entities": []}], "tableCaptions": []}