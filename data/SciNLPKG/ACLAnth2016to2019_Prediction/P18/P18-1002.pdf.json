{"title": [{"text": "A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "Motivations like domain adaptation, transfer learning, and feature learning have fu-eled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7524251639842987}, {"text": "transfer learning", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.9507276117801666}]}, {"text": "This paper introduces\u00e0introduces`introduces\u00e0 la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like em-beddings.", "labels": [], "entities": []}, {"text": "Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression.", "labels": [], "entities": []}, {"text": "This transform is applicable \"on the fly\" in the future when anew text feature or rare word is encountered , even if only a single usage example is available.", "labels": [], "entities": []}, {"text": "We introduce anew dataset showing how th\u00e8 a la carte method requires fewer examples of words in context to learn high-quality embed-dings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.", "labels": [], "entities": [{"text": "document classification tasks", "start_pos": 215, "end_pos": 244, "type": "TASK", "confidence": 0.7555514474709829}]}], "introductionContent": [{"text": "Distributional word embeddings, which represent the \"meaning\" of a word via a low-dimensional vector, have been widely applied by many natural language processing (NLP) pipelines and algorithms.", "labels": [], "entities": []}, {"text": "Following the success of recent neural) and matrixfactorization () methods, researchers have sought to extend the approach to other text features, from subword elements to n-grams to sentences.", "labels": [], "entities": []}, {"text": "However, the performance of both word embeddings and their extensions is known to degrade in small corpus settings  or when embedding sparse, low-frequency features.", "labels": [], "entities": []}, {"text": "Attempts to address these issues often involve task-specific approaches or extensively tuning existing architectures such as skip-gram (.", "labels": [], "entities": []}, {"text": "For computational efficiency it is desirable that methods be able to induce embeddings for only those features (e.g. bigrams or synsets) needed by the downstream task, rather than having to pay a computational prix fixe to learn embeddings for all features occurring frequently-enough in a corpus.", "labels": [], "entities": []}, {"text": "We propose an alternative, novel solution vi\u00e0vi\u00e0 a la carte embedding, a method which bootstraps existing high-quality word vectors to learn a feature representation in the same semantic space via a linear transformation of the average word embeddings in the feature's available contexts.", "labels": [], "entities": []}, {"text": "This can be seen as a shallow extension of the distributional hypothesis, \"a feature is characterized by the words in its context,\" rather than the computationally more-expensive \"a feature is characterized by the features in its context\" that has been used implicitly by past work.", "labels": [], "entities": []}, {"text": "Despite its elementary formulation, we demonstrate that th\u00e8 a la carte method can learn faithful word embeddings from single examples and feature vectors improving performance on important downstream tasks.", "labels": [], "entities": []}, {"text": "Furthermore, the approach is resource-efficient, needing only pretrained embed-dings of common words and the text corpus used to train them, and easy to implement and compute via vector addition and linear regression.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 179, "end_pos": 194, "type": "TASK", "confidence": 0.7875432968139648}]}, {"text": "After motivating and specifying the method, we illustrate these benefits through several applications: \u2022 Embeddings of rare words: we introduce a dataset 1 for few-shot learning of word vectors and achieve state-of-the-art results on the task of representing unseen words using only the definition).", "labels": [], "entities": []}, {"text": "\u2022 Synset embeddings: we show how the method can be applied to learn more finegrained lexico-semantic representations and give evidence of its usefulness for standard word-sense disambiguation tasks).", "labels": [], "entities": [{"text": "word-sense disambiguation tasks", "start_pos": 166, "end_pos": 197, "type": "TASK", "confidence": 0.7763574322064718}]}, {"text": "\u2022 n-gram embeddings: we build seven million n-gram embeddings from large text corpora and use them to construct document embeddings that are competitive with unsupervised deep learning approaches when evaluated on linear text classification.", "labels": [], "entities": [{"text": "linear text classification", "start_pos": 214, "end_pos": 240, "type": "TASK", "confidence": 0.6236742436885834}]}, {"text": "Our experimental results 2 clearly demonstrate the advantages of\u00e0of`of\u00e0 la carte embedding.", "labels": [], "entities": []}, {"text": "For word embeddings, the approach is an easy way to get a good vector fora new word from its definition or a few examples in context.", "labels": [], "entities": []}, {"text": "For feature embeddings, the method can embed anything that does not need labeling (such as a bigram) or occurs in an annotated corpus (such as a word-sense).", "labels": [], "entities": []}, {"text": "Our document embeddings, constructed directly using\u00e0 using`using\u00e0 la carte n-gram vectors, compete well with recent deep neural representations; this provides further evidence that simple methods can outperform modern deep learning on many NLP benchmarks ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison with baselines and nonce2vec (Herbelot and Baroni, 2017) on few-shot embedding  tasks. Performance on the chimeras task is measured using the Spearman correlation with human ratings.  Note that the additive baseline requires removing stop-words in order to improve with more data.", "labels": [], "entities": []}, {"text": " Table 2: Application of\u00e0of`of\u00e0 la carte synset embeddings to two standard WSD tasks. As all systems always  return exactly one answer, performance is measured in terms of accuracy. Results due to", "labels": [], "entities": [{"text": "WSD tasks", "start_pos": 75, "end_pos": 84, "type": "TASK", "confidence": 0.8250860273838043}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9985760450363159}]}, {"text": " Table 4: Performance of document embeddings built using\u00e0using`using\u00e0 la carte n-gram vectors and recent unsu- pervised word-level approaches on classification tasks, with the character LSTM of", "labels": [], "entities": []}]}