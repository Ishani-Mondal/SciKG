{"title": [{"text": "A Neural Approach to Pun Generation", "labels": [], "entities": [{"text": "Pun Generation", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.8288766741752625}]}], "abstractContent": [{"text": "Automatic pun generation is an interesting and challenging text generation task.", "labels": [], "entities": [{"text": "Automatic pun generation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6712494194507599}, {"text": "text generation", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7530774176120758}]}, {"text": "Previous efforts rely on templates or laboriously manually annotated pun datasets, which heavily constrains the quality and diversity of generated puns.", "labels": [], "entities": []}, {"text": "Since sequence-to-sequence models provide an effective technique for text generation , it is promising to investigate these models on the pun generation task.", "labels": [], "entities": [{"text": "text generation", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.8174451887607574}, {"text": "pun generation task", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.8376784920692444}]}, {"text": "In this paper, we propose neural network models for homographic pun generation, and they can generate puns without requiring any pun data for training.", "labels": [], "entities": [{"text": "homographic pun generation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.7117438713709513}]}, {"text": "We first train a conditional neural language model from a general text corpus, and then generate puns from the language model with an elaborately designed decoding algorithm.", "labels": [], "entities": []}, {"text": "Automatic and human evaluations show that our models are able to generate homo-graphic puns of good readability and quality .", "labels": [], "entities": []}], "introductionContent": [{"text": "Punning is an ingenious way to make conversation enjoyable and plays important role in entertainment, advertising and literature.", "labels": [], "entities": []}, {"text": "A pun is a means of expression, the essence of which is in the given context the word or phrase can be understood in two meanings simultaneously.", "labels": [], "entities": []}, {"text": "Puns can be classified according to various standards, and the most essential distinction for our research is between homographic and homophonic puns.", "labels": [], "entities": []}, {"text": "A homographic pun exploits distinct meanings of the same written word while a homophonic pun exploits distinct meanings of the same spoken word.", "labels": [], "entities": []}, {"text": "Puns can be homographic, homophonic, both, or neither.", "labels": [], "entities": []}, {"text": "Puns have the potential to combine novelty and familiarity appropriately, which can induce pleasing effect to advertisement.", "labels": [], "entities": []}, {"text": "Using puns also contributes to elegancy in literary writing, as laborious manual counts revealed that puns are one of the most commonly used rhetoric of Shakespeare, with the frequency in certain of his plays ranging from 17 to 85 instances per thousand lines.", "labels": [], "entities": []}, {"text": "It is not an overstatement to say that pun generation has significance inhuman society.", "labels": [], "entities": []}, {"text": "However, as a special branch of humor, generating puns is not easy for humans, let alone automatically generating puns with artificial intelligence techniques.", "labels": [], "entities": []}, {"text": "While text generation is a topic of interest in the natural language processing community, pun generation has received little attention.", "labels": [], "entities": [{"text": "text generation", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.8409512937068939}, {"text": "pun generation", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.8606802225112915}]}, {"text": "Recent sequence-to-sequence (seq2seq) framework is proved effective on text generation tasks including machine translation), image captioning (, and text summarization (.", "labels": [], "entities": [{"text": "text generation", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.7138413041830063}, {"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7279951721429825}, {"text": "image captioning", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7697896361351013}, {"text": "text summarization", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.7692959308624268}]}, {"text": "The end-to-end framework has the potential to train a language model which can generate fluent and creative sentences from a large corpus.", "labels": [], "entities": []}, {"text": "Great progress has achieved on the tasks with sufficient training data like machine translation, achieving state-of-the-art performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8030602633953094}]}, {"text": "Unfortunately, due to the limited puns which are deemed insufficient for training a language model, there has not been any research concentrated on generating puns based on the seq2seq framework as far as we know.", "labels": [], "entities": []}, {"text": "The inherent property of humor makes the pun generation task more challenging.", "labels": [], "entities": [{"text": "pun generation", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7748708128929138}]}, {"text": "Despite decades devoted to theories and algorithms for humor, computerized humor still lacks of creativity, sophistication of language, world knowledge, empathy and cognitive mechanisms compared to humans, which are extremely difficult to model.", "labels": [], "entities": []}, {"text": "In this paper, we study the challenging task of generating puns with seq2seq models without using a pun corpus for training.", "labels": [], "entities": []}, {"text": "We propose a brandnew method to generate homographic puns using normal text corpus which can result in good quality of language model and avoid considerable expense of human annotators on the limited pun resources.", "labels": [], "entities": []}, {"text": "Our proposed method can generate puns according to the given two senses of a target word.", "labels": [], "entities": []}, {"text": "We achieve this by first proposing an improved language model that is able to generate a sentence containing a given word with a specific sense.", "labels": [], "entities": []}, {"text": "Based on the improved language model, we are able to generate a pun sentence that is suitable for two specified senses of a homographic word, using a novel joint beam search algorithm we propose.", "labels": [], "entities": []}, {"text": "Moreover, based on the observed characteristics of human generated puns, we further enhance the model to generate puns highlighting intended word senses.", "labels": [], "entities": []}, {"text": "The proposed method demonstrates the ability to generate homographic puns containing the assigned two senses of a target word.", "labels": [], "entities": []}, {"text": "Our approach only requires a general text corpus, and we use the Wikipedia corpus in our experiment.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9720945656299591}]}, {"text": "We introduce both manual ways and automatic metrics to evaluate the generated puns.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our methods are powerful and inspiring in generating homographic puns.", "labels": [], "entities": [{"text": "generating homographic puns", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.7204030553499857}]}, {"text": "The contributions of our work are as follows: \u2022 To our knowledge, our work is the first attempt to adopt neural language models on pun generation.", "labels": [], "entities": [{"text": "pun generation", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.7789952754974365}]}, {"text": "And we do not use any templates or pun data sets in training the model.", "labels": [], "entities": []}, {"text": "\u2022 We propose a brand-new algorithm to generate sentences containing assigned distinct senses of a target word.", "labels": [], "entities": []}, {"text": "\u2022 We further ameliorate our model with associative words and multinomial sampling to produce better pun sentences.", "labels": [], "entities": []}, {"text": "\u2022 Our approach yields substantial results on generating homographic puns with high accuracy of assigned senses and low perplexity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9990955591201782}]}], "datasetContent": [{"text": "We select 100 target words and two senses for each word for test.", "labels": [], "entities": []}, {"text": "We use the language modeling toolkit SRILM 2 to train a trigram model with another 7,746,703 sentences extracted from Wikipedia, which are different from the data set used before.", "labels": [], "entities": []}, {"text": "The perplexity scores (PPL) of our models and baseline models are estimated based on the trained language model, as shown in Table 1.", "labels": [], "entities": [{"text": "perplexity scores (PPL)", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.8422332286834717}]}, {"text": "Normal Language Model has no constraint of generating sentences suitable for both senses.", "labels": [], "entities": []}, {"text": "This means at each time step the beam search algorithm can select the candidates with highest probabilities.", "labels": [], "entities": []}, {"text": "And thus it is natural that it obtains the lowest perplexity.", "labels": [], "entities": []}, {"text": "Taking the constraint of senses into consideration, the perplexity scores of Joint Model and Highlight Model are still comparable to that of Normal Language Model.", "labels": [], "entities": []}, {"text": "However, Pun Language Model could not be trained well considering the limit of the pun training data, so it gets the highest perplexity score.", "labels": [], "entities": []}, {"text": "This result reveals that it is not feasible to build a homographic pun generation system based on the pun data set since pun data is far from enough.", "labels": [], "entities": [{"text": "homographic pun generation", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.6429481108983358}, {"text": "pun data set", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.7563069860140482}]}, {"text": "In the table, We further compare the diversity of the generated sentences of four models following.", "labels": [], "entities": []}, {"text": "Distinct-1 (d.-1) and distinct-2 (d.-2) are the ratios of the distinct unigrams and bigrams in generated sentences, i.e., the number of distinct unigrams or bigrams divided by the total number of unigrams or bigrams.", "labels": [], "entities": [{"text": "Distinct-1", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9361997246742249}]}, {"text": "The results show our models are more creative than Normal Language and: Results of human evaluation.", "labels": [], "entities": []}, {"text": "Pun Language models, and Highlight Model can generate sentences with the best diversity.", "labels": [], "entities": []}, {"text": "Because of the subtle and delicate structure of puns, automatic evaluation is not enough.", "labels": [], "entities": []}, {"text": "So we sample one sentence for each word from four models mentioned above and then get 100 sentences of each model generated from the target words, together with 100 puns containing the same target words from homographic pun data set in.", "labels": [], "entities": []}, {"text": "We ask judges on Amazon Mechanical Turk to evaluate all the sentences and the rating score ranges from 1 to 5.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 17, "end_pos": 39, "type": "DATASET", "confidence": 0.9270041386286417}]}, {"text": "Five native English speakers are asked to give a score on each sentence in three aspects with the following information: Readability indicates whether the sentence is easy to understand semantically; Accuracy indicates whether the given senses are suitable in a sentence; Fluency indicates whether the sentence is fluent and consistent with the rules of grammar.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9947590231895447}, {"text": "Fluency", "start_pos": 272, "end_pos": 279, "type": "METRIC", "confidence": 0.9577675461769104}]}, {"text": "The results in show that pun data is not enough to train an ideal language model, while Normal Language Model has enough corpus to train a good language model.", "labels": [], "entities": []}, {"text": "But Normal Language Model is unable to make the given two senses appear in one sentence and in a few cases even cannot assure the appearance of the target  To test the potential of the sentences generated by our models to be homographic puns, we further design a Soft Turing Test.", "labels": [], "entities": []}, {"text": "We select 30 sentences generated by Joint Model and 30 sentences generated by Highlight Model independently, together with 30 gold puns from the homographic pun data set.", "labels": [], "entities": [{"text": "Highlight Model", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.8977528810501099}, {"text": "homographic pun data set", "start_pos": 145, "end_pos": 169, "type": "DATASET", "confidence": 0.6902172490954399}]}, {"text": "We mix them up, and give the definition of homographic pun and ask 10 people on Amazon Mechanical Turk to judge each sentence.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.8995879292488098}]}, {"text": "People can judge each sentence as one of three categories: definitely by human, might by human and definitely by machine.", "labels": [], "entities": []}, {"text": "The three categories correspond to the scores of 2, 1 and 0, respectively.", "labels": [], "entities": []}, {"text": "If the average score of one sentence is equal or greater than 1, we regard it as judged to be generated by human.", "labels": [], "entities": []}, {"text": "The number of sentences judged as by human for each model and the average score for each model are shown in.", "labels": [], "entities": []}, {"text": "Due to the flexible language structure of Highlight Model, the generated homographic puns outperform those generated by Joint Model in the Soft Turing Test, however still far from gold-standard puns.", "labels": [], "entities": []}, {"text": "Our models are adept at generating homographic puns containing assigned senses but weak in making homographic puns humorous.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of automatic evaluation.", "labels": [], "entities": []}, {"text": " Table 2: Results of Soft Turing Test.", "labels": [], "entities": []}]}