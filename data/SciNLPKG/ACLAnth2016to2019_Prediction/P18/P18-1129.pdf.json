{"title": [{"text": "Distilling Knowledge for Search-based Structured Prediction", "labels": [], "entities": [{"text": "Distilling Knowledge", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8500518202781677}, {"text": "Search-based Structured Prediction", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.5980430742104849}]}], "abstractContent": [{"text": "Many natural language processing tasks can be modeled into structured prediction and solved as a search problem.", "labels": [], "entities": []}, {"text": "In this paper, we distill an ensemble of multiple models trained with different initialization into a single model.", "labels": [], "entities": []}, {"text": "In addition to learning to match the ensemble's probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration.", "labels": [], "entities": []}, {"text": "Experimental results on two typical search-based structured prediction tasks-transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model's performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong base-lines and it outperforms the greedy struc-tured prediction models in previous litera-tures.", "labels": [], "entities": [{"text": "search-based structured prediction tasks-transition-based dependency parsing", "start_pos": 36, "end_pos": 112, "type": "TASK", "confidence": 0.7184133330980936}, {"text": "neural machine translation", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.6912850737571716}, {"text": "LAS", "start_pos": 275, "end_pos": 278, "type": "METRIC", "confidence": 0.9923266172409058}, {"text": "BLEU", "start_pos": 291, "end_pos": 295, "type": "METRIC", "confidence": 0.9992415904998779}]}], "introductionContent": [{"text": "Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (;.", "labels": [], "entities": [{"text": "Search-based structured prediction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5873537460962931}]}, {"text": "It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9994893074035645}]}, {"text": "A stochastic policy that controls the whole search process is usually learned by imitating a reference policy.", "labels": [], "entities": []}, {"text": "The imitation is usually addressed as training a classifier to predict the ref- * * Email corresponding.: Workflow of our knowledge distillation for search-based structured prediction.", "labels": [], "entities": [{"text": "search-based structured prediction", "start_pos": 149, "end_pos": 183, "type": "TASK", "confidence": 0.699142316977183}]}, {"text": "The yellow bracket represents the ensemble of multiple models trained with different initialization.", "labels": [], "entities": []}, {"text": "The dashed red line shows our distillation from reference ( \u00a73.2).", "labels": [], "entities": []}, {"text": "The solid blue line shows our distillation from exploration ( \u00a73.3).", "labels": [], "entities": []}, {"text": "erence policy's search action on the encountered states when performing the reference policy.", "labels": [], "entities": []}, {"text": "Such imitation process can sometimes be problematic.", "labels": [], "entities": []}, {"text": "One problem is the ambiguities of the reference policy, in which multiple actions lead to the optimal structure but usually, only one is chosen as training instance).", "labels": [], "entities": []}, {"text": "Another problem is the discrepancy between training and testing, in which during the test phase, the learned policy enters non-optimal states whose search action is never learned.", "labels": [], "entities": []}, {"text": "All these problems harm the generalization ability of search-based structured prediction and lead to poor performance.", "labels": [], "entities": [{"text": "search-based structured prediction", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.6469167371590933}]}, {"text": "Previous works tackle these problems from two directions.", "labels": [], "entities": []}, {"text": "To overcome the ambiguities in data, techniques like ensemble are often adopted (Di-", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on two tasks: transitionbased dependency parsing and neural machine translation.", "labels": [], "entities": [{"text": "transitionbased dependency parsing", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.5831910967826843}, {"text": "neural machine translation", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.6320917904376984}]}, {"text": "Both these two tasks are converted to search-based structured prediction as Section 2.1.", "labels": [], "entities": [{"text": "search-based structured prediction", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.6498473385969797}]}, {"text": "For the transition-based parsing, we use the stack-lstm parsing model proposed by to parameterize the classifier.", "labels": [], "entities": []}, {"text": "1 For the neural machine translation, we parameterize the classifier as an LSTM encoder-decoder model by following.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7226956685384115}]}, {"text": "We encourage the reader of this paper to refer corresponding papers for more details.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The dependency parsing results. Signif- icance test", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7953986525535583}, {"text": "Signif- icance test", "start_pos": 42, "end_pos": 61, "type": "METRIC", "confidence": 0.671558290719986}]}, {"text": " Table 2. The second group shows the  greedy transition-based parsers in previous litera- tures. Andor et al. (2016) presented an alternative  state representation and explored both greedy and  beam search decoding. (Ballesteros et al., 2016)  explores training the greedy parser with dynamic  oracle. Our distillation parser outperforms all  these greedy counterparts. The third group shows", "labels": [], "entities": []}, {"text": " Table 4: The ranking performance of parsers' out- put distributions evaluated in MAP on \"problem- atic\" states.", "labels": [], "entities": []}, {"text": " Table 5: The minimal, maximum, and standard  derivation values on differently-seeded runs.", "labels": [], "entities": []}]}