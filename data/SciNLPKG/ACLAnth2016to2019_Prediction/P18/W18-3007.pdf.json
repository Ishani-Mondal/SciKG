{"title": [{"text": "Natural Language Inference with Definition Embedding Considering Context On the Fly", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5913635591665903}]}], "abstractContent": [{"text": "Natural language inference (NLI) is one of the most important tasks in NLP.", "labels": [], "entities": [{"text": "Natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7854539553324381}]}, {"text": "In this study, we propose a novel method using word dictionaries, which are pairs of a word and its definition, as external knowledge.", "labels": [], "entities": []}, {"text": "Our neural definition embedding mechanism encodes input sentences with the definitions of each word of the sentences on the fly.", "labels": [], "entities": []}, {"text": "It can encode definitions of words considering the context of the input sentences by using an attention mechanism.", "labels": [], "entities": []}, {"text": "We evaluated our method using WordNet as a dictionary and confirmed that it performed better than baseline models when using the full or a subset of 100d GloVe as word embeddings.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9427712559700012}]}], "introductionContent": [{"text": "Recognition of the entailment relationship between two sentences is one of the most important tasks in the field of natural language processing.", "labels": [], "entities": [{"text": "Recognition of the entailment relationship between two sentences", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.8822088688611984}, {"text": "natural language processing", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.6530145903428396}]}, {"text": "An understanding of entailment relationships among sentences is useful for performing tasks such as question answering, information retrieval, and summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.847282201051712}, {"text": "information retrieval", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.8017964661121368}, {"text": "summarization", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.9913307428359985}]}, {"text": "The task of recognizing the entailment relationship between two sentences is called recognizing textual entailment (RTE) or natural language inference (NLI).", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 84, "end_pos": 120, "type": "TASK", "confidence": 0.6977082639932632}]}, {"text": "NLI has recently been getting more attention from researchers, owing to the release of large-scale corpora such as SNLI () and MNLI (.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.8541291952133179}]}, {"text": "These corpora consist of pairs of sentences, such as 'A soccer game with multiple males playing.' and 'Some men are playing a sport.', and ground-truth labels.", "labels": [], "entities": []}, {"text": "Each label is a judgment of whether the latter sentence, which is the premise, is inferred from the former one, which is the hypothesis.", "labels": [], "entities": []}, {"text": "In this example, the label is 'entailment'.", "labels": [], "entities": []}, {"text": "In this study, we propose a novel method that uses word dictionaries as external knowledge.", "labels": [], "entities": []}, {"text": "Word dictionaries are useful for domain adaptation, where we need to understand rare or novel words in which we do not have good embedding representations.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7424592822790146}]}, {"text": "For NLI, there is related work that does use dictionaries (.", "labels": [], "entities": []}, {"text": "In it, a definition embedding method is proposed that obtains representations of out-of-vocabulary (OOV) words from dictionaries on the fly.", "labels": [], "entities": []}, {"text": "In this method, however, the description of a word is converted into the same embedding anytime without considering the context of the input sentences.", "labels": [], "entities": []}, {"text": "On the other hand, we consider that word representation from dictionaries should reflect the context of the input sentences.", "labels": [], "entities": []}, {"text": "In the dictionary, we can explain the meaning of a word from many aspects.", "labels": [], "entities": []}, {"text": "However, the required information varies depending on the context of the input sentences.", "labels": [], "entities": []}, {"text": "This problem also occurs for pre-trained word embeddings, which are usually fixed for all contexts in the previous studies.", "labels": [], "entities": []}, {"text": "The proposed method can obtain different representations of words according to the contexts of the input sentences.", "labels": [], "entities": []}, {"text": "It introduces an attention mechanism that improves the encoded representations of each word in input sentences, by using the encoded word definitions of each word in the input sentences.", "labels": [], "entities": []}, {"text": "Moreover, unlike Bahdanau's method, it obtains the representation of all words from dictionaries on the fly in order to improve the representations of non-OOV words.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the results of the evaluation of the proposed method.", "labels": [], "entities": []}, {"text": "We chose ESIM () and one of the methods in as the baseline models.", "labels": [], "entities": []}, {"text": "ESIM is based on the model in Section 4.1.", "labels": [], "entities": [{"text": "ESIM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8224303126335144}]}, {"text": "BDN and our method each add a DEM to ESIM.", "labels": [], "entities": [{"text": "BDN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9136411547660828}]}, {"text": "In BDN, the target tokens of the definition embedding are not contained in the pre-trained word embedding vocabulary, because Figure 1: Classification accuracy of each model in the not-many-OOV setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9371852874755859}]}, {"text": "The vertical axis is accuracy, and the horizontal axis is the number of the vocabulary entries of the dictionary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995751976966858}]}, {"text": "The performance of ESIM and BDN was constant because their dictionary size is less than 1000.", "labels": [], "entities": []}, {"text": "BDN intends to supplement the embeddings of OOV words.", "labels": [], "entities": [{"text": "BDN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8603059649467468}]}, {"text": "However, in our method, the target tokens do not depend on a pre-trained word embedding vocabulary, because we intend to improve the representation of all the words by considering the context.", "labels": [], "entities": []}, {"text": "Our experiments were on the SNLI and MNLI benchmarks.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.6112155914306641}, {"text": "MNLI benchmarks", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.8590554296970367}]}, {"text": "For MNLI, we used a matched domain development dataset as our development data and a mismatched domain development dataset as our test data.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.5131919980049133}]}, {"text": "The word embeddings were pre-trained 100d GloVe 6B vectors and 300d GloVe 840B vectors).", "labels": [], "entities": []}, {"text": "The embeddings were fixed during training, because we were interested in the difference in representation between pre-trained embeddings with and without dictionary information.", "labels": [], "entities": []}, {"text": "We used the vocabulary and definitions in WordNet as dictionaries.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9609977602958679}]}, {"text": "For polysemic words with multiple definitions, we used the top-5 definitions connected in descending order of frequency of synsets, which are provided by WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9858527183532715}]}, {"text": "The number of headwords that appear in SNLI is 24103, and 45225 in MNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7529484629631042}, {"text": "MNLI", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9815435409545898}]}, {"text": "The other settings are described in Appendix A.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 shows the results. When there were  many OOV words, our method improved test ac-", "labels": [], "entities": []}]}