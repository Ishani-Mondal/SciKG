{"title": [{"text": "How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures", "labels": [], "entities": [{"text": "Granular Analysis of Neural Machine Translation Architectures", "start_pos": 34, "end_pos": 95, "type": "TASK", "confidence": 0.66663801244327}]}], "abstractContent": [{"text": "With recent advances in network ar-chitectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolu-tional or self-attentional approaches, such as in the Transformer.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.8443032741546631}]}, {"text": "While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines.", "labels": [], "entities": []}, {"text": "In this work we take a fine-grained look at the different ar-chitectures for NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.8717485666275024}]}, {"text": "We introduce an Architecture Definition Language (ADL) allowing fora flexible combination of common building blocks.", "labels": [], "entities": []}, {"text": "Making use of this language, we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention.", "labels": [], "entities": []}, {"text": "Additionally, we find that self-attention is much more important for the encoder side than for the decoder side, where it can be replaced by a RNN or CNN without a loss in performance inmost settings.", "labels": [], "entities": []}, {"text": "Surprisingly, even a model without any target side self-attention performs well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the introduction of attention mechanisms () Neural Machine Translation (NMT) ) has shown some impressive results.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.8374202847480774}]}, {"text": "Initially, approaches to NMT mainly relied on Recurrent Neural Networks (RNNs)) such as Long Short-Term Memory (LSTM) networks (Hochreiter and) or the Gated Rectified Unit (GRU) ( . Recently, other approaches relying on convolutional networks and self-attention () have been introduced.", "labels": [], "entities": [{"text": "NMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9741811752319336}]}, {"text": "These approaches remove the dependency between source language time steps, leading to considerable speed-ups in training time and improvements in quality.", "labels": [], "entities": []}, {"text": "The Transformer, however, contains other differences besides self-attention, including layer normalization across the entire model, multiple source attention mechanisms, a multi-head dot attention mechanism, and the use of residual feedforward layers.", "labels": [], "entities": []}, {"text": "This raises the question of how much each of these components matters.", "labels": [], "entities": []}, {"text": "To answer this question we first introduce a flexible Architecture Definition Language (ADL) ( \u00a72).", "labels": [], "entities": []}, {"text": "In this language we standardize existing components in a consistent way making it easier to compare structural differences of architectures.", "labels": [], "entities": []}, {"text": "Additionally, it allows us to efficiently perform a granular analysis of architectures, where we can evaluate the impact of individual components, rather than comparing entire architectures as a whole.", "labels": [], "entities": []}, {"text": "This ability leads us to the following observations: \u2022 Source attention on lower encoder layers brings no additional benefit ( \u00a74.2).", "labels": [], "entities": []}, {"text": "\u2022 Multiple source attention layers and residual feed-forward layers are key ( \u00a74.3).", "labels": [], "entities": []}, {"text": "\u2022 Self-attention is more important for the source than for the target side ( \u00a74.4).", "labels": [], "entities": []}], "datasetContent": [{"text": "What follows is an extensive empirical analysis of current NMT architectures and how certain sublayers as defined through our ADL affect performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores on WMT'14 EN\u2192DE.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992768168449402}, {"text": "WMT'14 EN\u2192", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8245327472686768}, {"text": "DE", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.6900442838668823}]}, {"text": " Table 3: Transforming an RNN into a Transformer style architecture. + shows the incrementally added  variation. / denotes an alternative variation to which the subsequent + is relative to.", "labels": [], "entities": []}, {"text": " Table 4: Transforming a CNN based model into a Transformer style architecture.", "labels": [], "entities": []}, {"text": " Table 5: Different variations of the encoder and decoder self-attention layer.", "labels": [], "entities": []}]}