{"title": [{"text": "A Co-Matching Model for Multi-choice Reading Comprehension", "labels": [], "entities": [{"text": "Multi-choice Reading Comprehension", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.5826347470283508}]}], "abstractContent": [{"text": "Multi-choice reading comprehension is a challenging task, which involves the matching between a passage and a question-answer pair.", "labels": [], "entities": [{"text": "Multi-choice reading comprehension", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8444750905036926}]}, {"text": "This paper proposes anew co-matching approach to this problem , which jointly models whether a passage can match both a question and a candidate answer.", "labels": [], "entities": []}, {"text": "Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance .", "labels": [], "entities": [{"text": "RACE dataset", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.7465041726827621}]}], "introductionContent": [{"text": "Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 87, "end_pos": 114, "type": "TASK", "confidence": 0.674597422281901}]}, {"text": "Recently, released anew multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China.", "labels": [], "entities": []}, {"text": "shows an example passage and two related questions from RACE.", "labels": [], "entities": [{"text": "RACE", "start_pos": 56, "end_pos": 60, "type": "TASK", "confidence": 0.7117734551429749}]}, {"text": "The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset () and SQuAD () is that the answers in RACE often cannot be directly extracted from the given passages, as illustrated by the two example questions (Q1 & Q2) in.", "labels": [], "entities": [{"text": "RACE", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.9710444211959839}, {"text": "CNN/Daily Mail dataset", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.9104957938194275}, {"text": "RACE", "start_pos": 160, "end_pos": 164, "type": "TASK", "confidence": 0.9336475133895874}]}, {"text": "Thus, answering these questions is more challenging and requires more inferences.", "labels": [], "entities": []}, {"text": "Previous approaches to machine comprehension are usually based on pairwise sequence matching, where either the passage is matched against the sequence that concatenates both the question and a candidate answer (, or the passage is matched against the question alone followed by a second step of selecting an answer using the matching result of the first step (.", "labels": [], "entities": [{"text": "pairwise sequence matching", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.6604886452356974}]}, {"text": "However, these approaches may not be suitable for multi-choice reading comprehension since questions and answers are often equally important.", "labels": [], "entities": []}, {"text": "Matching the passage only against the question may not be meaningful and may lead to loss of information from the original passage, as we can see from the first example question in.", "labels": [], "entities": []}, {"text": "On the other hand, concatenating the question and the answer into a single sequence for matching may notwork, either, due to the loss of interaction information between a question and an answer.", "labels": [], "entities": []}, {"text": "As illustrated by Q2 in, the model may need to recognize what \"he\" and \"it\" in candidate answer (c) refer to in the question, in order to select (c) as the correct answer.", "labels": [], "entities": []}, {"text": "This observation of the RACE dataset shows that we face anew challenge of matching sequence triplets (i.e., passage, question and answer) instead of pairwise matching.", "labels": [], "entities": [{"text": "RACE dataset", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9234787225723267}]}, {"text": "In this paper, we propose anew model to match a question-answer pair to a given passage.", "labels": [], "entities": []}, {"text": "Our comatching approach explicitly treats the question and the candidate answer as two sequences and jointly matches them to the given passage.", "labels": [], "entities": []}, {"text": "Specifically, for each position in the passage, we compute two attention-weighted vectors, where one is from the question and the other from the candidate answer.", "labels": [], "entities": []}, {"text": "Then, two matching representations are constructed: the first one matches the passage with the question while the second one matches the passage with the candidate answer.", "labels": [], "entities": []}, {"text": "These two newly constructed matching representations together form a co-matching state.", "labels": [], "entities": []}, {"text": "Intuitively, it encodes the locational information of the question and the candidate answer matched to a specific context of the passage.", "labels": [], "entities": []}, {"text": "Finally, we apply a hierar-Passage: My father wasn't a king, he was a taxi driver, but I am a prince-Prince Renato II, of the country Pontinha , an island fort on Funchal harbour.", "labels": [], "entities": []}, {"text": "In 1903, the king of Portugal sold the land to a wealthy British family, the Blandys, who make Madeira wine.", "labels": [], "entities": [{"text": "Blandys", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.921626627445221}]}, {"text": "Fourteen years ago the family decided to sell it for just EUR25,000, but nobody wanted to buy it either.", "labels": [], "entities": []}, {"text": "I met Blandy at a party and he asked if I'd like to buy the island.", "labels": [], "entities": [{"text": "Blandy", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.972065806388855}]}, {"text": "Of course I said yes, but I had no money-I was just an art teacher.", "labels": [], "entities": []}, {"text": "I tried to find some business partners, who all thought I was crazy.", "labels": [], "entities": []}, {"text": "So I sold some of my possessions, put my savings together and bought it.", "labels": [], "entities": []}, {"text": "Of course, my family and my friends-all thought I was mad ...", "labels": [], "entities": []}, {"text": "If l want to have a national flag, it could be blue today, red tomorrow.", "labels": [], "entities": []}, {"text": "My family sometimes drops by, and other people come everyday because the country is free for tourists to visit ...", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the effectiveness of our hierarchical co-matching model, we use the RACE dataset, which consists of two subsets: RACE-M comes from middle school examinations while RACE-H comes from high school examinations.", "labels": [], "entities": [{"text": "RACE dataset", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.7429641485214233}]}, {"text": "RACE is the combination of the two.", "labels": [], "entities": [{"text": "RACE", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.713756799697876}]}, {"text": "We compare our model with a number of baseline models.", "labels": [], "entities": []}, {"text": "We also compare with two variants of our model for an ablation study.", "labels": [], "entities": []}, {"text": "Comparison with Baselines We compare our model with the following baselines: \u2022 Sliding Window based method () computes the matching score based on the sum of the tf-idf values of the matched words between the question-answer pair and each subpassage with a fixed a window size.", "labels": [], "entities": []}, {"text": "\u2022 Stanford Attentive Reader (AR)) first builds a question-related passage representation through attention mechanism and then compares it with each candidate answer representation to get the answer probabilities.", "labels": [], "entities": [{"text": "Stanford Attentive Reader (AR))", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.5394354710976282}]}, {"text": "\u2022 GA () uses gated attention mechanism with multiple hops to extract the question-related information of the passage and compares it with candidate answers.", "labels": [], "entities": [{"text": "GA", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.8101721405982971}]}, {"text": "\u2022) tries to first eliminate the most irrelevant choices and then select the best answer.", "labels": [], "entities": []}, {"text": "\u2022 HAF (Zhou et al., 2018) considers not only the matching between the three sequences, namely, passage, question and candidate answer, but also the matching between the candidate answers.", "labels": [], "entities": [{"text": "HAF", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.6596102714538574}]}, {"text": "\u2022 MUSIC () integrates different sequence matching strategies into the model and also adds a unit of multi-step reasoning for selecting the answer.", "labels": [], "entities": [{"text": "MUSIC", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.841417133808136}, {"text": "sequence matching", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7264672517776489}]}, {"text": "Besides, we also report the following two results as reference points: Turkers is the performance of Amazon Turkers on a randomly sampled subset of the RACE test set.", "labels": [], "entities": [{"text": "RACE test set", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.8193773428599039}]}, {"text": "Ceiling is the percentage of the unambiguous questions with a correct answer in a subset of the test set.", "labels": [], "entities": []}, {"text": "The performance of our model together with the baselines are shown in.", "labels": [], "entities": []}, {"text": "We can see that our proposed complete model, Hier-CoMatching, achieved the best performance among all the public results.", "labels": [], "entities": []}, {"text": "Still, there is a huge gap between the best machine reading performance and the human performance, showing the great potential for further research.", "labels": [], "entities": []}, {"text": "Ablation Study Moreover, we conduct an ablation study of our model architecture.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.859753429889679}]}, {"text": "In this study, we are mainly interested in the contribution of each component introduced in this work to our final results.", "labels": [], "entities": []}, {"text": "We studied two key factors: (1) the comatching module and (2) the hierarchical aggregation approach.", "labels": [], "entities": [{"text": "comatching", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.9461707472801208}]}, {"text": "We observed a 4 percentage performance decrease by replacing the co-matching module with a single matching state (i.e., only M a in Eqn (3)) by directly concatenating the question with each candidate answer ().", "labels": [], "entities": []}, {"text": "We also observe about 2 percentage decrease when we treat the passage as a plain sequence, and run a two-layer LSTM (to ensure the numbers of parameters are comparable) over the whole passage instead of the hierarchical LSTM.", "labels": [], "entities": []}, {"text": "Question Type Analysis We also conducted an analysis on what types of questions our model can handle better.", "labels": [], "entities": [{"text": "Question Type Analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6812307834625244}]}, {"text": "We find that our model obtains similar performance on the \"wh\" questions such as \"why,\" \"what,\" \"when\" and \"where\" questions, on which the performance is usually around 50%.", "labels": [], "entities": []}, {"text": "We also check statement-justification questions with the keyword \"true\" (e.g., \"Which of the following statements is true\"), negation questions with the keyword \"not\" (e.g., \"which of the following is not true\"), and summarization questions with the keyword \"title\" (e.g., \"what is the best title for the passage?\"), and their performance is 51%, 52% and 48%, respectively.", "labels": [], "entities": []}, {"text": "We can see that the performance of our model on different types of questions in the RACE dataset is quite similar.", "labels": [], "entities": [{"text": "RACE dataset", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9093256294727325}]}, {"text": "However, our model is only based on wordlevel matching and may not have the ability of reasoning.", "labels": [], "entities": [{"text": "wordlevel matching", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7362227737903595}]}, {"text": "In order to answer questions that require summarization, inference or reasoning, we still need to further explore the dataset and improve the model.", "labels": [], "entities": [{"text": "summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9723827838897705}]}, {"text": "Finally, we further compared our model to the baseline, which concatenates the question with each candidate answer, and our model can achieve better performance on different types of questions.", "labels": [], "entities": []}, {"text": "For example, on the subset of the questions with pronouns, our model can achieve better accuracy of 49.8% than 47.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.999556839466095}]}, {"text": "Similarly, on statement-justification questions with the keyword \"true\", our model could achieve better accuracy of 51% than 47%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9993995428085327}]}], "tableCaptions": [{"text": " Table 2: Experiment Results.  *  means it's signifi- cant to the models ablating either the hierarchical  aggregation or co-matching state.", "labels": [], "entities": []}]}