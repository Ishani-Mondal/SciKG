{"title": [{"text": "A Multi-Axis Annotation Scheme for Event Temporal Relations", "labels": [], "entities": [{"text": "Event Temporal Relations", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.807335635026296}]}], "abstractContent": [{"text": "Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition.", "labels": [], "entities": [{"text": "inter-annotator agreements (IAA)", "start_pos": 71, "end_pos": 103, "type": "METRIC", "confidence": 0.8814665675163269}]}, {"text": "This paper proposes anew multi-axis modeling to better capture the temporal structure of events.", "labels": [], "entities": []}, {"text": "In addition, we identify that event end-points area major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only.", "labels": [], "entities": []}, {"text": "A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60's to 80's (Cohen's Kappa).", "labels": [], "entities": [{"text": "IAA", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9869979619979858}, {"text": "Cohen's Kappa)", "start_pos": 132, "end_pos": 146, "type": "DATASET", "confidence": 0.81751399487257}]}, {"text": "This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each anno-tator.", "labels": [], "entities": []}, {"text": "We hope that this work can foster more interesting studies towards event understanding.", "labels": [], "entities": [{"text": "event understanding", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8268445134162903}]}], "introductionContent": [{"text": "Temporal relation (TempRel) extraction is an important task for event understanding, and it has drawn much attention in the natural language processing (NLP) community recently (.", "labels": [], "entities": [{"text": "Temporal relation (TempRel) extraction", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8495632906754812}, {"text": "event understanding", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7841864824295044}]}, {"text": "Initiated by TimeBank (TB) (), a number of TempRel datasets have been collected, including but not limited to the verbclause augmentation to TB),), TimeBank-Dense (TB-Dense) ),, and datasets with both temporal and other types of relations (e.g., coreference and causality) such as CaTeRs ( and RED.", "labels": [], "entities": [{"text": "TempRel datasets", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.8385231196880341}]}, {"text": "These datasets were annotated by experts, but most still suffered from low inter-annotator agreements (IAA).", "labels": [], "entities": [{"text": "inter-annotator agreements (IAA)", "start_pos": 75, "end_pos": 107, "type": "METRIC", "confidence": 0.7726822376251221}]}, {"text": "For instance, the IAAs of TB-Dense, RED and THYME-TimeML were only below or near 60% (given that events are already annotated).", "labels": [], "entities": [{"text": "IAAs", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9931914210319519}, {"text": "TB-Dense", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.8962005972862244}, {"text": "RED", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.5888144969940186}, {"text": "THYME-TimeML", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.5287207365036011}]}, {"text": "Since a low IAA usually indicates that the task is difficult even for humans (see Examples 1-3), the community has been looking into ways to simplify the task, by reducing the label set, and by breaking up the overall, complex task into subtasks (e.g., getting agreement on which event pairs should have a relation, and then what that relation should be).", "labels": [], "entities": [{"text": "IAA", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9847413897514343}]}, {"text": "In contrast to other existing datasets, achieved an agreement as high as 90%, but the scope of its annotation was narrowed down to a very special verb-clause structure.", "labels": [], "entities": [{"text": "agreement", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9845303297042847}]}, {"text": "(e1, e2), (e3, e4), and (e5, e6): TempRels that are difficult even for humans.", "labels": [], "entities": []}, {"text": "Note that only relevant events are highlighted here.", "labels": [], "entities": []}, {"text": "Example 1: Serbian police tried to eliminate the proindependence Kosovo Liberation Army and (e1:restore) order.", "labels": [], "entities": []}, {"text": "At least 51 people were (e2:killed) in clashes between Serb police and ethnic Albanians in the troubled region.", "labels": [], "entities": []}, {"text": "Example 2: Service industries (e3:showed) solid job gains, as did manufacturers, two areas expected to be hardest (e4:hit) when the effects of the Asian crisis hit the American economy.", "labels": [], "entities": []}, {"text": "Example 3: We will act again if we have evidence he is (e5:rebuilding) his weapons of mass destruction capabilities, senior officials say.", "labels": [], "entities": []}, {"text": "Ina bit of television diplomacy, Iraq's deputy foreign minister (e6:responded) from Baghdad in less than one hour, saying that . .", "labels": [], "entities": []}, {"text": "This paper proposes anew approach to handling these issues in TempRel annotation.", "labels": [], "entities": []}, {"text": "First, we introduce multi-axis modeling to represent the temporal structure of events, based on which we anchor events to different semantic axes; only events from the same axis will then be temporally compared (Sec. 2).", "labels": [], "entities": []}, {"text": "As explained later, those event pairs in Examples 1-3 are difficult because they represent different semantic phenomena and belong to different axes.", "labels": [], "entities": []}, {"text": "Second, while we represent an event pair using two time intervals (say, , we suggest that comparisons involving end-points (e.g., t 1 end vs. t 2 end ) are typically more difficult than comparing start-points (i.e., t 1 start vs. t 2 start ); we attribute this to the ambiguity of expressing and perceiving durations of events).", "labels": [], "entities": []}, {"text": "We believe that this is an important consideration, and we propose in Sec.", "labels": [], "entities": []}, {"text": "3 that TempRel annotation should focus on start-points.", "labels": [], "entities": []}, {"text": "Using the proposed annotation scheme, a pilot study done by experts achieved a high IAA of .84 (Cohen's Kappa) on a subset of TB-Dense, in contrast to the conventional 60's.", "labels": [], "entities": [{"text": "IAA", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.999579131603241}, {"text": "Cohen's Kappa)", "start_pos": 96, "end_pos": 110, "type": "METRIC", "confidence": 0.7068763002753258}, {"text": "TB-Dense", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.9082913398742676}]}, {"text": "In addition to the low IAA issue, TempRel annotation is also known to belabor intensive.", "labels": [], "entities": [{"text": "IAA issue", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.7469545304775238}]}, {"text": "Our third contribution is that we facilitate, for the first time, the use of crowdsourcing to collect anew, high quality (under multiple metrics explained later) TempRel dataset.", "labels": [], "entities": [{"text": "TempRel dataset", "start_pos": 162, "end_pos": 177, "type": "DATASET", "confidence": 0.9004510343074799}]}, {"text": "We explain how the crowdsourcing quality was controlled and how vague relations were handled in Sec.", "labels": [], "entities": []}, {"text": "4, and present some statistics and the quality of the new dataset in Sec.", "labels": [], "entities": []}, {"text": "5. A baseline system is also shown to achieve much better performance on the new dataset, when compared with system performance in the literature (Sec. 6).", "labels": [], "entities": []}, {"text": "The paper's results are very encouraging and hopefully, this work would significantly benefit research in this area.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Annotations involving the end-points of events  are found to be much harder than only comparing the  start-points.", "labels": [], "entities": []}, {"text": " Table 3: IAA of two experts' annotations in a pilot  study on the main axis. Notations: before, after, equal,  and vague.", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.978877604007721}]}, {"text": " Table 4: Quality analysis of the relation annotation step  of MATRES. \"Q1\" and \"Q2\" refer to the two ques- tions crowdsourcers were asked (see Sec. 4.2 for de- tails). Line 1 measures the level of consistency be- tween crowdsourcers and the authors and line 2 mea- sures the level of consistency among the crowdsourcers  themselves.", "labels": [], "entities": [{"text": "MATRES", "start_pos": 63, "end_pos": 69, "type": "TASK", "confidence": 0.8412940502166748}]}, {"text": " Table 5: An evaluation of MATRES against TB-Dense.  Horizontal: MATRES. Vertical: TB-Dense (with inter- val relations mapped to start-point relations). Please  see explanation of these numbers in text.", "labels": [], "entities": []}, {"text": " Table 6: Performance of the proposed baseline sys- tem on MATRES. Line \"Original\" is the same system  retrained on the original TB-Dense and tested on the  same subset of event pairs. Due to the limited number  of equal examples, the system did not make any equal  predictions on the testset.", "labels": [], "entities": []}]}