{"title": [{"text": "Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms", "labels": [], "entities": []}], "abstractContent": [{"text": "Many deep learning architectures have been proposed to model the composition-ality in text sequences, requiring a substantial number of parameters and expensive computations.", "labels": [], "entities": []}, {"text": "However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions.", "labels": [], "entities": []}, {"text": "In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models.", "labels": [], "entities": []}, {"text": "Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered.", "labels": [], "entities": [{"text": "SWEMs", "start_pos": 14, "end_pos": 19, "type": "TASK", "confidence": 0.9397034645080566}]}, {"text": "Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences.", "labels": [], "entities": []}, {"text": "We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification ; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging.", "labels": [], "entities": [{"text": "document classification", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7584909498691559}, {"text": "text sequence matching", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.6188437243302664}]}], "introductionContent": [{"text": "Word embeddings, learned from massive unstructured text data, are widely-adopted building blocks for Natural Language Processing (NLP).", "labels": [], "entities": []}, {"text": "By representing each word as a fixed-length vector, these embeddings can group semantically similar words, while implicitly encoding rich linguistic regularities and patterns ().", "labels": [], "entities": []}, {"text": "Leveraging the word-embedding construct, many deep architectures have been proposed to model the compositionality in variable-length text sequences.", "labels": [], "entities": []}, {"text": "These methods range from simple operations like addition (, to more sophisticated compositional functions such as Recurrent Neural Networks (RNNs)), Convolutional Neural Networks (CNNs) () and Recursive Neural Networks (.", "labels": [], "entities": []}, {"text": "Models with more expressive compositional functions, e.g., RNNs or CNNs, have demonstrated impressive results; however, they are typically computationally expensive, due to the need to estimate hundreds of thousands, if not millions, of parameters (.", "labels": [], "entities": []}, {"text": "In contrast, models with simple compositional functions often compute a sentence or document embedding by simply adding, or averaging, over the word embedding of each sequence element obtained via, e.g.,, or GloVe ().", "labels": [], "entities": []}, {"text": "Generally, such a Simple Word-Embedding-based Model (SWEM) does not explicitly account for spatial, word-order information within a text sequence.", "labels": [], "entities": []}, {"text": "However, they possess the desirable property of having significantly fewer parameters, enjoying much faster training, relative to RNN-or CNN-based models.", "labels": [], "entities": [{"text": "training", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9710482358932495}]}, {"text": "Hence, there is a computation-vs.-expressiveness tradeoff regarding how to model the compositionality of a text sequence.", "labels": [], "entities": []}, {"text": "In this paper, we conduct an extensive experimental investigation to understand when, and why, simple pooling strategies, operated over word embeddings alone, already carry sufficient information for natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 200, "end_pos": 230, "type": "TASK", "confidence": 0.6504325270652771}]}, {"text": "To account for the distinct nature of various NLP tasks that may require different semantic features, we compare SWEM-based models with existing recurrent and convolutional networks in a pointby-point manner.", "labels": [], "entities": []}, {"text": "Specifically, we consider 17 datasets, including three distinct NLP tasks: document classification (Yahoo news, Yelp reviews, etc.), natural language sequence matching (SNLI, WikiQA, etc.) and (short) sentence classification/tagging (Stanford sentiment treebank, TREC, etc.).", "labels": [], "entities": [{"text": "document classification", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.6956773549318314}, {"text": "natural language sequence matching", "start_pos": 133, "end_pos": 167, "type": "TASK", "confidence": 0.6664047688245773}, {"text": "sentence classification/tagging", "start_pos": 201, "end_pos": 232, "type": "TASK", "confidence": 0.7951373159885406}, {"text": "Stanford sentiment treebank", "start_pos": 234, "end_pos": 261, "type": "DATASET", "confidence": 0.7539847095807394}]}, {"text": "Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered.", "labels": [], "entities": [{"text": "SWEMs", "start_pos": 14, "end_pos": 19, "type": "TASK", "confidence": 0.9397034645080566}]}, {"text": "In order to validate our experimental findings, we conduct additional investigations to understand to what extent the word-order information is utilized/required to make predictions on different tasks.", "labels": [], "entities": []}, {"text": "We observe that in text representation tasks, many words (e.g., stop words, or words that are not related to sentiment or topic) do not meaningfully contribute to the final predictions (e.g., sentiment label).", "labels": [], "entities": [{"text": "text representation tasks", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.7669228613376617}]}, {"text": "Based upon this understanding, we propose to leverage a max-pooling operation directly over the word embedding matrix of a given sequence, to select its most salient features.", "labels": [], "entities": []}, {"text": "This strategy is demonstrated to extract complementary features relative to the standard averaging operation, while resulting in a more interpretable model.", "labels": [], "entities": []}, {"text": "Inspired by a case study on sentiment analysis tasks, we further propose a hierarchical pooling strategy to abstract and preserve the spatial information in the final representations.", "labels": [], "entities": [{"text": "sentiment analysis tasks", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.9558647076288859}]}, {"text": "This strategy is demonstrated to exhibit comparable empirical results to LSTM and CNN on tasks that are sensitive to word-order features, while maintaining the favorable properties of not having compositional parameters, thus fast training.", "labels": [], "entities": []}, {"text": "Our work presents a simple yet strong baseline for text representation learning that is widely ignored in benchmarks, and highlights the general computation-vs.-expressiveness tradeoff associated with appropriately selecting compositional functions for distinct NLP problems.", "labels": [], "entities": [{"text": "text representation learning", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.8254291216532389}]}, {"text": "Furthermore, we quantitatively show that the word-embeddingbased text classification tasks can have the similar level of difficulty regardless of the employed models, using the subspace training ( to constrain the trainable parameters.", "labels": [], "entities": [{"text": "word-embeddingbased text classification", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.5962726771831512}]}, {"text": "Thus, according to Occam's razor, simple models are preferred.", "labels": [], "entities": [{"text": "Occam's razor", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9192853569984436}]}], "datasetContent": [{"text": "We evaluate different compositional functions on a wide variety of supervised tasks, including document categorization, text sequence matching (given a sentence pair, X 1 , X 2 , predict their relationship, y) as well as (short) sentence classification.", "labels": [], "entities": [{"text": "document categorization", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.7682895362377167}, {"text": "text sequence matching", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.6635608474413554}, {"text": "sentence classification", "start_pos": 229, "end_pos": 252, "type": "TASK", "confidence": 0.7437054812908173}]}, {"text": "We experiment on 17 datasets concerning natural language understanding, with corresponding data statistics summarized in the Supplementary Material.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6528638005256653}, {"text": "Supplementary Material", "start_pos": 125, "end_pos": 147, "type": "DATASET", "confidence": 0.7313215136528015}]}, {"text": "Our code will be released to encourage future research.", "labels": [], "entities": []}, {"text": "We use GloVe word embeddings with K = 300 () as initialization for all our models.", "labels": [], "entities": []}, {"text": "Out-Of-Vocabulary (OOV) words are initialized from a uniform distribution with range [\ud97b\udf590.01, 0.01].", "labels": [], "entities": []}, {"text": "The GloVe embeddings are employed in two ways to learn refined word embeddings: (i) directly updating each word embedding during training; and (ii) training a 300-dimensional Multilayer Perceptron (MLP) layer with ReLU activation, with GloVe embeddings as input to the MLP and with output defining the refined word embeddings.", "labels": [], "entities": []}, {"text": "The latter approach corresponds to learning an MLP model that adapts GloVe embeddings to the dataset and task of interest.", "labels": [], "entities": []}, {"text": "The advantages of these two methods differ from dataset to dataset.", "labels": [], "entities": []}, {"text": "We choose the better strategy based on their corresponding performances on the validation set.", "labels": [], "entities": []}, {"text": "The final classifier is implemented as an MLP layer with dimension selected from the set [100, 300, 500, 1000], followed by a sigmoid or softmax function, depending on the specific task.", "labels": [], "entities": []}, {"text": "Adam) is used to optimize all models, with learning rate selected from the set [1 \u21e5 10 \ud97b\udf593 , 3 \u21e5 10 \ud97b\udf594 , 2 \u21e5 10 \ud97b\udf594 , 1 \u21e5 10 \ud97b\udf595 ] (with cross-validation used to select the appropriate parameter fora given dataset and task).", "labels": [], "entities": []}, {"text": "Dropout regularization () is    Interestingly, for the sentiment analysis tasks, both CNN and LSTM compositional functions perform better than SWEM, suggesting that wordorder information maybe required for analyzing sentiment orientations.", "labels": [], "entities": [{"text": "Dropout regularization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7003672868013382}, {"text": "sentiment analysis", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.9354958236217499}, {"text": "analyzing sentiment orientations", "start_pos": 206, "end_pos": 238, "type": "TASK", "confidence": 0.7560420632362366}]}, {"text": "This finding is consistent with, where they hypothesize that the positional information of a word in text sequences maybe beneficial to predict sentiment.", "labels": [], "entities": [{"text": "predict sentiment", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.7606766223907471}]}, {"text": "This is intuitively reasonable since, for instance, the phrase \"not really good\" and \"really not good\" convey different levels of negative sentiment, while being different only by their word orderings.", "labels": [], "entities": []}, {"text": "Contrary to SWEM, CNN and LSTM models can both capture this type of information via convolutional filters or recurrent transition functions.", "labels": [], "entities": []}, {"text": "However, as suggested above, such word-order patterns maybe much less useful for predicting the topic of a document.", "labels": [], "entities": [{"text": "predicting the topic of a document", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.8670003513495127}]}, {"text": "This maybe attributed to the fact that word embeddings alone already provide sufficient topic information of a document, at least when the text sequences considered are relatively long.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Test accuracy on (long) document classification tasks, in percentage. Results marked with \u21e4 are  reported in Zhang et al. (2015b), with  \u2020 are reported in Conneau et al. (2016), and with  \u2021 are reported in  Joulin et al. (2016).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9917284846305847}, {"text": "document classification tasks", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.7241501013437907}]}, {"text": " Table 5: Performance of different models on matching natural language sentences. Results with \u21e4 are  for Bidirectional LSTM, reported in Williams et al. (2017). Our reported results on MultiNLI are only  trained MultiNLI training set (without training data from SNLI). For MSRP dataset, we follow the setup  in Hu et al. (2014) and do not use any additional features.", "labels": [], "entities": [{"text": "MSRP dataset", "start_pos": 274, "end_pos": 286, "type": "DATASET", "confidence": 0.8578874468803406}]}, {"text": " Table 6: Test accuracy for LSTM model trained on  original/shuffled training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9774252772331238}]}, {"text": " Table 6. Somewhat surprisingly, for Yahoo and  SNLI datasets, the LSTM model trained on shuf- fled training set shows comparable accuracies to  those trained on the original dataset, indicating", "labels": [], "entities": [{"text": "SNLI datasets", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8888032734394073}]}, {"text": " Table 8: Test accuracies with different compositional functions on (short) sentence classifications.", "labels": [], "entities": []}, {"text": " Table 8. Compared with CNN/LSTM com- positional functions, SWEM yields inferior accu- racies on sentiment analysis datasets, consistent  with our observation in the case of document cat-", "labels": [], "entities": [{"text": "accu- racies", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.9674115777015686}]}]}