{"title": [], "abstractContent": [{"text": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic.", "labels": [], "entities": [{"text": "story generation", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7834127843379974}]}, {"text": "We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum.", "labels": [], "entities": []}, {"text": "Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text.", "labels": [], "entities": [{"text": "hierarchical story generation", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.71819669008255}]}, {"text": "We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding anew gated multi-scale self-attention mechanism to model long-range context.", "labels": [], "entities": []}, {"text": "Experiments show large improvements over strong baselines on both automated and human evaluations.", "labels": [], "entities": []}, {"text": "Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.", "labels": [], "entities": []}], "introductionContent": [{"text": "Story-telling is on the frontier of current text generation technology: stories must remain thematically consistent across the complete document, requiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation (.", "labels": [], "entities": []}, {"text": "We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story.", "labels": [], "entities": []}, {"text": "Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot.", "labels": [], "entities": []}, {"text": "It also reduces the tendency of standard sequence models to drift off topic.", "labels": [], "entities": []}, {"text": "Prompt: The Mage, the Warrior, and the Priest Story: A light breeze swept the ground, and carried with it still the distant scents of dust and time-worn stone.", "labels": [], "entities": []}, {"text": "The Warrior led the way, heaving her mass of armour and muscle over the uneven terrain.", "labels": [], "entities": []}, {"text": "She soon crested the last of the low embankments, which still bore the unmistakable fingerprints of haste and fear.", "labels": [], "entities": [{"text": "haste", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9411132335662842}]}, {"text": "She lifted herself up onto the top the rise, and looked out at the scene before her.", "labels": [], "entities": []}, {"text": "Figure 1: Example prompt and beginning of a story from our dataset.", "labels": [], "entities": []}, {"text": "We train a hierarchical model that first generates a prompt, and then conditions on the prompt when generating a story.", "labels": [], "entities": []}, {"text": "We find that standard sequence-to-sequence (seq2seq) models) applied to hierarchical story generation are prone to degenerating into language models that pay little attention to the writing prompt (a problem that has been noted in other domains, such as dialogue response generation ().", "labels": [], "entities": [{"text": "hierarchical story generation", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.6875261763731638}, {"text": "dialogue response generation", "start_pos": 254, "end_pos": 282, "type": "TASK", "confidence": 0.7964068253835043}]}, {"text": "This failure is due to the complex and underspecified dependencies between the prompt and the story, which are much harder to model than the closer dependencies required for language modeling (for example, consider the subtle relationship between the first sentence and prompt in.", "labels": [], "entities": []}, {"text": "To improve the relevance of the generated story to its prompt, we introduce a fusion mechanism () where our model is trained on top of an pre-trained seq2seq model.", "labels": [], "entities": []}, {"text": "To improve over the pre-trained model, the second model must focus on the link between the prompt and the story.", "labels": [], "entities": []}, {"text": "For the first time, we show that fusion mechanisms can help seq2seq models build dependencies between their input and output.", "labels": [], "entities": []}, {"text": "Another major challenge in story generation is the inefficiency of modeling long documents with standard recurrent architectures-stories contain 734 words on average in our dataset.", "labels": [], "entities": [{"text": "story generation", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.8141401708126068}]}, {"text": "We improve efficiency using a convolutional architecture, al- Experiments show that our fusion and selfattention mechanisms improve over existing techniques on both automated and human evaluation measures.", "labels": [], "entities": []}, {"text": "Our new dataset and neural architectures allow for models which can creatively generate longer, more consistent and more fluent passages of text.", "labels": [], "entities": []}, {"text": "Human judges prefer our hierarchical model's stories twice as often as those of a nonhierarchical baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collect a hierarchical story generation dataset 1 from Reddit's WRITINGPROMPTS forum.", "labels": [], "entities": []}, {"text": "WRITINGPROMPTS is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond.", "labels": [], "entities": []}, {"text": "Each prompt can have multiple story responses.", "labels": [], "entities": []}, {"text": "The prompts have a large diversity of topic, length, and detail.", "labels": [], "entities": [{"text": "length", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9643377065658569}, {"text": "detail", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.954563558101654}]}, {"text": "The stories must beat least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement).", "labels": [], "entities": []}, {"text": "We scraped three years of prompts and their associated stories using the official Reddit API.", "labels": [], "entities": []}, {"text": "We clean the dataset by removing automated bot posts, deleted posts, special announcements, com-ments from moderators, and stories shorter than 30 words.", "labels": [], "entities": []}, {"text": "We use NLTK for tokenization.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9727947115898132}]}, {"text": "The dataset models full text to generate immediately human-readable stories.", "labels": [], "entities": []}, {"text": "We reserve 5% of the prompts fora validation set and 5% fora test set, and present additional statistics about the dataset in.", "labels": [], "entities": []}, {"text": "For our experiments, we limit the length of the stories to 1000 words maximum and limit the vocabulary size for the prompts and the stories to words appearing more than 10 times each.", "labels": [], "entities": []}, {"text": "We model an unknown word token and an end of document token.", "labels": [], "entities": []}, {"text": "This leads to a vocabulary size of 19,025 for the prompts and 104,960 for the stories.", "labels": [], "entities": [{"text": "vocabulary size", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.92188960313797}]}, {"text": "As the dataset is scraped from an online forum, the number of rare words and misspellings is quite large, so modeling the full vocabulary is challenging and computationally intensive.", "labels": [], "entities": []}, {"text": "We propose a number of evaluation metrics to quantify the performance of our models.", "labels": [], "entities": []}, {"text": "Many commonly used metrics, such as BLEU for ma-  : Accuracy on the prompt/story pairing task vs. number of generated stories.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9989917874336243}, {"text": "Accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9945776462554932}, {"text": "prompt/story pairing task", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6303632020950317}]}, {"text": "Our generative fusion model can produce many stories without degraded performance, while the KNN can only produce a limited number relevant stories.", "labels": [], "entities": [{"text": "generative fusion", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9667378067970276}, {"text": "KNN", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.8852987885475159}]}], "tableCaptions": [{"text": " Table 1: Statistics of WRITINGPROMPTS dataset", "labels": [], "entities": [{"text": "WRITINGPROMPTS dataset", "start_pos": 24, "end_pos": 46, "type": "DATASET", "confidence": 0.7723408639431}]}, {"text": " Table 2: Effect of new attention mechanism.  Gated multi-scale attention significantly improves  the perplexity on the WRITINGPROMPTS dataset.", "labels": [], "entities": [{"text": "WRITINGPROMPTS dataset", "start_pos": 120, "end_pos": 142, "type": "DATASET", "confidence": 0.7441618889570236}]}, {"text": " Table 3: Perplexity on WRITINGPROMPTS. We dramatically improve over standard seq2seq models.", "labels": [], "entities": [{"text": "WRITINGPROMPTS", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.8337819576263428}]}]}