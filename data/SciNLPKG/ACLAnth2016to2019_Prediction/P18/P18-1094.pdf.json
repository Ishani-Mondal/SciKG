{"title": [], "abstractContent": [{"text": "Learning by contrasting positive and negative samples is a general strategy adopted by many methods.", "labels": [], "entities": []}, {"text": "Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach.", "labels": [], "entities": [{"text": "Noise contrastive estimation (NCE", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6046184539794922}]}, {"text": "In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sam-pler.", "labels": [], "entities": []}, {"text": "The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data.", "labels": [], "entities": []}, {"text": "We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embed-dings and observe both faster convergence and improved results on multiple metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many models learn by contrasting losses on observed positive examples with those on some fictitious negative examples, trying to decrease some score on positive ones while increasing it on negative ones.", "labels": [], "entities": []}, {"text": "There are multiple reasons why such contrastive learning approach is needed.", "labels": [], "entities": []}, {"text": "For instance, instead of using softmax to predict a word for learning word embeddings, noise contrastive estimation (NCE) can be used in skip-gram or CBOW word embedding models (.", "labels": [], "entities": [{"text": "noise contrastive estimation (NCE)", "start_pos": 87, "end_pos": 121, "type": "METRIC", "confidence": 0.6496975521246592}]}, {"text": "Another reason is * authors contributed equally \u2020 Work done while author was an intern at Borealis AI modeling need, ascertain assumptions are best expressed as some score or energy in margin based or un-normalized probability models).", "labels": [], "entities": []}, {"text": "For example, modeling entity relations as translations or variants thereof in a vector space naturally leads to a distance-based score to be minimized for observed entity-relation-entity triplets (.", "labels": [], "entities": []}, {"text": "Given a scoring function, the gradient of the model's parameters on observed positive examples can be readily computed, but the negative phase requires a design decision on how to sample data.", "labels": [], "entities": []}, {"text": "In noise contrastive estimation for word embeddings, a negative example is formed by replacing a component of a positive pair by randomly selecting a sampled word from the vocabulary, resulting in a fictitious word-context pair which would be unlikely to actually exist in the dataset.", "labels": [], "entities": [{"text": "noise contrastive estimation", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7316591540972391}]}, {"text": "This negative sampling by corruption approach is also used in learning knowledge graph embeddings (, order embeddings (, caption generation, etc.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.8350518941879272}]}, {"text": "Typically the corruption distribution is the same for all inputs like in skip-gram or CBOW NCE, rather than being a conditional distribution that takes into account information about the input sample under consideration.", "labels": [], "entities": [{"text": "CBOW NCE", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8952958583831787}]}, {"text": "Furthermore, the corruption process usually only encodes a human prior as to what constitutes a hard negative sample, rather than being learned from data.", "labels": [], "entities": []}, {"text": "For these two reasons, the simple fixed corruption process often yields only easy negative examples.", "labels": [], "entities": []}, {"text": "Easy negatives are sub-optimal for learning discriminative representation as they do not force the model to find critical characteristics of observed positive data, which has been independently discovered in applications outside NLP previously (.", "labels": [], "entities": [{"text": "learning discriminative representation", "start_pos": 35, "end_pos": 73, "type": "TASK", "confidence": 0.802211344242096}]}, {"text": "Even if hard negatives are occasionally reached, the infrequency means slow convergence.", "labels": [], "entities": []}, {"text": "Designing a more sophisticated corruption process could be fruitful, but requires costly trialand-error by a human expert.", "labels": [], "entities": []}, {"text": "In this work, we propose to augment the simple corruption noise process in various embedding models with an adversarially learned conditional distribution, forming a mixture negative sampler that adapts to the underlying data and the embedding model training progress.", "labels": [], "entities": []}, {"text": "The resulting method is referred to as adversarial contrastive estimation (ACE).", "labels": [], "entities": [{"text": "adversarial contrastive estimation (ACE)", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.6566565533479055}]}, {"text": "The adaptive conditional model engages in a minimax game with the primary embedding model, much like in Generative Adversarial Networks (GANs) (, where a discriminator net (D), tries to distinguish samples produced by a generator (G) from real data ().", "labels": [], "entities": []}, {"text": "In ACE, the main model learns to distinguish between areal positive example and a negative sample selected by the mixture of a fixed NCE sampler and an adversarial generator.", "labels": [], "entities": []}, {"text": "The main model and the generator takes alternating turns to update their parameters.", "labels": [], "entities": []}, {"text": "In fact, our method can be viewed as a conditional GAN) on discrete inputs, with a mixture generator consisting of a learned and a fixed distribution, with additional techniques introduced to achieve stable and convergent training of embedding models.", "labels": [], "entities": []}, {"text": "In our proposed ACE approach, the conditional sampler finds harder negatives than NCE, while being able to gracefully fallback to NCE whenever the generator cannot find hard negatives.", "labels": [], "entities": []}, {"text": "We demonstrate the efficacy and generality of the proposed method on three different learning tasks, word embeddings (, order embeddings () and knowledge graph embeddings ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate ACE with experiments on word embeddings, order embeddings, and knowledge graph embeddings tasks.", "labels": [], "entities": [{"text": "ACE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.924837589263916}]}, {"text": "In short, whenever the original learning objective is contrastive (all tasks except Glove fine-tuning) our results consistently show that ACE improves over NCE.", "labels": [], "entities": [{"text": "ACE", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9012320041656494}, {"text": "NCE", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.8281602263450623}]}, {"text": "In some cases, we include additional comparisons to the state-of-art results on the task to put the significance of such improvements in context: the generic ACE can often make a reasonable baseline competitive with SOTA methods that are optimized for the task.", "labels": [], "entities": []}, {"text": "For word embeddings, we evaluate models trained from scratch as well as fine-tuned Glove models () on word similarity tasks that consist of computing the similarity   between word pairs where the ground truth is an average of human scores.", "labels": [], "entities": []}, {"text": "We choose the Rare word dataset ( and WordSim-353 () by virtue of our hypothesis that ACE learns better representations for both rare and frequent words.", "labels": [], "entities": [{"text": "Rare word dataset", "start_pos": 14, "end_pos": 31, "type": "DATASET", "confidence": 0.7121155063311259}, {"text": "WordSim-353", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9609875082969666}]}, {"text": "We also qualitatively evaluate ACE word embeddings by inspecting the nearest neighbors of selected words.", "labels": [], "entities": [{"text": "ACE word embeddings", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.6872051358222961}]}, {"text": "For the hypernym prediction task, following, hypernym pairs are created from the WordNet hierarchy's transitive closure.", "labels": [], "entities": [{"text": "hypernym prediction task", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.8600522677103678}]}, {"text": "We use the released random development split and test split from, which both contain 4000 edges.", "labels": [], "entities": []}, {"text": "For knowledge graph embeddings, we use TransD () as our base model, and perform ablation study to analyze the behavior of ACE with various add-on features, and confirm that entropy regularization is crucial for good performance in ACE.", "labels": [], "entities": []}, {"text": "We also obtain link prediction results that are competitive or superior to the stateof-arts on the WN18 dataset ().", "labels": [], "entities": [{"text": "link prediction", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6721022725105286}, {"text": "WN18 dataset", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.9874401986598969}]}], "tableCaptions": [{"text": " Table 1: Top 5 Nearest Neighbors of Words followed by Neighbors 45-50 for different Models.", "labels": [], "entities": []}, {"text": " Table 2: Spearman score (\u03c1  *  100) on RW and  WS353 Datasets. We trained a skipgram model  from scratch under various settings for only 1  epoch on wikipedia. For finetuned models we re- computed the scores based on the publicly avail- able 6B tokens Glove models and we finetuned un- til roughly 75% of the vocabulary was seen.", "labels": [], "entities": [{"text": "Spearman score", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8700495660305023}, {"text": "RW", "start_pos": 40, "end_pos": 42, "type": "DATASET", "confidence": 0.9183655381202698}, {"text": "WS353 Datasets", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.8473248481750488}]}, {"text": " Table 4: WN18 experiments: the first portion of  the table contains results where the base model is  TransD, the last separated line is the COMPLEX  embedding model (Trouillon et al., 2016), which  achieves the SOTA on this dataset. Among all  TransD based models (the best results in this group  is underlined), ACE improves over basic NCE and  another GAN based approach KBGAN. The gap  on MRR is likely due to the difference between  TransD and COMPLEX models.", "labels": [], "entities": [{"text": "SOTA", "start_pos": 212, "end_pos": 216, "type": "METRIC", "confidence": 0.9978927969932556}, {"text": "MRR", "start_pos": 393, "end_pos": 396, "type": "TASK", "confidence": 0.5664023756980896}]}]}