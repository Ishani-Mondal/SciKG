{"title": [{"text": "Iterative Back-Translation for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.7178287307421366}]}], "abstractContent": [{"text": "We present iterative back-translation, a method for generating increasingly better synthetic parallel data from monolin-gual data to train neural machine translation systems.", "labels": [], "entities": []}, {"text": "Our proposed method is very simple yet effective and highly applicable in practice.", "labels": [], "entities": []}, {"text": "We demonstrate improvements in neural machine translation quality in both high and low re-sourced scenarios, including the best reported BLEU scores for the WMT 2017 German\u2194English tasks.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6525701979796091}, {"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9991920590400696}, {"text": "WMT 2017 German\u2194English tasks", "start_pos": 157, "end_pos": 186, "type": "DATASET", "confidence": 0.8116417626539866}]}], "introductionContent": [{"text": "The exploitation of monolingual training data for neural machine translation is an open challenge.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6831615169843038}]}, {"text": "One successful method is back-translation, whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation).", "labels": [], "entities": []}, {"text": "The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model.", "labels": [], "entities": []}, {"text": "In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation systems in forward and backward directions, which in turn is used to reback-translate monolingual data.", "labels": [], "entities": []}, {"text": "This process can be \"iterated\" several times.", "labels": [], "entities": []}, {"text": "This is a form of co-training () where the two models over both translation directions can be used to train one another.", "labels": [], "entities": []}, {"text": "We show that iterative back-translation leads to improved results oversimple back-translation, under both high and reverse system final system real synthetic real+synthetic: Creating a synthetic parallel corpus through back-translation.", "labels": [], "entities": []}, {"text": "First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system.", "labels": [], "entities": []}, {"text": "low resource conditions, improving over the state of the art.", "labels": [], "entities": []}], "datasetContent": [{"text": "In \u00a73 we demonstrated that the quality of the backtranslation system has significant impact on the effectiveness of the back-translation approach under high-resource data conditions such as WMT 2017 German-English.", "labels": [], "entities": [{"text": "WMT 2017 German-English", "start_pos": 190, "end_pos": 213, "type": "DATASET", "confidence": 0.9106350342432658}]}, {"text": "Here we ask: how much additional benefit can be realised for repeating this process?", "labels": [], "entities": []}, {"text": "Also, do the gains for state-of-the-art systems that use deeper models, i.e., more layers in encoder and decoder ) still apply in this setting?", "labels": [], "entities": []}, {"text": "We evaluate on German-English and EnglishGerman, under the same data conditions as in Section 3.", "labels": [], "entities": []}, {"text": "We experiment with both shallow and deep stacked-layer encoder/decoder architectures.", "labels": [], "entities": []}, {"text": "The base translation system is trained on the parallel data only.", "labels": [], "entities": [{"text": "base translation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.6888422667980194}]}, {"text": "We train a shallow system using 4-checkpoint ensembling (Chen et al., 2017).", "labels": [], "entities": []}, {"text": "The system is used to translate the monolingual data using abeam size of 2.", "labels": [], "entities": []}, {"text": "The first back-translation system is trained on: WMT News Translation Task GermanEnglish, comparing the quality of different backtranslation systems with different final system architectures.", "labels": [], "entities": [{"text": "WMT News Translation Task GermanEnglish", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.6810672402381897}]}, {"text": "*Note that the quality for the backtranslation system (Back) is measured in the opposite language direction.", "labels": [], "entities": []}, {"text": "the parallel data and the synthetic data generated by the base translation system.", "labels": [], "entities": []}, {"text": "For better performance, we train a deep model with 8-checkpoint ensembling; again we use abeam size of 2.", "labels": [], "entities": []}, {"text": "The final back-translation systems were trained using several different systems: a shallow architecture, a deep architecture, and an ensemble system of 4 independent training runs.", "labels": [], "entities": []}, {"text": "Across the board, the final systems with reback-translation outperform the final systems with simple back-translation, by a margin of 0.5-1.1 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9983367919921875}]}, {"text": "Notably, the final deep systems trained by re-back-translation outperform the state-of-the-art established at the WMT 2017 evaluation campaign for these language pairs, by a margin of about 1 BLEU point.", "labels": [], "entities": [{"text": "WMT 2017 evaluation campaign", "start_pos": 114, "end_pos": 142, "type": "DATASET", "confidence": 0.660947173833847}, {"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9992014765739441}]}, {"text": "These are the best published results for this dataset, to the best of our knowledge.", "labels": [], "entities": []}, {"text": "Drop-out settings are the same as above.", "labels": [], "entities": []}, {"text": "Decoding during test time is done with abeam size of 12, while back-translation uses only abeam size of 2.", "labels": [], "entities": []}, {"text": "This difference is reflected in the reported BLEU score for the deep system after backtranslation (35.0 for German-English, 28.3 for English-German) and the score reported for the quality of the back-translation system (34.8 (-0.2) and 27.9 (-0.4), respectively) in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9995198249816895}]}, {"text": "For all experiments, the true-casing model and the list of BPE operations is left constant.", "labels": [], "entities": [{"text": "BPE", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.5673233270645142}]}, {"text": "Both were learned from the original parallel training corpus.", "labels": [], "entities": []}, {"text": "NMT is a data-hungry approach, requiring a large amount of parallel data to reach reasonable performance (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8330265879631042}]}, {"text": "Ina lowresource setting, only small amount of parallel data exist.", "labels": [], "entities": []}, {"text": "Previous work has attempted to incorporate prior or external knowledge to compensate for the lack of parallel data, e.g. injecting inductive bias via linguistic constraints ( or linguistic factors (.", "labels": [], "entities": []}, {"text": "However, it is much cheaper and easier to obtain monolingual data in either the source or target language.", "labels": [], "entities": []}, {"text": "An interesting question is whether the (iterative) back-translation can compensate for the lack of parallel data in such low-resource settings.", "labels": [], "entities": []}, {"text": "To explore this question, we conducted experiments on two datasets: A simulated low-resource setting with English-French, and a more realistic setting with English-Farsi.", "labels": [], "entities": []}, {"text": "For the EnglishFrench dataset, we used the original WMT dataset, sub-sampled to create smaller sets of 100K and 1M parallel sentence pairs.", "labels": [], "entities": [{"text": "EnglishFrench dataset", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.9712305963039398}, {"text": "WMT dataset", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9628483057022095}]}, {"text": "For English-Farsi, we used the available datasets from LDC and TED Talks, totaling about 100K sentence pairs.", "labels": [], "entities": [{"text": "LDC", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.8760610818862915}]}, {"text": "Following the same experimental setup as in high-resource setting, we obtain similar patterns of improvement of translation quality.", "labels": [], "entities": []}, {"text": "Back-Translation Generally, it is our expectation that the back-translation approach still improves the translation accuracy in all language pairs with a low-resource setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.934477686882019}]}, {"text": "In the EnglishFrench experiments, large improvements over the baseline are observed in both directions, with +3.5", "labels": [], "entities": [{"text": "EnglishFrench", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9708713293075562}]}], "tableCaptions": [{"text": " Table 1: Parallel and monolingual corpora used, including English-German, English-French and English- Farsi. Numbers denote the number of words, and l 2 is the second language in each pair. The de-en data  is from WMT 2017 (parallel) and a subset of News 2016 (monolingual).", "labels": [], "entities": [{"text": "WMT 2017", "start_pos": 215, "end_pos": 223, "type": "DATASET", "confidence": 0.9566762745380402}, {"text": "News 2016", "start_pos": 251, "end_pos": 260, "type": "DATASET", "confidence": 0.9461659491062164}]}, {"text": " Table 2:  WMT News Translation Task  English\u2194German, reporting cased BLEU on  newstest2017, evaluating the impact of the quality  of the back-translation system on the final system.  Note that the back-translation systems run in the  opposite direction and are not comparable to the  numbers in the same row.", "labels": [], "entities": [{"text": "WMT News Translation Task  English\u2194German", "start_pos": 11, "end_pos": 52, "type": "TASK", "confidence": 0.8042189819472176}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9690882563591003}]}, {"text": " Table 3: WMT News Translation Task German- English, comparing the quality of different back- translation systems with different final system ar- chitectures. *Note that the quality for the back- translation system (Back) is measured in the op- posite language direction.", "labels": [], "entities": [{"text": "WMT News Translation Task German- English", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.7515508787972587}]}, {"text": " Table 4: Low Resource setting: Impact of the quality of the back-translation systems on the benefit of the  synthetic parallel for the final system in a low-resource setting. Note that, we reported the single NMT  systems in all numbers.", "labels": [], "entities": []}]}