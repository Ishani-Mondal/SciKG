{"title": [{"text": "Neural Cross-Lingual Coreference Resolution And Its Application To Entity Linking", "labels": [], "entities": [{"text": "Neural Cross-Lingual Coreference Resolution", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7812118381261826}, {"text": "Entity Linking", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7293245792388916}]}], "abstractContent": [{"text": "We propose an entity-centric neural cross-lingual coreference model that builds on multilingual embeddings and language-independent features.", "labels": [], "entities": [{"text": "entity-centric neural cross-lingual coreference", "start_pos": 14, "end_pos": 61, "type": "TASK", "confidence": 0.5740466192364693}]}, {"text": "We perform both intrinsic and extrinsic evaluations of our model.", "labels": [], "entities": []}, {"text": "In the intrinsic evaluation, we show that our model, when trained on En-glish and tested on Chinese and Spanish, achieves competitive results to the models trained directly on Chinese and Span-ish respectively.", "labels": [], "entities": []}, {"text": "In the extrinsic evaluation , we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Span-ish.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9755454659461975}]}], "introductionContent": [{"text": "Cross-lingual models for NLP tasks are important since they can be used on data from anew language without requiring annotation from the new language (.", "labels": [], "entities": []}, {"text": "This paper investigates the use of multi-lingual embeddings) for building cross-lingual models for the task of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.975858598947525}]}, {"text": "Consider the following text from a Spanish news article: \"Tormenta de nieve afecta a 100 millones de personas en EEUU.", "labels": [], "entities": [{"text": "Tormenta", "start_pos": 58, "end_pos": 66, "type": "TASK", "confidence": 0.968502938747406}, {"text": "EEUU", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.897477388381958}]}, {"text": "Unos 100 millones de personas enfrentaban el s\u00e1bado nuevas dificultades tras la enorme tormenta de nieve de hace d\u00edas en la costa este de Estados Unidos.\"", "labels": [], "entities": []}, {"text": "The mentions \"EEUU\" (\"US\" in English) and \"Estados Unidos\" (\"United States\" in English) are coreferent.", "labels": [], "entities": [{"text": "EEUU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9487310647964478}]}, {"text": "A coreference model trained on English data is unlikely to coreference these two mentions in Spanish since these mentions did not appear in English data and a regular English style abbreviation of \"Estados Unidos\" will be \"EU\" instead of \"EEUU\".", "labels": [], "entities": [{"text": "EEUU", "start_pos": 239, "end_pos": 243, "type": "METRIC", "confidence": 0.8537244200706482}]}, {"text": "But in the bilingual EnglishSpanish word embedding space, the word embedding of \"EEUU\" sits close to the word embedding of \"US\" and the sum of word embeddings of \"Estados Unidos\" sit close to the sum of word embeddings of \"United States\".", "labels": [], "entities": [{"text": "EEUU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.868184506893158}]}, {"text": "Therefore, a coreference model trained using English-Spanish bilingual word embeddings on English data has the potential to make the correct coreference decision between \"EEUU\" and \"Estados Unidos\" without ever encountering these mentions in training data.", "labels": [], "entities": [{"text": "EEUU", "start_pos": 171, "end_pos": 175, "type": "DATASET", "confidence": 0.7311893105506897}]}, {"text": "The contributions of this paper are two-fold.", "labels": [], "entities": []}, {"text": "Firstly, we propose an entity-centric neural crosslingual coreference model.", "labels": [], "entities": [{"text": "entity-centric neural crosslingual coreference", "start_pos": 23, "end_pos": 69, "type": "TASK", "confidence": 0.5833256915211678}]}, {"text": "This model, when trained on English and tested on Chinese and Spanish from the TAC 2015 Trilingual Entity Discovery and Linking (EDL) Task (, achieves competitive results to models trained directly on Chinese and Spanish respectively.", "labels": [], "entities": [{"text": "TAC 2015 Trilingual Entity Discovery and Linking (EDL) Task", "start_pos": 79, "end_pos": 138, "type": "TASK", "confidence": 0.846277735450051}]}, {"text": "Secondly, a pipeline consisting of this coreference model and an Entity Linking (henceforth EL) model can achieve superior linking accuracy than the official top ranking system in 2015 on Chinese and Spanish test sets, without using any supervision in Chinese or Spanish.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.952998161315918}]}, {"text": "Although most of the active coreference research is on solving the problem of noun phrase coreference resolution in the Ontonotes data set, invigorated by the 2011 and 2012 CoNLL shared task, there are many important applications/end tasks where the mentions of interest are not noun phrases.", "labels": [], "entities": [{"text": "noun phrase coreference resolution", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.7675597667694092}, {"text": "Ontonotes data set", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.9807769656181335}]}, {"text": "Consider the sentence, \"(U.S. president Barack Obama who started ((his) political career) in (Illinois)), was born in (Hawaii).\"", "labels": [], "entities": []}, {"text": "The bracketing represents the Ontonotes style noun phrases and underlines represent the phrases that should be linked to Wikipedia by an EL system.", "labels": [], "entities": [{"text": "Ontonotes style noun phrases", "start_pos": 30, "end_pos": 58, "type": "DATASET", "confidence": 0.8851864784955978}]}, {"text": "Note that mentions like \"U.S.\" and \"Barack Obama\" do not align with any noun phrase.", "labels": [], "entities": []}, {"text": "Therefore, in this work, we focus on coreference on mentions that arise in our end task of entity linking and conduct experiments on TAC TriLingual 2015 data sets consisting of English, Chinese and Spanish.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.7553619742393494}, {"text": "TAC TriLingual 2015 data sets", "start_pos": 133, "end_pos": 162, "type": "DATASET", "confidence": 0.9435953974723816}]}], "datasetContent": [{"text": "We evaluate cross-lingual transfer of coreference models on the TAC 2015 Tri-Lingual EL datasets.", "labels": [], "entities": [{"text": "TAC 2015 Tri-Lingual EL datasets", "start_pos": 64, "end_pos": 96, "type": "DATASET", "confidence": 0.940024983882904}]}, {"text": "It contains mentions annotated with their grounded Freebase 1 links (if such links exist) or corpus-wide clustering information for 3 languages: English (henceforth, En), Chinese (henceforth, Zh) and Spanish (henceforth, Es).", "labels": [], "entities": []}, {"text": "shows the size of the training and test sets for the three languages.", "labels": [], "entities": []}, {"text": "The documents come from two genres of newswire and discussion forums.", "labels": [], "entities": []}, {"text": "The mentions in this dataset are either named entities or nominals that belong to five types: PER, ORG, GPE, LOC and FAC.", "labels": [], "entities": [{"text": "PER", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9944685697555542}, {"text": "ORG", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9789907932281494}, {"text": "GPE", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9168359637260437}, {"text": "LOC", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9477021098136902}, {"text": "FAC", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9694284796714783}]}, {"text": "Hyperparameters: Every feature is embedded in a 50 dimensional space except the words which reside in a 300 dimensional space.", "labels": [], "entities": []}, {"text": "The Relu and Sigmoid layers have 100 and 500 neurons respectively.", "labels": [], "entities": []}, {"text": "We use SGD for optimization with an initial learning rate of 0.05 which is linearly reduced to  0.0001.", "labels": [], "entities": []}, {"text": "Our mini batch size is 32 and we train for 50 epochs and keep the best model based on dev set.", "labels": [], "entities": []}, {"text": "Coreference Results: For each language, we follow the official train-test splits made in the TAC 2015 competition.", "labels": [], "entities": [{"text": "TAC 2015 competition", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.8409490386644999}]}, {"text": "Except, a small portion of the training set is held out as development set for tuning the models.", "labels": [], "entities": []}, {"text": "All experimental results on all languages reported in this paper were obtained on the official test sets.", "labels": [], "entities": []}, {"text": "We used the official CoNLL 2012 evaluation script and report MUC, B and CEAF scores and their average (CONLL score).", "labels": [], "entities": [{"text": "CoNLL 2012 evaluation script", "start_pos": 21, "end_pos": 49, "type": "DATASET", "confidence": 0.9460225850343704}, {"text": "MUC", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.4684005677700043}, {"text": "B and CEAF scores", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.6575842127203941}, {"text": "CONLL score", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.7679485082626343}]}, {"text": "To test the competitiveness of our model with other SOTA models, we train the publicly available system of Clark and Manning (2016) (henceforth, C&M16) on the TAC 15 En training set and test on the TAC 15 En test set.", "labels": [], "entities": [{"text": "TAC 15 En training set", "start_pos": 159, "end_pos": 181, "type": "DATASET", "confidence": 0.944462513923645}, {"text": "TAC 15 En test set", "start_pos": 198, "end_pos": 216, "type": "DATASET", "confidence": 0.9675703644752502}]}, {"text": "The C&M16 system normally outputs both noun phrase mentions and their coreference and is trained on Ontonotes.", "labels": [], "entities": [{"text": "C&M16 system", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8529365807771683}, {"text": "Ontonotes", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.8948366641998291}]}, {"text": "To ensure a fair comparison, we changed the configuration of the system to accept gold mention boundaries both during training and testing.", "labels": [], "entities": [{"text": "gold mention boundaries", "start_pos": 82, "end_pos": 105, "type": "METRIC", "confidence": 0.8129484057426453}]}, {"text": "Since the system was unable to deal with partially overlapping mentions, we excluded such mentions in the evaluation.", "labels": [], "entities": []}, {"text": "shows that our model outperforms C&M16 by 8 points.", "labels": [], "entities": [{"text": "C&M16", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8875388701756796}]}, {"text": "For cross-lingual experiments, we build monolingual embeddings for En, Zh and Es using the widely used CBOW word2vec model (.", "labels": [], "entities": []}, {"text": "Recently Canonical Correlation Analysis (CCA)), Multi-CCA ( and Weighted Regression () have been proposed for building the multi-lingual embedding space from monolingual embedding.", "labels": [], "entities": [{"text": "Canonical Correlation Analysis (CCA))", "start_pos": 9, "end_pos": 46, "type": "TASK", "confidence": 0.7537629504998525}]}, {"text": "In our prelimi-, \"En Model\" refers to the model that was trained on the En training set of TAC 15 using multi-lingual embeddings and tested on the Es and Zh testing set of TAC 15.", "labels": [], "entities": [{"text": "En training set of TAC 15", "start_pos": 72, "end_pos": 97, "type": "DATASET", "confidence": 0.8363913297653198}, {"text": "Es and Zh testing set of TAC 15", "start_pos": 147, "end_pos": 178, "type": "DATASET", "confidence": 0.7289563305675983}]}, {"text": "\"Es Model\" refers to the model trained on Es training set of TAC 15 using Es embeddings.", "labels": [], "entities": [{"text": "Es training set of TAC 15", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.7938638925552368}]}, {"text": "\"Zh Model\" refers to the model trained on the Zh training set of TAC 15 using Zh embeddings.", "labels": [], "entities": [{"text": "Zh training set of TAC 15", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.8139754484097163}]}, {"text": "The En model performs 0.5 point below the Es model on the Es test set.", "labels": [], "entities": [{"text": "Es test set", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.7460537056128184}]}, {"text": "On the Zh test set, the En model performs only 0.3 point below the Zh model.", "labels": [], "entities": [{"text": "Zh test set", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.8298928538958231}]}, {"text": "Hence, we show that without using any target language training data, the En model with multi-lingual embeddings gives comparable results to models trained on the target language.", "labels": [], "entities": []}, {"text": "EL Results: We replace the in-document coreference system (trained on the target language) of SIL18 with our En model to investigate the performance of our proposed algorithm on an extrinsic task.", "labels": [], "entities": [{"text": "EL", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6231958866119385}, {"text": "En", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9301577210426331}]}, {"text": "shows the EL results on Es and Zh test sets respectively.", "labels": [], "entities": [{"text": "EL", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9951619505882263}]}, {"text": "\"EL -Coref\" refers to the case where the first step of coreference is not used and EL is used to link the mentions directly to Freebase.", "labels": [], "entities": []}, {"text": "\"EL + En Coref\" refers to the case where the neural english coreference model is first used on Zh or Es data followed by the EL model.", "labels": [], "entities": []}, {"text": "The former is 3 points below the latter on Es and 2.6 points below Zh, implying coreference is a vital task for EL.", "labels": [], "entities": [{"text": "EL", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.7146104574203491}]}, {"text": "Our \"EL + En Coref\" outperforms the 2015 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively.", "labels": [], "entities": [{"text": "TAC best", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.6442858576774597}]}, {"text": "Finally, we show the SOTA results on these two data sets recently reported by SIL18.", "labels": [], "entities": [{"text": "SOTA", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.6066287755966187}, {"text": "SIL18", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.7869606614112854}]}, {"text": "Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as  the test set .Without using any in-language training data, our results are competitive to their results (1.2% below on Es and 0.5% below on Zh).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.830457329750061}]}], "tableCaptions": [{"text": " Table 1: No of documents for the TAC 2015 Tri- Lingual EL Dataset", "labels": [], "entities": [{"text": "TAC 2015 Tri- Lingual EL Dataset", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.8123284918921334}]}, {"text": " Table 2: Coreference results on the En test set of  TAC 15 competition. Our model significantly out- performs C&M16.", "labels": [], "entities": [{"text": "En test set of  TAC 15 competition", "start_pos": 37, "end_pos": 71, "type": "DATASET", "confidence": 0.8716222303254264}, {"text": "C&M16", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.8091726104418436}]}, {"text": " Table 3: Coreference results on the Es and Zh test  sets of TAC 15. En model performs competitively  to the models trained on target language data.", "labels": [], "entities": [{"text": "TAC 15", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.7371852695941925}]}, {"text": " Table 4: Performance comparison on the TAC  2015 Es and Zh datasets. EL + En Coref outper- forms the best 2015 TAC system (Rank 1) without  requiring any Es or Zh coreference data.", "labels": [], "entities": [{"text": "TAC  2015 Es and Zh datasets", "start_pos": 40, "end_pos": 68, "type": "DATASET", "confidence": 0.7952938278516134}, {"text": "EL + En Coref outper", "start_pos": 70, "end_pos": 90, "type": "METRIC", "confidence": 0.5788716793060302}]}]}