{"title": [{"text": "Generating Informative Responses with Controlled Sentence Function", "labels": [], "entities": []}], "abstractContent": [{"text": "Sentence function is a significant factor to achieve the purpose of the speaker, which, however, has not been touched in large-scale conversation generation so far.", "labels": [], "entities": []}, {"text": "In this paper, we present a model to generate informative responses with controlled sentence function.", "labels": [], "entities": []}, {"text": "Our model utilizes a continuous latent variable to capture various word patterns that realize the expected sentence function, and introduces a type controller to deal with the compatibility of controlling sentence function and generating informative content.", "labels": [], "entities": []}, {"text": "Conditioned on the latent variable, the type controller determines the type (i.e., function-related, topic, and ordinary word) of a word to be generated at each decoding position.", "labels": [], "entities": []}, {"text": "Experiments show that our model outper-forms state-of-the-art baselines, and it has the ability to generate responses with both controlled sentence function and informative content.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence function is an important linguistic feature and atypical taxonomy in terms of the purpose of the speaker.", "labels": [], "entities": []}, {"text": "There are four major function types in the language including interrogative, declarative, imperative, and exclamatory, as described in.", "labels": [], "entities": []}, {"text": "Each sentence function possesses its own structure, and transformation between sentence functions needs a series of changes in word order, syntactic patterns and other aspects.", "labels": [], "entities": []}, {"text": "Since sentence function is regarding the purpose of the speaker, it can be a significant factor indicating the conversational purpose during interac- * *Corresponding author: Minlie Huang.", "labels": [], "entities": []}, {"text": "Interrogative responses can be used to acquire further information from the user; imperative responses are used to make requests, directions, instructions or invitations to elicit further interactions; and declarative responses commonly make statements to state or explain something.", "labels": [], "entities": []}, {"text": "Interrogative and imperative responses can be used to avoid stalemates (), which can be viewed as important proactive behaviors in conversation ().", "labels": [], "entities": []}, {"text": "Thus, conversational systems equipped with the ability to control the sentence function can adjust its strategy for different purposes within different contexts, behave more proactively, and may lead the dialogue to go further.", "labels": [], "entities": []}, {"text": "Generating responses with controlled sentence functions differs significantly from other tasks on controllable text generation (.", "labels": [], "entities": [{"text": "text generation", "start_pos": 111, "end_pos": 126, "type": "TASK", "confidence": 0.8154740631580353}]}, {"text": "These studies, involving the control of sentiment polarity, emotion, or tense, fall into local control, more or less, because the controllable variable can be locally re-flected by decoding local variable-related words, e.g., terrible for negative sentiment (, glad for happy emotion (, and was for past tense (.", "labels": [], "entities": []}, {"text": "By contrast, sentence function is a global attribute of text, and controlling sentence function is more challenging in that it requires to adjust the global structure of the entire text, including changing word order and word patterns.", "labels": [], "entities": []}, {"text": "Controlling sentence function in conversational systems faces another challenge: in order to generate informative and meaningful responses, it has to deal with the compatibility of the sentence function and the content.", "labels": [], "entities": []}, {"text": "Similar to most existing neural conversation models (, we are also struggling with universal and meaningless responses for different sentence functions, e.g., \"Is that right?\" for interrogative responses, \"Please!\" for imperative responses and \"Me, too.\" for declarative responses.", "labels": [], "entities": []}, {"text": "The lack of meaningful topics in responses will definitely degrade the utility of the sentence function so that the desired conversational purpose cannot be achieved.", "labels": [], "entities": []}, {"text": "Thus, the task needs to generate responses with both informative content and controllable sentence functions.", "labels": [], "entities": []}, {"text": "In this paper, we propose a conversation generation model to deal with the global control of sentence function and the compatibility of controlling sentence function and generating informative content.", "labels": [], "entities": [{"text": "conversation generation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.7498967051506042}]}, {"text": "We devise an encoder-decoder structure equipped with a latent variable in conditional variational autoencoder (CVAE) (, which cannot only project different sentence functions into different regions in a latent space, but also capture various word patterns within each sentence function.", "labels": [], "entities": []}, {"text": "The latent variable, supervised by a discriminator with the expected function label, is also used to realize the global control of sentence function.", "labels": [], "entities": []}, {"text": "To address the compatibility issue, we use a type controller which lexicalizes the sentence function and the content explicitly.", "labels": [], "entities": []}, {"text": "The type controller estimates a distribution over three word types, i.e., function-related, topic, and ordinary words.", "labels": [], "entities": []}, {"text": "During decoding, the word type distribution will be used to modulate the generation distribution in the decoder.", "labels": [], "entities": []}, {"text": "The type sequence of a response can be viewed as an abstract representation of sentence function.", "labels": [], "entities": []}, {"text": "By this means, the model has an explicit and strong control on the function and the content.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We investigate how to control sentence functions to achieve different conversational purposes in open-domain dialogue systems.", "labels": [], "entities": []}, {"text": "We analyze the difference between this task and other controllable generation tasks.", "labels": [], "entities": []}, {"text": "\u2022 We devise a structure equipped with a latent variable and a type controller to achieve the global control of sentence function and deal with the compatibility of controllable sentence function and informative content in generation.", "labels": [], "entities": []}, {"text": "Experiments show the effectiveness of the model.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model was implemented with TensorFlow . We applied bidirectional GRU with 256 cells to the encoder and GRU with 512 cells to the decoder.", "labels": [], "entities": []}, {"text": "The dimensions of word embedding and function category embedding were both set to 100.", "labels": [], "entities": []}, {"text": "We also set the dimension of latent variables to 128.", "labels": [], "entities": []}, {"text": "The vocabulary size was set to 40,000.", "labels": [], "entities": []}, {"text": "Stochastic gradient descent) was used to optimize our model, with a learning rate of 0.1, a decay rate of 0.9995, and a momentum of 0.9.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.9598252177238464}]}, {"text": "The batch size was set to 128.", "labels": [], "entities": []}, {"text": "Our codes are available at https://github.com/ kepei1106/SentenceFunction.", "labels": [], "entities": []}, {"text": "We chose several state-of-the-art baselines, which were implemented with the settings provided in the original papers: Conditional Seq2Seq (c-seq2seq): A Seq2Seq variant which takes the category (i.e., function type) embedding as additional input at each decoding position.", "labels": [], "entities": []}, {"text": "Mechanism-aware (MA): This model assumes that there are multiple latent responding mechanisms (.", "labels": [], "entities": [{"text": "Mechanism-aware (MA)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6174448579549789}]}, {"text": "The number of responding mechanisms is set to 3, equal to the number of function types.", "labels": [], "entities": []}, {"text": "Knowledge-guided CVAE (KgCVAE): A modified CVAE which aims to control the dialog act of a generated response ().", "labels": [], "entities": []}, {"text": "Metrics: We adopted Perplexity (PPL), Distinct-1 (Dist-1), Distinct-2 (Dist-2) (, and Accuracy (ACC) to evaluate the models at the content and function level.", "labels": [], "entities": [{"text": "Accuracy (ACC)", "start_pos": 86, "end_pos": 100, "type": "METRIC", "confidence": 0.9705098867416382}]}, {"text": "Perplexity can measure the grammaticality of generated responses.", "labels": [], "entities": []}, {"text": "Distinct-1/distinct-2 is the proportion of distinct unigrams/bigrams in all the generated tokens, respectively.", "labels": [], "entities": [{"text": "Distinct-1", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9359022378921509}]}, {"text": "Accuracy measures how accurately the sentence function can be controlled.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9892743229866028}]}, {"text": "Specifically, we compared the prespecified function (as input to the model) with the function of a generated response, which is predicted by the self-attentive classifier (see Section 4.1).: Automatic evaluation with perplexity (PPL), distinct-1 (Dist-1), distinct-2 (Dist-2), and accuracy (ACC).", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 281, "end_pos": 295, "type": "METRIC", "confidence": 0.8673930913209915}]}, {"text": "The integers in the Dist-* cells denote the total number of distinct n-grams.", "labels": [], "entities": [{"text": "Dist-* cells", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.5673656761646271}]}, {"text": "Results: Our model has lower perplexity than cseq2seq and KgCVAE, indicating that the model is comparable with other models in generating grammatical responses.", "labels": [], "entities": []}, {"text": "Note that MA has the lowest perplexity because it tends to generate generic responses.", "labels": [], "entities": [{"text": "MA", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.6448375582695007}]}, {"text": "The scores indicate the percentages that our model wins the baselines after removing tie pairs.", "labels": [], "entities": []}, {"text": "The scores of our model marked with * are significantly better than the competitors (Sign Test, p-value < 0.05).", "labels": [], "entities": [{"text": "Sign Test", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.5638688504695892}]}, {"text": "As for distinct-1 and distinct-2, our model generates remarkably more distinct unigrams and bigrams than the baselines, indicating that our model can generate more diverse and informative responses compared to the baselines.", "labels": [], "entities": []}, {"text": "In terms of sentence function accuracy, our model outperforms all the baselines and achieves the best accuracy of 0.992, which indicates that our model can control the sentence function more precisely.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9766610264778137}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.998309850692749}]}, {"text": "MA has a very low score because there is no direct way to control sentence function, instead, it learns automatically from the data.", "labels": [], "entities": [{"text": "MA", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.5088981986045837}]}, {"text": "To evaluate the generation quality and how well the models can control sentence function, we conducted pair-wise comparison.", "labels": [], "entities": []}, {"text": "200 posts were randomly sampled from the test set and each model was required to generate responses with three function types to each post.", "labels": [], "entities": []}, {"text": "For each pair of responses (one by our model and the other by a baseline, along with the post), annotators were hired to give a preference (win, lose, or tie).", "labels": [], "entities": []}, {"text": "The total annotation amounts to 200\u00d73\u00d73\u00d73=5,400 since we have three baselines, three function types, and three metrics.", "labels": [], "entities": []}, {"text": "We resorted to a crowdsourcing service for annotation, and each pair-wise comparison was judged by 5 curators.", "labels": [], "entities": []}, {"text": "Metrics: We designed three metrics to evaluate the models from the perspectives of sentence function and content: grammaticality (whether a response is grammatical and coherent with the sentence function we prespecified), appropriateness (whether a response is a logical and appropriate reply to its post), and informativeness (whether a response provides meaningful information via the topic words relevant to the post).", "labels": [], "entities": []}, {"text": "Note that the three metrics were separately evaluated.", "labels": [], "entities": []}, {"text": "Results: The scores in represent the percentages that our model wins a baseline after removing tie pairs.", "labels": [], "entities": []}, {"text": "A value larger than 0.5 indicates that our model outperforms its competitor.", "labels": [], "entities": []}, {"text": "Our model outperforms the baselines significantly inmost cases (Sign Test, with p-value < 0.05).", "labels": [], "entities": [{"text": "Sign Test", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.6308948993682861}]}, {"text": "Among the three function types, our model performs significantly better than the baselines when generating declarative and imperative responses.", "labels": [], "entities": []}, {"text": "As for interrogative responses, our model is better but the difference is not significant in some settings.", "labels": [], "entities": []}, {"text": "This is because interrogative patterns are more apparent and easier to learn, thereby all the models can capture some of the patterns to generate grammatical and appropriate responses, resulting in more ties.", "labels": [], "entities": []}, {"text": "By contrast, declarative and imperative responses have less apparent patterns whereas our model is better at capturing the global patterns through modeling the word types explicitly.", "labels": [], "entities": []}, {"text": "We can also see that our model obtains particularly high scores in informativeness.", "labels": [], "entities": []}, {"text": "This demonstrates that our model is better to generate more informative responses, and is able to control sentence functions at the same time.", "labels": [], "entities": []}, {"text": "The annotation statistics are shown in.", "labels": [], "entities": []}, {"text": "The percentage of annotations that at least 4 judges assign the same label (at least 4/5 agreement) is larger than 50%, and the percentage for at least 3/5 agreement is about 90%, indicating that annotators reached a moderate agreement.: Annotation statistics.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 238, "end_pos": 248, "type": "METRIC", "confidence": 0.6575405597686768}]}, {"text": "At least n/5 means there are no less than n judges assigning the same label to a record during annotation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Automatic evaluation with perplexity  (PPL), distinct-1 (Dist-1), distinct-2 (Dist-2), and  accuracy (ACC). The integers in the Dist-* cells  denote the total number of distinct n-grams.", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 102, "end_pos": 116, "type": "METRIC", "confidence": 0.7840122580528259}]}, {"text": " Table  5. The percentage of annotations that at least 4  judges assign the same label (at least 4/5 agree- ment) is larger than 50%, and the percentage for at  least 3/5 agreement is about 90%, indicating that  annotators reached a moderate agreement.", "labels": [], "entities": [{"text": "agree- ment)", "start_pos": 101, "end_pos": 113, "type": "METRIC", "confidence": 0.8771572411060333}]}, {"text": " Table 5: Annotation statistics. At least n/5 means  there are no less than n judges assigning the same  label to a record during annotation.", "labels": [], "entities": []}]}