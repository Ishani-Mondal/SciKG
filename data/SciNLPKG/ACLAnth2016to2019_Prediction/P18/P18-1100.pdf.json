{"title": [{"text": "TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring", "labels": [], "entities": [{"text": "TDNN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9075040221214294}, {"text": "Prompt-independent Automated Essay Scoring", "start_pos": 42, "end_pos": 84, "type": "TASK", "confidence": 0.7144912332296371}]}], "abstractContent": [{"text": "Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data.", "labels": [], "entities": [{"text": "essay scoring (AES)", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.6958196043968201}]}, {"text": "Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available.", "labels": [], "entities": []}, {"text": "To close this gap, a two-stage deep neural network (TDNN) is proposed.", "labels": [], "entities": []}, {"text": "In particular, in the first stage, using the rated essays for non-target prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step.", "labels": [], "entities": []}, {"text": "Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8485732972621918}]}], "introductionContent": [{"text": "Automated essay scoring (AES) utilizes natural language processing and machine learning techniques to automatically rate essays written fora target prompt).", "labels": [], "entities": [{"text": "Automated essay scoring (AES)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7546573927005132}]}, {"text": "Currently, the AES systems have been widely used in large-scale English writing tests, e.g. Graduate Record Examination (GRE), to reduce the human efforts in the writing assessments).", "labels": [], "entities": [{"text": "AES", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.8606725931167603}]}, {"text": "Existing AES approaches are promptdependent, where, given a target prompt, rated essays for this particular prompt are required for training).", "labels": [], "entities": []}, {"text": "While the established models are effective, we argue that the models for prompt-independent AES are also desirable to allow for better feasibility and flexibility of AES systems especially when the rated essays fora target prompt are difficult to obtain or even unaccessible.", "labels": [], "entities": []}, {"text": "For example, in a writing test within a small class, students are asked to write essays fora target prompt without any rated examples, where the prompt-dependent methods are unlikely to provide effective AES due to the lack of training data.", "labels": [], "entities": [{"text": "AES", "start_pos": 204, "end_pos": 207, "type": "METRIC", "confidence": 0.9901874661445618}]}, {"text": "Prompt-independent AES, however, has drawn little attention in the literature, where there only exists unrated essays written for the target prompt, as well as the rated essays for several non-target prompts.", "labels": [], "entities": [{"text": "Prompt-independent AES", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5943158864974976}]}, {"text": "We argue that it is not straightforward, if possible, to apply the established promptdependent AES methods for the mentioned prompt-independent scenario.", "labels": [], "entities": [{"text": "AES", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.6755503416061401}]}, {"text": "On one hand, essays for different prompts may differ a lot in the uses of vocabulary, the structure, and the grammatic characteristics; on the other hand, however, established prompt-dependent AES models are designed to learn from these prompt-specific features, including the on/off-topic degree, the tfidf weights of topical terms), and the n-gram features extracted from word semantic embeddings (.", "labels": [], "entities": []}, {"text": "Consequently, the prompt-dependent models can hardly learn generalized rules from rated essays for nontarget prompts, and are not suitable for the promptindependent AES.", "labels": [], "entities": [{"text": "AES", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.8415181636810303}]}, {"text": "Being aware of this difficulty, to this end, a twostage deep neural network, coined as TDNN, is proposed to tackle the prompt-independent AES problem.", "labels": [], "entities": []}, {"text": "In particular, to mitigate the lack of the prompt-dependent labeled data, at the first stage, a shallow model is trained on a number of rated essays for several non-target prompts; given a target prompt and a set of essays to rate, the trained model is employed to generate pseudo training data by selecting essays with the extreme quality.", "labels": [], "entities": []}, {"text": "At the second stage, a novel end-to-end hybrid deep neural network learns prompt-dependent features from these selected training data, by considering semantic, part-of-speech, and syntactic features.", "labels": [], "entities": []}, {"text": "The contributions in this paper are threefold: 1) a two-stage learning framework is proposed to bridge the gap between the target and non-target prompts, by only consuming rated essays for nontarget prompts as training data; 2) a novel deep model is proposed to learn from pseudo labels by considering semantic, part-of-speech, and syntactic features; and most importantly, 3) to the best of our knowledge, the proposed TDNN is actually the first approach dedicated to addressing the prompt-independent AES.", "labels": [], "entities": []}, {"text": "Evaluation on the standard ASAP dataset demonstrates the effectiveness of the proposed method.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.7411034852266312}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe our novel TDNN model, including the two-stage framework and the proposed deep model.", "labels": [], "entities": []}, {"text": "Following that, we describe the setup of our empirical study in Section 3, thereafter present the results and provide analyzes in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 recaps existing literature and put our work in context, before drawing final conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Automated Student Assessment Prize (ASAP) dataset has been widely used for AES (, and is also employed as the prime evaluation instrument herein.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP) dataset", "start_pos": 4, "end_pos": 53, "type": "DATASET", "confidence": 0.5949781909584999}, {"text": "AES", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.49032217264175415}]}, {"text": "In total, AS-AP consists of eight sets of essays, each of which associates to one prompt, and is originally written by students between Grade 7 and Grade 10.", "labels": [], "entities": []}, {"text": "As summarized in, essays from different sets differ in their rating criteria, length, as well as the rating distribution 1 . Cross-validation.", "labels": [], "entities": []}, {"text": "To fully employ the rated data, a prompt-wise eight-fold cross validation on the ASAP is used for evaluation.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.4067608118057251}]}, {"text": "In each fold, essays corresponding to a prompt is reserved for testing, and the remaining essays are used as training data.", "labels": [], "entities": []}, {"text": "The model outputs are first uniformly re-scaled into, mirroring the range of ratings in practice.", "labels": [], "entities": []}, {"text": "Thereafter, akin to, we report our results primarily based on the quadratic weighted Kappa (QWK), examining the agreement between the predicted ratings and the ground truth.", "labels": [], "entities": [{"text": "quadratic weighted Kappa (QWK)", "start_pos": 66, "end_pos": 96, "type": "METRIC", "confidence": 0.8777764737606049}]}, {"text": "Pearson correlation coefficient (PCC) and Spearman rankorder correlation coefficient (SCC) are also reported.", "labels": [], "entities": [{"text": "Pearson correlation coefficient (PCC)", "start_pos": 0, "end_pos": 37, "type": "METRIC", "confidence": 0.9277232786019644}, {"text": "Spearman rankorder correlation coefficient (SCC)", "start_pos": 42, "end_pos": 90, "type": "METRIC", "confidence": 0.8100606841700417}]}, {"text": "The correlations obtained from individual folds, as well as the average overall eight folds, are reported as the ultimate results.", "labels": [], "entities": []}, {"text": "Since the promptindependent AES is of interests in this work, the existing AES models are adapted for prompt-independent rating prediction, serving as baselines.", "labels": [], "entities": [{"text": "prompt-independent rating prediction", "start_pos": 102, "end_pos": 138, "type": "TASK", "confidence": 0.6326890687147776}]}, {"text": "This is due to the facts that the Average depth of the parser tree of each sentence in an essay 11 Average depth of each leaf node in the parser tree of each sentence: Handcrafted features used in learning the prompt-independent RankSVM.", "labels": [], "entities": []}, {"text": "prompt-dependent and -independent models differ a lot in terms of problem settings and model designs, especially in their requirements for the training data, where the latter ones release the prompt-dependent requirements and thereby are accessible to more data.", "labels": [], "entities": []}, {"text": "-RankSVM, using handcrafted features for AES, is trained on a set of pre-defined promptindependent features as listed in, where the features are standardized beforehand to remove the mean and variance.", "labels": [], "entities": [{"text": "RankSVM", "start_pos": 1, "end_pos": 8, "type": "DATASET", "confidence": 0.9237874150276184}]}, {"text": "The RankSVM is also used for the prompt-independent stage in our proposed TDNN model.", "labels": [], "entities": [{"text": "RankSVM", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8657832741737366}]}, {"text": "In particular, the linear kernel RankSVM 2 is employed, where C is set to 5 according to our pilot experiments.", "labels": [], "entities": []}, {"text": "Two-layer bi-LSTM with GloVe for AES () is employed as another baseline.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9541765451431274}]}, {"text": "Regularized word embeddings are dropped to avoid over-fitting the prompt-specific features.", "labels": [], "entities": []}, {"text": "This model (Taghipour and Ng, 2016) employs a convolutional (CNN) layer over one-hot representations of words, followed by an LSTM layer to encode word sequences in a given essay.", "labels": [], "entities": []}, {"text": "A linear layer with sigmoid activation function is then employed to predict the essay rating.", "labels": [], "entities": []}, {"text": "This model ( ) employs a CNN layer to encode word sequences into sentences, followed by an LSTM layer to generate the essay representation.", "labels": [], "entities": []}, {"text": "An attention mechanism is added to model the influence of individual sentences on the final essay representation.", "labels": [], "entities": []}, {"text": "For the proposed TDNN model, as introduced in Section 2.2, different variants of TDNN are examined by using one or multiple components out of the semantic, POS and the syntactic networks.", "labels": [], "entities": []}, {"text": "The combinations being considered are listed in the following.", "labels": [], "entities": []}, {"text": "In particular, the dimensions of POS tags and syntactic network are fixed to 50, whereas the sizes of the hidden units in LSTM, as well as the output units of the linear layers are tuned by grid search.", "labels": [], "entities": []}, {"text": "-TDNN(Sem) only includes the semantic building block, which is similar to the two-layer LSTM neural network from (Alikaniotis et al., 2016) but without regularizing the word embeddings; -TDNN(Sem+POS) employs the semantic and the POS building blocks; -TDNN(Sem+Synt) uses the semantic and the syntactic network building blocks; -TDNN(POS+Synt) includes the POS and the syntactic network building blocks; -TDNN(ALL) employs all three building blocks.", "labels": [], "entities": []}, {"text": "The use of POS or syntactic network alone is not presented for brevity given the facts that they perform no better than TDNN(POS+Synt) in our pilot experiments.", "labels": [], "entities": []}, {"text": "Source code of the TDNN model is publicly available to enable further comparison 3 .", "labels": [], "entities": [{"text": "TDNN", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.8506653308868408}]}], "tableCaptions": [{"text": " Table 2: Handcrafted features used in learning the  prompt-independent RankSVM.", "labels": [], "entities": [{"text": "RankSVM", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.8621370196342468}]}, {"text": " Table 3: Correlations between AES and manual ratings for different competing methods are reported for  individual prompts. The average results among different prompts are summarized in the bottom right.  The best results are highlighted in bold for individual prompts.", "labels": [], "entities": [{"text": "AES", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.8379836678504944}]}, {"text": " Table 4: Linear correlations between the performance of", "labels": [], "entities": []}, {"text": " Table 5: The numbers of the selected positive and negative", "labels": [], "entities": []}]}