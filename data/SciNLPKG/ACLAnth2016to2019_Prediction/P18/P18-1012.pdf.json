{"title": [{"text": "Towards Understanding the Geometry of Knowledge Graph Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Knowledge Graph (KG) embedding has emerged as a very active area of research over the last few years, resulting in the development of several embedding methods.", "labels": [], "entities": []}, {"text": "These KG embedding methods represent KG entities and relations as vectors in a high-dimensional space.", "labels": [], "entities": []}, {"text": "Despite this popularity and effectiveness of KG em-beddings in various tasks (e.g., link prediction), geometric understanding of such embeddings (i.e., arrangement of entity and relation vectors in vector space) is un-explored-we fill this gap in the paper.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.8226920664310455}]}, {"text": "We initiate a study to analyze the geometry of KG embeddings and correlate it with task performance and other hyperparame-ters.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first study of its kind.", "labels": [], "entities": []}, {"text": "Through extensive experiments on real-world datasets, we discover several insights.", "labels": [], "entities": []}, {"text": "For example, we find that there are sharp differences between the geometry of embeddings learnt by different classes of KG embeddings methods.", "labels": [], "entities": []}, {"text": "We hope that this initial study will inspire other follow-up research on this important but unexplored problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge Graphs (KGs) are multi-relational graphs where nodes represent entities and typededges represent relationships among entities.", "labels": [], "entities": []}, {"text": "Recent research in this area has resulted in the development of several large KGs, such as NELL (,, and Freebase (), among others.", "labels": [], "entities": [{"text": "NELL", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.79862380027771}, {"text": "Freebase", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.9430605173110962}]}, {"text": "These KGs contain thousands of predicates (e.g., person, city, mayorOf(person, city), etc.), and millions of triples involving such predicates, e.g., (Bill de Blasio, mayorOf, New York City).", "labels": [], "entities": []}, {"text": "The problem of learning embeddings for Knowledge Graphs has received significant attention in recent years, with several methods being proposed (.", "labels": [], "entities": []}, {"text": "These methods represent entities and relations in a KG as vectors in high dimensional space.", "labels": [], "entities": []}, {"text": "These vectors can then be used for various tasks, such as, link prediction, entity classification etc.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.841022253036499}, {"text": "entity classification", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.8119995594024658}]}, {"text": "Starting with, there have been many KG embedding methods such as TransH (), TransR ( and) which represent relations as translation vectors from head entities to tail entities.", "labels": [], "entities": []}, {"text": "These are additive models, as the vectors interact via addition and subtraction.", "labels": [], "entities": []}, {"text": "Other KG embedding models, such as, DistMult), HolE ( are multiplicative where entityrelation-entity triple likelihood is quantified by a multiplicative score function.", "labels": [], "entities": [{"text": "HolE", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9072728753089905}]}, {"text": "All these methods employ a score function for distinguishing correct triples from incorrect ones.", "labels": [], "entities": []}, {"text": "In spite of the existence of many KG embedding methods, our understanding of the geometry and structure of such embeddings is very shallow.", "labels": [], "entities": []}, {"text": "A recent work analyzed the geometry of word embeddings.", "labels": [], "entities": []}, {"text": "However, the problem of analyzing geometry of KG embeddings is still unexplored -we fill this important gap.", "labels": [], "entities": []}, {"text": "In this paper, we analyze the geometry of such vectors in terms of their lengths and conicity, which, as defined in Section 4, describes their positions and orientations in the vector space.", "labels": [], "entities": []}, {"text": "We later study the effects of model type and training hyperparameters on the geometry of KG embeddings and correlate geometry with performance.", "labels": [], "entities": []}, {"text": "We make the following contributions: \u2022 We initiate a study to analyze the geometry of various Knowledge Graph (KG) embeddings.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first study of its kind.", "labels": [], "entities": []}, {"text": "We also formalize various metrics which can be used to study geometry of a set of vectors.", "labels": [], "entities": []}, {"text": "\u2022 Through extensive analysis, we discover several interesting insights about the geometry of KG embeddings.", "labels": [], "entities": []}, {"text": "For example, we find systematic differences between the geometries of embeddings learned by additive and multiplicative KG embedding methods.", "labels": [], "entities": []}, {"text": "\u2022 We also study the relationship between geometric attributes and predictive performance of the embeddings, resulting in several new insights.", "labels": [], "entities": []}, {"text": "For example, in case of multiplicative models, we observe that for entity vectors generated with a fixed number of negative samples, lower conicity (as defined in Section 4) or higher average vector length lead to higher performance.", "labels": [], "entities": []}, {"text": "Source code of all the analysis tools developed as part of this paper is available at https://github.com/malllabiisc/ kg-geometry.", "labels": [], "entities": []}, {"text": "We are hoping that these resources will enable one to quickly analyze the geometry of any KG embedding, and potentially other embeddings as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: We run our experiments on subsets of two widely used datasets, viz., Freebase (Bollacker et al., 2008) and WordNet, called FB15k and WN18 (), respectively.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9380584359169006}, {"text": "FB15k", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.9690858721733093}, {"text": "WN18", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.6764003038406372}]}, {"text": "We detail the characteristics of these datasets in.", "labels": [], "entities": []}, {"text": "Please note that while the results presented in Section 6 are on the FB15K dataset, we reach the same conclusions on WN18.", "labels": [], "entities": [{"text": "FB15K dataset", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.993681013584137}, {"text": "WN18", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.9832173585891724}]}, {"text": "The plots for our experiments on WN18 can be found in the Supplementary Section.", "labels": [], "entities": [{"text": "WN18", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9240986704826355}, {"text": "Supplementary Section", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.727820947766304}]}, {"text": "Hyperparameters: We experiment with multiple values of hyperparameters to understand their effect on the geometry of KG embeddings.", "labels": [], "entities": []}, {"text": "Specifically, we vary the dimension of the generated vectors between {50, 100, 200} and the number of negative samples used during training between {1, 50, 100}.", "labels": [], "entities": []}, {"text": "For more details on algorithm specific hyperparameters, we refer the reader to the Supplementary Section.", "labels": [], "entities": [{"text": "Supplementary Section", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.7100076228380203}]}, {"text": "2 2 For training, we used codes from https://github.", "labels": [], "entities": []}, {"text": "Frequency Bins: We follow () for entity and relation samples used in the analysis.", "labels": [], "entities": []}, {"text": "Multiple bins of entities and relations are created based on their frequencies and 100 randomly sampled vectors are taken from each bin.", "labels": [], "entities": []}, {"text": "These set of sampled vectors are then used for our analysis.", "labels": [], "entities": []}, {"text": "For more information about sampling vectors, please refer to (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of datasets used in the paper.", "labels": [], "entities": []}, {"text": " Table 2.  Please note that while the results presented in  Section 6 are on the FB15K dataset, we reach the  same conclusions on WN18. The plots for our ex- periments on WN18 can be found in the Supple- mentary Section.  Hyperparameters: We experiment with multiple  values of hyperparameters to understand their ef- fect on the geometry of KG embeddings. Specif- ically, we vary the dimension of the generated  vectors between {50, 100, 200} and the number  of negative samples used during training between  {1, 50, 100}. For more details on algorithm spe- cific hyperparameters, we refer the reader to the  Supplementary Section. 2", "labels": [], "entities": [{"text": "FB15K dataset", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9953047037124634}, {"text": "WN18", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.9867553114891052}, {"text": "WN18", "start_pos": 171, "end_pos": 175, "type": "DATASET", "confidence": 0.9800277352333069}]}]}