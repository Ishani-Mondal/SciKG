{"title": [{"text": "Multimodal Relational Tensor Network for Sentiment and Emotion Classification", "labels": [], "entities": [{"text": "Multimodal Relational Tensor", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7775718967119852}, {"text": "Sentiment and Emotion Classification", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.8701835125684738}]}], "abstractContent": [{"text": "Understanding Affect from video segments has brought researchers from the language, audio and video domains together.", "labels": [], "entities": []}, {"text": "Most of the current multimodal research in this area deals with various techniques to fuse the modalities, and mostly treat the segments of a video independently.", "labels": [], "entities": []}, {"text": "Motivated by the work of (Zadeh et al., 2017) and (Poria et al., 2017), we present Relational Tensor Network architecture where we use the intermodal interactions within a segment and also consider the sequence of segments in a video to model the inter-segment intermodal interactions.", "labels": [], "entities": [{"text": "Relational Tensor Network", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.8086560368537903}]}, {"text": "We also generate rich representations of text and audio modalities by leveraging richer audio and linguistic context alongwith fusing fine-grained knowledge based polarity scores from text.", "labels": [], "entities": []}, {"text": "We present the results of our model on CMU-MOSEI dataset and show that our model outperforms many baselines and state of the art methods for sentiment classification and emotion recognition.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.9819561541080475}, {"text": "sentiment classification", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.9522707462310791}, {"text": "emotion recognition", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.7126685678958893}]}], "introductionContent": [{"text": "Sentiment Analysis is broadly defined as the computational study of subjective elements such as opinions, attitudes, and emotions towards other objects or persons.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.940933108329773}]}, {"text": "Sentiments attach to modalities such as text, audio and video at different levels of granularity and are useful in deriving social insights about various entities such as movies, products, persons or organizations.", "labels": [], "entities": []}, {"text": "Emotion Understanding is another closely related field that commonly deals with analysis of audio, video, and other sensory signals forgetting psychological and behavioral insights about an individual's mental state.", "labels": [], "entities": [{"text": "Emotion Understanding", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9130150675773621}, {"text": "analysis of audio, video, and other sensory signals forgetting psychological and behavioral insights about an individual's mental state", "start_pos": 80, "end_pos": 215, "type": "Description", "confidence": 0.6710187154156821}]}, {"text": "Emotions are defined as brief organically synchronized evaluations of major events whereas sentiments on the other hand are considered as more enduring beliefs and dispositions towards objects or persons.", "labels": [], "entities": []}, {"text": "The field of Emotion Understanding has rich literature with many interesting models of understanding).", "labels": [], "entities": [{"text": "Emotion Understanding", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.9361610412597656}]}, {"text": "In this work, we explore methods that combine various unimodal techniques for classification alongwith multimodal techniques for fusion of cross modal interactions to perform sentiment analysis and emotion understanding.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.9621270596981049}, {"text": "emotion understanding", "start_pos": 198, "end_pos": 219, "type": "TASK", "confidence": 0.7832561135292053}]}, {"text": "We develop and test our approaches on the CMU-MOSEI dataset () as part of the ACL Multimodal Emotion Recognition grand challenge.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.9750395119190216}, {"text": "ACL Multimodal Emotion Recognition grand challenge", "start_pos": 78, "end_pos": 128, "type": "TASK", "confidence": 0.6870437761147817}]}, {"text": "CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset is a newly released large dataset of multimodal sentiment analysis and emotion recognition on YouTube video segments.", "labels": [], "entities": [{"text": "CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset", "start_pos": 0, "end_pos": 74, "type": "DATASET", "confidence": 0.7175894325429742}, {"text": "multimodal sentiment analysis", "start_pos": 112, "end_pos": 141, "type": "TASK", "confidence": 0.6760874092578888}, {"text": "emotion recognition on YouTube video segments", "start_pos": 146, "end_pos": 191, "type": "TASK", "confidence": 0.7753656307856241}]}, {"text": "The dataset contains more than 23,500 sentence utterance videos from more than 1000 online YouTube speakers.", "labels": [], "entities": []}, {"text": "The dataset has several interesting properties such as being gender balanced, containing various topics and monologue videos from people with different personality traits.", "labels": [], "entities": []}, {"text": "The videos are manually transcribed and properly punctuated.", "labels": [], "entities": []}, {"text": "Since the dataset comprises of natural audio-visual opinionated expressions of the speakers, it provides an excellent testbed for research in emotion and sentiment understanding.", "labels": [], "entities": [{"text": "emotion and sentiment understanding", "start_pos": 142, "end_pos": 177, "type": "TASK", "confidence": 0.6572255343198776}]}, {"text": "The videos are cut into continuous segments and the segments are annotated with 7 point scale sentiment labels and 4 point scale emotion categories corresponding to the Eckman'a 6 basic emotion classes).", "labels": [], "entities": []}, {"text": "The opinionated expressions in the segments contain visual cues, audio variations in signal as well textual expressions showing various subtle and non-obvious interactions across the modalities for both sentiment and emotion classification.", "labels": [], "entities": [{"text": "sentiment and emotion classification", "start_pos": 203, "end_pos": 239, "type": "TASK", "confidence": 0.7128014788031578}]}, {"text": "What differentiates our work from existing lit-erature is (i) application of a novel cross modal fusion technique across the temporal segments of the multimodal channel (ii) use of rich shallow semantic domain knowledge that include a large number of psycholinguistic features and resources for sentiment and emotion classification and (iii) extraction of emotion aware acoustic phoneme level features using a novel method and architecture.", "labels": [], "entities": [{"text": "sentiment and emotion classification", "start_pos": 295, "end_pos": 331, "type": "TASK", "confidence": 0.761560395359993}]}, {"text": "Our unimodal research focus in this paper is an exploration of speech sentiment and emotion recognition using various text dependent and text independent techniques.", "labels": [], "entities": [{"text": "speech sentiment and emotion recognition", "start_pos": 63, "end_pos": 103, "type": "TASK", "confidence": 0.6924327254295349}]}, {"text": "On the text modality experiments, we've explored (i) fusion of Lexicons as additional input features (ii) fusion of polarity discriminating lexico-syntactic fine-grained scores as additional input features (iii) fusion of rich contextualized embeddings as additional input features to the classification pipeline.", "labels": [], "entities": []}, {"text": "On audio modality, we've used a novel pipeline to generate the iVectors and Phoneme level utterance features.", "labels": [], "entities": []}, {"text": "For fusion of multimodal information, we have explored techniques that leverage intramodal and inter-modal dynamics and fused them together in a novel Relational Tensor Network architecture.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present multiple sets of experiments in order to evaluate the different models, impact of textual and audio features on sentiment and emotion prediction.", "labels": [], "entities": [{"text": "sentiment and emotion prediction", "start_pos": 123, "end_pos": 155, "type": "TASK", "confidence": 0.7606372982263565}]}, {"text": "Our training data consists of CMU-MOSEI training set where we do a 90/10 split for validation and early stopping experiments.", "labels": [], "entities": [{"text": "CMU-MOSEI training set", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.8910871346791586}]}, {"text": "All our results in this paper are reported on the CMU-MOSEI validation set 1 . show the performance of the various models on sentiment and emotion classification.", "labels": [], "entities": [{"text": "CMU-MOSEI validation set", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.9588224093119303}, {"text": "sentiment and emotion classification", "start_pos": 125, "end_pos": 161, "type": "TASK", "confidence": 0.7843175679445267}]}, {"text": "We have used three LSTM based unimodal baselines, each for audio, video and text modalities.", "labels": [], "entities": []}, {"text": "From the table, we see that unimodal-text network outperforms both audio and video modalities for sentiment.", "labels": [], "entities": [{"text": "sentiment", "start_pos": 98, "end_pos": 107, "type": "TASK", "confidence": 0.9677212834358215}]}, {"text": "Unimodal-text also outperforms SVM multimodal for sentiment analysis, which is an SVM model trained on concatenated features from all the three modalities.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9504427015781403}]}, {"text": "The early fusion network is an LSTM based network(an extension of the unimodal networks),that takes in concatenated features from the three modalities.", "labels": [], "entities": []}, {"text": "This LSTM model outperforms the SVM multimodal baseline by almost 5% binary class accuracy scores for sentiment analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.7663300633430481}, {"text": "sentiment analysis", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.9599750638008118}]}, {"text": "All of these LSTM based networks outperform SVM by a huge margin in the 7-class classification scores and MAE for sentiment analysis.", "labels": [], "entities": [{"text": "MAE", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9957451224327087}, {"text": "sentiment analysis", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.953759491443634}]}, {"text": "The T F N network with rich set of textual features slightly outperforms the simple concatenation technique(early fusion model) for sentiment and emotion recognition.", "labels": [], "entities": [{"text": "sentiment and emotion recognition", "start_pos": 132, "end_pos": 165, "type": "TASK", "confidence": 0.7920932024717331}]}, {"text": "The model with the best performance is the Relational Tensor Network model for both sentiment and emotion recognition that considers the neighboring tensor fusion networks fora given segment.", "labels": [], "entities": [{"text": "sentiment and emotion recognition", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.8155320584774017}]}, {"text": "The test set was not released at the time of writing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentiment Analysis Model Results", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9787261188030243}]}, {"text": " Table 2: Emotion Recognition Model Results -MAE scores", "labels": [], "entities": [{"text": "Emotion Recognition", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8762479424476624}, {"text": "MAE", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9830264449119568}]}]}