{"title": [{"text": "Efficient Large-Scale Neural Domain Classification with Personalized Attention", "labels": [], "entities": [{"text": "Efficient Large-Scale Neural Domain Classification", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.5734849989414215}]}], "abstractContent": [{"text": "In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs).", "labels": [], "entities": [{"text": "mapping spoken language utterances", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.7185332328081131}]}, {"text": "This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment built-in first party domains to rapidly increase domain coverage and overall IPDA capabilities.", "labels": [], "entities": []}, {"text": "We propose a scalable neu-ral model architecture with a shared en-coder, a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently.", "labels": [], "entities": []}, {"text": "Our architecture is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speedup compared to full model retraining.", "labels": [], "entities": []}, {"text": "We consider the practical constraints of real-time production systems, and design to minimize memory footprint and runtime la-tency.", "labels": [], "entities": []}, {"text": "We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7611016631126404}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.8829489946365356}]}], "introductionContent": [{"text": "Intelligent personal digital assistants (IPDAs) are one of the most advanced and successful artificial intelligence applications that have spoken language understanding (SLU).", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 139, "end_pos": 174, "type": "TASK", "confidence": 0.7571293612321218}]}, {"text": "Many IPDAs have recently emerged in industry including Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana.", "labels": [], "entities": []}, {"text": "IPDAs have traditionally supported only dozens of well-separated domains, each defined in terms of a specific application or functionality such as calendar and local search.", "labels": [], "entities": [{"text": "IPDAs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8591073751449585}]}, {"text": "To rapidly increase domain coverage and extend capabilities, some IPDAs have released Software Development Toolkits (SDKs) to allow third-party developers to quickly build and integrate new domains, which we refer to as skills henceforth.", "labels": [], "entities": []}, {"text": "Amazon's Alexa Skills Kit (), Google's Actions and Microsoft's Cortana Skills Kit are all examples of such SDKs.", "labels": [], "entities": [{"text": "Alexa Skills Kit", "start_pos": 9, "end_pos": 25, "type": "DATASET", "confidence": 0.774467408657074}]}, {"text": "Alexa Skills Kit is the largest of these services with over 40,000 skills.", "labels": [], "entities": [{"text": "Alexa Skills Kit", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9443249305089315}]}, {"text": "For IPDAs, finding the most relevant skill to handle an utterance is an open problem for three reasons.", "labels": [], "entities": []}, {"text": "First, the sheer number of skills makes the task difficult.", "labels": [], "entities": []}, {"text": "Unlike traditional systems that have on the order of 10-20 built-in domains, largescale IPDAs can have up to 40,000 skills.", "labels": [], "entities": []}, {"text": "Second, the number of skills is rapidly expanding with 100+ new skills added per week.", "labels": [], "entities": []}, {"text": "Largescale IPDAs should be able to accommodate new skills efficiently without compromising performance.", "labels": [], "entities": []}, {"text": "Third, unlike traditional built-in domains that are carefully designed to be disjoint by a central team, skills are built independently by different developers and can cover overlapping functionalities.", "labels": [], "entities": []}, {"text": "For instance, there are over 50 recipe skills in Alexa that can handle recipe-related utterances.", "labels": [], "entities": [{"text": "Alexa", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9294179081916809}]}, {"text": "One simple solution to this problem has been to require the user to explicitly mention a skill name and follow a strict invocation pattern as in \"Ask {Uber} to {get me a ride}.\"", "labels": [], "entities": []}, {"text": "However, this significantly limits the natural interaction with IPDAs.", "labels": [], "entities": []}, {"text": "Users have to remember skill names and invocation patterns, and it places a cognitive burden on users who tend to forget both.", "labels": [], "entities": []}, {"text": "Skill discovery is difficult with a pure voice user interface, it is hard for users to know the capabilities of thousands of skills a priori, which may leads to limited user en-gagement with skills and potentially with IPDAs.", "labels": [], "entities": [{"text": "Skill discovery", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9495508670806885}, {"text": "IPDAs", "start_pos": 219, "end_pos": 224, "type": "DATASET", "confidence": 0.887883722782135}]}, {"text": "In this paper, we propose a solution that addresses all three practical challenges without requiring skill names or invocation patterns.", "labels": [], "entities": []}, {"text": "Our approach is based on a scalable neural model architecture with a shared encoder, a skill attention mechanism and skill-specific classification networks that can efficiently perform large-scale skill classification in IPDAs using a weakly supervised training dataset.", "labels": [], "entities": [{"text": "skill classification", "start_pos": 197, "end_pos": 217, "type": "TASK", "confidence": 0.7589788138866425}]}, {"text": "We demonstrate that our model achieves a high accuracy on a manually transcribed test set after being trained with weak supervision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.998936116695404}]}, {"text": "Moreover, our architecture is designed to efficiently integrate new skills that appear in-between full model retraining cycles into the model.", "labels": [], "entities": []}, {"text": "Besides accuracy, we also keep practical constraints in mind and focus on minimizing memory footprint and runtime latency, while ensuring architecture is scalable to thousands of skills, all of which are important for real-time production systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9979871511459351}]}, {"text": "Furthermore, we investigate two different ways of incorporating user personalization information into the model, our naive baseline method adds the information as a 1-bit flag in the feature space of the skill-specific networks, the personalized attention technique computes a convex combination of skill embeddings for the user's enabled skills and significantly outperforms the naive personalization baseline.", "labels": [], "entities": []}, {"text": "We show the effectiveness of our approach with extensive experiments using 1,500 skills from a deployed IPDA system.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we aim to demonstrate the effectiveness of our model architecture in two settings.", "labels": [], "entities": []}, {"text": "First, we will demonstrate that attention based personalization significantly outperforms the baseline approach.", "labels": [], "entities": []}, {"text": "Secondly, we will show that our model new domain bootstrapping procedure results in accuracies comparable to full retraining while requiring less than 1% of the orignal training time.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9658324122428894}]}, {"text": "Weak: This is a weakly supervised dataset was generated by preprocessing utterances with strict invocation patterns according to the setup mentioned in Section 3.", "labels": [], "entities": []}, {"text": "The dataset consists of 5.34M utterances from 637,975 users across 1,500 different skills.", "labels": [], "entities": []}, {"text": "Since we are interested in capturing the temporal effects of the dataset as well as personalization effects, we partitioned the data based both on user and time.", "labels": [], "entities": []}, {"text": "Our core training data for the experiments in this paper was drawn from one month of live usage, the validation data came from the next 15 days of usage, and the test data came from the subsequent 15 days.", "labels": [], "entities": []}, {"text": "The training, validation and test sets are user-independent, and each user belongs to only one of the three sets to ensure no leakage of information.", "labels": [], "entities": []}, {"text": "MTurk: Since the Weak dataset is generated by weak supervision, we verified the performance of our approach with human generated utterances.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7932288646697998}, {"text": "Weak dataset", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8566078245639801}]}, {"text": "A random sample of 12,428 utterances from the test partition of users were presented to 300 human judges, who were asked to produce two natural ways to issue the same command.", "labels": [], "entities": []}, {"text": "This dataset is treated as a representative clean held out test set on which we can observe the generalization of our weakly supervised training and validation data to natural language.", "labels": [], "entities": []}, {"text": "New Skills: In order to simulate the scenario in which new skills appear within a week between model updates, we selected 250 new skills which do not overlap with the skills in the Weak dataset.", "labels": [], "entities": [{"text": "Weak dataset", "start_pos": 181, "end_pos": 193, "type": "DATASET", "confidence": 0.9715704321861267}]}, {"text": "The vocabulary size of 1,500 skills is 200K words, and on average, 5% of the vocabulary for new skills is not covered.", "labels": [], "entities": []}, {"text": "We randomly sampled 4,000 unique utterances for each skill using the same weak supervision method, and split them into 3,000 utterances for training and 1,000 for testing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performance of different variants of our neural", "labels": [], "entities": []}, {"text": " Table 2: Comparison of per-epoch training time (seconds)", "labels": [], "entities": []}, {"text": " Table 3: Top-N prediction accuracy (%) on the full skill set", "labels": [], "entities": [{"text": "prediction", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.42920783162117004}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8753156661987305}]}]}