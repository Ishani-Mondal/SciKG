{"title": [{"text": "Translating a Language You Don't Know in the Chinese Room", "labels": [], "entities": [{"text": "Translating a Language You Don't Know in the Chinese Room", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.8540164611556313}]}], "abstractContent": [{"text": "Ina corruption of John Searle's famous AI thought experiment, the Chinese Room (Searle, 1980), we twist its original intent by enabling humans to translate text, e.g. from Uyghur to English, even if they don't have any prior knowledge of the source language.", "labels": [], "entities": []}, {"text": "Our enabling tool, which we call the Chinese Room, is equipped with the same resources made available to a machine translation engine.", "labels": [], "entities": [{"text": "Chinese Room", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.8770667016506195}, {"text": "machine translation engine", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.7786689798037211}]}, {"text": "We find that our superior language model and world knowledge allows us to create perfectly fluent and nearly adequate translations, with human expertise required only for the target language.", "labels": [], "entities": []}, {"text": "The Chinese Room tool can be used to rapidly create small corpora of parallel data when bilingual translators are not readily available, in particular for low-resource languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain adaptation for machine translation is a well-studied problem.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7759875059127808}, {"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7968620955944061}]}, {"text": "1 Most works assume a system-builder has an adequate amount of outof-domain or 'general' domain parallel sentence training data and some smaller corpus of indomain data that can be used, depending on the size of the in-domain corpus, for additional training, for parameter estimation, or, if the in-domain corpus is very small, simply for system evaluation.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 263, "end_pos": 283, "type": "TASK", "confidence": 0.700545147061348}]}, {"text": "Very little, however, is said of the scenario where there is no in-domain parallel data available, and yet an in-domain system must be built.", "labels": [], "entities": []}, {"text": "In such scenarios one may try to mine parallel data from comparable corpora (), but in cases where even scant (but See http://www.statmt.org/survey/Topic/ DomainAdaptation fora survey of methodologies.", "labels": [], "entities": []}, {"text": "not zero) in-domain monolingual resources are available this is not a feasible strategy and the only way to obtain any reliably measure of quality is to solicit human translations.", "labels": [], "entities": []}, {"text": "However, it maybe difficult to recruit translators to prepare such data, if the language is underrepresented or politically sensitive.", "labels": [], "entities": []}, {"text": "Al- describe an experiment where individual humans translated 10 sentences from Tetun to English, without any prior knowledge of Tetun, based solely on an in-domain bitext of 1,102 sentences.", "labels": [], "entities": []}, {"text": "Without any prior tools, translation was very tedious, inefficient, and impractical for the 10 sentences, taking about one sentence per hour.", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9840278625488281}]}, {"text": "But the experiment successfully showed in principle the feasibility of human translation without prior knowledge of the source language.", "labels": [], "entities": [{"text": "human translation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.6452418565750122}]}, {"text": "We introduce a tool, the Chinese Room, to facilitate efficient human translation without prior knowledge of a source language.", "labels": [], "entities": []}, {"text": "The name is inspired from Searle (1980) who envisioned a monolingual English-speaking human equipped with instructions for answering Chinese questions by manipulating symbols of a Chinese information corpus and the question text to form answers.", "labels": [], "entities": []}, {"text": "While Searle used this idea to argue against 'strong' AI, we thought the setup, i.e. giving a human the tools an NLP model is given (in this case, a machine translation model), was a good one for rapidly generating useful translation data.", "labels": [], "entities": []}, {"text": "Apart from generating human translation data, an additional use of the Chinese Room is to support computational linguists in identifying the challenges of machine translation fora specific language pair and language resources.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.732086569070816}]}, {"text": "By placing humans in the role of the MT, we may better understand the nature and magnitude of out-ofvocabulary gaps, and whether they might be due to morphological complexity, compounding, assimi-lation, spelling variations, insufficient or out-ofdomain parallel corpora or dictionaries, etc.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9726036190986633}]}, {"text": "We found that the Chinese Room can be a useful tool to help generate new ideas for machine translation research.", "labels": [], "entities": [{"text": "Chinese Room", "start_pos": 18, "end_pos": 30, "type": "DATASET", "confidence": 0.9012873470783234}, {"text": "machine translation research", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.8794505794843038}]}], "datasetContent": [{"text": "We have built Chinese Rooms for Bengali, Hungarian, Oromo, Somali, Swahili, Tagalog, Tigrinya, and Uyghur.", "labels": [], "entities": []}, {"text": "For Bengali, two of the authors of this paper translated an article of 10 Bengali sentences to English, without any prior knowledge of Bengali, using the Chinese Room.", "labels": [], "entities": []}, {"text": "To evaluate the results, we asked a native speaker from Bangladesh, a graduate student living in the US who is not a professional translator, to first translate the same 10 sentences independently and then to evaluate our translations.", "labels": [], "entities": []}, {"text": "According to the native speaker our translations were better; we only missed one Bengali word in translation, and were actually aware of it, but were unable to decode it with the resources at hand.", "labels": [], "entities": []}, {"text": "We used the Chinese Room to create small corpora of parallel data in a time-constrained MT system-building scenario.", "labels": [], "entities": [{"text": "Chinese Room", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.9587902724742889}, {"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9649533629417419}]}, {"text": "In this scenario we were required to translate documents from Uyghur to English describing earthquakes and disaster relief efforts.", "labels": [], "entities": []}, {"text": "However, we had no parallel data dealing with this topic, and our use of an unrelated test set (see to estimate overall task performance was not reliable.", "labels": [], "entities": []}, {"text": "We thus wanted to construct an in-domain Uyghur-English parallel corpus.", "labels": [], "entities": []}, {"text": "In the scenario we were given a small number of one-hour sessions with a native informant (NI), a Uyghur native who spoke English and was not a linguistics or computer science expert.", "labels": [], "entities": []}, {"text": "We initially asked the NI use the time to translate documents, one sentence at a time.", "labels": [], "entities": [{"text": "NI", "start_pos": 23, "end_pos": 25, "type": "DATASET", "confidence": 0.9101364016532898}]}, {"text": "This was accomplished at a rate of 360 words per hour, but required another 30-60 minutes of post-editing to ensure fluency.", "labels": [], "entities": []}, {"text": "We next tried typing for the NI (and ensured fluency); this yielded 320 words/hr but did not require post-editing.", "labels": [], "entities": [{"text": "NI", "start_pos": 29, "end_pos": 31, "type": "DATASET", "confidence": 0.8683324456214905}]}, {"text": "Finally we used the Chinese Room to translate and asked the NI to point out any errors.", "labels": [], "entities": [{"text": "Chinese Room", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9292144179344177}, {"text": "translate", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.972307562828064}, {"text": "NI", "start_pos": 60, "end_pos": 62, "type": "DATASET", "confidence": 0.8995139598846436}]}, {"text": "This hour yielded 480 words.", "labels": [], "entities": []}, {"text": "Machine translation quality on the resulting indomain set tracked much better with performance on the evaluation set.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5795891135931015}]}, {"text": "Later on we built a second in-domain set but did not have any further access to the NI.", "labels": [], "entities": [{"text": "NI", "start_pos": 84, "end_pos": 86, "type": "DATASET", "confidence": 0.9646205902099609}]}, {"text": "Using this set of approximate translation to tune parameters yielded a 0.3 BLEU increase in system performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9996211528778076}]}, {"text": "We have trained more than 20 people to use the Chinese Room with very good results for the training test case, Somali.", "labels": [], "entities": [{"text": "Chinese Room", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8989567160606384}, {"text": "Somali", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.9327362179756165}]}, {"text": "We are similarly confident in our translations for Hungarian and Uyghur.", "labels": [], "entities": []}, {"text": "Tagalog and Swahili are recent builds, and translations look very promising.", "labels": [], "entities": []}, {"text": "However, we found the dictionary and bitext resources for Tigrinya (to a lesser degree) and Oromo (to a larger degree) to be too small to confidently translate most sentences.", "labels": [], "entities": []}, {"text": "We were able to translate some sentences completely, and many others partially, but had to rely on the support of non-professional native speakers to complete the translations.", "labels": [], "entities": []}, {"text": "The Chinese Room nevertheless proved to be very useful in this very-low-resource scenario.", "labels": [], "entities": [{"text": "Chinese Room", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9417214393615723}]}, {"text": "We could already build glosses for many words and provide a partial translations, so that the native speaker could finish a sentence faster than starting from scratch.", "labels": [], "entities": []}, {"text": "The Chinese Room also helped the native speaker to more easily find the English words he/she was looking for, and allowed us to make sure that the translation covered all essential parts of the original text.", "labels": [], "entities": []}], "tableCaptions": []}