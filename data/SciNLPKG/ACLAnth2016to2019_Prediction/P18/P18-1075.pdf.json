{"title": [{"text": "Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable", "labels": [], "entities": [{"text": "Domain Adaptation of Bilingual Tasks", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.8368041038513183}]}], "abstractContent": [{"text": "Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6465276479721069}, {"text": "cross-lingual classification", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.7807270884513855}]}, {"text": "Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7605525553226471}]}, {"text": "First, we test a delightfully simple method for domain adaptation of bilingual word embeddings.", "labels": [], "entities": [{"text": "domain adaptation of bilingual word embeddings", "start_pos": 48, "end_pos": 94, "type": "TASK", "confidence": 0.7412803719441096}]}, {"text": "We evaluate these embed-dings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction.", "labels": [], "entities": [{"text": "cross-lingual twitter sentiment classification", "start_pos": 82, "end_pos": 128, "type": "TASK", "confidence": 0.7214782014489174}, {"text": "medical bilingual lexicon induction", "start_pos": 133, "end_pos": 168, "type": "TASK", "confidence": 0.611487440764904}]}, {"text": "Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks.", "labels": [], "entities": []}, {"text": "We show that this method also helps in low-resource setups.", "labels": [], "entities": []}, {"text": "Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we study two bilingual tasks that strongly depend on bilingual word embeddings (BWEs).", "labels": [], "entities": []}, {"text": "Previously, specialized domain adaptation approaches to such tasks were proposed.", "labels": [], "entities": []}, {"text": "We instead show experimentally that a simple adaptation process involving only unlabeled text is highly effective.", "labels": [], "entities": []}, {"text": "We then show that a semisupervised classification method from computer vision can be applied successfully for further gains in cross-lingual classification.", "labels": [], "entities": [{"text": "cross-lingual classification", "start_pos": 127, "end_pos": 155, "type": "TASK", "confidence": 0.8141694962978363}]}, {"text": "Our BWE adaptation method is delightfully simple.", "labels": [], "entities": [{"text": "BWE adaptation", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.6830297857522964}]}, {"text": "We begin by adapting monolingual word embeddings to the target domain for source and target languages by simply building them using both general and target-domain unlabeled data.", "labels": [], "entities": []}, {"text": "As a second step we use post-hoc mapping), i.e., we use a seed lexicon to transform the word embeddings of the two languages into the same vector space.", "labels": [], "entities": []}, {"text": "We show experimentally for the first time that the domain-adapted bilingual word embeddings we produce using this extremely simple technique are highly effective.", "labels": [], "entities": []}, {"text": "We study two quite different tasks and domains, where resources are lacking, showing that our simple technique performs well for both of them: cross-lingual twitter sentiment classification and medical bilingual lexicon induction.", "labels": [], "entities": [{"text": "cross-lingual twitter sentiment classification", "start_pos": 143, "end_pos": 189, "type": "TASK", "confidence": 0.7298348918557167}, {"text": "medical bilingual lexicon induction", "start_pos": 194, "end_pos": 229, "type": "TASK", "confidence": 0.68052177131176}]}, {"text": "In previous work, task-dependent approaches were used for this type of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7843868434429169}]}, {"text": "Our approach is simple and task independent.", "labels": [], "entities": []}, {"text": "Second, we adapt the semi-supervised image classification system of for NLP problems for the first time.", "labels": [], "entities": [{"text": "image classification", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7185737788677216}]}, {"text": "This approach is broadly applicable to many NLP classification tasks where unlabeled data is available.", "labels": [], "entities": [{"text": "NLP classification tasks", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.9252113699913025}]}, {"text": "We tailor it to both of our cross-lingual tasks.", "labels": [], "entities": []}, {"text": "The system exploits unlabeled data during the training of classifiers by learning similar features for similar labeled and unlabeled training examples, thereby extracting information from unlabeled examples as well.", "labels": [], "entities": []}, {"text": "As we show experimentally, the system further improves cross-lingual knowledge transfer for both of our tasks.", "labels": [], "entities": [{"text": "cross-lingual knowledge transfer", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.6218516329924265}]}, {"text": "After combining both techniques, the results of sentiment analysis are competitive with systems that use annotated data in the target language, an impressive result considering that we require no target-language annotated data.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9661752581596375}]}, {"text": "The method also yields impressive improvements for bilingual lexicon induction compared with baselines trained on in-domain data.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7536107103029887}]}, {"text": "We show that this system requires the high-quality domain-adapted bilingual word embeddings we previously created to use unlabeled data well.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our improved BWEs on the dataset provided by.", "labels": [], "entities": [{"text": "BWEs", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8545026183128357}]}, {"text": "The monolingual medical data consists of English and Dutch medical articles from Wikipedia.", "labels": [], "entities": []}, {"text": "The English (resp. Dutch) articles contain 52,336 (resp. 21,374) sentences.", "labels": [], "entities": []}, {"text": "A total of 7,368 manually annotated word translation pairs occurring in the English (source) and Dutch (target) monolingual corpora are provided as gold data.", "labels": [], "entities": []}, {"text": "This set is split 64%/16%/20% into trn/dev/test.", "labels": [], "entities": []}, {"text": "20% of the English words have multiple translations.", "labels": [], "entities": []}, {"text": "Given an English word, the task is to find the correct Dutch translation.", "labels": [], "entities": []}, {"text": "As monolingual general-domain data we use cosine similarity classifier: We report F 1 results for medical BLI with the cosine similarity and the classifier based systems.", "labels": [], "entities": [{"text": "F 1", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9393230974674225}]}, {"text": "We present baseline and our proposed domain adaptation method using both general and medical lexicons.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7442218363285065}]}, {"text": "the English and Dutch data from Europarl (v7) (), a corpus of 2 million sentence pairs.", "labels": [], "entities": [{"text": "Europarl (v7)", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.8763243854045868}]}, {"text": "Although Europarl is a parallel corpus, we use it in a monolingual way and shuffle each side of the corpus before training.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.9520318508148193}]}, {"text": "By using massive cheap data we create high-quality MWEs in each language which are still domain-specific (due to inclusion of medical data).", "labels": [], "entities": [{"text": "MWEs", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.9474326968193054}]}, {"text": "To obtain an out-ofdomain seed lexicon, we translated the English words in BNC to Dutch using Google Translate (just as we did before for the Twitter CLSC task).", "labels": [], "entities": []}, {"text": "We then use the out-of-domain BNC and the indomain medical seed lexicons in separate experiments to create BWEs with post-hoc mapping.", "labels": [], "entities": []}, {"text": "Note, we did not concatenate the two lexicons because (i) they have a small common subset of source words which have different target words, thus having a negative effect on the mapping and (ii) we did not want to modify the medical seed lexicon because it was taken from previous work.", "labels": [], "entities": [{"text": "medical seed lexicon", "start_pos": 225, "end_pos": 245, "type": "DATASET", "confidence": 0.7355767091115316}]}], "tableCaptions": [{"text": " Table 1: Accuracy of the BWE adaptation ap- proach on the target-level sentiment classification  task. The oracle systems used Spanish sentiment  training data instead of English.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9879621863365173}, {"text": "sentiment classification  task", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.7821580568949381}]}, {"text": " Table 2: We report F 1 results for medical BLI with the cosine similarity and the classifier based sys- tems. We present baseline and our proposed domain adaptation method using both general and medical  lexicons.", "labels": [], "entities": [{"text": "F 1", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9735814929008484}]}, {"text": " Table 4: Accuracy on CLSC of both target-aware and target-ignorant systems using English or/and  Spanish sentiment training data. Column lang shows the language of the used training data. Differences  comparing to semisup are indicated in brackets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9741010069847107}]}]}