{"title": [], "abstractContent": [{"text": "This paper proposes an improvement to the existing data-driven Neural Belief Tracking (NBT) framework for Dialogue State Tracking (DST).", "labels": [], "entities": [{"text": "data-driven Neural Belief Tracking (NBT)", "start_pos": 51, "end_pos": 91, "type": "TASK", "confidence": 0.8030077048710415}, {"text": "Dialogue State Tracking (DST)", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.8308920363585154}]}, {"text": "The existing NBT model uses a hand-crafted belief state update mechanism which involves an expensive manual retuning step whenever the model is deployed to anew dialogue domain.", "labels": [], "entities": [{"text": "NBT", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.937534511089325}]}, {"text": "We show that this update mechanism can be learned jointly with the semantic decoding and context modelling parts of the NBT model, eliminating the last rule-based module from this DST framework.", "labels": [], "entities": [{"text": "NBT model", "start_pos": 120, "end_pos": 129, "type": "DATASET", "confidence": 0.9542185962200165}]}, {"text": "We propose two different statistical update mechanisms and show that dialogue dynamics can be modelled with a very small number of additional model parameters.", "labels": [], "entities": [{"text": "dialogue dynamics", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.9213241338729858}]}, {"text": "In our DST evaluation over three languages, we show that this model achieves competitive performance and provides a robust framework for building resource-light DST models.", "labels": [], "entities": [{"text": "DST", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9874500036239624}, {"text": "DST", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9603740572929382}]}], "introductionContent": [{"text": "The problem of language understanding permeates the deployment of statistical dialogue systems.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7245894521474838}]}, {"text": "These systems rely on dialogue state tracking (DST) modules to model the user's intent at any point of an ongoing conversation.", "labels": [], "entities": [{"text": "dialogue state tracking (DST)", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.8011633058389028}]}, {"text": "In turn, DST models rely on domain-specific Spoken Language Understanding (SLU) modules to extract turn-level user goals, which are then incorporated into the belief state, the system's internal probability distribution over possible dialogue states.", "labels": [], "entities": [{"text": "DST", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9765791893005371}]}, {"text": "The dialogue states are defined by the domainspecific ontology: it enumerates the constraints the users can express using a collection of slots (e.g. price range) and their slot values (e.g. cheap, expensive for the aforementioned slots).", "labels": [], "entities": []}, {"text": "The belief state is used by the downstream dialogue management component to choose the next system response ( . A large number of DST models (, inter alia) treat SLU as a separate problem: the detached SLU modules area dependency for such systems as they require large amounts of annotated training data.", "labels": [], "entities": []}, {"text": "Moreover, recent research has demonstrated that systems which treat SLU and DST as a single problem have proven superior to those which decouple them (.", "labels": [], "entities": []}, {"text": "Delexicalisation-based models, such as the one proposed by offer unparalleled generalisation capability.", "labels": [], "entities": []}, {"text": "These models use exact matching to replace occurrences of slot names and values with generic tags, allowing them to share parameters across all slot values.", "labels": [], "entities": []}, {"text": "This allows them to deal with slot values not seen during training.", "labels": [], "entities": [{"text": "slot", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9425634145736694}]}, {"text": "However, their downside is shifting the problem of dealing with linguistic variation back to the system designers, who have to craft semantic lexicons to specify rephrasings for ontology values.", "labels": [], "entities": []}, {"text": "Examples of such rephrasings are for slot-value pair FOOD=CHEAP, or [with internet, has internet] for HAS INTERNET=TRUE.", "labels": [], "entities": [{"text": "FOOD", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9365495443344116}, {"text": "CHEAP", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.5833494663238525}, {"text": "HAS", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.8982732892036438}, {"text": "INTERNET", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.611140787601471}, {"text": "TRUE", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.598331868648529}]}, {"text": "The use of such lexicons has a profound effect on DST performance . Moreover, such lexicons introduce a design barrier for deploying these models to large real-world dialogue domains and other languages.", "labels": [], "entities": [{"text": "DST", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9935965538024902}]}, {"text": "The Neural Belief Tracker (NBT) framework) is a recent attempt to overcome these obstacles by using dense word embeddings in place of traditional n-gram features.", "labels": [], "entities": [{"text": "Neural Belief Tracker (NBT)", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.760500356554985}]}, {"text": "By making use of semantic relations embedded in the vector spaces, the NBT achieves DST performance competitive to lexicon-supplemented delexicalisation-based models without relying on any hand-crafted resources.", "labels": [], "entities": [{"text": "NBT", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.9528488516807556}, {"text": "DST", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9676154851913452}]}, {"text": "Moreover, the NBT The NBT models decompose the (per-slot) multiclass value prediction problem into many binary ones: they iterate through all slot values defined by the ontology and decide which ones have just been expressed by the user.", "labels": [], "entities": [{"text": "NBT", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.9694975018501282}, {"text": "multiclass value prediction problem", "start_pos": 58, "end_pos": 93, "type": "TASK", "confidence": 0.7316100299358368}]}, {"text": "To differentiate between slots, they take as input the word vector of the slot value that it is making a decision about.", "labels": [], "entities": []}, {"text": "In doing that, the previous belief state is discarded.", "labels": [], "entities": []}, {"text": "However, the previous state may contain pertinent information for making the turn-level decision.", "labels": [], "entities": []}, {"text": "Contribution In this work, we show that crossturn dependencies can be learned automatically: this eliminates the rule-based NBT component and effectively yields a fully statistical dialogue state tracker.", "labels": [], "entities": []}, {"text": "Our competitive results on the benchmarking WOZ dataset for three languages indicate that the proposed fully statistical model: 1) is robust with respect to the input vector space, and 2) is easily portable and applicable to different languages.", "labels": [], "entities": [{"text": "WOZ dataset", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.7011432200670242}]}, {"text": "Finally, we make the code of the novel NBT framework publicly available at: https://github.com/nmrksic/neural-belief-tracker, in hope of helping researchers to overcome the initial high-cost barrier to using DST as a real-world language understanding task.", "labels": [], "entities": [{"text": "language understanding task", "start_pos": 228, "end_pos": 255, "type": "TASK", "confidence": 0.7735098401705424}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The English DST performance (joint goal  accuracy) with standard input word vectors ( \u00a73).", "labels": [], "entities": [{"text": "DST", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.6958467960357666}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.5368036031723022}]}, {"text": " Table 2: DST performance on Italian and German.  Only results with the better scoring learned Con- strained Markovian Update are reported.", "labels": [], "entities": [{"text": "Con- strained Markovian Update", "start_pos": 95, "end_pos": 125, "type": "METRIC", "confidence": 0.8649900197982788}]}]}