{"title": [{"text": "Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning", "labels": [], "entities": [{"text": "Task-Completion Dialogue Policy Learning", "start_pos": 38, "end_pos": 78, "type": "TASK", "confidence": 0.6652503982186317}]}], "abstractContent": [{"text": "Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users.", "labels": [], "entities": [{"text": "task-completion dialogue agent via reinforcement learning (RL)", "start_pos": 11, "end_pos": 73, "type": "TASK", "confidence": 0.6893527110417684}]}, {"text": "One common alternative is to use a user simulator.", "labels": [], "entities": []}, {"text": "However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent.", "labels": [], "entities": []}, {"text": "To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning.", "labels": [], "entities": []}, {"text": "We incorporate into the dialogue agent a model of the environment , referred to as the world model, to mimic real user response and generate simulated experience.", "labels": [], "entities": []}, {"text": "During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience.", "labels": [], "entities": [{"text": "dialogue policy learning", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8374986052513123}]}, {"text": "The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings 1 .", "labels": [], "entities": [{"text": "movie-ticket booking task", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7315218051274618}]}], "introductionContent": [{"text": "Learning policies for task-completion dialogue is often formulated as a reinforcement learning (RL) problem (;).", "labels": [], "entities": []}, {"text": "However, applying RL to real-world dialogue systems can be challenging, due to the constraint that an RL learner needs an environment to operate in.", "labels": [], "entities": []}, {"text": "In the dialogue setting, this requires a dialogue agent to interact with real users and adjust its policy in an online fashion, as illustrated in.", "labels": [], "entities": []}, {"text": "Unlike simulation-based games such as Atari games and AlphaGo where RL has made its greatest strides, task-completion dialogue systems may incur significant real-world cost in case of failure.", "labels": [], "entities": []}, {"text": "Thus, except for very simple tasks), RL is too expensive to be applied to real users to train dialogue agents from scratch.", "labels": [], "entities": [{"text": "RL", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9002606272697449}]}, {"text": "One strategy is to convert human-interacting dialogue to a simulation problem (similar to Atari games), by building a user simulator using human conversational data (.", "labels": [], "entities": []}, {"text": "In this way, the dialogue agent can learn its policy by interacting with the simulator instead of real users).", "labels": [], "entities": []}, {"text": "The simulator, in theory, does not incur any real-world cost and can provide unlimited simulated experience for reinforcement learning.", "labels": [], "entities": [{"text": "reinforcement learning", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.8539912700653076}]}, {"text": "The dialogue agent trained with such a user simulator can then be deployed to real users and further enhanced by only a small number of human interactions.", "labels": [], "entities": []}, {"text": "Most of recent studies in this area have adopted this strategy ().", "labels": [], "entities": []}, {"text": "However, user simulators usually lack the conversational complexity of human interlocutors, and the trained agent is inevitably affected by biases in the design of the simulator.", "labels": [], "entities": []}, {"text": "demonstrated a significant discrepancy in a simulator-trained dialogue agent when evaluated with simulators and with real users.", "labels": [], "entities": []}, {"text": "Even more challenging is the fact that there is no universally accepted metric to evaluate a user simulator ().", "labels": [], "entities": []}, {"text": "Thus, it remains controversial whether training task-completion dialogue agent via simulated users is a valid approach.", "labels": [], "entities": []}, {"text": "We propose anew strategy of learning dialogue policy by interacting with real users.", "labels": [], "entities": []}, {"text": "Compared to previous works, our dialogue agent learns in a much more efficient way, using only a small number of real user interactions, which amounts to an affordable cost in many nontrivial dialogue tasks.", "labels": [], "entities": []}, {"text": "Our approach is based on the Dyna-Q framework where planning is integrated into policy learning for task-completion dialogue.", "labels": [], "entities": []}, {"text": "Specifically, we incorporate a model of the environment, referred to as the world model, into the dialogue agent, which simulates the environment and generates simulated user experience.", "labels": [], "entities": []}, {"text": "During the dialogue policy learning, real user experience plays two pivotal roles: first, it can be used to improve the world model and make it behave more like real users, via supervised learning; second, it can also be used to directly improve the dialogue policy via RL.", "labels": [], "entities": []}, {"text": "The former is referred to as world model learning, and the latter direct reinforcement learning.", "labels": [], "entities": []}, {"text": "Dialogue policy can be improved either using real experience directly (i.e., direct reinforcement learning) or via the world model indirectly (referred to as planning or indirect reinforcement learning).", "labels": [], "entities": []}, {"text": "The interaction between world model learning, direct reinforcement learning and planning is illustrated in(c), following the Dyna-Q framework).", "labels": [], "entities": []}, {"text": "The original papers on Dyna-Q and most its early extensions used tabular methods for both planning and learning.", "labels": [], "entities": []}, {"text": "This table-lookup representation limits its application to small problems only.", "labels": [], "entities": []}, {"text": "extends the Dyna architecture to linear function approximation, making it applicable to larger problems.", "labels": [], "entities": [{"text": "linear function approximation", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.6935757597287496}]}, {"text": "In the dialogue setting, we are dealing with a much larger action-state space.", "labels": [], "entities": []}, {"text": "Inspired by, we propose Deep Dyna-Q (DDQ) by combining Dyna-Q with deep learning approaches to representing the state-action space by neural networks (NN).", "labels": [], "entities": []}, {"text": "By employing the world model for planning, the DDQ method can be viewed as a model-based RL approach, which has drawn growing interest in the research community.", "labels": [], "entities": []}, {"text": "However, most model-based RL methods () are developed for simulation-based, synthetic problems (e.g., games), but not for human-in-the-loop, real-world problems.", "labels": [], "entities": [{"text": "RL", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9235416054725647}]}, {"text": "To these ends, our main contributions in this work are two-fold: \u2022 We present Deep Dyna-Q, which to the best of our knowledge is the first deep RL framework that incorporates planning for taskcompletion dialogue policy learning.", "labels": [], "entities": [{"text": "taskcompletion dialogue policy learning", "start_pos": 188, "end_pos": 227, "type": "TASK", "confidence": 0.7224336415529251}]}, {"text": "\u2022 We demonstrate that a task-completion dialogue agent can efficiently adapt its policy on the fly, by interacting with real users via RL.", "labels": [], "entities": []}, {"text": "This results in a significant improvement in success rate on a nontrivial task.", "labels": [], "entities": [{"text": "success rate", "start_pos": 45, "end_pos": 57, "type": "METRIC", "confidence": 0.943830817937851}]}, {"text": "As illustrated in(c), starting with an initial dialogue policy and an initial world model (both trained with pre-collected human conversational data), the training of the DDQ agent consists of three processes: (1) direct reinforcement learning, where the agent interacts with areal user, collects real experience and improves the dialogue policy; (2) world model learning, where the world model is learned and refined using real experience; and (3) planning, where the agent improves the dialogue policy using simulated experience.", "labels": [], "entities": []}, {"text": "Although these three processes conceptually can occur simultaneously in the DDQ agent, we implement an iterative training procedure, as shown in Algorithm 1, where we specify the order in which they occur within each iteration.", "labels": [], "entities": []}, {"text": "In what follows, we will describe these processes in details.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the DDQ method on a movie-ticket booking task in both simulation and human-in-theloop settings.", "labels": [], "entities": [{"text": "movie-ticket booking task", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.7487293581167856}]}, {"text": "Raw conversational data in the movie-ticket booking scenario was collected via Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "movie-ticket booking", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.6895566135644913}, {"text": "Amazon Mechanical Turk", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.9419466455777487}]}, {"text": "The dataset has been manually labeled based on a schema defined by domain experts, as shown in, which consists of 11 dialogue acts and 16 slots.", "labels": [], "entities": []}, {"text": "In total, the dataset contains 280 annotated dialogues, the average length of which is approximately 11 turns.", "labels": [], "entities": []}, {"text": "In this setting the dialogue agents are optimized by interacting with user simulators, instead of real users.", "labels": [], "entities": []}, {"text": "Thus, the world model is learned to mimic user simulators.", "labels": [], "entities": []}, {"text": "Although the simulator-trained agents are sub-optimal when applied to real users due to the discrepancy between simulators and real users, the simulation setting allows us to perform a detailed analysis of DDQ without much cost and to reproduce the experimental results easily.", "labels": [], "entities": []}, {"text": "We found in our experiments that setting Z > 1 improves the performance of all agents, but does not change the conclusion of this study: DDQ consistently outperforms DQN by a statistically significant margin.", "labels": [], "entities": []}, {"text": "Conceptually, the optimal value of Z used in planning is different from that indirect reinforcement learning, and should vary according to the quality of the world model.", "labels": [], "entities": []}, {"text": "The better the world model is, the more aggressive update (thus bigger Z) is being used in planning.", "labels": [], "entities": []}, {"text": "We leave it to future work to investigate how to optimize Z for planning in DDQ.", "labels": [], "entities": []}, {"text": "User Simulator We adapted a publicly available user simulator () to the taskcompletion dialogue setting.", "labels": [], "entities": []}, {"text": "During training, the simulator provides the agent with a simulated user response in each dialogue turn and a reward signal at the end of the dialogue.", "labels": [], "entities": []}, {"text": "A dialogue is considered successful only when a movie ticket is booked successfully and when the information provided by the agent satisfies all the user's constraints.", "labels": [], "entities": []}, {"text": "At the end of each dialogue, the agent receives a positive reward of 2 * L for success, or a negative reward of \u2212L for failure, where L is the maximum number of turns in each dialogue, and is set to 40 in our experiments.", "labels": [], "entities": []}, {"text": "Furthermore, in each turn, the agent receives a reward of \u22121, so that shorter dialogues are encouraged.", "labels": [], "entities": []}, {"text": "Readers can refer to Appendix B for details on the user simulator.", "labels": [], "entities": [{"text": "Appendix B", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9307463765144348}]}, {"text": "In this setting, five dialogue agents (i.e., DQN, DDQ(10), DDQ(10, rand-init \u03b8 M ), DDQ(5), and DDQ(5, rand-init \u03b8 M )) are trained via RL by interacting with real human users.", "labels": [], "entities": []}, {"text": "In each dialogue session, one of the agents was randomly picked to converse with a user.", "labels": [], "entities": []}, {"text": "The user was presented with a user goal sampled from the corpus, and was instructed to converse with the agent to complete the task.", "labels": [], "entities": []}, {"text": "The user had the choice of abandoning the task and ending the dialogue at anytime, if she or he believed that the dialogue was unlikely to succeed or simply because the dialogue dragged on for too many turns.", "labels": [], "entities": []}, {"text": "In such cases, the dialogue session is considered failed.", "labels": [], "entities": []}, {"text": "At the end of each session, the user was asked to give explicit feedback whether the dialogue succeeded (i.e., whether the movie tickets were booked with all the user constraints satisfied).", "labels": [], "entities": []}, {"text": "Each learning curve is trained with two runs, with each run generating 150 dialogues (and K * 150 additional simulated dialogues when planning is applied).", "labels": [], "entities": []}, {"text": "In total, we collected 1500 dialogue sessions for training all five agents.", "labels": [], "entities": []}, {"text": "The main results are presented in and Simulation Sample Real User Sample movie-ticket booking user goal: { \"request slots\": { \"constraint slots\": { \"ticket\": \"?\"", "labels": [], "entities": [{"text": "Simulation Sample Real User Sample movie-ticket booking", "start_pos": 38, "end_pos": 93, "type": "TASK", "confidence": 0.8064632075173515}]}, {"text": "\"numberofpeople\":\"2\" \"theater\": \"?\"", "labels": [], "entities": []}, {"text": "\"moviename\": \"deadpool\" \"starttime\": \"?\"", "labels": [], "entities": []}, {"text": "\"city\": \"seattle\" \"date\": \"?\"", "labels": [], "entities": [{"text": "seattle", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.7952009439468384}, {"text": "date", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.7674697637557983}]}, {"text": "} } } movie-ticket booking user goal: { \"request slots\": { \"constraint slots\": { \"ticket\": \"?\"", "labels": [], "entities": []}, {"text": "\"date\":\"this weekend\" \"theater\": \"?\"", "labels": [], "entities": []}, {"text": "\"numberofpeople\": \"1\" \"starttime\": \"?\"", "labels": [], "entities": []}, {"text": "\"moviename\": \"batman\" } } }, with each agent averaged over two independent runs.", "labels": [], "entities": []}, {"text": "The results confirm what we observed in the simulation experiments.", "labels": [], "entities": []}, {"text": "The conclusions are summarized as below: \u2022 The DDQ agent significantly outperforms DQN, as demonstrated by the comparison between DDQ(10) and DQN.", "labels": [], "entities": [{"text": "DQN", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.9201197028160095}]}, {"text": "presents four example dialogues produced by two dialogue agents interacting with simulated and human users, respectively.", "labels": [], "entities": []}, {"text": "The DQN agent, after being trained with 100 dialogues, still behaved like a naive rule-based agent that requested information bit by bit in a fixed order.", "labels": [], "entities": []}, {"text": "When the user did not answer the request explicitly (e.g., usr: which theater is available?), the agent failed to respond properly.", "labels": [], "entities": []}, {"text": "On the other hand, with planning, the DDQ agent trained with 100 real dialogues is much more robust and can complete 50% of user tasks successfully.", "labels": [], "entities": []}, {"text": "\u2022 A larger K leads to more aggressive planning and better results, as shown by DDQ(10) vs. DDQ(5).", "labels": [], "entities": [{"text": "DDQ", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8590695261955261}, {"text": "DDQ", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9079447984695435}]}, {"text": "\u2022 Pre-training world model with human con-versational data improves the learning efficiency and the agent's performance, as shown by DDQ(5) vs. DDQ(5, rand-init \u03b8 M ), and DDQ(10) vs. DDQ(10, rand-init \u03b8 M ).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The performance of different agents at training epoch = {100, 150, 200} in the human-in-the- loop experiments. The difference between the results of all agent pairs evaluated at the same epoch is  statistically significant (p < 0.01). (Success: success rate)", "labels": [], "entities": []}]}