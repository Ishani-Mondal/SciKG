{"title": [{"text": "Disconnected Recurrent Neural Networks for Text Categorization", "labels": [], "entities": [{"text": "Text Categorization", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7740446925163269}]}], "abstractContent": [{"text": "Recurrent neural network (RNN) has achieved remarkable performance in text categorization.", "labels": [], "entities": [{"text": "Recurrent neural network (RNN", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.529570358991623}, {"text": "text categorization", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.823213666677475}]}, {"text": "RNN can model the entire sequence and capture long-term dependencies , but it does not do well in extracting key patterns.", "labels": [], "entities": []}, {"text": "In contrast, convo-lutional neural network (CNN) is good at extracting local and position-invariant features.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel model named disconnected recurrent neu-ral network (DRNN), which incorporates position-invariance into RNN.", "labels": [], "entities": []}, {"text": "By limiting the distance of information flow in RNN, the hidden state at each time step is restricted to represent words near the current position.", "labels": [], "entities": []}, {"text": "The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text cate-gorization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text categorization is a fundamental and traditional task in natural language processing (NLP), which can be applied in various applications such as sentiment analysis, question classification and topic classification).", "labels": [], "entities": [{"text": "Text categorization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7604537308216095}, {"text": "natural language processing (NLP)", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.7817180554072062}, {"text": "sentiment analysis", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.9541028738021851}, {"text": "question classification", "start_pos": 169, "end_pos": 192, "type": "TASK", "confidence": 0.8610746264457703}, {"text": "topic classification", "start_pos": 197, "end_pos": 217, "type": "TASK", "confidence": 0.8094182312488556}]}, {"text": "Nowadays, one of the most commonly used methods to handle the task is to represent a text with a low dimensional vector, then feed the vector into a softmax function to calculate the probability of each category.", "labels": [], "entities": []}, {"text": "Recurrent neural network (RNN) and convolutional neural network (CNN) are two kinds of neural networks usually used to represent the text.", "labels": [], "entities": []}, {"text": "RNN can model the whole sequence and capture long-term dependencies ().", "labels": [], "entities": []}, {"text": "However, modeling the entire sequence sometimes case1: One of the seven great unsolved mysteries of mathematics may have been cracked by a reclusive Russian.", "labels": [], "entities": [{"text": "case1", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.8887148499488831}]}, {"text": "case2: A reclusive Russian may have cracked one of the seven great unsolved mysteries of mathematics.", "labels": [], "entities": []}, {"text": "can be a burden, and it may neglect key parts for text categorization.", "labels": [], "entities": []}, {"text": "In contrast, CNN is able to extract local and position-invariant features well (). is an example of topic classification, where both sentences should be classified as Science and Technology.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.8293086290359497}]}, {"text": "The key phrase that determines the category is unsolved mysteries of mathematics, which can be well extracted by CNN due to position-invariance.", "labels": [], "entities": []}, {"text": "RNN, however, doesn't address such issues well because the representation of the key phrase relies on all the previous terms and the representation changes as the key phrase moves.", "labels": [], "entities": [{"text": "RNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8568584322929382}]}, {"text": "In this paper, we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network (DRNN).", "labels": [], "entities": []}, {"text": "Concretely, we disconnect the information transmission of RNN and limit the maximal transmission step length as a fixed value k, so that the representation at each step only depends on the previous k \u2212 1 words and the current word.", "labels": [], "entities": []}, {"text": "In this way, DRNN can also alleviate the burden of modeling the entire document.", "labels": [], "entities": []}, {"text": "To maintain the position-invariance, we utilize max pooling to extract the important information, which has been suggested by.", "labels": [], "entities": []}, {"text": "Our proposed model can also be regarded as a special 1D CNN where convolution kernels are replaced with recurrent units.", "labels": [], "entities": []}, {"text": "Therefore, the maximal transmission step length can also be consid-ered as the window size in CNN.", "labels": [], "entities": []}, {"text": "Another difference to CNN is that DRNN can increase the window size k arbitrarily without increasing the number of parameters.", "labels": [], "entities": [{"text": "CNN", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.6340052485466003}, {"text": "DRNN", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8039430379867554}]}, {"text": "We also find that there is a trade-off between position-invariance and long-term dependencies in the DRNN.", "labels": [], "entities": [{"text": "DRNN", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.9780543446540833}]}, {"text": "When the window size is too large, the position-invariance will disappear like RNN.", "labels": [], "entities": []}, {"text": "By contrast, when the window size is too small, we will lose the ability to model long-term dependencies just like CNN.", "labels": [], "entities": []}, {"text": "We find that the optimal window size is related to the type of task, but affected little by training dataset sizes.", "labels": [], "entities": []}, {"text": "Thus, we can search the optimal window size by training on a small dataset.", "labels": [], "entities": []}, {"text": "We conduct experiments on seven large-scale text classification datasets introduced by.", "labels": [], "entities": [{"text": "text classification", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7079527676105499}]}, {"text": "The experimental results show that our proposed model outperforms the other models on all of these datasets.", "labels": [], "entities": []}, {"text": "Our contributions can be concluded as follows: 1.", "labels": [], "entities": []}, {"text": "We propose a novel model to incorporate position-variance into RNN.", "labels": [], "entities": []}, {"text": "Our proposed model can both capture long-term dependencies and local information well.", "labels": [], "entities": []}, {"text": "2. We study the effect of different recurrent units, pooling operations and window sizes on model performance.", "labels": [], "entities": []}, {"text": "Based on this, we propose an empirical method to find the optimal window size.", "labels": [], "entities": []}, {"text": "3. Our proposed model outperforms the other models and achieves the best performance on seven text classification datasets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.6751230359077454}]}], "datasetContent": [{"text": "Datasets Introduction We use 7 large-scale text classification datasets which are proposed by.", "labels": [], "entities": []}, {"text": "We summarize the datasets in.", "labels": [], "entities": []}, {"text": "AG corpus is news and DBPedia is an ontology which comes from the Wikipedia.", "labels": [], "entities": [{"text": "AG corpus", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9425114095211029}]}, {"text": "Yelp and Amazon corpus are reviews for which we should predict the sentiment.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9564697742462158}, {"text": "Amazon corpus", "start_pos": 9, "end_pos": 22, "type": "DATASET", "confidence": 0.8925843834877014}]}, {"text": "Here P. means that we only need to predict the polarities of the dataset, while F. indicates that we need predict the star number of the review.", "labels": [], "entities": []}, {"text": "Answers (Yah. A.) is a question answering dataset.", "labels": [], "entities": [{"text": "question answering", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.687673956155777}]}, {"text": "We can see that these datasets contain various domains and sizes, which would be credible to validate our models.", "labels": [], "entities": []}, {"text": "The: DGRU compared with CNN better performance in these datasets by simply setting a large window size.", "labels": [], "entities": []}, {"text": "Char-CRNN () in the fourth block is a model which combines positioninvariance of CNN and long-term dependencies of RNN.", "labels": [], "entities": []}, {"text": "Nevertheless, they do not achieve great improvements over other models.", "labels": [], "entities": []}, {"text": "They first utilize convolution operation to extract position-invariant features, and then use RNN to capture long-term dependencies.", "labels": [], "entities": []}, {"text": "Here, modeling the whole sequence with RNN leads to a loss of position-invariance.", "labels": [], "entities": []}, {"text": "Compared with their model, our model can better maintain the position-invariance by max pooling ().", "labels": [], "entities": []}, {"text": "shows that our model achieves 10-50% relative error reduction compared with char-CRNN in these datasets.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7570344607035319}]}], "tableCaptions": [{"text": " Table 2: Dataset information. Here SA refers to sentiment analysis, and QA refers to question answering.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.9529167413711548}, {"text": "QA", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9546052813529968}, {"text": "question answering", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7767584323883057}]}, {"text": " Table 2. AG corpus is news and DBPedia is  an ontology which comes from the Wikipedia.  Yelp and Amazon corpus are reviews for which  we should predict the sentiment. Here P. means  that we only need to predict the polarities of the  dataset, while F. indicates that we need predict  the star number of the review. Yahoo! Answers  (Yah. A.) is a question answering dataset. We can  see that these datasets contain various domains  and sizes, which would be credible to validate our  models.", "labels": [], "entities": [{"text": "Amazon corpus", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.8981856405735016}, {"text": "Yahoo! Answers  (Yah. A.)", "start_pos": 316, "end_pos": 341, "type": "DATASET", "confidence": 0.7505692328725543}]}, {"text": " Table 3. The words not  in vocabulary are replaced with a special token  UNK.", "labels": [], "entities": []}, {"text": " Table 4: Error rates (%) on seven datasets", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9969095587730408}]}, {"text": " Table 6: Examples of error analysis. The case 1 is a negative review and case 2 is a positive review.  The first example is misclassified by CNN and classified correctly by GRU. The second one is just the  contrary. DGRU classify both examples correctly.", "labels": [], "entities": [{"text": "CNN", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.9663111567497253}, {"text": "GRU", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.9027959108352661}, {"text": "DGRU", "start_pos": 217, "end_pos": 221, "type": "DATASET", "confidence": 0.8470370173454285}]}]}