{"title": [{"text": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences", "labels": [], "entities": [{"text": "Require Simple Lexical Inferences", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.7216503173112869}]}], "abstractContent": [{"text": "We create anew NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge.", "labels": [], "entities": [{"text": "NLI test set", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.772076944510142}]}, {"text": "The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set.", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.8204530676205953}]}, {"text": "Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.7877564430236816}]}], "introductionContent": [{"text": "Recognizing textual entailment (RTE) (, recently framed as natural language inference (NLI)) is a task concerned with identifying whether a premise sentence entails, contradicts or is neutral with the hypothesis sentence.", "labels": [], "entities": [{"text": "Recognizing textual entailment (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8993478318055471}]}, {"text": "Following the release of the large-scale SNLI dataset, many end-to-end neural models have been developed for the task, achieving high accuracy on the test set.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.740372508764267}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9975343942642212}]}, {"text": "As opposed to previous-generation methods, which relied heavily on lexical resources, neural models only make use of pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "The few efforts to incorporate external lexical knowledge resulted in negligible performance gain.", "labels": [], "entities": []}, {"text": "This raises the question whether (1) neural methods are inherently stronger, obviating the need of external lexical knowledge; (2) large-scale training data allows for implicit learning of previously explicit lexical knowledge; or (3) the NLI datasets are simpler than early RTE datasets, requiring less knowledge.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 239, "end_pos": 251, "type": "DATASET", "confidence": 0.8023355305194855}, {"text": "RTE datasets", "start_pos": 275, "end_pos": 287, "type": "DATASET", "confidence": 0.7049097567796707}]}], "datasetContent": [{"text": "We trained each model on 3 different datasets: (1) SNLI train set, (2) a union of the SNLI train set and the MultiNLI train set, and (3) a union of the SNLI train set and the SciTail train set.", "labels": [], "entities": [{"text": "SNLI train set", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9093231956164042}, {"text": "SNLI train set", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.9029463728268942}, {"text": "MultiNLI train set", "start_pos": 109, "end_pos": 127, "type": "DATASET", "confidence": 0.9598141511281332}, {"text": "SNLI train set", "start_pos": 152, "end_pos": 166, "type": "DATASET", "confidence": 0.8943342169125875}, {"text": "SciTail train set", "start_pos": 175, "end_pos": 192, "type": "DATASET", "confidence": 0.8884560068448385}]}, {"text": "The motivation is that while SNLI might lack the training data needed to learn the required lexical knowledge, it maybe available in the other datasets, which are presumably richer.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.6826491355895996}]}, {"text": "displays the results for all the models on the original SNLI test set and the new test set.", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9469561576843262}]}, {"text": "Despite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 points inaccuracy.", "labels": [], "entities": []}, {"text": "Adding MultiNLI to the training data somewhat mitigates this drop inaccuracy, thanks to almost doubling the amount of training data.", "labels": [], "entities": []}, {"text": "We note that adding SciTail to the training data did not similarly improve the performance; we conjecture that this stems from the differences between the datasets.", "labels": [], "entities": []}, {"text": "KIM substantially outperforms the other neural models, demonstrating that lexical knowledge is the only requirement for good performance on the new test set, and stressing the inability of the other models to learn it.", "labels": [], "entities": []}, {"text": "Both WordNet-informed models leave room for improvement: possibly due to limited WordNet coverage and the implications of applying lexical inferences within context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the test sets. 9,815 is the  number of samples with majority agreement in the  SNLI test set, whose full size is 9,824.", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.9305127461751302}]}, {"text": " Table 3: Accuracy of various models trained on SNLI or a union of SNLI with another dataset (MultiNLI,  SciTail), and tested on the original SNLI test set and the new test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9830295443534851}, {"text": "SNLI", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.909597635269165}, {"text": "SNLI test set", "start_pos": 142, "end_pos": 155, "type": "DATASET", "confidence": 0.8639402190844218}]}, {"text": " Table 4: The number of instances and accuracy per category achieved by each model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9991030693054199}]}]}