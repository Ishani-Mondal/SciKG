{"title": [{"text": "Retrieval of the Best Counterargument without Prior Topic Knowledge", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9815436601638794}]}], "abstractContent": [{"text": "Given any argument on any controversial topic, how to counter it?", "labels": [], "entities": []}, {"text": "This question implies the challenging retrieval task of finding the best counterargument.", "labels": [], "entities": []}, {"text": "Since prior knowledge of a topic cannot be expected in general, we hypothesize the best counterar-gument to invoke the same aspects as the argument while having the opposite stance.", "labels": [], "entities": []}, {"text": "To operationalize our hypothesis, we simultaneously model the similarity and dissim-ilarity of pairs of arguments, based on the words and embeddings of the arguments' premises and conclusions.", "labels": [], "entities": []}, {"text": "A salient property of our model is its independence from the topic at hand, i.e., it applies to arbitrary arguments.", "labels": [], "entities": []}, {"text": "We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org.", "labels": [], "entities": []}, {"text": "Systematic ranking experiments suggest that our hypothesis is true for many arguments: For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9977700710296631}]}, {"text": "Even among all 2801 test set pairs as candidates, we still find the best one about every third time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many controversial topics in real life divide us into opposing camps, such as whether to ban guns, who should become president, or what phone to buy.", "labels": [], "entities": []}, {"text": "When being confronted with arguments against our stance, but also when forming own arguments, we need to think about how they could best be countered.", "labels": [], "entities": []}, {"text": "Argumentation theory tells us that -aside from ad-hominem attacks -a counterargument denies either an argument's premises, its conclusion, or the reasoning between them (.", "labels": [], "entities": []}, {"text": "Take the following argument in favor of the right to bear arms from the web portal idebate.org: Argument \"Gun ownership is an integral aspect of the right to self defence.", "labels": [], "entities": [{"text": "Gun ownership", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.7124210149049759}]}, {"text": "(conclusion) Law-abiding citizens deserve the right to protect their families in their own homes, especially if the police are judged incapable of dealing with the threat of attack.\"", "labels": [], "entities": []}, {"text": "(premise) While the conclusion seems well-reasoned, the web portal directly provides a counter to the argument: Counterargument \"Burglary should not be punished by vigilante killings of the offender.", "labels": [], "entities": []}, {"text": "No amount of property is worth a human life.", "labels": [], "entities": []}, {"text": "Perversely, the danger of attack by homeowners may make it more likely that criminals will carry their own weapons.", "labels": [], "entities": []}, {"text": "If aright to self-defence is granted in this way, many accidental deaths are bound to result.", "labels": [], "entities": []}, {"text": "[...]\" As in this example, we observe that a counterargument often takes on the aspects of the topic invoked by the argument, while adding anew perspective to its conclusion and/or premises, conveying the opposite stance.", "labels": [], "entities": []}, {"text": "Research has tackled the stance of argument units as well as the attack relations between arguments.", "labels": [], "entities": []}, {"text": "However, existing approaches learn the interplay of aspects and topics on training data or infer it from external knowledge bases (details in Section 2).", "labels": [], "entities": []}, {"text": "This does notwork for topics unseen before.", "labels": [], "entities": []}, {"text": "Moreover, to our knowledge, no work so far aims at actual counterarguments.", "labels": [], "entities": []}, {"text": "This paper studies the task of automatically finding the best counterargument to any argument.", "labels": [], "entities": []}, {"text": "In the general case, we cannot expect prior knowledge of an argument's topic.", "labels": [], "entities": []}, {"text": "Following the observation above, we thus just hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance.", "labels": [], "entities": []}, {"text": "sketches how we operationalize the hypothesis.", "labels": [], "entities": []}, {"text": "In particular, we simultaneously model the topic similarity and stance dissimilarity of a candidate counterargument to the argument.", "labels": [], "entities": []}, {"text": "Both are inferred -in different ways -from the similarities to the argument's conclusion and premises, since it is unclear in advance, whether either of these units or the reasoning between them is countered.", "labels": [], "entities": []}, {"text": "Thereby, we find the most dissimilar among the most similar arguments.", "labels": [], "entities": []}, {"text": "To study counteraguments, we provide anew corpus with 6753 argument-counterargument pairs, taken from 1069 debates on idebate.org, as well as millions of false pairs derived from them.", "labels": [], "entities": []}, {"text": "Given the corpus, we define eight retrieval tasks that differ in the types of candidate counterarguments.", "labels": [], "entities": []}, {"text": "Based on the words and embeddings of the arguments, we develop similarity functions that realize the outlined model as a ranking approach.", "labels": [], "entities": []}, {"text": "In systematic experiments, we evaluate the different building blocks of our model on all defined tasks.", "labels": [], "entities": []}, {"text": "The results suggest that our hypothesis is true for many arguments.", "labels": [], "entities": []}, {"text": "The best model configuration improves common word and embedding similarity measures by eight to ten points accuracy in all tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9992094039916992}]}, {"text": "Inter alia, we rank 60.3% of the best counterarguments highest when given all arguments with opposite stance (7.6 on average).", "labels": [], "entities": []}, {"text": "Even with all 2801 test arguments as candidates, we still achieve 32.4% (and a mean rank of 15), fitting the intuition that offtopic arguments are easier to discard.", "labels": [], "entities": []}, {"text": "Our analysis reveals notable gaps across topical themes though.", "labels": [], "entities": []}, {"text": "Contributions We believe that our findings will be important for applications such as automatic debating technologies () and argument search ().", "labels": [], "entities": [{"text": "automatic debating", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.5512109100818634}, {"text": "argument search", "start_pos": 125, "end_pos": 140, "type": "TASK", "confidence": 0.8709376752376556}]}, {"text": "To summarize, our main contributions are: \u2022 A large corpus for studying multiple counterargument retrieval tasks (Sections 3 and 4).", "labels": [], "entities": [{"text": "counterargument retrieval tasks", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.7575821280479431}]}, {"text": "\u2022 A topic-independent approach to find the best counterargument to any argument (Section 5).", "labels": [], "entities": []}, {"text": "\u2022 Evidence that many counterarguments can be found without topic knowledge (Section 6).", "labels": [], "entities": []}, {"text": "The corpus as well as the Java source code for reproducing the experiments are available at http: //www.arguana.com.", "labels": [], "entities": []}], "datasetContent": [{"text": "We split the corpus into a training set, consisting of the first 60% of all debates of each theme (ordered by alphabet), as well as a validation set and a test set, each covering 20%.", "labels": [], "entities": []}, {"text": "The dataset sizes are found at the bottom of.", "labels": [], "entities": []}, {"text": "By putting all arguments from a debate into a single dataset, no specific topic knowledge can be gained from the training or validation set.", "labels": [], "entities": []}, {"text": "We include all themes in all datasets, because we expect the set of themes to be stable.", "labels": [], "entities": []}, {"text": "Among the 13 532 point and counters, 3407 appear twice, 723 three times, 36 four times, and 1 five times.", "labels": [], "entities": []}, {"text": "We ensure that no true pair is used as a false pair in our tasks.", "labels": [], "entities": []}, {"text": "We now report on systematic ranking experiments with our counterargument scoring model.", "labels": [], "entities": []}, {"text": "The goal is to evaluate on all eight retrieval tasks from Section 4 to what extent our hypothesis holds that the best counterargument to an argument invokes the same aspects while having opposing stance.", "labels": [], "entities": []}, {"text": "The Java source code of the experiments is available at: http://www.arguana.com/software  We evaluated the following set-up of tasks, data, measures, baselines, and approaches.", "labels": [], "entities": []}, {"text": "Tasks We tackled each of the eight retrieval tasks as a ranking problem, i.e., we aimed to rank the best counterargument to each argument highest, given all candidates.", "labels": [], "entities": []}, {"text": "Accordingly, only one candidate counterargument per argument is correct.", "labels": [], "entities": []}, {"text": "4 One alternative would be to see each argument pair as one instance of a classification problem.", "labels": [], "entities": []}, {"text": "However, our preliminary tests confirmed the intuition that identifying the best counterargument is hard without knowing the other candidates, i.e., there is no general (dis)similarity threshold that makes an argument the best counterargument.", "labels": [], "entities": []}, {"text": "Rather, how similar or dissimilar a counterargument needs to be depends on the topic and on the other candidates.", "labels": [], "entities": []}, {"text": "Another alternative would be to treat all candidates for an argument as one instance, but this makes the experimental set-up very intricated. has shown the true and false argument pairs in all datasets.", "labels": [], "entities": []}, {"text": "We undersampled each training set, resulting in 4065 true and 4065 false training pairs in all tasks.", "labels": [], "entities": []}, {"text": "Our model does not do any learning-to-rank on these pairs, but we derived lexicons for the word similarities from them (all stems included in at least 1% of all pairs).", "labels": [], "entities": []}, {"text": "As detailed below, we then determined the best model configurations on the validation sets and evaluated these configurations on the test sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of debates, points, and coun- ters over the themes in the counterargs-18 corpus.  The bottom rows show the size of the datasets.", "labels": [], "entities": [{"text": "counterargs-18 corpus", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.7812585532665253}]}, {"text": " Table 2: Number of true and false argument-counterargument pairs as well as their ratio for each evaluated  context and type of candidate counterarguments in the three datasets. Each line defines one retrieval task.", "labels": [], "entities": []}, {"text": " Table 3: Test set accuracy of ranking the best counterargument highest (@1) and mean rank (R) for 14  baselines and approaches (w, e, w \u2193 , . . . , r) in all eight tasks (given by Context and Candidates). Each best  accuracy value (bold) significantly outperforms the best baseline with 99% ( \u2020) or 99.9% ( \u2021) confidence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9810547828674316}, {"text": "mean rank (R)", "start_pos": 81, "end_pos": 94, "type": "METRIC", "confidence": 0.9604785919189454}, {"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9791614413261414}]}, {"text": " Table 4: Accuracy@1 and mean rank of the best  baseline (w + ) and approach (we \u2191 ) on each theme  when all 2801 test set arguments are candidates.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992935657501221}]}]}