{"title": [{"text": "Subword-level Word Vector Representations for Korean", "labels": [], "entities": [{"text": "Subword-level Word Vector Representations", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8005125224590302}]}], "abstractContent": [{"text": "Research on distributed word representations is focused on widely-used languages such as English.", "labels": [], "entities": []}, {"text": "Although the same methods can be used for other languages, language-specific knowledge can enhance the accuracy and richness of word vector representations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9969758987426758}]}, {"text": "In this paper, we look at improving distributed word representations for Korean using knowledge about the unique linguistic structure of Korean.", "labels": [], "entities": []}, {"text": "Specifically, we decompose Korean words into the jamo level, beyond the character-level, allowing a systematic use of sub-word information.", "labels": [], "entities": []}, {"text": "To evaluate the vectors, we develop Korean test sets for word similarity and analogy and make them publicly available.", "labels": [], "entities": [{"text": "Korean test sets", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.8873485525449117}, {"text": "word similarity", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.6745298951864243}]}, {"text": "The results show that our simple method outperforms word2vec and character-level Skip-Grams on semantic and syntactic similarity and analogy tasks and contributes positively toward downstream NLP tasks such as sentiment analysis .", "labels": [], "entities": [{"text": "semantic and syntactic similarity and analogy tasks", "start_pos": 95, "end_pos": 146, "type": "TASK", "confidence": 0.6659965600286212}, {"text": "sentiment analysis", "start_pos": 210, "end_pos": 228, "type": "TASK", "confidence": 0.962815523147583}]}], "introductionContent": [{"text": "Word vector representations built from a large corpus embed useful semantic and syntactic knowledge.", "labels": [], "entities": []}, {"text": "They can be used to measure the similarity between words and can be applied to various downstream tasks such as document classification, conversation modeling, and machine translation ().", "labels": [], "entities": [{"text": "document classification", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.8003177046775818}, {"text": "conversation modeling", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.8219119012355804}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.7960882484912872}]}, {"text": "Most previous research for learning the vectors focuses on English and thus leads to difficulties and limitations in directly applying those techniques to a language with a different internal structure from that of English.", "labels": [], "entities": []}, {"text": "The mismatch is especially significant for morphologically rich languages such as Korean where the morphological richness could be captured by subword level embedding such as character embedding.", "labels": [], "entities": []}, {"text": "It has been already shown that decomposing a word into subword units and using them as inputs improves performance for downstream NLP such as text classification (, language modeling ( , and machine translation (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7839443683624268}, {"text": "language modeling", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.7425092458724976}, {"text": "machine translation", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.7850939035415649}]}, {"text": "Despite their effectiveness in capturing syntactic features of diverse languages, decomposing a word into a set of n-grams and learning n-gram vectors does not consider the unique linguistic structures of various languages.", "labels": [], "entities": []}, {"text": "Thus, researchers have integrated language-specific structures to learn word vectors, for example subcharacter components of Chinese characters (  and syntactic information (such as prefixes or post-fixes) derived from external sources for English.", "labels": [], "entities": []}, {"text": "For Korean, integrating Korean linguistic structure at the level of jamo, the consonants and vowels that are much more rigidly defined than English, is shown to be effective for sentence parsing.", "labels": [], "entities": [{"text": "sentence parsing", "start_pos": 178, "end_pos": 194, "type": "TASK", "confidence": 0.7534517347812653}]}, {"text": "Previous work has looked at improving the vector representations of Korean using the character-level decomposition (), but there is room for further investigation because Korean characters can be decomposed to jamos which are smaller units than the characters.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to integrate Korean-specific subword information to learn Korean word vectors and show improvements over previous baselines methods for word similarity, analogy, and sentiment analysis.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 171, "end_pos": 186, "type": "TASK", "confidence": 0.7164182215929031}, {"text": "sentiment analysis", "start_pos": 201, "end_pos": 219, "type": "TASK", "confidence": 0.8933360278606415}]}, {"text": "Our first contri-bution is the method to decompose the words into both character-level units and jamo-level units and train the subword vectors through the Skip-Gram model.", "labels": [], "entities": []}, {"text": "Our second major contribution is the Korean evaluation datasets for word similarity and analogy tasks, a translation of the WS-353 with annotations by 14 Korean native speakers, and 10,000 items for semantic and syntactic analogies, developed with Korean linguistic expertise.", "labels": [], "entities": [{"text": "Korean evaluation datasets", "start_pos": 37, "end_pos": 63, "type": "DATASET", "confidence": 0.5811692674954733}, {"text": "word similarity and analogy tasks", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.8395428717136383}, {"text": "WS-353", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.8187102675437927}]}, {"text": "Using those datasets, we show that our model improves performance over other baseline methods without relying on external resources for word decomposition.", "labels": [], "entities": [{"text": "word decomposition", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7809692919254303}]}], "datasetContent": [{"text": "We evaluate the performance of word vectors through word similarity task and word analogy task.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7133185565471649}, {"text": "word analogy", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.8023340702056885}]}, {"text": "However, to best of our knowledge, there is no Korean evaluation dataset for either task.", "labels": [], "entities": [{"text": "Korean evaluation dataset", "start_pos": 47, "end_pos": 72, "type": "DATASET", "confidence": 0.7234677871068319}]}, {"text": "Thus we first develop the evaluation datasets.", "labels": [], "entities": []}, {"text": "We also test the word vectors for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9685556888580322}]}, {"text": "We develop a Korean version of the word similarity evaluation set.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.7443641126155853}]}, {"text": "Two graduate students who speak Korean as native language translated the English word pairs in WS-353 ().", "labels": [], "entities": [{"text": "WS-353", "start_pos": 95, "end_pos": 101, "type": "DATASET", "confidence": 0.8750454783439636}]}, {"text": "Then, 14 Korean native speakers annotated the similarity between pairs by giving scores from 0 to 10 for the translated pairs, following written instructions.", "labels": [], "entities": []}, {"text": "The original English instructions were translated into Korean as well.", "labels": [], "entities": []}, {"text": "Among the 14 scores for each pair, we exclude the minimum and maximum scores and compute the mean of the rest of the scores.", "labels": [], "entities": []}, {"text": "The correlation between the original scores and the annotated scores of the translated pairs is .82, which indicates that the translations are sufficiently reliable.", "labels": [], "entities": []}, {"text": "We attribute the difference to the linguistic and cultural differences.", "labels": [], "entities": []}, {"text": "We make the Korean version of WS-353 publicly available.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.5921911001205444}]}, {"text": "We develop the word analogy test items to evaluate the performance of word vectors.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.7408798635005951}]}, {"text": "The evaluation dataset consists of 10,000 items and includes 5,000 items for evaluating the semantic features and 5,000 for the syntactic features.", "labels": [], "entities": []}, {"text": "We also release our word analogy evaluation dataset for future research.", "labels": [], "entities": [{"text": "word analogy evaluation", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.6921658515930176}]}, {"text": "\u2022 Capital-Country (Capt.) includes two word pairs representing the relation between the country name and its capital:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of our method and comparison models. Average cosine distance for each category  in word analogy task are reported. Overall, our model outperforms comparison models, showing close  distance between predicted vector a + b \u2212 c and the target vector d (a:b=c:d). Specifically, performance  is improved more in syntactic analogies.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.8067697485287985}]}, {"text": " Table 3: Performance of sentiment classification  task. 3-5 jamo n-grams and 1-6 chracter n-grams  show slightly higher performance in terms of ac- curacy and f1-score over comparison models.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.9658539295196533}, {"text": "ac- curacy", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9647075136502584}]}]}