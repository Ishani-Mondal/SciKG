{"title": [{"text": "Extracting Commonsense Properties from Embeddings with Limited Human Guidance", "labels": [], "entities": []}], "abstractContent": [{"text": "Intelligent systems require commonsense, but automatically extracting this knowledge from text can be difficult.", "labels": [], "entities": []}, {"text": "We propose and assess methods for extracting one type of commonsense knowledge, object-property comparisons, from pre-trained embeddings.", "labels": [], "entities": []}, {"text": "In experiments, we show that our approach exceeds the accuracy of previous work but requires substantially less hand-annotated knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9989237189292908}]}, {"text": "Further, we show that an active learning approach that synthesizes common-sense queries can boost accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9964929223060608}]}], "introductionContent": [{"text": "Automatically extracting commonsense from text is a long-standing challenge in natural language processing; Van Durme and.", "labels": [], "entities": [{"text": "Automatically extracting commonsense from text", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7445658624172211}]}, {"text": "As argued by, typical language use may reflect commonsense, but the commonsense knowledge itself is not often explicitly stated, due to reporting bias).", "labels": [], "entities": []}, {"text": "Thus, additional human knowledge or annotated training data are often used to help systems learn commonsense.", "labels": [], "entities": []}, {"text": "In this paper, we study methods for reducing the amount of human input needed to learn commonsense.", "labels": [], "entities": []}, {"text": "Specifically, we focus on learning relative comparisons of (one-dimensional) object properties, such as the fact that a cantaloupe is more round than a hammer.", "labels": [], "entities": []}, {"text": "Methods for learning this kind of commonsense have been developed previously (e.g., but the best-performing methods in that previous work requires dozens of manually-annotated frames for each comparison property, to connect the property to how it is indirectly reflected in text-e.g., if text asserts that \"x carries y,\" this implies that x is probably larger than y.", "labels": [], "entities": []}, {"text": "Our architecture for relative comparisons follows the zero-shot learning paradigm (.", "labels": [], "entities": []}, {"text": "It takes the form of a neural network that compares a projection of embeddings for each of two objects (e.g. \"elephant\" and \"tiger\") to the embeddings for the two poles of the target dimension of comparison (e.g., \"big\" and \"small\" for the size property).", "labels": [], "entities": []}, {"text": "The projected object embeddings are trained to be closer to the appropriate pole, using a small training set of hand-labeled comparisons.", "labels": [], "entities": []}, {"text": "Our experiments reveal that our architecture outperforms previous work, despite using less annotated data.", "labels": [], "entities": []}, {"text": "Further, because our architecture takes the property (pole) labels as arguments, it can extend to the zero-shot setting in which we evaluate on properties not seen in training.", "labels": [], "entities": []}, {"text": "We find that in zero-shot, our approach outperforms baselines and comes close to supervised results, but providing labels for both poles of the relation rather than just one is important.", "labels": [], "entities": []}, {"text": "Finally, because the number of properties we wish to learn is large, we experiment with active learning (AL) over a larger property space.", "labels": [], "entities": []}, {"text": "We show that synthesizing AL queries can be effective using an approach that explicitly models which comparison questions are nonsensical (e.g., is Batman taller than Democracy?).", "labels": [], "entities": []}, {"text": "We release our code base and anew commonsense data set to the research community.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present our experimental results on both the three-way and four-way tasks.", "labels": [], "entities": []}, {"text": "We experiment with three types of embeddings: GloVe, normalized 300-dimensional embeddings trained on a corpus of 6B tokens) (the F&C method (Forbes and Choi, 2017) uses the 100-dimensional version, as it achieves the highest validation accuracy for their methods); Word2vec, normalized 300-dimensional embeddings trained on 100B tokens (; and LSTM, the normalized 1024-dimensional weight matrix from the softmax layer of the Google 1B LSTM language model ().", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 266, "end_pos": 274, "type": "DATASET", "confidence": 0.9386063814163208}]}, {"text": "For training PCE, we use an identity activation function and apply 50% dropout.", "labels": [], "entities": [{"text": "PCE", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.7543630599975586}]}, {"text": "We use the Adam optimizer with default settings to train the models for 800 epochs, minimizing cross entropy loss.", "labels": [], "entities": []}, {"text": "For zero-shot learning, we adopt a hold-oneproperty-out scheme to test our models' zero-shot performance.", "labels": [], "entities": [{"text": "zero-shot learning", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8853213489055634}]}, {"text": "Finally, for active learning, we use Word2vec embeddings.", "labels": [], "entities": [{"text": "Word2vec embeddings", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.9097396731376648}]}, {"text": "All the models are trained on 200 random training examples to warm up.", "labels": [], "entities": []}, {"text": "We train for 20 epochs after each label acquisition.", "labels": [], "entities": []}, {"text": "To smooth noise, we report the average of 20 different runs of random (passive learning) and least confident (LC) pool-based active learning (Culotta and) baselines.", "labels": [], "entities": []}, {"text": "We report the average of only 6 runs for an expected model change (EMC) pool-based active learning) baseline due to its high computational cost, and of only 2 runs for our synthesis active learning approach due to its high labeling cost.", "labels": [], "entities": []}, {"text": "The pool size is 1540 examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on the VERB PHYSICS data set. PCE outperforms the F&C model from previous  work. PCE(one-pole) and PCE(no reverse) use LSTM embeddings.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.993560791015625}, {"text": "VERB PHYSICS data set", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.850220337510109}, {"text": "F&C", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8816597064336141}]}, {"text": " Table 2: Accuracy of zero-shot learning on the VERB PHYSICS data set(using LSTM embeddings).  PCE outperforms the baselines, and using both poles is important for accuracy.", "labels": [], "entities": [{"text": "VERB PHYSICS data set", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.8977399915456772}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9973146319389343}]}, {"text": " Table 3: Accuracy on the four-way task on the  PROPERTY COMMON SENSE data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9972028732299805}, {"text": "PROPERTY COMMON SENSE data", "start_pos": 48, "end_pos": 74, "type": "DATASET", "confidence": 0.5264922529459}]}, {"text": " Table 4: Scores of smallness for 5 randomly  picked objects in VERB PHYSICS data set", "labels": [], "entities": [{"text": "VERB PHYSICS data set", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.870092049241066}]}, {"text": " Table 5: Trained and zero-shot accuracies for dif- ferent word choices", "labels": [], "entities": []}]}