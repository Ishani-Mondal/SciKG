{"title": [{"text": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context", "labels": [], "entities": []}], "abstractContent": [{"text": "We know very little about how neural language models (LM) use prior linguistic context.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the role of context in an LSTM LM, through ablation studies.", "labels": [], "entities": [{"text": "LSTM LM", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.7353634834289551}]}, {"text": "Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped.", "labels": [], "entities": []}, {"text": "On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9941672086715698}, {"text": "WikiText-2", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.894051194190979}]}, {"text": "The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic.", "labels": [], "entities": []}, {"text": "We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context.", "labels": [], "entities": []}, {"text": "Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models are an important component of natural language generation tasks, such as machine translation and summarization.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.6961522698402405}, {"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.824735701084137}, {"text": "summarization", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.9525814056396484}]}, {"text": "They use context (a sequence of words) to estimate a probability distribution of the upcoming word.", "labels": [], "entities": []}, {"text": "For several years now, neural language models (NLMs)) have consistently outperformed classical n-gram models, an improvement often attributed to their ability to model long-range dependencies in faraway context.", "labels": [], "entities": []}, {"text": "Yet, how these NLMs use the context is largely unexplained.", "labels": [], "entities": []}, {"text": "Recent studies have begun to shed light on the information encoded by Long Short-Term Memory (LSTM) networks.", "labels": [], "entities": []}, {"text": "They can remember sentence lengths, word identity, and word order (, can capture some syntactic structures such as subject-verb agreement (, and can model certain kinds of semantic compositionality such as negation and intensification (.", "labels": [], "entities": []}, {"text": "However, all of the previous work studies LSTMs at the sentence level, even though they can potentially encode longer context.", "labels": [], "entities": []}, {"text": "Our goal is to complement the prior work to provide a richer understanding of the role of context, in particular, long-range context beyond a sentence.", "labels": [], "entities": []}, {"text": "We aim to answer the following questions: (i) How much context is used by NLMs, in terms of the number of tokens?", "labels": [], "entities": []}, {"text": "(ii) Within this range, are nearby and long-range contexts represented differently?", "labels": [], "entities": []}, {"text": "(iii) How do copy mechanisms help the model use different regions of context?", "labels": [], "entities": []}, {"text": "We investigate these questions via ablation studies on a standard LSTM language model () on two benchmark language modeling datasets: Penn Treebank and WikiText-2.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.9925702810287476}]}, {"text": "Given a pretrained language model, we perturb the prior context in various ways attest time, to study how much the perturbed information affects model performance.", "labels": [], "entities": []}, {"text": "Specifically, we alter the context length to study how many tokens are used, permute tokens to see if LSTMs care about word order in both local and global contexts, and drop and replace target words to test the copying abilities of LSTMs with and without an external copy mechanism, such as the neural cache ().", "labels": [], "entities": []}, {"text": "The cache operates by first recording tar-get words and their context representations seen in the history, and then encouraging the model to copy a word from the past when the current context representation matches that word's recorded context vector.", "labels": [], "entities": []}, {"text": "We find that the LSTM is capable of using about 200 tokens of context on average, with no observable differences from changing the hyperparameter settings.", "labels": [], "entities": []}, {"text": "Within this context range, word order is only relevant within the 20 most recent tokens or about a sentence.", "labels": [], "entities": [{"text": "word order", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.7298313677310944}]}, {"text": "In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words.", "labels": [], "entities": []}, {"text": "Finally, we find that LSTMs can regenerate some words seen in the nearby context, but heavily rely on the cache to help them copy words from the long-range context.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Dataset statistics and performance rele- vant to our experiments.", "labels": [], "entities": [{"text": "performance rele- vant", "start_pos": 33, "end_pos": 55, "type": "METRIC", "confidence": 0.7439585328102112}]}]}