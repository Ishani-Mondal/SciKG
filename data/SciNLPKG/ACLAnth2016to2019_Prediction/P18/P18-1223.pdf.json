{"title": [{"text": "Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval", "labels": [], "entities": [{"text": "Neural Information Retrieval", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.6632370551427206}]}], "abstractContent": [{"text": "This paper presents the Entity-Duet Neu-ral Ranking Model (EDRM), which introduces knowledge graphs to neural search systems.", "labels": [], "entities": []}, {"text": "EDRM represents queries and documents by their words and entity annotations.", "labels": [], "entities": [{"text": "EDRM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8363844156265259}]}, {"text": "The semantics from knowledge graphs are integrated in the distributed representations of their entities, while the ranking is conducted by interaction-based neural ranking networks.", "labels": [], "entities": []}, {"text": "The two components are learned end-to-end, making EDRM a natural combination of entity-oriented search and neural information retrieval.", "labels": [], "entities": []}, {"text": "Our experiments on a commercial search log demonstrate the effectiveness of EDRM.", "labels": [], "entities": []}, {"text": "Our analyses reveal that knowledge graph semantics significantly improve the generalization ability of neu-ral ranking models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The emergence of large scale knowledge graphs has motivated the development of entity-oriented search, which utilizes knowledge graphs to improve search engines.", "labels": [], "entities": []}, {"text": "The recent progresses in entity-oriented search include better text representations with entity annotations (, richer ranking features), entity-based connections between query and documents (, and soft-match query and documents through knowledge graph relations or embeddings (.", "labels": [], "entities": []}, {"text": "These approaches bring in entities and semantics from knowledge graphs and have greatly improved the effectiveness of feature-based search systems.", "labels": [], "entities": []}, {"text": "* Corresponding author: M.", "labels": [], "entities": []}, {"text": "Sun (sms@tsinghua.edu.cn) Another frontier of information retrieval is the development of neural ranking models (neural-IR).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7616308927536011}]}, {"text": "Deep learning techniques have been used to learn distributed representations of queries and documents that capture their relevance relations (representation-based)), or to model the query-document relevancy directly from their word-level interactions (interactionbased).", "labels": [], "entities": []}, {"text": "Neural-IR approaches, especially the interaction-based ones, have greatly improved the ranking accuracy when large scale training data are available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9926432967185974}]}, {"text": "Entity-oriented search and neural-IR push the boundary of search engines from two different aspects.", "labels": [], "entities": []}, {"text": "Entity-oriented search incorporates human knowledge from entities and knowledge graph semantics.", "labels": [], "entities": []}, {"text": "It has shown promising results on feature-based ranking systems.", "labels": [], "entities": []}, {"text": "On the other hand, neural-IR leverages distributed representations and neural networks to learn more sophisticated ranking models form large-scale training data.", "labels": [], "entities": []}, {"text": "However, it remains unclear how these two approaches interact with each other and whether the entity-oriented search has the same advantage in neural-IR methods as in feature-based systems.", "labels": [], "entities": []}, {"text": "This paper explores the role of entities and semantics in neural-IR.", "labels": [], "entities": []}, {"text": "We present an EntityDuet Neural Ranking Model (EDRM) that incorporates entities in interaction-based neural ranking models.", "labels": [], "entities": []}, {"text": "EDRM first learns the distributed representations of entities using their semantics from knowledge graphs: descriptions and types.", "labels": [], "entities": [{"text": "EDRM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.790553092956543}]}, {"text": "Then it follows a recent state-of-the-art entity-oriented search framework, the word-entity duet (, and matches documents to queries with both bag-of-words and bag-of-entities.", "labels": [], "entities": []}, {"text": "Instead of manual features, EDRM uses interactionbased neural models ( to match query and documents with word-entity duet rep-resentations.", "labels": [], "entities": []}, {"text": "As a result, EDRM combines entityoriented search and the interaction based neural-IR; it brings the knowledge graph semantics to neural-IR and enhances entity-oriented search with neural networks.", "labels": [], "entities": []}, {"text": "One advantage of being neural is that EDRM can be learned end-to-end.", "labels": [], "entities": []}, {"text": "Given a large amount of user feedback from a commercial search log, the integration of knowledge graph semantics to neural ranker, is learned jointly with the modeling of query-document relevance in EDRM.", "labels": [], "entities": []}, {"text": "It provides a convenient data-driven way to leverage external semantics in neural-IR.", "labels": [], "entities": []}, {"text": "Our experiments on a Sogou query log and CNDBpedia demonstrate the effectiveness of entities and semantics in neural models.", "labels": [], "entities": [{"text": "Sogou query log", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.8239361643791199}, {"text": "CNDBpedia", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.8221308588981628}]}, {"text": "EDRM significantly outperforms the word-interaction-based neural ranking model, K-NRM (, confirming the advantage of entities in enriching word-based ranking., the recent stateof-the-art neural ranker that models phrase level interactions, provides a more interesting observation: Conv-KNRM predicts user clicks reasonably well, but integrating knowledge graphs using EDRM significantly improves the neural model's generalization ability on more difficult scenarios.", "labels": [], "entities": []}, {"text": "Our analyses further revealed the source of EDRM's generalization ability: the knowledge graph semantics.", "labels": [], "entities": []}, {"text": "If only treating entities as ids and ignoring their semantics from the knowledge graph, the entity annotations are only a cleaner version of phrases.", "labels": [], "entities": []}, {"text": "In neural-IR systems, the embeddings and convolutional neural networks have already done a decent job in modeling phraselevel matches.", "labels": [], "entities": []}, {"text": "However, the knowledge graph semantics brought by EDRM cannot yet be captured solely by neural networks; incorporating those human knowledge greatly improves the generalization ability of neural ranking systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the dataset, evaluation metrics, knowledge graph, baselines, and implementation details of our experiments.", "labels": [], "entities": []}, {"text": "Our experiments use a query log from Sogou.com, a major Chinese searching engine ().", "labels": [], "entities": []}, {"text": "The exact same dataset and training-testing splits in the previous research are used.", "labels": [], "entities": []}, {"text": "They defined the ad-hoc ranking task in this dataset as re-ranking the candidate documents provided by the search engine.", "labels": [], "entities": []}, {"text": "All Chinese texts are segmented by ICTCLAS (, after that they are treated the same as English.", "labels": [], "entities": [{"text": "ICTCLAS", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.6826536059379578}]}, {"text": "Prior research leverages clicks to model user behaviors and infer reliable relevance signals using click models (.", "labels": [], "entities": []}, {"text": "DCTR and TACM are two click models: DCTR calculates the relevance scores of a query-document pair based on their click through rates (CTR); TACM () is a more sophisticated model that uses both clicks and dwell times.", "labels": [], "entities": [{"text": "DCTR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8481869697570801}, {"text": "click through rates (CTR)", "start_pos": 113, "end_pos": 138, "type": "METRIC", "confidence": 0.8068502843379974}, {"text": "TACM", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.7351389527320862}]}, {"text": "Following previous research (Xiong et al., 2017b), both DCTR and TACM are used to infer labels.", "labels": [], "entities": [{"text": "DCTR", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.6881979703903198}, {"text": "TACM", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9967139959335327}]}, {"text": "DCTR inferred relevance labels are used in training.", "labels": [], "entities": [{"text": "DCTR inferred relevance labels", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5027313083410263}]}, {"text": "Three testing scenarios are used: Testing-SAME, Testing-DIFF and Testing-RAW.", "labels": [], "entities": []}, {"text": "Testing-SAME uses DCTR labels, the same as in training.", "labels": [], "entities": []}, {"text": "Testing-DIFF evaluates models performance based on TACM inferred relevance labels.", "labels": [], "entities": [{"text": "TACM inferred relevance labels", "start_pos": 51, "end_pos": 81, "type": "METRIC", "confidence": 0.8079111129045486}]}, {"text": "Testing-RAW evaluates ranking models through user clicks, which tests ranking performance for the most satisfying document.", "labels": [], "entities": []}, {"text": "Testing-DIFF and Testing-RAW are harder scenarios that challenge the generalization ability of all models, because their training labels and testing labels are generated differently ( documents.", "labels": [], "entities": []}, {"text": "As shown in, the majority of queries have at least one entity annotation; the average number of entity annotated per document title is about four.", "labels": [], "entities": []}, {"text": "The baselines include feature-based ranking models and neural ranking models.", "labels": [], "entities": []}, {"text": "Most of the baselines are borrowed from previous research (.", "labels": [], "entities": []}, {"text": "Feature-based baselines include two learning to rank systems, RankSVM (Joachims, 2002) and coordinate ascent (Coor-Accent)).", "labels": [], "entities": []}, {"text": "The standard word-based unsupervised retrieval model, BM25, is also compared.", "labels": [], "entities": [{"text": "BM25", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.6228709816932678}]}, {"text": "Neural baselines include CDSSM (), MatchPyramid (MP) (, DRMM (Grauman and), K-NRM () and Conv-KNRM (.", "labels": [], "entities": []}, {"text": "It uses CNN to build query and document representations on word letter-tri-grams (or Chinese characters).", "labels": [], "entities": []}, {"text": "MP and DRMM are both interaction based models.", "labels": [], "entities": [{"text": "DRMM", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.800381064414978}]}, {"text": "They use CNNs or histogram pooling to extract features from embedding based translation matrix.", "labels": [], "entities": []}, {"text": "Our main baselines are K-NRM and Conv-KNRM, the recent state-of-the-art neural models on the Sogou-Log dataset.", "labels": [], "entities": [{"text": "Sogou-Log dataset", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.9431577920913696}]}, {"text": "The goal of our experiments is to explore the effectiveness of knowledge graphs in these state-of-the-art interaction based neural models.", "labels": [], "entities": []}, {"text": "The dimension of word embedding, entity embedding and type embedding are 300.", "labels": [], "entities": []}, {"text": "Vocabulary size of entities and words are 44,930 and 165,877.", "labels": [], "entities": [{"text": "Vocabulary size", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.954976886510849}]}, {"text": "Conv-KNRM uses one layer CNN with 128 filter size for the ngram composition.", "labels": [], "entities": []}, {"text": "Entity description encoder is a one layer CNN with 128 and 300 filter size for Conv-KNRM and K-NRM respectively.", "labels": [], "entities": []}, {"text": "All models are implemented with PyTorch.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9204777479171753}]}, {"text": "Adam is utilized to optimize all parameters with learning rate = 0.001, = 1e \u2212 5 and early stopping with the practice of 5 epochs.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.9358340501785278}]}, {"text": "There are two versions of EDRM: EDRM-KNRM and EDRM-CKNRM, integrating with K-NRM and Conv-KNRM respectively.", "labels": [], "entities": [{"text": "EDRM", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.8420838713645935}, {"text": "EDRM-KNRM", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.8246920704841614}, {"text": "EDRM-CKNRM", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.8028741478919983}]}, {"text": "The first one (K-NRM) enriches the word based neural ranking model with entities and knowledge graph semantics; the second one (Conv-KNRM) enriches the n-gram based neural ranking model.", "labels": [], "entities": []}, {"text": "Four experiments are conducted to study the effectiveness of EDRM: the overall performance, the contributions of matching kernels, the ablation study, and the influence of entities in different scenarios.", "labels": [], "entities": []}, {"text": "We also do case studies to show effect of EDRM on document ranking.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Ranking accuracy of EDRM-KNRM, EDRM-CKNRM and baseline methods. Relative per- formances compared with K-NRM are in percentages.  \u2020,  \u2021,  \u00a7,  \u00b6,  *  indicate statistically significant  improvements over DRMM  \u2020 , CDSSM  \u2021 , MP  \u00a7 , K-NRM  \u00b6 and Conv-KNRM  *  respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8982380032539368}, {"text": "EDRM-CKNRM", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.8789564967155457}, {"text": "Relative per- formances", "start_pos": 74, "end_pos": 97, "type": "METRIC", "confidence": 0.9362595677375793}]}, {"text": " Table 2: Ranking accuracy of adding diverse semantics based on K-NRM and Conv-KNRM. Rela- tive performances compared are in percentages.  \u2020,  \u2021,  \u00a7,  \u00b6,  * ,  *  *  indicate statistically significant im- provements over K-NRM  \u2020 (or Conv-KNRM  \u2020 ), +Embed  \u2021 , +Type  \u00a7 , +Description  \u00b6 , +Embed+Type  *  and  +Embed+Description  *  *  respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.954732358455658}, {"text": "Rela", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9951905012130737}]}]}