{"title": [{"text": "HotFlip: White-Box Adversarial Examples for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7968288362026215}]}], "abstractContent": [{"text": "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier.", "labels": [], "entities": []}, {"text": "We find that only a few manipulations are needed to greatly decrease the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9989414811134338}]}, {"text": "Our method relies on anatomic flip operation , which swaps one token for another , based on the gradients of the one-hot input vectors.", "labels": [], "entities": []}, {"text": "Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks attest time.", "labels": [], "entities": []}, {"text": "With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Adversarial examples are inputs to a predictive machine learning model that are maliciously designed to cause poor performance (.", "labels": [], "entities": []}, {"text": "Adversarial examples expose regions of the input space where the model performs poorly, which can aid in understanding and improving the model.", "labels": [], "entities": []}, {"text": "By using these examples as training data, adversarial training learns models that are more robust, and may even perform better on non-adversarial examples.", "labels": [], "entities": []}, {"text": "Interest in understanding vulnerabilities of NLP systems is growing (.", "labels": [], "entities": []}, {"text": "Previous work has focused on heuristics for creating adversarial examples in the black-box setting, without any explicit knowledge of the model parameters.", "labels": [], "entities": []}, {"text": "In the white-box setting, we use complete knowledge of the model to develop worst-case attacks, which can reveal much larger vulnerabilities.", "labels": [], "entities": []}, {"text": "We propose a white-box adversary against differentiable text classifiers.", "labels": [], "entities": []}, {"text": "We find that only a few South Africa's historic Soweto township marks its 100th birthday on Tuesday in a mood of optimism.", "labels": [], "entities": []}, {"text": "57% World South Africa's historic Soweto township marks its 100th birthday on Tuesday in a mooP of optimism.", "labels": [], "entities": [{"text": "57% World South Africa's historic Soweto township", "start_pos": 0, "end_pos": 49, "type": "DATASET", "confidence": 0.6748460796144273}]}, {"text": "95% Sci/Tech Chancellor Gordon Brown has sought to quell speculation over who should run the Labour Party and turned the attack on the opposition Conservatives.", "labels": [], "entities": []}, {"text": "75% World Chancellor Gordon Brown has sought to quell speculation over who should run the Labour Party and turned the attack on the oBposition Conservatives.", "labels": [], "entities": [{"text": "World Chancellor Gordon Brown", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.9550465792417526}, {"text": "oBposition Conservatives", "start_pos": 132, "end_pos": 156, "type": "DATASET", "confidence": 0.967503160238266}]}, {"text": "94% Business: Adversarial examples with a single character change, which will be misclassified by a neural classifier.", "labels": [], "entities": []}, {"text": "manipulations are needed to greatly increase the misclassification error.", "labels": [], "entities": []}, {"text": "Furthermore, fast generation of adversarial examples allows feasible adversarial training, which helps the model defend against adversarial examples and improve accuracy on clean examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9974592328071594}]}, {"text": "At the core of our method lies anatomic flip operation, which changes one token to another by using the directional derivatives of the model with respect to the one-hot vector input.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We propose an efficient gradient-based optimization method to manipulate discrete text structure at its one-hot representation.", "labels": [], "entities": []}, {"text": "2. We investigate the robustness of a classifier trained with adversarial examples, by studying its resilience to attacks and its accuracy on clean test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9985802173614502}]}], "datasetContent": [{"text": "In principle, HotFlip could be applied to any differentiable character-based classifier.", "labels": [], "entities": []}, {"text": "Here, we focus on the CharCNN-LSTM architecture, which can be adapted for classification via a single dense layer after the last recurrent hidden unit.", "labels": [], "entities": []}, {"text": "We use the AG's news dataset 3 , which consists of 120,000 training and 7,600 test instances from four equal-sized classes: World, Sports, Business, and Science/Technology.", "labels": [], "entities": [{"text": "AG's news dataset 3", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.9448287963867188}]}, {"text": "The architecture consists of a 2-layer stacked LSTM with 500 hidden units, a character embedding size of 25, and 1000 kernels of width 6 for temporal convolutions.", "labels": [], "entities": []}, {"text": "This classifier was able to outperform (, which has achieved the state-of-the-art result on some benchmarks, on AG's news.", "labels": [], "entities": [{"text": "AG's news", "start_pos": 112, "end_pos": 121, "type": "DATASET", "confidence": 0.9807570377985636}]}, {"text": "The model is trained with SGD and gradient clipping, and the batch size was set to 64.", "labels": [], "entities": []}, {"text": "We used 10% of the training data as the development set, and trained fora maximum of 25 epochs.", "labels": [], "entities": []}, {"text": "We only allow character changes if the new word does not exist in the vocabulary, to avoid changes that are more likely to change the meaning of text.", "labels": [], "entities": []}, {"text": "The adversary uses abeam size of 10, and has a budget of maximum of 10% of characters in the document.", "labels": [], "entities": []}, {"text": "In, we plot the success rate of the adversary against an acceptable confidence score for the misclassification.", "labels": [], "entities": []}, {"text": "That is, we consider the adversary successful only if the classifier misclassifies the instance with a given confidence score.", "labels": [], "entities": []}, {"text": "For this experiment, we create adversarial examples for 10% of the test set.", "labels": [], "entities": []}, {"text": "We compare with a (greedy) black-box adversary, which does not have access to model parameters, and simply queries the classifier with random character changes.", "labels": [], "entities": []}, {"text": "define an attack, Key, in which a character is replaced with an adjacent character in the keyboard.", "labels": [], "entities": []}, {"text": "We allow a stronger black-box attacker to change a character to any character in the alphabet, and we call it Key * . As expected a white-box adversary is much more damaging, and has a higher success rate.", "labels": [], "entities": []}, {"text": "As can be seen, the beam-search strategy is very effective in fooling the classifier even with an 0.9 confidence constraint, tricking the classifier for more than 90% of the instances.", "labels": [], "entities": []}, {"text": "A greedy search is less effective especially in producing high-confidence scores.", "labels": [], "entities": []}, {"text": "We use a maximum of 10% of characters in the document as the budget for the adversary, but our adversary changes an average of 4.18% of the characters to trick the classifier at confidence 0.5.", "labels": [], "entities": []}, {"text": "The adversary picks the flip operation around 80% of the times, and favors delete over insert by two to one.", "labels": [], "entities": []}], "tableCaptions": []}