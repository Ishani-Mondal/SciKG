{"title": [{"text": "Multilingual Named Entity Recognition on Spanish-English Code-switched Tweets using Support Vector Machines", "labels": [], "entities": [{"text": "Multilingual Named Entity Recognition", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6314440593123436}]}], "abstractContent": [{"text": "This paper describes our system submission for the ACL 2018 shared task on named entity recognition (NER) in code-switched Twitter data.", "labels": [], "entities": [{"text": "ACL 2018 shared task on named entity recognition (NER) in code-switched Twitter", "start_pos": 51, "end_pos": 130, "type": "TASK", "confidence": 0.6886200479098729}]}, {"text": "Our best result (F1 = 53.65) was obtained using a Support Vector Machine (SVM) with 14 features combined with rule-based post-processing.", "labels": [], "entities": [{"text": "F1", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.999459445476532}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) is apart of information extraction and refers to the automatic identification of named entities in text.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7760950326919556}, {"text": "information extraction", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7331398576498032}, {"text": "automatic identification of named entities in text", "start_pos": 84, "end_pos": 134, "type": "TASK", "confidence": 0.7303919451577323}]}, {"text": "The ACL 2018 shared task invited participants to extract and classify the following named entities in codeswitched data obtained from Twitter: person, location, organization, group, title, product, event, time, and other ().", "labels": [], "entities": [{"text": "ACL 2018 shared task invited participants to extract and classify the following named entities in codeswitched data obtained from Twitter: person, location, organization, group, title, product, event, time, and", "start_pos": 4, "end_pos": 214, "type": "Description", "confidence": 0.730350324982091}]}, {"text": "The Tweets are either Spanish-English or Modern Standard Arabic-Egyptian, and participants were free to participate in either language pair.", "labels": [], "entities": []}, {"text": "This paper describes our system for the Spanish-English NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 56, "end_pos": 64, "type": "TASK", "confidence": 0.7920103967189789}]}, {"text": "This particular NER task is challenging for two reasons.", "labels": [], "entities": [{"text": "NER task", "start_pos": 16, "end_pos": 24, "type": "TASK", "confidence": 0.9158171117305756}]}, {"text": "Firstly, NER has proved to be more difficult for Tweets than for longer text, as accuracy in NER ranges from 85-90% on longer texts compared to 30-50% on Tweets (.", "labels": [], "entities": [{"text": "NER", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.8132286667823792}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.999344527721405}]}, {"text": "One of the reasons for this difference is that Tweets contain non-standard spelling, unusual punctuation, and unreliable capitalization.", "labels": [], "entities": []}, {"text": "also point out that another difficulty stems from the rapidly changing topics and linguistic conventions on Twitter.", "labels": [], "entities": []}, {"text": "The 2015 and 2016 shared tasks for NER on Noisy Usergenerated Text (W-NUT) reported F1 scores between 16.47 and 52.41 for identifying 10 different NE categories ().", "labels": [], "entities": [{"text": "NER on Noisy Usergenerated Text (W-NUT)", "start_pos": 35, "end_pos": 74, "type": "TASK", "confidence": 0.8156676143407822}, {"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9994435906410217}]}, {"text": "NER methods range from bidirectional long short-term memory (LSTM) and Conditional Random Fields (CRF) (, to Named Entity Linking (.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9131802320480347}, {"text": "Named Entity Linking", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.6617648204167684}]}, {"text": "The second added challenge for the data in this task is that the Tweets contain English and Spanish named entities.", "labels": [], "entities": []}, {"text": "Both languages need to betaken into account in order to accurately identify the NEs in this data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of Tweets, tokens and Named  Entities in the Spanish-English data sets.", "labels": [], "entities": [{"text": "Spanish-English data sets", "start_pos": 62, "end_pos": 87, "type": "DATASET", "confidence": 0.813982625802358}]}, {"text": " Table 2: Results for the train/test set without post- processing (Macro F1) and the held-out test set  (FB1).", "labels": [], "entities": [{"text": "FB1", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.7346768975257874}]}, {"text": " Table 3: Performance of the classifiers with the  different training sizes.", "labels": [], "entities": []}, {"text": " Table 4: Results of best performing SVM per cat- egory including post-processing.", "labels": [], "entities": [{"text": "SVM", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9644254446029663}]}, {"text": " Table 5: Number of times the tag '-TIME' occurs  for the days of the week in the training Tweets.", "labels": [], "entities": [{"text": "TIME", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9231035113334656}]}, {"text": " Table 6: Distribution of NEs in the training data.  The overlap refers to the percentage of types that  was present in both the training set and the test set  extracted from the training.", "labels": [], "entities": [{"text": "overlap", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9602363109588623}]}]}