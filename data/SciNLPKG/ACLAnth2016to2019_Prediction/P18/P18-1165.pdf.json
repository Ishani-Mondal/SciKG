{"title": [{"text": "Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning", "labels": [], "entities": [{"text": "Sequence-to-Sequence Reinforcement Learning", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.9090512792269388}]}], "abstractContent": [{"text": "We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT).", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.7252140760421752}, {"text": "bandit neural machine translation (NMT)", "start_pos": 139, "end_pos": 178, "type": "TASK", "confidence": 0.8047309177262443}]}, {"text": "We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task.", "labels": [], "entities": [{"text": "RL task", "start_pos": 206, "end_pos": 213, "type": "TASK", "confidence": 0.8787818849086761}]}, {"text": "Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra-and inter-annotator \u03b1-agreement is comparable.", "labels": [], "entities": []}, {"text": "Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from.", "labels": [], "entities": [{"text": "reliability", "start_pos": 5, "end_pos": 16, "type": "METRIC", "confidence": 0.9774152636528015}]}, {"text": "Finally , improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9990983009338379}]}, {"text": "This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.", "labels": [], "entities": [{"text": "RL", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9735236763954163}]}], "introductionContent": [{"text": "Recent work has received high attention by successfully scaling reinforcement learning (RL) to games with large state-action spaces, achieving human-level ( or even superhuman performance . This success and the ability of RL to circumvent the data annotation bottleneck in supervised learning has led to renewed interest in RL in sequenceto-sequence learning problems with exponential * The work for this paper was done while the second author was an intern in Heidelberg.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.7061648190021514}]}, {"text": "A typical approach is to combine REINFORCE with policies based on deep sequence-to-sequence learning (, for example, in machine translation (, semantic parsing (, or summarization (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7394695580005646}, {"text": "semantic parsing", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.7046387493610382}, {"text": "summarization", "start_pos": 166, "end_pos": 179, "type": "TASK", "confidence": 0.9818466305732727}]}, {"text": "These RL approaches focus on improving performance in automatic evaluation by simulating reward signals by evaluation metrics such as BLEU, F1-score, or ROUGE, computed against gold standards.", "labels": [], "entities": [{"text": "RL", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9791496396064758}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9992035031318665}, {"text": "F1-score", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9963353872299194}, {"text": "ROUGE", "start_pos": 153, "end_pos": 158, "type": "METRIC", "confidence": 0.9943608641624451}]}, {"text": "Despite coming from different fields of application, RL in games and sequence-to-sequence learning share firstly the existence of a clearly specified reward function, e.g., defined by winning or losing a game, or by computing an automatic sequence-level evaluation metric.", "labels": [], "entities": [{"text": "RL", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9725815057754517}]}, {"text": "Secondly, both RL applications rely on a sufficient exploration of the action space, e.g., by evaluating multiple game moves for the same game state, or various sequence predictions for the same input.", "labels": [], "entities": [{"text": "RL", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9769616723060608}]}, {"text": "The goal of this paper is to advance the stateof-the-art of sequence-to-sequence RL, exemplified by bandit learning for neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 120, "end_pos": 152, "type": "TASK", "confidence": 0.838793029387792}]}, {"text": "Our aim is to show that successful learning from simulated bandit feedback () does in fact carryover to learning from actual human bandit feedback.", "labels": [], "entities": []}, {"text": "The promise of bandit NMT is that human feedback on the quality of translations is easier to obtain in large amounts than human references, thus compensating the weaker nature of the signals by their quantity.", "labels": [], "entities": []}, {"text": "However, the human factor entails several differences to the above sketched simulation scenarios of RL.", "labels": [], "entities": [{"text": "RL", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.8996940851211548}]}, {"text": "Firstly, human rewards are not well-defined functions, but complex and inconsistent signals.", "labels": [], "entities": []}, {"text": "For example, in general every input sentence has a multitude of correct translations, each of which humans may judge differ-ently, depending on many contextual and personal factors.", "labels": [], "entities": []}, {"text": "Secondly, exploration of the space of possible translations is restricted in real-world scenarios where a user judges one displayed translation, but cannot be expected to rate an alternative translation, let alone large amounts of alternatives.", "labels": [], "entities": []}, {"text": "In this paper we will show that despite the fact that human feedback is ambiguous and partial in nature, a catalyst for successful learning from human reinforcements is the reliability of the feedback signals.", "labels": [], "entities": []}, {"text": "The first deployment of bandit NMT in an e-commerce translation scenario conjectured lacking reliability of user judgments as the reason for disappointing results when learning from 148k user-generated 5-star ratings for around 70k product title translations ().", "labels": [], "entities": []}, {"text": "We thus raise the question of how human feedback can be gathered in the most reliable way, and what effect reliability will have in downstream tasks.", "labels": [], "entities": [{"text": "reliability", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.9653705954551697}]}, {"text": "In order to answer these questions, we measure intra-and inter-annotator agreement for two feedback tasks for bandit NMT, using cardinal feedback (on a 5-point scale) and ordinal feedback (by pairwise preferences) for 800 translations, conducted by 16 and 14 human raters, respectively.", "labels": [], "entities": []}, {"text": "Perhaps surprisingly, while relative feedback is often considered easier for humans to provide, our investigation shows that \u03b1-reliability for intra-and inter-rater agreement is similar for both tasks, with highest inter-rater reliability for standardized 5-point ratings.", "labels": [], "entities": []}, {"text": "Ina next step, we address the issue of machine learnability of human rewards.", "labels": [], "entities": []}, {"text": "We use deep learning models to train reward estimators by regression against cardinal feedback, and by fitting a Bradley-Terry model to ordinal feedback.", "labels": [], "entities": []}, {"text": "Learnability is understood by a slight misuse of the machine learning notion of learnability as the question how well reward estimates can approximate human rewards.", "labels": [], "entities": []}, {"text": "Our experiments reveal that rank correlation of reward estimates with TER against human references is higher for regression models trained on standardized cardinal rewards than for Bradley-Terry models trained on pairwise preferences.", "labels": [], "entities": [{"text": "TER", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9932761192321777}]}, {"text": "This emphasizes the influence of the reliability of human feedback signals on the quality of reward estimates learned from them.", "labels": [], "entities": []}, {"text": "Lastly, we investigate machine learnability of the overall NMT task, in the sense of who posed the question of how well an MT system can be tuned on post-edits.", "labels": [], "entities": [{"text": "NMT task", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.7773000597953796}, {"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.9606171250343323}]}, {"text": "We use an RL approach for tuning, where a crucial difference of our work to previous work on RL from human rewards () is that our RL scenario is not interactive, but rewards are collected in an offline log.", "labels": [], "entities": [{"text": "tuning", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.9607415199279785}]}, {"text": "RL then can proceed either by off-policy learning using logged single-shot human rewards directly, or by using estimated rewards.", "labels": [], "entities": [{"text": "RL", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9665091037750244}]}, {"text": "An expected advantage of estimating rewards is to tackle a simpler problem first -learning a reward estimator instead of a full RL task for improving NMT -and then to deploy unlimited feedback from the reward estimator for off-policy RL.", "labels": [], "entities": [{"text": "estimating rewards", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8944652080535889}]}, {"text": "Our results show that significant improvements can be achieved by training NMT from both estimated and logged human rewards, with best results for integrating a regression-based reward estimator into RL.", "labels": [], "entities": []}, {"text": "This completes the argumentation that high reliability influences quality of reward estimates, which in turn affects the quality of the overall NMT task.", "labels": [], "entities": []}, {"text": "Since the size of our training data is tiny in machine translation proportions, this result points towards a great potential for larger-scaler applications of RL from human feedback.", "labels": [], "entities": [{"text": "RL", "start_pos": 159, "end_pos": 161, "type": "TASK", "confidence": 0.9690078496932983}]}], "datasetContent": [{"text": "We use the WMT 2017 data 3 for training a general domain (here: out-of-domain) model for  Architecture.", "labels": [], "entities": [{"text": "WMT 2017 data 3", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9713401347398758}]}, {"text": "Our NMT model is a standard subword-based encoder-decoder architecture with attention ().", "labels": [], "entities": []}, {"text": "An encoder Recurrent Neural Network (RNN) reads in the source sentence and a decoder RNN generates the target sentence conditioned on the encoded source.", "labels": [], "entities": []}, {"text": "We implemented RL and OPL objectives in Neural Monkey (Helcl and Libovick\u00b4yLibovick\u00b4y, 2017).", "labels": [], "entities": [{"text": "RL", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.828201949596405}, {"text": "Neural Monkey", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8961750566959381}]}, {"text": "The NMT has a bidirectional encoder and a singlelayer decoder with 1,024 GRUs each, and subword embeddings of size 500 fora shared vocabulary of subwords obtained from 30k byte-pair merges.", "labels": [], "entities": [{"text": "NMT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9391394853591919}]}, {"text": "For model selection we use greedy decoding, for test set evaluation beam search with abeam of width 10.", "labels": [], "entities": [{"text": "model selection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7367260754108429}]}, {"text": "We sample k = 5 translations for RL models and set the softmax temperature \u03c4 = 0.5.", "labels": [], "entities": []}, {"text": "Further hyperparameters are given in the supplementary material.) to cover a diverse set of automatic measures for translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8286051154136658}]}, {"text": "We test for statistical significance with approximate randomization.", "labels": [], "entities": []}, {"text": "The out-of-domain model is trained with MLE on WMT.", "labels": [], "entities": [{"text": "MLE", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9887804985046387}, {"text": "WMT", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9144967198371887}]}, {"text": "The task is now to improve the generalization of this model to the TED domain.", "labels": [], "entities": []}, {"text": "Table 3 compares the out-of-domain baseline with domain-adapted models that were further trained on TED in a fully-supervised manner (supervised fine-tuning as introduced by;).", "labels": [], "entities": []}, {"text": "The supervised domain-adapted model serves as an upper bound for domain adaptation with human rewards: if we had references, we could improve up to 7 BLEU.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7104573994874954}, {"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9987208247184753}]}, {"text": "What if references are not available, but we can obtain rewards for sample translations?", "labels": [], "entities": []}, {"text": "Results for RL from Simulated Rewards.", "labels": [], "entities": [{"text": "RL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.8975880146026611}, {"text": "Simulated Rewards", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.825698584318161}]}, {"text": "First we simulate \"clean\" and deterministic rewards by comparing sample translations to references using GLEU (  for RL, and smoothed sBLEU for estimated rewards and OPL.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9966377019882202}, {"text": "OPL", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.8273200988769531}]}, {"text": "lists the results for this simulation experiment in rows 2-5 (S).", "labels": [], "entities": []}, {"text": "If unlimited clean feedback was given (RL with direct simulated rewards), improvements of over 5 BLEU can be achieved.", "labels": [], "entities": [{"text": "RL", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9429798126220703}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9993802309036255}]}, {"text": "When limiting the amount of feedback to a log of 800 translations, the improvements over the baseline are only marginal (OPL).", "labels": [], "entities": [{"text": "OPL", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9926353096961975}]}, {"text": "When replacing the direct reward by the simulated reward estimators from \u00a75, i.e. having unlimited amounts of approximately clean rewards, however, improvements of 1.2 BLEU for MSE estimators (RL+MSE) and 0.8 BLEU for pairwise estimators (RL+PW) are found.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9986497759819031}, {"text": "BLEU", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.997978150844574}]}, {"text": "This suggests that the reward estimation model helps to tackle the challenge of generalization over a small set of ratings.", "labels": [], "entities": [{"text": "reward estimation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6481970101594925}]}, {"text": "Results for RL from Human Rewards.", "labels": [], "entities": [{"text": "RL from Human Rewards", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.6903806179761887}]}, {"text": "Knowing what to expect in an ideal setting with nonnoisy feedback, we now move to the experiments with human feedback.", "labels": [], "entities": []}, {"text": "OPL is trained with the logged normalized, averaged and re-scaled human reward (see \u00a75).", "labels": [], "entities": []}, {"text": "RL is trained with the direct reward provided by the reward estimators trained on human rewards from \u00a75.", "labels": [], "entities": [{"text": "RL", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6372588276863098}]}, {"text": "shows the results for training with human rewards in rows 6-8: The improvements for OPL are very similar to OPL with simulated rewards, both suffering from overfitting.", "labels": [], "entities": []}, {"text": "For RL we observe that the MSEbased reward estimator (RL+MSE) leads to significantly higher improvements as a the pairwise reward estimator (RL+PW) -the same trend as for simulated ratings.", "labels": [], "entities": []}, {"text": "Finally, the improvement of 1.1 BLEU over the baseline showcases that we are able to improve NMT with only a small number of human rewards.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9867522716522217}, {"text": "NMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.8012319207191467}]}, {"text": "Learning from estimated filtered 5-point ratings, does not significantly improve over these results, since the improvement of the reward estimator is only marginal (see \u00a7 5).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-and intra-reliability measured by  Krippendorff's \u03b1 for 5-point and pairwise ratings  of 1,000 translations of which 200 translations are  repeated twice. The filtered variants are restricted  to either a subset of participants (part.) or a subset  of translations (trans.).", "labels": [], "entities": []}, {"text": " Table 3: Results on test data for in-and out-of- domain fully-supervised models. Both are trained  with MLE, the TED model is obtained by fine- tuning the WMT model in TED data.", "labels": [], "entities": [{"text": "MLE", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9810572862625122}]}, {"text": " Table 4: Results on TED test data for training with  estimated (E) and direct (D) rewards from simula- tion (S), humans (H) and filtered (F) human rat- ings. Significant (p \u2264 0.05) differences to the  baseline are marked with . For RL experiments  we show three runs with different random seeds,  mean and standard deviation in subscript.", "labels": [], "entities": [{"text": "TED test data", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.7886493802070618}]}]}