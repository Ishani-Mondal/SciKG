{"title": [{"text": "Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora", "labels": [], "entities": [{"text": "Automatic Hypernym Detection", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.5281419456005096}]}], "abstractContent": [{"text": "Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms: pattern-based and distributional methods.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7847656607627869}]}, {"text": "In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets.", "labels": [], "entities": []}, {"text": "Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hierarchical relationships play a central role in knowledge representation and reasoning.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.7268677353858948}]}, {"text": "Hypernym detection, i.e., the modeling of word-level hierarchies, has long been an important task in natural language processing., pattern-based methods have been one of the most influential approaches to this problem.", "labels": [], "entities": [{"text": "Hypernym detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8333538174629211}]}, {"text": "Their key idea is to exploit certain lexico-syntactic patterns to detect is-a relations in text.", "labels": [], "entities": []}, {"text": "For instance, patterns like \"NP y such as NP x \", or \"NP x and other NP y \" often indicate hypernymy relations of the form x is-a y.", "labels": [], "entities": []}, {"text": "Such patterns maybe predefined, or they maybe learned automatically (.", "labels": [], "entities": []}, {"text": "However, a well-known problem of Hearst-like patterns is their extreme sparsity: words must co-occur in exactly the right configuration, or else no relation can be detected.", "labels": [], "entities": []}, {"text": "To alleviate the sparsity issue, the focus in hypernymy detection has recently shifted to distributional representations, wherein words are represented as vectors based on their distribution across large corpora.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8719965517520905}]}, {"text": "Such methods offer rich representations of lexical meaning, alleviating the sparsity problem, but require specialized similarity measures to distinguish different lexical relationships.", "labels": [], "entities": []}, {"text": "The most successful measures to date are generally inspired by the Distributional Inclusion Hypothesis (DIH)), which states roughly that contexts in which a narrow term x may appear (\"cat\") should be a subset of the contexts in which a broader term y (\"animal\") may appear.", "labels": [], "entities": []}, {"text": "Intuitively, the DIH states that we should be able to replace any occurrence of \"cat\" with \"animal\" and still have a valid utterance.", "labels": [], "entities": [{"text": "DIH", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.9264456629753113}]}, {"text": "An important insight from work on distributional methods is that the definition of context is often critical to the success of a system.", "labels": [], "entities": []}, {"text": "Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (.", "labels": [], "entities": []}, {"text": "While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8747681379318237}]}, {"text": "Here, we revisit the idea of using pattern-based methods for hypernym detection.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8681986331939697}]}, {"text": "We evaluate several pattern-based models on modern, large corpora and compare them to methods based on the DIH.", "labels": [], "entities": [{"text": "DIH", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9271230101585388}]}, {"text": "We find that simple pattern-based methods consistently outperform specialized DIH methods on several difficult hypernymy tasks, including detection, direction prediction, and graded entailment ranking.", "labels": [], "entities": [{"text": "direction prediction", "start_pos": 149, "end_pos": 169, "type": "TASK", "confidence": 0.9136503338813782}]}, {"text": "Moreover, we find that taking low-rank embeddings of pattern-based models substantially improves performance by remedying the sparsity issue.", "labels": [], "entities": []}, {"text": "Overall, our results show that Hearst patterns provide high-quality and robust predictions on large corpora by capturing important contextual constraints, which are not yet modeled in distributional methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "Pattern-based models: We extract Hearst patterns from the concatenation of Gigaword and Wikipedia, and prepare our corpus by tokenizing, lemmatizing, and POS tagging using CoreNLP 3.8.0.", "labels": [], "entities": []}, {"text": "The full set of Hearst patterns is provided in.", "labels": [], "entities": []}, {"text": "Our selected patterns match prototypical Hearst patterns, like \"animals such as cats,\" but also include broader patterns like \"New Year is the most important holiday.\"", "labels": [], "entities": []}, {"text": "Leading and following noun phrases are allowed to match limited modifiers (compound nouns, adjectives, etc.), in which case we also generate a hit for the head of the noun phrase.", "labels": [], "entities": []}, {"text": "During postprocessing, we remove pairs which were not extracted by at least two distinct patterns.", "labels": [], "entities": []}, {"text": "We also remove any pair (y, x) if p(y, x) < p(x, y).", "labels": [], "entities": []}, {"text": "The final corpus contains roughly 4.5M matched pairs, 431K unique pairs, and 243K unique terms.", "labels": [], "entities": []}, {"text": "For SVD-based models, we select the rank from r \u2208, 500, 1000} on the validation set.", "labels": [], "entities": []}, {"text": "The other pattern-based models do not have any hyperparameters.", "labels": [], "entities": []}, {"text": "Distributional models: For the distributional baselines, we employ the large, sparse distributional space of, which is computed from UkWaC and Wikipedia, and is known to have strong performance on several of the detection tasks.", "labels": [], "entities": [{"text": "UkWaC", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.9568979144096375}]}, {"text": "The corpus was POS tagged and dependency parsed.", "labels": [], "entities": []}, {"text": "Distributional contexts were constructed from adjacent words in dependency parses).", "labels": [], "entities": []}, {"text": "Targets and contexts which appeared fewer than 100 times in the corpus were filtered, and the resulting co-occurrence matrix was PPMI transformed.", "labels": [], "entities": []}, {"text": "The resulting space contains representations for 218K words over 732K context dimensions.", "labels": [], "entities": []}, {"text": "For the SLQS model, we selected the number of contexts N from the same set of options as the SVD rank in pattern-based models.", "labels": [], "entities": []}, {"text": "shows the results from all three experimental settings.", "labels": [], "entities": []}, {"text": "In nearly all cases, we find that patternbased approaches substantially outperform all three distributional models.", "labels": [], "entities": []}, {"text": "Particularly strong improvements can be observed on BLESS (0.76 average precision vs 0.19) and WBLESS (0.96 vs. 0.69) for the detection tasks and on all directionality tasks.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9983301758766174}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.6406487822532654}, {"text": "WBLESS", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.7423887848854065}]}, {"text": "For directionality prediction on BLESS, the SVD models surpass even the state-of-the-art supervised model of Vuli\u00b4c.", "labels": [], "entities": [{"text": "directionality prediction", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9535514116287231}, {"text": "BLESS", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.6926620006561279}]}, {"text": "Moreover, both SVD models perform generally better than their sparse counterparts on all tasks and datasets except on HYPERLEX.", "labels": [], "entities": []}, {"text": "We performed a posthoc analysis of the validation sets comparing the ppmi and spmi models, and found that the truncated SVD improved recall via its matrix completion properties.", "labels": [], "entities": [{"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9985917210578918}]}, {"text": "We also found that the spmi model downweighted: Experimental results comparing distributional and pattern-based methods in all settings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results comparing distributional and pattern-based methods in all settings.", "labels": [], "entities": []}]}