{"title": [{"text": "Embedding Learning Through Multilingual Concept Induction", "labels": [], "entities": [{"text": "Multilingual Concept Induction", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.6513308982054392}]}], "abstractContent": [{"text": "We present anew method for estimating vector space representations of words: embedding learning by concept induction.", "labels": [], "entities": []}, {"text": "We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space.", "labels": [], "entities": []}, {"text": "An extensive experimental evaluation on crosslin-gual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.", "labels": [], "entities": [{"text": "crosslin-gual word similarity", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6969202856222788}, {"text": "sentiment analysis", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.9368532299995422}]}], "introductionContent": [{"text": "Vector space representations of words are widely used because they improve performance on monolingual tasks.", "labels": [], "entities": []}, {"text": "This success has generated interest in multilingual embeddings, shared representation of words across languages (.", "labels": [], "entities": []}, {"text": "Such embeddings can be beneficial in machine translation in sparse data settings because multilingual embeddings provide meaning representations of source and target in the same space.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7893945574760437}]}, {"text": "Similarly, in transfer learning, models trained in one language on multilingual embeddings can be deployed in other languages).", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.9215013682842255}]}, {"text": "Automatically learned embeddings have the added advantage of requiring fewer resources for training ().", "labels": [], "entities": []}, {"text": "Thus, massively multilingual word embeddings (i.e., covering 100s or 1000s of languages) are likely to be important in NLP.", "labels": [], "entities": []}, {"text": "The basic information many embedding learners use is word-context information; e.g., the embedding of a word is optimized to predict a representation of its context.", "labels": [], "entities": []}, {"text": "We instead learn em- beddings from word-concept information.", "labels": [], "entities": []}, {"text": "As a first approximation, a concept is a set of semantically similar words.", "labels": [], "entities": []}, {"text": "shows an example concept and also indicates one way we learn concepts: we interpret cliques in the dictionary graph as concepts.", "labels": [], "entities": []}, {"text": "The nodes of the dictionary graph are words, its edges connect words that are translations of each other.", "labels": [], "entities": []}, {"text": "A dictionary node has the form prefix:word, e.g., \"tpi:wara\" (upper left node in the.", "labels": [], "entities": []}, {"text": "The prefix is the ISO 639-3 code of the language; tpi is Tok Pisin.", "labels": [], "entities": []}, {"text": "Our method takes a parallel corpus as input and induces a dictionary graph from the parallel corpus.", "labels": [], "entities": []}, {"text": "Concepts and word-concept pairs are then induced from the dictionary graph.", "labels": [], "entities": []}, {"text": "Finally, embeddings are learned from word-concept pairs.", "labels": [], "entities": []}, {"text": "A key application of multilingual embeddings is transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.9606408476829529}]}, {"text": "Transfer learning is mainly of interest if the target is resource-poor.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9718513488769531}]}, {"text": "We therefore select as our dataset 1664 translations in 1259 languages of the New Testament from PBC, the Parallel Bible Corpus.", "labels": [], "entities": [{"text": "PBC", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9083716869354248}, {"text": "Parallel Bible Corpus", "start_pos": 106, "end_pos": 127, "type": "DATASET", "confidence": 0.7095974385738373}]}, {"text": "Since \"translation\" is an ambiguous word, we will from now on refer to the 1664 translations as \"editions\".", "labels": [], "entities": []}, {"text": "PBC is aligned German Elberfelder 1905 Spanish Americas And he said , Do it the second time . And they did it the second time . .", "labels": [], "entities": [{"text": "PBC is aligned German Elberfelder 1905 Spanish Americas", "start_pos": 0, "end_pos": 55, "type": "DATASET", "confidence": 0.8445813804864883}]}, {"text": "Und er sprach : F\u00fcllet vier Eimer mit Wasser , und gie\u00dfet es auf das Brandopfer und auf das Holz . Und er sprach : Tut es zum zweiten Male ! Und sie taten es zum zweiten Male . .", "labels": [], "entities": []}, {"text": "Y dijo : Llenad cuatro c\u00e1ntaros de agua y derramadla sobre el holocausto y sobre la le\u00f1a . Despu\u00e9s dijo : Hacedlo por segunda vez ; y lo hicieron por segunda vez . .", "labels": [], "entities": []}, {"text": ".: Instances of verse 11018034.", "labels": [], "entities": []}, {"text": "This multi-sentence verse is an example of verse misalignment.", "labels": [], "entities": [{"text": "verse misalignment", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7013438045978546}]}, {"text": "on the verse level; most verses consist of a single sentence, but some contain several (see).", "labels": [], "entities": []}, {"text": "PBC is a good model for resource-poverty; e.g., the training set (see below) of KJV contains fewer than 150,000 tokens in 6458 verses.", "labels": [], "entities": []}, {"text": "We evaluate multilingual embeddings on two tasks, roundtrip translation (RT) and sentiment analysis.", "labels": [], "entities": [{"text": "roundtrip translation (RT)", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.8264824211597442}, {"text": "sentiment analysis", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.9572713375091553}]}, {"text": "RT on the word level is -to our knowledge -a novel evaluation method: a query word w of language L 1 is translated to its closest (with respect to embedding similarity) neighbor v in L 2 and then backtranslated to its closest neighbor win L 1 . RT is successful if w = w . There are well-known concerns about RT when it is used in the context of machine translation.", "labels": [], "entities": [{"text": "RT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9267158508300781}, {"text": "RT", "start_pos": 309, "end_pos": 311, "type": "TASK", "confidence": 0.9610306620597839}, {"text": "machine translation", "start_pos": 346, "end_pos": 365, "type": "TASK", "confidence": 0.7670774757862091}]}, {"text": "A successful roundtrip translation does not necessarily imply that v is of high quality and it is not possible to decide whether an error occurred in the forward or backward translations.", "labels": [], "entities": [{"text": "roundtrip translation", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.557624489068985}]}, {"text": "Despite these concerns about RT on the sentence level, we show that RT on the word level is a difficult task and an effective measure of embedding quality.", "labels": [], "entities": []}, {"text": "(i) We introduce anew embedding learning method, multilingual embedding learning through concept induction.", "labels": [], "entities": []}, {"text": "(ii) We show that this new concept-based method outperforms previous approaches to multilingual embeddings.", "labels": [], "entities": []}, {"text": "(iii) We propose both word-level and characterlevel dictionary induction methods and present evidence that concepts induced from word-level dictionaries are better for easily tokenizable languages and concepts induced from character-level dictionaries are better for difficult-to-tokenize languages.", "labels": [], "entities": [{"text": "characterlevel dictionary induction", "start_pos": 37, "end_pos": 72, "type": "TASK", "confidence": 0.6292928258577982}]}, {"text": "(iv) We evaluate our methods on a corpus of 1664 editions in 1259 languages.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first detailed evaluation, involving challenging tasks like word translation and crosslingual sentiment analysis, that has been done on such a large number of languages.", "labels": [], "entities": [{"text": "word translation", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7785834074020386}, {"text": "crosslingual sentiment analysis", "start_pos": 123, "end_pos": 154, "type": "TASK", "confidence": 0.8132354617118835}]}], "datasetContent": [{"text": "For sentiment analysis, we represent averse as the IDF-weighted sum of its embeddings.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9608399868011475}]}, {"text": "Sentiment classifiers (linear SVMs) are trained on the training set of the World English Bible edition for the two decision problems positive vs. nonpositive and negative vs. non-negative.", "labels": [], "entities": [{"text": "World English Bible edition", "start_pos": 75, "end_pos": 102, "type": "DATASET", "confidence": 0.9163050949573517}]}, {"text": "We create a silver standard by labeling verses in English editions with the NLTK () sentiment classifier.", "labels": [], "entities": [{"text": "NLTK () sentiment classifier", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.6472597494721413}]}, {"text": "A positive vs. negative classification is not reasonable for the New Testament because a large number of verses is mixed, e.g., \"Now is come salvation . .", "labels": [], "entities": []}, {"text": "the power of his Christ: for the accuser . .", "labels": [], "entities": []}, {"text": "cast down, which accused them before our God . .", "labels": [], "entities": []}, {"text": "\" Note that this verse also cannot be said to be neutral.", "labels": [], "entities": []}, {"text": "Splitting the sentiment analysis into two subtasks (\"contains positive sentiment: yes/no\" and \"contains negative sentiment: yes/no\") is an effective solution for this paper.", "labels": [], "entities": []}, {"text": "The two trained models are then applied to the test set of all 1664 editions.", "labels": [], "entities": []}, {"text": "All embeddings in this paper are learned on the training set only.", "labels": [], "entities": []}, {"text": "So no test information was used for learning the embeddings.", "labels": [], "entities": []}, {"text": "There are no gold standards for the genre of our corpus (the New Testament); for only a few languages out-of-domain gold standards are available.", "labels": [], "entities": []}, {"text": "Roundtrip evaluation is an evaluation method for multilingual embeddings that can be applied if no resources are available fora language.", "labels": [], "entities": [{"text": "Roundtrip evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7188253998756409}]}, {"text": "Loosely speaking, fora query q in a query language L q (in our case English) and a target language Lt , roundtrip translation finds the unit wt in Lt that is closest to q and then the English unit we that is closest tow t . If the semantics of q and we are identical (resp. are unrelated), this is deemed evidence for (resp.", "labels": [], "entities": []}, {"text": "counterevidence against) the quality of the embeddings.", "labels": [], "entities": []}, {"text": "We work on the level of Bible edition, i.e., two editions in the same language are considered different \"languages\".", "labels": [], "entities": []}, {"text": "For a query q, we denote the set of its k I nearest neighbors in the target edition e by I e (q) = {u 1 , u 2 , . .", "labels": [], "entities": []}, {"text": ", u k I }.", "labels": [], "entities": []}, {"text": "For each intermediate entry we then consider its k T nearest neighbors in English.", "labels": [], "entities": []}, {"text": "Overall we get a set Te (q) with k I k T predictions for each intermediate Bible edition e.", "labels": [], "entities": []}, {"text": "We evaluate the predictions Te (q) using two sets G s (q) (strict) and Gr (q) (relaxed) of ground-truth semantic equivalences in English.", "labels": [], "entities": []}, {"text": "Precision fora query q is defined asp i (q) := 1/|E| e\u2208E min{1, |T e (q) \u2229 G i (q)|} where E is the set of all Bible editions and i \u2208 {s, r}.", "labels": [], "entities": []}, {"text": "We report the mean and median across a interquery mediate predictions woman \u21d2 mujer \u21d2 wife woman women widows daughters daughter marry married \u21d2 esposa \u21d2 marry wife woman married marriage virgin daughters bridegroom  set of 70 queries selected from Swadesh (1946)'s list of 100 universal linguistic concepts.", "labels": [], "entities": []}, {"text": "We create G sand Gr as follows.", "labels": [], "entities": []}, {"text": "For WORD, we define G s (q) = {q} and Gr (q) = L(q) where L(q) is the set of words with the same lemma and POS as q.", "labels": [], "entities": [{"text": "POS", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9628757238388062}]}, {"text": "For CHAR, we need to find ngrams that correspond uniquely to the query q.", "labels": [], "entities": [{"text": "CHAR", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6330817341804504}]}, {"text": "Given a candidate ngram g we consider c qg := 1/c(g) q \u2208L(q),substring(g,q ) c(q ) where c(x) is the count of character sequence x across all editions in the query language.", "labels": [], "entities": []}, {"text": "We add g to G i (q) if c qg > \u03c3 i where \u03c3 s = .75 and \u03c3 r = .5.", "labels": [], "entities": []}, {"text": "We only consider queries where G s (q) is non-empty.", "labels": [], "entities": []}, {"text": "We vary the evaluation parameters (i, k I , k T ) as follows: \"S1\" represents (s, 1, 1), \"S4\" (s, 2, 2), \"S16\" (s, 2, 8), and \"R1\" (r, 1, 1).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Our ten pivot languages, the languages in  PBC with the lowest number of types. Tokens in  1000s. Tok Pisin and Bislama are English-based  and Sango is a Ngbandi-based creole. PNG =  Papua New Guinea", "labels": [], "entities": []}, {"text": " Table 3: Roundtrip translation (mean/median accuracy) and sentiment analysis (F 1 ) results for word- based (WORD) and character-based (CHAR) multilingual embeddings. N (coverage): # queries con- tained in the embedding space. The best result across WORD and CHAR is set in bold.", "labels": [], "entities": [{"text": "Roundtrip translation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7733190357685089}, {"text": "mean/median accuracy)", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.7206467747688293}, {"text": "F 1 )", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9340153137842814}]}]}