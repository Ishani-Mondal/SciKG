{"title": [{"text": "Token-level and sequence-level loss smoothing for RNN language models", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite the effectiveness of recurrent neu-ral network language models, their maximum likelihood estimation suffers from two limitations.", "labels": [], "entities": []}, {"text": "It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space.", "labels": [], "entities": []}, {"text": "Second, it suffers from \"ex-posure bias\": during training tokens are predicted given ground-truth sequences, while attest time prediction is conditioned on generated output sequences.", "labels": [], "entities": []}, {"text": "To overcome these limitations we build upon the recent reward augmented maximum likelihood approach i.e. sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric.", "labels": [], "entities": []}, {"text": "We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach.", "labels": [], "entities": [{"text": "token-level loss smoothing", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.8138302564620972}]}, {"text": "Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7809244990348816}, {"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7974120676517487}]}], "introductionContent": [{"text": "Recurrent neural networks (RNNs) have recently proven to be very effective sequence modeling tools, and are now state of the art for tasks such as machine translation ( , image captioning () and automatic speech recognition (.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7208282053470612}, {"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.8206061124801636}, {"text": "image captioning", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.7227450162172318}, {"text": "automatic speech recognition", "start_pos": 195, "end_pos": 223, "type": "TASK", "confidence": 0.596240371465683}]}, {"text": "The basic principle of RNNs is to iteratively compute a vectorial sequence representation, by applying at each time-step the same trainable function to compute the new network state from the previous state and the last symbol in the sequence.", "labels": [], "entities": []}, {"text": "These models are typically trained by maximizing the likelihood of the target sentence given an encoded source (text, image, speech).", "labels": [], "entities": []}, {"text": "Maximum likelihood estimation (MLE), however, has two main limitations.", "labels": [], "entities": [{"text": "Maximum likelihood estimation (MLE)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6175643901030222}]}, {"text": "First, the training signal only differentiates the ground-truth target output from all other outputs.", "labels": [], "entities": []}, {"text": "It treats all other output sequences as equally incorrect, regardless of their semantic proximity from the ground-truth target.", "labels": [], "entities": []}, {"text": "While such a \"zero-one\" loss is probably acceptable for coarse grained classification of images, e.g. across a limited number of basic object categories) it becomes problematic as the output space becomes larger and some of its elements become semantically similar to each other.", "labels": [], "entities": []}, {"text": "This is in particular the case for tasks that involve natural language generation (captioning, translation, speech recognition) where the number of possible outputs is practically unbounded.", "labels": [], "entities": [{"text": "natural language generation (captioning, translation", "start_pos": 54, "end_pos": 106, "type": "TASK", "confidence": 0.7119549896035876}, {"text": "speech recognition)", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.770749161640803}]}, {"text": "For natural language generation tasks, evaluation measures typically do take into account structural similarity, e.g. based on n-grams, but such structural information is not reflected in the MLE criterion.", "labels": [], "entities": [{"text": "natural language generation tasks", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7316761240363121}]}, {"text": "The second limitation of MLE is that training is based on predicting the next token given the input and preceding ground-truth output tokens, while attest time the model predicts conditioned on the input and the so-far generated output sequence.", "labels": [], "entities": [{"text": "MLE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9471791386604309}]}, {"text": "Given the exponentially large output space of natural language sentences, it is not obvious that the learned RNNs generalize well beyond the relatively sparse distribution of ground-truth sequences used during MLE optimization.", "labels": [], "entities": [{"text": "MLE optimization", "start_pos": 210, "end_pos": 226, "type": "TASK", "confidence": 0.9785200655460358}]}, {"text": "This phenomenon is known as \"exposure bias\".", "labels": [], "entities": [{"text": "exposure bias", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.853448212146759}]}, {"text": "MLE minimizes the KL divergence between a target Dirac distribution on the ground-truth sentence(s) and the model's distribution.", "labels": [], "entities": []}, {"text": "In this pa-per, we build upon the \"loss smoothing\" approach by, which smooths the Dirac target distribution over similar sentences, increasing the support of the training data in the output space.", "labels": [], "entities": []}, {"text": "We make the following main contributions: \u2022 We propose a token-level loss smoothing approach, using word-embeddings, to achieve smoothing among semantically similar terms, and we introduce a special procedure to promote rare tokens.", "labels": [], "entities": [{"text": "token-level loss smoothing", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6924152175585429}]}, {"text": "\u2022 For sequence-level smoothing, we propose to use restricted token replacement vocabularies, and a \"lazy evaluation\" method that significantly speeds up training.", "labels": [], "entities": [{"text": "sequence-level smoothing", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.7478917241096497}]}, {"text": "\u2022 We experimentally validate our approach on the MSCOCO image captioning task and the WMT'14 English to French machine translation task, showing that on both tasks combining token-level and sequence-level loss smoothing improves results significantly over maximum likelihood baselines.", "labels": [], "entities": [{"text": "MSCOCO image captioning task", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.761875793337822}, {"text": "WMT'14 English to French machine translation task", "start_pos": 86, "end_pos": 135, "type": "TASK", "confidence": 0.7855324745178223}]}, {"text": "In the remainder of the paper, we review the existing methods to improve RNN training in Section 2.", "labels": [], "entities": [{"text": "RNN training", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.8828820586204529}]}, {"text": "Then, we present our token-level and sequence-level approaches in Section 3.", "labels": [], "entities": []}, {"text": "Experimental evaluation results based on image captioning and machine translation tasks are laid out in Section 4.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.768642246723175}, {"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7462649643421173}]}], "datasetContent": [{"text": "In this section, we compare sequence prediction models trained with maximum likelihood (MLE) with our token and sequence-level loss smoothing on two different tasks: image captioning and machine translation.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7660419940948486}, {"text": "sequence-level loss smoothing", "start_pos": 112, "end_pos": 141, "type": "TASK", "confidence": 0.5936970313390096}, {"text": "image captioning", "start_pos": 166, "end_pos": 182, "type": "TASK", "confidence": 0.7453688383102417}, {"text": "machine translation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7991407215595245}]}], "tableCaptions": [{"text": " Table 1: MS-COCO 's test set evaluation measures.", "labels": [], "entities": []}, {"text": " Table 2: MS-COCO 's server evaluation . ( + ) for ensemble submissions, (  \u2020 ) for submissions with CIDEr  optimization and ( \u2022 ) for models using additional data.", "labels": [], "entities": []}, {"text": " Table 3: Tokenized BLEU score on WMT'14  En-Fr evaluated on the news-test-2014 set. And  Tokenzied, case-insensitive BLEU on IWSLT'14  De-En.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.951263427734375}, {"text": "WMT'14  En-Fr", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.7301174402236938}, {"text": "news-test-2014 set", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.9433891475200653}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9691811800003052}, {"text": "IWSLT'14  De-En", "start_pos": 126, "end_pos": 141, "type": "DATASET", "confidence": 0.920723557472229}]}]}