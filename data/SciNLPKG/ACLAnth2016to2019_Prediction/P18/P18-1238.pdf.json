{"title": [{"text": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7224470376968384}, {"text": "Automatic Image Captioning", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.6258466641108195}]}], "abstractContent": [{"text": "We present anew dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles.", "labels": [], "entities": [{"text": "image caption annotations", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.7402589619159698}, {"text": "Conceptual Captions", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.672336995601654}, {"text": "MS-COCO dataset", "start_pos": 133, "end_pos": 148, "type": "DATASET", "confidence": 0.9576772749423981}]}, {"text": "We achieve this by extracting and filtering image caption annotations from billions of webpages.", "labels": [], "entities": [{"text": "filtering image caption annotations from billions of webpages", "start_pos": 34, "end_pos": 95, "type": "TASK", "confidence": 0.7830283455550671}]}, {"text": "We also present quantitative evaluations of a number of image cap-tioning models and show that a model architecture based on Inception-ResNet-v2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.", "labels": [], "entities": [{"text": "image-feature extraction", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.7657372355461121}, {"text": "Conceptual Captions", "start_pos": 308, "end_pos": 327, "type": "TASK", "confidence": 0.6781817674636841}]}], "introductionContent": [{"text": "Automatic image description is the task of producing a natural-language utterance (usually a sentence) which correctly reflects the visual content of an image.", "labels": [], "entities": [{"text": "Automatic image description is the task of producing a natural-language utterance (usually a sentence) which correctly reflects the visual content of an image", "start_pos": 0, "end_pos": 158, "type": "Description", "confidence": 0.7810671436786651}]}, {"text": "This task has seen an explosion in proposed solutions based on deep learning architectures, starting with the winners of the 2015 COCO challenge (, and continuing with a variety of improvements (see e.g. fora review).", "labels": [], "entities": []}, {"text": "Practical applications of automatic image description systems include leveraging descriptions for image indexing or retrieval, and helping those with visual impairments by transforming visual signals into information that can be communicated via text-to-speech technology.", "labels": [], "entities": [{"text": "automatic image description", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6748343408107758}, {"text": "image indexing or retrieval", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.795946791768074}]}, {"text": "The scientific challenge is seen as aligning, exploiting, and pushing further the latest improvements at the intersection of Computer Vision and Natural Language Processing.", "labels": [], "entities": []}, {"text": "Conceptual Captions: pop artist performs at the festival in a city.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7148780226707458}]}, {"text": "Figure 1: Examples of images and image descriptions from the Conceptual Captions dataset; we start from existing alt-text descriptions, and automatically process them into Conceptual Captions with a balance of cleanliness, informativeness, fluency, and learnability.", "labels": [], "entities": [{"text": "Conceptual Captions dataset", "start_pos": 61, "end_pos": 88, "type": "DATASET", "confidence": 0.6927609741687775}]}, {"text": "There are two main categories of advances responsible for increased interest in this task.", "labels": [], "entities": []}, {"text": "The first is the availability of large amounts of annotated data.", "labels": [], "entities": []}, {"text": "Relevant datasets include the ImageNet dataset, with over 14 million images and 1 million bounding-box annotations, and the MS-COCO dataset (, with 120,000 images and 5-way image-caption annotations.", "labels": [], "entities": [{"text": "ImageNet dataset", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.9709093272686005}, {"text": "MS-COCO dataset", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.972930908203125}]}, {"text": "The second is the availability of powerful modeling mechanisms such as modern Convolutional Neural Networks (e.g.), which are capable of converting image pixels into high-level features with no manual featureengineering.", "labels": [], "entities": []}, {"text": "In this paper, we make contributions to both the data and modeling categories.", "labels": [], "entities": []}, {"text": "First, we present anew dataset of caption annotations * , Conceptual Captions (, which has an order of magnitude more images than the COCO dataset.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.633687824010849}, {"text": "COCO dataset", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.9695081114768982}]}, {"text": "Conceptual Captions consists of about 3.3M image, description pairs.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6704079508781433}]}, {"text": "In contrast with the curated style of the COCO images, Conceptual Captions images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.6933826953172684}]}, {"text": "The raw descriptions are harvested from the Alt-text HTML attribute \u2020 associated with web images.", "labels": [], "entities": []}, {"text": "We developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.", "labels": [], "entities": []}, {"text": "As a contribution to the modeling category, we evaluate several image-captioning models.", "labels": [], "entities": []}, {"text": "Based on the findings of, we use Inception-ResNet-v2 () for image-feature extraction, which confers optimization benefits via residual connections and computationally efficient Inception units.", "labels": [], "entities": [{"text": "image-feature extraction", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7642426788806915}]}, {"text": "For caption generation, we use both RNN-based (Hochreiter and Schmidhuber, 1997) and Transformerbased () models.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9833104014396667}]}, {"text": "Our results indicate that Transformer-based models achieve higher output accuracy; combined with the reports of regarding the reduced number of parameters and FLOPs required for training & serving (compared with RNNs), models such as T2T8x8 (Section 4) push forward the performance on image-captioning and deserve further attention.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9812285304069519}, {"text": "FLOPs", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.9984096884727478}]}], "datasetContent": [{"text": "The Conceptual Captions dataset is programmatically created using a Flume ( pipeline.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6615132838487625}, {"text": "Flume", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.6747549772262573}]}, {"text": "This pipeline processes billions of Internet webpages in parallel.", "labels": [], "entities": []}, {"text": "From these webpages, it extracts, filters, and processes candidate image, caption pairs.", "labels": [], "entities": []}, {"text": "The filtering and processing steps are described in detail in the following sections.", "labels": [], "entities": []}, {"text": "Image-based Filtering The first filtering stage, image-based filtering, discards images based on encoding format, size, aspect ratio, and offensive content.", "labels": [], "entities": [{"text": "image-based filtering", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7103897929191589}]}, {"text": "It only keeps JPEG images where both dimensions are greater than 400 pixels, and the ratio of larger to smaller dimension is no more than 2.", "labels": [], "entities": []}, {"text": "It excludes images that trigger pornography or profanity detectors.", "labels": [], "entities": []}, {"text": "These filters discard more than 65% of the candidates.", "labels": [], "entities": []}, {"text": "We analyze candidate Alt-text using the Google Cloud Natural Language APIs, specifically partof-speech (POS), sentiment/polarity, and pornography/profanity annotations.", "labels": [], "entities": []}, {"text": "On top of these annotations, we have the following heuristics: \u2022 a well-formed caption should have a high unique word ratio covering various POS tags; candidates with no determiner, no noun, or no preposition are discarded; candidates with a high noun ratio are also discarded; \u2022 candidates with a high rate of token repetition are discarded; \u2022 capitalization is a good indicator of wellcomposed sentences; candidates where the first word is not capitalized, or with too high capitalized-word ratio are discarded; \u2022 highly unlikely tokens area good indicator of not desirable text; we use a vocabulary V W of 1B token types, appearing at least 5 times in the English Wikipedia, and discard candidates that contain tokens that are not found in this vocabulary.", "labels": [], "entities": []}, {"text": "\u2022 candidates that score too high or too low on the polarity annotations, or trigger the pornography/profanity detectors, are discarded; \u2022 predefined boiler-plate prefix/suffix sequences matching the text are cropped, e.g. \"click to enlarge picture\", \"stock photo\"; we also drop text which begins/ends in certain patterns, e.g. \"embedded image permalink\", \"profile photo\".", "labels": [], "entities": []}, {"text": "These filters only allow around 3% of the incoming candidates to pass to the later stages.", "labels": [], "entities": []}, {"text": "Image&Text-based Filtering In addition to the separate filtering based on image and text content, we filter out candidates for which none of the text tokens can be mapped to the content of the image.", "labels": [], "entities": []}, {"text": "To this end, we use classifiers available via the Google Cloud Vision APIs to assign class labels to images, using an image classifier with a large number of labels (order of magnitude of 10 5 ).", "labels": [], "entities": []}, {"text": "Notably, these labels are also 100% covered by the V w token types.", "labels": [], "entities": []}, {"text": "Images are generally assigned between 5 to 20 labels, though the exact number depends on the  In this section, we evaluate the impact of using the Conceptual Captions dataset (referred to as 'Conceptual' in what follows) for training image captioning models.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.6920476108789444}, {"text": "image captioning", "start_pos": 234, "end_pos": 250, "type": "TASK", "confidence": 0.7627923786640167}]}, {"text": "To this end, we train the models described in Section 4 under two experimental conditions: using the training & development sets provided by the COCO dataset (), versus training & development sets using the Conceptual dataset.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.9673156142234802}, {"text": "Conceptual dataset", "start_pos": 207, "end_pos": 225, "type": "DATASET", "confidence": 0.6954022794961929}]}, {"text": "We quantitatively evaluate the resulting models using three different test sets: the blind COCO-C40 test set (indomain for COCO-trained models, out-of-domain for Conceptual-trained models); the Conceptual test set (out-of-domain for COCO-trained models, in-domain for Conceptual-trained models); and the Flickr () 1K test set (outof-domain for both COCO-trained models and Conceptual-trained models).", "labels": [], "entities": [{"text": "COCO-C40 test set", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.8639004230499268}, {"text": "Flickr () 1K test set", "start_pos": 304, "end_pos": 325, "type": "DATASET", "confidence": 0.6868930995464325}]}, {"text": "COCO Image Captions The COCO image captioning dataset is normally divided into 82K images for training, and 40K images for validation.", "labels": [], "entities": [{"text": "COCO Image Captions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8615426222483317}, {"text": "COCO image captioning dataset", "start_pos": 24, "end_pos": 53, "type": "DATASET", "confidence": 0.7727200388908386}]}, {"text": "Each of these images comes with at least 5 groundtruth captions.", "labels": [], "entities": []}, {"text": "Following standard practice, we combine the training set with most of the validation dataset for training our model, and only holdout a subset of 4K images for validation.", "labels": [], "entities": []}, {"text": "Conceptual Captions The Conceptual Captions dataset contains around 3.3M images for training, 28K for validation and 22.5K for the test set.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.699791356921196}]}, {"text": "For more detailed statistics, see.", "labels": [], "entities": []}, {"text": "Image Preprocessing Each input image is first preprocessed by random distortion and cropping (using a random ratio from 50%\u223c100%).", "labels": [], "entities": [{"text": "Image Preprocessing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6765119880437851}]}, {"text": "This prevents models from overfitting individual pixels of the training images.", "labels": [], "entities": []}, {"text": "Encoder-Decoder For RNN-based models, we use a 1-layer, 512-dim LSTM as the RNN cell.", "labels": [], "entities": []}, {"text": "For the Transformer-based models, we use the default setup from (, with N = 6 encoder and decoder layers, a hidden-layer size of 512, and 8 attention heads.", "labels": [], "entities": []}, {"text": "Text Handling Training captions are truncated to maximum 15 tokens.", "labels": [], "entities": [{"text": "Text Handling Training captions", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5818449705839157}]}, {"text": "We use a token type mincount of 4, which results in around 9,000 token types for the COCO dataset, and around 25,000 token types for the Conceptual Captions dataset.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.9608827233314514}, {"text": "Conceptual Captions dataset", "start_pos": 137, "end_pos": 164, "type": "TASK", "confidence": 0.6506861448287964}]}, {"text": "All other tokens are replaced with special token UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8066774606704712}]}, {"text": "The word embedding matrix has size 512 and is tied to the output projection matrix.", "labels": [], "entities": []}, {"text": "Optimization All models are trained using MLE loss and optimized using Adagrad (Duchi et al., 2011) with learning rate 0.01.", "labels": [], "entities": [{"text": "learning", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9608952403068542}]}, {"text": "All model parameters are trained fora total number of 5M steps, with batch updates asynchronously distributed across 40 workers.", "labels": [], "entities": []}, {"text": "The final model is selected based on the best CIDEr score on the development set for the given training condition.", "labels": [], "entities": []}, {"text": "Inference During inference, the decoder prediction of the previous position is fed to the input of the next position.", "labels": [], "entities": []}, {"text": "We use abeam search of beam size 4 to compute the most likely output sequence.", "labels": [], "entities": []}, {"text": "For human evaluations, we use a pool of professional raters (tens of raters), with a double-blind evaluation condition.", "labels": [], "entities": []}, {"text": "Raters are asked to assign a GOOD or BAD label to a given image, caption input, using just common-sense judgment.", "labels": [], "entities": [{"text": "GOOD", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.7747159600257874}, {"text": "BAD", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.7638728022575378}]}, {"text": "This approximates the reaction of atypical user, who normally would not accept predefined notions of GOOD vs. BAD.", "labels": [], "entities": [{"text": "GOOD", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.5192006230354309}, {"text": "BAD", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.5653151273727417}]}, {"text": "We ask 3 separate raters to rate each input pair and report the percentage of pairs that receive k or more (k+) GOOD annotations.", "labels": [], "entities": []}, {"text": "In, we report the results on the Flickr 1K test set.", "labels": [], "entities": [{"text": "Flickr 1K test set", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.9467704594135284}]}, {"text": "This evaluation is out-of-domain for both training conditions, so all models are on relatively equal footing.", "labels": [], "entities": []}, {"text": "The results indicate that the Conceptual-based models are superior.", "labels": [], "entities": []}, {"text": "In 50.6% (for the: Auto metrics on the Flickr 1K Test.", "labels": [], "entities": [{"text": "Flickr 1K Test", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9337011377016703}]}, {"text": "In this section, we report automatic evaluation results, using established image captioning metrics.", "labels": [], "entities": []}, {"text": "For the COCO C40 test set (, we report the numerical values returned by the COCO online evaluation server \u2021 , using the CIDEr (, ROUGE-L (), and METEOR () metrics.", "labels": [], "entities": [{"text": "COCO C40 test set", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.954720064997673}, {"text": "COCO online evaluation server", "start_pos": 76, "end_pos": 105, "type": "DATASET", "confidence": 0.9034019857645035}, {"text": "ROUGE-L", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.994805634021759}, {"text": "METEOR", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9870067834854126}]}, {"text": "For Conceptual Captions and Flickr test sets, we report numerical values for the CIDEr, ROUGE-L, and SPICE () \u00a7 . For all metrics, higher number means closer distance between the candidates and the groundtruth captions.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7207017838954926}, {"text": "CIDEr", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.759039580821991}, {"text": "ROUGE-L", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9946494698524475}, {"text": "SPICE", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9853249192237854}]}, {"text": "The automatic metrics are good at detecting invs out-of-domain situations.", "labels": [], "entities": []}, {"text": "For COCO-models tested on COCO, the results in show CIDEr scores in the 1.02-1.04 range, for both RNN-and Transformer-based models; the scores drop in the 0.35-0.41 range (CIDEr) for the Conceptual-based models tested against COCO groundtruth.", "labels": [], "entities": [{"text": "CIDEr", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9858193397521973}, {"text": "CIDEr", "start_pos": 172, "end_pos": 177, "type": "METRIC", "confidence": 0.8794774413108826}]}, {"text": "For Conceptual-models tested on the Conceptual Captions test set, the results in show scores as high as 1.468 CIDEr for the T2T8x8 model, which corroborates the human-eval results for the Transformer-based models being superior to the RNN-based models; the scores for the COCObased models tested against Conceptual Captions groundtruth are all below 0.2 CIDEr.", "labels": [], "entities": [{"text": "Conceptual Captions test set", "start_pos": 36, "end_pos": 64, "type": "DATASET", "confidence": 0.7642376124858856}, {"text": "CIDEr", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.9385724067687988}]}, {"text": "The automatic metrics fail to corroborate the \u2021 http://mscoco.org/dataset/#captions-eval.", "labels": [], "entities": []}, {"text": "\u00a7 https://github.com/tylin/coco-caption.", "labels": [], "entities": []}, {"text": "According to the automatic metrics, the COCO-trained models are superior to the Conceptual-trained models (CIDEr scores in the mid-0.3 for the COCO-trained condition, versus mid-0.2 for the Conceptual-trained condition), and the RNN-based models are superior to Transformer-based models.", "labels": [], "entities": []}, {"text": "Notably, these are the same metrics which score humans lower than the methods that won the COCO 2015 challenge (, despite the fact that humans are still much better at this task.", "labels": [], "entities": [{"text": "COCO 2015 challenge", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.6999656359354655}]}, {"text": "The failure of these metrics to align with the human evaluation results casts again grave doubts on their ability to drive progress in this field.", "labels": [], "entities": []}, {"text": "A significant weakness of these metrics is that hallucination effects are under-penalized (a small precision penalty for tokens with no correspondent in the reference), compared to human judgments that tend to dive dramatically in the presence of hallucinations.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9881641864776611}]}], "tableCaptions": [{"text": " Table 2: Human evaluation results on a sample  from Conceptual Captions.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7512355744838715}]}, {"text": " Table 3: Statistics over Train/Validation/Test splits  for Conceptual Captions.", "labels": [], "entities": [{"text": "Conceptual Captions", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7257527410984039}]}, {"text": " Table 4: Human eval results on Flickr 1K Test.", "labels": [], "entities": [{"text": "Flickr 1K Test", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.8725603024164835}]}, {"text": " Table 5: Auto metrics on the COCO C40 Test.", "labels": [], "entities": [{"text": "COCO C40 Test", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9536998073259989}]}, {"text": " Table 6: Auto metrics on the 22.5K Conceptual  Captions Test set.", "labels": [], "entities": [{"text": "Conceptual  Captions Test set", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7130213901400566}]}, {"text": " Table 7: Auto metrics on the Flickr 1K Test.", "labels": [], "entities": [{"text": "Flickr 1K Test", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.8991721471150717}]}]}