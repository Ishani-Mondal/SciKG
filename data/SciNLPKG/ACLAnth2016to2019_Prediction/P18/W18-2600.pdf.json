{"title": [{"text": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications Robust and Scalable Differentiable Neural Computer for Question Answering A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset RECIPE: Applying Open Domain Question Answering to Privacy Policies Neural Models for Key Phrase Extraction and Question Generation Comparative Analysis of Neural QA models on SQuAD Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task15:30 Neural Models for Key Phrase Extraction and Question Generation Comparative Analysis of Neural QA models on SQuAD", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7170165330171585}, {"text": "ARC Dataset RECIPE", "start_pos": 237, "end_pos": 255, "type": "DATASET", "confidence": 0.9328714807828268}, {"text": "Open Domain Question Answering", "start_pos": 266, "end_pos": 296, "type": "TASK", "confidence": 0.6244557872414589}, {"text": "Question Generation", "start_pos": 361, "end_pos": 380, "type": "TASK", "confidence": 0.6873350888490677}, {"text": "BLEU", "start_pos": 456, "end_pos": 460, "type": "METRIC", "confidence": 0.9954181909561157}]}], "abstractContent": [], "introductionContent": [{"text": "Machine Reading for Question Answering (MRQA) has become an important testbed for evaluating how well computer systems understand human language, as well as a crucial technology for industry applications such as search engines and dialog systems.", "labels": [], "entities": [{"text": "Machine Reading for Question Answering (MRQA)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8591378554701805}]}, {"text": "The research community has recently created a multitude of large-scale datasets over text sources such as Wikipedia (WikiReading, SQuAD, WikiHop), news and other articles (CNN/Daily Mail, NewsQA, RACE), fictional stories (MCTest, CBT, NarrativeQA), and general web sources (MS MARCO, TriviaQA, SearchQA).", "labels": [], "entities": [{"text": "CNN/Daily Mail, NewsQA", "start_pos": 172, "end_pos": 194, "type": "DATASET", "confidence": 0.8016580939292908}, {"text": "MS MARCO", "start_pos": 274, "end_pos": 282, "type": "DATASET", "confidence": 0.8882659077644348}]}, {"text": "These new datasets have in turn inspired an even wider array of new question answering systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8671503067016602}]}, {"text": "Despite this rapid progress, there is yet much to understand about these datasets and systems.", "labels": [], "entities": []}, {"text": "While model performance is rapidly improving in domain for each dataset, generalization suffers when models are evaluated on new domains and datasets.", "labels": [], "entities": []}, {"text": "Moreover, current model development focuses primarily on improving the test accuracy of models trained on in-domain data.", "labels": [], "entities": [{"text": "model development", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7229996025562286}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9924193024635315}]}, {"text": "Focusing solely on accuracy obscures other important desiderata, including model interpretability, robustness to distributional shift, ability to abstain from answering when there is no adequate answer, and adequate modeling of inference (e.g., entailment and multi-sentence reasoning).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9976102113723755}, {"text": "model interpretability", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.658551812171936}]}, {"text": "Similarly, the diversity of recent datasets call for an analysis of various natural language phenomena (coreference, paraphrase, entailment, multi-hop reasoning) these datasets present.", "labels": [], "entities": []}, {"text": "The goal of this workshop is to gather researchers to address and discuss recent research on MRQA systems and datasets.", "labels": [], "entities": []}, {"text": "Currently, reading comprehension models are commonly presented at various venues such as ACL, EMNLP, NIPS, ICML, ICLR; this workshop will benefit the community by serving as a central venue for discussions.", "labels": [], "entities": [{"text": "ACL", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.9519110918045044}, {"text": "EMNLP", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.8435041904449463}, {"text": "NIPS", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.8676077723503113}, {"text": "ICML", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.8586392998695374}, {"text": "ICLR", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.7161908149719238}]}, {"text": "The program features 11 new papers and 5 cross-submissions from related areas, to be presented as both posters and talks.", "labels": [], "entities": []}, {"text": "We are also excited to host remarkable invited speakers, including Phil Blunsom, Antoine Bordes, Jianfeng Gao, Hannaneh Hajishirzi, Sebastian Riedel, Richard Socher.", "labels": [], "entities": []}, {"text": "We thank the program committee, the ACL workshop chairs, the invited speakers, our sponsors Facebook and Naver and our steering committee: Antoine Bordes, Percy Liang, Luke Zettlemoyer.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}