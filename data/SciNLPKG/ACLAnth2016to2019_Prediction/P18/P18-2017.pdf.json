{"title": [{"text": "Neural Coreference Resolution with Deep Biaffine Attention by Joint Mention Detection and Mention Clustering", "labels": [], "entities": [{"text": "Neural Coreference Resolution", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8811257680257162}]}], "abstractContent": [{"text": "Coreference resolution aims to identify in a text all mentions that refer to the same real-world entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9183365702629089}]}, {"text": "The state-of-the-art end-to-end neural coreference model considers all text spans in a document as potential mentions and learns to link an antecedent for each possible mention.", "labels": [], "entities": []}, {"text": "In this paper, we propose to improve the end-to-end coreference resolution system by (1) using a biaffine attention model to get antecedent scores for each possible mention, and (2) jointly optimizing the mention detection accuracy and the mention clustering log-likelihood given the mention cluster labels.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.9166962206363678}, {"text": "accuracy", "start_pos": 223, "end_pos": 231, "type": "METRIC", "confidence": 0.5315237045288086}]}, {"text": "Our model achieves the state-of-the-art performance on the CoNLL-2012 Shared Task English test set.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task English test set", "start_pos": 59, "end_pos": 98, "type": "DATASET", "confidence": 0.8942323724428812}]}], "introductionContent": [{"text": "End-to-end coreference resolution is the task of identifying and grouping mentions in a text such that all mentions in a cluster refer to the same entity.", "labels": [], "entities": [{"text": "End-to-end coreference resolution", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7068390250205994}]}, {"text": "An example is given below) where mentions for two entities are labeled in two clusters:  Many traditional coreference systems, either rulebased () * Work done during the internship at IBM or learning-based (, usually solve the problem in two separate stages: (1) a mention detector to propose entity mentions from the text, and (2) a coreference resolver to cluster proposed mentions.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 334, "end_pos": 354, "type": "TASK", "confidence": 0.7973252236843109}]}, {"text": "At both stages, they rely heavily on complicated, fine-grained, conjoined features via heuristics.", "labels": [], "entities": []}, {"text": "This pipeline approach can cause cascading errors, and in addition, since both stages rely on a syntactic parser and complicated handcraft features, it is difficult to generalize to new data sets and languages.", "labels": [], "entities": []}, {"text": "Very recently, proposed the first state-of-the-art end-to-end neural coreference resolution system.", "labels": [], "entities": [{"text": "end-to-end neural coreference resolution", "start_pos": 51, "end_pos": 91, "type": "TASK", "confidence": 0.6338045597076416}]}, {"text": "They consider all text spans as potential mentions and therefore eliminate the need of carefully hand-engineered mention detection systems.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7519364058971405}]}, {"text": "In addition, thanks to the representation power of pre-trained word embeddings and deep neural networks, the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width).", "labels": [], "entities": []}, {"text": "The core of the end-to-end neural coreference resolver is the scoring function to compute the mention scores for all possible spans and the antecedent scores fora pair of spans.", "labels": [], "entities": [{"text": "end-to-end neural coreference resolver", "start_pos": 16, "end_pos": 54, "type": "TASK", "confidence": 0.6456892117857933}]}, {"text": "Furthermore, one major challenge of coreference resolution is that most mentions in the document are singleton or non-anaphoric, i.e., not coreferent with any previous mention.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.9827358722686768}]}, {"text": "Since the data set only have annotations for mention clusters, the end-to-end coreference resolution system needs to detect mentions, detect anaphoricity, and perform coreference linking.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.8893871009349823}, {"text": "coreference linking", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.8719922602176666}]}, {"text": "Therefore, research questions still remain on good designs of the scoring architecture and the learning strategy for both mention detection and antecedent scoring given only the gold cluster labels.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7735591232776642}]}, {"text": "To this end, we propose to use a biaffine atten-: Model architecture.", "labels": [], "entities": []}, {"text": "We consider all text spans up to 10-word length as possible mentions.", "labels": [], "entities": []}, {"text": "For brevity, we only show three candidate antecedent spans (\"Drug Emporium Inc.\", \"Gary Wilber\", \"was named CEO\") for the current span \"this drugstore chain\".", "labels": [], "entities": [{"text": "Drug Emporium Inc.", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.9349484244982401}]}, {"text": "tion model instead of pure feed forward networks to compute antecedent scores.", "labels": [], "entities": []}, {"text": "Furthermore, instead of training only to maximize the marginal likelihood of gold antecedent spans, we jointly optimize the mention detection accuracy and the mention clustering log-likelihood given the mention cluster labels.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.6256172955036163}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.7348927855491638}]}, {"text": "We optimize mention detection loss explicitly to extract mentions and also perform anaphoricity detection.", "labels": [], "entities": [{"text": "anaphoricity detection", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7034262269735336}]}, {"text": "We evaluate our model on the CoNLL-2012 English data set and achieve new state-of-the-art performances of 67.8% F1 score using a single model and 69.2% F1 score using a 5-model ensemble.", "labels": [], "entities": [{"text": "CoNLL-2012 English data set", "start_pos": 29, "end_pos": 56, "type": "DATASET", "confidence": 0.9602000266313553}, {"text": "F1 score", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9837681651115417}, {"text": "F1 score", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9842544794082642}]}], "datasetContent": [{"text": "Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data ( which is based on the OntoNotes corpus (: Ablation study on the development set.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task English data", "start_pos": 53, "end_pos": 88, "type": "DATASET", "confidence": 0.8935506105422973}, {"text": "OntoNotes corpus", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.8951087892055511}]}, {"text": "genre, span distance, span width) features as 20-dimensional learned embeddings.", "labels": [], "entities": [{"text": "span width", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.6529904007911682}]}, {"text": "Word and character embeddings use 0.5 dropout.", "labels": [], "entities": []}, {"text": "All hidden layers and feature embeddings use 0.2 dropout.", "labels": [], "entities": []}, {"text": "The batch size is 1 document.", "labels": [], "entities": []}, {"text": "Based on the results on the development set, \u03bb detection = 0.1 works best from {0.05, 0.1, 0.5, 1.0}.", "labels": [], "entities": []}, {"text": "Model is trained with ADAM optimizer ( and converges in around 200K updates, which is faster than that of.", "labels": [], "entities": []}, {"text": "Overall Performance In, we compare our model with previous state-of-the-art systems.", "labels": [], "entities": []}, {"text": "We obtain the best results in all F1 metrics.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9909102320671082}]}, {"text": "Our single model achieves 67.8% F1 and our 5-model ensemble achieves 69.2% F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9996768236160278}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9988000392913818}]}, {"text": "In particular, compared with, our improvement mainly results from the precision scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9997404217720032}]}, {"text": "This indicates that the mention detection loss does produce better mention scores and the biaffine attention more effectively determines if two spans are coreferent.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.6185900270938873}]}, {"text": "Ablation Study To understand the effect of different proposed components, we perform ablation study on the development set.", "labels": [], "entities": []}, {"text": "As shown in, removing the mention detection loss term or the biaffine attention decreases 0.3/0.4 final F1 score, but still higher than the baseline.", "labels": [], "entities": [{"text": "mention detection loss term", "start_pos": 26, "end_pos": 53, "type": "METRIC", "confidence": 0.7936496138572693}, {"text": "biaffine attention", "start_pos": 61, "end_pos": 79, "type": "METRIC", "confidence": 0.8799871504306793}, {"text": "F1 score", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9736033380031586}]}, {"text": "This shows: Mention detection subtask on development set.", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7522781491279602}]}, {"text": "We plot accuracy and frequency breakdown by span widths. that both components have contributions and when they work together the total gain is even higher.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9993645548820496}]}, {"text": "Mention Detection Subtask To further understand our model, we perform a mention detection subtask where spans with mention scores higher than 0 are considered as mentions.", "labels": [], "entities": [{"text": "Mention Detection Subtask", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.728422611951828}]}, {"text": "We show the mention detection accuracy breakdown by span widths in.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.6738981306552887}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.5984241962432861}]}, {"text": "Our model indeed performs better thanks to the mention detection loss.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 47, "end_pos": 64, "type": "METRIC", "confidence": 0.7733649015426636}]}, {"text": "The advantage is even clearer for longer spans which consist of 5 or more words.", "labels": [], "entities": []}, {"text": "In addition, it is important to note that our model can detect mentions that do not exist in the training data.", "labels": [], "entities": []}, {"text": "While observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly de-tect 1048 mentions which are not detected by, consisting of 386 mentions existing in training data and 662 mentions not existing in training data.", "labels": [], "entities": []}, {"text": "From those 662 mentions, some examples are (1) a suicide murder (2) Hong Kong Island (3) a US Airforce jet carrying robotic undersea vehicles (4) the investigation into who was behind the apparent suicide attack.", "labels": [], "entities": []}, {"text": "This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memorizing the existing mentions in training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results on the CoNLL-2012 Englisth test set. The F1 improvements are statistical  significant with p < 0.05 under the paired bootstrap resample test (Koehn, 2004) compared with Lee  et al. (2017).", "labels": [], "entities": [{"text": "CoNLL-2012 Englisth test set", "start_pos": 38, "end_pos": 66, "type": "DATASET", "confidence": 0.8635165691375732}, {"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9990348815917969}]}, {"text": " Table 2: Ablation study on the development set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9497019648551941}]}]}