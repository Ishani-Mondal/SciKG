{"title": [{"text": "Domain Adaptation with Adversarial Training and Graph Embeddings", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7226908057928085}]}], "abstractContent": [{"text": "The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data.", "labels": [], "entities": []}, {"text": "However, obtaining labeled data is a big challenge in many real-world problems.", "labels": [], "entities": []}, {"text": "In such scenarios, a DNN model can leverage labeled and un-labeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains.", "labels": [], "entities": []}, {"text": "In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake).", "labels": [], "entities": [{"text": "classifying social media posts during a crisis event", "start_pos": 39, "end_pos": 91, "type": "TASK", "confidence": 0.839217521250248}]}, {"text": "For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unla-beled data for the current event.", "labels": [], "entities": []}, {"text": "We propose a novel model that performs adver-sarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework.", "labels": [], "entities": [{"text": "adver-sarial learning based domain adaptation", "start_pos": 39, "end_pos": 84, "type": "TASK", "confidence": 0.7324541807174683}]}, {"text": "Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "The application that motivates our work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters (.", "labels": [], "entities": []}, {"text": "In such events, affected people post timely and useful information of various types such as reports of injured or dead people, infrastructure damage, urgent needs (e.g., food, shelter, medical assistance) on these social networks.", "labels": [], "entities": []}, {"text": "Humanitarian organizations believe timely access to this important information from social networks can help significantly and reduce both human loss and economic damage (.", "labels": [], "entities": [{"text": "human loss", "start_pos": 139, "end_pos": 149, "type": "TASK", "confidence": 0.730020672082901}]}, {"text": "In this paper, we consider the basic task of classifying each incoming tweet during a crisis event (e.g., Earthquake) into one of the predefined classes of interest (e.g., relevant vs. nonrelevant) in real-time.", "labels": [], "entities": [{"text": "classifying each incoming tweet during a crisis event (e.g., Earthquake)", "start_pos": 45, "end_pos": 117, "type": "TASK", "confidence": 0.7036081850528717}]}, {"text": "Recently, deep neural networks (DNNs) have shown great performance in classification tasks in NLP and data mining.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8970733880996704}, {"text": "data mining", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.8089551627635956}]}, {"text": "However the success of DNNs on a task depends heavily on the availability of a large labeled dataset, which is not a feasible option in our setting (i.e., classifying tweets at the onset of an Earthquake).", "labels": [], "entities": []}, {"text": "On the other hand, inmost cases, we can have access to a good amount of labeled and abundant unlabeled data from past similar events (e.g., Floods) and possibly some unlabeled data for the current event.", "labels": [], "entities": []}, {"text": "In such situations, we need methods that can leverage the labeled and unlabeled data in a past event (we refer to this as a source domain), and that can adapt to anew event (we refer to this as a target domain) without requiring any labeled data in the new event.", "labels": [], "entities": []}, {"text": "In other words, we need models that can do domain adaptation to deal with the distribution drift between the domains and semi-supervised learning to leverage the unlabeled data in both domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7237056344747543}]}, {"text": "Most recent approaches to semi-supervised learning ( and domain adaptation () use the automatic feature learning capability of DNN models.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7227032333612442}]}, {"text": "In this paper, we extend these methods by proposing a novel model that performs domain adaptation and semi-supervised learning within a single unified deep learning framework.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7253937870264053}]}, {"text": "In this framework, the basic task-solving network (a convolutional neural network in our case) is put together with two other networks -one for semi-supervised learning and the other for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.7314534485340118}]}, {"text": "The semisupervised component learns internal representa-tions (features) by predicting contextual nodes in a graph that encodes similarity between labeled and unlabeled training instances.", "labels": [], "entities": []}, {"text": "The domain adaptation is achieved by training the feature extractor (or encoder) in adversary with respect to a domain discriminator, a binary classifier that tries to distinguish the domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7432092428207397}]}, {"text": "The overall idea is to learn high-level abstract representation that is discriminative for the main classification task, but is invariant across the domains.", "labels": [], "entities": []}, {"text": "We propose a stochastic gradient descent (SGD) algorithm to train the components of our model simultaneously.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.7906288206577301}]}, {"text": "The evaluation of our proposed model is conducted using two Twitter datasets on scenarios where there is only unlabeled data in the target domain.", "labels": [], "entities": []}, {"text": "Our results demonstrate the following.", "labels": [], "entities": []}, {"text": "1. When the network combines the semisupervised component with the supervised component, depending on the amount of labeled data used, it gives 5% to 26% absolute gains in F1 compared to when it uses only the supervised component.", "labels": [], "entities": [{"text": "F1", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.9988395571708679}]}], "datasetContent": [{"text": "In this section, we describe our experimental settings -datasets used, settings of our models, compared baselines, and evaluation metrics.", "labels": [], "entities": []}, {"text": "To conduct the experiment and evaluate our system, we used two real-world Twitter datasets collected during the 2015 Nepal earthquake (NEQ) and the 2013 Queensland floods (QFL).", "labels": [], "entities": [{"text": "Twitter datasets collected during the 2015 Nepal earthquake (NEQ)", "start_pos": 74, "end_pos": 139, "type": "DATASET", "confidence": 0.7310179688713767}, {"text": "Queensland floods (QFL)", "start_pos": 153, "end_pos": 176, "type": "DATASET", "confidence": 0.8162265181541443}]}, {"text": "These datasets are comprised of millions of tweets collected through the Twitter streaming API 4 using event-specific keywords/hashtags.", "labels": [], "entities": []}, {"text": "To obtain the labeled examples for our task we employed paid workers from the Crowdflower 5 -a crowdsourcing platform.", "labels": [], "entities": [{"text": "Crowdflower 5 -a crowdsourcing platform", "start_pos": 78, "end_pos": 117, "type": "DATASET", "confidence": 0.9423199991385142}]}, {"text": "The annotation consists of two classes relevant and non-relevant.", "labels": [], "entities": []}, {"text": "For the annotation, we randomly sampled 11,670 and 10,033 tweets from the Nepal earthquake and the Queensland floods datasets, respectively.", "labels": [], "entities": [{"text": "Queensland floods datasets", "start_pos": 99, "end_pos": 125, "type": "DATASET", "confidence": 0.9539504249890646}]}, {"text": "Given a tweet, we asked crowdsourcing workers to assign the \"relevant\" label if the tweet conveys/reports information useful for crisis response such as a report of injured or dead people, some kind of infrastructure damage, urgent needs of affected people, donations requests or offers, otherwise assign the \"non-relevant\" label.", "labels": [], "entities": []}, {"text": "We split the labeled data into 60% as training, 30% as test and 10% as development.", "labels": [], "entities": []}, {"text": "shows the resulting datasets with class-wise distributions.", "labels": [], "entities": []}, {"text": "Data preprocessing was performed by following the same steps used to train the word2vec model (Subsection 2.5).", "labels": [], "entities": [{"text": "Data preprocessing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7202486991882324}]}, {"text": "In all the experiments, the classification task consists of two classes: relevant and non-relevant.", "labels": [], "entities": []}, {"text": "To measure the performance of the trained models using different approaches described above, we use weighted average precision, recall, F-measure, and Area Under ROC-Curve (AUC), which are standard evaluation measures in the NLP and machine learning communities.", "labels": [], "entities": [{"text": "weighted average precision", "start_pos": 100, "end_pos": 126, "type": "METRIC", "confidence": 0.6322833299636841}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9987764954566956}, {"text": "F-measure", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9968169331550598}, {"text": "Area Under ROC-Curve (AUC)", "start_pos": 151, "end_pos": 177, "type": "METRIC", "confidence": 0.9606528878211975}]}, {"text": "The rationale behind choosing the weighted metric is that it takes into account the class imbalance problem.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of labeled datasets for Nepal  earthquake (NEQ) and Queensland flood (QFL).", "labels": [], "entities": [{"text": "Nepal  earthquake (NEQ)", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.6386471390724182}, {"text": "Queensland flood (QFL)", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.9139247298240661}]}, {"text": " Table 2: Results using supervised, self-training,  and graph-based semi-supervised approaches in  terms of Weighted average AUC, precision (P), re- call (R) and F-measure (F1).", "labels": [], "entities": [{"text": "Weighted average AUC", "start_pos": 108, "end_pos": 128, "type": "METRIC", "confidence": 0.8383726080258688}, {"text": "precision (P)", "start_pos": 130, "end_pos": 143, "type": "METRIC", "confidence": 0.9524039477109909}, {"text": "re- call (R)", "start_pos": 145, "end_pos": 157, "type": "METRIC", "confidence": 0.9537990689277649}, {"text": "F-measure (F1)", "start_pos": 162, "end_pos": 176, "type": "METRIC", "confidence": 0.9444848001003265}]}, {"text": " Table 3: Weighted average F-measure for the  graph-based semi-supervised settings using differ- ent batch sizes. L refers to labeled data, U refers to  unlabeled data, All L refers to all labeled instances  for that particular dataset.", "labels": [], "entities": [{"text": "Weighted average F-measure", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.743448277314504}]}]}