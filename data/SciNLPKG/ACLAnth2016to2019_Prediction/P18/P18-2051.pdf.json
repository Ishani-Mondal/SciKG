{"title": [{"text": "Multi-representation Ensembles and Delayed SGD Updates Improve Syntax-based NMT", "labels": [], "entities": [{"text": "NMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.5463756322860718}]}], "abstractContent": [{"text": "We explore strategies for incorporating target syntax into Neural Machine Translation.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.8059670726458231}]}, {"text": "We specifically focus on syntax in ensembles containing multiple sentence representations.", "labels": [], "entities": []}, {"text": "We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax.", "labels": [], "entities": [{"text": "formulate beam search", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7324230273564657}, {"text": "WFSTs", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8428385853767395}]}, {"text": "Our approach gives state-of-the-art performance on a difficult Japanese-English task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ensembles of multiple NMT models consistently and significantly improve over single models (.", "labels": [], "entities": []}, {"text": "Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily.", "labels": [], "entities": []}, {"text": "We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way.", "labels": [], "entities": []}, {"text": "However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step.", "labels": [], "entities": []}, {"text": "Models producing different sentence representations are necessarily synchronized to enable this.", "labels": [], "entities": []}, {"text": "We propose an approach to decoding ensembles of models generating different representations, focusing on models generating syntax.", "labels": [], "entities": []}, {"text": "As part of our investigation we suggest strategies for practical NMT with very long target sequences.", "labels": [], "entities": []}, {"text": "These long sequences may arise through the use of linearized constituency trees and can be much longer than their plain byte pair encoded (BPE) equivalent representations).", "labels": [], "entities": []}, {"text": "Long sequences make training more difficult, which we address with an adjusted training procedure for the Transformer architecture (, using delayed SGD updates which accumulate gradients over multiple batches.", "labels": [], "entities": []}, {"text": "We also suggest a syntax representation which results in much shorter sequences.", "labels": [], "entities": []}, {"text": "perform NMT with syntax annotation in the form of Combinatory Categorial Grammar (CCG) supertags.", "labels": [], "entities": []}, {"text": "translate from source BPE into target linearized parse trees, but omit POS tags to reduce sequence length.", "labels": [], "entities": []}, {"text": "They demonstrate improved target language reordering when producing syntax.", "labels": [], "entities": []}, {"text": "combine recurrent neural network grammar (RNNG) models) with attention-based models to produce well-formed dependency trees.", "labels": [], "entities": [{"text": "recurrent neural network grammar (RNNG)", "start_pos": 8, "end_pos": 47, "type": "TASK", "confidence": 0.678459175995418}]}, {"text": "similarly produce both words and arcstandard algorithm actions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first explore the effect of our delayed SGD update training scheme on single models, contrasting updates every batch with accumulated updates every 8 batches.", "labels": [], "entities": [{"text": "SGD update", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.8826943933963776}]}, {"text": "To compare target representations we train Transformer models with target representations,, and shown in For comparison with earlier target syntax work, we also train two RNN attention-based seq2seq models () with normal SGD to produce plain BPE sequences and linearized derivations.", "labels": [], "entities": []}, {"text": "For these models we use embedding size 400, a single BiLSTM layer of size 750, and batch size 80.", "labels": [], "entities": []}, {"text": "We report all experiments for JapaneseEnglish, using the first 1M training sentences of the Japanese-English ASPEC data ().", "labels": [], "entities": [{"text": "JapaneseEnglish", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.8751501441001892}, {"text": "ASPEC data", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.7055429369211197}]}, {"text": "All models use plain BPE Japanese source sentences.", "labels": [], "entities": [{"text": "BPE Japanese source sentences", "start_pos": 21, "end_pos": 50, "type": "DATASET", "confidence": 0.7636616080999374}]}, {"text": "English constituency trees are obtained using CKYlark (), with words replaced by BPE subwords.", "labels": [], "entities": [{"text": "CKYlark", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9174617528915405}, {"text": "BPE", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8356236219406128}]}, {"text": "We train separate Japanese (lowercased) and English (cased) BPE vocabularies on the plain-text, with 30K merges each.", "labels": [], "entities": []}, {"text": "Non-terminals are included as separate tokens.", "labels": [], "entities": []}, {"text": "The linearized derivation uses additional tokens for non-terminals with </R> .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Single Transformers trained to con- vergence on 1M WAT Ja-En, batch size 4096", "labels": [], "entities": [{"text": "1M WAT Ja-En", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.7015813787778219}]}, {"text": " Table 4: Single models on Ja-En. Previous  evaluation result included for comparison.", "labels": [], "entities": [{"text": "Ja-En", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8958045840263367}]}]}