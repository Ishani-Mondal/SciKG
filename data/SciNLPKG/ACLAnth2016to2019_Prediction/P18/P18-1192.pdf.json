{"title": [{"text": "Syntax for Semantic Role Labeling, To Be, Or Not To Be", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7776182095209757}]}], "abstractContent": [{"text": "Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8441476623217264}, {"text": "recognizing the predicate-argument structure of a sentence", "start_pos": 45, "end_pos": 103, "type": "TASK", "confidence": 0.7352760263851711}]}, {"text": "Previous studies have shown syntactic information has a remarkable contribution to SRL performance.", "labels": [], "entities": [{"text": "SRL", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9958899617195129}]}, {"text": "However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone.", "labels": [], "entities": []}, {"text": "This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework.", "labels": [], "entities": [{"text": "dependency SRL", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.6163828074932098}]}, {"text": "We propose an enhanced argument labeling model companying with an extended k-order argument pruning algorithm for effectively exploiting syntactic information.", "labels": [], "entities": [{"text": "argument labeling", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7063529938459396}]}, {"text": "Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neu-ral SRL together with a thorough empirical survey over existing models.", "labels": [], "entities": [{"text": "CoNLL-2008, 2009 benchmarks", "start_pos": 51, "end_pos": 78, "type": "DATASET", "confidence": 0.8729395717382431}, {"text": "SRL", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.7051931619644165}]}], "introductionContent": [{"text": "Semantic role labeling (SRL), namely semantic parsing, is a shallow semantic parsing task, which aims to recognize the predicate-argument structure of each predicate in a sentence, such as who did what to whom, where and when, etc.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8477508723735809}, {"text": "semantic parsing", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.760585218667984}, {"text": "semantic parsing task", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8307144244511923}]}, {"text": "Specifically, we seek to identify arguments and label their semantic roles given a predicate.", "labels": [], "entities": []}, {"text": "SRL is an impor-tant method to obtain semantic information beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (, question answering ( and discourse relation sense classification.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8186203241348267}, {"text": "machine translation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7705247700214386}, {"text": "question answering", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.8076207637786865}, {"text": "discourse relation sense classification", "start_pos": 188, "end_pos": 227, "type": "TASK", "confidence": 0.6380844041705132}]}, {"text": "There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies.", "labels": [], "entities": []}, {"text": "The latter proposed by the CoNLL-2008 shared task () is also called semantic dependency parsing, which annotates the heads of arguments rather than phrasal arguments.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6789969801902771}]}, {"text": "Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument identification and classification.", "labels": [], "entities": [{"text": "SRL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9875088334083557}, {"text": "predicate identification and disambiguation", "start_pos": 104, "end_pos": 147, "type": "TASK", "confidence": 0.8085688501596451}, {"text": "argument identification", "start_pos": 149, "end_pos": 172, "type": "TASK", "confidence": 0.7136067450046539}]}, {"text": "In prior work of SRL, considerable attention has been paid to feature engineering that struggles to capture sufficient discriminative information, while neural network models are capable of extracting features automatically.", "labels": [], "entities": [{"text": "SRL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9664612412452698}]}, {"text": "In particular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of.", "labels": [], "entities": [{"text": "SRL", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9868900775909424}]}, {"text": "However, all the work had to take the risk of erroneous syntactic input, leading to an unsatisfactory performance.", "labels": [], "entities": []}, {"text": "To alleviate the above issues,  propose a simple but effective model for dependency SRL without syntactic input.", "labels": [], "entities": [{"text": "dependency SRL", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.704682320356369}]}, {"text": "It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as.", "labels": [], "entities": [{"text": "SRL", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.7972157001495361}, {"text": "SRL", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.9940379858016968}]}, {"text": "This dramatic contradiction motivates us to make a thorough exploration on syntactic contribution to SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.947348952293396}]}, {"text": "This paper will focus on semantic dependency parsing and formulate SRL as one or two se-quence tagging tasks with predicate-specific encoding.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.7622609337170919}, {"text": "formulate SRL", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.809643417596817}]}, {"text": "With the help of the proposed k-order argument pruning algorithm over syntactic tree, our model obtains state-of-the-art scores on the CoNLL benchmarks for both In order to quantitatively evaluate the contribution of syntax to SRL, we adopt the ratio between labeled F 1 score for semantic dependencies (Sem-F 1 ) and the labeled attachment score (LAS) for syntactic dependencies introduced by CoNLL-2008 Shared Task 1 as evaluation metric.", "labels": [], "entities": [{"text": "CoNLL benchmarks", "start_pos": 135, "end_pos": 151, "type": "DATASET", "confidence": 0.9400503635406494}, {"text": "SRL", "start_pos": 227, "end_pos": 230, "type": "TASK", "confidence": 0.9793399572372437}, {"text": "labeled attachment score (LAS)", "start_pos": 322, "end_pos": 352, "type": "METRIC", "confidence": 0.8008696287870407}]}, {"text": "Considering that various syntactic parsers contribute different syntactic inputs with various range of quality levels, the ratio provides a fairer comparison between syntactically-driven SRL systems, which will be surveyed by our empirical study.", "labels": [], "entities": [{"text": "SRL", "start_pos": 187, "end_pos": 190, "type": "TASK", "confidence": 0.9128602743148804}]}], "datasetContent": [{"text": "Our model 2 is evaluated on the CoNLL-2009 shared task both for English and Chinese datasets, following the standard training, development and test splits.", "labels": [], "entities": [{"text": "CoNLL-2009 shared task", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.8313744862874349}]}, {"text": "The hyperparameters in our model were selected based on the development set, and are summarized in.", "labels": [], "entities": []}, {"text": "Note that the parameters of predicate model are the same as these in argument model.", "labels": [], "entities": []}, {"text": "All real vectors are randomly initialized, and the pre-trained word embeddings for English are GloVe vectors ().", "labels": [], "entities": []}, {"text": "For Chinese, we exploit Wikipedia documents to train Word2Vec embeddings (Mikolov.", "labels": [], "entities": []}, {"text": "We train models fora maximum of 20 epochs and obtain the nearly best model based on development results.", "labels": [], "entities": []}, {"text": "For argument labeling, we preprocess corpus with k-order argument pruning algorithm.", "labels": [], "entities": [{"text": "argument labeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7299726009368896}]}, {"text": "In addition, we use four CNN layers with singlelayer BiLSTM to induce character representations derived from sentences.", "labels": [], "entities": []}, {"text": "For English 3 , to further enhance the representation, we adopt CNNBiLSTM character embedding structure from AllenNLP toolkit ().", "labels": [], "entities": [{"text": "CNNBiLSTM character embedding structure", "start_pos": 64, "end_pos": 103, "type": "DATASET", "confidence": 0.8777130544185638}, {"text": "AllenNLP toolkit", "start_pos": 109, "end_pos": 125, "type": "DATASET", "confidence": 0.9375869333744049}]}], "tableCaptions": [{"text": " Table 2: Results on the English test set (WSJ).", "labels": [], "entities": [{"text": "English test set (WSJ)", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.9470161894957224}]}, {"text": " Table 3: Results on English out-of-domain test set  (Brown).", "labels": [], "entities": [{"text": "English out-of-domain test set  (Brown)", "start_pos": 21, "end_pos": 60, "type": "DATASET", "confidence": 0.7832156802926745}]}, {"text": " Table 4: Results on the Chinese test set.", "labels": [], "entities": [{"text": "Chinese test set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9822543660799662}]}, {"text": " Table 5: SRL results without predicate sense.", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9418637752532959}]}, {"text": " Table 6: Ablation on development set. The \"+\"  denotes a specific version over the basic model.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9884523749351501}]}, {"text": " Table 7: Comparison of results on CoNLL-2009  data between our end-to-end and pipeline models.", "labels": [], "entities": [{"text": "CoNLL-2009  data", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9157985150814056}]}, {"text": " Table 8: Results on the CoNLL-2008 in-domain  (WSJ) test set. The results in parenthesis are on  WSJ + Brown test set.", "labels": [], "entities": [{"text": "CoNLL-2008 in-domain  (WSJ) test set", "start_pos": 25, "end_pos": 61, "type": "DATASET", "confidence": 0.8223263791629246}, {"text": "WSJ + Brown test set", "start_pos": 98, "end_pos": 118, "type": "DATASET", "confidence": 0.9322134375572204}]}, {"text": " Table 9: Results on English test set, in terms of labeled attachment score for syntactic dependencies  (LAS), semantic precision (P), semantic recall (R), semantic labeled F 1 score (Sem-F 1 ), the ratio Sem- F 1 /LAS. A superscript * indicates LAS results from our personal communication with the authors.", "labels": [], "entities": [{"text": "English test set", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.8487778703371683}, {"text": "semantic precision (P)", "start_pos": 111, "end_pos": 133, "type": "METRIC", "confidence": 0.8416396617889405}, {"text": "recall (R)", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.9279684573411942}, {"text": "semantic labeled F 1 score", "start_pos": 156, "end_pos": 182, "type": "METRIC", "confidence": 0.6607589542865753}]}]}