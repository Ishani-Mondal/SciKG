{"title": [{"text": "Paper Abstract Writing through Editing Mechanism", "labels": [], "entities": [{"text": "Paper Abstract Writing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5101775527000427}]}], "abstractContent": [{"text": "We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract.", "labels": [], "entities": []}, {"text": "We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract.", "labels": [], "entities": []}, {"text": "With two series of Turing tests, where the human judges are asked to distinguish the system-generated abstracts from human-written ones, our system passes Turing tests by junior domain experts at a rate up to 30% and by non-expert at a rate up to 80%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Routine writing, such as writing scientific papers or patents, is a very common exercise.", "labels": [], "entities": [{"text": "Routine writing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8969408571720123}]}, {"text": "It can be traced back to the \"Eight legged essay\", an austere writing style in the Ming-Qing dynasty.", "labels": [], "entities": []}, {"text": "We explore automated routine writing, with paper abstract writing as a case study.", "labels": [], "entities": []}, {"text": "Given a title, we aim to automatically generate a paper abstract.", "labels": [], "entities": []}, {"text": "We hope our approach can serve as an assistive technology for human to write paper abstracts more efficiently and professionally, by generating an initial draft for humans further editing, correction and enrichment.", "labels": [], "entities": []}, {"text": "A scientific paper abstract should always focus on the topics specified in the title.", "labels": [], "entities": []}, {"text": "However, atypical recurrent neural network based approach easily loses focus.", "labels": [], "entities": []}, {"text": "Given the title \"An effective method of using Web based information for Relation Extraction\" from, we compare the human written abstract and system generated abstracts in.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.9651294648647308}]}, {"text": "The LSTM LM baseline generated abstract misses the key term \"Web\" mentioned in the paper title.", "labels": [], "entities": []}, {"text": "We introduce a title attention () into a sequence-to-sequence model) to guide the generation process so the abstract is topically relevant to the given title, as shown in the \"Seq2seq with attention\" row of.", "labels": [], "entities": []}, {"text": "Previous work usually models natural language generation as a one-way decision problem, where models generate a sequence of tokens as output and then moves on, never coming back to modify or improve the output.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.6779273947079977}]}, {"text": "However, human writers usually start with a draft and keep polishing and revising it.", "labels": [], "entities": []}, {"text": "J. Cherryh once said, \"it is perfectly okay to write garbage -as long as you edit brilliantly.\"", "labels": [], "entities": []}, {"text": "We model abstract generation as a conditioned, iterative text generation problem and design anew Writing-editing Network with an Attentive Revision Gate to iteratively examine, improve, and edit the abstract with guidance from the paper title as well as the previously generated abstract.", "labels": [], "entities": [{"text": "abstract generation", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7606695890426636}, {"text": "text generation", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7810741662979126}]}, {"text": "A result of the Writing-editing Network is shown in, where we can see that the initial draft contains more topically relevant and richer concepts than the title, such as the term 'IE'.", "labels": [], "entities": []}, {"text": "By adding this initial draft as feedback and guidance, it eases the next generation iteration, allowing the model to focus on a more limited learning space, and generate more concise and coherent abstracts.", "labels": [], "entities": []}, {"text": "We propose a method that incorporates paraphrase information from the Web to boost the performance of a supervised relation extraction system.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.702086940407753}]}, {"text": "Contextual information is extracted from the Web using a semisupervised process, and summarized by skip-bigram overlap measures over the entire extract.", "labels": [], "entities": []}, {"text": "This allows the capture of local contextual information as well as more distant associations.", "labels": [], "entities": []}, {"text": "We observe a statistically significant boost in relation extraction performance.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.9198274612426758}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Method Comparison (%).", "labels": [], "entities": []}, {"text": " Table 3: Plagiarism Check: Percentage (%) of n- grams in test abstracts generated by system/human  which appeared in training data.", "labels": [], "entities": [{"text": "Plagiarism Check", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8978379964828491}]}]}