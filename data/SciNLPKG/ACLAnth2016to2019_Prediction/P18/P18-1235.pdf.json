{"title": [{"text": "A Helping Hand: Transfer Learning for Deep Sentiment Analysis", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.8888674676418304}, {"text": "Deep Sentiment Analysis", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6572136183579763}]}], "abstractContent": [{"text": "Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.8913489778836569}]}, {"text": "In this work, we present an approach to feed generic cues into the training process of such networks, leading to better generalization abilities given limited training data.", "labels": [], "entities": []}, {"text": "We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the model via a dedicated memory-based component.", "labels": [], "entities": [{"text": "sentiment embeddings", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8636404573917389}]}, {"text": "We observe significant gains in effectiveness on a range of different datasets in seven different languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past decades, sentiment analysis has grown from an academic endeavour to an essential analytics tool.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9768100678920746}]}, {"text": "Across the globe, people are voicing their opinion in online social media, product review sites, booking platforms, blogs, etc.", "labels": [], "entities": []}, {"text": "Hence, it is important to keep abreast of ongoing developments in all pertinent markets, accounting for different domains as well as different languages.", "labels": [], "entities": []}, {"text": "In recent years, deep neural architectures based on convolutional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification.", "labels": [], "entities": [{"text": "supervised sentiment polarity classification", "start_pos": 139, "end_pos": 183, "type": "TASK", "confidence": 0.7008094191551208}]}, {"text": "At the same time, it is also frequently observed that deep neural networks tend to be particularly data-hungry.", "labels": [], "entities": []}, {"text": "This is a problem in many real-world settings, where large amounts of training examples maybe too costly to obtain for every target domain.", "labels": [], "entities": []}, {"text": "A model trained on movie reviews, for instance, will fare very poorly on the task of assessing restaurant or hotel reviews, let alone tweets about politicians.", "labels": [], "entities": []}, {"text": "In this paper, we investigate how extrinsic signals can be incorporated into deep neural networks for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.9754953980445862}]}, {"text": "Numerous papers have found the use of regular pre-trained word vector representations to be beneficial for sentiment analysis (; dos Santos and de C..", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.9839612543582916}]}, {"text": "In our paper, we instead consider word embeddings specifically specialized for the task of sentiment analysis, studying how they can lead to stronger and more consistent gains, despite the fact that the embeddings were obtained using out-of-domain data.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.9539025127887726}]}, {"text": "An intuitive solution would be to concatenate regular embeddings, which provide semantic relatedness cues, with sentiment polarity cues that are captured in additional dimensions.", "labels": [], "entities": []}, {"text": "We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings.", "labels": [], "entities": []}, {"text": "Our empirical study shows that the sentiment embeddings can lead to consistent gains across different datasets in a diverse set of domains and languages if a suitable neural network architecture is used.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now turn to our extensive empirical evaluation, which assesses the effectiveness of our novel architecture with sentiment word vectors.", "labels": [], "entities": []}, {"text": "For evaluation, we use real world datasets for 7 different languages, taken from a range of different sources that cover several domains.", "labels": [], "entities": []}, {"text": "These are summarized in, with ISO 639-3 language codes.", "labels": [], "entities": [{"text": "ISO 639-3 language codes", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.847632959485054}]}, {"text": "In our experimental setup, these are all cast as binary polarity classification tasks, for which we use accuracy as our evaluation metric.", "labels": [], "entities": [{"text": "binary polarity classification", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.7116689483324686}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9988143444061279}]}, {"text": "\u2022 \u2022 From TripAdvisor (TA), we crawled German, Russian, Italian, Czech, and Japanese reviews of restaurants and hotels.", "labels": [], "entities": []}, {"text": "We removed threestar reviews, as these can be regarded as neutral ones, so reviews with a rating < 3 are considered negative, while those with a rating > 3 were deemed positive.", "labels": [], "entities": []}, {"text": "\u2022 The Amazon Fine Food Reviews AFF) dataset provides food reviews left on Amazon.", "labels": [], "entities": [{"text": "Amazon Fine Food Reviews AFF) dataset", "start_pos": 6, "end_pos": 43, "type": "DATASET", "confidence": 0.9465307763644627}]}, {"text": "We chose a random subset of it with preprocessing as for TripAdvisor.", "labels": [], "entities": [{"text": "TripAdvisor", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9677661657333374}]}, {"text": "As there was no test set provided for TripAdvisor or for the Amazon Fine Food Reviews data, we randomly partitioned this data into training, validation, and test splits with a 80%/10%/20% ratio.", "labels": [], "entities": [{"text": "TripAdvisor", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9764764904975891}, {"text": "Amazon Fine Food Reviews data", "start_pos": 61, "end_pos": 90, "type": "DATASET", "confidence": 0.9399521470069885}]}, {"text": "Additionally, 10% of the training sets from SE16-T5 were randomly extracted and reserved for validation, while SST provides its own validation set.", "labels": [], "entities": [{"text": "SST", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.8582956790924072}]}, {"text": "The new datasets are available from http: //gerard.demelo.org/sentiment/.", "labels": [], "entities": []}, {"text": "The standard pre-trained word vectors used for English are the GloVe () ones trained on 840 billion tokens of For our transfer learning approach, our experiments rely on the multi-domain sentiment dataset by, collected from Amazon customers reviews.", "labels": [], "entities": []}, {"text": "This dataset includes 25 categories of products and is used to generate our sentiment embeddings using linear models.", "labels": [], "entities": []}, {"text": "Specifically, we train linear SVMs using scikit-learn to extract word coefficients in each domain and also for the union of all domains together, yielding a 26-dimensional sentiment embedding.", "labels": [], "entities": []}, {"text": "For comparison and analysis, we also consider several alternative forms of infusing external cues.", "labels": [], "entities": []}, {"text": "Firstly, lexicon-driven methods have often been used for domain-independent sentiment analysis.", "labels": [], "entities": [{"text": "domain-independent sentiment analysis", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.6755513747533163}]}, {"text": "We consider a recent sentiment lexicon called VADER ().", "labels": [], "entities": [{"text": "VADER", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.962525486946106}]}, {"text": "The polarity scores assigned to words by the lexicon are taken as the components of a set of 1-dimensional word vectors (dividing the original scores by the difference between max and min polarity scores for normalization).", "labels": [], "entities": []}, {"text": "Secondly, as another particularly strong alternative, we consider the SocialSent Reddit community-specific lexicons mined by the Stanford NLP group ().", "labels": [], "entities": [{"text": "SocialSent Reddit community-specific lexicons mined by the Stanford NLP group", "start_pos": 70, "end_pos": 147, "type": "DATASET", "confidence": 0.8336105108261108}]}, {"text": "These contain separate domain-specific scores for 250 different Reddit communities, and hence result in 250-dimensional embeddings.", "labels": [], "entities": []}, {"text": "For cross-lingual projection, we extract links between words from a 2017 dump of the English edition of Wiktionary.", "labels": [], "entities": [{"text": "cross-lingual projection", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.862783282995224}, {"text": "words from a 2017 dump of the English edition of Wiktionary", "start_pos": 55, "end_pos": 114, "type": "DATASET", "confidence": 0.8016481670466337}]}, {"text": "We restrict the vocabulary link set to include the languages in, mining corresponding translation, synonymy, derivation, and etymological links from Wiktionary.", "labels": [], "entities": []}, {"text": "For CNNs, we make use of the well-known CNN-non-static architecture and hyperparameters proposed by, with a learning rate of 0.0006, obtained by tuning on the validation data.", "labels": [], "entities": []}, {"text": "For our DM-MCNN models, the configuration of the convolutional module is the same as for CNNs, and the remaining hyperparameter values were as well tuned on the validation sets.", "labels": [], "entities": []}, {"text": "An overview of the relevant network parameter values is given in.", "labels": [], "entities": []}, {"text": "For greater efficiency and better convergence properties, the training relies on mini-batches.", "labels": [], "entities": []}, {"text": "Our implementation considers the maximal sentence length in each mini-batch and zero-pads all other sentences to this length under convolutional module, thus enabling uniform and fast processing of each mini-batch.", "labels": [], "entities": []}, {"text": "All neural network architectures are implemented using the PyTorch framework 2 .", "labels": [], "entities": [{"text": "PyTorch framework", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9004871845245361}]}], "tableCaptions": [{"text": " Table 2: DM-MCNN Model Parameter Settings.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy on several different English and non-English datasets from different domains, compar- ing our architecture against CNNs. Rest.: restaurants domain.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9786508083343506}]}, {"text": " Table 4: Accuracy on SST with increasing training sizes", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985149502754211}, {"text": "SST", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9941547513008118}]}]}