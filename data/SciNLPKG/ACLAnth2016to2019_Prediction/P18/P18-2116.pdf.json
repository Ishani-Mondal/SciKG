{"title": [{"text": "Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum", "labels": [], "entities": []}], "abstractContent": [{"text": "LSTMs were introduced to combat vanishing gradients in simple RNNs by augmenting them with gated additive recurrent connections.", "labels": [], "entities": []}, {"text": "We present an alternative view to explain the success of LSTMs: the gates themselves are versatile recurrent models that provide more representational power than previously appreciated.", "labels": [], "entities": []}, {"text": "We do this by decoupling the LSTM's gates from the embedded simple RNN, producing anew class of RNNs where the recurrence computes an element-wise weighted sum of context-independent functions of the input.", "labels": [], "entities": []}, {"text": "Ablations on a range of problems demonstrate that the gating mechanism alone performs as well as an LSTM inmost settings, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.", "labels": [], "entities": []}], "introductionContent": [{"text": "Long short-term memory (LSTM)) has become the de-facto recurrent neural network (RNN) for learning representations of sequences in NLP.", "labels": [], "entities": []}, {"text": "Like simple recurrent neural networks (S-RNNs), LSTMs are able to learn non-linear functions of arbitrary-length input sequences.", "labels": [], "entities": []}, {"text": "However, they also introduce an additional memory cell to mitigate the vanishing gradient problem.", "labels": [], "entities": []}, {"text": "This memory is controlled by a mechanism of gates, whose additive connections allow long-distance dependencies to be learned more easily during backpropagation.", "labels": [], "entities": []}, {"text": "While this view is mathematically accurate, in this paper we argue that it does not provide a complete picture of why LSTMs work in practice.", "labels": [], "entities": []}, {"text": "* The first two authors contributed equally to this paper.", "labels": [], "entities": []}, {"text": "We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previously realized.", "labels": [], "entities": []}, {"text": "To demonstrate this, we first show that LSTMs can be seen as a combination of two recurrent models: (1) an S-RNN, and (2) an element-wise weighted sum of the S-RNN's outputs overtime, which is implicitly computed by the gates.", "labels": [], "entities": []}, {"text": "We hypothesize that, for many practical NLP problems, the weighted sum serves as the main modeling component.", "labels": [], "entities": []}, {"text": "The S-RNN, while theoretically expressive, is in practice only a minor contributor that clouds the mathematical clarity of the model.", "labels": [], "entities": []}, {"text": "By replacing the S-RNN with a context-independent function of the input, we arrive at a much more restricted class of RNNs, where the main recurrence is via the element-wise weighted sums that the gates are computing.", "labels": [], "entities": []}, {"text": "We test our hypothesis on NLP problems, where LSTMs are wildly popular at least in part due to their ability to model crucial phenomena such as word order, syntactic structure (, and even long-range semantic dependencies).", "labels": [], "entities": []}, {"text": "We consider four challenging tasks: language modeling, question answering, dependency parsing, and machine translation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.8095901906490326}, {"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.893625795841217}, {"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8670850992202759}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.82355996966362}]}, {"text": "Experiments show that while removing the gates from an LSTM can severely hurt performance, replacing the S-RNN with a simple linear transformation of the input results in minimal or no loss in model performance.", "labels": [], "entities": []}, {"text": "We also show that, in many cases, LSTMs can be further simplified by removing the output gate, arriving at an even more transparent architecture, where the output is a context-independent function of the weighted sum.", "labels": [], "entities": []}, {"text": "Together, these results suggest that the gates' ability to compute an element-wise weighted sum, rather than the non-linear transition dynamics of S-RNNs, are the driving force behind LSTM's success.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare model performance on four NLP tasks, with an experimental setup that is lenient towards LSTMs and harsh towards its simplifications.", "labels": [], "entities": []}, {"text": "In each case, we use existing implementations and previously reported hyperparameter settings.", "labels": [], "entities": []}, {"text": "Since these settings were tuned for LSTMs, any simplification that performs equally to (or better than) LSTMs under these LSTM-friendly settings provides strong evidence that the ablated component is not a contributing factor.", "labels": [], "entities": []}, {"text": "For each task we also report the mean and standard deviation of 5 runs of the LSTM settings to demonstrate the typical variance observed due to training with different random initializations.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.9550987184047699}]}, {"text": "Language Modeling We evaluate the models on the Penn Treebank (PTB) language modeling benchmark.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7043823599815369}, {"text": "Penn Treebank (PTB) language modeling benchmark", "start_pos": 48, "end_pos": 95, "type": "DATASET", "confidence": 0.9599074944853783}]}, {"text": "We use the implementation of from TensorFlow's tutorial while replacing any invocation of LSTMs with simpler models.", "labels": [], "entities": []}, {"text": "We test two of their configurations: medium and large Dependency Parsing For dependency parsing, we use the Deep Biaffine Dependency Parser, which relies on stacked bidirectional LSTMs to learn contextsensitive word embeddings for determining arcs between a pair of words.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.740625411272049}]}, {"text": "We directly use their released implementation, which is evaluated on the Universal Dependencies English Web Treebank v1.3 ().", "labels": [], "entities": [{"text": "Universal Dependencies English Web Treebank v1.3", "start_pos": 73, "end_pos": 121, "type": "DATASET", "confidence": 0.8615681628386179}]}, {"text": "In our experiments, we use the existing hyperparameters and only replace the LSTMs with the simplified architectures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on language modeling  benchmarks, measured by perplexity.", "labels": [], "entities": []}, {"text": " Table 2: Performance on SQuAD, measured by  exact match (EM) and span overlap (F1).", "labels": [], "entities": [{"text": "exact match (EM)", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.9457884192466736}, {"text": "span overlap (F1)", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.9352335453033447}]}, {"text": " Table 3: Performance on the universal dependen- cies parsing benchmark, measured by unlabeled  (UAS) and labeled attachment score (LAS).", "labels": [], "entities": [{"text": "universal dependen- cies parsing", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.565066260099411}, {"text": "UAS) and labeled attachment score (LAS)", "start_pos": 97, "end_pos": 136, "type": "METRIC", "confidence": 0.7766307791074117}]}, {"text": " Table 4: Performance on the WMT 2016 multi- modal English to German benchmark.", "labels": [], "entities": [{"text": "WMT 2016 multi- modal English to German benchmark", "start_pos": 29, "end_pos": 78, "type": "DATASET", "confidence": 0.6102583441469405}]}]}