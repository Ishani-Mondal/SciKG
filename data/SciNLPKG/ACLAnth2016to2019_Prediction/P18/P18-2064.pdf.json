{"title": [{"text": "Judicious Selection of Training Data in Assisting Language for Multilingual Neural NER", "labels": [], "entities": [{"text": "Judicious Selection of Training", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7958876639604568}, {"text": "Multilingual Neural NER", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.6590768694877625}]}], "abstractContent": [{"text": "Multilingual learning for Neural Named Entity Recognition (NNER) involves jointly training a neural network for multiple languages.", "labels": [], "entities": [{"text": "Neural Named Entity Recognition (NNER)", "start_pos": 26, "end_pos": 64, "type": "TASK", "confidence": 0.7902078458241054}]}, {"text": "Typically, the goal is improving the NER performance of one of the languages (the primary language) using the other assisting languages.", "labels": [], "entities": [{"text": "NER", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9523903131484985}]}, {"text": "We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning.", "labels": [], "entities": []}, {"text": "To alleviate this problem, we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language.", "labels": [], "entities": []}, {"text": "We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data.", "labels": [], "entities": [{"text": "NER", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9839023947715759}]}], "introductionContent": [{"text": "Neural NER trains a deep neural network for the NER task and has become quite popular as they minimize the need for hand-crafted features and, learn feature representations from the training data itself.", "labels": [], "entities": [{"text": "Neural NER", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.6246696412563324}, {"text": "NER task", "start_pos": 48, "end_pos": 56, "type": "TASK", "confidence": 0.8963204622268677}]}, {"text": "Recently, multilingual learning has been shown to benefit Neural NER in a resource-rich language setting (.", "labels": [], "entities": [{"text": "Neural NER", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.7459190487861633}]}, {"text": "Multilingual learning aims to improve the NER performance on the language under consideration (primary language) by adding training data from one or more assisting languages.", "labels": [], "entities": [{"text": "NER", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9792450666427612}]}, {"text": "The neural network is trained on the combined data of the primary (D P ) and the assisting languages (D A ).", "labels": [], "entities": []}, {"text": "The neural network has a combination of languagedependent and language-independent layers, and, the network learns better cross-lingual features via these language-independent layers.", "labels": [], "entities": []}, {"text": "* This work began when the second author was a research scholar at IIT Bombay Existing approaches add all training sentences from the assisting language to the primary language and train the neural network on the combined data.", "labels": [], "entities": []}, {"text": "However, data from assisting languages can introduce adrift in the tag distribution for named entities, since the common named entities from the two languages may have vastly divergent tag distributions.", "labels": [], "entities": []}, {"text": "For example, the entity China appears in training split of Spanish (primary) and English (assisting) with the corresponding tag frequencies, Spanish = { Loc : 20, Org : 49, Misc : 1 } and English = { Loc : 91, Org : 7 }.", "labels": [], "entities": []}, {"text": "By adding English data to Spanish, the tag distribution of China is skewed towards Location entity in Spanish.", "labels": [], "entities": []}, {"text": "This leads to a drop in named entity recognition performance.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6469960908095042}]}, {"text": "In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language.", "labels": [], "entities": [{"text": "tag distribution", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.8329724967479706}]}, {"text": "The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task.", "labels": [], "entities": []}, {"text": "For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7891106605529785}]}, {"text": "For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7545825242996216}]}, {"text": "The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data.", "labels": [], "entities": []}, {"text": "Out-of-domain sentences most similar to the in-domain data are added.", "labels": [], "entities": []}, {"text": "Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named entities in the train- Following are the contributions of the paper: (a) We present a simple approach to select assisting language sentences based on symmetric KLDivergence of overlapping entities (b) We demonstrate the benefits of multilingual Neural NER on low-resource languages.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7193586826324463}]}, {"text": "We compare the proposed data selection approach with monolingual Neural NER system, and the multilingual Neural NER system trained using all assisting language sentences.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, ours is the first work for judiciously selecting a subset of sentences from an assisting language for multilingual Neural NER.", "labels": [], "entities": [{"text": "multilingual Neural NER", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.5820912718772888}]}], "datasetContent": [{"text": "In this section we list the datasets used and the network configurations used in our experiments.", "labels": [], "entities": []}, {"text": "The thereby, allowing sharing of sub-word features across the Indian languages.", "labels": [], "entities": []}, {"text": "For Indian languages, the annotated data followed the IOB format.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F-Score for German and Italian Test data using Monolingual and Multilingual learning strate- gies.  \u2020 indicates that the SKL results are statistically significant compared to adding all assisting language  data with p-value < 0.05 using two-sided Welch t-test.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9949034452438354}, {"text": "German and Italian Test data", "start_pos": 22, "end_pos": 50, "type": "DATASET", "confidence": 0.6196300864219666}]}, {"text": " Table 3: Test set F-Score from monolingual and multilingual learning on Indian languages. Result  from monolingual training on the primary language is underlined.  \u2020 indicates SKL results statistically  significant compared to adding all assisting language data with p-value < 0.05 using two-sided Welch  t-test.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9089545607566833}, {"text": "Result", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9651896357536316}]}]}