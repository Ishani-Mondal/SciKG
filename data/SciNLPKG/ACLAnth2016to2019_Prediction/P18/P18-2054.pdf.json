{"title": [{"text": "Improving Beam Search by Removing Monotonic Constraint for Neural Machine Translation", "labels": [], "entities": [{"text": "Improving Beam Search", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8085334300994873}, {"text": "Neural Machine Translation", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.7349860072135925}]}], "abstractContent": [{"text": "To achieve high translation performance, neural machine translation models usually rely on the beam search algorithm for decoding sentences.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7163445353507996}]}, {"text": "The beam search finds good candidate translations by considering multiple hypotheses of translations simultaneously.", "labels": [], "entities": []}, {"text": "However, as the algorithm searches in a monotonic left-to-right order, a hypothesis cannot be revisited once it is discarded.", "labels": [], "entities": []}, {"text": "We found such monotonicity forces the algorithm to sacrifice some decoding paths to explore new paths.", "labels": [], "entities": []}, {"text": "As a result, the overall quality of the hypotheses selected by the algorithm is lower than expected.", "labels": [], "entities": []}, {"text": "To mitigate this problem, we relax the monotonic constraint of the beam search by maintaining all found hypotheses in a single priority queue and using a universal score function for hypothesis selection.", "labels": [], "entities": [{"text": "hypothesis selection", "start_pos": 183, "end_pos": 203, "type": "TASK", "confidence": 0.8111084401607513}]}, {"text": "The proposed algorithm allows discarded hypotheses to be recovered in a later step.", "labels": [], "entities": []}, {"text": "Despite its simplicity, we show that the proposed decoding algorithm enhances the quality of selected hypotheses and improve the translations even for high-performance models in English-Japanese translation task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation models composed of endto-end neural networks ( are starting to become mainstream.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7711226344108582}]}, {"text": "Essentially, neural machine translation (NMT) models define a probabilistic distribution p(y t |y 1 , ..., y t\u22121 , X) to generate translations.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.8121166030565897}]}, {"text": "During translation phase, new words are sampled from this distribution.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9733510613441467}]}, {"text": "As the search space of possible outputs is incredibly large, we can only afford to explore a limited number of search paths.", "labels": [], "entities": []}, {"text": "In practice, NMT models use the beam search algorithm to generate output sequences in a limited time budget).", "labels": [], "entities": []}, {"text": "Beam search limits the search space by considering only a fixed number of hypotheses (i.e., partial translations) in each step, and predicting next output words only for the selected hypotheses.", "labels": [], "entities": [{"text": "Beam search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8801521956920624}]}, {"text": "The fixed number B is referred to as beam size.", "labels": [], "entities": []}, {"text": "Beam search keeps decoding until B finished translations that end with an end-of-sequence token \"/s\" are found.", "labels": [], "entities": []}, {"text": "Comparing to the greedy search that only considers the best hypothesis in each step, beam search can find a good candidate translation that suffers in a middle step.", "labels": [], "entities": []}, {"text": "Generally, using beam search can improve the quality of outputs over the greedy search.", "labels": [], "entities": []}, {"text": "However, we found that the hard restriction of hypothesis selection imposed by the beam search affects the quality of the decoding paths negatively.", "labels": [], "entities": []}, {"text": "We can think the decoding process of an NMT model as solving a pathfinding problem, where we search for an optimal path starts from \"s\" and ends at \"/s\".", "labels": [], "entities": []}, {"text": "For any pathfinding algorithm, a certain amount of exploration is crucial for making sure that the algorithm is following aright path.", "labels": [], "entities": []}, {"text": "For beam search, since the beam size is fixed, it must give up some currently searching paths in order to explore new paths.", "labels": [], "entities": [{"text": "beam search", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9537796974182129}]}, {"text": "The problem has a similar flavor as the exploration-exploitation dilemma in reinforcement learning.", "labels": [], "entities": []}, {"text": "As the beam search decodes in left-to-right order monotonically, a discarded decoding path cannot be recovered later.", "labels": [], "entities": []}, {"text": "As the decoding algorithm is essentially driven by a language model, an output with high probability (local score) is not guaranteed to have high scores for future predictions.", "labels": [], "entities": []}, {"text": "Beam search can be trapped by such a high-confidence output.", "labels": [], "entities": [{"text": "Beam search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8274630010128021}]}, {"text": "This is there 6 tree 5 1 fruit here (a) beam search (b) SQD: An intuitive comparison between beam search and single-queue decoding (SQD) with abeam size of 2.", "labels": [], "entities": []}, {"text": "In each step, two selected hypotheses (solid boxes) and one immediately discarded hypothesis (dashed boxes) are shown in the In the top right of selected hypotheses, the step numbers when they are selected are marked.", "labels": [], "entities": []}, {"text": "The hypothesis \"an apple tree\" is discarded in step 3 in both algorithms.", "labels": [], "entities": []}, {"text": "Comparing to beam search, SQD is able to recover this hypothesis in step 4 when other hypotheses have worse scores.", "labels": [], "entities": [{"text": "beam search", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.8732747733592987}, {"text": "SQD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.7839035391807556}]}, {"text": "sue is more severe for language pairs that are not well aligned.", "labels": [], "entities": [{"text": "sue", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9496827721595764}]}, {"text": "One solution is to predict the expected future scores, which is considerably difficult.", "labels": [], "entities": []}, {"text": "Another workaround for this problem is to enable the algorithm to revisit a previous hypothesis when the quality of current ones degrades.", "labels": [], "entities": []}, {"text": "In this work, we extend the beam search to introduce more flexibility.", "labels": [], "entities": [{"text": "beam search", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9150527715682983}]}, {"text": "We manage all found hypotheses in a single priority queue so that they can be selected later when necessary.", "labels": [], "entities": []}, {"text": "Based on a universal score function, the hypotheses with highest scores are selected to be expanded.", "labels": [], "entities": []}, {"text": "The proposed algorithm is referred to as single-queue decoding (SQD) in this paper.", "labels": [], "entities": [{"text": "single-queue decoding (SQD)", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6849813044071198}]}, {"text": "As the priority queue can contain massive hypotheses, we design two auxiliary score functions to help the algorithm select proper candidates.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed algorithm is able to improve the quality of selected hypotheses and thus results in better performance in EnglishJapanese translation task.", "labels": [], "entities": [{"text": "EnglishJapanese translation task", "start_pos": 141, "end_pos": 173, "type": "TASK", "confidence": 0.7590556442737579}]}], "datasetContent": [{"text": "We evaluate the proposed decoding algorithm mainly with an off-the-shelf NMT model), which has a bi-directional LSTM encoder and a single-layer LSTM decoder.", "labels": [], "entities": []}, {"text": "The embeddings and LSTM layers have a size of 1000.", "labels": [], "entities": []}, {"text": "We evaluate the algorithms on AS-PEC English-Japanese translation task ().", "labels": [], "entities": [{"text": "AS-PEC English-Japanese translation task", "start_pos": 30, "end_pos": 70, "type": "TASK", "confidence": 0.761223629117012}]}, {"text": "The vocabulary contains 80k words for English side and 40k words for the Japanese side.", "labels": [], "entities": []}, {"text": "We report BLEU score based on a standard post-processing procedure . All NMT models in this work are trained with Nesterov's accelerated gradient with an initial learning rate of 0.25.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9757561981678009}]}, {"text": "The learning rate is decreased by a factor of 10 if no improvement is observed in 20k iterations.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9544926881790161}]}, {"text": "The training ends after the learning rate is annealed for three times.", "labels": [], "entities": []}, {"text": "The models are trained on 4 GPUs; each GPU computes the gradient of a mini-batch.", "labels": [], "entities": []}, {"text": "The gradients are averaged and distributed to each GPU using the nccl framework.", "labels": [], "entities": []}, {"text": "The hyperparameters of the decoding algorithms are tuned by Bayesian optimization) on a small validation set composed of 500 sentences.", "labels": [], "entities": []}, {"text": "We utilize the \"bayes opt\" package for Bayesian optimization.", "labels": [], "entities": [{"text": "Bayesian optimization", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7679560482501984}]}, {"text": "We use the default acquisition function \"ucb\" with a \u03ba value of 5.", "labels": [], "entities": []}, {"text": "We first explore 50 initial points, then optimize for another 50 iterations.", "labels": [], "entities": []}, {"text": "We allow the decoding algorithms to run fora maximum of 150 steps.", "labels": [], "entities": []}, {"text": "If the algorithm fails to find a finished translation in the limited steps, an empty translation is outputted.", "labels": [], "entities": []}, {"text": "In order to see whether the performance gain can be generalized to deeper models, we train a large NMT model with two layers of LSTM decoders.", "labels": [], "entities": []}, {"text": "We apply residual connection ( to the decoder states.", "labels": [], "entities": []}, {"text": "Before the softmax layer, an additional fully-connected layer with 600 hidden units is applied.", "labels": [], "entities": []}, {"text": "For the attention mechanism, we use a variant of the key-value attention, where the keys are computed by a linear transformation of the encoder states, the queries of the attention are the sum of the feedback word embeddings and the LSTM states of the first decoder.", "labels": [], "entities": []}, {"text": "Dropout () is applied everywhere after non-recurrent layers with a dropping rate of 0.2.", "labels": [], "entities": []}, {"text": "To further enhance the model performance, we use byte pair encoding) with a coding size of 40k to segment the sentences of the training data into subwords.", "labels": [], "entities": []}, {"text": "The experiment results are shown in.", "labels": [], "entities": []}, {"text": "By applying various techniques, the NMT model achieves high single-model BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9933120608329773}]}, {"text": "The results indicate that SQD is still effective with a high-performance NMT model.", "labels": [], "entities": [{"text": "SQD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.7901827096939087}]}, {"text": "The proposed algorithm is more effective with a small beam size.", "labels": [], "entities": []}, {"text": "For this model, the contribution of length matching penalty is only beneficial when the beam size is smaller than 8, which maybe a side-effect of applying byte pair encoding (BPE).", "labels": [], "entities": []}, {"text": "As it is more difficult to correctly predict the number of output tokens in sub-word level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results using a baseline model  with a beam size of 5", "labels": [], "entities": []}]}