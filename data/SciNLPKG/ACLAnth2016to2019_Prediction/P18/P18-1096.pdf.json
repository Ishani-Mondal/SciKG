{"title": [{"text": "Strong Baselines for Neural Semi-Supervised Learning under Domain Shift", "labels": [], "entities": []}], "abstractContent": [{"text": "Novel neural models have been proposed in recent years for learning under domain shift.", "labels": [], "entities": []}, {"text": "Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult.", "labels": [], "entities": []}, {"text": "In this paper , we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training.", "labels": [], "entities": []}, {"text": "Extensive experiments on two benchmarks are negative: while our novel method establishes anew state-of-the-art for sentiment analysis, it does not fare consistently the best.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.9708389937877655}]}, {"text": "More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions , outperforms the state of the art.", "labels": [], "entities": []}, {"text": "We conclude that classic approaches constitute an important and strong baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing, named entity recognition (, and semantic role labeling (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.8277789652347565}, {"text": "named entity recognition", "start_pos": 167, "end_pos": 191, "type": "TASK", "confidence": 0.6057357887427012}, {"text": "semantic role labeling", "start_pos": 199, "end_pos": 221, "type": "TASK", "confidence": 0.6384961406389872}]}, {"text": "In contrast, learning from unlabeled data, especially under domain shift, remains a challenge.", "labels": [], "entities": []}, {"text": "This is common in many real-world applications where the distribution of the training and test data differs.", "labels": [], "entities": []}, {"text": "Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words; or distributional features which do not generalize to other tasks.", "labels": [], "entities": []}, {"text": "Other approaches that are in theory more general only evaluate on proprietary datasets () or on a single benchmark (, which carries the risk of overfitting to the task.", "labels": [], "entities": []}, {"text": "In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature).", "labels": [], "entities": []}, {"text": "In this work, we make the argument that such algorithms make strong baselines for any task inline with recent efforts highlighting the usefulness of classic approaches.", "labels": [], "entities": []}, {"text": "We re-evaluate bootstrapping algorithms in the context of DNNs.", "labels": [], "entities": []}, {"text": "These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily-with a few additions-with the current generation of NLP models.", "labels": [], "entities": []}, {"text": "Many of these methods, though, were originally developed with in-domain performance in mind, so their effectiveness in a domain adaptation setting remains unexplored.", "labels": [], "entities": []}, {"text": "In particular, we re-evaluate three traditional bootstrapping methods, self-training, tri-training (, and tritraining with disagreement for neural network-based approaches on two NLP tasks with different characteristics, namely, a sequence prediction and a classification task (POS tagging and sentiment analysis).", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 231, "end_pos": 250, "type": "TASK", "confidence": 0.7024634778499603}, {"text": "POS tagging", "start_pos": 278, "end_pos": 289, "type": "TASK", "confidence": 0.66239795088768}]}, {"text": "We evaluate the methods across multiple domains on two wellestablished benchmarks, without taking any further task-specific measures, and compare to the best results published in the literature.", "labels": [], "entities": []}, {"text": "We make the somewhat surprising observation that classic tri-training outperforms task-agnostic state-of-the-art semi-supervised learning and recent neural adaptation approaches ().", "labels": [], "entities": []}, {"text": "In addition, we propose multi-task tri-training, which reduces the main deficiency of tri-training, namely its time and space complexity.", "labels": [], "entities": []}, {"text": "It establishes anew state of the art on unsupervised domain adaptation for sentiment analysis but it is outperformed by classic tri-training for POS tagging.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9736188650131226}, {"text": "POS tagging", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.8419803380966187}]}, {"text": "Contributions Our contributions are: a) We propose a novel multi-task tri-training method.", "labels": [], "entities": []}, {"text": "b) We show that tri-training can serve as a strong and robust semi-supervised learning baseline for the current generation of NLP models.", "labels": [], "entities": []}, {"text": "c) We perform an extensive evaluation of bootstrapping 1 algorithms compared to state-of-the-art approaches on two benchmark datasets.", "labels": [], "entities": []}, {"text": "d) We shed light on the task and data characteristics that yield the best performance for each model.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to ascertain which methods are robust across different domains, we evaluate on two widely used unsupervised domain adaptation datasets for two tasks, a sequence labeling and a classification task, cf. for data statistics.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.6228384673595428}]}], "tableCaptions": [{"text": " Table 1: Number of labeled and unlabeled sen- tences for each domain in the SANCL 2012 dataset  (Petrov and McDonald, 2012) for POS tagging  (above) and the Amazon Reviews dataset (Blitzer  et al., 2006) for sentiment analysis (below).", "labels": [], "entities": [{"text": "SANCL 2012 dataset", "start_pos": 77, "end_pos": 95, "type": "DATASET", "confidence": 0.9195343653361002}, {"text": "POS tagging", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.7844660878181458}, {"text": "Amazon Reviews dataset", "start_pos": 158, "end_pos": 180, "type": "DATASET", "confidence": 0.9523107409477234}, {"text": "sentiment analysis", "start_pos": 209, "end_pos": 227, "type": "TASK", "confidence": 0.9539580941200256}]}, {"text": " Table 2: Average accuracy scores for each SA tar- get domain. *: result from Saito et al. (2017).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9796662926673889}]}, {"text": " Table 3: Accuracy scores on dev set of target domain for POS tagging for 10% labeled data. Avg: average  over the 5 SANCL domains. Hyperparameter ep (epochs) is tuned on Answers dev. \u00b5 pseudo : average  amount of added pseudo-labeled data. FLORS: results for Batch (u:big) from (Yin et al., 2015) (see  \u00a73).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.8181018531322479}, {"text": "Avg", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9950183629989624}, {"text": "Hyperparameter ep (epochs)", "start_pos": 132, "end_pos": 158, "type": "METRIC", "confidence": 0.9058202266693115}, {"text": "FLORS", "start_pos": 241, "end_pos": 246, "type": "METRIC", "confidence": 0.9965601563453674}]}, {"text": " Table 4: Accuracy for POS tagging on the dev and test sets of the SANCL domains, models trained on  full source data setup. Values for methods with * are from (", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9904552102088928}, {"text": "POS tagging", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.858471006155014}, {"text": "SANCL domains", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.8532538115978241}]}, {"text": " Table 5: Accuracy scores on dev sets for OOV and  unknown word-tag (UWT) tokens.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9979299306869507}]}]}