{"title": [{"text": "Backpropagating through Structured Argmax using a SPIGOT", "labels": [], "entities": [{"text": "SPIGOT", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.5093338489532471}]}], "abstractContent": [{"text": "We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), anew method for backpropagating through neural networks that include hard-decision struc-tured predictions (e.g., parsing) in intermediate layers.", "labels": [], "entities": [{"text": "structured projection of intermediate gradients optimization", "start_pos": 17, "end_pos": 77, "type": "TASK", "confidence": 0.5833401878674825}]}, {"text": "SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017).", "labels": [], "entities": [{"text": "SPIGOT", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.7239965796470642}]}, {"text": "Like so-called straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondif-ferentiable operations, allowing backprop-agation before and after them; SPIGOT's proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed.", "labels": [], "entities": []}, {"text": "We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification.", "labels": [], "entities": [{"text": "syntactic-then-semantic dependency parsing", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.6730034053325653}, {"text": "semantic parsing", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7126214057207108}, {"text": "sentiment classification", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.827170342206955}]}, {"text": "We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching anew state of the art on semantic dependency parsing.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 214, "end_pos": 241, "type": "TASK", "confidence": 0.7036430835723877}]}], "introductionContent": [{"text": "Learning methods for natural language processing are increasingly dominated by end-to-end differentiable functions that can be trained using gradient-based optimization.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6614011923472086}]}, {"text": "Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a phrase-structure or dependency tree, then semantically analyzed.", "labels": [], "entities": []}, {"text": "Pipelines, which make \"hard\" (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing.", "labels": [], "entities": []}, {"text": "Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing (, we argue that pipelines can be treated as layers in neural architectures for NLP tasks.", "labels": [], "entities": []}, {"text": "Several solutions are readily available: \u2022 Reinforcement learning (most notably the REINFORCE algorithm;, and structured attention (SA;).", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.8329689204692841}, {"text": "REINFORCE", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9556621313095093}]}, {"text": "These methods replace argmax with a sampling or marginalization operation.", "labels": [], "entities": [{"text": "argmax", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9787713885307312}]}, {"text": "We note two potential downsides of these approaches: (i) not all argmax-able operations have corresponding sampling or marginalization methods that are efficient, and (ii) inspection of intermediate outputs, which could benefit error analysis and system improvement, is more straightforward for hard decisions than for posteriors.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 228, "end_pos": 242, "type": "TASK", "confidence": 0.6609856635332108}]}, {"text": "\u2022 The straight-through estimator (STE; Hinton, 2012) treats discrete decisions as if they were differentiable and simply passes through gradients.", "labels": [], "entities": [{"text": "STE; Hinton, 2012", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.651510339975357}]}, {"text": "While fast and surprisingly effective, it ignores constraints on the argmax problem, such as the requirement that every word has exactly one syntactic parent.", "labels": [], "entities": []}, {"text": "We will find, experimentally, that the quality of intermediate representations degrades substantially under STE.", "labels": [], "entities": [{"text": "STE", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.5995725989341736}]}, {"text": "This paper introduces anew method, the structured projection of intermediate gradients optimization technique (SPIGOT; \u00a72), which defines a proxy for the gradient of a loss function with respect to the input to argmax.", "labels": [], "entities": [{"text": "structured projection of intermediate gradients optimization", "start_pos": 39, "end_pos": 99, "type": "TASK", "confidence": 0.5941406140724818}]}, {"text": "Unlike STE's gradient proxy, SPIGOT aims to respect the constraints in the argmax problem.", "labels": [], "entities": [{"text": "STE", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.561700165271759}]}, {"text": "SPIGOT can be applied with any intermediate layer that is expressible as a constrained maximization problem, and whose feasible set can be projected onto.", "labels": [], "entities": []}, {"text": "We show empirically that SPIGOT works even when the maximization and the projection are done approximately.", "labels": [], "entities": [{"text": "SPIGOT", "start_pos": 25, "end_pos": 31, "type": "TASK", "confidence": 0.6083866953849792}]}, {"text": "We offer two concrete architectures that employ structured argmax as an intermediate layer: semantic parsing with syntactic parsing in the middle, and sentiment analysis with semantic parsing in the middle ( \u00a73).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7328707873821259}, {"text": "sentiment analysis", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.9096737504005432}]}, {"text": "These architectures are trained using a joint objective, with one part using data for the intermediate task, and the other using data for the end task.", "labels": [], "entities": []}, {"text": "The datasets are not assumed to overlap at all, but the parameters for the intermediate task are affected by both parts of the training data.", "labels": [], "entities": []}, {"text": "Our experiments ( \u00a74) show that our architecture improves over a state-of-the-art semantic dependency parser, and that SPIGOT offers stronger performance than a pipeline, SA, and STE.", "labels": [], "entities": [{"text": "STE", "start_pos": 179, "end_pos": 182, "type": "METRIC", "confidence": 0.7470798492431641}]}, {"text": "On sentiment classification, we show that semantic parsing offers improvement over a BiLSTM, more so with SPIGOT than with alternatives.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.9624523818492889}, {"text": "semantic parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7880875468254089}]}, {"text": "Our analysis considers how the behavior of the intermediate parser is affected by the end task ( \u00a75).", "labels": [], "entities": []}, {"text": "Our code is open-source and available at https:// github.com/Noahs-ARK/SPIGOT.", "labels": [], "entities": []}], "datasetContent": [{"text": "We empirically evaluate our method with two sets of experiments: using syntactic tree structures in semantic dependency parsing, and using semantic dependency graphs in sentiment classification.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6540723741054535}, {"text": "sentiment classification", "start_pos": 169, "end_pos": 193, "type": "TASK", "confidence": 0.9038232564926147}]}, {"text": "For semantic dependencies, we use the DM dataset introduced in \u00a74.1.2.", "labels": [], "entities": [{"text": "DM dataset", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.7852177023887634}]}, {"text": "We consider a binary classification task using the Stanford Sentiment Treebank (.", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.7440970738728842}, {"text": "Stanford Sentiment Treebank", "start_pos": 51, "end_pos": 78, "type": "DATASET", "confidence": 0.9317437807718912}]}, {"text": "It consists of roughly 10K movie review sentences from Rotten Tomatoes.", "labels": [], "entities": []}, {"text": "The full dataset includes a rating on a scale from 1 to 5 for each constituent (including the full sentences), resulting in more than 200K instances.", "labels": [], "entities": []}, {"text": "Following previous work, we only use full-sentence  instances, with neutral instances excluded (3s) and the remaining four rating levels converted to binary \"positive\" or \"negative\" labels.", "labels": [], "entities": []}, {"text": "This results in a 6,920/872/1,821 train/dev./test split.", "labels": [], "entities": []}, {"text": "compares our SPIGOT method to three baselines.", "labels": [], "entities": []}, {"text": "Pipelined semantic dependency predictions brings 0.9% absolute improvement in classification accuracy, and SPIGOT outperforms all baselines.", "labels": [], "entities": [{"text": "classification", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.9352024793624878}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9558013677597046}, {"text": "SPIGOT", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.6687541007995605}]}, {"text": "In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE.", "labels": [], "entities": [{"text": "STE", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.941274881362915}]}], "tableCaptions": [{"text": " Table 1: Semantic dependency parsing perfor- mance in both unlabeled (UF ) and labeled (LF )  F 1 scores. Bold font indicates the best perfor- mance. Peng et al. (2017) does not report UF .", "labels": [], "entities": [{"text": "Semantic dependency parsing", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7820871671040853}, {"text": "F 1 scores", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9071887930234274}, {"text": "UF", "start_pos": 186, "end_pos": 188, "type": "METRIC", "confidence": 0.803534984588623}]}, {"text": " Table 2: Test accuracy of sentiment classification  on Stanford Sentiment Treebank. Bold font indi- cates the best performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9990742206573486}, {"text": "sentiment classification", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.9427239000797272}, {"text": "Stanford Sentiment Treebank", "start_pos": 56, "end_pos": 83, "type": "DATASET", "confidence": 0.9526009758313497}]}]}