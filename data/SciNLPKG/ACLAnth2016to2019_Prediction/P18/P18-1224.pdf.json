{"title": [{"text": "Neural Natural Language Inference Models Enhanced with External Knowledge", "labels": [], "entities": [{"text": "Neural Natural Language Inference", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7192337810993195}]}], "abstractContent": [{"text": "Modeling natural language inference is a very challenging task.", "labels": [], "entities": [{"text": "Modeling natural language inference", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8494058698415756}]}, {"text": "With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data?", "labels": [], "entities": []}, {"text": "If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it?", "labels": [], "entities": []}, {"text": "In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge.", "labels": [], "entities": []}, {"text": "We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.8601500391960144}, {"text": "MultiNLI datasets", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.9270946383476257}]}], "introductionContent": [{"text": "Reasoning and inference are central to both human and artificial intelligence.", "labels": [], "entities": []}, {"text": "Natural language inference (NLI), also known as recognizing textual entailment (RTE), is an important NLP problem concerned with determining inferential relationship (e.g., entailment, contradiction, or neutral) between a premise p and a hypothesis h.", "labels": [], "entities": [{"text": "Natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7676095068454742}, {"text": "recognizing textual entailment (RTE)", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.7097437083721161}]}, {"text": "In general, modeling informal inference in language is a very challenging and basic problem towards achieving true natural language understanding.", "labels": [], "entities": []}, {"text": "In the last several years, larger annotated datasets were made available, e.g., the SNLI ( and MultiNLI datasets (, which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI.", "labels": [], "entities": [{"text": "MultiNLI datasets", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.901238888502121}]}, {"text": "Such models have shown to achieve the state-of-the-art performance (.", "labels": [], "entities": []}, {"text": "While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data.", "labels": [], "entities": []}, {"text": "In this paper, we relax this assumption and explore whether external knowledge can further help NLI.", "labels": [], "entities": [{"text": "NLI", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9099554419517517}]}, {"text": "Consider an example: \u2022 p: A lady standing in a wheat field.", "labels": [], "entities": []}, {"text": "\u2022 h: A person standing in acorn field.", "labels": [], "entities": []}, {"text": "In this simplified example, when computers are asked to predict the relation between these two sentences and if training data do not provide the knowledge of relationship between \"wheat\" and \"corn\" (e.g., if one of the two words does not appear in the training data or they are not paired in any premise-hypothesis pairs), it will be hard for computers to correctly recognize that the premise contradicts the hypothesis.", "labels": [], "entities": []}, {"text": "In general, although in many tasks learning tabula rasa achieved state-of-the-art performance, we believe complicated NLP problems such as NLI could benefit from leveraging knowledge accumulated by humans, particularly in a foreseeable future when machines are unable to learn it by themselves.", "labels": [], "entities": [{"text": "tabula rasa", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.6770927906036377}]}, {"text": "In this paper we enrich neural-network-based NLI models with external knowledge in coattention, local inference collection, and inference composition components.", "labels": [], "entities": []}, {"text": "We show the proposed model improves the state-of-the-art NLI models to achieve better performances on the SNLI and MultiNLI datasets.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.8675651550292969}, {"text": "MultiNLI datasets", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.9149110615253448}]}, {"text": "The advantage of using external knowledge is more significant when the size of training data is restricted, suggesting that if more knowledge can be obtained, it may bring more benefit.", "labels": [], "entities": []}, {"text": "In addition to attaining the state-of-theart performance, we are also interested in understanding how external knowledge contributes to the major components of typical neural-networkbased NLI models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use Stanford Natural Language Inference (SNLI) dataset and Multi-Genre Natural Language Inference (MultiNLI) () dataset, which focus on three basic relations between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral).", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 27, "end_pos": 77, "type": "DATASET", "confidence": 0.5728660151362419}]}, {"text": "We use the same data split as in previous work and classification accuracy as the evaluation metric.", "labels": [], "entities": [{"text": "classification", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.9448699951171875}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9664504528045654}]}, {"text": "In addition, we test our models (trained on the SNLI training set) on anew test set (, which assesses the lexical inference abilities of NLI systems and consists of 8,193 samples.", "labels": [], "entities": [{"text": "SNLI training set", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.9060105681419373}]}, {"text": "WordNet 3.0 is used to extract semantic relation features between words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9401239156723022}]}, {"text": "The words are lemmatized using Stanford CoreNLP 3.7.0 ( ).", "labels": [], "entities": [{"text": "Stanford CoreNLP 3.7.0", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.9495865106582642}]}, {"text": "The premise and the hypothesis sentences fed into the input encoding layer are tokenized.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of lexical relation features.", "labels": [], "entities": []}, {"text": " Table 2: Accuracies of models on SNLI.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9939113855361938}, {"text": "SNLI", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6189199090003967}]}, {"text": " Table 3: Accuracies of models on MultiNLI. * in- dicates models using extra SNLI training set.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9908497333526611}, {"text": "MultiNLI", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9008045196533203}]}, {"text": " Table 4: Accuracies of models on the SNLI and  (Glockner et al., 2018) test set. * indicates the re- sults taken from (Glockner et al., 2018).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9978047013282776}, {"text": "SNLI and  (Glockner et al., 2018) test set", "start_pos": 38, "end_pos": 80, "type": "DATASET", "confidence": 0.8189930482344194}]}, {"text": " Table 5: The number of instances and accu- racy per category achieved by ESIM and KIM on  the (Glockner et al., 2018) test set.", "labels": [], "entities": [{"text": "accu- racy", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9238746960957845}, {"text": "Glockner et al., 2018) test set", "start_pos": 96, "end_pos": 127, "type": "DATASET", "confidence": 0.7904549241065979}]}, {"text": " Table 6: Detailed Analysis on MultiNLI.", "labels": [], "entities": [{"text": "Detailed Analysis", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8079898655414581}, {"text": "MultiNLI", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.8705523610115051}]}]}