{"title": [{"text": "Are BLEU and Meaning Representation in Opposition?", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9566715359687805}, {"text": "Meaning Representation", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.9204469621181488}]}], "abstractContent": [{"text": "One of possible ways of obtaining continuous space sentence representations is by training neural machine translation (NMT) systems.", "labels": [], "entities": []}, {"text": "The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted.", "labels": [], "entities": []}, {"text": "We propose several variations of the attentive NMT architecture bringing this meeting point back.", "labels": [], "entities": []}, {"text": "Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep learning has brought the possibility of automatically learning continuous representations of sentences.", "labels": [], "entities": []}, {"text": "On the one hand, such representations can be geared towards particular tasks such as classifying the sentence in various aspects (e.g. sentiment, register, question type) or relating the sentence to other sentences (e.g. semantic similarity, paraphrasing, entailment).", "labels": [], "entities": []}, {"text": "On the other hand, we can aim at \"universal\" sentence representations, that is representations performing reasonably well in a range of such tasks.", "labels": [], "entities": []}, {"text": "Regardless the evaluation criterion, the representations can be learned either in an unsupervised way (from simple, unannotated texts) or supervised, relying on manually constructed training sets of sentences equipped with annotations of the appropriate type.", "labels": [], "entities": []}, {"text": "A different approach is to obtain sentence representations from training neural machine translation models (.", "labels": [], "entities": [{"text": "training neural machine translation", "start_pos": 64, "end_pos": 99, "type": "TASK", "confidence": 0.6717252433300018}]}, {"text": "Since, NMT has seen substantial advances in translation quality and it is thus natural to ask how these improvements affect the learned representations.", "labels": [], "entities": []}, {"text": "One of the key technological changes was the introduction of \"attention\" ( ), making it even the very central component in the network.", "labels": [], "entities": []}, {"text": "Attention allows the NMT system to dynamically choose which parts of the source are most important when deciding on the current output token.", "labels": [], "entities": []}, {"text": "As a consequence, there is no longer a static vector representation of the sentence available in the system.", "labels": [], "entities": []}, {"text": "In this paper, we remove this limitation by proposing a novel encoder-decoder architecture with a structured fixed-size representation of the input that still allows the decoder to explicitly focus on different parts of the input.", "labels": [], "entities": []}, {"text": "In other words, our NMT system has both the capacity to attend to various parts of the input and to produce static representations of input sentences.", "labels": [], "entities": []}, {"text": "We train this architecture on English-to-German and English-to-Czech translation and evaluate the learned representations of English on a wide range of tasks in order to assess its performance in learning \"universal\" meaning representations.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly review recent efforts in obtaining sentence representations.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce a number of variants of our novel architecture.", "labels": [], "entities": []}, {"text": "Section 4 describes some standard and our own methods for evaluating sentence representations.", "labels": [], "entities": [{"text": "evaluating sentence representations", "start_pos": 58, "end_pos": 93, "type": "TASK", "confidence": 0.6059098045031229}]}, {"text": "Section 5 then provides experimental results: translation and representation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9820082187652588}]}, {"text": "The relation between the two is discussed in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8366472721099854}]}], "datasetContent": [{"text": "Continuous sentence representations can be evaluated in many ways, see e.g., or the RepEval workshops.", "labels": [], "entities": []}, {"text": "We evaluate our learned representations with classification and similarity tasks from SentEval (Section 4.1) and by examining clusters of sentence paraphrase representations (Section 4.2).", "labels": [], "entities": []}, {"text": "We trained English-to-German and English-toCzech NMT models using Neural Monkey 4 (Helcl and Libovick\u00b4yLibovick\u00b4y, 2017a).", "labels": [], "entities": []}, {"text": "In the following, we distinguish these models using the code of the target language, i.e. de or cs.", "labels": [], "entities": []}, {"text": "The de models were trained on the Multi30K multilingual image caption dataset (Elliott et al., https://github.com/ufal/neuralmonkey 2016), extended by Helcl and Libovick\u00b4yLibovick\u00b4y (2017b), who acquired additional parallel data using backtranslation () and perplexitybased selection (.", "labels": [], "entities": [{"text": "Multi30K multilingual image caption", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.5761799737811089}]}, {"text": "This extended dataset contains 410k sentence pairs, with the average sentence length of 12 \u00b1 4 tokens in English.", "labels": [], "entities": []}, {"text": "We train each model for 20 epochs with the batch size of 32.", "labels": [], "entities": []}, {"text": "We truecased the training data as well as all data we evaluate on.", "labels": [], "entities": []}, {"text": "For German, we employed Neural Monkey's reversible pre-processing scheme, which expands contractions and performs morphological segmentation of determiners.", "labels": [], "entities": []}, {"text": "We used a vocabulary of at most 30k tokens for each language (no subword units).", "labels": [], "entities": []}, {"text": "The cs models were trained on CzEng 1.7 (Bojar et al., 2016).", "labels": [], "entities": [{"text": "CzEng 1.7", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.9240607619285583}]}, {"text": "We used byte-pair encoding (BPE) with a vocabulary of 30k sub-word units, shared for both languages.", "labels": [], "entities": []}, {"text": "For English, the average sentence length is 15 \u00b1 19 BPE tokens and the original vocabulary size is 1.9M.", "labels": [], "entities": [{"text": "BPE", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9875049591064453}]}, {"text": "We performed 1 training epoch with the batch size of 128 on the entire training section (57M sentence pairs).", "labels": [], "entities": []}, {"text": "The datasets for both de and cs models come with their respective development and test sets of sentence pairs, which we use for the evaluation of translation quality.", "labels": [], "entities": []}, {"text": "(We use 1k randomly selected sentence pairs from CzEng 1.7 dtest as a development set. For evaluation, we use the entire etest.)", "labels": [], "entities": []}, {"text": "We also evaluate the InferSent model 6 (Conneau et al., 2017) as pre-trained on the natural language inference (NLI) task.", "labels": [], "entities": []}, {"text": "InferSent has been shown to achieve state-of-the-art results on the SentEval tasks.", "labels": [], "entities": []}, {"text": "We also include a bag-ofwords baseline (GloVe-BOW) obtained by averaging GloVe 7 word vectors ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: SentEval classification tasks. Tasks without a test set use 10-fold cross-validation.", "labels": [], "entities": [{"text": "SentEval classification tasks", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.9261499245961508}]}, {"text": " Table 4: Translation quality of de models.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9580745100975037}]}, {"text": " Table 5: Translation quality of cs models.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9653392434120178}]}, {"text": " Table 6: Abridged SentEval and paraphrase evaluation results. Full results in supplementary material.", "labels": [], "entities": [{"text": "Abridged", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9791865348815918}]}, {"text": " Table 7: Comparison of state-of-the-art SentEval results with our best models and the Glove-BOW base- line. \"H.\" is the number of attention heads. Reprinted results are marked with  \u2020, others are our measure- ments.", "labels": [], "entities": [{"text": "Glove-BOW base- line", "start_pos": 87, "end_pos": 107, "type": "METRIC", "confidence": 0.7233074009418488}]}]}