{"title": [{"text": "Sparse and Constrained Attention for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.7514590422312418}]}], "abstractContent": [{"text": "In NMT, words are sometimes dropped from the source or generated repeatedly in the translation.", "labels": [], "entities": []}, {"text": "We explore novel strategies to address the coverage problem that change only the attention transformation.", "labels": [], "entities": [{"text": "attention transformation", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.6814229935407639}]}, {"text": "Our approach allocates fertilities to source words, used to bound the attention each word can receive.", "labels": [], "entities": []}, {"text": "We experiment with various sparse and constrained attention transformations and propose anew one, constrained sparsemax, shown to be differ-entiable and sparse.", "labels": [], "entities": []}, {"text": "Empirical evaluation is provided in three languages pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) emerged in the last few years as a very successful paradigm.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.833428810040156}]}, {"text": "While NMT is generally more fluent than previous statistical systems, adequacy is still a major concern (: common mistakes include dropping source words and repeating words in the generated translation.", "labels": [], "entities": []}, {"text": "Previous work has attempted to mitigate this problem in various ways.", "labels": [], "entities": []}, {"text": "incorporate coverage and length penalties during beam search-a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam.", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9936999082565308}, {"text": "length", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9518222212791443}, {"text": "beam search-a", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.8730451166629791}]}, {"text": "Other approaches involve architectural changes: providing coverage vectors to track the attention history (, using gating architectures and adaptive attention to control the amount of source context provided (, or adding a reconstruction loss ().", "labels": [], "entities": []}, {"text": "also use the notion of fertility * Work done during an internship at implicitly in their proposed model.", "labels": [], "entities": []}, {"text": "Their fertility conditioned decoder uses a coverage vector and an extract gate which are incorporated in the decoding recurrent unit, increasing the number of parameters.", "labels": [], "entities": []}, {"text": "In this paper, we propose a different solution that does not change the overall architecture, but only the attention transformation.", "labels": [], "entities": [{"text": "attention transformation", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.7098205834627151}]}, {"text": "Namely, we replace the traditional softmax by other recently proposed transformations that either promote attention sparsity or upper bound the amount of attention a word can receive.", "labels": [], "entities": []}, {"text": "The bounds are determined by the fertility values of the source words.", "labels": [], "entities": []}, {"text": "While these transformations have given encouraging results in various NLP problems, they have never been applied to NMT, to the best of our knowledge.", "labels": [], "entities": []}, {"text": "Furthermore, we combine these two ideas and propose a novel attention transformation, constrained sparsemax, which produces both sparse and bounded attention weights, yielding a compact and interpretable set of alignments.", "labels": [], "entities": []}, {"text": "While being in-between soft and hard alignments), the constrained sparsemax transformation is end-to-end differentiable, hence amenable for training with gradient backpropagation.", "labels": [], "entities": []}, {"text": "To sum up, our contributions are as follows: 1 \u2022 We formulate constrained sparsemax and derive efficient linear and sublinear-time algorithms for running forward and backward propagation.", "labels": [], "entities": []}, {"text": "This transformation has two levels of sparsity: overtime steps, and over the attended words at each step.", "labels": [], "entities": []}, {"text": "\u2022 We provide a detailed empirical comparison of various attention transformations, including softmax (), sparse-max, constrained softmax, and our newly proposed constrained sparsemax.", "labels": [], "entities": []}, {"text": "We provide error analysis including two new metrics targeted at detecting coverage problems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our attention transformations on three language pairs.", "labels": [], "entities": []}, {"text": "We focused on small datasets, as they are the most affected by coverage mistakes.", "labels": [], "entities": []}, {"text": "We use the IWSLT 2014 corpus for DE-EN, the KFTT corpus for JA-EN (Neubig, 2011), and the WMT 2016 dataset for RO-EN.", "labels": [], "entities": [{"text": "IWSLT 2014 corpus", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.9288447300593058}, {"text": "KFTT corpus", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9734773635864258}, {"text": "WMT 2016 dataset", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.9486217101414999}]}, {"text": "The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.", "labels": [], "entities": []}, {"text": "Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods.", "labels": [], "entities": []}, {"text": "We tokenized the data using the Moses scripts and preprocessed it with subword units () with a joint vocabulary and 32k merge operations.", "labels": [], "entities": []}, {"text": "Our implementation was done on a fork of the OpenNMT-py toolkit () with the default parameters . We used a validation set to tune hyperparameters introduced by our model.", "labels": [], "entities": []}, {"text": "Even though our attention implementations are CPU-based using NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices.", "labels": [], "entities": []}, {"text": "As baselines, we use softmax attention, as well as two recently proposed coverage models: \u2022 COVPENALTY (.", "labels": [], "entities": [{"text": "COVPENALTY", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9318901896476746}]}, {"text": "At test time, the hypotheses in the beam are rescored with a global score that includes a length and a coverage penalty.", "labels": [], "entities": [{"text": "length", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9959160685539246}, {"text": "coverage", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.997612714767456}]}, {"text": "We tuned \u03b1 and \u03b2 with grid search on {0.2k} 5 k=0 , as in.", "labels": [], "entities": []}, {"text": "At training and test time, coverage vectors \u03b2 and additional parameters v are used to condition the next attention step.", "labels": [], "entities": []}, {"text": "We adapted this to our bilinear attention by defining z t,j = s t\u22121 (W h j + v\u03b2 t\u22121,j ).", "labels": [], "entities": []}, {"text": "We also experimented combining the strategies above with the sparsemax transformation.", "labels": [], "entities": []}, {"text": "As evaluation metrics, we report tokenized BLEU, METEOR, as well as two new metrics that we describe next to account for over and under-translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9919079542160034}, {"text": "METEOR", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9901665449142456}]}, {"text": "6 We used a 2-layer LSTM, embedding and hidden size of 500, dropout 0.3, and the SGD optimizer for 13 epochs.", "labels": [], "entities": []}, {"text": "Since our sparse attention can become 0 for some words, we extended the original coverage penalty by adding another parameter , set to 0.1: cp(x; y) := \u03b2 J j=1 log max{, min{1, |y| t=1 \u03b1jt}}.", "labels": [], "entities": [{"text": "coverage", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9742351770401001}]}, {"text": "REP-score: anew metric to count repetitions.", "labels": [], "entities": [{"text": "REP-score", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9591404795646667}, {"text": "repetitions", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.8480920195579529}]}, {"text": "Formally, given an n-gram s \u2208 V n , let t(s) and r(s) be the its frequency in the model translation and reference.", "labels": [], "entities": []}, {"text": "We first compute a sentence-level score The REP-score is then given by summing \u03c3(t, r) over sentences, normalizing by the number of words on the reference corpus, and multiplying by 100.", "labels": [], "entities": [{"text": "REP-score", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.869706392288208}]}, {"text": "We used n = 2, \u03bb 1 = 1 and \u03bb 2 = 2.", "labels": [], "entities": []}, {"text": "DROP-score: anew metric that accounts for possibly dropped words.", "labels": [], "entities": [{"text": "DROP-score", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.5067768096923828}]}, {"text": "To compute it, we first compute two sets of word alignments: from source to reference translation, and from source to the predicted translation.", "labels": [], "entities": []}, {"text": "In our experiments, the alignments were obtained with fast align, trained on the training partition of the data.", "labels": [], "entities": []}, {"text": "Then, the DROP-score computes the percentage of source words that aligned with some word from the reference translation, but not with any word from the predicted translation.", "labels": [], "entities": [{"text": "DROP-score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.7666730880737305}]}, {"text": "We can see that on average, the sparse models (csparsemax as well as sparsemax combined with coverage models) have higher scores on both BLEU and METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9981278777122498}, {"text": "METEOR", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.6130560040473938}]}, {"text": "Generally, they also obtain better REP and DROP scores than csoftmax and softmax, which suggests that sparse attention alleviates the problem of coverage to some extent.", "labels": [], "entities": [{"text": "REP", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.992468535900116}, {"text": "DROP scores", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9385665655136108}]}, {"text": "To compare different fertility strategies, we ran experiments on the DE-EN for the csparsemax transformation.", "labels": [], "entities": [{"text": "DE-EN", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9652373194694519}, {"text": "csparsemax transformation", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6850896328687668}]}, {"text": "We see that the PRE-DICTED strategy outperforms the others both in terms of BLEU and METEOR, albeit slightly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9992982149124146}, {"text": "METEOR", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9813635349273682}]}, {"text": "shows examples of sentences for which the csparsemax fixed repetitions, along with the corresponding attention maps.", "labels": [], "entities": []}, {"text": "We see that in the case of softmax repetitions, the decoder attends repeatedly to the same portion of the source sentence (the expression \"letzten hundert\" in the first sentence and \"regierung\" in the second sentence).", "labels": [], "entities": []}, {"text": "Not only did csparsemax avoid repetitions, but it also yielded a sparse set of alignments, as expected.", "labels": [], "entities": []}, {"text": "Appendix B provides more examples of translations from all models in discussion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU, METEOR, REP and DROP scores on the test sets for different attention transformations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999642014503479}, {"text": "METEOR", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9983603358268738}, {"text": "REP", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9980688691139221}, {"text": "DROP", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9950664043426514}]}, {"text": " Table 2: Impact of various fertility strategies for  the csparsemax attention model (DE-EN).", "labels": [], "entities": []}]}