{"title": [{"text": "Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data", "labels": [], "entities": [{"text": "Multimodal Neural Machine Translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6454858854413033}]}], "abstractContent": [{"text": "In this paper, we investigate the effectiveness of training a multimodal neu-ral machine translation (MNMT) system with image features fora low-resource language pair, Hindi and En-glish, using synthetic data.", "labels": [], "entities": [{"text": "multimodal neu-ral machine translation (MNMT)", "start_pos": 62, "end_pos": 107, "type": "TASK", "confidence": 0.7523241468838283}]}, {"text": "A three-way parallel corpus which contains bilingual texts and corresponding images is required to train a MNMT system with image features.", "labels": [], "entities": []}, {"text": "However, such a corpus is not available for low resource language pairs.", "labels": [], "entities": []}, {"text": "To address this, we developed both a synthetic training dataset and a manually curated de-velopment/test dataset for Hindi based on an existing English-image parallel corpus.", "labels": [], "entities": []}, {"text": "We used these datasets to build our image description translation system by adopting state-of-the-art MNMT models.", "labels": [], "entities": [{"text": "image description translation", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7676226099332174}]}, {"text": "Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a system can benefit from image features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have witnessed a surge in application of multimodal neural models as a sequence to sequence learning problem) for solving different tasks such as machine translations, image and video description generation, visual question answering (, etc.", "labels": [], "entities": [{"text": "machine translations", "start_pos": 159, "end_pos": 179, "type": "TASK", "confidence": 0.7263539880514145}, {"text": "image and video description generation", "start_pos": 181, "end_pos": 219, "type": "TASK", "confidence": 0.6831735014915467}, {"text": "question answering", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.6967923492193222}]}, {"text": "However, neural machine translation (NMT), which is an inherently data-dependent procedure, continues to be a challenging problem in the context of lowresourced and out-of-domain settings (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.8207313219706217}]}, {"text": "In other words, there is a concern that the model will perform poorly with languages having limited resources, especially in comparison with well-resourced major languages.", "labels": [], "entities": []}, {"text": "Although English(En) and Hindi(Hi) languages belong to the same family (IndoEuropean), they differ significantly in terms of word order, syntax and morphological structure (.", "labels": [], "entities": []}, {"text": "While English maintains a Subject-Verb-Object (SVO) template, Hindi follows a Subject-Object-Verb (SOV) convention.", "labels": [], "entities": []}, {"text": "Moreover, compared to English, Hindi has a more complex inflection system, where nouns, verbs and adjectives are inflected according to number, gender and case.", "labels": [], "entities": []}, {"text": "These issues, combined with the data scarcity problem, makes Hi\u2192En machine translation a challenging task.", "labels": [], "entities": [{"text": "Hi\u2192En machine translation", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.5292566299438477}]}, {"text": "Bilingual corpora, which are an important component for machine translation systems, suffer from the problem of data scarcity when one of the languages is resource-poor.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7655514776706696}]}, {"text": "To achieve better quality translation, a potential solution is to extend along the language dimension to construct bilingual corpora.", "labels": [], "entities": []}, {"text": "In particular, fora distant language pair such as Hindi and English, building a bilingual corpus can prove to be a useful endeavor in multiple aspects.", "labels": [], "entities": []}, {"text": "We are inspired by the recent successes of using visual inputs for translation tasks (see Section 2 for relevant studies).", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.928804337978363}]}, {"text": "For translating image descriptions, given both the source image and it's description, it can be seen that both modalities can bring more useful information for generating the target language description.", "labels": [], "entities": [{"text": "translating image descriptions", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.8544667363166809}]}, {"text": "With the goal of preventing a lowresource language such as Hindi from being left behind in the advancement of multimodal machine translation, we take the first steps towards applying MNMT methods for Hi\u2192En translation.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.6348620653152466}, {"text": "Hi\u2192En translation", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.5931472554802895}]}, {"text": "Our contributions in this study are as follows: \u2022 To the best of our knowledge, we are the first to tackle the problem of multimodal translation from Hindi into English.", "labels": [], "entities": [{"text": "multimodal translation from Hindi into English", "start_pos": 122, "end_pos": 168, "type": "TASK", "confidence": 0.8293846150239309}]}, {"text": "\u2022 We examine if visual features help to improve machine translation (MT) performance in low resource scenarios.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.8429725706577301}]}, {"text": "\u2022 We investigate whether the multimodal machine translation system for lessresourced language can benefit from synthetic data.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.6594904462496439}]}, {"text": "\u2022 We augment the Flickr30k dataset with synthetic Hindi descriptions, obtained from a MT system.", "labels": [], "entities": [{"text": "Flickr30k dataset", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.9683841466903687}]}, {"text": "\u2022 We manually develop a validation and test corpus of the English counterpart in the Flickr30k dataset.", "labels": [], "entities": [{"text": "Flickr30k dataset", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.9884103238582611}]}, {"text": "We plan to release this dataset publicly for research purposes.", "labels": [], "entities": []}, {"text": "This paper is divided as follows: Section 2 provides the necessary background and establishes the relevance of the presented work, both in terms of low-resourced MT and MT in multimodal contexts.", "labels": [], "entities": [{"text": "MT", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.9182648062705994}, {"text": "MT", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.9243423342704773}]}, {"text": "Section 3 describes the overall methodology.", "labels": [], "entities": []}, {"text": "In Section 4 we outline the backgrounds of datasets used for training, validation and testing.", "labels": [], "entities": []}, {"text": "Section 5 provides detailed descriptions of the multimodal models used in our experiments.", "labels": [], "entities": []}, {"text": "Section 6 details the experimental set-ups.", "labels": [], "entities": []}, {"text": "Results and analysis are presented in Section 7.", "labels": [], "entities": []}, {"text": "Finally, in Section 8, we provide conclusions and indicate possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we briefly describe the experimental settings used to generate the synthetic Hindi data and further expand it into a multimodal NMT framework.", "labels": [], "entities": []}, {"text": "The Hindi side of the Hi c \u2212 En c is normalized using the Indic_NLP_Library 2 to ensure the canonical Unicode representation.", "labels": [], "entities": [{"text": "Indic_NLP_Library 2", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.764719178279241}]}, {"text": "We used the scripts from the above library to tokenize and normalize the Hindi sentences.", "labels": [], "entities": []}, {"text": "For English, we used the scripts from the Moses tokenizer tokenizer.perl 3 to tokenize and low-ercase the English representations for our experiments.", "labels": [], "entities": []}, {"text": "We use settings similar to that of () to develop Hi t . They used the news stories from the WMT 2014 English-Hindi shared task () as the development(dev) and test corpora which we concatenate together to create our dev set.", "labels": [], "entities": [{"text": "WMT 2014 English-Hindi shared task", "start_pos": 92, "end_pos": 126, "type": "DATASET", "confidence": 0.8589722514152527}]}, {"text": "The training and dev corpora consist of 1,492,827 and 3,207 sentence segments respectively.", "labels": [], "entities": []}, {"text": "We used the HindMono corpus () which contains roughly 45 million sentences to build our language model in Hindi.", "labels": [], "entities": [{"text": "HindMono corpus", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.9504392743110657}]}, {"text": "The corpus statistics are shown in2.", "labels": [], "entities": []}, {"text": "For training the Hi c \u2212 En c corpus, we use the Moses SMT system ( . We use the SRILM toolkit for building a language model and GIZA++) with the grow-diag-final-and heuristic for extracting phrases from Hi c \u2212 En c .The trained system is tuned using Minimum Error Rate Training.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 250, "end_pos": 277, "type": "METRIC", "confidence": 0.7678074985742569}]}, {"text": "For other parameters of Moses, default values are used.", "labels": [], "entities": []}, {"text": "If the sentences in English or Hindi are longer than 80 tokens, they are discarded.", "labels": [], "entities": []}, {"text": "To measure the performance of the system, we also translate the En r testset into Hi r both manually and automatically.", "labels": [], "entities": [{"text": "En r testset", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.7116638521353403}]}, {"text": "We also perform Hindi\u2192English (Hi\u2192En) translation using a PBSMT system with the general domain Hi c \u2212 En c corpus.", "labels": [], "entities": [{"text": "Hindi\u2192English (Hi\u2192En) translation", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.606177462471856}]}, {"text": "We use the News Crawl articles 2016 from the WMT17 4 as additional English monolingual corpora to train the 4-gram language model.", "labels": [], "entities": [{"text": "News Crawl articles 2016 from the WMT17 4", "start_pos": 11, "end_pos": 52, "type": "DATASET", "confidence": 0.9516801536083221}]}, {"text": "This contain roughly 20 million sentence for English..", "labels": [], "entities": []}, {"text": "To build our Multi-modal NMT systems we use OpenNMT-py (the pytorch port of Open-NMT () following the settings of which implements the encoder as a bi-directional RNN with GRU, one 1024D single-layer forward RNN and one 1024D single-layer backward RNN.", "labels": [], "entities": [{"text": "GRU", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9417920708656311}]}, {"text": "Throughout the experiments, the models are parameterised using 620D source and target word embeddings, and both are trained jointly with the model.", "labels": [], "entities": []}, {"text": "All non-recurrent matrices are initialised by sampling from a Gaussian distribution (\u00b5 = 0, \u03c3 = 0.01), re-current matrices are random orthogonal and bias vectors are all initialised to 0.", "labels": [], "entities": []}, {"text": "Dropout with a probability of 0.3 in source and target word embeddings, in the image features (in all MNMT models), in the encoder and decoder RNNs inputs and recurrent connections, and before the readout operation in the decoder RNN was applied.", "labels": [], "entities": []}, {"text": "Following (, dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps are also applied.", "labels": [], "entities": []}, {"text": "The models are trained for 25 epochs using Adam () with learning rate 0.002 and mini-batches of size 40, where each training instance consists of one English sentence, one Hindi sentence and one image.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.9523599147796631}]}, {"text": "Finally, we evaluate translation quality quantitatively in terms of BLEU () and METEOR) and report statistical significance for the metrics using approximate randomisation computed with).", "labels": [], "entities": [{"text": "translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.957284688949585}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9990436434745789}, {"text": "METEOR", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9965444207191467}]}], "tableCaptions": [{"text": " Table 1: Statistics of data sets used to train  PBSMT system", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.5691466331481934}]}, {"text": " Table 5: Evaluation metrics scores Hi-En  translation systems before and after applying  the image features on manually curated dev  data. Bold numbers indicate that improve- ments are statistically significant compared to  NMT text with p = 0.05", "labels": [], "entities": [{"text": "Hi-En  translation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.676887720823288}, {"text": "improve- ments", "start_pos": 167, "end_pos": 181, "type": "METRIC", "confidence": 0.9464973012606303}]}]}