{"title": [{"text": "Learning Matching Models with Weak Supervision for Response Selection in Retrieval-based Chatbots", "labels": [], "entities": [{"text": "Learning Matching", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7305550575256348}, {"text": "Response Selection", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7941529750823975}]}], "abstractContent": [{"text": "We propose a method that can leverage un-labeled data to learn a matching model for response selection in retrieval-based chat-bots.", "labels": [], "entities": [{"text": "response selection", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7511352896690369}]}, {"text": "The method employs a sequence-to-sequence architecture (Seq2Seq) model as a weak annotator to judge the matching degree of unlabeled pairs, and then performs learning with both the weak signals and the unlabeled data.", "labels": [], "entities": []}, {"text": "Experimental results on two public data sets indicate that matching models get significant improvements when they are learned with the proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, more and more attention from both academia and industry is paying to building nontask-oriented chatbots that can naturally converse with humans on any open domain topics.", "labels": [], "entities": []}, {"text": "Existing approaches can be categorized into generationbased methods) which synthesize a response with natural language generation techniques, and retrievalbased methods () which select a response from a pre-built index.", "labels": [], "entities": []}, {"text": "In this work, we study response selection for retrieval-based chatbots, not only because retrieval-based methods can return fluent and informative responses, but also because they have been successfully applied to many real products such as the social-bot XiaoIce from Microsoft ( and the E-commerce assistant AliMe Assist from Alibaba Group ( . * Corresponding Author A key step to response selection is measuring the matching degree between a response candidate and an input which is either a single message () or a conversational context consisting of multiple utterances ( . While existing research focuses on how to define a matching model with neural networks, little attention has been paid to how to learn such a model when few labeled data are available.", "labels": [], "entities": [{"text": "XiaoIce", "start_pos": 256, "end_pos": 263, "type": "DATASET", "confidence": 0.9247598052024841}, {"text": "response selection", "start_pos": 383, "end_pos": 401, "type": "TASK", "confidence": 0.7409639954566956}]}, {"text": "In practice, because human labeling is expensive and exhausting, one cannot have large scale labeled data for model training.", "labels": [], "entities": [{"text": "human labeling", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.5951360613107681}]}, {"text": "Thus, a common practice is to transform the matching problem to a classification problem with human responses as positive examples and randomly sampled ones as negative examples.", "labels": [], "entities": []}, {"text": "This strategy, however, oversimplifies the learning problem, as most of the randomly sampled responses are either far from the semantics of the messages or the contexts, or they are false negatives which pollute the training data as noise.", "labels": [], "entities": []}, {"text": "As a result, there often exists a significant gap between the performance of a model in training and the same model in practice (.", "labels": [], "entities": []}, {"text": "We propose anew method that can effectively leverage unlabeled data for learning matching models.", "labels": [], "entities": []}, {"text": "To simulate the real scenario of a retrieval-based chatbot, we construct an unlabeled data set by retrieving response candidates from an index.", "labels": [], "entities": []}, {"text": "Then, we employ a weak annotator to provide matching signals for the unlabeled inputresponse pairs, and leverage the signals to supervise the learning of matching models.", "labels": [], "entities": []}, {"text": "The weak annotator is pre-trained from large scale humanhuman conversations without any annotations, and thus a Seq2Seq model becomes a natural choice.", "labels": [], "entities": []}, {"text": "Our approach is compatible with any matching models, and falls in a teacher-student framework) where the Seq2Seq model transfers the knowledge from human-human conversations to the learning process of the matching models.", "labels": [], "entities": []}, {"text": "Broadly speaking, both of ( and our work let a neural network supervise the learning of another network.", "labels": [], "entities": []}, {"text": "An advantage of our method is that it turns the hard zero-one labels in the existing learning paradigm to soft (weak) matching scores.", "labels": [], "entities": []}, {"text": "Hence, the model can learn a large margin between a true response with a true negative example, and the semantic distance between a true response and a false negative example is short.", "labels": [], "entities": []}, {"text": "Furthermore, due to the simulation of real scenario, harder examples can been seen in the training phase that makes the model more robust in the testing.", "labels": [], "entities": []}, {"text": "We conduct experiments on two public data sets, and experimental results on both data sets indicate that models learned with our method can significantly outperform their counterparts learned with the random sampling strategy.", "labels": [], "entities": []}, {"text": "Our contributions include: (1) proposal of anew method that can leverage unlabeled data to learn matching models for retrieval-based chatbots; and (2) empirical verification of the effectiveness of the method on public data sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on two public data sets: STC data set () for single-turn response selection and Douban Conversation Corpus ( ) for multi-turn response selection.", "labels": [], "entities": [{"text": "STC data set", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8620557983716329}, {"text": "Douban Conversation Corpus", "start_pos": 103, "end_pos": 129, "type": "DATASET", "confidence": 0.7822872002919515}, {"text": "multi-turn response selection", "start_pos": 138, "end_pos": 167, "type": "TASK", "confidence": 0.6401071349779764}]}, {"text": "Note that we do not test the proposed approach on Ubuntu Corpus (, because both training and test data in the corpus are constructed by random sampling.", "labels": [], "entities": [{"text": "Ubuntu Corpus", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9738011956214905}]}], "tableCaptions": [{"text": " Table 2: Results on Douban Conversation Corpus", "labels": [], "entities": [{"text": "Douban Conversation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7021494507789612}]}, {"text": " Table 4: The effect of instance number", "labels": [], "entities": []}]}