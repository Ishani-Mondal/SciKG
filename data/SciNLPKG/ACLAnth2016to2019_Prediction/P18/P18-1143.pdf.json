{"title": [{"text": "Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language.", "labels": [], "entities": []}, {"text": "We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory.", "labels": [], "entities": []}, {"text": "We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model.", "labels": [], "entities": []}, {"text": "We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Code-switching or code-mixing (CM) refers to the juxtaposition of linguistic units from two or more languages in a single conversation or sometimes even a single utterance.", "labels": [], "entities": [{"text": "code-mixing (CM) refers to the juxtaposition of linguistic units from two or more languages in a single conversation or sometimes even a single utterance", "start_pos": 18, "end_pos": 171, "type": "Description", "confidence": 0.7441081347373816}]}, {"text": "It is quite commonly observed in speech conversations of multilingual societies across the world.", "labels": [], "entities": []}, {"text": "Although, traditionally, CM has been associated with informal or casual speech, there is evidence that in several societies, such as urban India and Mexico, CM has become the default code of communication (, and it has also pervaded written text, especially in computer-mediated communication and social media (.", "labels": [], "entities": []}, {"text": "* Work done during author's internship at Microsoft Research According to some linguists, code-switching refers to inter-sentential mixing of languages, whereas code-mixing refers to intra-sentential mixing.", "labels": [], "entities": []}, {"text": "Since the latter is more general, we will use code-mixing in this paper to mean both.", "labels": [], "entities": []}, {"text": "It is, therefore, imperative to build NLP technology for CM text and speech.", "labels": [], "entities": []}, {"text": "There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech (, and tasks like language identification (), POS tagging (, parsing and sentiment analysis () for CM text.", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6221107343832651}, {"text": "language identification", "start_pos": 126, "end_pos": 149, "type": "TASK", "confidence": 0.7432662397623062}, {"text": "POS tagging", "start_pos": 154, "end_pos": 165, "type": "TASK", "confidence": 0.8487010300159454}, {"text": "parsing", "start_pos": 169, "end_pos": 176, "type": "TASK", "confidence": 0.9620605707168579}, {"text": "sentiment analysis", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.6669708788394928}]}, {"text": "Nevertheless, the accuracies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9939210414886475}]}, {"text": "Intuitively, since CM happens between two (or more languages), one would typically need twice as much, if not more, data to train a CM system.", "labels": [], "entities": []}, {"text": "Furthermore, any CM corpus will contain large chunks of monolingual fragments, and relatively far fewer code-switching points, which are extremely important to learn patterns of CM from data.", "labels": [], "entities": []}, {"text": "This implies that the amount of data required would not just be twice, but probably 10 or 100 times more than that for training a monolingual system with similar accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9926689863204956}]}, {"text": "On the other hand, apart from user-generated content on the Web and social media, it is extremely difficult to gather large volumes of CM data because (a) CM is rare informal text, and (b) speech data is hard to gather and even harder to transcribe.", "labels": [], "entities": []}, {"text": "In order to circumvent the data scarcity issue, in this paper we propose the use of linguisticallymotivated synthetically generated CM data (as a supplement to real CM data) for development of CM NLP systems.", "labels": [], "entities": []}, {"text": "In particular, we use the Equivalence Constraint Theory) for generating linguistically valid CM sentences from a pair of parallel sentences in the two languages.", "labels": [], "entities": []}, {"text": "We then use these generated sentences, along with monolingual and little amount of real CM data to train a CM Language Model (LM).", "labels": [], "entities": []}, {"text": "Our experiments show that, when trained following certain sampling strategies and training curriculum, the synthetic CM sentences are indeed able to improve the perplexity of the trained LM over a baseline model that uses only monolingual and real CM data.", "labels": [], "entities": []}, {"text": "LM is useful fora variety of downstream NLP tasks such as Speech Recognition and Machine Translation.", "labels": [], "entities": [{"text": "Speech Recognition", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8183560073375702}, {"text": "Machine Translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.8324041366577148}]}, {"text": "By definition, it is a discriminator between natural and unnatural language data.", "labels": [], "entities": []}, {"text": "The fact that linguistically constrained synthetic data can be used to develop better LM for CM text is, on one hand an indirect statistical and task-based validation of the linguistic theory used to generate the data, and on the other hand an indication that the approach in general is promising and can help solve the issue of data scarcity fora variety of NLP tasks for CM text and speech.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we use three types of language data: monolingual data in English and Spanish (Mono), real code-mixed data (rCM), and artificial or generated code-mixed data (gCM).", "labels": [], "entities": []}, {"text": "In this section, we describe these datasets and their CM properties.", "labels": [], "entities": []}, {"text": "We begin with description of some metrics that we shall use for quantification of the complexity of a CM dataset.", "labels": [], "entities": [{"text": "CM dataset", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.7011339068412781}]}, {"text": "We chose to conduct all our experiments on English-Spanish CM tweets because EnglishSpanish CM is well documented, is one of the most commonly mixed language pairs on social media (, and a couple of CM tweet datasets are readily available ( For our experiments, we use a subset of the tweets collected by that were automatically identified as English, Spanish or English-Spanish CM.", "labels": [], "entities": []}, {"text": "The authors provided us around 4.5M monolingual tweets per language, and 283K CM tweets.", "labels": [], "entities": []}, {"text": "These were already deduplicated and tagged for hashtags, URLs, emoticons and language labels automatically through the method proposed in the paper.", "labels": [], "entities": []}, {"text": "shows the sizes of the various datasets, which are also described below.", "labels": [], "entities": []}, {"text": "Mono: 50K tweets were sampled for Spanish and English from the entire collection of monolingual tweets.", "labels": [], "entities": []}, {"text": "The Spanish tweets were translated to English and vice versa, which gives us a total of 100K monolingual tweets in each language.", "labels": [], "entities": []}, {"text": "We shall refer to this dataset as Mono.", "labels": [], "entities": [{"text": "Mono", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8959256410598755}]}, {"text": "The sampling strategy and reason for generating translations will become apparent in Sec.", "labels": [], "entities": []}, {"text": "rCM: We use two real CM datasets in our experiment.", "labels": [], "entities": []}, {"text": "The 283K real CM tweets provided by were randomly divided into training, validation and test sets of nearly equal sizes.", "labels": [], "entities": []}, {"text": "Note that for most of our experiments, we will use a very small subset of the training set consisting of 5000 tweets as train data, because the fundamental assumption of this work is that very little amount of CM data is available for most language pairs (which is in fact true for most pairs beyond some very popularly mixed languages like English-Spanish).", "labels": [], "entities": []}, {"text": "Nevertheless, the much larger training set is required for studying the effect of varying the amount of real CM data on our models.", "labels": [], "entities": []}, {"text": "We shall refer to this training dataset as rCM.", "labels": [], "entities": []}, {"text": "The test set with 83K tweets will be referred to as Test-17.", "labels": [], "entities": []}, {"text": "We also use another dataset of English-Spanish CM tweets for testing our models which was released during the language labeling shared task at the Workshop on \"Computational Approaches to Code-switching, EMNLP 2014\" ().", "labels": [], "entities": [{"text": "language labeling shared task", "start_pos": 110, "end_pos": 139, "type": "TASK", "confidence": 0.7692888751626015}, {"text": "EMNLP 2014", "start_pos": 204, "end_pos": 214, "type": "DATASET", "confidence": 0.7747827768325806}]}, {"text": "We mixed the training, validation and test datasets released during this shared task to construct a set of 13K tweets, which we shall refer to as Test-14.", "labels": [], "entities": []}, {"text": "The two test datasets are tweets that were collected three years apart, and therefore, will help us estimate the robustness of the language models.", "labels": [], "entities": []}, {"text": "As shown in Table 1, these datasets are quite different in terms of CMI and average number of SP per tweet.", "labels": [], "entities": [{"text": "CMI", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9950240254402161}, {"text": "average number of SP", "start_pos": 76, "end_pos": 96, "type": "METRIC", "confidence": 0.7494738698005676}]}, {"text": "For computing the CMI and SP, we used a EnglishSpanish LID to language tag the words.", "labels": [], "entities": [{"text": "SP", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.886136531829834}]}, {"text": "In fact, 9500 tweets in the Test-14 dataset are monolingual, but we chose to retain them because it reflects the real distribution of CM data.", "labels": [], "entities": [{"text": "Test-14 dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9848150610923767}]}, {"text": "Further, Test-14 also has manually annotated language labels, which will be helpful while conducting an in-depth analysis of the models.", "labels": [], "entities": []}, {"text": "For all our experiments, we use a 2 layered RNN with LSTM units and hidden layer dimension of 100.", "labels": [], "entities": []}, {"text": "While training, we use sampled softmax with 5000 samples instead of a full softmax to speedup the training process.", "labels": [], "entities": []}, {"text": "The sampling is based on the word frequency in the training corpus.", "labels": [], "entities": []}, {"text": "We use momentum SGD with a learning rate of 0.002.", "labels": [], "entities": []}, {"text": "We have used the CNTK toolkit for building our models.", "labels": [], "entities": [{"text": "CNTK toolkit", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9039509296417236}]}, {"text": "We use a fixed k=5 (from each monolingual pair) for sampling the gCM data.", "labels": [], "entities": []}, {"text": "We observed the performance on \u2191-gCM to be the best when trained till CMI 0.4 and similarly on \u2193-gCM when trained from 1.0 to 0.6.", "labels": [], "entities": []}, {"text": "presents the perplexities on validation, Test-14 and Test-17 datasets for all the models (Col. 3, 4 and 5).", "labels": [], "entities": [{"text": "Test-17 datasets", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9427378475666046}]}, {"text": "We observe the following trends: (1) Model 5(b)-\u03c1 has the least perplexity value (significantly different from the second lowest value in the column, p < 0.00001 fora paired t-test).", "labels": [], "entities": []}, {"text": "(2) There is 55 and 90 point reduction in perplexity on Test-17 and Test-14 sets respectively from the baseline experiment 3, that does not use gCM data.", "labels": [], "entities": [{"text": "Test-17 and Test-14 sets", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.7208893895149231}]}, {"text": "Thus, addition of gCM data is helpful.", "labels": [], "entities": [{"text": "gCM data", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.812057226896286}]}, {"text": "(3) Only the 4a and 4b models are worse than 3, while 5a and 5b models are better.", "labels": [], "entities": []}, {"text": "Hence, rCM is indispensable, even though gCM helps.", "labels": [], "entities": []}, {"text": "(4) SPF based sampling performs significantly better (again p < 0.00001) than other sampling techniques.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of the datasets. Numbers in paren- thesis show the vocabulary size, i.e., the no. of  unique words.", "labels": [], "entities": []}]}