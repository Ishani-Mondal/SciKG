{"title": [{"text": "Language Production Dynamics with Recurrent Neural Networks", "labels": [], "entities": [{"text": "Language Production Dynamics", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7480109433333079}]}], "abstractContent": [{"text": "We present an analysis of the internal mechanism of the recurrent neural model of sentence production presented by Calvillo et al.", "labels": [], "entities": []}, {"text": "The results show clear patterns of computation related to each layer in the network allowing to infer an algorithmic account, where the semantics activates the semantically related words, then each word generated at each time step activates syntactic and semantic constraints on possible continuations, while the recurrence preserves information through time.", "labels": [], "entities": []}, {"text": "We propose that such insights could generalize to other models with similar architecture, including some used in computational linguistics for language modeling, machine translation and image caption generation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.7066714316606522}, {"text": "machine translation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.8120572865009308}, {"text": "image caption generation", "start_pos": 186, "end_pos": 210, "type": "TASK", "confidence": 0.8206093112627665}]}], "introductionContent": [{"text": "A Recurrent Neural Network (RNN) is an artificial neural network that contains at least one layer whose activation at a time step t serves as input to itself at a time step t + 1.", "labels": [], "entities": [{"text": "Recurrent Neural Network (RNN)", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.5762157092491785}]}, {"text": "Theoretically, RNNs have been shown to beat least as powerful as a Turing Machine (.", "labels": [], "entities": []}, {"text": "Empirically, in computational linguistics they achieve remarkable results in several tasks, most notably in language modeling and machine translation (e.g.).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7520982623100281}, {"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7685911655426025}]}, {"text": "In the human language processing literature, they have been used to model language comprehension (e.g.) and production (e.g.).", "labels": [], "entities": []}, {"text": "In spite of their success, RNNs are often used as a black box with little understanding of their internal dynamics, and rather evaluating them in terms of prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.911177396774292}]}, {"text": "This is due to the typically high dimensionality of the internal states of the network, coupled with highly complex interactions between layers.", "labels": [], "entities": []}, {"text": "Here we try to open the black box presenting an analysis of the internal behavior of the sentence production model presented by.", "labels": [], "entities": []}, {"text": "This model can be seen as a semantically conditioned language model that maps a semantic representation onto a sequence of words forming a sentence, by implementing an extension of a Simple Recurrent Network).", "labels": [], "entities": []}, {"text": "Because of its simple architecture and its relatively low dimensionality, this model can be analyzed as a whole, showing clear patterns of computation, which could give insights into the dynamics of larger language models with similar architecture.", "labels": [], "entities": []}, {"text": "The method that we applied is based on Layerwise Relevance Propagation (.", "labels": [], "entities": []}, {"text": "This algorithm starts at the output layer and moves in the graph towards the input units, tracking the amount of relevance that each unit in layer l i\u22121 has on the activation of units in layer l i , back to the input units, which are usually human-interpretable.", "labels": [], "entities": []}, {"text": "For a review of this and some other techniques for interpreting neural networks, see; for related work to this paper see; ;;;.", "labels": [], "entities": [{"text": "interpreting neural networks", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.9100708961486816}]}, {"text": "Our analysis reveals that the overall behavior of the model is approximately as follows: the input semantic representation activates the hidden units related to all the semantically relevant words, where words that are normally produced early in the sentence receive relatively more activation; after producing a word, the word produced activates syntactic and semantic constraints for the production of the next word, for example, after a deter-miner, all the nouns are activated, similarly, after a given verb, only semantically fit objects are activated; meanwhile, the recurrent units present a tendency for self-activation, suggesting a mechanism where activation is preserved overtime, allowing the model to implement dynamics over multiple time steps.", "labels": [], "entities": []}, {"text": "While some of the results presented here have been suggested previously, we present a holistic integrative view of the internal mechanics of the model, in contrast to previous analyses that focus on specific examples.", "labels": [], "entities": []}, {"text": "The next subsection describes the semantic representations used by the model.", "labels": [], "entities": []}, {"text": "Section 2 describes the language production model.", "labels": [], "entities": []}, {"text": "Section 3 presents the analysis.", "labels": [], "entities": []}, {"text": "Discussion and Conclusion are presented in sections 4 and 5 respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "The model was trained using cross-entropy backpropagation () with weight updates after each word.", "labels": [], "entities": []}, {"text": "All weights on the projections between layers were initialized with random values drawn from a normal distribution N (0, 0.1).", "labels": [], "entities": []}, {"text": "The weights on the bias projections were initially set to zero.", "labels": [], "entities": []}, {"text": "During training, the monitoring units were set at time t to what the model was supposed to produce at time t \u2212 1 (zeros fort = 0).", "labels": [], "entities": []}, {"text": "During testing, the monitoring units are set to 1.0 for the word that is actually produced and 0.0 everywhere else.", "labels": [], "entities": []}, {"text": "The model was trained fora maximum of 200 epochs, each epoch consisting of a full presentation of the training set, which was randomized before each epoch.", "labels": [], "entities": []}, {"text": "Each item in this set is a pair (dss i , sent), where sent is a sentence related to dss i , such that there is one training item per sentence related to each dss i . We employed an initial learning rate of 0.124 which was halved each time there was no improvement of performance on the training set during 15 epochs.", "labels": [], "entities": []}, {"text": "Training halted if the maximum number of epochs was reached or if there was no performance improvement on the training set over 40 epochs.", "labels": [], "entities": []}, {"text": "The model was evaluated using a 10-fold crossvalidation schema, with 5 testing conditions assessing different levels of generalization.", "labels": [], "entities": []}, {"text": "A full report can be seen in.", "labels": [], "entities": []}, {"text": "For a given semantic representation dss i , a Levenshtein similarity value was obtained comparing the sentence produced with the most similar sentence in \u03d5 i . The performance was very high, obtaining an average across conditions of 97.1% in similarity scores, with 88.57% of perfect matches.", "labels": [], "entities": []}], "tableCaptions": []}