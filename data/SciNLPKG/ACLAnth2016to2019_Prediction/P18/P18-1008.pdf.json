{"title": [{"text": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.7304902970790863}]}], "abstractContent": [{"text": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8699289202690125}]}, {"text": "The classic RNN-based approaches to MT were first out-performed by the convolu-tional seq2seq model, which was then out-performed by the more recent Transformer model.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9959473013877869}]}, {"text": "Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures.", "labels": [], "entities": []}, {"text": "In this paper , we tease apart the new architectures and their accompanying techniques in two ways.", "labels": [], "entities": []}, {"text": "First, we identify several key mod-eling and training techniques, and apply them to the RNN architecture, yielding anew RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English\u2192French and English\u2192German tasks.", "labels": [], "entities": [{"text": "WMT'14 English\u2192French", "start_pos": 209, "end_pos": 230, "type": "TASK", "confidence": 0.6587115526199341}]}, {"text": "Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths.", "labels": [], "entities": []}, {"text": "Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, the emergence of seq2seq models () has revolutionized the field of MT by replacing traditional phrasebased approaches with neural machine translation (NMT) systems based on the encoder-decoder paradigm.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9937278032302856}]}, {"text": "In the first architectures that surpassed * Equal contribution.", "labels": [], "entities": [{"text": "Equal contribution", "start_pos": 44, "end_pos": 62, "type": "METRIC", "confidence": 0.9665760397911072}]}, {"text": "the quality of phrase-based MT, both the encoder and decoder were implemented as Recurrent Neural Networks (RNNs), interacting via a soft-attention mechanism (.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.7858970165252686}]}, {"text": "The RNN-based NMT approach, or RNMT, was quickly established as the de-facto standard for NMT, and gained rapid adoption into large-scale systems in industry, e.g. Baidu (), Google (, and Systran (.", "labels": [], "entities": []}, {"text": "Following RNMT, convolutional neural network based approaches ( to NMT have recently drawn research attention due to their ability to fully parallelize training to take advantage of modern fast computing devices.", "labels": [], "entities": []}, {"text": "such as GPUs and Tensor Processing Units (TPUs) (.", "labels": [], "entities": []}, {"text": "Well known examples are ByteNet () and ConvS2S ().", "labels": [], "entities": []}, {"text": "The ConvS2S model was shown to outperform the original RNMT architecture in terms of quality, while also providing greater training speed.", "labels": [], "entities": []}, {"text": "Most recently, the Transformer model (, which is based solely on a selfattention mechanism and feed-forward connections, has further advanced the field of NMT, both in terms of translation quality and speed of convergence.", "labels": [], "entities": []}, {"text": "In many instances, new architectures are accompanied by a novel set of techniques for performing training and inference that have been carefully optimized to work in concert.", "labels": [], "entities": []}, {"text": "This 'bag of tricks' can be crucial to the performance of a proposed architecture, yet it is typically under-documented and left for the enterprising researcher to discover in publicly released code (if any) or through anecdotal evidence.", "labels": [], "entities": []}, {"text": "This is not simply a problem for reproducibility; it obscures the central scientific question of how much of the observed gains come from the new architecture and how much can be attributed to the associated training and inference techniques.", "labels": [], "entities": []}, {"text": "In some cases, these new techniques maybe broadly applicable to other architectures and thus constitute a major, though implicit, contribution of an architecture paper.", "labels": [], "entities": []}, {"text": "Clearly, they need to be considered in order to ensure a fair comparison across different model architectures.", "labels": [], "entities": []}, {"text": "In this paper, we therefore take a step back and look at which techniques and methods contribute significantly to the success of recent architectures, namely ConvS2S and Transformer, and explore applying these methods to other architectures, including RNMT models.", "labels": [], "entities": []}, {"text": "In doing so, we come up with an enhanced version of RNMT, referred to as RNMT+, that significantly outperforms all individual architectures in our setup.", "labels": [], "entities": []}, {"text": "We further introduce new architectures built with different components borrowed from RNMT+, ConvS2S and Transformer.", "labels": [], "entities": []}, {"text": "In order to ensure a fair setting for comparison, all architectures were implemented in the same framework, use the same pre-processed data and apply no further post-processing as this may confound bare model performance.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: 1.", "labels": [], "entities": []}, {"text": "In ablation studies, we quantify the effect of several modeling improvements (including multi-head attention and layer normalization) as well as optimization techniques (such as synchronous replica training and labelsmoothing), which are used in recent architectures.", "labels": [], "entities": []}, {"text": "We demonstrate that these techniques are applicable across different model architectures.", "labels": [], "entities": []}, {"text": "2. Combining these improvements with the RNMT model, we propose the new RNMT+ model, which significantly outperforms all fundamental architectures on the widely-used WMT'14 En\u2192Fr and En\u2192De benchmark datasets.", "labels": [], "entities": [{"text": "WMT'14 En\u2192Fr and En\u2192De benchmark datasets", "start_pos": 166, "end_pos": 207, "type": "DATASET", "confidence": 0.5693922817707062}]}, {"text": "We provide a detailed model analysis and comparison of RNMT+, ConvS2S and Transformer in terms of model quality, model size, and training and inference speed.", "labels": [], "entities": []}, {"text": "3. Inspired by our understanding of the relative strengths and weaknesses of individual model architectures, we propose new model architectures that combine components from the RNMT+ and the Transformer model, and achieve better results than both individual architectures.", "labels": [], "entities": []}, {"text": "We quickly note two prior works that provided empirical solutions to the difficulty of training NMT architectures (specifically RNMT).", "labels": [], "entities": []}, {"text": "In () the authors systematically explore which elements of NMT architectures have a significant impact on translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9619184732437134}]}, {"text": "In) the authors recommend three specific techniques for strengthening NMT systems and empirically demonstrated how incorporating those techniques improves the reliability of the experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our models on the standard WMT'14 En\u2192Fr and En\u2192De datasets that comprise 36.3M and 4.5M sentence pairs, respectively.", "labels": [], "entities": [{"text": "WMT'14 En\u2192Fr and En\u2192De datasets", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.5830469297038184}]}, {"text": "Each sentence was encoded into a sequence of sub-word units obtained by first tokenizing the sentence with the Moses tokenizer, then splitting tokens into subword units (also known as \"wordpieces\") using the approach described in ().", "labels": [], "entities": []}, {"text": "We use a shared vocabulary of 32K sub-word units for each source-target language pair.", "labels": [], "entities": []}, {"text": "No further manual or rule-based post processing of the output was performed beyond combining the subword units to generate the targets.", "labels": [], "entities": []}, {"text": "We report all our results on newstest 2014, which serves as the test set.", "labels": [], "entities": [{"text": "newstest 2014", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9649946689605713}]}, {"text": "A combination of newstest 2012 and newstest 2013 is used for validation.", "labels": [], "entities": [{"text": "newstest 2012", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.903677225112915}, {"text": "newstest 2013", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8676815032958984}]}, {"text": "To evaluate the models, we compute the BLEU metric on tokenized, true-case output.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9745394885540009}]}, {"text": "For each training run, we evaluate the model every 30 minutes on the dev set.", "labels": [], "entities": []}, {"text": "Once the model converges, we determine the best window based on the average dev-set BLEU score over 21 consecutive evaluations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9911865592002869}]}, {"text": "We report the mean test score and standard deviation over the selected window.", "labels": [], "entities": [{"text": "mean test score", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.9349311788876852}]}, {"text": "This allows us to compare model architectures based on their mean performance after convergence rather than individual checkpoint evaluations, as the latter can be quite noisy for some models.", "labels": [], "entities": []}, {"text": "To enable a fair comparison of architectures, we use the same pre-processing and evaluation methodology for all our experiments.", "labels": [], "entities": []}, {"text": "We refrain from using checkpoint averaging (exponential moving averages of parameters) (JunczysDowmunt et al., 2016) or checkpoint ensembles () to focus on evaluating the performance of individual models.", "labels": [], "entities": []}, {"text": "In this section, we evaluate the importance of four main techniques for both the RNMT+ and the Transformer Big models.", "labels": [], "entities": []}, {"text": "We believe that these techniques are universally applicable across different model architectures, and should always be employed by NMT practitioners for best performance.", "labels": [], "entities": []}, {"text": "We take our best RNMT+ and Transformer Big models and remove each one of these techniques independently.", "labels": [], "entities": []}, {"text": "By doing this we hope to learn two things about each technique: (1) How much does it affect the model performance?", "labels": [], "entities": []}, {"text": "From we draw the following conclusions about the four techniques: \u2022 Label Smoothing We observed that label smoothing improves both models, leading to an average increase of 0.7 BLEU for RNMT+ and 0.2 BLEU for Transformer Big models.", "labels": [], "entities": [{"text": "label smoothing", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7425112128257751}, {"text": "BLEU", "start_pos": 177, "end_pos": 181, "type": "METRIC", "confidence": 0.9984986782073975}, {"text": "BLEU", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9980626702308655}]}, {"text": "\u2022 Multi-head Attention Multi-head attention contributes significantly to the quality of both models, resulting in an average increase of 0.6 BLEU for RNMT+ and 0.9 BLEU for Transformer Big models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9987210631370544}, {"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9980143308639526}]}, {"text": "\u2022 Layer Normalization Layer normalization is most critical to stabilize the training process of either model, especially when multi-head attention is used.", "labels": [], "entities": [{"text": "Layer Normalization Layer normalization", "start_pos": 2, "end_pos": 41, "type": "TASK", "confidence": 0.6683302447199821}]}, {"text": "Removing layer normalization results in unstable training runs for both models.", "labels": [], "entities": []}, {"text": "Since by design, we remove one technique at a time in our ablation experiments, we were unable to quantify how much layer normalization helped in either case.", "labels": [], "entities": []}, {"text": "To be able to successfully train a model without layer normalization, we would have to adjust other parts of the model and retune its hyper-parameters.", "labels": [], "entities": []}, {"text": "\u2022 Synchronous training Removing synchronous training has different effects on RNMT+ and Transformer.", "labels": [], "entities": []}, {"text": "For RNMT+, it results in a significant quality drop, while for the Transformer Big model, it causes the model to become unstable.", "labels": [], "entities": []}, {"text": "We also notice that synchronous training is only successful when coupled with a tailored learning rate schedule that has a warmup stage at the beginning (cf. Eq.", "labels": [], "entities": []}, {"text": "1 for RNMT+ and Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.5057042241096497}]}, {"text": "For RNMT+, removing this warmup stage during synchronous training causes the model to become unstable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on WMT14 En\u2192Fr. The num- bers before and after '\u00b1' are the mean and stan- dard deviation of test BLEU score over an eval- uation window. Note that Transformer models  are trained using 16 GPUs, while ConvS2S and  RNMT+ are trained using 32 GPUs.", "labels": [], "entities": [{"text": "WMT14 En\u2192Fr", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.6602908372879028}, {"text": "stan- dard deviation", "start_pos": 86, "end_pos": 106, "type": "METRIC", "confidence": 0.7234065234661102}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9647297859191895}]}, {"text": " Table 2: Results on WMT14 En\u2192De. Note that  Transformer models are trained using 16 GPUs,  while ConvS2S and RNMT+ are trained using 32  GPUs.", "labels": [], "entities": [{"text": "WMT14 En", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.565131813287735}]}, {"text": " Table 3: Performance comparison. Examples/s are  normalized by the number of GPUs used in the  training job. FLOPs are computed assuming that  source and target sequence length are both 50.", "labels": [], "entities": [{"text": "FLOPs", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.9940328001976013}]}, {"text": " Table 4: Ablation results of RNMT+ and the  Transformer Big model on WMT'14 En \u2192 Fr. We  report average BLEU scores on the test set. An as- terisk '*' indicates an unstable training run (train- ing halts due to non-finite elements).", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9947474598884583}, {"text": "RNMT", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.7844038009643555}, {"text": "WMT'14 En \u2192 Fr", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.8071586787700653}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9994127750396729}]}, {"text": " Table 6: Results for hybrids with cascaded en- coder and multi-column encoder.", "labels": [], "entities": []}]}