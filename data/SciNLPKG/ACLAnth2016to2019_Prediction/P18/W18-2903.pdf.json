{"title": [{"text": "Latent Tree Learning with Differentiable Parsers: Shift-Reduce Parsing and Chart Parsing", "labels": [], "entities": [{"text": "Chart Parsing", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.6267681270837784}]}], "abstractContent": [{"text": "Latent tree learning models represent sentences by composing their words according to an induced parse tree, all based on a downstream task.", "labels": [], "entities": []}, {"text": "These models often outperform baselines which use (exter-nally provided) syntax trees to drive the composition order.", "labels": [], "entities": []}, {"text": "This work contributes (a) anew latent tree learning model based on shift-reduce parsing, with competitive downstream performance and non-trivial induced trees, and (b) an analysis of the trees learned by our shift-reduce model and by a chart-based model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Popular recurrent neural networks in NLP, such as the Gated Recurrent Unit ( and Long Short-Term Memory, compute sentence representations by reading their words in a sequence.", "labels": [], "entities": []}, {"text": "In contrast, the Tree-LSTM architecture) processes words according to an input parse tree, and manages to achieve improved performance on a number of linguistic tasks.", "labels": [], "entities": []}, {"text": "Recently,,, and all proposed sentence embedding models which work similarly to a Tree-LSTM, but do not require any parse trees as input.", "labels": [], "entities": []}, {"text": "These models function without the assistance of an external automatic parser, and without ever being given any syntactic information as supervision.", "labels": [], "entities": []}, {"text": "Rather, they induce parse trees by training on a downstream task such as natural language inference.", "labels": [], "entities": []}, {"text": "At the heart of these models is a mechanism to assign trees to sentences -effectively, a natural language parser.", "labels": [], "entities": []}, {"text": "have recently investigated the tree structures induced by two of these models, trained fora natural language inference task.", "labels": [], "entities": []}, {"text": "Their analysis showed that learns mostly trivial left-branching trees, and has inconsistent performance; while outperforms all baselines (including those using trees from conventional parsers), but learns trees that do not correspond to those of conventional treebanks.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew latent tree learning model.", "labels": [], "entities": []}, {"text": "Similarly to, we base our approach on shift-reduce parsing.", "labels": [], "entities": []}, {"text": "Unlike their work, our model is trained via standard backpropagation, which is made possible by exploiting beam search to obtain an approximate gradient.", "labels": [], "entities": []}, {"text": "We show that this model performs well compared to baselines, and induces trees that are not as trivial as those learned by the Yogatama et al. model in the experiments of.", "labels": [], "entities": []}, {"text": "This paper also presents an analysis of the trees learned by our model, in the style of.", "labels": [], "entities": []}, {"text": "We further analyse the trees learned by the model of, had not yet been done, and perform evaluations on both the SNLI data) and the MultiNLI data ().", "labels": [], "entities": [{"text": "SNLI data", "start_pos": 113, "end_pos": 122, "type": "DATASET", "confidence": 0.864628255367279}, {"text": "MultiNLI data", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9420052766799927}]}, {"text": "The former corpus had not been used for the evaluation of trees of, and we find that it leads to more consistent induced trees.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data To match the settings of  For each model and dataset, we train five instances using different random initialisations, fora total of 2 \u00d7 2 \u00d7 5 = 20 instances.", "labels": [], "entities": []}, {"text": "NLI Accuracy We measure SNLI and MultiNLI test set accuracy for CKY and BSSR.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9055408239364624}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.5806220173835754}, {"text": "CKY", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8495917320251465}]}, {"text": "The aim is to ensure that they perform reasonably, and are inline with other latent tree learning models of a similar size and complexity.", "labels": [], "entities": []}, {"text": "Results for the best mod-   els, chosen based on development set performance, are reported in.", "labels": [], "entities": []}, {"text": "While our models do not reach the state of the art, they perform at least as well as other latent tree models using 100D embeddings, and are competitive with some 300D models.", "labels": [], "entities": []}, {"text": "They also outperform the 100D Tree-LSTM of, which is given syntax trees, and match or outperform 300D SPINN, which is explicitly trained to parse.", "labels": [], "entities": []}, {"text": "Self-consistency Next, we examine the consistency of the trees produced for the development sets.", "labels": [], "entities": []}, {"text": "Adapting the code of, we measure the models' self F1, defined as the unlabelled F1 between trees by two instances of the same model (given by different random initializations), averaged overall possible pairs.", "labels": [], "entities": [{"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.8303965926170349}]}, {"text": "In order to test whether BSSR and CKY learn similar grammars, we calculate the inter-model F1, defined as the unlabelled F1 between instances of BSSR and CKY trained on the same data, averaged overall possible pairs.", "labels": [], "entities": [{"text": "BSSR", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8408500552177429}, {"text": "F1", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.7075046300888062}]}, {"text": "We find an average F1 of 42.6 for MultiNLI+ and 55.0 for SNLI, both above the random baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9995070695877075}]}, {"text": "Our Self F1 results are all above the baseline of random trees.", "labels": [], "entities": []}, {"text": "For MultiNLI+, they are inline with ST-Gumbel.", "labels": [], "entities": [{"text": "ST-Gumbel", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.8052210211753845}]}, {"text": "Remarkably, the models trained on SNLI are noticeably more self-consistent.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.7514350414276123}]}, {"text": "This shows that the specifics of the training data play an important role, even when the downstream task is the same.", "labels": [], "entities": []}, {"text": "A possible explanation is that MultiNLI has longer sentences, as well as multiple genres, including telephone conversations which often do not constitute full sentences).", "labels": [], "entities": []}, {"text": "This would require the models to learn how to parse a wide variety of styles of data.", "labels": [], "entities": []}, {"text": "It is also interesting to note that the inter-model F1 scores are not much lower than the self F1 scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9629688858985901}, {"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9073405861854553}]}, {"text": "This shows that, given the same training data, the grammars learned by the two different models are not much more different than the grammars learned by two instances of the same model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SNLI and MultiNLI (matched) test set ac- curacy.  \u2020: results are for the model variant without  the leaf RNN transformation.", "labels": [], "entities": []}, {"text": " Table 2: Unlabelled F1 scores of the trees induced by various models against: other runs of the same  model, fully left-and right-branching trees, and Stanford Parser trees provided with the datasets. The  baseline results on MultiNLI are from Williams et al. (2017a).  \u2020: results are for the model variant without  the leaf RNN transformation.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9585923552513123}, {"text": "MultiNLI", "start_pos": 227, "end_pos": 235, "type": "DATASET", "confidence": 0.8952558040618896}]}]}