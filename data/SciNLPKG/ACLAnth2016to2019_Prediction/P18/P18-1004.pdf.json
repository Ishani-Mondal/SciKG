{"title": [{"text": "Explicit Retrofitting of Distributional Word Vectors", "labels": [], "entities": [{"text": "Explicit Retrofitting of Distributional Word Vectors", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.6628198524316152}]}], "abstractContent": [{"text": "Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation.", "labels": [], "entities": [{"text": "Semantic specialization of distributional word vectors", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7250341475009918}]}, {"text": "Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints.", "labels": [], "entities": []}, {"text": "In this work, in contrast, we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model (ER).", "labels": [], "entities": []}, {"text": "The ER model allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well.", "labels": [], "entities": []}, {"text": "We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks-lexical simplification and dialog state tracking.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.728776345650355}, {"text": "dialog state tracking", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.7586495478947958}]}, {"text": "Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces.", "labels": [], "entities": []}], "introductionContent": [{"text": "Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks.", "labels": [], "entities": [{"text": "Algebraic modeling of word vector spaces", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8689896861712137}, {"text": "Natural Language Processing (NLP)", "start_pos": 85, "end_pos": 118, "type": "TASK", "confidence": 0.6834543695052465}]}, {"text": "Commonly employed distributional models for word vector induction are based on the distributional hypothesis, i.e., they rely on word co-occurrences obtained from large text corpora (.", "labels": [], "entities": [{"text": "word vector induction", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.8377042611440023}]}, {"text": "The dependence on purely distributional knowledge results in a well-known tendency of fusing semantic similarity with other types of semantic relatedness () in the induced vector spaces.", "labels": [], "entities": []}, {"text": "Consequently, the similarity between distributional vectors indicates just an abstract semantic association and not a precise semantic relation (.", "labels": [], "entities": []}, {"text": "For example, it is difficult to discern synonyms from antonyms in distributional spaces.", "labels": [], "entities": []}, {"text": "This property has a particularly negative effect on NLP applications like text simplification and statistical dialog modeling, in which discerning semantic similarity from other types of semantic relatedness is pivotal to the system performance.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7813086807727814}, {"text": "statistical dialog modeling", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.7893306414286295}]}, {"text": "A standard solution is to move beyond purely unsupervised learning of word representations, in a process referred to as word vector space specialization or retrofitting.", "labels": [], "entities": [{"text": "word vector space specialization", "start_pos": 120, "end_pos": 152, "type": "TASK", "confidence": 0.6091345474123955}]}, {"text": "Specialization models leverage external lexical knowledge from lexical resources, such as WordNet, the Paraphrase Database (, or BabelNet (, to specialize distributional spaces fora particular lexical relation, e.g., synonymy) or hypernymy.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9673281908035278}]}, {"text": "External constraints are commonly pairs of words between which a particular relation holds.", "labels": [], "entities": []}, {"text": "Existing specialization methods exploit the external linguistic constraints in two prominent ways: (1) joint specialization models modify the learning objective of the original distributional model by integrating the constraints into it (, inter alia); (2) post-processing models fine-tune distributional vectors retroactively after training to satisfy the external constraints", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to estimate the robustness of the proposed explicit retrofitting procedure, we experiment with three different publicly available and widely used collections of pre-trained distributional vectors for English:, comprise a total of 1,023,082 synonymy word pairs and 380,873 antonymy word pairs.", "labels": [], "entities": []}, {"text": "Although this seems like a large number of linguistic constraints, there is only 57,320 unique words in all synonymy and antonymy constraints combined, and not all of these words are found in the dictionary of the pre-trained distributional vector space.", "labels": [], "entities": []}, {"text": "For example, only 15.3% of the words from constraints are found in the whole vocabulary of SGNS-W2 embeddings.", "labels": [], "entities": []}, {"text": "Similarly, we find only 13.3% and 14.6% constraint words among the 200K most frequent words from the GLOVE-CC and FASTTEXT vocabularies, respectively.", "labels": [], "entities": [{"text": "GLOVE-CC", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.8951259851455688}, {"text": "FASTTEXT vocabularies", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.6893622577190399}]}, {"text": "This low coverage emphasizes the core limitation of current retrofitting methods, being able to specialize only the vectors of words seen in the external constraints, and the need for our global ER method which can specialize all word vectors from the distributional space.", "labels": [], "entities": []}, {"text": "In all experiments, we set the distance function g to cosine distance: g(x 1 , x 2 ) = 1 \u2212 (x 1 \u00b7 x 2 /(x 1 x 2 )) and use the hyperbolic tangent as activation, \u03c6 = tanh.", "labels": [], "entities": []}, {"text": "For each constraint (w i , w j ), we create K = 4 corresponding negative examples for both w i and w j , resulting in micro-batches with 2K + 1 = 9 training instances.", "labels": [], "entities": []}, {"text": "We separate 10% of the created micro-batches as the validation set.", "labels": [], "entities": []}, {"text": "We then tune the hyper-parameter values, the number of hidden layers H = 5 and their size d h = 1000, and the topological regularization factor \u03bb = 0.3 by minimizing the model's objective Jon the validation set.", "labels": [], "entities": []}, {"text": "We train the model in mini-batches, each containing Nb = 100 constraints (i.e., 900 training instances, see above), using the Adam optimizer () with initial learning rate set to 10 \u22124 . We use the loss on the validation set as the early stopping criteria.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman's \u03c1 correlation scores for three standard English distributional vectors spaces on  English SimLex-999 (SL) and SimVerb-3500 (SV), using explicit retrofitting models with two different  objective functions (ER-MSD and ER-CNT, cf. Section 3.3).", "labels": [], "entities": [{"text": "\u03c1 correlation scores", "start_pos": 21, "end_pos": 41, "type": "METRIC", "confidence": 0.6557476123174032}]}, {"text": " Table 2: Performance (\u03c1) on SL and SV for ER- CNT models trained with different constraints.", "labels": [], "entities": []}, {"text": " Table 3: Spearman's \u03c1 correlation scores for Ger- man, Italian, and Croatian embeddings in the trans- fer setup: the vectors are specialized using the mod- els trained on English constraints and evaluated on  respective language-specific SimLex-999 variants.", "labels": [], "entities": [{"text": "\u03c1 correlation", "start_pos": 21, "end_pos": 34, "type": "METRIC", "confidence": 0.8724099695682526}]}, {"text": " Table 4: Lexical simplification performance with  explicit retrofitting applied on three input spaces.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9053761065006256}]}]}