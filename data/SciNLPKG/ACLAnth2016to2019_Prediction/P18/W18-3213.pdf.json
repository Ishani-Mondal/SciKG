{"title": [{"text": "Simple Features for Strong Performance on Named Entity Recognition in Code-Switched Twitter Data", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.6088335514068604}]}], "abstractContent": [{"text": "In this work, we address the problem of Named Entity Recognition (NER) in code-switched tweets as apart of the Workshop on Computational Approaches to Linguistic Code-switching (CALCS) at ACL'18 (Aguilar et al., 2018).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.772355854511261}]}, {"text": "Code-switching is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances , known as intra-sentential or inter-sentential code-switching, respectively.", "labels": [], "entities": []}, {"text": "Processing such data is challenging using state of the art methods since such technology is generally geared towards processing monolingual text.", "labels": [], "entities": []}, {"text": "In this paper we explored ways to use language identification and translation to recognize named entities in such data, however, utilizing simple features (sans multilingual features) with Conditional Random Field (CRF) classi-fier achieved the best results.", "labels": [], "entities": [{"text": "language identification", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7336313724517822}]}, {"text": "Our experiments were mainly aimed at the (ENG-SPA) English-Spanish dataset but we submitted a language-independent version of our system to the (MSA-EGY) Arabic-Egyptian dataset as well and achieved good results.", "labels": [], "entities": [{"text": "ENG-SPA) English-Spanish dataset", "start_pos": 42, "end_pos": 74, "type": "DATASET", "confidence": 0.5449413880705833}, {"text": "MSA-EGY) Arabic-Egyptian dataset", "start_pos": 145, "end_pos": 177, "type": "DATASET", "confidence": 0.6004930287599564}]}], "introductionContent": [{"text": "Recently, social media texts such as tweets and Facebook posts have attracted attention from the Natural Language Processing (NLP) research community.", "labels": [], "entities": []}, {"text": "This content has many applications as it provides clues to analyze sentiments of the masses towards areas ranging from basic electronic products to mental health issues to even national political candidates.", "labels": [], "entities": []}, {"text": "These applications have motivated the NLP community to rethink strategies for common tools, such as tokenizers, named entity taggers, POS taggers, dependency parsers, in the context of informal and noisy text.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 134, "end_pos": 145, "type": "TASK", "confidence": 0.6759113520383835}]}, {"text": "As access to the internet becomes more and more universal, a linguistically diverse population has come online.", "labels": [], "entities": []}, {"text": "showed that in a collection of 62 million tweets, only a little over 50% of them were in English.", "labels": [], "entities": []}, {"text": "This multilingualism has given rise to such interesting patterns as transliteration and code-switching.", "labels": [], "entities": []}, {"text": "The multilingual behavior combined with the informal nature of the content makes the task of building NLP tools even harder.", "labels": [], "entities": []}, {"text": "In this paper, we solve the problem of Named Entity Recognition (NER) for code-switched twitter data as apart of the ACL'18 Computational Approaches to Linguistic Code-switching (CALCS) Shared Task (.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.7258962293465933}]}, {"text": "Code-switching is a phenomenon that occurs when multilingual speakers alternate between two or more languages or dialects.", "labels": [], "entities": []}, {"text": "This phenomenon can be observed across different sentences, within the same sentence or even in the same word.", "labels": [], "entities": []}, {"text": "This shared task is similar to other social media tasks, except that the data is explicitly chosen to contain code-switching.", "labels": [], "entities": []}, {"text": "The entities for the task are: Event, Group, Location, Organization, Other, Person, Product, Time, and Title.", "labels": [], "entities": []}, {"text": "Below is an example of some code-switched data, switching between English and Spanish: My", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we describe the data, evaluation, and the model we used.", "labels": [], "entities": []}, {"text": "We used the standard harmonic mean F1 score to evaluate the system performance.", "labels": [], "entities": [{"text": "harmonic mean F1 score", "start_pos": 21, "end_pos": 43, "type": "METRIC", "confidence": 0.7067949026823044}]}, {"text": "Additionally, we used surface form F1 score as described in.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9642746150493622}]}, {"text": "Both of these metrics were apart of the evaluation in the CALCS shared task.", "labels": [], "entities": [{"text": "CALCS shared task", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.7384096185366312}]}, {"text": "This section gives an overview of our experiments.", "labels": [], "entities": []}, {"text": "First, we identify various local and global features using a variety of monolingual tweets and Gazetteers and train a CRF-based classifier on the data.", "labels": [], "entities": []}, {"text": "Second, we try to improve system recall using a 2-step NER process.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9899308681488037}]}, {"text": "Third, we convert the convert the code-mixed data to monolingual data using language identification (using a characterbased language model) and translation.", "labels": [], "entities": [{"text": "language identification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.6995227783918381}]}, {"text": "Of the three experiments that we tried, the first method gave the best results.", "labels": [], "entities": []}, {"text": "We compare against the best performing system in the shared task as well as the organizer's baseline in.", "labels": [], "entities": []}, {"text": "The baseline was provided by the organizers and used Bi-directional LSTMs followed by softmax layer (trained for 5 epochs) to infer the output labels.", "labels": [], "entities": []}, {"text": "The shared task used Surface Form F1 scores as well, but we omit them from our results as they were the same as harmonic mean F1 in all cases.", "labels": [], "entities": [{"text": "Surface Form F1 scores", "start_pos": 21, "end_pos": 43, "type": "METRIC", "confidence": 0.5811785161495209}, {"text": "harmonic mean F1", "start_pos": 112, "end_pos": 128, "type": "METRIC", "confidence": 0.5751846631368002}]}, {"text": "All scores are reported in.", "labels": [], "entities": []}, {"text": "Detailed scores are available in the appendix.", "labels": [], "entities": []}, {"text": "Our first experiment used a standard set of features, augmented with some task-specific ideas, and defined as follows.", "labels": [], "entities": []}, {"text": "Given a sequence of words in a sentence: ..., w i\u22122 , w i\u22121 , w i , w i+1 , w i+2 , ... and the current word in consideration is w i , we used the following features: \u2022 If w i is in the beginning of sentence: Results on all submissions.", "labels": [], "entities": []}, {"text": "Bold indicates best performance for that language.", "labels": [], "entities": []}, {"text": "\u2022) model on the combined tweets dataset (dimension: 100 ; window: 7).", "labels": [], "entities": []}, {"text": "Then, we clustered these embeddings into 40 clusters and used cluster IDs as features.", "labels": [], "entities": []}, {"text": "\u2022 Gazetteer: We used the Gazetteer (extracted from Wikidata by) labels as features.", "labels": [], "entities": []}, {"text": "\u2022 For each word wk in a context window of \u00b12: -The word wk itself -If wk is uppercase -Shape and Short shape (where same consecutive characters in the shape are compressed to a single character) of wk -If wk contains any special symbol like: ,#,$,-,,,etc. or an emoji.", "labels": [], "entities": [{"text": "Short", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9538137912750244}]}, {"text": "-If wk is alphabetic or alphanumeric -Emoji Description: We identified the 40 most common emojis present in our dataset and manually labelled them with representative words, such as smile, kiss, sad, etc.", "labels": [], "entities": [{"text": "Emoji Description", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6966456323862076}]}, {"text": "These emoji description (sense) of every context word were used as another feature.", "labels": [], "entities": []}, {"text": "We also ran the experiment on the MSA-EGY dataset (without the Gazetteer features).", "labels": [], "entities": [{"text": "MSA-EGY dataset", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9647366404533386}]}, {"text": "Following the first experiment, our main observation was that the recall was quite low.", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9652822613716125}]}, {"text": "One reason for this could be the presence of a large amount of tokens tagged as 'O' (\u223c97%).", "labels": [], "entities": [{"text": "O", "start_pos": 81, "end_pos": 82, "type": "METRIC", "confidence": 0.976664662361145}]}, {"text": "In contrast, the standard CONLL 2002 Spanish training NER corpus) had \u223c87% of the tokens tagged as 'O'.", "labels": [], "entities": [{"text": "CONLL 2002 Spanish training NER corpus)", "start_pos": 26, "end_pos": 65, "type": "DATASET", "confidence": 0.9258467725345066}]}, {"text": "To solve this issue, we experimented with a 2-step NER process (similar to): 1.", "labels": [], "entities": [{"text": "NER", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.900028645992279}]}, {"text": "Train a CRF model to identify whether a token is 'O' or not 2.", "labels": [], "entities": []}, {"text": "Train a CRF model to identify the type of named-entity (if identified as non-'O') As expected, we saw major improvements in recall, but these were offset by a substantial drop in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9995424747467041}, {"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9989014863967896}]}, {"text": "Overall, this led to a lower F1 score than before.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9691680371761322}]}, {"text": "In light of these results, we did not use the 2-step approach for any other experiments.", "labels": [], "entities": []}, {"text": "In this experiment, we tried to eliminate the codeswitching by converting the data to a monolingual form.", "labels": [], "entities": []}, {"text": "Our method is to identify the language of each token in the dataset and translate into a common language.", "labels": [], "entities": []}, {"text": "We collected training data for language identification using the Twitter API.", "labels": [], "entities": [{"text": "language identification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7698142230510712}]}, {"text": "We downloaded tweets for English and Spanish and assumed that each word in those tweets belonged to that particular language.", "labels": [], "entities": []}, {"text": "The statistics for the downloaded data is shown below: 1. 3000 Spanish tweets (7700 tokens \u223c56%) 2. 1900 English tweets (6100 tokens \u223c44%) Then, we trained a character-level RNN-based language model on this data to do language identification.", "labels": [], "entities": [{"text": "language identification", "start_pos": 218, "end_pos": 241, "type": "TASK", "confidence": 0.6917543262243271}]}, {"text": "In order to validate, we split our data and used 80% for training and rest for validating, achieving an accuracy of 79% on this validation data.", "labels": [], "entities": [{"text": "validate", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.9570918083190918}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9995148181915283}]}, {"text": "We used this model to identify the language of all the tokens in dataset, then used Google Translate API to translate English tokens to Spanish.", "labels": [], "entities": []}, {"text": "Finally, we used the language identification and the translation as features in our CRF model, in addition to all the features used in experiment 1.", "labels": [], "entities": [{"text": "language identification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7313536405563354}]}, {"text": "As compared to the results from experiment 1, this improved the recall on both development and test sets, but again, the loss in precision caused a slight overall drop in performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9997588992118835}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9994366765022278}]}], "tableCaptions": [{"text": " Table 1: (ENG-SPA) English-Spanish number of tweets and tokens for train, development, and test data", "labels": [], "entities": []}, {"text": " Table 2: (MSA-EGY) Modern Standard Arabic-Egyptian number of tweets and tokens for train, devel- opment, and test data", "labels": [], "entities": []}, {"text": " Table 3: (ENG-SPA) English-Spanish named entities counts for train and development data", "labels": [], "entities": []}, {"text": " Table 4: (MSA-EGY) Modern Standard Arabic-Egyptian entities counts for train and development data", "labels": [], "entities": [{"text": "Modern Standard Arabic-Egyptian entities counts", "start_pos": 20, "end_pos": 67, "type": "DATASET", "confidence": 0.7681166291236877}]}, {"text": " Table 5: (ENG-SPA) and (MSA-EGY) Our best  F1 scores on the test datasets compared with the  organizer's baseline and the top performing sys- tem in the Shared Task.", "labels": [], "entities": [{"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9987303614616394}]}, {"text": " Table 6: Results on all submissions. Bold indicates best performance for that language.", "labels": [], "entities": []}, {"text": " Table 7: (ENG-SPA) Results for Experiment 1: simple features and gazetteers", "labels": [], "entities": []}, {"text": " Table 8: (ENG-SPA) Results for Experiment 2: 2-step NER", "labels": [], "entities": [{"text": "NER", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.908385157585144}]}, {"text": " Table 9: (ENG-SPA) Results for Experiment 3: Language Identification + Translation", "labels": [], "entities": [{"text": "Language Identification + Translation", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.7804978638887405}]}]}