{"title": [{"text": "Learning Simplifications for Specific Target Audiences", "labels": [], "entities": [{"text": "Specific Target Audiences", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.8431959946950277}]}], "abstractContent": [{"text": "Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text.", "labels": [], "entities": [{"text": "Text simplification (TS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8884533882141114}]}, {"text": "Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8572727918624878}]}, {"text": "Different from MT, TS data comprises more elaborate transformations , such as sentence splitting.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.8956859111785889}, {"text": "sentence splitting", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7425357550382614}]}, {"text": "It can also contain multiple simplifications of the same original text targeting different audiences , such as school grade levels.", "labels": [], "entities": []}, {"text": "We explore these two features of TS to build models tailored for specific grade levels.", "labels": [], "entities": [{"text": "TS", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9215442538261414}]}, {"text": "Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation.", "labels": [], "entities": []}, {"text": "We show that it outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.", "labels": [], "entities": [{"text": "TS", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.8123739361763}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9994433522224426}, {"text": "SARI", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9755311608314514}]}], "introductionContent": [{"text": "Text simplification (TS) is the task of modifying an original text into a simpler version of it.", "labels": [], "entities": [{"text": "Text simplification (TS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8962452173233032}]}, {"text": "One of the main parameters for defining a suitable simplification is the target audience.", "labels": [], "entities": []}, {"text": "Examples include elderly, children, cognitively impaired users, nonnative speakers and low-literacy readers.", "labels": [], "entities": []}, {"text": "Traditionally, work on TS has been divided in lexical simplification (LS) and syntactic simplification (SS).", "labels": [], "entities": [{"text": "TS", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9794054627418518}, {"text": "syntactic simplification (SS)", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.8048745393753052}]}, {"text": "LS) deals with the identification and replacement of complex words or phrases.", "labels": [], "entities": [{"text": "identification and replacement of complex words or phrases", "start_pos": 19, "end_pos": 77, "type": "TASK", "confidence": 0.8202776238322258}]}, {"text": "SS) performs structural transformations such as changing a sentence from passive to active voice.", "labels": [], "entities": []}, {"text": "However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether.", "labels": [], "entities": []}, {"text": "These include either learning tree-based transformations) or using machine translation (MT)-based techniques (.", "labels": [], "entities": [{"text": "machine translation (MT)-", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7734728336334229}]}, {"text": "This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is \"translated\" into its simplified version.", "labels": [], "entities": [{"text": "TS", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9587129354476929}, {"text": "MT task", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9073220491409302}]}, {"text": "In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.9843764305114746}]}, {"text": "For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) () and the Newsela Article Corpus.", "labels": [], "entities": [{"text": "Wikipedia-Simple Wikipedia (W-SW)", "start_pos": 50, "end_pos": 83, "type": "DATASET", "confidence": 0.9260935425758362}, {"text": "Newsela Article Corpus", "start_pos": 95, "end_pos": 117, "type": "DATASET", "confidence": 0.9370355606079102}]}, {"text": "The former is a collection of original Wikipedia articles and their simplified versions created by volunteers.", "labels": [], "entities": []}, {"text": "The latter consists of news articles professionally simplified for various specific audiences following the US school grade system.", "labels": [], "entities": [{"text": "US school grade system", "start_pos": 108, "end_pos": 130, "type": "DATASET", "confidence": 0.7445983812212944}]}, {"text": "To build simplification models, the pairs of articles in these corpora have been aligned at the level of smaller units using standard algorithms.", "labels": [], "entities": []}, {"text": "Based on the number of sentences involved in these alignments, one can categorise alignments into four types of coarse-grained simplification operations: \u2022 Identical: an original sentence is aligned to itself, i.e. no simplification is performed.", "labels": [], "entities": []}, {"text": "\u2022 Elaboration: an original sentence is aligned to a single, rewritten simplified sentence.", "labels": [], "entities": []}, {"text": "\u2022 One-to-many: splitting -an original sentence is aligned to 2+ simplified sentences.", "labels": [], "entities": []}, {"text": "\u2022 Many-to-one: joining -2+ original sentences are aligned to a single simplified sentence.", "labels": [], "entities": []}, {"text": "We hereafter refer to the unit of simplification, i.e. one or more original or simplified sentences, as instances.", "labels": [], "entities": []}, {"text": "The Newsela corpus is seen as having higher quality than W-SW because its simplifications are created by professionals, following well defined guidelines (.", "labels": [], "entities": [{"text": "Newsela corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9442294836044312}]}, {"text": "It is also larger which is preferable for training corpus-based models.", "labels": [], "entities": []}, {"text": "More interestingly, the Newsela corpus has a feature that has been ignored thus far: Each instance in the corpus was created for readers with a certain school grade level.", "labels": [], "entities": [{"text": "Newsela corpus", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9608615338802338}]}, {"text": "Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each fora different grade level.", "labels": [], "entities": []}, {"text": "For example, a level 12 article may have simplified counterparts for levels 8 and 4.", "labels": [], "entities": []}, {"text": "In other words, the corpus contains instances where the same input leads to different outputs.", "labels": [], "entities": []}, {"text": "Disregarding this factor may lead to suboptimal models.", "labels": [], "entities": []}, {"text": "To avoid this problem, previous work (Alva-) has used subsets of the corpus with only certain combinations of complex-simplified article pairs, e.g. adjacent or non-adjacent pairs.", "labels": [], "entities": []}, {"text": "This however reduces the amount of data available for training.", "labels": [], "entities": []}, {"text": "We propose away of making use of this information to build more informed TS models that are aware of different types of target audiences, while still making use of the full dataset for learning.", "labels": [], "entities": [{"text": "TS", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9308128952980042}]}, {"text": "Inspired by the work of for MT, we add to each original instance an artificial token that represents the target grade level of that instance in order to guide a sequence-to-sequence attentional encoder-decoder neural approach (Bahdanau et al., 2015) ( \u00a72).", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9849715828895569}]}, {"text": "Ina similar vein, we also annotate the coarse-grained type of operation that should be performed to simplify the original instance, under the hypothesis that certain operations are more often used to simplify into certain grade levels.", "labels": [], "entities": []}, {"text": "Deciding on the operation is an easier problem than performing the actual operation.", "labels": [], "entities": [{"text": "Deciding", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9591265320777893}]}, {"text": "We rely on both gold and predicted operation types.", "labels": [], "entities": []}, {"text": "Experiments with models built with these artificial tokens outperform state-of-the-art neural models for TS, with the best approach combining grade level and type of operation ( \u00a73).", "labels": [], "entities": [{"text": "TS", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9702910780906677}]}, {"text": "Interestingly, such an approach also enables zero-shot TS, where a simplification fora grade level pair unseen at training time can still be generated during testing.", "labels": [], "entities": [{"text": "zero-shot TS", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.5220907628536224}]}, {"text": "We show that our zero-shot learning models perform virtually as well as our grade/operationinformed models ( \u00a74).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to build TS models for specific target audiences and to explore zero-shot learning for this application.", "labels": [], "entities": [{"text": "TS", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.945445716381073}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on the Newsela test set.", "labels": [], "entities": [{"text": "Newsela test set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9889634450276693}]}, {"text": " Table 3: Flesch-Kincaid scores for instances of  each grade level simplified using s2s, s2s+to-grade  and s2s+to-grade+operation (gold) models.", "labels": [], "entities": []}, {"text": " Table 5: Zero-shot data distribution.", "labels": [], "entities": []}, {"text": " Table 6: Results of zero-shot experiments for TS.", "labels": [], "entities": [{"text": "TS", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9620990753173828}]}]}