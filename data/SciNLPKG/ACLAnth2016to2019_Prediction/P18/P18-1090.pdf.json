{"title": [{"text": "Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach", "labels": [], "entities": [{"text": "Unpaired Sentiment-to-Sentiment Translation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5732033650080363}]}], "abstractContent": [{"text": "The goal of sentiment-to-sentiment \"trans-lation\" is to change the underlying sentiment of a sentence while keeping its content.", "labels": [], "entities": []}, {"text": "The main challenge is the lack of parallel data.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutral-ization module and an emotionalization module.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two review datasets, Yelp and Amazon.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9574075937271118}]}, {"text": "Experimental results show that our approach significantly outperforms the state-of-the-art systems.", "labels": [], "entities": []}, {"text": "Especially, the proposed method substantially improves the content preservation performance.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7928083837032318}]}, {"text": "The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9682481288909912}]}], "introductionContent": [{"text": "Sentiment-to-sentiment \"translation\" requires the system to change the underlying sentiment of a sentence while preserving its non-emotional semantic content as much as possible.", "labels": [], "entities": [{"text": "Sentiment-to-sentiment \"translation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.783672571182251}]}, {"text": "It can be regarded as a special style transfer task that is important in Natural Language Processing (NLP) (.", "labels": [], "entities": []}, {"text": "It has broad applications, including review sentiment transformation, news rewriting, etc.", "labels": [], "entities": [{"text": "review sentiment transformation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.8115352988243103}, {"text": "news rewriting", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.7752174735069275}]}, {"text": "Yet the lack of parallel training data poses a great obstacle to a satisfactory performance.", "labels": [], "entities": []}, {"text": "Recently, several related studies for language style transfer ( have been proposed.", "labels": [], "entities": [{"text": "language style transfer", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6657925546169281}]}, {"text": "However, when applied * Equal Contribution.", "labels": [], "entities": [{"text": "Equal Contribution", "start_pos": 24, "end_pos": 42, "type": "METRIC", "confidence": 0.9495343863964081}]}, {"text": "The released code can be found in https://github.com/lancopku/unpaired-sentiment-translation to the sentiment-to-sentiment \"translation\" task, most existing studies only change the underlying sentiment and fail in keeping the semantic content.", "labels": [], "entities": []}, {"text": "For example, given \"The food is delicious\" as the source input, the model generates \"What a bad movie\" as the output.", "labels": [], "entities": []}, {"text": "Although the sentiment is successfully transformed from positive to negative, the output text focuses on a different topic.", "labels": [], "entities": []}, {"text": "The reason is that these methods attempt to implicitly separate the emotional information from the semantic information in the same dense hidden vector, where all information is mixed together in an uninterpretable way.", "labels": [], "entities": []}, {"text": "Due to the lack of supervised parallel data, it is hard to only modify the underlying sentiment without any loss of the nonemotional semantic information.", "labels": [], "entities": []}, {"text": "To tackle the problem of lacking parallel data, we propose a cycled reinforcement learning approach that contains two parts: a neutralization module and an emotionalization module.", "labels": [], "entities": []}, {"text": "The neutralization module is responsible for extracting non-emotional semantic information by explicitly filtering out emotional words.", "labels": [], "entities": [{"text": "extracting non-emotional semantic information", "start_pos": 45, "end_pos": 90, "type": "TASK", "confidence": 0.7578860223293304}]}, {"text": "The advantage is that only emotional words are removed, which does not affect the preservation of non-emotional words.", "labels": [], "entities": []}, {"text": "The emotionalization module is responsible for adding sentiment to the neutralized semantic content for sentiment-to-sentiment translation.", "labels": [], "entities": [{"text": "sentiment-to-sentiment translation", "start_pos": 104, "end_pos": 138, "type": "TASK", "confidence": 0.6792081594467163}]}, {"text": "In cycled training, given an emotional sentence with sentiment s, we first neutralize it to the nonemotional semantic content, and then force the emotionalization module to reconstruct the original sentence by adding the sentiment s.", "labels": [], "entities": []}, {"text": "Therefore, the emotionalization module is taught to add sentiment to the semantic context in a supervised way.", "labels": [], "entities": []}, {"text": "By adding opposite sentiment, we can achieve the goal of sentiment-to-sentiment translation.", "labels": [], "entities": [{"text": "sentiment-to-sentiment translation", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.6754628717899323}]}, {"text": "Because of the discrete choice of neutral words, the gradient is no longer differentiable over the neutralization module.", "labels": [], "entities": []}, {"text": "Thus, we use policy gradient, one of the reinforcement learning methods, to reward the output of the neutralization module based on the feedback from the emotionalization module.", "labels": [], "entities": []}, {"text": "We add different sentiment to the semantic content and use the quality of the generated text as reward.", "labels": [], "entities": []}, {"text": "The quality is evaluated by two useful metrics: one for identifying whether the generated text matches the target sentiment; one for evaluating the content preservation performance.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 148, "end_pos": 168, "type": "TASK", "confidence": 0.6926124840974808}]}, {"text": "The reward guides the neutralization module to better identify non-emotional words.", "labels": [], "entities": []}, {"text": "In return, the improved neutralization module further enhances the emotionalization module.", "labels": [], "entities": []}, {"text": "Our contributions are concluded as follows: \u2022 For sentiment-to-sentiment translation, we propose a cycled reinforcement learning approach.", "labels": [], "entities": [{"text": "sentiment-to-sentiment translation", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.8004461824893951}]}, {"text": "It enables training with unpaired data, in which only reviews and sentiment labels are available.", "labels": [], "entities": []}, {"text": "\u2022 Our approach tackles the bottleneck of keeping semantic information by explicitly separating sentiment information from semantic content.", "labels": [], "entities": []}, {"text": "\u2022 Experimental results show that our approach significantly outperforms the state-of-the-art systems, especially in content preservation.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.7720896303653717}]}], "datasetContent": [{"text": "In this section, we evaluate our method on two review datasets.", "labels": [], "entities": []}, {"text": "We first introduce the datasets, the training details, the baselines, and the evaluation metrics.", "labels": [], "entities": []}, {"text": "Then, we compare our approach with the state-of-the-art systems.", "labels": [], "entities": []}, {"text": "Finally, we show the experimental results and provide the detailed analysis of the key components.", "labels": [], "entities": []}, {"text": "We conduct experiments on two review datasets that contain user ratings associated with each review.", "labels": [], "entities": []}, {"text": "Following previous work, we consider reviews with rating above three as positive reviews and reviews below three as negative reviews.", "labels": [], "entities": []}, {"text": "The positive and negative reviews are not paired.", "labels": [], "entities": []}, {"text": "Since our approach focuses on sentence-level sentiment-to-sentiment translation where sentiment annotations are provided at the document level, we process the two datasets with the following steps.", "labels": [], "entities": [{"text": "sentence-level sentiment-to-sentiment translation", "start_pos": 30, "end_pos": 79, "type": "TASK", "confidence": 0.6202580730120341}]}, {"text": "First, following previous work), we filter out the reviews that exceed 20 words.", "labels": [], "entities": []}, {"text": "Second, we construct textsentiment pairs by extracting the first sentence in a review associated with its sentiment label, because the first sentence usually expresses the core idea.", "labels": [], "entities": []}, {"text": "Finally, we train a sentiment classifier and filter out the text-sentiment pairs with the classifier confidence below 0.8.", "labels": [], "entities": []}, {"text": "Specially, we use the proposed self-attention based sentiment classifier for implementation.", "labels": [], "entities": [{"text": "self-attention based sentiment classifier", "start_pos": 31, "end_pos": 72, "type": "TASK", "confidence": 0.6455410346388817}]}, {"text": "The details of the processed datasets are introduced as follows.", "labels": [], "entities": []}, {"text": "Yelp Review Dataset (Yelp): This dataset is provided by Yelp Dataset Challenge.", "labels": [], "entities": [{"text": "Yelp Review Dataset (Yelp)", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.9154403805732727}, {"text": "Yelp Dataset Challenge", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9568694233894348}]}, {"text": "The processed Yelp dataset contains 400K, 10K, and 3K pairs for training, validation, and testing, respectively.", "labels": [], "entities": [{"text": "Yelp dataset", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.9244853556156158}]}, {"text": "Amazon Food Review Dataset (Amazon): This dataset is provided by.", "labels": [], "entities": [{"text": "Amazon Food Review Dataset (Amazon)", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.9317898580006191}]}, {"text": "It consists of amounts of food reviews from Amazon.", "labels": [], "entities": []}, {"text": "The processed Amazon dataset contains 230K, 10K, and 3K pairs for training, validation, and testing, respectively.", "labels": [], "entities": [{"text": "Amazon dataset", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.9725127518177032}]}, {"text": "We conduct two evaluations in this work, including an automatic evaluation and a human evaluation.", "labels": [], "entities": []}, {"text": "The details of evaluation metrics are shown as follows.", "labels": [], "entities": []}, {"text": "We quantitatively measure sentiment transformation by evaluating the accuracy of generating designated sentiment.", "labels": [], "entities": [{"text": "sentiment transformation", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.905119389295578}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9988996982574463}]}, {"text": "For a fair comparison, we do not use the proposed sentiment classification model.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.9055711627006531}]}, {"text": "Following previous work, we instead use a stateof-the-art sentiment classifier, called TextCNN, to automatically evaluate the transferred sentiment accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.917236864566803}]}, {"text": "TextCNN achieves the accuracy of 89% and 88% on two datasets.", "labels": [], "entities": [{"text": "TextCNN", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9298772811889648}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995666146278381}]}, {"text": "Specifically, we generate sentences given sentiment s, and use the pre-trained sentiment classifier to assign sentiment labels to the generated sentences.", "labels": [], "entities": []}, {"text": "The accuracy is calculated as the percentage of the predictions that match the sentiment s.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995970129966736}]}, {"text": "To evaluate the content preservation performance, we use the BLEU score () between the transferred sentence and the source sentence as an evaluation metric.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.764462947845459}, {"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9826233685016632}]}, {"text": "BLEU is a widely used metric for text generation tasks, such as machine translation, summarization, etc.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9844904541969299}, {"text": "text generation tasks", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8107739090919495}, {"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8314906060695648}, {"text": "summarization", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.9031933546066284}]}, {"text": "The metric compares the automatically produced text with the reference text by computing overlapping lexical n-gram units.", "labels": [], "entities": []}, {"text": "To evaluate the overall performance, we use the geometric mean of ACC and BLEU as an evaluation metric.", "labels": [], "entities": [{"text": "ACC", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9570155739784241}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9973342418670654}]}, {"text": "The G-score is one of the most commonly used \"single number\" measures in Information Retrieval, Natural Language Processing, and Machine Learning.", "labels": [], "entities": [{"text": "G-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9069777131080627}, {"text": "Information Retrieval", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.8280014991760254}]}, {"text": "While the quantitative evaluation provides indication of sentiment transfer quality, it cannot evaluate the quality of transferred text accurately.", "labels": [], "entities": [{"text": "sentiment transfer", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8495593369007111}]}, {"text": "Automatic evaluation results are shown in.", "labels": [], "entities": []}, {"text": "BLEU evaluates semantic content preservation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9588443636894226}, {"text": "semantic content preservation", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.7432477275530497}]}, {"text": "G-score represents the geometric mean of ACC and BLEU.", "labels": [], "entities": [{"text": "G-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9346650838851929}, {"text": "ACC", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9783161282539368}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9946704506874084}]}, {"text": "CAAE and MDAL achieve much lower BLEU scores, 1.17 and 1.64 on the Yelp dataset, 0.56 and 0.27 on the Amazon dataset.", "labels": [], "entities": [{"text": "CAAE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6000319123268127}, {"text": "MDAL", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.6762028932571411}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9990979433059692}, {"text": "Yelp dataset", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9907624125480652}, {"text": "Amazon dataset", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.9674886465072632}]}, {"text": "The low BLEU scores indicate the worrying content preservation performance to some extent.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9993430972099304}, {"text": "content preservation", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.6928873062133789}]}, {"text": "Even with the desired sentiment, the irrelevant generated text leads to worse overall performance.", "labels": [], "entities": []}, {"text": "In general, these two systems work more like sentiment-aware language models that generate text only based on the target sentiment and neglect the source input.", "labels": [], "entities": []}, {"text": "The main reason is that these two systems attempt to separate emotional information from non-emotional content in a hidden layer, where all information is complicatedly mixed together.", "labels": [], "entities": []}, {"text": "It is difficult to only modify emotional information without any loss of non-emotional semantic content.", "labels": [], "entities": []}, {"text": "In comparison, our proposed method achieves the best overall performance on the two datasets,  demonstrating the ability of learning knowledge from unpaired data.", "labels": [], "entities": []}, {"text": "This result is attributed to the improved BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9589501321315765}]}, {"text": "The BLEU score is largely improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9623652696609497}]}, {"text": "The score improvements mainly come from the fact that we separate emotional information from semantic content by explicitly filtering out emotional words.", "labels": [], "entities": []}, {"text": "The extracted content is preserved and fed into the emotionalization module.", "labels": [], "entities": []}, {"text": "Given the overall quality of transferred text as the reward, the neutralization module is taught to extract non-emotional semantic content better.", "labels": [], "entities": []}, {"text": "shows the human evaluation results.", "labels": [], "entities": []}, {"text": "It can be clearly seen that the proposed method obviously improves semantic preservation.", "labels": [], "entities": [{"text": "semantic preservation", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.8474873304367065}]}, {"text": "The semantic score is increased from 3.87 to 5.08 on the Yelp dataset, and from 3.22 to 4.67 on the Amazon dataset.", "labels": [], "entities": [{"text": "Yelp dataset", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9920664429664612}, {"text": "Amazon dataset", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.9641167223453522}]}, {"text": "In general, our proposed model achieves the best overall performance.", "labels": [], "entities": []}, {"text": "Furthermore, it also needs to be noticed that with the large improvement in content preservation, the sentiment accuracy of the proposed method is lower than that of CAAE on the two datasets.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7440399527549744}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9034090042114258}]}, {"text": "It shows that simultaneously promoting sentiment transformation and content preservation remains to be studied further.", "labels": [], "entities": [{"text": "sentiment transformation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.9403278529644012}, {"text": "content preservation", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.8333250284194946}]}, {"text": "By comparing two evaluation results, we find that there is an agreement between the human evaluation and the automatic evaluation.", "labels": [], "entities": []}, {"text": "It indicates the usefulness of automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "However, we also notice that the human evaluation has a smaller performance gap between the baselines and the proposed method than the automatic evaluation.", "labels": [], "entities": []}, {"text": "It shows the limitation of automatic metrics forgiving accurate results.", "labels": [], "entities": []}, {"text": "For evaluating sentiment transformation, even with a high accuracy, the sentiment classifier sometimes generates noisy results, especially for those neutral sentences (e.g., \"I ate a cheese sandwich\").", "labels": [], "entities": [{"text": "sentiment transformation", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.785761833190918}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.994531512260437}]}, {"text": "For evaluating content preservation, the BLEU score Input: I would strongly advise against using this company.", "labels": [], "entities": [{"text": "content preservation", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7581621408462524}, {"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9582481682300568}, {"text": "Input", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.5810049176216125}]}, {"text": "CAAE: I love this place fora great experience here.", "labels": [], "entities": [{"text": "CAAE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7159004807472229}]}, {"text": "MDAL: I have been a great place was great.", "labels": [], "entities": [{"text": "MDAL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9108065962791443}]}, {"text": "Proposed Method: I would love using this company.", "labels": [], "entities": []}, {"text": "Input: The service was nearly non-existent and extremely rude.", "labels": [], "entities": []}, {"text": "CAAE: The best place in the best area in vegas.", "labels": [], "entities": [{"text": "CAAE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6333156228065491}]}, {"text": "MDAL: The food is very friendly and very good.", "labels": [], "entities": [{"text": "MDAL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6850544214248657}]}, {"text": "Proposed Method: The service was served and completely fresh.", "labels": [], "entities": []}, {"text": "Input: Asked for the roast beef and mushroom sub, only received roast beef.", "labels": [], "entities": [{"text": "Input", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9529765248298645}]}, {"text": "CAAE: We had a great experience with.", "labels": [], "entities": [{"text": "CAAE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5875024795532227}]}, {"text": "MDAL: This place fora great place fora great food and best.", "labels": [], "entities": [{"text": "MDAL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7041316628456116}]}, {"text": "Proposed Method: Thanks for the beef and spring bbq.", "labels": [], "entities": []}, {"text": "Input: Worst cleaning job ever!", "labels": [], "entities": []}, {"text": "CAAE: Great food and great service!", "labels": [], "entities": []}, {"text": "MDAL: Great food, food!", "labels": [], "entities": [{"text": "MDAL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.46612435579299927}]}, {"text": "Proposed Method: Excellent outstanding job ever!", "labels": [], "entities": []}, {"text": "Input: Most boring show I've ever been.", "labels": [], "entities": [{"text": "Input", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8182966113090515}]}, {"text": "CAAE: Great place is the best place in town.", "labels": [], "entities": [{"text": "CAAE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6454545855522156}]}, {"text": "MDAL: Great place I've ever ever had.", "labels": [], "entities": [{"text": "MDAL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.976488471031189}]}, {"text": "Proposed Method: Most amazing show I've ever been.", "labels": [], "entities": []}, {"text": "Input: Place is very clean and the food is delicious.", "labels": [], "entities": [{"text": "Input", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9503179788589478}]}, {"text": "CAAE: Don't go to this place.", "labels": [], "entities": [{"text": "CAAE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7761853337287903}]}, {"text": "MDAL: This place wasn't worth the worst place is horrible.", "labels": [], "entities": [{"text": "MDAL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9699345827102661}]}, {"text": "Proposed Method: Place is very small and the food is terrible.", "labels": [], "entities": []}, {"text": "Input: Really satisfied with experience buying clothes.", "labels": [], "entities": [{"text": "Input", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9717282056808472}]}, {"text": "CAAE: Don't go to this place.", "labels": [], "entities": [{"text": "CAAE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7761853337287903}]}, {"text": "MDAL: Do not impressed with this place.", "labels": [], "entities": [{"text": "MDAL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.870457649230957}]}, {"text": "Proposed Method: Really bad experience.: Examples generated by the proposed approach and baselines on the Yelp dataset.", "labels": [], "entities": [{"text": "Yelp dataset", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.9820962250232697}]}, {"text": "The two baselines change not only the polarity of examples, but also the semantic content.", "labels": [], "entities": []}, {"text": "In comparison, our approach changes the sentiment of sentences with higher semantic similarity. is computed based on the percentage of overlapping n-grams between the generated text and the reference text.", "labels": [], "entities": []}, {"text": "However, the overlapping n-grams contain not only content words but also function words, bringing the noisy results.", "labels": [], "entities": []}, {"text": "In general, accurate automatic evaluation metrics are expected in future work.: Performance of key components in the proposed approach.", "labels": [], "entities": []}, {"text": "\"NM\" denotes the neutralization module.", "labels": [], "entities": []}, {"text": "\"Cycled RL\" represents cycled reinforcement learning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluations of the proposed  method and baselines. ACC evaluates sentiment  transformation. BLEU evaluates content preserva- tion. G-score is the geometric mean of ACC and  BLEU.", "labels": [], "entities": [{"text": "sentiment  transformation", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.8248705267906189}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9980596899986267}, {"text": "G-score", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.987114429473877}, {"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9936642646789551}]}, {"text": " Table 2: Human evaluations of the proposed  method and baselines. Sentiment evaluates senti- ment transformation. Semantic evaluates content  preservation.", "labels": [], "entities": [{"text": "senti- ment transformation", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.5878764763474464}, {"text": "content  preservation", "start_pos": 134, "end_pos": 155, "type": "TASK", "confidence": 0.777837872505188}]}, {"text": " Table 4: Performance of key components in the  proposed approach. \"NM\" denotes the neutraliza- tion module. \"Cycled RL\" represents cycled rein- forcement learning.", "labels": [], "entities": []}]}