{"title": [{"text": "Straight to the Tree: Constituency Parsing with Neural Syntactic Distance", "labels": [], "entities": [{"text": "Constituency Parsing", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.6165815889835358}]}], "abstractContent": [{"text": "In this work, we propose a novel constituency parsing scheme.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8660631477832794}]}, {"text": "The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence.", "labels": [], "entities": []}, {"text": "The syntactic distances specify the order in which the split points will be selected, recur-sively partitioning the input, in a top-down fashion.", "labels": [], "entities": []}, {"text": "Compared to traditional shift-reduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize.", "labels": [], "entities": []}, {"text": "Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.", "labels": [], "entities": [{"text": "PTB dataset", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.9806056022644043}, {"text": "CTB dataset", "start_pos": 146, "end_pos": 157, "type": "DATASET", "confidence": 0.9790217280387878}]}], "introductionContent": [{"text": "Devising fast and accurate constituency parsing algorithms is an important, long-standing problem in natural language processing.", "labels": [], "entities": [{"text": "constituency parsing algorithms", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.8197142680486044}, {"text": "natural language processing", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.6261936724185944}]}, {"text": "Parsing has been useful for incorporating linguistic prior in several related tasks, such as relation extraction, paraphrase detection, and more recently, natural language inference ( and machine translation (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8808971345424652}, {"text": "paraphrase detection", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.91721111536026}, {"text": "machine translation", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.7663774788379669}]}, {"text": "Neural network-based approaches relying on dense input representations have recently achieved competitive results for constituency parsing (;).", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.8725723922252655}]}, {"text": "Generally speaking, either these approaches produce the parse tree sequentially, by governing: An example of how syntactic distances (d1 and d2) describe the structure of a parse tree: consecutive words with larger predicted distance are split earlier than those with smaller distances, in a process akin to divisive clustering.", "labels": [], "entities": []}, {"text": "the sequence of transitions in a transition-based parser, or use a chart-based approach by estimating non-linear potentials and performing exact structured inference by dynamic programming (.", "labels": [], "entities": []}, {"text": "Transition-based models decompose the structured prediction problem into a sequence of local decisions.", "labels": [], "entities": []}, {"text": "This enables fast greedy decoding but also leads to compounding errors because the model is never exposed to its own mistakes during training ().", "labels": [], "entities": []}, {"text": "Solutions to this problem usually complexify the training procedure by using structured training through beamsearch ( and dynamic oracles (.", "labels": [], "entities": []}, {"text": "On the other hand, chartbased models can incorporate structured loss functions during training and benefit from exact inference via the CYK algorithm but suffer from higher computational cost during decoding.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel, fully-parallel model for constituency parsing, based on the concept of \"syntactic distance\", recently introduced by) for language modeling.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8623011708259583}, {"text": "language modeling", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.780450701713562}]}, {"text": "To construct a parse tree from a sentence, one can proceed in a top-down manner, recursively splitting larger constituents into smaller constituents, where the order of the splits defines the hierarchical structure.", "labels": [], "entities": []}, {"text": "The syntactic distances are defined for each possible split point in the sentence.", "labels": [], "entities": []}, {"text": "The order induced by the syntactic distances fully specifies the order in which the sentence needs to be recursively split into smaller constituents): in case of a binary tree, there exists a oneto-one correspondence between the ordering and the tree.", "labels": [], "entities": []}, {"text": "Therefore, our model is trained to reproduce the ordering between split points induced by the ground-truth distances by means of a margin rank loss.", "labels": [], "entities": []}, {"text": "Crucially, our model works in parallel: the estimated distance for each split point is produced independently from the others, which allows for an easy parallelization in modern parallel computing architectures for deep learning, such as GPUs.", "labels": [], "entities": []}, {"text": "Along with the distances, we also train the model to produce the constituent labels, which are used to build the fully labeled tree.", "labels": [], "entities": []}, {"text": "Our model is fully parallel and thus does not require computationally expensive structured inference during training.", "labels": [], "entities": []}, {"text": "Mapping from syntactic distances to a tree can be efficiently done in O(n log n), which makes the decoding computationally attractive.", "labels": [], "entities": [{"text": "O", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9748503565788269}]}, {"text": "Despite our strong conditional independence assumption on the output predictions, we achieve good performance for single model discriminative parsing in PTB (91.8 F1) and CTB (86.5 F1) matching, and sometimes outperforming, recent chart-based and transition-based parsing models.", "labels": [], "entities": [{"text": "single model discriminative parsing", "start_pos": 114, "end_pos": 149, "type": "TASK", "confidence": 0.592687152326107}, {"text": "PTB (91.8 F1)", "start_pos": 153, "end_pos": 166, "type": "METRIC", "confidence": 0.8676123023033142}, {"text": "CTB (86.5 F1)", "start_pos": 171, "end_pos": 184, "type": "METRIC", "confidence": 0.6903039991855622}]}], "datasetContent": [{"text": "We evaluate our model described above on 2 different datasets, the standard Wall Street Journal (WSJ) part of the Penn Treebank (PTB) dataset, and the Chinese Treebank (CTB) dataset.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 76, "end_pos": 101, "type": "DATASET", "confidence": 0.9475264151891073}, {"text": "Penn Treebank (PTB) dataset", "start_pos": 114, "end_pos": 141, "type": "DATASET", "confidence": 0.9708830912907919}, {"text": "Chinese Treebank (CTB) dataset", "start_pos": 151, "end_pos": 181, "type": "DATASET", "confidence": 0.9661990106105804}]}, {"text": "For evaluating the F1 score, we use the standard evalb 1 tool.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9846331775188446}]}, {"text": "We provide both labeled and unlabeled F1 score, where the former takes into consideration the constituent label for each predicted http://nlp.cs.nyu.edu/evalb/ constituent, while the latter only considers the position of the constituents.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9800428450107574}]}, {"text": "In the tables below, we report the labeled F1 scores for comparison with previous work, as this is the standard metric usually reported in the relevant literature.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.970575213432312}]}], "tableCaptions": [{"text": " Table 1: Results on the PTB dataset WSJ test set,  Section 23. LP, LR represents labeled precision  and recall respectively.", "labels": [], "entities": [{"text": "PTB dataset WSJ test set", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.9266765594482422}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.8564499020576477}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9990354776382446}]}, {"text": " Table 2: Test set performance comparison on the  CTB dataset", "labels": [], "entities": [{"text": "CTB dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.9716193675994873}]}, {"text": " Table 3: Detailed experimental results on PTB and CTB datasets", "labels": [], "entities": [{"text": "PTB and CTB datasets", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.7767903953790665}]}, {"text": " Table 5: Parsing speed in sentences per second on  the PTB dataset.", "labels": [], "entities": [{"text": "PTB dataset", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9856722950935364}]}]}