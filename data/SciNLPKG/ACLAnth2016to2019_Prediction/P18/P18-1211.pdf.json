{"title": [{"text": "A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a generative probabilistic model of documents as sequences of sentences, and show that inference in it can lead to extraction of long-range latent discourse structure from a collection of documents.", "labels": [], "entities": []}, {"text": "The approach is based on embedding sequences of sentences from longer texts into a 2-or 3-D spatial grids, in which one or two coordinates model smooth topic transitions , while the third captures the sequential nature of the modeled text.", "labels": [], "entities": []}, {"text": "A significant advantage of our approach is that the learned models are naturally visualiz-able and interpretable, as semantic similarity and sequential structure are modeled along orthogonal directions in the grid.", "labels": [], "entities": []}, {"text": "We show that the method can capture discourse structures in narrative text across multiple genres, including biographies, stories, and newswire reports.", "labels": [], "entities": []}, {"text": "In particular, our method can capture biographical templates from Wikipedia, and is competitive with state-of-the-art generative approaches on tasks such as predicting the outcome of a story, and sentence ordering.", "labels": [], "entities": [{"text": "predicting the outcome of a story", "start_pos": 157, "end_pos": 190, "type": "TASK", "confidence": 0.9008680582046509}, {"text": "sentence ordering", "start_pos": 196, "end_pos": 213, "type": "TASK", "confidence": 0.775649219751358}]}], "introductionContent": [{"text": "The ability to identify discourse patterns and narrative themes from language is useful in a wide range of applications and data analysis.", "labels": [], "entities": []}, {"text": "From a perspective of language understanding, learning such latent structure from large corpora can provide background information that can aid machine reading.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7168922871351242}, {"text": "machine reading", "start_pos": 144, "end_pos": 159, "type": "TASK", "confidence": 0.7979496419429779}]}, {"text": "For example, computers can use such knowledge to predict what is likely to happen next * *Work done while first author was an intern at Microsoft Research in a narrative (, or reason about which narratives are coherent and which do not make sense (.", "labels": [], "entities": []}, {"text": "Similarly, knowledge of discourse is increasingly important for language generation models.", "labels": [], "entities": [{"text": "language generation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7223667055368423}]}, {"text": "Modern neural generation models, while good at capturing surface properties of text -by fusing elements of syntax and style -are still poor at modeling long range dependencies that go across sentences (.", "labels": [], "entities": []}, {"text": "Models of long range flow in the text can thus be useful as additional input to such methods.", "labels": [], "entities": []}, {"text": "Previously, the question of modeling discourse structure in language has been explored through several lenses, including from perspectives of linguistics, cognitive science and information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 177, "end_pos": 198, "type": "TASK", "confidence": 0.7581896185874939}]}, {"text": "Prominent among linguistic approaches are Discourse Representation Theory and Rhetorical Structure Theory (; which formalize how discourse context can constrain the semantics of a sentence, and layout ontologies of discourse relation types between parts of a document.", "labels": [], "entities": [{"text": "Discourse Representation Theory", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7835728228092194}, {"text": "Rhetorical Structure Theory", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.6817609667778015}]}, {"text": "This line of research has been largely constrained by the unavailability of corpora of discourse relations, which are expensive to annotate.", "labels": [], "entities": []}, {"text": "Another line of research has focused on the task of automatic script induction, building on earlier work in the 1970's (.", "labels": [], "entities": [{"text": "automatic script induction", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6846579313278198}]}, {"text": "More recently, methods based on neural distributed representations have been explored () to model the flow of discourse.", "labels": [], "entities": []}, {"text": "While these methods have had varying degrees of success, they are largely opaque and hard to interpret.", "labels": [], "entities": []}, {"text": "In this work, we seek to provide a scalable model that can extract latent sequential structures from a collection of documents, and can be naturally visualized to provide a summary of the learned semantics and discourse trajectories.", "labels": [], "entities": []}, {"text": "In this work, we present an approach for extract-: Modeling principle for Sequential Counting Grids.", "labels": [], "entities": []}, {"text": "We design the method to capture semantic similarities between documents along XY planes (e.g., biographies might be more similar to literary fiction than news reports), as well extract sequential trajectories along the Z axes similar to those shown.", "labels": [], "entities": []}, {"text": "The sequence of sentences in a document is latently aligned to positions in the grid, such that the model prefers alignments of contiguous sentences to grid cells that are spatially close.", "labels": [], "entities": []}, {"text": "ing and visualizing sequential structure from a collection of text documents.", "labels": [], "entities": []}, {"text": "Our method is based on embedding sentences in a document in a 3-dimensional grid, such that contiguous sentences in the document are likely to be embedded in the same order in the grid.", "labels": [], "entities": []}, {"text": "Further, sentences across documents that are semantically similar are also likely to be embedded in the same neighborhood in the grid.", "labels": [], "entities": []}, {"text": "By leveraging the sequential order of sentences in a large document collection, the method can induce lexical semantics, as well as extract latent discourse trajectories in the documents.", "labels": [], "entities": []}, {"text": "shows a conceptual schematic of our approach.", "labels": [], "entities": []}, {"text": "The method can learn semantic similarity (across XY planes), as well as sequential discourse chains (along the Z-axis).", "labels": [], "entities": []}, {"text": "The parameters and latent structure of the grid are learned by optimizing the likelihood of a collection of documents under a generative model.", "labels": [], "entities": []}, {"text": "Our method outperforms state-of-the-art generative methods on two tasks: predicting the outcome of a story and coherence prediction; and is seen to yield a flexible range of interpretable visualizations in different domains of text.", "labels": [], "entities": [{"text": "predicting the outcome of a story", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.8797912001609802}, {"text": "coherence prediction", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.6677290946245193}]}, {"text": "Our method is scalable, and can incorporate abroad range of features.", "labels": [], "entities": []}, {"text": "In particular, the approach can work on simple tokenized text.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly summarize other related work.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our method in detail.", "labels": [], "entities": []}, {"text": "We present experimental results in Section 4, and conclude with a brief discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we analyze the performance of our approach on text collections from several domains (including short stories, newswire text and biographies).", "labels": [], "entities": []}, {"text": "We first qualitatively evaluate our generative method on a dataset of biographical extracts from Wikipedia, which visually illustrates biographical trajectories learned by the model, operationalizing our model concept from in real data (see).", "labels": [], "entities": []}, {"text": "Next, we evaluate our method on two standard tasks requiring document understanding: story cloze evaluation and sentence ordering.", "labels": [], "entities": [{"text": "document understanding", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.6803781986236572}, {"text": "story cloze evaluation", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.5761633217334747}, {"text": "sentence ordering", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7454580962657928}]}, {"text": "Since our method is completely unsupervised and is not tailored to specific tasks, competitive performance on these tasks would indicate that the method learns helpful regularities in text structure, useful for general-purpose language understanding.", "labels": [], "entities": [{"text": "general-purpose language understanding", "start_pos": 211, "end_pos": 249, "type": "TASK", "confidence": 0.7177862326304117}]}], "tableCaptions": [{"text": " Table 1: Performance of our approach on story- cloze task from Mostafazadeh et al.", "labels": [], "entities": []}, {"text": " Table 2: Performance of our approach on sentence  ordering dataset from", "labels": [], "entities": [{"text": "sentence  ordering", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7137787193059921}]}]}