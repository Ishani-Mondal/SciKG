{"title": [{"text": "IIT (BHU) Varanasi at MSR-SRST 2018: A Language Model Based Approach for Natural Language Generation", "labels": [], "entities": [{"text": "IIT (BHU) Varanasi at MSR-SRST 2018", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.6363982111215591}]}], "abstractContent": [{"text": "This paper describes our submission system for the Shallow Track of Surface Realization Shared Task 2018 (SRST'18).", "labels": [], "entities": [{"text": "Shallow Track of Surface Realization Shared Task 2018 (SRST'18)", "start_pos": 51, "end_pos": 114, "type": "TASK", "confidence": 0.7281349030407992}]}, {"text": "The task was to convert genuine UD structures , from which word order information had been removed and the tokens had been lemmatized, into their correct senten-tial form.", "labels": [], "entities": []}, {"text": "We divide the problem statement into two parts, word reinflection and correct word order prediction.", "labels": [], "entities": [{"text": "word order prediction", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.5877189834912618}]}, {"text": "For the first sub-problem, we use a Long Short Term Memory based Encoder-Decoder approach.", "labels": [], "entities": []}, {"text": "For the second sub-problem, we present a Language Model (LM) based approach.", "labels": [], "entities": []}, {"text": "We apply two different sub-approaches in the LM Based approach and the combined result of these two approaches is considered as the final output of the system.", "labels": [], "entities": []}], "introductionContent": [{"text": "SRST'18 (, organized under ACL 2018, Melbourne, Australia aims to re-obtain the word order information which has been removed from the UD Structures (.", "labels": [], "entities": [{"text": "SRST'18", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8537461161613464}]}, {"text": "Universal Dependency (UD) structure is a tree representation of the dependency relations between words in a sentence of any language.", "labels": [], "entities": [{"text": "Universal Dependency (UD) structure", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5802392909924189}]}, {"text": "Made using the UD framework, the structure of the tree is determined by the relation between a word and its dependents.", "labels": [], "entities": [{"text": "UD framework", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.8623137772083282}]}, {"text": "Each node of this tree holds the Part of Speech (PoS) tag and morphological information as found in the original annotations of the word corresponding to that node.", "labels": [], "entities": [{"text": "Part of Speech (PoS) tag", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.605478069611958}]}, {"text": "The morphological information of a word includes the information gained from the formation of the word and its relationship with other words.", "labels": [], "entities": []}, {"text": "Morphological information includes gender, animacy, number, mood, tense etc.", "labels": [], "entities": []}, {"text": "In this problem, we are given 1.", "labels": [], "entities": []}, {"text": "Unordered dependency trees with lemmatized nodes.", "labels": [], "entities": []}, {"text": "2. The nodes hold PoS tags and morphological information as found in the original annotations.", "labels": [], "entities": []}, {"text": "3. The corresponding ordered sentences.", "labels": [], "entities": []}, {"text": "Our system may find its use in various NLP applications like Natural Language Generation (NLG)).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG))", "start_pos": 61, "end_pos": 95, "type": "TASK", "confidence": 0.7976093192895254}]}, {"text": "NLG is a major and relatively unexplored sub-field of NLP.", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9536249041557312}]}, {"text": "Our system can be used in tasks like Question Answering, where you have the knowledge base with you which may not necessarily be holding the correct word order information but must be holding the dependencies between the words.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8100663423538208}]}, {"text": "This is where NLG is useful, where you take all the dependencies available with you and try to generate language from it which can be understood and interpreted easily by the person or user.", "labels": [], "entities": []}, {"text": "This system also finds its application in other important tasks like abstractive text summarization) and image caption generation (, since having the correct word order is a must for any text.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.6212330559889475}, {"text": "image caption generation", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.8728782335917155}]}, {"text": "Our system makes use of a Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) based Encoder-Decoder ( approach to tackle the subproblem-1 of this track, i.e word re-inflection and then we make use of a probabilistic and statistical Language Model to determine the correct word order from the unordered sentences.", "labels": [], "entities": []}, {"text": "Statistical Language Modeling, or Language Modeling or LM in short, is a technique which uses probabilistic models that are able to predict the next word in the sequence given the words that precede it.", "labels": [], "entities": [{"text": "Statistical Language Modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7807515462239584}, {"text": "Language Modeling or LM", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.80861034989357}]}, {"text": "This is: Architecture of the Proposed Model -Word sequence (w 1 , w 2 , w 3 , ...,w n ) is reinfected into (w 1 , w 2 , w 3 , ..., w m ), where w i are the changed words due to reinfection.", "labels": [], "entities": []}, {"text": "Final output gives the LM Score for the sequence of reinflected words.", "labels": [], "entities": [{"text": "LM Score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9162276983261108}]}, {"text": "Model is run on different possible combinations and the sequence with best LM Score is chosen.", "labels": [], "entities": [{"text": "LM Score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8437845408916473}]}, {"text": "done by assigning a probability to the whole sequence.", "labels": [], "entities": []}, {"text": "The shared task organizers provided the training and a small development dataset for building our systems.", "labels": [], "entities": []}, {"text": "A period of about 3 weeks was given for submitting our predictions on the test set.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses, in brief, the dataset for the task.", "labels": [], "entities": []}, {"text": "Section 3 explains our proposed approach in detail.", "labels": [], "entities": []}, {"text": "We discuss what models we have used to re-inflect the words and generate ordered sentences from the jumbled sentences.", "labels": [], "entities": []}, {"text": "Section 4 explains how the system is evaluated and Section 5 states the results we have obtained.", "labels": [], "entities": []}, {"text": "We have also included an analysis of our system in Section 6.", "labels": [], "entities": []}, {"text": "We conclude our paper and discuss its future prospects in Section 7.", "labels": [], "entities": [{"text": "Section 7", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8503189384937286}]}], "datasetContent": [{"text": "Cross Validation (CV): We trained our model on the training data and did predictions on the development data, both of which were provided by the shared task organizers.", "labels": [], "entities": [{"text": "Cross Validation (CV)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7889903247356415}]}, {"text": "These predictions were considered as the CV Score of our model.", "labels": [], "entities": [{"text": "CV Score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.5068295747041702}]}, {"text": "The metrics that were used to evaluate the model were BLEU (), NE DIST and NIST).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9987302422523499}, {"text": "NE DIST", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.7174166142940521}, {"text": "NIST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.6496804356575012}]}, {"text": "Evaluation script for the same was also provided by the organizers.", "labels": [], "entities": []}, {"text": "Test: Once we were done with the optimal tuning of our model using the CV score, we used our model to generate ordered sentences on the test data.", "labels": [], "entities": []}, {"text": "We trained on the full training data for the re-inflection task and combined the training and development data to generate the language model (.lm) file for the word-ordering task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scores for English on test data.", "labels": [], "entities": []}]}