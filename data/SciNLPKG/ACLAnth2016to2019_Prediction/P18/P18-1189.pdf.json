{"title": [{"text": "Neural Models for Documents with Metadata", "labels": [], "entities": []}], "abstractContent": [{"text": "Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information.", "labels": [], "entities": []}, {"text": "While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm.", "labels": [], "entities": []}, {"text": "In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models.", "labels": [], "entities": []}, {"text": "Our approach achieves strong performance, with a manageable tradeoff between perplex-ity, coherence, and sparsity.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models comprise a family of methods for uncovering latent structure in text corpora, and are widely used tools in the digital humanities, political science, and other related fields.", "labels": [], "entities": []}, {"text": "Latent Dirichlet allocation (LDA;) is often used when there is no prior knowledge about a corpus.", "labels": [], "entities": [{"text": "Latent Dirichlet allocation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7220397790273031}]}, {"text": "In the real world, however, most documents have non-textual attributes such as author), timestamp ), rating, or ideology (), which we refer to as metadata.", "labels": [], "entities": []}, {"text": "Many customizations of LDA have been developed to incorporate document metadata.", "labels": [], "entities": []}, {"text": "Two models of note are supervised LDA, which jointly models words and labels (e.g., ratings) as being generated from a latent representation, and sparse additive generative models (SAGE;), which assumes that observed covariates (e.g., author ideology) have a sparse effect on the relative probabilities of words given topics.", "labels": [], "entities": []}, {"text": "The structural topic model (STM;), which adds correlations between topics to SAGE, is also widely used, but like SAGE it is limited in the types of metadata it can efficiently make use of, and how that metadata is used.", "labels": [], "entities": []}, {"text": "Note that in this work we will distinguish labels (metadata that are generated jointly with words from latent topic representations) from covariates (observed metadata that influence the distribution of labels and words).", "labels": [], "entities": []}, {"text": "The ability to create variations of LDA such as those listed above has been limited by the expertise needed to develop custom inference algorithms for each model.", "labels": [], "entities": []}, {"text": "As a result, it is rare to see such variations being widely used in practice.", "labels": [], "entities": []}, {"text": "In this work, we take advantage of recent advances in variational methods () to facilitate approximate Bayesian inference without requiring model-specific derivations, and propose a general neural framework for topic models with metadata, SCHOLAR.", "labels": [], "entities": []}, {"text": "SCHOLAR combines the abilities of SAGE and SLDA, and allows for easy exploration of the following options for customization: 1.", "labels": [], "entities": []}, {"text": "Covariates: as in SAGE and STM, we incorporate explicit deviations for observed covariates, as well as effects for interactions with topics.", "labels": [], "entities": []}, {"text": "2. Supervision: as in SLDA, we can use metadata as labels to help infer topics that are relevant in predicting those labels.", "labels": [], "entities": [{"text": "Supervision", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9712668061256409}]}, {"text": "3. Rich encoder network: we use the encoding network of a variational autoencoder (VAE) to incorporate additional prior knowledge in the form of word embeddings, and/or to provide interpretable embeddings of covariates.", "labels": [], "entities": []}, {"text": "4. Sparsity: as in SAGE, a sparsity-inducing prior can be used to encourage more interpretable topics, represented as sparse deviations from a background log-frequency.", "labels": [], "entities": []}, {"text": "We begin with the necessary background and motivation ( \u00a72), and then describe our basic framework and its extensions ( \u00a73), followed by a series of experiments ( \u00a74).", "labels": [], "entities": []}, {"text": "In an unsupervised setting, we can customize the model to trade off between perplexity, coherence, and sparsity, with improved coherence through the introduction of word vectors.", "labels": [], "entities": []}, {"text": "Alternatively, by incorporating metadata we can either learn topics that are more predictive of labels than SLDA, or learn explicit deviations for particular parts of the metadata.", "labels": [], "entities": []}, {"text": "Finally, by combining all parts of our model we can meaningfully incorporate metadata in multiple ways, which we demonstrate through an exploration of a corpus of news articles about US immigration.", "labels": [], "entities": []}, {"text": "In presenting this particular model, we emphasize not only its ability to adapt to the characteristics of the data, but the extent to which the VAE approach to inference provides a powerful framework for latent variable modeling that suggests the possibility of many further extensions.", "labels": [], "entities": []}, {"text": "Our implementation is available at https://github.", "labels": [], "entities": []}, {"text": "com/dallascard/scholar.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate and demonstrate the potential of this model, we present a series of experiments below.", "labels": [], "entities": []}, {"text": "We first test SCHOLAR without observed metadata, and explore the effects of using regularization and/or word vector initialization, compared to LDA, SAGE, and NVDM ( \u00a74.1).", "labels": [], "entities": [{"text": "word vector initialization", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.6364632646242777}, {"text": "NVDM", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.8840409517288208}]}, {"text": "We then evaluate our model in terms of predictive performance, in comparison to SLDA and an l 2 -regularized logistic regression baseline.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate the ability to incorporate covariates and/or labels in an exploratory data analysis ( \u00a74.3).", "labels": [], "entities": []}, {"text": "The scores we report are generalization to heldout data, measured in terms of perplexity; coherence, measured in terms of non-negative point-wise mutual information (NPMI;, and classification accuracy on test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9157509207725525}]}, {"text": "For coherence we evaluate NPMI using the top 10 words of each topic, both internally (using test data), and externally, using a decade of articles from the English Gigaword dataset ().", "labels": [], "entities": [{"text": "English Gigaword dataset", "start_pos": 156, "end_pos": 180, "type": "DATASET", "confidence": 0.8653664390246073}]}, {"text": "Since our model employs variational methods, the reported perplexity is an upper bound based on the ELBO.", "labels": [], "entities": [{"text": "ELBO", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9931648373603821}]}, {"text": "As datasets we use the familiar 20 newsgroups, the IMDB corpus of 50,000 movie reviews, and the UIUC Yahoo answers dataset with 150,000 documents in 15 categories (.", "labels": [], "entities": [{"text": "IMDB corpus", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9445745050907135}, {"text": "UIUC Yahoo answers dataset", "start_pos": 96, "end_pos": 122, "type": "DATASET", "confidence": 0.9436009526252747}]}, {"text": "For further exploration, we also make use of a corpus of approximately 4,000 timestamped news articles about US immigration, each annotated with pro-or anti-immigration tone.", "labels": [], "entities": []}, {"text": "We use the original author-provided implementations of SAGE 11 and SLDA, 12 while for LDA we use Mallet.", "labels": [], "entities": [{"text": "Mallet", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.9765384793281555}]}, {"text": "Our implementation of SCHOLAR is in TensorFlow, but we have also provided a preliminary PyTorch implementation of the core of our model.", "labels": [], "entities": []}, {"text": "For additional details about datasets and implementation, please refer to the supplementary material.", "labels": [], "entities": []}, {"text": "It is challenging to fairly evaluate the relative computational efficiency of our approach compared to past work (due to the stochastic nature of our ap-proach to inference, choices about hyperparameters such as tolerance, and because of differences in implementation).", "labels": [], "entities": []}, {"text": "Nevertheless, in practice, the performance of our approach is highly appealing.", "labels": [], "entities": []}, {"text": "For all experiments in this paper, our implementation was much faster than SLDA or SAGE (implemented in C and Matlab, respectively), and competitive with Mallet.", "labels": [], "entities": [{"text": "Mallet", "start_pos": 154, "end_pos": 160, "type": "DATASET", "confidence": 0.9747742414474487}]}, {"text": "Although the emphasis of this work is on incorporating observed labels and/or covariates, we briefly report on experiments in the unsupervised setting.", "labels": [], "entities": []}, {"text": "Recall that, without metadata, SCHOLAR equates to ProdLDA, but with an explicit background term.", "labels": [], "entities": [{"text": "SCHOLAR", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.7673271298408508}, {"text": "ProdLDA", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9153649806976318}]}, {"text": "We therefore use the same experimental setup as Srivastava and Sutton (2017) (learning rate, momentum, batch size, and number of epochs) and find the same general patterns as they reported (see and supplementary material): our model returns more coherent topics than LDA, but at the cost of worse perplexity.", "labels": [], "entities": []}, {"text": "SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA.", "labels": [], "entities": []}, {"text": "As expected, the NVDM produces relatively low perplexity, but very poor coherence, due to its lack of constraints on \u03b8.", "labels": [], "entities": []}, {"text": "Further experimentation revealed that the VAE framework involves a tradeoff among the scores; running for more epochs tends to result in better perplexity on held-out data, but at the cost of worse coherence.", "labels": [], "entities": []}, {"text": "Adding regularization to encourage sparse topics has a similar effect as in SAGE, leading to worse perplexity and coherence, but it does create sparse topics.", "labels": [], "entities": []}, {"text": "Interestingly, initializing the encoder with pretrained word2vec embeddings, and not updating them returned a model with the best internal coherence of any model we considered for IMDB and Yahoo answers, and the second-best for 20 newsgroups.", "labels": [], "entities": []}, {"text": "The background term in our model does not have much effect on perplexity, but plays an important role in producing coherent topics; as in SAGE, the background can account for common words, so they are mostly absent among the most heavily weighted words in the topics.", "labels": [], "entities": []}, {"text": "For instance, words like film and movie in the IMDB corpus are relatively unimportant in the topics learned by our: Performance of our various models in an unsupervised setting (i.e., without labels or covariates) on the IMDB dataset using a 5,000-word vocabulary and 50 topics.", "labels": [], "entities": [{"text": "IMDB corpus", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9103288352489471}, {"text": "IMDB dataset", "start_pos": 221, "end_pos": 233, "type": "DATASET", "confidence": 0.9683815836906433}]}, {"text": "The supplementary materials contain additional results for 20 newsgroups and Yahoo answers.", "labels": [], "entities": []}, {"text": "model, but would be much more heavily weighted without the background term, as they are in topics learned by LDA.", "labels": [], "entities": [{"text": "LDA", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.8236528635025024}]}], "tableCaptions": [{"text": " Table 1: Performance of our various models in  an unsupervised setting (i.e., without labels or co- variates) on the IMDB dataset using a 5,000-word  vocabulary and 50 topics. The supplementary ma- terials contain additional results for 20 newsgroups  and Yahoo answers.", "labels": [], "entities": [{"text": "IMDB dataset", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.9794996082782745}]}, {"text": " Table 2: Accuracy of various models on three  datasets with categorical labels.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9841232895851135}]}]}