{"title": [{"text": "Code-Switched Named Entity Recognition with Embedding Attention", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.5666060249010721}]}], "abstractContent": [{"text": "We describe our work for the CALCS 2018 shared task on named entity recognition on code-switched data.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.5884289443492889}]}, {"text": "Our system ranked first place for MS Arabic-Egyptian named entity recognition and third place for English-Spanish.", "labels": [], "entities": [{"text": "MS Arabic-Egyptian named entity recognition", "start_pos": 34, "end_pos": 77, "type": "TASK", "confidence": 0.6506341159343719}]}], "introductionContent": [{"text": "The tendency for multilingual speakers to engage in code-switching-i.e, alternating between multiple languages or language varieties-poses important problems for NLP systems: traditional monolingual techniques quickly breakdown with input from mixed languages.", "labels": [], "entities": []}, {"text": "Even for problems such as POS-tagging and language identification, which the community often considers \"solved\", performance deteriorates proportional to the degree of code-switching in the data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7421349287033081}]}, {"text": "The shared task for the third workshop on Computational Approaches on Linguistic Code-Switching concerned named entity recognition (NER) for two code-switched language pairs (: Modern Standard Arabic and Egyptian (MSA-EGY); and English-Spanish (ENG-SPA).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.733891487121582}]}, {"text": "Here, we describe our work on the shared task.", "labels": [], "entities": []}, {"text": "Traditional NER systems used to rely heavily on hand-crafted features and gazetteers, but have since been replaced by neural architectures that combine bidirectional LSTMs and CRFs (.", "labels": [], "entities": []}, {"text": "Equipped with supervised characterlevel representations and pre-trained unsupervised word embeddings, such neural architectures have not only come to dominate named entity recognition, but have also successfully been applied to code-switched language identification (, which makes them highly suitable for the current task as well.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 159, "end_pos": 183, "type": "TASK", "confidence": 0.6728544334570566}, {"text": "code-switched language identification", "start_pos": 228, "end_pos": 265, "type": "TASK", "confidence": 0.6179563800493876}]}, {"text": "In this paper, we exploit recent advances in neural NLP systems, tailored to code-switching.", "labels": [], "entities": []}, {"text": "We use high-quality FastText embeddings trained on Common Crawl ( ) and employ shortcut-stacked sentence encoders) to obtain deep token-level representations to feed into the CRF.", "labels": [], "entities": []}, {"text": "In addition, we make use of an embedding-level attention mechanism that learns task-specific attention weights for multilingual and character-level representations, inspired by context-attentive embeddings (.", "labels": [], "entities": []}, {"text": "In what follows, we describe our system in detail.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for ENG-SPA.", "labels": [], "entities": [{"text": "ENG-SPA", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.576361894607544}]}, {"text": " Table 2: Results for MSA-EGY.", "labels": [], "entities": [{"text": "MSA-EGY", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.5688518285751343}]}, {"text": " Table 3: ENG-SPA test performance breakdown.", "labels": [], "entities": []}]}