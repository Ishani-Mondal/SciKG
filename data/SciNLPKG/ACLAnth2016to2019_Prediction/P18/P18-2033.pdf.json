{"title": [{"text": "Task-oriented Dialogue System for Automatic Diagnosis", "labels": [], "entities": [{"text": "Automatic Diagnosis", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7443662583827972}]}], "abstractContent": [{"text": "In this paper, we make a move to build a dialogue system for automatic diagnosis.", "labels": [], "entities": [{"text": "automatic diagnosis", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8482950627803802}]}, {"text": "We first build a dataset collected from an online medical forum by extracting symptoms from both patients' self-reports and conversational data between patients and doctors.", "labels": [], "entities": []}, {"text": "Then we propose a task-oriented dialogue system framework to make the diagnosis for patients automatically , which can converse with patients to collect additional symptoms beyond their self-reports.", "labels": [], "entities": []}, {"text": "Experimental results on our dataset show that additional symptoms extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these symptoms automatically and make a better diagnosis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.999035120010376}, {"text": "disease identification", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7253755182027817}]}], "introductionContent": [{"text": "Automatic phenotype identification using electronic health records (EHRs) has been a rising topic in recent years (.", "labels": [], "entities": [{"text": "Automatic phenotype identification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.668553759654363}]}, {"text": "Researchers explore with various machine learning approaches to identify symptoms and diseases for patients given multiple types of information (both numerical data and pure texts).", "labels": [], "entities": []}, {"text": "Experimental results prove the effectiveness of the identification of heart failure (, type 2 diabetes (, autism spectrum disorders), infection detection ( etc.", "labels": [], "entities": [{"text": "identification", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.9699698686599731}, {"text": "infection detection", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7384103536605835}]}, {"text": "Currently, most attempts focus on some * Corresponding author specific types of diseases and it is difficult to transfer models from one disease to another.", "labels": [], "entities": []}, {"text": "In general, each EHR contains multiple types of data, including personal information, admission note, diagnose tests, vital signs and medical image.", "labels": [], "entities": []}, {"text": "And it is collected accumulatively following a diagnostic procedure in clinic, which involves interactions between patients and doctors and some complicated medical tests.", "labels": [], "entities": []}, {"text": "Therefore, it is very expensive to collect EHRs for different diseases.", "labels": [], "entities": []}, {"text": "How to collect the information from patient automatically remains the challenge for automatic diagnosis.", "labels": [], "entities": [{"text": "automatic diagnosis", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.6695145070552826}]}, {"text": "Recently, due to its promising potentials and alluring commercial values, research about taskoriented dialogue system (DS) has attracted increasing attention in different domains, including ticket booking (), online shopping () and restaurant searching.", "labels": [], "entities": [{"text": "taskoriented dialogue system (DS)", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.6543721556663513}, {"text": "ticket booking", "start_pos": 190, "end_pos": 204, "type": "TASK", "confidence": 0.8019803166389465}, {"text": "restaurant searching", "start_pos": 232, "end_pos": 252, "type": "TASK", "confidence": 0.741622805595398}]}, {"text": "We believe that applying DS in the medical domain has great potential to reduce the cost of collecting data from patients.", "labels": [], "entities": []}, {"text": "However, there is a gap to fill for applying DS in disease identification.", "labels": [], "entities": [{"text": "disease identification", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7167956233024597}]}, {"text": "There are basically two major challenges.", "labels": [], "entities": []}, {"text": "First, the lack of annotated medical dialogue dataset.", "labels": [], "entities": []}, {"text": "Second, no available DS framework for disease identification.", "labels": [], "entities": [{"text": "disease identification", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7492981553077698}]}, {"text": "By addressing these two problems, we make the first move to build a dialogue system facilitating automatic information collection and diagnosis making for medical domain.", "labels": [], "entities": [{"text": "automatic information collection", "start_pos": 97, "end_pos": 129, "type": "TASK", "confidence": 0.6622334520022074}, {"text": "diagnosis making", "start_pos": 134, "end_pos": 150, "type": "TASK", "confidence": 0.9203384220600128}]}, {"text": "Contributions are two-fold: \u2022 We annotate the first medical dataset for dialogue system that consists of two parts, one is self-reports from patients and the other is conversational data between patients and doctors.", "labels": [], "entities": []}, {"text": "\u2022 We propose a reinforcement learning based framework for medical DS.", "labels": [], "entities": [{"text": "medical DS", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.6825195550918579}]}, {"text": "Experiment results on our dataset show that our dialogue system is able to collect symptoms from patients via conversation and improve the accuracy for automatic diagnosis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9992501139640808}, {"text": "automatic diagnosis", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.6481672376394272}]}], "datasetContent": [{"text": "Our dataset is collected from the pediatric department in a Chinese online healthcare community . It is a popular website for users to inquire with doctors online.", "labels": [], "entities": []}, {"text": "Usually, a patient would provide apiece of self-report presenting his/her basic conditions.", "labels": [], "entities": []}, {"text": "Then a doctor will initialize a conversation to collect more information and make a diagnosis based on both the self-report and the conversational data.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "As we can see, the doctor can obtain additional symptoms during conversation beyond the self-report.", "labels": [], "entities": []}, {"text": "For each patient, we can also obtain the final diagnosis from doctors as the label.", "labels": [], "entities": []}, {"text": "For clarity, we term symptoms from self-reports as explicit symptoms while those from conversational data as implicit symptoms.", "labels": [], "entities": []}, {"text": "We choose four types of diseases for annotation, including upper respiratory infection, children functional dyspepsia, infantile diarrhea and children's bronchitis.", "labels": [], "entities": [{"text": "upper respiratory infection", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6896769801775614}]}, {"text": "We invite three annotators (one with medical background) to label all the symptom phrases in both self-reports and conversational data.", "labels": [], "entities": []}, {"text": "The annotation is performed in two steps, namely symptom extraction and symptom normalization.", "labels": [], "entities": [{"text": "symptom extraction", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8336523473262787}]}, {"text": "Symptom Extraction We follow the BIO (begin-in-out) schema for symptom identification).", "labels": [], "entities": [{"text": "Symptom Extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9351309239864349}, {"text": "BIO", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9915012121200562}, {"text": "symptom identification", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7954308986663818}]}, {"text": "Each Chinese character is assigned a label of \"B\", \"I\" or \"O\".", "labels": [], "entities": []}, {"text": "Also, each extracted symptom expression is tagged with True or False indicating whether the patient suffers from this symptom or not.", "labels": [], "entities": [{"text": "True", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9930324554443359}, {"text": "False", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.8243206143379211}]}, {"text": "In order to improve the annotation agreement between annotators, we create two guidelines for the self-report and the conversational data respectively.", "labels": [], "entities": []}, {"text": "Each record is annotated by at least two annotators.", "labels": [], "entities": []}, {"text": "Any inconsistency would be further judged by the third one.", "labels": [], "entities": []}, {"text": "The Cohen's kappa coefficient between two annotators are 71% and 67% for self-reports and conversations respectively.: An example of a user record.", "labels": [], "entities": []}, {"text": "Each record consists of two parts: self-report from the patient and the conversation between the doctor and the patient.", "labels": [], "entities": []}, {"text": "Underlined phrases are symptom expressions.", "labels": [], "entities": []}, {"text": "\u4f4e\u70ed(low-grade fever): Examples of extracted symptom expressions and the related concepts in SNOMED CT.", "labels": [], "entities": [{"text": "low-grade fever)", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.633196234703064}, {"text": "SNOMED CT", "start_pos": 91, "end_pos": 100, "type": "TASK", "confidence": 0.6766210496425629}]}, {"text": "Symptom Normalization After symptom expression identification, medical experts manually link each symptom expression to the most relevant concept on SNOMED CT 2 for normalization.", "labels": [], "entities": [{"text": "Symptom Normalization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9210357367992401}, {"text": "symptom expression identification", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.7132574121157328}, {"text": "SNOMED CT 2", "start_pos": 149, "end_pos": 160, "type": "DATASET", "confidence": 0.7986447612444559}]}, {"text": "shows some phrases that describe symptoms in the example and some related concepts in SNOMED CT.", "labels": [], "entities": [{"text": "SNOMED CT", "start_pos": 86, "end_pos": 95, "type": "TASK", "confidence": 0.5846490561962128}]}, {"text": "The overview of dataset is presented in.", "labels": [], "entities": []}, {"text": "After symptom extraction and normalization, there are 144 unique symptoms identified.", "labels": [], "entities": [{"text": "symptom extraction", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7813944816589355}]}, {"text": "In order to reduce the size of action space of the DS, only 67 symptoms with a frequency greater than or equal to 10 are kept.", "labels": [], "entities": []}, {"text": "Samples are then generated, called user goal.", "labels": [], "entities": []}, {"text": "As we know, each user goal (see) is derived from one real world patient record 3 .   The max dialogue turn T is 22.", "labels": [], "entities": [{"text": "max dialogue turn T", "start_pos": 89, "end_pos": 108, "type": "METRIC", "confidence": 0.8448512703180313}]}, {"text": "A positive reward of +44 is given to the agent at the end of a success dialogue, and a \u221222 reward is given to a failure one.", "labels": [], "entities": []}, {"text": "We apply a step penalty of \u22121 for each turn to encourage shorter dialogues.", "labels": [], "entities": []}, {"text": "The dataset is divided into two parts: 80% for training with 568 user goals and 20% for testing with 142 user goals.", "labels": [], "entities": []}, {"text": "The of -greedy strategy is set to 0.1 for effective action space exploration and the \u03b3 in Bellman equation is 0.9.", "labels": [], "entities": [{"text": "action space exploration", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.6702640354633331}]}, {"text": "The size of buffer Dis 10000 and the batch size is 30.", "labels": [], "entities": [{"text": "Dis", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.8724191188812256}]}, {"text": "And the neural network of DQN is a single layer network.", "labels": [], "entities": []}, {"text": "The learning rate is 0.001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9841130673885345}]}, {"text": "Each simulation epoch consists of 100 dialogue sessions and the current network is evaluated on 500 dialogue sessions at the end of each epoch.", "labels": [], "entities": []}, {"text": "Before training, the buffer is pre-filled with the experiences of the rule-based agent (see below) to warm start our dialogue system.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the proposed framework, we compare our model with baselines in terms of three evaluation metrics following  and, namely, success rate, average reward and the average number of turns per dialogue session.", "labels": [], "entities": []}, {"text": "As for classification models, we use accuracy as the metric.", "labels": [], "entities": [{"text": "classification", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.9615181088447571}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9993088245391846}]}, {"text": "The baselines include: (1) SVM: This model treats the automatic diagnosis as a multi-class classification problem.", "labels": [], "entities": [{"text": "SVM", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.881074845790863}, {"text": "automatic diagnosis", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.6697491705417633}]}, {"text": "It takes one-hot representation of symptoms in the user goal as input, and predicts the disease.", "labels": [], "entities": []}, {"text": "There are two configurations: one takes both explicit and implicit symptoms as input (denoted as SVM-ex&im), and the other takes only explicit symptoms to predict the disease (denoted as SVM-ex).", "labels": [], "entities": []}, {"text": "(2) Random Agent: At each turn, the random agent takes an action randomly from the action space as the response to the user's action.", "labels": [], "entities": []}, {"text": "(3) Rule-based Agent: The rule-based agent takes an action based on handcrafted rules.", "labels": [], "entities": []}, {"text": "Conditioned on the current dialogue state st , the agent will inform disease if all the known symptoms related are detected.", "labels": [], "entities": []}, {"text": "If no disease can be identified, the agent will select one of the left symptoms randomly to inform.", "labels": [], "entities": []}, {"text": "The relations between diseases and symptoms are extracted from the annotated corpus in advance.", "labels": [], "entities": []}, {"text": "In this work, only the first T /2.5 4 symptoms with high frequency are kept for each disease so that the rule-based agent could inform a disease within the max dialogue turn T . shows the accuracy of two SVM-based models.", "labels": [], "entities": [{"text": "T /2.5 4 symptoms", "start_pos": 29, "end_pos": 46, "type": "METRIC", "confidence": 0.8851009964942932}, {"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9969286322593689}]}, {"text": "The result shows that the implicit symptoms can greatly improve the accuracy of disease identification for all the four diseases, which demonstrates the contribution of implicit symptoms when making diagnosis for patients.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9992133378982544}, {"text": "disease identification", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7330504655838013}]}, {"text": "shows the learning curve of all the three dialogue systems and shows the performance of these agents on testing set.", "labels": [], "entities": []}, {"text": "Due to the large action space, the random agent performs badly.", "labels": [], "entities": []}, {"text": "The rule-based agent outperforms the random agent in a large margin.", "labels": [], "entities": []}, {"text": "This indicates that the rulebased agent is well designed.", "labels": [], "entities": []}, {"text": "We can also see that the RL-based DQN agent outperforms rule-based agent significantly.", "labels": [], "entities": []}, {"text": "Moreover, DQN agent outperforms SVM-ex by collecting additional implicit symptoms via conversing with patients.", "labels": [], "entities": []}, {"text": "However, there is still a gap between the performance of DQN agent and SVM-ex&im in terms of accuracy, which indicates that there is still rooms for the improvement of the dialogue system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9992707371711731}]}], "tableCaptions": [{"text": " Table 3: Overview of the dataset. of user goal is  the number of dialogue sessions of each disease,  Ave of explicit symptoms and Ave of implicit  symptoms are the average number of explicit and  implicit symptoms among user goals respectively.", "labels": [], "entities": [{"text": "Ave", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.983642578125}, {"text": "Ave", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.976966917514801}]}, {"text": " Table 4: Accuracy of classification models", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963330030441284}]}, {"text": " Table 5: Performance of three dialogue systems on  5K simulated dialogues", "labels": [], "entities": []}]}