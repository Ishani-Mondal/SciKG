{"title": [{"text": "Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding", "labels": [], "entities": [{"text": "Marrying Up Regular Expressions", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8639113903045654}]}], "abstractContent": [{"text": "The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.7540917495886484}]}, {"text": "In this paper, we ask the question: \"Can we combine a neural network (NN) with regular expressions (RE) to improve supervised learning for NLP?\".", "labels": [], "entities": []}, {"text": "In answer, we develop novel methods to exploit the rich expres-siveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available.", "labels": [], "entities": []}, {"text": "We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.664676825205485}, {"text": "intent detection", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.893070638179779}, {"text": "slot filling", "start_pos": 98, "end_pos": 110, "type": "TASK", "confidence": 0.9347370564937592}]}, {"text": "Experimental results show that our approach is highly effective in exploiting the available training data, giving a clear boost to the RE-unaware NN.", "labels": [], "entities": [{"text": "RE-unaware NN", "start_pos": 135, "end_pos": 148, "type": "DATASET", "confidence": 0.5421418100595474}]}], "introductionContent": [{"text": "Regular expressions (REs) are widely used in various natural language processing (NLP) tasks like pattern matching, sentence classification, sequence labeling, etc.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7005486935377121}, {"text": "sentence classification", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.7496601939201355}, {"text": "sequence labeling", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.6811797767877579}]}, {"text": "(. As a technique based on human-crafted rules, it is concise, interpretable, tunable, and does not rely on much training data to generate.", "labels": [], "entities": []}, {"text": "As such, it is commonly used in industry, especially when the available training examples are limited -a problem known as few-shot learning.", "labels": [], "entities": []}, {"text": "While powerful, REs have a poor generalization ability because all synonyms and variations in a RE must be explicitly specified.", "labels": [], "entities": []}, {"text": "As a result, REs are often ensembled with data-driven methods, such as neural network (NN) based techniques, where a set of carefully-written REs are used to handle certain cases with high precision, leaving the rest for data-driven methods.", "labels": [], "entities": [{"text": "REs", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9553021788597107}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9847272634506226}]}, {"text": "We believe the use of REs can go beyond simple pattern matching.", "labels": [], "entities": [{"text": "REs", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9402065277099609}, {"text": "pattern matching", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7263433784246445}]}, {"text": "In addition to being a separate classifier to be ensembled, a RE also encodes a developer's knowledge for the problem domain.", "labels": [], "entities": []}, {"text": "The knowledge could be, for example, the informative words (clue words) within a RE's surface form.", "labels": [], "entities": [{"text": "RE's surface form", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7348818331956863}]}, {"text": "We argue that such information can be utilized by data-driven methods to achieve better prediction results, especially in few-shot learning.", "labels": [], "entities": []}, {"text": "This work investigates the use of REs to improve NNs -a learning framework that is widely used in many NLP tasks.", "labels": [], "entities": []}, {"text": "The combination of REs and a NN allows us to exploit the conciseness and effectiveness of REs and the strong generalization ability of NNs.", "labels": [], "entities": []}, {"text": "This also provides us an opportunity to learn from various kinds of REs, since NNs are known to be good at tolerating noises (.", "labels": [], "entities": []}, {"text": "This paper presents novel approaches to combine REs with a NN at different levels.", "labels": [], "entities": []}, {"text": "At the input layer, we propose to use the evaluation outcome of REs as the input features of a NN (Sec.3.2).", "labels": [], "entities": []}, {"text": "At the network module level, we show how to exploit the knowledge encoded in REs to guide the attention mechanism of a NN (Sec. 3.3).", "labels": [], "entities": []}, {"text": "At the output layer, we combine the evaluation outcome of a RE with the NN output in a learnable manner.", "labels": [], "entities": []}, {"text": "We evaluate our approach by applying it to two spoken language understanding (SLU) tasks, namely intent detection and slot filling, which respectively correspond to two fundamental NLP tasks: sentence classification and sequence labeling.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 47, "end_pos": 82, "type": "TASK", "confidence": 0.7699151039123535}, {"text": "intent detection", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7164506316184998}, {"text": "slot filling", "start_pos": 118, "end_pos": 130, "type": "TASK", "confidence": 0.7299023121595383}, {"text": "sentence classification", "start_pos": 192, "end_pos": 215, "type": "TASK", "confidence": 0.759188175201416}, {"text": "sequence labeling", "start_pos": 220, "end_pos": 237, "type": "TASK", "confidence": 0.6742178499698639}]}, {"text": "To demonstrate the usefulness of REs in realworld scenarios where the available number of annotated data can vary, we explore both the fewshot learning setting and the one with full training data.", "labels": [], "entities": [{"text": "REs", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9632286429405212}]}, {"text": "Experimental results show that our approach is highly effective in utilizing the available annotated data, yielding significantly better learning performance over the RE-unaware method.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "We present the first work to systematically investigate methods for combining REs with NNs.", "labels": [], "entities": []}, {"text": "(2) The proposed methods are shown to clearly improve the NN performance in both the few-shot learning and the full annotation settings.", "labels": [], "entities": []}, {"text": "(3) We provide a set of guidance on how to combine REs with NNs and RE annotation.", "labels": [], "entities": []}], "datasetContent": [{"text": "REs are word-level tags, we can simply embed and average the REtags into a vector f i for each word, and append it to the corresponding word embedding w i (as shown in 1 in).", "labels": [], "entities": []}, {"text": "Note that we also extend the slot REtags into the BIO format, e.g., the REtags of phrase New York are B-city and I-city if its original tag is city.", "labels": [], "entities": [{"text": "REtags", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.8348919749259949}]}, {"text": "Our experiments aim to answer three questions: Q1: Does the use of REs enhance the learning quality when the number of annotated instances is small?", "labels": [], "entities": [{"text": "Q1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9090272784233093}]}, {"text": "Q2: Does the use of REs still help when using the full training data?", "labels": [], "entities": []}, {"text": "Q3: How can we choose from different combination methods?", "labels": [], "entities": []}, {"text": "We use the ATIS dataset () to evaluate our approach.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9232206046581268}]}, {"text": "This dataset is widely used in SLU research.", "labels": [], "entities": [{"text": "SLU", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9599968791007996}]}, {"text": "It includes queries of flights, meal, etc.", "labels": [], "entities": []}, {"text": "We follow the setup of Liu and Lane (2016) by using 4,978 queries for training and 893 for testing, with 18 intent labels and 127 slot labels.", "labels": [], "entities": []}, {"text": "We also split words like Miami's into Miami 's during the tokenization phase to reduce the number of words that do not have a pre-trained word embedding.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.9751470685005188}]}, {"text": "This strategy is useful for fewshot learning.", "labels": [], "entities": [{"text": "fewshot learning", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.9095197319984436}]}, {"text": "To answer Q1 , we also exploit the full few-shot learning setting.", "labels": [], "entities": []}, {"text": "Specifically, for intent detection, we randomly select 5, 10, 20 training instances for each intent to form the few-shot training set; and for slot filling, we also explore 5, 10, 20 shots settings.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.9045167565345764}, {"text": "slot filling", "start_pos": 143, "end_pos": 155, "type": "TASK", "confidence": 0.8459762930870056}]}, {"text": "However, since a sentence typically contains multiple slots, the number of mentions of frequent slot labels may inevitably exceeds the target shot count.", "labels": [], "entities": []}, {"text": "To better approximate the target shot count, we select sentences for each slot label in ascending order of label frequencies.", "labels": [], "entities": []}, {"text": "That is k 1 -shot dataset will contain k 2 -shot dataset if k 1 > k 2 . All settings use the original test set.", "labels": [], "entities": []}, {"text": "Since most existing few-shot learning methods require either many few-shot classes or some classes with enough data for training, we also explore the partial few-shot learning setting for intent detection to provide a fair comparison for existing few-shot learning methods.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 188, "end_pos": 204, "type": "TASK", "confidence": 0.8687117695808411}]}, {"text": "Specifically, we let the 3 most frequent intents have 300 training instances, and the rest remains untouched.", "labels": [], "entities": []}, {"text": "This is also a common scenario in real world, where we often have several frequent classes and many classes with limited data.", "labels": [], "entities": []}, {"text": "As for slot filling, however, since the number of mentions of frequent slot labels already exceeds the target shot count, the original slot filling few-shot dataset can be directly used to train existing few-shot learning methods.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.8769985735416412}]}, {"text": "Therefore, we do not distinguish full and partial few-shot learning for slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 72, "end_pos": 84, "type": "TASK", "confidence": 0.9241271913051605}]}, {"text": "Our hyper-parameters for the BLSTM are similar to the ones used by.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.6466754078865051}]}, {"text": "Specifically, we use batch size 16, dropout probability 0.5, and BLSTM cell size 100.", "labels": [], "entities": [{"text": "BLSTM cell size 100", "start_pos": 65, "end_pos": 84, "type": "METRIC", "confidence": 0.6444806456565857}]}, {"text": "The attention loss weight is 16 (both positive and negative) for full few-shot learning settings and 1 for other settings.", "labels": [], "entities": [{"text": "attention loss weight", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7874134381612142}]}, {"text": "We use the 100d GloVe word vectors () pre-trained on, and the Adam optimizer () with learning rate 0.001.", "labels": [], "entities": []}, {"text": "We report accuracy and macro-F1 for intent detection, and micro/macro-F1 for slot filling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995075464248657}, {"text": "intent detection", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.8325100243091583}, {"text": "slot filling", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.8379472494125366}]}, {"text": "Micro/macro-F1 are the harmonic mean of micro/macro precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9949702620506287}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9974748492240906}]}, {"text": "Macro-precision/recall are calculated by averaging precision/recall of each label, and microprecision/recall are averaged over each prediction.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9980989098548889}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9987291693687439}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.8136215209960938}, {"text": "microprecision", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.9703484773635864}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.771661102771759}]}, {"text": "Here, a bold Courier typeface like BLSTM denotes the notations of the models that we will compare in Sec.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.560286819934845}]}, {"text": "5. Specifically, we compare our methods with the baseline BLSTM model (Sec. 3.1).", "labels": [], "entities": []}, {"text": "Since our attention loss method (Sec. 3.3) uses two-side attention, we include the raw two-side attention model without attention loss (+two) for comparison as well.", "labels": [], "entities": []}, {"text": "Besides, we also evaluate the RE output (REO), which uses the REtags as prediction directly, to show the quality of the REs that we will use in the experiments.", "labels": [], "entities": [{"text": "RE output (REO)", "start_pos": 30, "end_pos": 45, "type": "METRIC", "confidence": 0.9542614340782165}]}, {"text": "As for our methods for combinging REs with NN, +feat refers to using REtag as input features (Sec.", "labels": [], "entities": []}, {"text": "3.2), +posi and +neg refer to using positive and negative attention loss respectively, +both refers to using both postive and negative attention losses (Sec.", "labels": [], "entities": []}, {"text": "3.3), and +logit means using REtag to modify NN output (Sec. 3.4).", "labels": [], "entities": []}, {"text": "Moverover, since the REs can also be formatted as first-order-logic (FOL) rules, we also compare our methods with the teacher-student framework proposed by, which is a general framework for distilling knowledge from FOL rules into NN (+hu16).", "labels": [], "entities": []}, {"text": "Besides, since we consider few-short learning, we also include the memory module proposed by, which performs well in various few-shot datasets (+mem) . Finally, the state-of-art model on the ATIS dataset is also included (L&L16), which jointly models the intent detection and slot filling in a single network (.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 191, "end_pos": 203, "type": "DATASET", "confidence": 0.9763228595256805}, {"text": "intent detection", "start_pos": 255, "end_pos": 271, "type": "TASK", "confidence": 0.7196185886859894}, {"text": "slot filling", "start_pos": 276, "end_pos": 288, "type": "TASK", "confidence": 0.6897164434194565}]}, {"text": "To answer Q2, we also evaluate our methods on the full dataset.", "labels": [], "entities": []}, {"text": "As seen in, for intent detection, while two+both still works, feat and logit no longer give improvements.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.8606950342655182}, {"text": "feat", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9679131507873535}]}, {"text": "This shows For compactness, we only combine the best method in each task with mem, but others can also be combined.: Results on 20-Shot Data with Simple REs.", "labels": [], "entities": []}, {"text": "+both refers to +two +both for short.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Full Few-Shot Learning Settings. For slot filling, we do not distinguish full and  partial few-shot learning settings (see Sec. 4.1).", "labels": [], "entities": [{"text": "slot filling", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.8398003876209259}]}, {"text": " Table 2: Intent Detection Results on Partial Few- Shot Learning Setting.", "labels": [], "entities": [{"text": "Intent Detection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6734118014574051}, {"text": "Partial Few- Shot Learning Setting", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.5177645981311798}]}, {"text": " Table 3: Results on Full Dataset. The left side of  '/' applies for intent, and the right side for slot.", "labels": [], "entities": [{"text": "Full Dataset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.6867419928312302}, {"text": "intent", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9206227660179138}, {"text": "slot", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9978639483451843}]}, {"text": " Table 4: Results on 20-Shot Data with Simple  REs. +both refers to +two +both for short.", "labels": [], "entities": []}]}