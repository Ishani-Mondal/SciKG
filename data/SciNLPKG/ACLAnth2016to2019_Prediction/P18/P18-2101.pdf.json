{"title": [{"text": "Scoring Lexical Entailment with a Supervised Directional Similarity Network", "labels": [], "entities": [{"text": "Scoring Lexical Entailment", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9233121077219645}, {"text": "Supervised Directional Similarity", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.6097549696763357}]}], "abstractContent": [{"text": "We present the Supervised Directional Similarity Network (SDSN), a novel neural architecture for learning task-specific transformation functions on top of general-purpose word embeddings.", "labels": [], "entities": [{"text": "Supervised Directional Similarity Network (SDSN)", "start_pos": 15, "end_pos": 63, "type": "TASK", "confidence": 0.7508354272161212}]}, {"text": "Relying on only a limited amount of supervision from task-specific scores on a subset of the vocabulary , our architecture is able to gener-alise and transform a general-purpose dis-tributional vector space to model the relation of lexical entailment.", "labels": [], "entities": []}, {"text": "Experiments show excellent performance on scoring graded lexical entailment, raising the state-of-the-art on the HyperLex dataset by approximately 25%.", "labels": [], "entities": [{"text": "HyperLex dataset", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.9474959075450897}]}], "introductionContent": [{"text": "Standard word embedding models () are based on the distributional hypothesis by.", "labels": [], "entities": []}, {"text": "However, purely distributional models coalesce various lexico-semantic relations (e.g., synonymy, antonymy, hypernymy) into a joint distributed representation.", "labels": [], "entities": []}, {"text": "To address this, previous work has focused on introducing supervision into individual word embeddings, allowing them to better capture the desired lexical properties.", "labels": [], "entities": []}, {"text": "For example, and proposed methods for using annotated lexical relations to condition the vector space and bring synonymous words closer together.", "labels": [], "entities": []}, {"text": "Mrk\u0161i\u00b4 and Mrk\u0161i\u00b4 improved the optimisation function and introduced an additional constraint for pushing antonym pairs further apart.", "labels": [], "entities": [{"text": "Mrk\u0161i\u00b4", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.8707085251808167}]}, {"text": "While these methods integrate hand-crafted features from external lexical resources with distributional information, they improve only the embeddings of words that have annotated lexical relations in the training resource.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel approach to leveraging external knowledge with generalpurpose unsupervised embeddings, focusing on the directional graded lexical entailment task , whereas previous work has mostly investigated simpler non-directional semantic similarity tasks.", "labels": [], "entities": []}, {"text": "Instead of optimising individual word embeddings, our model uses general-purpose embeddings and optimises a separate neural component to adapt these to the specific task.", "labels": [], "entities": []}, {"text": "In particular, our neural Supervised Directional Similarity Network (SDSN) dynamically produces task-specific embeddings optimised for scoring the asymmetric lexical entailment relation between any two words, regardless of their presence in the training resource.", "labels": [], "entities": []}, {"text": "Our results with task-specific embeddings indicate large improvements on the HyperLex dataset, a standard graded lexical entailment benchmark.", "labels": [], "entities": [{"text": "HyperLex dataset", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.8868786692619324}]}, {"text": "The model also yields improvements on a simpler nongraded entailment detection task.", "labels": [], "entities": [{"text": "entailment detection task", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7912399570147196}]}], "datasetContent": [{"text": "As input to the SDSN network we use 300-dimensional dependency-based word embeddings by.", "labels": [], "entities": []}, {"text": "Layers m 1 and m 2 also have size 300 and layer h has size 100.", "labels": [], "entities": []}, {"text": "For regularisation, we apply dropout to the embeddings with p = 0.5.", "labels": [], "entities": [{"text": "regularisation", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9700020551681519}]}, {"text": "The margin R is set to 1 for the supervised pre-training stage.", "labels": [], "entities": [{"text": "margin R", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9300797581672668}]}, {"text": "The model is optimised using AdaDelta (Zeiler, 2012) with learning rate 1.0.", "labels": [], "entities": [{"text": "AdaDelta (Zeiler, 2012)", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.7979987760384878}, {"text": "learning rate 1.0", "start_pos": 58, "end_pos": 75, "type": "METRIC", "confidence": 0.9356823960940043}]}, {"text": "In order to control for random noise, we run each experiment with 10 different random seeds and average the results.", "labels": [], "entities": []}, {"text": "Our code and detailed configuration files will be made available online.", "labels": [], "entities": []}, {"text": "We evaluate graded lexical entailment on the HyperLex dataset ) which contains 2,616 word pairs in total scored for the asymmetric graded lexical entailment relation.", "labels": [], "entities": [{"text": "HyperLex dataset", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9832389950752258}]}, {"text": "Following a standard practice, we report Spearman's \u03c1 correlation of the model output to the given human-annotated scores.", "labels": [], "entities": [{"text": "Spearman's \u03c1 correlation", "start_pos": 41, "end_pos": 65, "type": "METRIC", "confidence": 0.6247625648975372}]}, {"text": "We conduct experiments on two standard data splits for supervised learning: random split and lexical split.", "labels": [], "entities": []}, {"text": "In the random split the data is randomly divided into training, validation, and test subsets containing 1831, 130, and 655 word pairs, respectively.", "labels": [], "entities": []}, {"text": "In the lexical  Since plenty of related research on lexical entailment is still focused on the simpler binary detection of asymmetric relations, we also run experiments on the large binary detection HypeNet dataset (, where the SDSN output is converted to binary decisions.", "labels": [], "entities": [{"text": "HypeNet dataset", "start_pos": 199, "end_pos": 214, "type": "DATASET", "confidence": 0.7465044260025024}]}, {"text": "We again report scores for both random and lexical split.", "labels": [], "entities": []}, {"text": "The results on two HyperLex splits are presented in, along with the best configurations reported by Vuli\u00b4c . We refer the interested reader to the original HyperLex paper ) fora detailed description of the best performing baseline models.", "labels": [], "entities": []}, {"text": "The Supervised Directional Similarity Network (SDSN) achieves substantially better scores than all other tested systems, despite relying on a much simpler supervision signal.", "labels": [], "entities": [{"text": "Supervised Directional Similarity Network (SDSN", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.7031790266434351}]}, {"text": "The previous top approaches, including the Paragram+CF embeddings, make use of numerous annotations provided by WordNet or similarly rich lexical resources, while for SDSN and SDSN+SDF only use the designated relation-specific training set and corpus statistics.", "labels": [], "entities": [{"text": "Paragram+CF embeddings", "start_pos": 43, "end_pos": 65, "type": "DATASET", "confidence": 0.7742076367139816}, {"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9660859107971191}]}, {"text": "By also including these extra training instances (SDSN+SDF+AS), we can gain additional perfor-  mance and push the correlation to 0.692 on the random split and 0.544 on the lexical split of HyperLex, an improvement of approximately 25% to the standard supervised training regime.", "labels": [], "entities": [{"text": "correlation", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.9656475186347961}, {"text": "HyperLex", "start_pos": 190, "end_pos": 198, "type": "DATASET", "confidence": 0.872344434261322}]}, {"text": "In we provide some example output from the final SDSN+SDF+AS model.", "labels": [], "entities": [{"text": "SDSN+SDF+AS", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.6306090116500854}]}, {"text": "It is able to successfully assign a high score to (captain, officer) and also identify with high confidence that wing is not a type of airplane, even though they are semantically related.", "labels": [], "entities": []}, {"text": "As an example of incorrect output, the model fails to assign a high score to (prince, royalty), possibly due to the usage patterns of these words being different in context.", "labels": [], "entities": []}, {"text": "In contrast, it assigns an unexpectedly high score to (kid, parent), likely due to the high distributional similarity of these words.", "labels": [], "entities": []}, {"text": "Glava\u0161 and Ponzetto (2017) proposed a related dual tensor model for the binary detection of asymmetric relations (Dual-T).", "labels": [], "entities": []}, {"text": "In order to compare our system to theirs, we train our model on HypeNet and convert the output to binary decisions.", "labels": [], "entities": []}, {"text": "We also compare SDSN to the best reported models of and, which combine distributional and pattern-based information for hypernymy detection (HypeNethybrid and H-feature, respectively).", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7081294804811478}]}, {"text": "We do not include additional WordNet and PPDB examples in these experiments, as the HypeNet data already subsumes most of them.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9466665387153625}, {"text": "HypeNet data", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.8413071632385254}]}, {"text": "As can be seen in, our SDSN+SDF model achieves the best results also on the HypeNet dataset, outperforming previous models on both data splits.", "labels": [], "entities": [{"text": "HypeNet dataset", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.957999050617218}]}], "tableCaptions": [{"text": " Table 1: Graded lexical entailment detection results  on the random and lexical splits of the HyperLex  dataset. We report Spearman's \u03c1 on both validation  and test sets.", "labels": [], "entities": [{"text": "Graded lexical entailment detection", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.6433640792965889}, {"text": "HyperLex  dataset", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.9640050232410431}, {"text": "Spearman's \u03c1", "start_pos": 124, "end_pos": 136, "type": "METRIC", "confidence": 0.6553722818692526}]}, {"text": " Table 2: Results on the HypeNet binary hypernymy  detection dataset.", "labels": [], "entities": [{"text": "HypeNet binary hypernymy  detection", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.7003505975008011}]}, {"text": " Table 3: Example word pairs from the HyperLex  development set. S is the human-annotated score  in the HyperLex dataset. P is the predicted score  using the SDSN+SDF+AS model.", "labels": [], "entities": [{"text": "HyperLex  development set", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8926376501719157}, {"text": "HyperLex dataset", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.9705207049846649}]}]}