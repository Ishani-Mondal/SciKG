{"title": [{"text": "Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper examines the problem of generating natural language descriptions of chess games.", "labels": [], "entities": [{"text": "generating natural language descriptions of chess games", "start_pos": 35, "end_pos": 90, "type": "TASK", "confidence": 0.7190315851143428}]}, {"text": "We introduce anew large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game.", "labels": [], "entities": []}, {"text": "The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games.", "labels": [], "entities": []}, {"text": "We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.6512883206208547}]}, {"text": "We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that maybe commented upon to describe a given chess move.", "labels": [], "entities": []}, {"text": "Through a human study on predictions fora subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency.", "labels": [], "entities": []}], "introductionContent": [{"text": "A variety of work in NLP has sought to produce fluent natural language descriptions conditioned on a contextual grounding.", "labels": [], "entities": []}, {"text": "For example, several lines of work explore methods for describing images of scenes and videos, while others have conditioned on structured sources like Wikipedia infoboxes (Lebret et al., * HJ and VG contributed equally for this paper We will make the code-base (including data collection and processing) publicly available at https://github. com/harsh19/ChessCommentaryGeneration 2016).", "labels": [], "entities": []}, {"text": "In most cases, progress has been driven by the availability of large training corpora that pair natural language with examples from the grounding (.", "labels": [], "entities": []}, {"text": "One line of work has investigated methods for producing and interpreting language in the context of a game, a space that has rich pragmatic structure, but where training data has been hard to come by.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew large-scale resource for learning to correlate natural language with individual moves in the game of chess.", "labels": [], "entities": []}, {"text": "We collect a dataset of more than 298K chess move/commentary pairs across \u2248 11K chess games from online chess forums.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first such dataset of this scale fora game commentary generation task.", "labels": [], "entities": [{"text": "game commentary generation task", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.7804693952202797}]}, {"text": "We provide an analysis of the dataset and highlight the large variety in commentary texts by categorizing them into six different aspects of the game that they respectively discuss.", "labels": [], "entities": []}, {"text": "Automated game commentary generation can be a useful learning aid.", "labels": [], "entities": [{"text": "Automated game commentary generation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6583426520228386}]}, {"text": "Novices and experts alike can learn more about the game by hearing expla-nations of the motivations behind moves, or their quality.", "labels": [], "entities": []}, {"text": "In fact, on sites for game aficionados, these commentaries are standard features, speaking to their interestingness and utility as complements to concrete descriptions of the game boards themselves.", "labels": [], "entities": []}, {"text": "Game commentary generation poses a number of interesting challenges for existing approaches to language generation.", "labels": [], "entities": [{"text": "Game commentary generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.816066324710846}, {"text": "language generation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7420924007892609}]}, {"text": "First, modeling human commentary is challenging because human commentators rely both on their prior knowledge of game rules as well as their knowledge of effective strategy when interpreting and referring to the game state.", "labels": [], "entities": [{"text": "interpreting and referring to the game state", "start_pos": 178, "end_pos": 222, "type": "TASK", "confidence": 0.7565182191984994}]}, {"text": "Secondly, there are multiple aspects of the game state that can be talked about fora given move -the commentator's choice depends on the pragmatic context of the game.", "labels": [], "entities": []}, {"text": "For example, for the move shown in, one can comment simply that the pawn was moved, or one may comment on how the check was blocked by that move.", "labels": [], "entities": []}, {"text": "Both descriptions are true, but the latter is most salient given the player's goal.", "labels": [], "entities": []}, {"text": "However, sometimes, none of the aspects may standout as being most salient, and the most salient aspect may even change from commentator to commentator.", "labels": [], "entities": []}, {"text": "Moreover, a human commentator may introduce variations in the aspects he or she chooses to talk about, in order to reduce monotony in the commentary.", "labels": [], "entities": []}, {"text": "This makes the dataset a useful testbed not only for NLG but also for related work on modeling pragmatics in language ().", "labels": [], "entities": []}, {"text": "Prior work has explored game commentary generation.; have explored chess commentary generation, but for lack of large-scale training data their methods have been mainly rule-based.", "labels": [], "entities": [{"text": "game commentary generation.", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8359766801198324}, {"text": "chess commentary generation", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.8813479940096537}]}, {"text": "have explored commentary generation for the game of Shogi, proposing a twostep process where salient terms are generated from the game state and then composed in a language model.", "labels": [], "entities": [{"text": "commentary generation", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.8124582767486572}]}, {"text": "In contrast, given the larger amount of training data available to us, our proposed model uses an end-to-end trainable neural architecture to predict commentaries given the game state.", "labels": [], "entities": []}, {"text": "Our model conditions on semantic and pragmatic information about the current state and explicitly learns to compose, conjoin, and select these features in a recurrent decoder module.", "labels": [], "entities": []}, {"text": "We perform an experimental evaluation comparing against baselines and variants of our model that ablate various aspects of our proposed archi-  tecture.", "labels": [], "entities": []}, {"text": "Outputs on the 'Move Description' subset of data from our final model were judged by humans to be as good as human written ground truth commentaries on measures of fluency and correctness.", "labels": [], "entities": [{"text": "Move Description' subset of data", "start_pos": 16, "end_pos": 48, "type": "DATASET", "confidence": 0.6496147513389587}]}], "datasetContent": [{"text": "In this section we introduce our new large-scale Chess Commentary dataset, share some statistics about the data, and discuss the variety in type of commentaries.", "labels": [], "entities": [{"text": "Chess Commentary dataset", "start_pos": 49, "end_pos": 73, "type": "DATASET", "confidence": 0.788837194442749}]}, {"text": "The data is collected from the online chess discussion forum gameknot.com, which features multiple games self-annotated with move-by-move commentary.", "labels": [], "entities": []}, {"text": "The dataset consists of 298K aligned game move/commentary pairs.", "labels": [], "entities": []}, {"text": "Some commentaries are written fora sequence of few moves) while others correspond to a single move.", "labels": [], "entities": []}, {"text": "For the purpose of initial analysis and modeling, we limit ourselves to only those data points where commentary text corresponds to a single move.", "labels": [], "entities": [{"text": "initial analysis", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.9280796349048615}]}, {"text": "Additionally, we split the multi-sentence commentary texts to create multiple data points with the same chessboard and move inputs.", "labels": [], "entities": []}, {"text": "We observe that there is a large variety in the commentary  texts.", "labels": [], "entities": []}, {"text": "To analyze this variety, we consider labelling the commentary texts in the data with a predefined set of categories.", "labels": [], "entities": []}, {"text": "The choice of these categories is made based on a manual inspection of a sub-sample of data.", "labels": [], "entities": []}, {"text": "We consider the following set of commentary categories (Also shown in): \u2022 Direct move description (MoveDesc 3 ): Explicitly or implicitly describe the current move.", "labels": [], "entities": []}, {"text": "\u2022 Quality of move (Quality 4 ): Describe the quality of the current move.", "labels": [], "entities": [{"text": "Quality", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9830339550971985}]}, {"text": "\u2022 Comparative: Compare multiple possible moves.", "labels": [], "entities": []}, {"text": "\u2022 Move Rationale or Planning (Planning): Describe the rationale for the current move, in terms of the future gameplay, advantage over other potential moves etc.", "labels": [], "entities": []}, {"text": "\u2022 Contextual game information: Describe not the current move alone, but the overall game state -such as possibility of win/loss, overall aggression/defence, etc.", "labels": [], "entities": []}, {"text": "\u2022 General information: General idioms & advice about chess, information about players/tournament, emotional remarks, retorts, etc.", "labels": [], "entities": []}, {"text": "The examples in illustrate these classes.", "labels": [], "entities": []}, {"text": "Note that the commentary texts are not necessarily limited to one tag, though that is true for most MoveDesc & 'Move Description' used interchangeably 4 Quality and 'Move Quality' used interchangeably of the data.", "labels": [], "entities": []}, {"text": "A total of 1K comments are annotated by two annotators.", "labels": [], "entities": []}, {"text": "A SVM classifier) is trained for each comment class, considering the annotation as ground truth and using word unigrams as features.", "labels": [], "entities": []}, {"text": "This classifier is then used to predict tags for the train, validation and test sets.", "labels": [], "entities": []}, {"text": "For \"Comparative\" category, we found that a classifier with manually defined rules such as presence of word \"better\" performs better than the classifier, perhaps due to the paucity of data, and thus we use this instead . As can be observed in, the classifiers used are able to generalize well on the held out dataset  We split each of the data subsets in a 70:10:20 ratio into train, validation and test.", "labels": [], "entities": []}, {"text": "All our models are implemented in Pytorch version 0.3.1 ().", "labels": [], "entities": [{"text": "Pytorch version 0.3.1", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.877751628557841}]}, {"text": "We use the ADAM optimizer () with its default parameters and a mini-batch size of 32.", "labels": [], "entities": []}, {"text": "Validation set perplexity is used for early-stopping.", "labels": [], "entities": []}, {"text": "At test-time, we use greedy search to generate the model output.", "labels": [], "entities": []}, {"text": "We observed that beam decoding does not lead to any significant improvement in terms of validation BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.951581597328186}]}, {"text": "We observe the BLEU () and BLEU-2 () scores to measure the performance of the models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.998683750629425}, {"text": "BLEU-2", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9955030083656311}]}, {"text": "Addi-tionally, we consider a measure to quantify the diversity in the generated outputs.", "labels": [], "entities": []}, {"text": "Finally, we also conduct a human evaluation study.", "labels": [], "entities": []}, {"text": "In the remainder of this section, we discuss baselines along with various experiments and results.", "labels": [], "entities": []}, {"text": "As discussed in the qualitative examples above, we often found the outputs to be good -though BLEU scores are low.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.999346911907196}]}, {"text": "BLEU is known to correlate poorly) with human relevance scores for NLG tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9891047477722168}]}, {"text": "Hence, we conduct a human evaluation study for the best 2 neural (GAC,GAC-sparse) and best 2 non-neural methods (TEMP,NN).", "labels": [], "entities": [{"text": "TEMP,NN", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.8403257131576538}]}, {"text": "Setup: Specifically, annotators are shown a chess move through previous board and resulting board snapshots, along with information on which piece moved (a snapshot of a HIT 7 is provided in the Appendix D).", "labels": [], "entities": []}, {"text": "With this context, they were shown text commentary based on this move and were asked to judge the commentary via three questions, shortened versions of which can be seen in the first column of.", "labels": [], "entities": []}, {"text": "We randomly select 100 data points from the test split of 'Move Description' category and collect the predictions from each of the methods under consideration.", "labels": [], "entities": []}, {"text": "We hired two Anglophone (Lifetime HIT acceptance % > 80) annotators for every human-evaluated test example.", "labels": [], "entities": [{"text": "Lifetime HIT acceptance %", "start_pos": 25, "end_pos": 50, "type": "METRIC", "confidence": 0.7746097296476364}]}, {"text": "We additionally assess chess proficiency of the annotators using questions from the chess-QA dataset by.", "labels": [], "entities": [{"text": "chess-QA dataset", "start_pos": 84, "end_pos": 100, "type": "DATASET", "confidence": 0.7805134356021881}]}, {"text": "Within each HIT, we ask two randomly selected questions from the chess-QA dataset.", "labels": [], "entities": [{"text": "chess-QA dataset", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.8715150356292725}]}, {"text": "Finally we consider only those HITs wherein the annotator was able to answer the proficiency questions correctly.", "labels": [], "entities": []}, {"text": "Results: We conducted a human evaluation study for the MoveDesc subset of the data.", "labels": [], "entities": [{"text": "MoveDesc subset of the data", "start_pos": 55, "end_pos": 82, "type": "DATASET", "confidence": 0.8996451735496521}]}, {"text": "As can be observed from, outputs from our method attain slightly more favorable scores compared to the ground truth commentaries.", "labels": [], "entities": []}, {"text": "This shows that the predicted outputs from our model are not worse than ground truth on the said measures.", "labels": [], "entities": []}, {"text": "This is in spite of the fact that the BLEU-4 score for the predicted outputs is only \u223c 2 w.r.t. the ground truth outputs.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.999273955821991}]}, {"text": "One reason for slightly lower performance of the ground truth outputs on the said measures is that some of the human writ-7 Human Intelligence Task ten commentaries are either very ungrammatical or too concise.", "labels": [], "entities": [{"text": "human writ-7 Human Intelligence Task ten commentaries", "start_pos": 111, "end_pos": 164, "type": "DATASET", "confidence": 0.6333645241601127}]}, {"text": "A more surprising observation is that around 30% of human written ground truth outputs were also marked as not valid forgiven board move.", "labels": [], "entities": []}, {"text": "On inspection, it seems that commentary often contains extraneous game information beyond that of move alone, which indicates that an ideal comparison should be over commentary for an entire game, although this is beyond the scope of the current work.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement for our experiments) is 0.45 for Q1 and 0.32 for Q2.", "labels": [], "entities": []}, {"text": "We notice some variation in \u03ba coefficients across different systems.", "labels": [], "entities": []}, {"text": "While TEMP and GAC responses had a 0.5-0.7 coefficient range, the responses for CLM had a much lower coefficient.", "labels": [], "entities": []}, {"text": "In our setup, each HIT consists of 7 comments, one from each system.", "labels": [], "entities": []}, {"text": "For Q3 (fluency), which is on an ordinal scale, we measure rank-order consistency between the responses of the two annotators of a HIT.", "labels": [], "entities": []}, {"text": "Mean Kendall \u03c4 () across all HITs was found to be 0.39.", "labels": [], "entities": [{"text": "Kendall \u03c4", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9131509959697723}]}, {"text": "To measures significance of results, we perform bootstrap tests on 1000 subsets of size 50 with a significance threshold of p = 0.05 for each pair of systems.", "labels": [], "entities": []}, {"text": "For Q1, we observe that GAC(M), GAC(M+T) and GAC(M+T+S) methods are significantly better than baselines NN and GAC-sparse.", "labels": [], "entities": []}, {"text": "We find that neither of GAC(M+T) and GT significantly outperform each other on Q1 as well as Q2.", "labels": [], "entities": []}, {"text": "But we do find that GAC(M+T) does better than GAC(M) on both Q1 and Q2.", "labels": [], "entities": []}, {"text": "For fluency scores, we find that GAC(M+T) is more fluent than GT, NN , GAC-sparse, GAC(M).", "labels": [], "entities": []}, {"text": "Neither of GAC(M) and GAC(M+T+S) is significantly more fluent than the other.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset and Vocabulary Statistics", "labels": [], "entities": []}, {"text": " Table 2: Commentary texts have a large variety making the  problem of content selection an important challenge in our  dataset. We classify the commentaries into 6 different cate- gories using a classifier trained on some hand-labelled data,  a fraction of which is kept for validation. % data refers to  the percentage of commentary sentences in the tagged data  belonging to the respective category.", "labels": [], "entities": []}, {"text": " Table 3: Performance of baselines and our model with differ- ent subsets of features as per various quantitative measures.  ( S = Score, M= Move, T = Threat features; ) On all data sub- sets, our model outperforms the TEMP and NN baselines.  Among proposed models, GAC performs better than GAC- sparse & RAW in general. For NN, GAC-sparse and GAC  methods, we experiment with multiple feature combinations  and report only the best as per BLEU scores.", "labels": [], "entities": [{"text": "Threat", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.8652366995811462}, {"text": "BLEU", "start_pos": 440, "end_pos": 444, "type": "METRIC", "confidence": 0.9981619715690613}]}, {"text": " Table 4: Performance of the GAC model with different fea- ture sets. ( S = Score, M= Move, T = Threat features; ) Dif- ferent subset of features work best for different subsets. For  instance, Score features seem to help only in the Quality cat- egory. Note that the results for Quality are from 5-fold cross- validation, since the number of datapoints in the category is  much lesser than the other two.", "labels": [], "entities": [{"text": "Threat", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9289260506629944}]}, {"text": " Table 5: The COMB approaches show the combined per- formance of separately trained models on the respective test  subsets.", "labels": [], "entities": [{"text": "COMB", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.6546521186828613}]}, {"text": " Table 6: Human study results on MoveDesc data category. Outputs from GAC are in general better than ground truth, NN and  GAC-sparse. TEMP outperforms other methods, though as shown earlier, outputs from TEMP lack diversity.", "labels": [], "entities": [{"text": "MoveDesc data category", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.8541180690129598}]}]}