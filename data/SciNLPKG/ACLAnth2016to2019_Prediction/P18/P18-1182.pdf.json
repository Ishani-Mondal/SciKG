{"title": [{"text": "NeuralREG: An end-to-end approach to referring expression generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and grammatical function.", "labels": [], "entities": [{"text": "Referring Expression Generation (REG)", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.8387061953544617}]}, {"text": "In this paper, we present anew approach (NeuralREG), relying on deep neural networks, which makes decisions about form and content in one go without explicit feature extraction.", "labels": [], "entities": []}, {"text": "Using a delexicalized version of the WebNLG corpus, we show that the neu-ral model substantially improves over two strong baselines.", "labels": [], "entities": [{"text": "WebNLG corpus", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.9808102250099182}]}, {"text": "Data and models are publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) is the task of automatically converting non-linguistic data into coherent natural language text.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7662008702754974}]}, {"text": "Since the input data will often consist of entities and the relations between them, generating references for these entities is a core task in many NLG systems (.", "labels": [], "entities": []}, {"text": "Referring Expression Generation (REG), the task responsible for generating these references, is typically presented as a twostep procedure.", "labels": [], "entities": [{"text": "Referring Expression Generation (REG)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8498955965042114}]}, {"text": "First, the referential form needs to be decided, asking whether a reference at a given point in the text should assume the form of, for example, a proper name (\"Frida Kahlo\"), a pronoun (\"she\") or description (\"the Mexican painter\").", "labels": [], "entities": []}, {"text": "In addition, the REG model must account for the different ways in which a particular referential form can be realized.", "labels": [], "entities": []}, {"text": "For example, both \"Frida\" and \"Kahlo\" are name-variants that may occur in a text, and she can alternatively also be described as, say, \"the famous female painter\".", "labels": [], "entities": []}, {"text": "Most of the earlier REG approaches focus either on selecting referential form (, or on selecting referential content, typically zooming in on one specific kind of reference such as a pronoun (e.g.,), definite description (e.g., or proper name generation (e.g.,).", "labels": [], "entities": [{"text": "REG", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9775390625}, {"text": "proper name generation", "start_pos": 231, "end_pos": 253, "type": "TASK", "confidence": 0.7189190983772278}]}, {"text": "Instead, in this paper, we propose NeuralREG: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content.", "labels": [], "entities": []}, {"text": "Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any feature extraction technique.", "labels": [], "entities": []}, {"text": "Besides its use in traditional pipeline NLG systems), REG has also become relevant in modern \"end-to-end\" NLG approaches, which perform the task in a more integrated manner (see e.g.).", "labels": [], "entities": []}, {"text": "Some of these approaches have recently focused on inputs which references to entities are delexicalized to general tags (e.g., ENTITY-1, ENTITY-2) in order to decrease data sparsity.", "labels": [], "entities": []}, {"text": "Based on the delexicalized input, the model generates outputs which maybe likened to templates in which references to the discourse entities are not realized (as in \"The ground of ENTITY-1 is located in ENTITY-2.\").", "labels": [], "entities": []}, {"text": "While our approach, dubbed as NeuralREG, is compatible with different applications of REG models, in this paper, we concentrate on the last one, relying on a specifically constructed set of 78,901 referring expressions to 1,501 entities in the context of the semantic web, derived from a (delexicalized) version of the WebNLG corpus (.", "labels": [], "entities": [{"text": "WebNLG corpus", "start_pos": 319, "end_pos": 332, "type": "DATASET", "confidence": 0.9399444460868835}]}, {"text": "Both this data set and the model will be made publicly available.", "labels": [], "entities": []}, {"text": "We compare NeuralREG against two baselines in an automatic and human evaluation, showing that the integrated neural model is a marked improvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We evaluated our models on the training, development and test referring expression sets described in Section 3.3.", "labels": [], "entities": []}, {"text": "Metrics We compared the referring expressions produced by the evaluated models with the goldstandards ones using accuracy and String Edit Distance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9995757937431335}, {"text": "String Edit Distance", "start_pos": 126, "end_pos": 146, "type": "METRIC", "confidence": 0.7510890563329061}]}, {"text": "Since pronouns are highlighted as the most likely referential form to be used when a referent is salient in the discourse, as argued in the introduction, we also computed pronoun accuracy, precision, recall and F1-score in order to evaluate the performance of the models for capturing discourse salience.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9880302548408508}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9996495246887207}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9995344877243042}, {"text": "F1-score", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9993888139724731}]}, {"text": "Finally, we lexicalized the original templates with the referring expressions produced by the models and compared them with the original texts in the corpus using accuracy and BLEU score () as a measure of fluency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9996137022972107}, {"text": "BLEU score", "start_pos": 176, "end_pos": 186, "type": "METRIC", "confidence": 0.9895592927932739}]}, {"text": "Since our model does not handle referring expressions for constants (dates and numbers), we just copied their source version into the template.", "labels": [], "entities": []}, {"text": "Post-hoc McNemar's and Wilcoxon signed ranked tests adjusted by the Bonferroni method were used to test the statistical significance of the models in terms of accuracy and string edit distance, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.99920254945755}, {"text": "string edit distance", "start_pos": 172, "end_pos": 192, "type": "METRIC", "confidence": 0.609222819407781}]}, {"text": "To test the statistical significance of the BLEU scores of the models, we used a bootstrap resampling together with an approximate randomization method (Clark et al., 2011) 2 . Settings NeuralREG was implemented using Dynet ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9988986253738403}]}, {"text": "Source and target word embeddings were 300D each and trained jointly with the model, whereas hidden units were 512D for each direction, totaling 1024D in the bidirection layers.", "labels": [], "entities": []}, {"text": "All non-recurrent matrices were initialized following the method of.", "labels": [], "entities": []}, {"text": "Models were trained using stochastic gradient descent with Adadelta and mini-batches of size 40.", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.8906885385513306}]}, {"text": "We ran each model for 60 epochs, applying early stopping for model selection based on accuracy on the development set with patience of 20 epochs.", "labels": [], "entities": [{"text": "model selection", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.6579706370830536}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9989657402038574}]}, {"text": "For each decoding version (Seq2Seq, CAtt and HierAtt), we searched for the best combination of drop-out probability of 0.2 or 0.3 in both the encoding and decoding layers, using beam search with a size of 1 or 5 with predictions up to 30 tokens or until 2 ending tokens were predicted (EOS).", "labels": [], "entities": [{"text": "HierAtt", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.7192032933235168}, {"text": "EOS", "start_pos": 286, "end_pos": 289, "type": "METRIC", "confidence": 0.8548486232757568}]}, {"text": "The results described in the next section were obtained on the test set by the NeuralREG version with the highest accuracy on the development set over the epochs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9976414442062378}]}, {"text": "Results summarizes the results for all models on all metrics on the test set and depicts a text example lexicalized by each model.", "labels": [], "entities": []}, {"text": "The first thing to note in the results of the first table is that the baselines in the top two rows performed quite strong on this task, generating more than half of the referring expressions exactly as in the goldstandard.", "labels": [], "entities": []}, {"text": "The method based on Castro Ferreira et al.", "labels": [], "entities": []}, {"text": "(2016) performed statistically better than OnlyNames on all metrics due to its capability, albeit to a limited extent, to predict pronominal references (which OnlyNames obviously cannot).", "labels": [], "entities": []}, {"text": "We reported results on the test set for NeuralREG+Seq2Seq and NeuralREG+CAtt using: (1) Accuracy (Acc.) and String Edit Distance (SED) results in the prediction of all referring expressions; (2) Accuracy (Acc.), Precision (Prec.), Recall (Rec.) and F-Score results in the prediction of pronominal forms; and (3) Accuracy (Acc.) and BLEU score results of the texts with the generated referring expressions.", "labels": [], "entities": [{"text": "Accuracy (Acc.)", "start_pos": 88, "end_pos": 103, "type": "METRIC", "confidence": 0.7820071280002594}, {"text": "String Edit Distance (SED)", "start_pos": 108, "end_pos": 134, "type": "METRIC", "confidence": 0.9015340805053711}, {"text": "Accuracy (Acc.)", "start_pos": 195, "end_pos": 210, "type": "METRIC", "confidence": 0.8263818919658661}, {"text": "Precision (Prec.)", "start_pos": 212, "end_pos": 229, "type": "METRIC", "confidence": 0.8888833075761795}, {"text": "Recall (Rec.)", "start_pos": 231, "end_pos": 244, "type": "METRIC", "confidence": 0.9149307012557983}, {"text": "F-Score", "start_pos": 249, "end_pos": 256, "type": "METRIC", "confidence": 0.9675477743148804}, {"text": "Accuracy", "start_pos": 312, "end_pos": 320, "type": "METRIC", "confidence": 0.9992602467536926}, {"text": "Acc.", "start_pos": 322, "end_pos": 326, "type": "METRIC", "confidence": 0.6438946723937988}, {"text": "BLEU score", "start_pos": 332, "end_pos": 342, "type": "METRIC", "confidence": 0.9695077836513519}]}, {"text": "Rankings were determined by statistical significance.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 28, "end_pos": 52, "type": "METRIC", "confidence": 0.9538352787494659}]}, {"text": "dropout probability 0.3 and beam size 5, and NeuralREG+HierAtt with dropout probability of 0.3 and beam size of 1 selected based on the highest accuracy on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9873329997062683}]}, {"text": "Importantly, the three NeuralREG variant models statistically outperformed the two baseline systems.", "labels": [], "entities": []}, {"text": "They achieved BLEU scores, text and referential accuracies as well as string edit distances in the range of 79.01-79.39, 28%-30%, 73%-74% and 2.25-2.36, respectively.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9709796607494354}, {"text": "accuracies", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.8304272890090942}, {"text": "string edit distances", "start_pos": 70, "end_pos": 91, "type": "METRIC", "confidence": 0.6871836185455322}]}, {"text": "This means that NeuralREG predicted 3 out of 4 references completely correct, whereas the incorrect ones needed an average of 2 post-edition operations in character level to be equal to the gold-standard.", "labels": [], "entities": []}, {"text": "When considering the texts lexicalized with the referring expressions produced by NeuralREG, at least 28% of them are similar to the original texts.", "labels": [], "entities": []}, {"text": "Especially noteworthy was the score on pronoun accuracy, indicating that the model was well capable of predicting when to generate a pronominal reference in our dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9859960675239563}]}, {"text": "The results for the different decoding methods for NeuralREG were similar, with the NeuralREG+CAtt performing slightly better in terms of the BLEU score, text accuracy and String Edit Distance.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.9809233844280243}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.8420342803001404}, {"text": "String Edit Distance", "start_pos": 172, "end_pos": 192, "type": "METRIC", "confidence": 0.5346607069174448}]}, {"text": "The more complex NeuralREG+HierAtt yielded the lowest results, even though the differences with the other two models were small and not even statistically significant in many of the cases.", "labels": [], "entities": []}, {"text": "Complementary to the automatic evaluation, we performed an evaluation with human judges, comparing the quality judgments of the original texts to the versions generated by our various models.", "labels": [], "entities": []}, {"text": "Material We quasi-randomly selected 24 instances from the delexicalized version of the WebNLG corpus related to the test part of the referring expression collection.", "labels": [], "entities": [{"text": "WebNLG corpus", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9617858529090881}]}, {"text": "For each of the selected instances, we took into account its source triple set and its 6 target texts: one original (randomly chosen) and its versions with the referring expressions generated by each of the 5 models introduced in this study (two baselines, three neural models).", "labels": [], "entities": []}, {"text": "Instances were chosen following 2 criteria: the number of triples in the source set (ranging from 2 to 7) and the differences between the target texts.", "labels": [], "entities": []}, {"text": "For each size group, we randomly selected 4 instances (of varying degrees of variation between the generated texts) giving rise to 144 trials (= 6 triple set sizes * 4 instances * 6 text versions), each consisting of a set of triples and a target text describing it with the lexicalized referring expressions highlighted in yellow.", "labels": [], "entities": []}, {"text": "Method The experiment had a latin-square design, distributing the 144 trials over 6 different lists such that each participant rated 24 trials, one for each of the 24 corpus instances, making sure that participants saw equal numbers of triple set sizes and generated versions.", "labels": [], "entities": []}, {"text": "Once introduced to atrial, the participants were asked to rate the fluency (\"does the text flow in a natural, easy to read manner?\"), grammaticality (\"is the text grammatical (no spelling or grammatical errors)?\") and clarity (\"does the text clearly express the data?\") of each target text on a 7-Likert scale, focussing on the highlighted referring expressions.", "labels": [], "entities": [{"text": "clarity", "start_pos": 218, "end_pos": 225, "type": "METRIC", "confidence": 0.9977433681488037}]}, {"text": "The experiment is available on the website of the author 3 . Participants We recruited 60 participants, 10 per list, via Mechanical Turk.", "labels": [], "entities": []}, {"text": "Their average age was 36 years and 27 of them were females.", "labels": [], "entities": []}, {"text": "The majority declared themselves native speakers of", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: (1) Accuracy (Acc.) and String Edit Distance (SED) results in the prediction of all referring  expressions; (2) Accuracy (Acc.), Precision (Prec.), Recall (Rec.) and F-Score results in the prediction  of pronominal forms; and (3) Accuracy (Acc.) and BLEU score results of the texts with the generated  referring expressions. Rankings were determined by statistical significance.", "labels": [], "entities": [{"text": "Accuracy (Acc.", "start_pos": 14, "end_pos": 28, "type": "METRIC", "confidence": 0.7842405438423157}, {"text": "String Edit Distance (SED)", "start_pos": 34, "end_pos": 60, "type": "METRIC", "confidence": 0.872867355744044}, {"text": "Accuracy (Acc.)", "start_pos": 122, "end_pos": 137, "type": "METRIC", "confidence": 0.7892821878194809}, {"text": "Precision (Prec.)", "start_pos": 139, "end_pos": 156, "type": "METRIC", "confidence": 0.916230320930481}, {"text": "Recall (Rec.)", "start_pos": 158, "end_pos": 171, "type": "METRIC", "confidence": 0.9242444187402725}, {"text": "F-Score", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.971952497959137}, {"text": "Accuracy (Acc.)", "start_pos": 240, "end_pos": 255, "type": "METRIC", "confidence": 0.8291899114847183}, {"text": "BLEU score", "start_pos": 260, "end_pos": 270, "type": "METRIC", "confidence": 0.9665670692920685}]}]}