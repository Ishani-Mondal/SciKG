{"title": [{"text": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph", "labels": [], "entities": [{"text": "Multimodal Language Analysis", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7520649234453837}, {"text": "CMU-MOSEI Dataset", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.958943784236908}]}], "abstractContent": [{"text": "Analyzing human multimodal language is an emerging area of research in NLP.", "labels": [], "entities": [{"text": "Analyzing human multimodal language", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8030483573675156}]}, {"text": "In-trinsically human communication is mul-timodal (heterogeneous), temporal and asynchronous; it consists of the language (words), visual (expressions), and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences.", "labels": [], "entities": []}, {"text": "From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of multimodal language.", "labels": [], "entities": []}, {"text": "In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date.", "labels": [], "entities": [{"text": "sentiment analysis and emotion recognition", "start_pos": 118, "end_pos": 160, "type": "TASK", "confidence": 0.7705997288227081}]}, {"text": "Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to investigate how modalities interact with each other inhuman multimodal language.", "labels": [], "entities": []}, {"text": "Unlike previously proposed fusion techniques , DFG is highly interpretable and achieves competitive performance compared to the current state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Theories of language origin identify the combination of language and nonverbal behaviors (vision and acoustic modality) as the prime form of communication utilized by humans throughout evolution.", "labels": [], "entities": []}, {"text": "In natural language processing, this form of language is regarded as human multimodal language.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6827038923899332}]}, {"text": "Modeling multimodal language has recently become a centric research direction in both NLP and multimodal machine learning.", "labels": [], "entities": []}, {"text": "Studies strive to model the dual dynamics of multimodal language: intra-modal dynamics (dynamics within each modality) and cross-modal dynamics (dynamics across different modalities).", "labels": [], "entities": []}, {"text": "However, from a resource perspective, previous multimodal language datasets have severe shortcomings in the following aspects: Diversity in the training samples: The diversity in training samples is crucial for comprehensive multimodal language studies due to the complexity of the underlying distribution.", "labels": [], "entities": []}, {"text": "This complexity is rooted in variability of intra-modal and crossmodal dynamics for language, vision and acoustic modalities (.", "labels": [], "entities": []}, {"text": "Previously proposed datasets for multimodal language are generally small in size due to difficulties associated with data acquisition and costs of annotations.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7286265790462494}]}, {"text": "Variety in the topics: Variety in topics opens the door to generalizable studies across different domains.", "labels": [], "entities": []}, {"text": "Models trained on only few topics generalize poorly as language and nonverbal behaviors tend to change based on the impression of the topic on speakers' internal mental state.", "labels": [], "entities": []}, {"text": "Diversity of speakers: Much like writing styles, speaking styles are highly idiosyncratic.", "labels": [], "entities": []}, {"text": "Training models on only few speakers can lead to degenerate solutions where models learn the identity of speakers as opposed to a generalizable model of multimodal language (.", "labels": [], "entities": []}, {"text": "Variety in annotations Having multiple labels to predict allows for studying the relations between labels.", "labels": [], "entities": []}, {"text": "Another positive aspect of having variety of labels is allowing for multi-task learning which has shown excellent performance in past research.", "labels": [], "entities": []}, {"text": "Our first contribution in this paper is to introduce the largest dataset of multimodal sentiment and emotion recognition called CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI).", "labels": [], "entities": [{"text": "multimodal sentiment and emotion recognition", "start_pos": 76, "end_pos": 120, "type": "TASK", "confidence": 0.610839331150055}, {"text": "CMU Multimodal Opinion Sentiment and Emotion Intensity", "start_pos": 128, "end_pos": 182, "type": "TASK", "confidence": 0.6101707560675484}]}, {"text": "CMU-MOSEI contains 23,453 annotated video segments from 1,000 distinct speakers and 250 topics.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9403700828552246}]}, {"text": "Each video segment contains manual transcription aligned with audio to phoneme level.", "labels": [], "entities": []}, {"text": "All the videos are gathered from online video sharing websites . The dataset is currently apart of the CMU Multimodal Data SDK and is freely available to the scientific community through Github 2 . Our second contribution is an interpretable fusion model called Dynamic Fusion Graph (DFG) to study the nature of cross-modal dynamics in multimodal language.", "labels": [], "entities": [{"text": "CMU Multimodal Data SDK", "start_pos": 103, "end_pos": 126, "type": "DATASET", "confidence": 0.9168152064085007}]}, {"text": "DFG contains built-in efficacies that are directly related to how modalities interact.", "labels": [], "entities": []}, {"text": "These efficacies are visualized and studied in detail in our experiments.", "labels": [], "entities": []}, {"text": "Aside interpretability, DFG achieves superior performance compared to previously proposed models for multimodal sentiment and emotion recognition on CMU-MOSEI.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.9666197896003723}, {"text": "multimodal sentiment and emotion recognition", "start_pos": 101, "end_pos": 145, "type": "TASK", "confidence": 0.609485524892807}, {"text": "CMU-MOSEI", "start_pos": 149, "end_pos": 158, "type": "DATASET", "confidence": 0.9283108115196228}]}], "datasetContent": [{"text": "We compare CMU-MOSEI to an extensive pool of datasets for sentiment analysis and emotion recognition.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9659275710582733}, {"text": "emotion recognition", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7574255764484406}]}, {"text": "The following datasets include a combination of language, visual and acoustic modalities as their input data.", "labels": [], "entities": []}, {"text": "CMU-MOSI () is a collection of 2199 opinion video clips each annotated with sentiment in the range.", "labels": [], "entities": [{"text": "CMU-MOSI", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9590902328491211}]}, {"text": "CMU-MOSEI is the next generation of CMU-MOSI.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9468516111373901}]}, {"text": "consists of online social review videos annotated at the video level for sentiment.) contains videos from the social media website YouTube that span a wide range of product reviews and opinion videos.", "labels": [], "entities": []}, {"text": "MOUD ( consists of product review videos in Spanish.", "labels": [], "entities": [{"text": "MOUD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.47209596633911133}]}, {"text": "Each video consists of multiple segments labeled to display positive, negative or neutral sentiment.", "labels": [], "entities": []}, {"text": "IEMO-CAP () is a collection of 2000 moviereview documents and sentences labeled with respect to their overall sentiment polarity or subjective rating.", "labels": [], "entities": [{"text": "IEMO-CAP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5366382002830505}]}, {"text": "Large Movie Review dataset (Maas et al., 2011) contains text from highly polar movie reviews.", "labels": [], "entities": [{"text": "Large Movie Review dataset", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.687017597258091}]}, {"text": "Sanders Tweets Sentiment (STS) consists of 5513 hand-classified tweets each classified with respect to one of four topics of Microsoft, Apple, Twitter, and Google.", "labels": [], "entities": [{"text": "Sanders Tweets Sentiment (STS)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7345312039057413}]}, {"text": "The Vera am Mittag (VAM) corpus consists of 12 hours of recordings of the German TV talk-show \"Vera am Mittag\".", "labels": [], "entities": [{"text": "Vera am Mittag (VAM) corpus", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.6675911375454494}]}, {"text": "This audio-visual data is labeled for continuous-valued scale for three emotion primitives: valence, activation and dominance.", "labels": [], "entities": []}, {"text": "VAM-Audio and VAMFaces are subsets that contain on acoustic and visual inputs respectively.", "labels": [], "entities": [{"text": "VAM-Audio", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7055795788764954}]}, {"text": "RECOLA ( consists of 9.5 hours of audio, visual, and physiological (electrocardiogram, and electrodermal activity) recordings of online dyadic interactions.", "labels": [], "entities": [{"text": "RECOLA", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9461532831192017}]}, {"text": "Mimicry ( consists of audiovisual recordings of human interactions in two situations: while discussing apolitical topic and while playing a role-playing game.", "labels": [], "entities": []}, {"text": "AFEW () is a dynamic temporal facial expressions data corpus consisting of close to real world environment extracted from movies.", "labels": [], "entities": []}, {"text": "Detailed comparison of CMU-MOSEI to the datasets in this section is presented in.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 23, "end_pos": 32, "type": "DATASET", "confidence": 0.9334099292755127}]}, {"text": "CMU-MOSEI has longer total duration as well as larger number of data point in total.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9279146194458008}, {"text": "duration", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8075929880142212}]}, {"text": "Furthermore, CMU-MOSEI has a larger variety in number of speakers and topics.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.9471062421798706}]}, {"text": "It has all three modalities provided, as well as annotations for both sentiment and emotions.", "labels": [], "entities": []}, {"text": "Understanding expressed sentiment and emotions are two crucial factors inhuman multimodal language.", "labels": [], "entities": []}, {"text": "We introduce a novel dataset for multimodal sentiment and emotion recognition called CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI).", "labels": [], "entities": [{"text": "multimodal sentiment and emotion recognition", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.6307197093963623}, {"text": "CMU Multimodal Opinion Sentiment and Emotion Intensity", "start_pos": 85, "end_pos": 139, "type": "TASK", "confidence": 0.5862604677677155}]}, {"text": "In the following subsections, we first explain the details of the CMU-MOSEI data acquisition, followed by details of annotation and feature extraction.", "labels": [], "entities": [{"text": "CMU-MOSEI data acquisition", "start_pos": 66, "end_pos": 92, "type": "DATASET", "confidence": 0.8698423107465109}, {"text": "feature extraction", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.6910801827907562}]}, {"text": "In our experiments, we seek to evaluate how modalities interact during multimodal fusion by studying the efficacies of DFG through time.", "labels": [], "entities": []}, {"text": "shows the results on CMU-MOSEI.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.9657572507858276}]}, {"text": "Accuracy is reported as A x where x is the number of sentiment classes as well as F1 measure.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9934606552124023}, {"text": "F1 measure", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9903595745563507}]}, {"text": "For regression we report MAE and correlation (r).", "labels": [], "entities": [{"text": "MAE", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9968840479850769}, {"text": "correlation", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9990553259849548}]}, {"text": "For emotion recognition due to the natural imbalances across various emotions, we use weighted accuracy () and F1 measure.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7769870758056641}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.8877511024475098}, {"text": "F1 measure", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9874373376369476}]}, {"text": "Graph-MFN shows superior performance in sentiment analysis and competitive performance in emotion recognition.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9640842080116272}, {"text": "emotion recognition", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7193193286657333}]}, {"text": "Therefore, DFG is both an effective and interpretable model for multimodal fusion.", "labels": [], "entities": [{"text": "multimodal fusion", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7418682277202606}]}, {"text": "To better understand the internal fusion mechanism between modalities, we visualize the behavior of the learned DFG efficacies in for various cases (deep red denotes high efficacy and deep blue denotes low efficacy).", "labels": [], "entities": []}, {"text": "Multimodal Fusion has a Volatile Nature: The first observation is that the structure of the DFG is changing case by case and for each case overtime.", "labels": [], "entities": [{"text": "Multimodal Fusion", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8123198747634888}]}, {"text": "As a result, the model seems to be selectively prioritizing certain dynamics over the others.", "labels": [], "entities": []}, {"text": "For example, in case (I) where all modalities are informative, all efficacies seem to be high, imply-  ing that the DFG is able to find useful information in unimodal, bimodal and trimodal interactions.", "labels": [], "entities": [{"text": "efficacies", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9582074284553528}]}, {"text": "However, in cases (II) and (III) where the visual modality is either uninformative or contradictory, the efficacies of v \u2192 l, v and v \u2192 l, a, v and l, a \u2192 l, a, v are reduced since no meaningful interactions involve the visual modality.", "labels": [], "entities": []}, {"text": "Priors in Fusion: Certain efficacies remain unchanged across cases and across time.", "labels": [], "entities": [{"text": "Priors in Fusion", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7154946525891622}, {"text": "efficacies", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9627236723899841}]}, {"text": "These are priors from Human Multimodal Language that DFG learns.", "labels": [], "entities": []}, {"text": "For example the model always seems to prioritize fusion between language and audio in (l \u2192 l, a), and (a \u2192 l, a).", "labels": [], "entities": []}, {"text": "Subsequently, DFG gives low values to efficacies that rely unilaterally on language or audio alone: the (l \u2192 \u03c4 ) and (a \u2192 \u03c4 ) efficacies seem to be consistently low.", "labels": [], "entities": [{"text": "DFG", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.6617026925086975}]}, {"text": "On the other hand, the visual modality appears to have a partially isolated behavior.", "labels": [], "entities": []}, {"text": "In the presence of informative visual information, the model increases the efficacies of (v \u2192 \u03c4 ) although the values of other visual efficacies also increase.", "labels": [], "entities": [{"text": "efficacies", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9768874645233154}]}, {"text": "Trace of Multimodal Fusion: We trace the dominant path that every modality undergoes during fusion: 1) language tends to first fuse with audio via (l \u2192 l, a) and the language and acoustic modalities together engage in higher level fusions such as (l, a \u2192 l, a, v).", "labels": [], "entities": []}, {"text": "Intuitively, this is aligned with the close ties between language and audio through word intonations.", "labels": [], "entities": []}, {"text": "2) The visual modality seems to engage in fusion only if it contains meaningful information.", "labels": [], "entities": []}, {"text": "In cases (I) and (IV), all the paths involving the visual modality are relatively active while in cases (II) and (III) the paths involving the visual modality have low efficacies.", "labels": [], "entities": []}, {"text": "3) The acoustic modality is mostly present in fusion with the language modality.", "labels": [], "entities": []}, {"text": "However, unlike language, the acoustic modality also appears to fuse with the visual modality if both modalities are meaningful, such as in case (I).", "labels": [], "entities": []}, {"text": "An interesting observation is that in almost all cases the efficacies of unimodal connections to terminal T is low, implying that T prefers to not rely on just one modality.", "labels": [], "entities": [{"text": "efficacies", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9854214191436768}]}, {"text": "Also, DFG always prefers to perform fusion between language and audio as inmost cases both l \u2192 l, a and a \u2192 l, a have high efficacies; intuitively inmost natural scenarios language and acoustic modalities are highly aligned.", "labels": [], "entities": []}, {"text": "Both of these cases show unchanging behaviors which we believe DFG has learned as natural priors of human communicative signal.", "labels": [], "entities": []}, {"text": "With these observations, we believe that DFG has successfully learned how to manage its internal structure to model human communication.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of CMU-MOSEI dataset statis- tics.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9318747222423553}]}, {"text": " Table 3: Results for sentiment analysis and emotion recognition on the MOSEI dataset (reported results  are as of 5/11/2018. please check the CMU Multimodal Data SDK github for current state of the art and  new features for CMU-MOSEI and other datasets). SOTA1 and SOTA2 refer to the previous best and  second best state-of-the-art models (from Section 2) respectively. Compared to the baselines Graph-MFN  achieves superior performance in sentiment analysis and competitive performance in emotion recognition.  For all metrics, higher values indicate better performance except for MAE where lower values indicate  better performance.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.962861180305481}, {"text": "emotion recognition", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7316537499427795}, {"text": "MOSEI dataset", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9763958752155304}, {"text": "CMU Multimodal Data SDK github", "start_pos": 143, "end_pos": 173, "type": "DATASET", "confidence": 0.9129754781723023}, {"text": "sentiment analysis", "start_pos": 441, "end_pos": 459, "type": "TASK", "confidence": 0.8543795645236969}, {"text": "emotion recognition", "start_pos": 491, "end_pos": 510, "type": "TASK", "confidence": 0.7559660375118256}, {"text": "MAE", "start_pos": 583, "end_pos": 586, "type": "METRIC", "confidence": 0.662604808807373}]}]}