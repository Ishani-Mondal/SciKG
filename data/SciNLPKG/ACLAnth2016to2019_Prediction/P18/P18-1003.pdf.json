{"title": [{"text": "Unsupervised Learning of Distributional Relation Vectors", "labels": [], "entities": [{"text": "Distributional Relation Vectors", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.7668770949045817}]}], "abstractContent": [{"text": "Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning.", "labels": [], "entities": []}, {"text": "While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships are based on manipulating pre-trained word vectors.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics.", "labels": [], "entities": []}, {"text": "To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors.", "labels": [], "entities": []}, {"text": "We then show how relation vectors can be naturally embedded into the resulting vector space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings are vector space representations of word meaning.", "labels": [], "entities": []}, {"text": "A remarkable property of these models is that they capture various lexical relationships, beyond mere similarity.", "labels": [], "entities": []}, {"text": "For example, ( found that analogy questions of the form \"a is to b what c is to ?\" can often be answered by finding the word d that maximizes cos(w b \u2212 w a + w c , w d ), where we write w x for the vector representation of a word x.", "labels": [], "entities": []}, {"text": "Intuitively, the word vector w a represents a in terms of its most salient features.", "labels": [], "entities": []}, {"text": "For example, w paris implicitly encodes that Paris is located in France and that it is a capital city, which is intuitively why the 'capital of' relation can be modeled in terms of a vector difference.", "labels": [], "entities": []}, {"text": "Other relationships, however, such as the fact that Macron succeeded Hollande as president of France, are unlikely to be captured byword embeddings.", "labels": [], "entities": []}, {"text": "Relation extraction methods can discover such information by analyzing sentences that contain both of the words or entities involved (), but they typically need a large number of training examples to be effective.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9236509501934052}]}, {"text": "A third alternative, which we consider in this paper, is to characterize the relatedness between two words sand t by learning a relation vector r st in an unsupervised way from corpus statistics.", "labels": [], "entities": []}, {"text": "Among others, such vectors can be used to find word pairs that are similar to a given word pair (i.e. finding analogies), or to find the most prototypical examples among a given set of relation instances.", "labels": [], "entities": []}, {"text": "They can also be used as an alternative to the aforementioned relation extraction methods, by subsequently training a classifier that uses the relation vectors as input, which might be particularly effective in cases where only limited amounts of training data are available (with the case of analogy finding from a single instance being an extreme example).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7794687151908875}]}, {"text": "The most common unsupervised approach for learning relation vectors consists of averaging the embeddings of the words that occur in between sand t, in sentences that contain both (.", "labels": [], "entities": []}, {"text": "While this strategy is often surprisingly effective (, it is sub-optimal for two reasons.", "labels": [], "entities": []}, {"text": "First, many of the words co-occurring with sand twill be semantically related to s or tot, but will not actually be descriptive for the relationship between sand t; e.g. the vector describing the relation between Paris and France should not be affected by words such as eiffel (which only relates to Paris).", "labels": [], "entities": []}, {"text": "Second, it gives too much weight to stopwords, which cannot be addressed in a straightforward way as some stop-words are actually crucial for modeling relationships (e.g. prepositions such as 'in' or 'of' or Hearst patterns).", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for learning relation vectors directly from co-occurrence statistics.", "labels": [], "entities": []}, {"text": "We first introduce a variant of GloVe, in which word vectors can be directly interpreted as smoothed PMI-weighted bag-of-words representations.", "labels": [], "entities": []}, {"text": "We then represent relationships between words as weighted bag-of-words representations, using generalizations of PMI to three arguments, and learn vectors that correspond to smoothed versions of these representations.", "labels": [], "entities": []}, {"text": "As far as the possible applications of our methodology is concerned, we imagine that relation vectors can be used in various ways to enrich the input to neural network models.", "labels": [], "entities": []}, {"text": "As a simple example, in a question answering system, we could \"annotate\" mentions of entities with relation vectors encoding their relationship to the different words from the question.", "labels": [], "entities": [{"text": "question answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7640860974788666}]}, {"text": "As another example, we could consider a recommendation system which takes advantage of vectors expressing the relationship between items that have been bought (or viewed) by a customer and other items from the catalogue.", "labels": [], "entities": []}, {"text": "Finally, relation vectors should also be useful for knowledge completion, especially in cases where few training examples per relation type are given (meaning that neural network models could not be used) and where relations cannot be predicted from the already available knowledge (meaning that knowledge graph embedding methods could not be used, or are at least not sufficient).", "labels": [], "entities": [{"text": "knowledge completion", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.8511784970760345}]}], "datasetContent": [{"text": "In our experiments, we have used the Wikipedia dump from November 2nd, 2015, which consists of 1,335,766,618 tokens.", "labels": [], "entities": [{"text": "Wikipedia dump from November 2nd", "start_pos": 37, "end_pos": 69, "type": "DATASET", "confidence": 0.9613507509231567}]}, {"text": "We have removed punctuations and HTML/XML tags, and we have lowercased all tokens.", "labels": [], "entities": []}, {"text": "Words with fewer than 10 occurrences have been removed from the corpus.", "labels": [], "entities": []}, {"text": "To detect sentence boundaries, we have used the Apache sentence segmentation tool.", "labels": [], "entities": [{"text": "Apache sentence segmentation", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6224484543005625}]}, {"text": "In all our experiments, we have set the number of dimensions to 300, which was found to be a good choice in previous work, e.g. ().", "labels": [], "entities": []}, {"text": "We use a context window size W of 10 words.", "labels": [], "entities": []}, {"text": "The number of iterations for SGD was set to 50.", "labels": [], "entities": [{"text": "SGD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9253297448158264}]}, {"text": "For our model, we have tuned the smoothing parameter \u03b1 based on held-out tuning data, considering values from {0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001}.", "labels": [], "entities": []}, {"text": "We have noticed that inmost of the cases the value of \u03b1 was automatically selected as 0.00001.", "labels": [], "entities": []}, {"text": "To efficiently compute the triples, we have used the Zettair 3 retrieval engine.", "labels": [], "entities": []}, {"text": "As our main baselines, we use three popular unsupervised methods for constructing relation vec-.", "labels": [], "entities": []}, {"text": "Second, Conc uses the concatenation of w i and wk . This model is more general than Diff but it uses twice as many dimensions, which may make it harder to learn a good classifier from few examples.", "labels": [], "entities": []}, {"text": "The use of concatenations is popular e.g. in the context of hypernym detection ().", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.834672212600708}]}, {"text": "Finally, Avg averages the vector representations of the words occurring in sentences that Diff, contain i and k.", "labels": [], "entities": [{"text": "Avg", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8776097893714905}]}, {"text": "In particular, let r avg ik be obtained by averaging the word vectors of the context words appearing between i and k for each sentence containing i and k (in that order), and then averaging the vectors obtained from each of these sentences.", "labels": [], "entities": []}, {"text": "Let s avg ik and t avg ik be similarly obtained from the words occurring before i and the words occurring after k respectively.", "labels": [], "entities": []}, {"text": "The considered relation vector is then defined as the concatenation of r avg ik , r avg ki , s avg ik , s avg ki , t avg ik , t avg ki , w i and wk . The Avg will allow us to directly compare how much we can improve relation vectors by deviating from the common strategy of averaging word vectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the relation induction task.", "labels": [], "entities": [{"text": "relation induction task", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.9392081300417582}]}, {"text": " Table 2: Results for the relation induction task using alternative word embedding models.", "labels": [], "entities": [{"text": "relation induction task", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.9425199429194132}]}, {"text": " Table 3: Relation induction without position  weighting (left) and without the relation vectors  s ik and t ik (right).", "labels": [], "entities": [{"text": "Relation induction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8374819457530975}]}, {"text": " Table 4: Results for measuring degrees of proto- typicality (Spearman \u03c1 \u00d7 100).", "labels": [], "entities": [{"text": "Spearman \u03c1 \u00d7 100)", "start_pos": 62, "end_pos": 79, "type": "METRIC", "confidence": 0.8916651248931885}]}]}