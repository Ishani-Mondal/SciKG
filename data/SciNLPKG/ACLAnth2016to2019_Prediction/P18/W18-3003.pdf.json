{"title": [{"text": "Extrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons", "labels": [], "entities": [{"text": "Word Representation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.6669178158044815}]}], "abstractContent": [{"text": "We propose post-processing method for enriching not only word representation but also its vector space using semantic lexicons , which we call extrofitting.", "labels": [], "entities": []}, {"text": "The method consists of 3 steps as follows: (i) Expanding 1 or more dimension(s) on all the word vectors, filling with their representative value.", "labels": [], "entities": []}, {"text": "(ii) Transferring semantic knowledge by averaging each representative values of synonyms and filling them in the expanded dimension(s).", "labels": [], "entities": [{"text": "Transferring semantic knowledge", "start_pos": 5, "end_pos": 36, "type": "TASK", "confidence": 0.8270365595817566}]}, {"text": "These two steps make representations of the synonyms close together.", "labels": [], "entities": []}, {"text": "(iii) Projecting the vector space using Linear Discriminant Analysis, which eliminates the expanded dimension(s) with semantic knowledge.", "labels": [], "entities": [{"text": "Linear Discriminant Analysis", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.666845699151357}]}, {"text": "When experimenting with GloVe, we find that our method outperforms Faruqui's retrofitting on some of word similarity task.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7629068791866302}]}, {"text": "We also report further analysis on our method in respect to word vector dimensions , vocabulary size as well as other well-known pretrained word vectors (e.g., Word2Vec, Fasttext).", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9593642354011536}]}], "introductionContent": [{"text": "As a method to represent natural language on computer, researchers have utilized distributed word representation.", "labels": [], "entities": [{"text": "distributed word representation", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.6267120639483134}]}, {"text": "The distributed word representation is to represent a word as n-dimensional float vector, hypothesizing that some or all of the dimensions may capture semantic meaning of the word.", "labels": [], "entities": []}, {"text": "The representation has worked well in various NLP tasks, substituting one-hot representation (.", "labels": [], "entities": []}, {"text": "Two major algorithms learning the distributed word representation are CBOW (Continuous Bag-of-Words) and skip-gram (.", "labels": [], "entities": []}, {"text": "Both CBOW and skip-gram learn the representation using one hidden neural networks.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.7564077377319336}]}, {"text": "The difference is that CBOW learns the representation of a center word from neighbor words whereas skip-gram gets the representation of neighbor words from a center word.", "labels": [], "entities": []}, {"text": "Therefore, the algorithms have to depend on word order, because their objective function is to maximize the probability of occurrence of neighbor words given the center word.", "labels": [], "entities": []}, {"text": "Then a problem occurs because the word representations do not have any information to distinguish synonyms and antonyms.", "labels": [], "entities": []}, {"text": "For example, worthy and desirable should be mapped closely on the vector space as well as agree and disagree should be mapped apart, although they occur on a very similar pattern.", "labels": [], "entities": []}, {"text": "Researchers have focused on the problem, and their main approaches are to use semantic lexicons.", "labels": [], "entities": []}, {"text": "One of the successful works is Faruqui's retrofitting 1 , which can be summarized as pulling word vectors of synonyms close together by weighted averaging the word vectors on a fixed vector space (it will be explained in Section 2.1).", "labels": [], "entities": []}, {"text": "The retrofitting greatly improves word similarity between synonyms, and the result not only corresponds with human intuition on words but also performs better on document classification tasks with comparison to original word embeddings (.", "labels": [], "entities": [{"text": "document classification", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.6986656039953232}]}, {"text": "From the idea of retrofitting, our method hypothesize that we can enrich not only word representation but also its vector space using semantic lexicons 2 . We call our method as extrofitting, which retrofits word vectors by expanding its dimensions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our methods on word similarity tasks using 4 different kinds of dataset.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.8097784717877706}]}, {"text": "MEN-3k (Bruni) has 65 words paired scored from 0 to 4.", "labels": [], "entities": [{"text": "MEN-3k (Bruni)", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7088837549090385}]}, {"text": "MEN-3k and WordSim-353 were split into train (or dev) set and test set, but we combined them together solely for evaluation purpose.", "labels": [], "entities": [{"text": "MEN-3k", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9657071232795715}, {"text": "WordSim-353", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9721465706825256}]}, {"text": "The other datasets have lots of out-ofvocabulary, so we disregard them for future work.", "labels": [], "entities": []}, {"text": "The word similarity task is to calculate Spearman's correlation) between two words as word vector format.", "labels": [], "entities": [{"text": "Spearman's correlation)", "start_pos": 41, "end_pos": 64, "type": "METRIC", "confidence": 0.7317051962018013}]}, {"text": "We first apply extrofitting to GloVe from different data sources and present the result in.", "labels": [], "entities": []}, {"text": "The result shows that although the number of the extrofitted word with FrameNet is less than the other lexicons, its performance is on par with other lexicons.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.8672242164611816}]}, {"text": "We can also ensure that our method improves the performance of original pretrained word vectors.", "labels": [], "entities": []}, {"text": "Next, we perform extrofitting on GloVe in different word dimension and compare the performance with retrofitting.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.910650372505188}]}, {"text": "We use WordNet all lexicon on both retrofitting and extrofitting to compare the performances in the ideal environment for retrofitting.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9567788243293762}]}, {"text": "We present the results in Table 2.", "labels": [], "entities": []}, {"text": "We can demonstrate that our method outperforms retrofitting on some of word similarity tasks, MEN-3k and WordSim-353.", "labels": [], "entities": [{"text": "MEN-3k", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.8708420395851135}, {"text": "WordSim-353", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.9648316502571106}]}, {"text": "We believe that extrofitting on SimLex-999 and RG-65 is less powerful because all word pairs in the datasets are included on WordNet all lexicon.", "labels": [], "entities": [{"text": "WordNet all lexicon", "start_pos": 125, "end_pos": 144, "type": "DATASET", "confidence": 0.9171362121899923}]}, {"text": "Since retrofitting forces the word similarity to be   improved by weighted averaging their word vectors, it is prone to be overfitted on semantic lexicons.", "labels": [], "entities": []}, {"text": "On the other hand, extrofitting also uses synonyms to improve word similarity but it works differently that extrofitting projects the synonyms both close together on anew vector space and far from the other words.", "labels": [], "entities": []}, {"text": "Therefore, our method can make more generalized word representation than retrofitting.", "labels": [], "entities": [{"text": "word representation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7025405913591385}]}, {"text": "We plot top-100 nearest words using t-SNE, as shown in.", "labels": [], "entities": []}, {"text": "We can find that retrofitting strongly collects synonym words together whereas extrofitting weakly disperses the words, resulting loss in cosine similarity score.", "labels": [], "entities": [{"text": "cosine similarity score", "start_pos": 138, "end_pos": 161, "type": "METRIC", "confidence": 0.7557053367296854}]}, {"text": "However, the result of extrofitting can be interpreted as generalization that the word vectors strengthen its own meaning by being faraway from each other, still keeping synonyms relatively close together (see).", "labels": [], "entities": []}, {"text": "When we list up top-10 nearest words, extrofitting shows more favorable results than retrofitting.", "labels": [], "entities": []}, {"text": "We can also observe that extrofitting even can be applied to words which are not included in semantic lexicons.", "labels": [], "entities": []}, {"text": "Lastly, we apply extrofitting to other well-known pretrained word vectors trained by different algorithms (see Subsection 4.1).", "labels": [], "entities": []}, {"text": "The result is presented in   representations except on WordSim-353 and RG-65, respectively.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9804984927177429}, {"text": "RG-65", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.73542320728302}]}, {"text": "We find that our method can distort the well-established word embeddings.", "labels": [], "entities": []}, {"text": "However, our results are noteworthy in that extrofitting can be applied to other kinds of pretrained word vectors for further enrichment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman's correlation of extrofitted word vectors for word similarity tasks using semantic  lexicon. Our method improves pretrained GloVe in different vocabulary size.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.7704969048500061}]}, {"text": " Table 2: Comparison of Spearman's correlation of retrofitted or extrofitted word vectors for word simi- larity tasks. Our method, extrofitting, outperforms retrofitting on MEN-3k and WordSim-353.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 184, "end_pos": 195, "type": "DATASET", "confidence": 0.9351168870925903}]}, {"text": " Table 3: List of top-10 nearest words of cue words in different post-processing methods. We show cosine  similarity scores of two words included in semantic lexicon (love) or not (soo).", "labels": [], "entities": []}, {"text": " Table 4: Spearman's correlation of extrofitted word vectors for word similarity tasks on pretrained word  vectors by Word2Vec and Fasttext. Extrofitting can be applied to other kinds of pretrained word vector.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.7923199931780497}, {"text": "Word2Vec", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.9656693935394287}]}]}