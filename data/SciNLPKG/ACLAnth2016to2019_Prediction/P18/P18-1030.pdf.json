{"title": [], "abstractContent": [{"text": "Bi-directional LSTMs area powerful tool for text representation.", "labels": [], "entities": [{"text": "text representation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8084059059619904}]}, {"text": "On the other hand, they have been shown to suffer various limitations due to their sequential nature.", "labels": [], "entities": []}, {"text": "We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word.", "labels": [], "entities": []}, {"text": "Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incre-mental reading of a sequence of words.", "labels": [], "entities": []}, {"text": "Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural models have become the dominant approach in the NLP literature.", "labels": [], "entities": []}, {"text": "Compared to handcrafted indicator features, neural sentence representations are less sparse, and more flexible in encoding intricate syntactic and semantic information.", "labels": [], "entities": []}, {"text": "Among various neural networks for encoding sentences, bi-directional LSTMs (BiLSTM)) have been a dominant method, giving state-of-the-art results in language modelling), machine translation (, syntactic parsing and question answering (.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.6883844435214996}, {"text": "machine translation", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.8208500146865845}, {"text": "syntactic parsing", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.7482089996337891}, {"text": "question answering", "start_pos": 215, "end_pos": 233, "type": "TASK", "confidence": 0.8546226620674133}]}, {"text": "Despite their success, BiLSTMs have been shown to suffer several limitations.", "labels": [], "entities": []}, {"text": "For example, their inherently sequential nature endows computation non-parallel within the same sentence (, which can lead to a computational bottleneck, hindering their use in the in- dustry.", "labels": [], "entities": []}, {"text": "In addition, local ngrams, which have been shown a highly useful source of contextual information for NLP, are not explicitly modelled ().", "labels": [], "entities": []}, {"text": "Finally, sequential information flow leads to relatively weaker power in capturing longrange dependencies, which results in lower performance in encoding longer sentences.", "labels": [], "entities": []}, {"text": "We investigate an alternative recurrent neural network structure for addressing these issues.", "labels": [], "entities": []}, {"text": "As shown in, the main idea is to model the hidden states of all words simultaneously at each recurrent step, rather than one word at a time.", "labels": [], "entities": []}, {"text": "In particular, we view the whole sentence as a single state, which consists of sub-states for individual words and an overall sentence-level state.", "labels": [], "entities": []}, {"text": "To capture local and non-local contexts, states are updated recurrently by exchanging information between each other.", "labels": [], "entities": []}, {"text": "Consequently, we refer to our model as sentence-state LSTM, or S-LSTM in short.", "labels": [], "entities": []}, {"text": "Empirically, S-LSTM can give effective sentence encoding after 3 -6 recurrent steps.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.699852705001831}]}, {"text": "In contrast, the number of recurrent steps necessary for BiLSTM scales with the size of the sentence.", "labels": [], "entities": []}, {"text": "At each recurrent step, information exchange is conducted between consecutive words in the sentence, and between the sentence-level state and each word.", "labels": [], "entities": []}, {"text": "In particular, each word receives information from its predecessor and successor simultaneously.", "labels": [], "entities": []}, {"text": "From an initial state without information exchange, each word-level state can obtain 3-gram, 5-gram and 7-gram information after 1, 2 and 3 recurrent steps, respectively.", "labels": [], "entities": []}, {"text": "Being connected with every word, the sentence-level state vector serves to exchange non-local information with each word.", "labels": [], "entities": []}, {"text": "In addition, it can also be used as a global sentence-level representation for classification tasks.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.9025813043117523}]}, {"text": "Results on both classification and sequence labelling show that S-LSTM gives better accuracies compared to BiLSTM using the same number of parameters, while being faster.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9919785261154175}]}, {"text": "We release our code and models at https://github.com/ leuchine/S-LSTM, which include all baselines and the final model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We empirically compare S-LSTMs and BiLSTMs on different classification and sequence labelling tasks.", "labels": [], "entities": []}, {"text": "All experiments are conducted using a GeForce GTX 1080 GPU with 8GB memory.", "labels": [], "entities": [{"text": "GeForce GTX 1080 GPU", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8897156715393066}]}, {"text": "We use the movie review development data to investigate different configurations of S-LSTMs and BiLSTMs.", "labels": [], "entities": [{"text": "movie review development data", "start_pos": 11, "end_pos": 40, "type": "DATASET", "confidence": 0.6357233971357346}]}, {"text": "For S-LSTMs, the default configuration uses \ud97b\udf59s\ud97b\udf59 and \ud97b\udf59/s\ud97b\udf59 words for augmenting words Hyperparameters: shows the development results of various S-LSTM settings, where Time refers to training time per epoch.", "labels": [], "entities": []}, {"text": "Without the sentence-level node, the accuracy of S-LSTM drops to 81.76%, demonstrating the necessity of global information exchange.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.999659538269043}]}, {"text": "Adding one additional sentence-level node as described in Section 3.2 does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9990981817245483}]}, {"text": "As a result, we use only 1 sentence-level node for the remaining experiments.", "labels": [], "entities": []}, {"text": "The accuracies of S-LSTM increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9945527911186218}]}, {"text": "We fix the hidden size to 300 accordingly.", "labels": [], "entities": []}, {"text": "Without using \ud97b\udf59s\ud97b\udf59 and \ud97b\udf59/s\ud97b\udf59, the performance of S-LSTM drops from 82.64% to 82.36%, showing the effectiveness of having these additional nodes.", "labels": [], "entities": []}, {"text": "Hyperparameters for BiLSTM models are also set according to the development data, which we omit here.", "labels": [], "entities": []}, {"text": "In, the number of recurrent state transition steps of S-LSTM is decided according to the best development performance.", "labels": [], "entities": []}, {"text": "draws the development accuracies of SLSTMs with various window sizes against the number of recurrent steps.", "labels": [], "entities": []}, {"text": "As can be seen from the figure, when the number of time steps increases from 1 to 11, the accuracies generally increase, before reaching a maximum value.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9726340770721436}]}, {"text": "This shows the effectiveness of recurrent information exchange in S-LSTM state transition.", "labels": [], "entities": []}, {"text": "On the other hand, no significant differences are observed on the peak accuracies given by different window sizes, although a larger window size (e.g.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Movie review DEV results of S-LSTM", "labels": [], "entities": [{"text": "Movie review DEV", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.6142623424530029}, {"text": "S-LSTM", "start_pos": 38, "end_pos": 44, "type": "TASK", "confidence": 0.4632735848426819}]}, {"text": " Table 3: Movie review development results", "labels": [], "entities": [{"text": "Movie review development", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6110636393229166}]}, {"text": " Table 4: Test set results on movie review dataset  (* denotes significance in all tables).", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.7886861165364584}]}, {"text": " Table 5: Results on the 16 datasets of Liu et al. (2017). Time format: train (test)", "labels": [], "entities": []}, {"text": " Table 6: Results on PTB (POS tagging)", "labels": [], "entities": [{"text": "PTB", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.5506573915481567}, {"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.6362931281328201}]}, {"text": " Table 7: Results on CoNLL03 (NER)", "labels": [], "entities": [{"text": "CoNLL03 (NER)", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.7938591539859772}]}]}