{"title": [], "abstractContent": [{"text": "Shi, Huang, and Lee (2017a) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirec-tional LSTM features.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7502436637878418}]}, {"text": "However, their results were limited to projective parsing.", "labels": [], "entities": [{"text": "projective parsing", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6491284668445587}]}, {"text": "In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH 4 algorithm, an Opn 4 q mildly non-projective dynamic-programming parser with very high coverage on non-projective treebanks.", "labels": [], "entities": []}, {"text": "To make MH 4 compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions.", "labels": [], "entities": []}, {"text": "We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages.", "labels": [], "entities": [{"text": "non-projective transition-based parsing", "start_pos": 63, "end_pos": 102, "type": "TASK", "confidence": 0.6613994042078654}]}], "introductionContent": [{"text": "Transition-based dependency parsers area popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (.", "labels": [], "entities": [{"text": "Transition-based dependency parsers", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7096958756446838}, {"text": "natural language parsing", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6312853495279948}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9984525442123413}]}, {"text": "Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search.", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.5343750566244125}]}, {"text": "While cubic-time exact inference algorithms for several well-known projective transition systems had been known since the work of and, they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resulted in jumps in asymptotic runtime complexity to impractical levels.", "labels": [], "entities": []}, {"text": "However, the recent popularization of bidirectional long-short term memory networks (biLSTMs;) to derive feature representations for parsing, given their capacity to capture long-range information, has demonstrated that one may not need to use complex feature models to obtain good accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 133, "end_pos": 140, "type": "TASK", "confidence": 0.9693314433097839}, {"text": "accuracy", "start_pos": 282, "end_pos": 290, "type": "METRIC", "confidence": 0.9849473834037781}]}, {"text": "In this context, presented an implementation of the exact inference algorithms of with a minimal set of only two bi-LSTM-based feature vectors.", "labels": [], "entities": []}, {"text": "This not only kept the complexity cubic, but also obtained state-of-the-art results in English and Chinese parsing.", "labels": [], "entities": []}, {"text": "While their approach provides both accurate parsing and the flexibility to use any of greedy, beam, or exact decoding with the same underlying transition systems, it does not support nonprojectivity.", "labels": [], "entities": [{"text": "parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9576582312583923}]}, {"text": "Trees with crossing dependencies makeup a significant portion of many treebanks, going as high as 63% for the Ancient Greek treebank in the Universal Dependencies 1 (UD) dataset version 2.0 and averaging around 12% overall languages in UD 2.0.", "labels": [], "entities": [{"text": "Universal Dependencies 1 (UD) dataset version", "start_pos": 140, "end_pos": 185, "type": "DATASET", "confidence": 0.5721720792353153}]}, {"text": "In this paper, we extend approach to mildly nonprojective parsing in what, to our knowledge, is the first implementation of exact decoding fora non-projective transition-based parser.", "labels": [], "entities": []}, {"text": "As in the projective case, a mildly non-projective decoder has been known for several years), corresponding to a variant of the transition-based parser of.", "labels": [], "entities": []}, {"text": "However, its Opn 7 q runtimeor the Opn 6 q of a recently introduced improvedcoverage variant) -is still prohibitively costly in practice.", "labels": [], "entities": []}, {"text": "Instead, we seek a more efficient algorithm to adapt, and thus develop a transition-based interpretation of G\u00f3mez-Rodr\u00edguez et al.'s (2011) MH 4 dynamic programming parser, which has been shown to provide very good non-projective coverage in Opn 4 q time (.", "labels": [], "entities": [{"text": "MH 4 dynamic programming parser", "start_pos": 140, "end_pos": 171, "type": "TASK", "confidence": 0.5324296891689301}]}, {"text": "While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (), its own relation to transition-based parsing was not known.", "labels": [], "entities": [{"text": "MH 4 parser", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6314172347386678}]}, {"text": "Here, we show that MH 4 can be interpreted as exploring a subset of the search space of a transition-based parser that generalizes the arc-hybrid system, under a mapping that differs from the \"push computation\" paradigm used by the previously-known dynamic-programming decoders for transition systems.", "labels": [], "entities": []}, {"text": "This allows us to extend's work to non-projective parsing, by implementing MH 4 with a minimal set of transition-based features.", "labels": [], "entities": [{"text": "MH", "start_pos": 75, "end_pos": 77, "type": "DATASET", "confidence": 0.8272112607955933}]}, {"text": "Experimental results show that our approach outperforms the projective approach of and maximum-spanning-tree nonprojective parsing on the most highly nonprojective languages in the CoNLL 2017 sharedtask data that have a single treebank.", "labels": [], "entities": [{"text": "CoNLL 2017 sharedtask data", "start_pos": 181, "end_pos": 207, "type": "DATASET", "confidence": 0.9338575154542923}]}, {"text": "We also compare with the third-order 1-Endpoint-Crossing (1EC) parser of, the only other practical implementation of an exact mildly nonprojective decoder that we know of, which also runs in Opn 4 q but without a transition-based interpretation.", "labels": [], "entities": []}, {"text": "We obtain comparable results for these two algorithms, in spite of the fact that the MH 4 algorithm is notably simpler than 1EC.", "labels": [], "entities": []}, {"text": "The MH 4 parser remains effective in parsing projective treebanks, while our baseline parser, the fully non-projective maximum spanning tree algorithm, falls behind due to its unnecessarily large search space in parsing these languages.", "labels": [], "entities": [{"text": "parsing projective treebanks", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.897727350393931}]}, {"text": "Our code, including our re-implementation of the third-order 1EC parser with neural scoring, is available at https://github.com/tzshi/ mh4-parser-acl18.: A non-projective dependency parse from the UD 2.0 English treebank.", "labels": [], "entities": [{"text": "UD 2.0 English treebank", "start_pos": 197, "end_pos": 220, "type": "DATASET", "confidence": 0.8894616514444351}]}], "datasetContent": [{"text": "Data and Evaluation We experiment with the Universal Dependencies (UD) 2.0 dataset used for the CoNLL 2017 shared task (.", "labels": [], "entities": [{"text": "Universal Dependencies (UD) 2.0 dataset", "start_pos": 43, "end_pos": 82, "type": "DATASET", "confidence": 0.5298929597650256}, {"text": "CoNLL 2017 shared task", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.8154950588941574}]}, {"text": "We restrict our choice of languages to be those with only one training treebank, fora better comparison with the shared task results.", "labels": [], "entities": []}, {"text": "Among these languages, we pick the top 10 most non-projective languages.", "labels": [], "entities": []}, {"text": "Their basic statistics are listed in Table 3.", "labels": [], "entities": []}, {"text": "For all development-set results, we assume gold-standard tokenization and sentence delimitation.", "labels": [], "entities": []}, {"text": "When comparing to the shared task results on test sets, we use the provided baseline UDPipe () segmentation.", "labels": [], "entities": []}, {"text": "Our models do not use part-of-speech tags or morphological tags as features, but rather leverage such information via stack propagation, i.e., we learn to predict them as a secondary training objective.", "labels": [], "entities": []}, {"text": "We report unlabeled attachment F1-scores (UAS) on the development sets for better focus on comparing our (unlabeled) parsing modules.", "labels": [], "entities": [{"text": "unlabeled attachment F1-scores (UAS)", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.7407627205053965}]}, {"text": "We report its labeled variant (LAS), the main metric of the shared task, on the test sets.", "labels": [], "entities": [{"text": "labeled variant (LAS)", "start_pos": 14, "end_pos": 35, "type": "METRIC", "confidence": 0.8616118550300598}]}, {"text": "For each experiment setting, we ran the model with 5 different random initializations, and report the mean and standard deviation.", "labels": [], "entities": []}, {"text": "We detail the implementation details in the supplementary material.", "labels": [], "entities": []}, {"text": "Baseline Systems For comparison, we include three baseline systems with the same underlying feature representations and scoring paradigm.", "labels": [], "entities": []}, {"text": "All When multiple treebanks are available, one can develop domain transfer strategies, which is not the focus of this work.", "labels": [], "entities": [{"text": "domain transfer", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.785876601934433}]}, {"text": "the following baseline systems are trained with the cost-augmented large-margin loss function.", "labels": [], "entities": []}, {"text": "The MH 3 parser is the projective instantiation of the MH k parser family.", "labels": [], "entities": []}, {"text": "This corresponds to the global version of the arc-hybrid transition system ().", "labels": [], "entities": []}, {"text": "We adopt the minimal feature representation ts 0 , b 0 u, following.", "labels": [], "entities": []}, {"text": "For this model, we also implement a greedy incremental version.", "labels": [], "entities": []}, {"text": "The edge-factored non-projective maximal spanning tree (MST) parser allows arbitrary non-projective structures.", "labels": [], "entities": []}, {"text": "This decoding approach has been shown to be very competitive in parsing non-projective treebanks, and was deployed in the top-performing system at the CoNLL 2017 shared task.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task", "start_pos": 151, "end_pos": 173, "type": "DATASET", "confidence": 0.8745888322591782}]}, {"text": "We score each edge individually, with the features being the bi-LSTM vectors th, mu, where h is the head, and m the modifier of the edge.", "labels": [], "entities": []}, {"text": "The crossing-sensitive third-order 1EC parser provides a hybrid dynamic program for parsing 1-Endpoint-Crossing non-projective dependency trees with higher-order factorization.", "labels": [], "entities": [{"text": "parsing 1-Endpoint-Crossing non-projective dependency trees", "start_pos": 84, "end_pos": 143, "type": "TASK", "confidence": 0.8571986436843873}]}, {"text": "Depending on whether an edge is crossed, we can access the modifier's grandparent g, head h, and sibling si.", "labels": [], "entities": []}, {"text": "We take their corresponding bi-LSTM features tg, h, m, siu for scoring each edge.", "labels": [], "entities": []}, {"text": "This is a re-implementation of Pitler (2014) with neural scoring functions.", "labels": [], "entities": []}, {"text": "shows the developmentset performance of our models as compared with baseline systems.", "labels": [], "entities": []}, {"text": "MST considers non-projective structures, and thus enjoys a theoretical advantage over projective MH 3 , especially for the most non-projective languages.", "labels": [], "entities": []}, {"text": "However, it has a vastly larger output space, making the selection of correct structures difficult.", "labels": [], "entities": []}, {"text": "Further, the scoring is edge-factored, and does not take any structural contexts into consideration.", "labels": [], "entities": []}, {"text": "This tradeoff leads to the similar performance of MST comparing to MH 3 . In comparison, both 1EC and MH 4 are mildly non-projective parsing algorithms, limiting the size of the output space.", "labels": [], "entities": []}, {"text": "1EC includes higherorder features that look at tree-structural contexts; MH 4 derives its features from parsing configurations of a transition system, hence leveraging contexts within transition sequences.", "labels": [], "entities": [{"text": "1EC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9303829669952393}]}, {"text": "These considerations explain their significant improvements over MST.", "labels": [], "entities": [{"text": "MST", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.7775031924247742}]}, {"text": "We also observe that MH 4 recovers more short dependencies than 1EC, while 1EC is better at longer-distance ones.", "labels": [], "entities": []}, {"text": "In comparison to MH 4 -two, the richer feature representation of MH 4 -hybrid helps in all our languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of local parsing models with  varying number of features. We report average  UAS over 10 languages on UD 2.0.", "labels": [], "entities": [{"text": "UD 2.0", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.8120876848697662}]}, {"text": " Table 2: Performance of global parsing models  with varying number of features.", "labels": [], "entities": [{"text": "global parsing", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.5068390965461731}]}, {"text": " Table 3: Statistics of selected training treebanks from Universal Dependencies 2.0 for the CoNLL 2017  shared task (Zeman et al., 2017), sorted by per-sentence projective ratio.", "labels": [], "entities": [{"text": "CoNLL 2017  shared task", "start_pos": 92, "end_pos": 115, "type": "DATASET", "confidence": 0.8157320469617844}]}, {"text": " Table 4: Experiment results (UAS, %) on the UD 2.0 development set. Bold: best result per language.", "labels": [], "entities": [{"text": "UAS", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.8618744015693665}, {"text": "UD 2.0 development set", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.8967062383890152}]}, {"text": " Table 5: Evaluation results (LAS, %) on the test set using the CoNLL 2017 shared task setup. The best  results for each language within each block are highlighted in bold.", "labels": [], "entities": [{"text": "LAS", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9941725134849548}, {"text": "CoNLL 2017 shared task setup", "start_pos": 64, "end_pos": 92, "type": "DATASET", "confidence": 0.9300564527511597}]}, {"text": " Table 6: CoNLL 2017 test set results (LAS, %) on the most projective languages (sorted by projective  ratio; ja (Japanese) is fully projective).", "labels": [], "entities": [{"text": "CoNLL 2017 test set", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.9347790777683258}, {"text": "LAS", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9977250695228577}]}]}