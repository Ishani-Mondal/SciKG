{"title": [{"text": "Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings", "labels": [], "entities": [{"text": "Morphosyntactic Tagging", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7572803199291229}]}], "abstractContent": [{"text": "The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy (Zeman et al., 2017).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7500193119049072}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.8703905940055847}]}, {"text": "One characteristic common among these models is the presence of rich initial word encodings.", "labels": [], "entities": []}, {"text": "These encod-ings typically are composed of a recurrent character-based representation with learned and pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts.", "labels": [], "entities": []}, {"text": "In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations.", "labels": [], "entities": []}, {"text": "In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states.", "labels": [], "entities": []}, {"text": "We present results on part-of-speech and morphological tagging with state-of-the-art performance on a number of languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphosyntactic tagging accuracy has seen dramatic improvements through the adoption of recurrent neural networks-specifically BiLSTMs () to create sentence-level context sensitive encodings of words.", "labels": [], "entities": [{"text": "Morphosyntactic tagging", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8188755810260773}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8030408620834351}]}, {"text": "A successful recipe is to first create an initial context insensitive word representation, which usually has three main parts: 1) A dynamically trained word embedding; 2) a fixed pre-trained word-embedding, induced from a large corpus; and 3) a sub-word character model, which itself is usually the final state of a recurrent model that ingests one character at a time.", "labels": [], "entities": []}, {"text": "Such word/sub-word models originated with.", "labels": [], "entities": []}, {"text": "Recently, used precisely such a context insensitive word representation as input to a BiLSTM in order to obtain context sensitive word encodings used to predict partof-speech tags.", "labels": [], "entities": []}, {"text": "The Dozat et al. model had the highest accuracy of all participating systems in the CoNLL 2017 shared task (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9993081092834473}, {"text": "CoNLL 2017 shared task", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.8120104074478149}]}, {"text": "In such a model, sub-word character-based representations only interact indirectly via subsequent recurrent layers.", "labels": [], "entities": []}, {"text": "For example, consider the sentence I had shingles, which is a painful disease.", "labels": [], "entities": []}, {"text": "Context insensitive character and word representations may have learned that for unknown or infrequent words like 'shingles', 's' and more so 'es' is a common way to end a plural noun.", "labels": [], "entities": []}, {"text": "It is up to the subsequent BiLSTM layer to override this once it sees the singular verb is to the right.", "labels": [], "entities": []}, {"text": "Note that this differs from traditional linear models where word and sub-word representations are directly concatenated with similar features in the surrounding context (.", "labels": [], "entities": []}, {"text": "In this paper we aim to investigate to what extent having initial sub-word and word context insensitive representations affects performance.", "labels": [], "entities": []}, {"text": "We propose a novel model where we learn context sensitive initial character and word representations through two separate sentence-level recurrent models.", "labels": [], "entities": []}, {"text": "These are then combined via a metaBiLSTM model that builds a unified representation of each word that is then used for syntactic tagging.", "labels": [], "entities": [{"text": "syntactic tagging", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.6729994714260101}]}, {"text": "Critically, while each of these three models-character, word and meta-are trained synchronously, they are ultimately separate models using different network configurations, training hyperparameters and loss functions.", "labels": [], "entities": []}, {"text": "Empirically, we found this optimal as it allowed control over the fact that each representation has a different learning capacity.", "labels": [], "entities": []}, {"text": "We tested the system on the 2017 CoNLL shared task data sets and gain improvements compared to the top performing systems for the majority of languages for part-of-speech and morphological tagging.", "labels": [], "entities": [{"text": "CoNLL shared task data sets", "start_pos": 33, "end_pos": 60, "type": "DATASET", "confidence": 0.8519902706146241}, {"text": "part-of-speech and morphological tagging", "start_pos": 156, "end_pos": 196, "type": "TASK", "confidence": 0.6472651809453964}]}, {"text": "As we will see, a pattern emerged where gains were largest for morphologically rich languages, especially those in the Slavic family group.", "labels": [], "entities": []}, {"text": "We also applied the approach to the benchmark English PTB data, where our model achieved 97.9 using the standard train/dev/test split, which constitutes a relative reduction in error of 12% over the previous best system.", "labels": [], "entities": [{"text": "English PTB data", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.8425024747848511}, {"text": "error", "start_pos": 177, "end_pos": 182, "type": "METRIC", "confidence": 0.9203053116798401}]}], "datasetContent": [{"text": "In this section, we present the experimental setup and the selected hyperparameter for the main experiments where we use the CoNLL Shared Task 2017 treebanks and compare with the best systems of the shared task.", "labels": [], "entities": [{"text": "CoNLL Shared Task 2017 treebanks", "start_pos": 125, "end_pos": 157, "type": "DATASET", "confidence": 0.9148281812667847}]}, {"text": "For our main results, we selected one network configuration and set of the hyperparameters.", "labels": [], "entities": []}, {"text": "These settings are not optimal for all languages.", "labels": [], "entities": []}, {"text": "However, since hyperparameter exploration is computationally demanding due to the number of languages we optimized these hyperparameters on initial development data experiments over a few languages.", "labels": [], "entities": [{"text": "hyperparameter exploration", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8139117360115051}]}, {"text": "shows an overview of the architecture, hyperparameters and the initialization settings of the network.", "labels": [], "entities": []}, {"text": "The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training.", "labels": [], "entities": []}, {"text": "The dropout used on the embeddings is achieved by a single dropout mask and we use dropout on the input and the states of the LSTM.", "labels": [], "entities": []}, {"text": "As is standard, model selection was done measuring development accuracy/F1 score after each epoch and taking the model with maximum value on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.847711443901062}, {"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.988302081823349}]}], "tableCaptions": [{"text": " Table 1: Selected hyperparameters and initializa- tion of parameters for our models. Chr, Wrd, and  Mt are used to indicate the character, word, and  meta models respectively. The Gaussian distribu- tion is used with a mean of 0 and variance of 1 to  generate the random values.", "labels": [], "entities": []}, {"text": " Table 2: Results for XPOS tags. The first column  shows the language acronym, the column named  DQM shows the results of Dozat et al. (2017). Our  system outperforms Dozat et al. (2017) on 32 out  of 54 treebanks and Dozat et al. outperforms our  model on 10 of 54 treebanks, with 13 ties. RRIE  is the relative reduction in error. We excluded ties  in the calculation of macro-avg since these tree- banks do not contain meaningful xpos tags.", "labels": [], "entities": [{"text": "RRIE", "start_pos": 291, "end_pos": 295, "type": "METRIC", "confidence": 0.9987810254096985}, {"text": "error", "start_pos": 326, "end_pos": 331, "type": "METRIC", "confidence": 0.9702736139297485}]}, {"text": " Table 3: Results on WSJ test set.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9342645804087321}]}, {"text": " Table 4: Results for morphological features. The  column CoNLL Winner shows the top system of  the ST 17, the DQM Reimpl. shows our reimple- mentation of", "labels": [], "entities": [{"text": "CoNLL Winner", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.6050440073013306}, {"text": "DQM Reimpl", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.8542197644710541}]}, {"text": " Table 5: Comparison of optimization methods:  Separate optimization of the word, character and  meta model is more accurate on average than full  back-propagation using a single loss function.The  results are statistically significant with two-tailed  paired t-test for xpos with p<0.001 and for mor- phology with p <0.0001.", "labels": [], "entities": []}, {"text": " Table 6: F1 score for selected languages on sen- tence vs. word level character models for the pre- diction of morphology using late integration.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9765107333660126}]}, {"text": " Table 7: F1 score for the character, word and  joint models. The standard deviation of 10 ran- dom restarts of each model is show in the last three  columns. The differences in means are all statisti- cally significant at p < 0.001 (paired t-test).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9783911108970642}]}, {"text": " Table 8: F1 score of char models and their per- formance on the dev. set for selected languages  with different gather strategies, concatenate to g i  (Equation 1). DQM shows results for our reimple- mentation of Dozat et al. (2017) (cf.  \u00a73.2), where  we feed in only the characters. The final column  shows the number of xpos tags in the training set.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9994093179702759}, {"text": "DQM", "start_pos": 166, "end_pos": 169, "type": "DATASET", "confidence": 0.9251790642738342}]}]}