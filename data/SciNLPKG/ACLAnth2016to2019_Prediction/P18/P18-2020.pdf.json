{"title": [{"text": "A Named Entity Recognition Shootout for German", "labels": [], "entities": [{"text": "Named Entity Recognition Shootout", "start_pos": 2, "end_pos": 35, "type": "TASK", "confidence": 0.62717554718256}]}], "abstractContent": [{"text": "We ask how to practically build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts, i.e., a big-data and a small-data scenario.", "labels": [], "entities": [{"text": "German named entity recognition (NER)", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.6891541182994843}]}, {"text": "The two best-performing model families are pitted against each other (linear-chain CRFs and BiLSTM) to observe the trade-off between expressiveness and data requirements.", "labels": [], "entities": []}, {"text": "BiL-STM outperforms the CRF when large datasets are available and performs inferior for the smallest dataset.", "labels": [], "entities": [{"text": "BiL-STM", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8298935294151306}]}, {"text": "BiLSTMs profit substantially from transfer learning, which enables them to be trained on multiple corpora, resulting in anew state-of-the-art model for German NER on two contemporary German corpora (CoNLL 2003 and GermEval 2014) and two historic corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition and classification (NER) is a central component in many natural language processing pipelines.", "labels": [], "entities": [{"text": "Named entity recognition and classification (NER)", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8272310271859169}]}, {"text": "High-quality NER is crucial for applications like information extraction, question answering, or entity linking.", "labels": [], "entities": [{"text": "NER", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9761229157447815}, {"text": "information extraction", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.8294951617717743}, {"text": "question answering", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.910581648349762}, {"text": "entity linking", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.8148740828037262}]}, {"text": "Since the goal of NER is to recognize instances of named entities in running text, it is established practice to treat NER as a \"word-by-word sequence labeling task\".", "labels": [], "entities": [{"text": "word-by-word sequence labeling task", "start_pos": 129, "end_pos": 164, "type": "TASK", "confidence": 0.6949792504310608}]}, {"text": "There are two families of sequence models that constitute promising candidates.", "labels": [], "entities": []}, {"text": "On the one hand, linearchain CRFs, which form the basis for many widely used systems (e.g.,, profit from hand-crafted features and can easily incorporate language-and domainspecific knowledge from dictionaries or gazetteers.", "labels": [], "entities": []}, {"text": "On the other hand, bidirectional LSTMSs (BiLSTMs, e.g.,) identify informative features directly from the data, presented as word and/or character embeddings (e.g.,).", "labels": [], "entities": []}, {"text": "When developing NER tools for new types of text, one requirement is the availability of different resources to inform features and/or embeddings.", "labels": [], "entities": []}, {"text": "Another one is the amount of training data: linearchain CRFs require only moderate amounts of training data compared to BiLSTM.", "labels": [], "entities": []}, {"text": "To perform representation learning, BiLSTMs require considerably annotated data to learn proper representations (see, e.g., the impact of training size by.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.9397109150886536}]}, {"text": "This consideration becomes particularly pressing when moving to \"small-data\" settings such as low-resource languages, specific domains, or historical corpora.", "labels": [], "entities": []}, {"text": "Thus, it is an open question, whether it is generally a better idea to choose different model families for different settings, or whether one model family can be optimized to perform well across settings.", "labels": [], "entities": []}, {"text": "This paper investigates this question empirically on a set of German corpora including two large, contemporary corpora and two small historical corpora.", "labels": [], "entities": []}, {"text": "We pit linear-chain CRF-and BiLSTM-based systems against each other and compare to state-ofthe-art models, performing three experiments.", "labels": [], "entities": []}, {"text": "Due to these experiments, we get the following results: (a), the BiLSTM system indeed performs best on contemporary corpora, both within and across domains; (b), the BiLSTM system performs worse than the CRF systems for the smallest historical corpus due to lack of data; (c), by applying transfer learning to adduce more training data, the RNN outperform CRFs substantially for all corpora.", "labels": [], "entities": []}, {"text": "The final BiLSTM models form anew state of the art for German NER and are freely available.", "labels": [], "entities": [{"text": "German NER", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.8677886426448822}]}], "datasetContent": [{"text": "For the evaluation, we use two established datasets for NER on contemporary German and two datasets for historical German.", "labels": [], "entities": [{"text": "NER", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.6980210542678833}]}, {"text": "The first large-scale German NER dataset was published as part of the CoNLL 2003 shared task.", "labels": [], "entities": [{"text": "German NER dataset", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.6954874595006307}, {"text": "CoNLL 2003 shared task", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.8289656341075897}]}, {"text": "It consists of about 220k tokens (for training) of annotated newspaper documents.", "labels": [], "entities": []}, {"text": "The tagset handles locations (LOC), organizations (ORG), persons (PER) and the remaining entities as miscellaneous (MISC).", "labels": [], "entities": []}, {"text": "The second dataset is the GermEval 2014 shared task dataset (GermEval,), consisting of some 450k tokens (for training) of Wikipedia articles.", "labels": [], "entities": [{"text": "GermEval 2014 shared task dataset (GermEval", "start_pos": 26, "end_pos": 69, "type": "DATASET", "confidence": 0.809997192450932}]}, {"text": "This dataset has two levels of annotations: outer and inner span named entities.", "labels": [], "entities": []}, {"text": "For example, the term Chicago Bulls is tagged as organization in the outer span annotation.", "labels": [], "entities": []}, {"text": "The nested term Chicago is annotated as location in the inner span annotation.", "labels": [], "entities": []}, {"text": "However, there are only few inner span annotations.", "labels": [], "entities": []}, {"text": "In addition to the standard tagsets also used in the CoNLL dataset, fine grained versions of these entities are marked with suffixes: -deriv marks derivations of the named entities (e.g. German actor -German is a derived location) and -part marks compounds including a named entity (e.g. in the word Rhineshore the compound Rhine is location).", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9706307947635651}, {"text": "Rhineshore", "start_pos": 300, "end_pos": 310, "type": "DATASET", "confidence": 0.929053008556366}]}, {"text": "To compare to previous state-of-the-art methods, we show results on the official metric (a combination of the outer and inner spans) in Section 4.", "labels": [], "entities": []}, {"text": "As there are only few inner span annotations, we additionally report results based on the outer spans.", "labels": [], "entities": []}, {"text": "To be more conform with the tagsets of the CoNLL task, we focus on outer spans and remove the fine-grained tags in the follow-up experiments (see Section 5 and 6).", "labels": [], "entities": []}, {"text": "We further consider two datasets based on historical texts: Evaluation on GermEval data, using the official metric (metric 1) of the GermEval 2014 task that combines inner and outer chunks.", "labels": [], "entities": [{"text": "GermEval data", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.8548335134983063}]}, {"text": "1926. Our second corpus is a collection of Austrian newspaper texts from the Austrian National Library (ONB), covering some 35k tokens between 1710 and 1873.", "labels": [], "entities": [{"text": "Austrian newspaper texts from the Austrian National Library (ONB)", "start_pos": 43, "end_pos": 108, "type": "DATASET", "confidence": 0.755763666196303}]}, {"text": "These corpora give rise to a number of challenges: they are considerably smaller than the contemporary corpora from above, contain a different language variety (19th century Austrian German), and include a high rate of OCR errors since they were originally printed in Gothic typeface.", "labels": [], "entities": [{"text": "OCR errors", "start_pos": 219, "end_pos": 229, "type": "METRIC", "confidence": 0.7512234747409821}]}, {"text": "We use 80% of the data for training and each 10% for development and testing.", "labels": [], "entities": []}, {"text": "In our first experiment, we compare the NER performances on the two contemporary, large datasets.", "labels": [], "entities": [{"text": "NER", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9501853585243225}]}, {"text": "For BiLSTM, we experiment with two options for word embeddings.", "labels": [], "entities": []}, {"text": "First, we use pre-trained embeddings computed on Wikipedia with 300 dimensions and standard parameters (WikiEmb) 8 , which are presumably more appropriate for contemporary texts.", "labels": [], "entities": []}, {"text": "Second, we compute embeddings with the same parameters from 1.5 billion tokens of historic German texts from Europeana (EuroEmb).", "labels": [], "entities": [{"text": "German texts from Europeana (EuroEmb)", "start_pos": 91, "end_pos": 128, "type": "DATASET", "confidence": 0.6226232988493783}]}, {"text": "These embeddings should be more appropriate for historical texts but may suffer from sparsity.", "labels": [], "entities": []}, {"text": "shows results on GermEval using the official metric (metric 1) for the best performing systems.", "labels": [], "entities": [{"text": "GermEval", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.5087595582008362}]}, {"text": "This measure considers both outer and inner span annotations.", "labels": [], "entities": []}, {"text": "Within the challenge, the ExB (H\u00e4nig et al., 2015) ensemble classifier achieved the best result with an F1 score of 76.38, followed by the RNN-based method from UKP () with 75.09.", "labels": [], "entities": [{"text": "ExB (H\u00e4nig et al., 2015) ensemble classifier", "start_pos": 26, "end_pos": 70, "type": "DATASET", "confidence": 0.9148834764957428}, {"text": "F1 score", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9806059896945953}, {"text": "UKP", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.9542242288589478}]}, {"text": "GermaNER achieves high precision, but cannot compete in terms of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9994825124740601}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9984846711158752}]}, {"text": "Our BiLSTM with Wikipedia word embeddings, scores highest (79.99) and outperforms the shared We cleaned the corpora by correcting named entity labels and tokenization.", "labels": [], "entities": []}, {"text": "We will make these versions available.", "labels": [], "entities": []}, {"text": "8 https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors.", "labels": [], "entities": []}, {"text": "md   task winner ExB significantly, based on a bootstrap resampling test.", "labels": [], "entities": [{"text": "ExB", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.428035706281662}]}, {"text": "Using Europeana embeddings, the performance drops to an F1 score of 73.03 -due to the difference in vocabulary.", "labels": [], "entities": [{"text": "Europeana", "start_pos": 6, "end_pos": 15, "type": "DATASET", "confidence": 0.9318442344665527}, {"text": "F1 score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9875534474849701}]}, {"text": "As the number of inner span annotations is marginal and hard to detect, we additionally present scores considering only outer span annotations in.", "labels": [], "entities": []}, {"text": "Whereas the scores are slightly higher, we observe the same trend as from the previous results shown in.", "labels": [], "entities": []}, {"text": "On the CoNLL dataset (see) GermaNER outperforms the currently best-performing RNNbased system ().", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9617857038974762}]}, {"text": "The BiLSTM again yields the significantly best performance, matching its high precision while substantially improving recall.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.8361374139785767}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9987695813179016}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9990423321723938}]}, {"text": "Again, lower F1 scores are achieved using the Europeana embeddings.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9826704561710358}, {"text": "Europeana embeddings", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.9866639673709869}]}, {"text": "In sum, we find that BiLSTM models can outperform CRF models when there is sufficient training data to profit from distributed representations.", "labels": [], "entities": []}, {"text": "A potential downside of BiLSTMs is that learned models maybe more text type specific, due to the high capacity of the models.", "labels": [], "entities": []}, {"text": "Experiment 2 evaluates how well the models do when trained on one corpus and tested on another one, including historical corpora.", "labels": [], "entities": []}, {"text": "To level the playing field, we reduce the detailed annotation of GermEval to the standard five-category set (PER, LOC, ORG, MISC, OTH).", "labels": [], "entities": [{"text": "PER", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9797484278678894}, {"text": "ORG", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.8708010911941528}, {"text": "MISC", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.8844350576400757}, {"text": "OTH", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.7582342028617859}]}, {"text": "Results for these experiments are presented in: Evaluation (F1) for two CRF-based methods and BiLSTM trained and tested on different corpora..", "labels": [], "entities": [{"text": "Evaluation (F1)", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.8203032314777374}, {"text": "BiLSTM", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.974775493144989}]}, {"text": "Unsurprisingly, the best results are gained when testing on the same dataset as the training has been performed.", "labels": [], "entities": []}, {"text": "GermaNER consistently outperforms StanfordNER again, highlighting the benefits of knowledge engineering when using CRFs.", "labels": [], "entities": [{"text": "StanfordNER", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9177442193031311}]}, {"text": "Interestingly, these benefits also extend to the historical datasets for which the CRF features were presumably not optimized: overall F1-scores are only a few points lower than for the contemporary corpora, and the CRFs significantly outperform the BiLSTM models on ONB and performs comparable on the larger LFT dataset.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9972303509712219}, {"text": "ONB", "start_pos": 267, "end_pos": 270, "type": "DATASET", "confidence": 0.9572486281394958}, {"text": "LFT dataset", "start_pos": 309, "end_pos": 320, "type": "DATASET", "confidence": 0.7744716107845306}]}, {"text": "The type of embeddings used by BiLSTM plays a minor role for the historical corpora (for contemporary corpora, Wikipedia is clearly better).", "labels": [], "entities": []}, {"text": "In sum, we conclude that BiLSTM models run into trouble when faced with very small training datasets, while CRF-based methods are more robust (Cotterell and Duh, 2017).", "labels": [], "entities": []}, {"text": "If the problems of BiLSTM from the last section are in fact due to lack of data, we might be able to obtain an improvement by combining them.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.6353514194488525}]}, {"text": "A simple way of doing this is transfer learning (: we simply start training on one corpus and at some point switch to another corpus.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.9374527931213379}]}, {"text": "In our scenario, we start by training on large contemporary \"source\" corpora until convergence and then train additional 15 epochs on the \"target\" corpus from the domain on which we evaluate.", "labels": [], "entities": []}, {"text": "The results in show significant improvements for the CoNLL dataset but performance drops for GermEval.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.939472883939743}, {"text": "GermEval", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.774604856967926}]}, {"text": "Combining contemporary sources with historic target corpora yields to consistent benefits.", "labels": [], "entities": []}, {"text": "Performance on LFT increases from 69.62 to 74.33 and on ONB from 73.31 to 78.56.", "labels": [], "entities": [{"text": "LFT", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.7402192950248718}, {"text": "ONB", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.5168346166610718}]}, {"text": "Cross-domain classification scores are also improved consistently.", "labels": [], "entities": [{"text": "Cross-domain classification", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7066506445407867}]}, {"text": "The GermEval corpus is more appropriate as a source corpus, presumably because it is both larger and drawn from encyclopaedic text, more varied than newswire.", "labels": [], "entities": [{"text": "GermEval corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8883746564388275}]}, {"text": "We conclude that transfer learning is beneficial for BiLSTMs, especially when training data for the target domain is scarce.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9461967647075653}]}, {"text": "We applied the same procedure to the CRFs, but did not obtain improvements for the \"target\" data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation on GermEval data, using the  official metric (metric 1) of the GermEval 2014  task that combines inner and outer chunks.", "labels": [], "entities": [{"text": "GermEval data", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.7877069413661957}, {"text": "GermEval 2014  task", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.732365886370341}]}, {"text": " Table 2: Evaluation on the test set of GermEval  2014 using the Outer Chunks evaluation schema.", "labels": [], "entities": [{"text": "GermEval  2014", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.7947559654712677}, {"text": "Outer Chunks evaluation schema", "start_pos": 65, "end_pos": 95, "type": "DATASET", "confidence": 0.8500888794660568}]}, {"text": " Table 3: Evaluation on the test set of the German  CoNLL 2003 dataset.", "labels": [], "entities": [{"text": "German  CoNLL 2003 dataset", "start_pos": 44, "end_pos": 70, "type": "DATASET", "confidence": 0.9155716300010681}]}, {"text": " Table 4: Evaluation (F1) for two CRF-based meth- ods and BiLSTM trained and tested on different  corpora.", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9890524744987488}, {"text": "F1)", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9247150719165802}, {"text": "BiLSTM", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.8625342845916748}]}, {"text": " Table 5: Results for different test sets when using transfer learning.  \u2020 marks results statistically significantly  better than the ones reported in", "labels": [], "entities": []}]}