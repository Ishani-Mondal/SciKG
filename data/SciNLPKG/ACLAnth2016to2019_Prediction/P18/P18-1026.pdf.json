{"title": [{"text": "Graph-to-Sequence Learning using Gated Graph Neural Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "Many NLP applications can be framed as a graph-to-sequence learning problem.", "labels": [], "entities": []}, {"text": "Previous work proposing neural architec-tures on this setting obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance.", "labels": [], "entities": []}, {"text": "In this work, we propose anew model that encodes the full structural information contained in the graph.", "labels": [], "entities": []}, {"text": "Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations , while tackling the parameter explosion problem present in previous work.", "labels": [], "entities": []}, {"text": "Experimental results show that our model outperforms strong baselines in generation from AMR graphs and syntax-based neu-ral machine translation.", "labels": [], "entities": [{"text": "syntax-based neu-ral machine translation", "start_pos": 104, "end_pos": 144, "type": "TASK", "confidence": 0.5708054602146149}]}], "introductionContent": [{"text": "Graph structures are ubiquitous in representations of natural language.", "labels": [], "entities": []}, {"text": "In particular, many wholesentence semantic frameworks employ directed acyclic graphs as the underlying formalism, while most tree-based syntactic representations can also be seen as graphs.", "labels": [], "entities": []}, {"text": "A range of NLP applications can be framed as the process of transducing a graph structure into a sequence.", "labels": [], "entities": []}, {"text": "For instance, language generation may involve realising a semantic graph into a surface form and syntactic machine translation involves transforming a tree-annotated source sentence to its translation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.731684073805809}, {"text": "syntactic machine translation", "start_pos": 97, "end_pos": 126, "type": "TASK", "confidence": 0.6331987281640371}]}, {"text": "Previous work in this setting rely on grammarbased approaches such as tree transducers) and hyperedge replacement grammars (.", "labels": [], "entities": []}, {"text": "A key limitation of these approaches is that alignments between graph nodes and surface tokens are required.", "labels": [], "entities": []}, {"text": "These alignments are usually automatically generated so they can propagate errors when building the grammar.", "labels": [], "entities": []}, {"text": "More recent approaches transform the graph into a linearised form and use off-the-shelf methods such as phrase-based machine translation ( or neural sequenceto-sequence (henceforth, s2s) models (.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 104, "end_pos": 136, "type": "TASK", "confidence": 0.6164715190728506}]}, {"text": "Such approaches ignore the full graph structure, discarding key information.", "labels": [], "entities": []}, {"text": "In this work we propose a model for graph-tosequence (henceforth, g2s) learning that leverages recent advances in neural encoder-decoder architectures.", "labels": [], "entities": []}, {"text": "Specifically, we employ an encoder based on Gated Graph Neural Networks (, which can incorporate the full graph structure without loss of information.", "labels": [], "entities": []}, {"text": "Such networks represent edge information as label-wise parameters, which can be problematic even for small sized label vocabularies (in the order of hundreds).", "labels": [], "entities": []}, {"text": "To address this limitation, we also introduce a graph transformation that changes edges to additional nodes, solving the parameter explosion problem.", "labels": [], "entities": []}, {"text": "This also ensures that edges have graphspecific hidden vectors, which gives more information to the attention and decoding modules in the network.", "labels": [], "entities": []}, {"text": "We benchmark our model in two graph-tosequence problems, generation from Abstract Meaning Representations (AMRs) and Neural Machine Translation (NMT) with source dependency information.", "labels": [], "entities": [{"text": "generation from Abstract Meaning Representations (AMRs)", "start_pos": 57, "end_pos": 112, "type": "TASK", "confidence": 0.7102898582816124}, {"text": "Neural Machine Translation (NMT)", "start_pos": 117, "end_pos": 149, "type": "TASK", "confidence": 0.8114894330501556}]}, {"text": "Our approach outperforms strong s2s baselines in both tasks without relying on standard RNN encoders, in contrast with previous work.", "labels": [], "entities": []}, {"text": "In particular, for NMT we show that we avoid the need for RNNs by adding sequential edges between contiguous words in the dependency tree.", "labels": [], "entities": []}, {"text": "This illustrates the generality of our Figure 1: Left: the AMR graph representing the sentence \"The boy wants the girl to believe him.\".", "labels": [], "entities": [{"text": "AMR", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.48917409777641296}]}, {"text": "Right: Our proposed architecture using the same AMR graph as input and the surface form as output.", "labels": [], "entities": [{"text": "AMR graph", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.7473180890083313}]}, {"text": "The first layer is a concatenation of node and positional embeddings, using distance from the root node as the position.", "labels": [], "entities": []}, {"text": "The GGNN encoder updates the embeddings using edge-wise parameters, represented by different colors (in this example, ARG0 and ARG1).", "labels": [], "entities": [{"text": "ARG0", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.9140945076942444}, {"text": "ARG1", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.8601704239845276}]}, {"text": "The encoder also add corresponding reverse edges (dotted arrows) and self edges for each node (dashed arrows).", "labels": [], "entities": []}, {"text": "All parameters are shared between layers.", "labels": [], "entities": []}, {"text": "Attention and decoder components are similar to standard s2s models.", "labels": [], "entities": []}, {"text": "This is a pictorial representation: in our experiments the graphs are transformed before being used as inputs (see \u00a73).", "labels": [], "entities": []}, {"text": "approach: linguistic biases can be added to the inputs by simple graph transformations, without the need for changes to the model architecture.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data and preprocessing We employ the same data and settings from, which use the News Commentary V11 corpora from the WMT16 translation task.", "labels": [], "entities": [{"text": "News Commentary V11 corpora", "start_pos": 80, "end_pos": 107, "type": "DATASET", "confidence": 0.8906116634607315}, {"text": "WMT16 translation task", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7676668564478556}]}, {"text": "English text is tokenised and parsed using SyntaxNet 7 while German and Czech texts are tokenised and split into subwords using byte-pair encodings (Sennrich et al., 2016, BPE) (8000 merge operations).", "labels": [], "entities": [{"text": "BPE", "start_pos": 172, "end_pos": 175, "type": "DATASET", "confidence": 0.6722893118858337}]}, {"text": "We refer to for further information on the preprocessing steps.", "labels": [], "entities": []}, {"text": "Labelled dependency trees in the source side are transformed into Levi graphs as a preprocessing step.", "labels": [], "entities": []}, {"text": "However, unlike AMR generation, in NMT the inputs are originally surface forms that contain important sequential information.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.9817638993263245}]}, {"text": "This information is lost when treating the input as dependency trees, which might explain why obtain the best performance when using an initial RNN layer in their encoder.", "labels": [], "entities": []}, {"text": "To investigate this phenomenon, we also perform experiments adding sequential connections to each word in the dependency tree, corresponding to their order in the original surface form (henceforth, g2s+).", "labels": [], "entities": []}, {"text": "These connections are represented as edges with specific left and right labels, which are added after the Levi graph transformation.", "labels": [], "entities": []}, {"text": "shows an example of an input graph for g2s+, with the additional sequential edges connecting the words (reverse and self edges are omitted for simplicity).", "labels": [], "entities": []}, {"text": "Models Our s2s and g2s models are almost the same as in the AMR generation experiments ( \u00a74.1).", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.6142378449440002}]}, {"text": "The only exception is the GGNN encoder dimensionality, where we use 512 for the experiments with dependency trees only and 448 when the inputs have additional sequential connections.", "labels": [], "entities": []}, {"text": "As in the AMR generation setting, we do this to ensure model capacity are comparable in the number of parameters.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7684877216815948}]}, {"text": "Another key difference is that the s2s baselines do not use dependency trees: they are trained on the sentences only.", "labels": [], "entities": []}, {"text": "In addition to neural models, we also report results for Phrase-Based Statistical MT (PB-SMT), using Moses (.", "labels": [], "entities": [{"text": "Phrase-Based Statistical MT", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.5413957238197327}]}, {"text": "The PB-SMT models are trained using the same data conditions as s2s (no dependency trees) and use the standard setup in Moses, except for the language model, where we use a 5-gram LM trained on the target side of the respective parallel corpus.", "labels": [], "entities": []}, {"text": "8 Evaluation We report results in terms of BLEU and CHRF++, using case-sensitive versions of both metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9992945194244385}, {"text": "CHRF", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9498143196105957}]}, {"text": "Other settings are kept the same as in the AMR generation experiments ( \u00a74.1).", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.8257037699222565}]}, {"text": "For PB-SMT, we also report the median result of 5 runs, obtained by tuning the model using MERT) 5 times.", "labels": [], "entities": [{"text": "MERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9789962768554688}]}, {"text": "Note that target data is segmented using BPE, which is not the usual setting for PB-SMT.", "labels": [], "entities": [{"text": "BPE", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9141600728034973}]}, {"text": "We decided to keep the segmentation to ensure data conditions are the same.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9729738831520081}]}, {"text": "There is a deeper issue at stake . shows the results on the respective test set for both language pairs.", "labels": [], "entities": []}, {"text": "The g2s models, which do not account for sequential information, lag behind our baselines.", "labels": [], "entities": []}, {"text": "This is inline with the findings of, who found that having a BiRNN layer was key to obtain the best results.", "labels": [], "entities": []}, {"text": "However, the g2s+ models outperform the baselines in terms of BLEU scores under the same parameter budget, in both single model and ensemble scenarios.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9989446997642517}]}, {"text": "This result show that it is possible to incorporate sequential biases in our model without relying on RNNs or any other modification in the architecture.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for AMR generation on the test  set. All score differences between our models and  the corresponding baselines are significantly dif- ferent (p<0.05). \"(-s)\" means input without scope  marking. KIYCZ17, PKH16, SPZWG17 and", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.9904233813285828}, {"text": "KIYCZ17", "start_pos": 212, "end_pos": 219, "type": "DATASET", "confidence": 0.8860556483268738}, {"text": "PKH16", "start_pos": 221, "end_pos": 226, "type": "DATASET", "confidence": 0.914171040058136}]}, {"text": " Table 2: Results for syntax-based NMT on the test  sets. All score differences between our models and  the corresponding baselines are significantly dif- ferent (p<0.05), including the negative CHRF++  result for En-Cs.", "labels": [], "entities": []}]}