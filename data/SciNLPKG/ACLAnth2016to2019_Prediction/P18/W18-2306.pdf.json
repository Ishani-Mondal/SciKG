{"title": [{"text": "Ontology Alignment in the Biomedical Domain Using Entity Definitions and Context", "labels": [], "entities": [{"text": "Ontology Alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9147457182407379}]}], "abstractContent": [{"text": "Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies.", "labels": [], "entities": [{"text": "Ontology alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8490515947341919}]}, {"text": "Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies.", "labels": [], "entities": []}, {"text": "We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment.", "labels": [], "entities": [{"text": "ontology alignment", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.8587511479854584}]}, {"text": "We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9994794726371765}]}], "introductionContent": [{"text": "Ontologies are used to ground lexical items in various NLP tasks including entity linking, question answering, semantic parsing and information retrieval.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.7885724902153015}, {"text": "question answering", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8812377452850342}, {"text": "semantic parsing", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.7179709821939468}, {"text": "information retrieval", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.7969089150428772}]}, {"text": "In biomedicine, an abundance of ontologies (e.g., MeSH, Gene Ontology) has been developed for different purposes.", "labels": [], "entities": []}, {"text": "Each ontology describes a large number of concepts in healthcare, public health or biology, enabling the use of ontology-based NLP methods in biomedical applications.", "labels": [], "entities": []}, {"text": "However, since these ontologies are typically curated independently by different groups, many important concepts are represented inconsistently across ontologies (e.g., \"Myoclonic Epilepsies, Progressive\" in MeSH is a broader concept that includes \"Dentatorubral-pallidoluysian atrophy\" from OMIM).", "labels": [], "entities": []}, {"text": "This poses a challenge for bioNLP applications where multiple ontologies are needed for grounding, but each concept must be represented by only one entity.", "labels": [], "entities": []}, {"text": "For instance, in www.semanticscholar.org, scientific publications related to carpal tunnel syndrome are linked to one of multiple entities derived from UMLS terminologies representing the same concept, 2 making it hard to find all relevant papers on this topic.", "labels": [], "entities": [{"text": "carpal tunnel syndrome", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7163437406222025}]}, {"text": "To address this challenge, we need to automatically map semantically equivalent entities from one ontology to another.", "labels": [], "entities": []}, {"text": "This task is referred to as ontology alignment or ontology matching.", "labels": [], "entities": [{"text": "ontology alignment", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7652124166488647}, {"text": "ontology matching", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.6637250036001205}]}, {"text": "Several methods have been applied to ontology alignment, including rule-based and statistical matchers.", "labels": [], "entities": [{"text": "ontology alignment", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.9075613915920258}]}, {"text": "Existing matchers rely on entity features such as names, synonyms, as well as relationships to other entities (.", "labels": [], "entities": []}, {"text": "However, it is unclear how to leverage the natural language text associated with entities to improve predictions.", "labels": [], "entities": []}, {"text": "We address this limitation by incorporating two types of natural language information (definitions and textual contexts) in a supervised learning framework for ontology alignment.", "labels": [], "entities": [{"text": "ontology alignment", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.8961285650730133}]}, {"text": "Since the definition and textual contexts of an entity often provide complementary information about the entity's meaning, we hypothesize that incorporating them will improve model predictions.", "labels": [], "entities": []}, {"text": "We also discuss how to automatically derive labeled data for training the model by leveraging existing resources.", "labels": [], "entities": []}, {"text": "In particular, we make the following contributions: \u2022 We propose a novel neural architecture for ontology alignment and show how to effectively: OntoEmma consists of three modules: a) candidate selection (see \u00a72.2 for details), b) feature generation (see \u00a72.2 for details), and c) prediction (see \u00a72.3 for deatils).", "labels": [], "entities": [{"text": "ontology alignment", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8289594352245331}, {"text": "feature generation", "start_pos": 231, "end_pos": 249, "type": "TASK", "confidence": 0.694060891866684}]}, {"text": "OntoEmma accepts two ontologies (a source and a target) as inputs, and outputs a list of alignments between their entities.", "labels": [], "entities": []}, {"text": "When using a neural network, the feature generation and prediction model are combined together in the network.", "labels": [], "entities": []}, {"text": "integrate natural language inputs such as definitions and contexts in this architecture (see \u00a72 for details).", "labels": [], "entities": []}, {"text": "3 \u2022 We use the UMLS Metathesaurus to extract large amounts of labeled data for supervised training of ontology alignment models (see \u00a73.1).", "labels": [], "entities": [{"text": "UMLS Metathesaurus", "start_pos": 15, "end_pos": 33, "type": "DATASET", "confidence": 0.9201539158821106}, {"text": "ontology alignment", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7154604196548462}]}, {"text": "We release our data set to help future research in ontology alignment.", "labels": [], "entities": [{"text": "ontology alignment", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9012477695941925}]}, {"text": "3 \u2022 We use external resources such as Wikipedia and scientific articles to find entity definitions and contexts (see \u00a73.2 for details).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we experiment with several variants of OntoEmma: In the first variant (OntoEmma:NN), we only encode native attributes obtained from the source and target ontologies: canonical name, aliases, and native definitions.", "labels": [], "entities": []}, {"text": "In the second variant (OntoEmma:NN+f), we also add the manually engineered features as described in \u00a72.2.", "labels": [], "entities": []}, {"text": "In the third variant (OntoEmma:NN+f+w), we incorporate external definitions from Wikipedia, as discussed in \u00a73.2.", "labels": [], "entities": []}, {"text": "In the fourth variant (OntoEmma:NN+f+w+c), we also encode the usage contexts we derived from Medline, also discussed in \u00a73.2.", "labels": [], "entities": [{"text": "Medline", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9376219511032104}]}, {"text": "We use the training section of the UMLSderived labeled data to train the model and use the development section to tune the model hyperparameters.", "labels": [], "entities": [{"text": "UMLSderived labeled data", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.9032997091611227}]}, {"text": "For evaluation, we use the test portion of our UMLS-derived data as well as the OAEI largebio subtrack SNOMED-NCI task, the largest task in OAEI largebio.", "labels": [], "entities": [{"text": "UMLS-derived data", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8764238357543945}, {"text": "OAEI largebio subtrack SNOMED-NCI task", "start_pos": 80, "end_pos": 118, "type": "DATASET", "confidence": 0.7542356014251709}, {"text": "OAEI largebio", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.8576540946960449}]}, {"text": "The UMLS test set includes 29,859 positive and negative mappings.", "labels": [], "entities": [{"text": "UMLS test set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9708439310391744}]}, {"text": "The OAEI reference alignments included 17,210 equivalent mappings and 1,623 uncertain mappings between the SNOMED and NCI ontologies.", "labels": [], "entities": [{"text": "OAEI reference alignments", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.920254131158193}, {"text": "SNOMED and NCI ontologies", "start_pos": 107, "end_pos": 132, "type": "DATASET", "confidence": 0.6266800984740257}]}, {"text": "Our main baseline is a logistic regression model (OntoEmma:LR) using the same engineered features described in \u00a72.2.", "labels": [], "entities": []}, {"text": "To illustrate how our proposed method performs compared to previous work on ontology matching, we compare  Implementation and configuration details.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7647230625152588}, {"text": "Implementation", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.5046610832214355}]}, {"text": "We provide an open source, modular, Python implementation of OntoEmma where different candidate selectors, feature generators, and prediction modules can be swapped in and outwith ease.", "labels": [], "entities": []}, {"text": "We implement the neural model using PyTorch and AllenNLP 9 libraries, and implement the logistic regression model using scikit-learn.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.853013813495636}, {"text": "AllenNLP 9", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.8498887419700623}]}, {"text": "Our 100-dimensional pretrained embeddings are learned using the default settings of word2vec based on the Medline corpus.", "labels": [], "entities": [{"text": "Medline corpus", "start_pos": 106, "end_pos": 120, "type": "DATASET", "confidence": 0.9708815515041351}]}, {"text": "The character-level CNN encoder uses 50 filters of size 4 and 5, and outputs a token embedding of size 100 with dropout probability of 0.2.", "labels": [], "entities": []}, {"text": "The LSTMs have output size 100, and have dropout probability of 0.2.", "labels": [], "entities": []}, {"text": "The performance of the models is reported in terms of precision, recall and F1 score on the held-out UMLS test set and the OAEI largebio SNOMED-NCI task in, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9997500777244568}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9997846484184265}, {"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9757691025733948}, {"text": "UMLS test set", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.9565921425819397}, {"text": "OAEI largebio SNOMED-NCI task", "start_pos": 123, "end_pos": 152, "type": "DATASET", "confidence": 0.8917058855295181}]}, {"text": "illustrates how different variants of OntoEmma perform on the held-out UMLS test The performance of the full AML system on the SNOMED-NCI subtask for OAEI 2017 is: precision: 0.90, recall: 0.67, F1: 0.77.", "labels": [], "entities": [{"text": "UMLS test", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.8652758896350861}, {"text": "SNOMED-NCI subtask for OAEI 2017", "start_pos": 127, "end_pos": 159, "type": "DATASET", "confidence": 0.8355810761451721}, {"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9995430707931519}, {"text": "recall", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.9997056126594543}, {"text": "F1", "start_pos": 195, "end_pos": 197, "type": "METRIC", "confidence": 0.9997525811195374}]}, {"text": "9 https://allennlp.org/ set.", "labels": [], "entities": []}, {"text": "We note that the bare-bones neural network model (OntoEmma:NN) does not match the performance of the baseline logistic regression model (OntoEmma:LR), suggesting that the representations learned by the neural network are not sufficient.", "labels": [], "entities": []}, {"text": "Indeed, adding the engineered features to the neural model in (OntoEmma:NN+f) provides substantial improvements, matching the F1 score of the baseline model.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9767335951328278}]}, {"text": "Adding definitions and usage context in (OntoEmma:NN+f+w+c) further improves the F1 score by one absolute point, outperforming the logistic regression model.", "labels": [], "entities": [{"text": "OntoEmma", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.776452362537384}, {"text": "F1 score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9850620925426483}]}, {"text": "While the UMLS-based test set in represents the realistic scenario of aligning new entities in partially-aligned ontologies, we also wanted to evaluate the performance of our method on the more challenging scenario where no labeled data is available in the source and target ontologies.", "labels": [], "entities": [{"text": "UMLS-based test set", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.8496541182200114}]}, {"text": "This is more challenging because the patterns learned from ontologies used in training may not transfer to the test ontologies.", "labels": [], "entities": []}, {"text": "illustrates how our method performs in this scenario using SNOMED-NCI as test ontologies.", "labels": [], "entities": []}, {"text": "For matching of the SNOMED and NCI ontologies, we enriched the entities first using Wikipedia queries.", "labels": [], "entities": []}, {"text": "At test time, we also identified and aligned pairs of entities with exact string matches, using the OntoEmma matcher only for those entities without an exact string match.", "labels": [], "entities": [{"text": "OntoEmma", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.6364096403121948}]}, {"text": "Unsurprisingly, the performance of OntoEmma on unseen ontologies (in) is much lower than its performance on seen ontologies (in).", "labels": [], "entities": []}, {"text": "With unseen ontologies, we gain a large F1 improvement of 4 absolute points by using the fully-featured neural model (OntoEmma:NN+f+w+c) instead of the logistic regression variant (OntoEmma:LR), suggesting that the neural model may transfer better to different domains.", "labels": [], "entities": [{"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9993997812271118}]}, {"text": "We note, however, that the OntoEmma:NN+f+w+c matcher performs slightly worse than the AML entity matcher.", "labels": [], "entities": [{"text": "AML entity matcher", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.6068651080131531}]}, {"text": "This is to be expected since AML incorporates many matchers which we did not implement in our model, e.g., using background knowledge, acronyms, and other features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Entities with definitions and contexts for  each of the training ontologies.", "labels": [], "entities": []}, {"text": " Table 2: Model performance on UMLS test dataset", "labels": [], "entities": [{"text": "UMLS test dataset", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.8011386593182882}]}, {"text": " Table 3: Model performance on OAEI largebio  SNOMED-NCI task  Model  Prec. Recall F1  AML:entity  0.81  0.62 0.70  OntoEmma:LR  0.75  0.56 0.65  OntoEmma:NN+f+w+c 0.80  0.61 0.69", "labels": [], "entities": [{"text": "OAEI largebio  SNOMED-NCI task  Model  Prec. Recall F1  AML", "start_pos": 31, "end_pos": 90, "type": "DATASET", "confidence": 0.8015996962785721}]}]}