{"title": [{"text": "Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7674513459205627}]}], "abstractContent": [{"text": "This paper describes a novel hierarchical attention network for reading comprehension style question answering, which aims to answer questions fora given narrative paragraph.", "labels": [], "entities": [{"text": "reading comprehension style question answering", "start_pos": 64, "end_pos": 110, "type": "TASK", "confidence": 0.57096848487854}]}, {"text": "In the proposed method, attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph.", "labels": [], "entities": []}, {"text": "Specifically, it first encode the question and paragraph with fine-grained language embeddings, to better capture the respective representations at semantic level.", "labels": [], "entities": []}, {"text": "Then it proposes a multi-granularity fusion approach to fully fuse information from both global and attended representations.", "labels": [], "entities": []}, {"text": "Finally, it introduces a hierarchical attention network to focuses on the answer span progressively with multi-level soft-alignment.", "labels": [], "entities": []}, {"text": "Extensive experiments on the large-scale SQuAD and TriviaQA datasets validate the effectiveness of the proposed method.", "labels": [], "entities": [{"text": "TriviaQA datasets", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9199006259441376}]}, {"text": "At the time of writing the paper (Jan. 12th 2018), our model achieves the first position on the SQuAD leader-board for both single and ensemble models.", "labels": [], "entities": [{"text": "SQuAD leader-board", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.7848901450634003}]}, {"text": "We also achieves state-of-the-art results on TriviaQA, AddSent and AddOne-Sent datasets.", "labels": [], "entities": [{"text": "TriviaQA", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9281718134880066}, {"text": "AddOne-Sent datasets", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.8121309280395508}]}], "introductionContent": [{"text": "As a brand new field in question answering community, reading comprehension is one of the key problems in artificial intelligence, which aims to read and comprehend a given text, and then answer questions based on it.", "labels": [], "entities": [{"text": "question answering community", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.8523431221644083}]}, {"text": "This task is challenging which requires a comprehensive understanding of natural languages and the ability to do further inference and reasoning.", "labels": [], "entities": []}, {"text": "Restricted by the limited volume of the annotated dataset, early studies mainly rely on a pipeline of NLP models to complete this task, such as semantic parsing and linguistic annotation ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.7300445735454559}]}, {"text": "Not until the release of large-scale clozestyle dataset, such as Children's Book Test and CNN/Daily Mail (, some preliminary end-to-end deep learning methods have begun to bloom and achieve superior results in reading comprehension task.", "labels": [], "entities": [{"text": "Children's Book Test", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.9076277911663055}, {"text": "CNN/Daily Mail", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.8245928585529327}]}, {"text": "However, these cloze-style datasets still have their limitations, where the goal is to predict the single missing word (often a named entity) in a passage.", "labels": [], "entities": []}, {"text": "It requires less reasoning than previously thought and no need to comprehend the whole passage ( . Therefore, Stanford publish anew large-scale dataset, in which all the question and answers are manually created through crowdsourcing.", "labels": [], "entities": []}, {"text": "Different from cloze-style reading comprehension dataset, SQuAD constrains answers to all possible text spans within the reference passage, which requires more logical reasoning and content understanding.", "labels": [], "entities": []}, {"text": "Benefiting from the availability of SQuAD benchmark dataset, rapid progress has been made these years.", "labels": [], "entities": [{"text": "SQuAD benchmark dataset", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.812022844950358}]}, {"text": "The work ( and) are among the first to investigate into this dataset, where Wang and Jiang propose an end-to-end architecture based on match-LSTM and pointer networks (, and Seo et al. introduce the bi-directional attention flow network which captures the questiondocument context at different levels of granularity ().", "labels": [], "entities": []}, {"text": "Chen et al. devise a simple and effective document reader, by introducing a bilinear match function and a few manual features ().", "labels": [], "entities": []}, {"text": "a gated attention-based recurrent network where self-match attention mechanism is first incorporated ().", "labels": [], "entities": []}, {"text": "In ( and , the multi-turn memory networks are designed to simulate multi-step reasoning in machine reading comprehension.", "labels": [], "entities": []}, {"text": "The idea of our approach derives from the normal human reading pattern.", "labels": [], "entities": []}, {"text": "First, people scan through the whole passage to catch a glimpse of the main body of the passage.", "labels": [], "entities": []}, {"text": "Then with the question in mind, people make connection between passage and question, and understand the main intent of the question related with the passage theme.", "labels": [], "entities": []}, {"text": "A rough answer span is then located from the passage and the attention can be focused onto the located context.", "labels": [], "entities": []}, {"text": "Finally, to prevent from forgetting the question, people comeback to the question and select a best answer according to the previously located answer span.", "labels": [], "entities": []}, {"text": "Inspired by this, we propose a hierarchical attention network which can gradually focus the attention on the right part of the answer boundary, while capturing the relation between the question and passage at different levels of granularity, as illustrated in.", "labels": [], "entities": []}, {"text": "Our model mainly consists of three joint layers: 1) encoder layer where pretrained language models and recurrent neural networks are used to build representation for questions and passages separately; 2) attention layer in which hierarchical attention networks are designed to capture the relation between question and passage at different levels of granularity; 3) match layer where refined question and passage are matched under a pointer-network () answer boundary predictor.", "labels": [], "entities": [{"text": "match", "start_pos": 366, "end_pos": 371, "type": "METRIC", "confidence": 0.9488848447799683}]}, {"text": "In encoder layer, to better represent the questions and passages in multiple aspects, we combine two different embeddings to give the fundamental word representations.", "labels": [], "entities": []}, {"text": "In addition to the typical glove word embeddings, we also utilize the ELMo embeddings () derived from a pre-trained language model, which shows superior performance in a wide range of NLP problems.", "labels": [], "entities": []}, {"text": "Different from the original fusion way for intermediate layer representations, we design a representation-aware fusion method to compute the output ELMo embeddings and the context information is also incorporated by further passing through a bi-directional LSTM network.", "labels": [], "entities": []}, {"text": "The key in machine reading comprehension solution lies in how to incorporate the question context into the paragraph, in which attention mechanism is most widely used.", "labels": [], "entities": [{"text": "machine reading comprehension solution", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.8048499673604965}]}, {"text": "Recently, many different attention functions and types have been designed (, which aims at properly aligning the question and passage.", "labels": [], "entities": []}, {"text": "In our attention layer, we propose a hierarchical attention network by leveraging both the co-attention and self-attention mechanism, to gradually focus our attention on the best answer span.", "labels": [], "entities": []}, {"text": "Different from the previous attention-based methods, we constantly complement the aligned representations with global information from the previous layer, and an additional fusion layer is used to further refine the representations.", "labels": [], "entities": []}, {"text": "In this way, our model can make some minor adjustment so that the attention will always be on the right place.", "labels": [], "entities": []}, {"text": "Based on the refined question and passage representation, a bilinear match layer is finally used to identify the best answer span with respect to the question.", "labels": [], "entities": []}, {"text": "Following the work of (, we predict the start and end boundary within a pointer-network output layer.", "labels": [], "entities": []}, {"text": "The proposed method achieves state-of-the-art results against strong baselines.", "labels": [], "entities": []}, {"text": "Our single model achieves 79.2% EM and 86.6% F1 score on the hidden test set, while the ensemble model further boosts the performance to 82.4% EM and 88.6% F1 score.", "labels": [], "entities": [{"text": "EM", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9933346509933472}, {"text": "F1 score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9883255362510681}, {"text": "EM", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.9865667223930359}, {"text": "F1 score", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9894423484802246}]}, {"text": "At the time of writing the paper (Jan. 12th 2018), our model SLQA+ (Semantic Learning for Question Answering) achieves the first position on the SQuAD leaderboard 1 for both single and ensemble models.", "labels": [], "entities": [{"text": "Question Answering)", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7430698474248251}, {"text": "SQuAD leaderboard 1", "start_pos": 145, "end_pos": 164, "type": "DATASET", "confidence": 0.8063791791598002}]}, {"text": "Besides, we are also among the first to surpass human EM performance on this golden benchmark dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first present the datasets used for evaluation.", "labels": [], "entities": []}, {"text": "Then we compare our end-to-end Hierarchical Attention Fusion Networks with existing machine reading models.", "labels": [], "entities": []}, {"text": "Finally, we conduct experiments to validate the effectiveness of our proposed components.", "labels": [], "entities": []}, {"text": "We evaluate our model on the task of question answering using recently released SQuAD and TriviaQA Wikipedia (, which have gained a huge attention over the past year.", "labels": [], "entities": [{"text": "question answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8943366408348083}, {"text": "SQuAD", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.8874199986457825}, {"text": "TriviaQA Wikipedia", "start_pos": 90, "end_pos": 108, "type": "DATASET", "confidence": 0.8614684343338013}]}, {"text": "An adversarial evaluation for the Stanford Question Answering SQuAD is also used to demonstrate the robust of our model under adversarial attacks (Jia and Liang, 2017).", "labels": [], "entities": [{"text": "Stanford Question Answering SQuAD", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.7035152465105057}]}, {"text": "We focus on the SQuAD dataset to train and evaluate our model.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.8691813945770264}]}, {"text": "SQuAD is a popular machine comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles.", "labels": [], "entities": []}, {"text": "Each context is a paragraph from an article and the answer to each question is guaranteed to be a span in the context.", "labels": [], "entities": []}, {"text": "The answer to each question is always a span in the context.", "labels": [], "entities": []}, {"text": "The model is given a credit if its answer matches one of the human chosen answers.", "labels": [], "entities": []}, {"text": "Two metrics are used to evaluate the model performance: Exact Match (EM) and a softer metric F1 score, which measures the weighted average of the precision and recall rate at a character level.: The performance of our SLQA model and competing approaches on SQuAD.", "labels": [], "entities": [{"text": "Exact Match (EM)", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.973173463344574}, {"text": "F1 score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9435953199863434}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9986705780029297}, {"text": "recall rate", "start_pos": 160, "end_pos": 171, "type": "METRIC", "confidence": 0.9627269506454468}]}, {"text": "To further examine the robustness of the proposed model, we also test the model performance on TriviaQA dataset.", "labels": [], "entities": [{"text": "TriviaQA dataset", "start_pos": 95, "end_pos": 111, "type": "DATASET", "confidence": 0.9840680658817291}]}, {"text": "The test performance of different methods on the leaderboard (on Jan. 12th 2018) is shown in.", "labels": [], "entities": []}, {"text": "From the results, we can see that the proposed model can also obtain state-of-the-art performance in the more complex TriviaQA dataset.", "labels": [], "entities": [{"text": "TriviaQA dataset", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.9574122130870819}]}], "tableCaptions": [{"text": " Table 1: The performance of our SLQA model  and competing approaches on SQuAD.", "labels": [], "entities": []}, {"text": " Table 2:  The F1 scores of different models  on AddSent and AddOneSent datasets (S: Single  Model, E: Ensemble).", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.972684770822525}, {"text": "AddOneSent datasets", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.9011679589748383}]}, {"text": " Table 5: Comparison of different attention styles  on the SQuAD dev set.", "labels": [], "entities": [{"text": "SQuAD dev set", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.8676768143971761}]}, {"text": " Table 6: Published and unpublished results on the  TriviaQA wikipedia leaderboard.", "labels": [], "entities": [{"text": "TriviaQA wikipedia leaderboard", "start_pos": 52, "end_pos": 82, "type": "DATASET", "confidence": 0.9537395636240641}]}]}