{"title": [], "abstractContent": [{"text": "We introduce Latent Vector Grammars (LVeGs), anew framework that extends latent variable grammars such that each non-terminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the non-terminal.", "labels": [], "entities": []}, {"text": "We show that previous models such as latent variable grammars and com-positional vector grammars can be interpreted as special cases of LVeGs.", "labels": [], "entities": []}, {"text": "We then present Gaussian Mixture LVeGs (GM-LVeGs), anew special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals.", "labels": [], "entities": []}, {"text": "A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning.", "labels": [], "entities": []}, {"text": "We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.7671831250190735}, {"text": "constituency parsing", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7886313199996948}]}, {"text": "Our code is available at https://github.com/zhaoyanpeng/lveg.", "labels": [], "entities": []}], "introductionContent": [{"text": "In constituency parsing, refining coarse syntactic categories of treebank grammars into fine-grained subtypes has been proven effective in improving parsing results.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.8775316774845123}, {"text": "parsing", "start_pos": 149, "end_pos": 156, "type": "TASK", "confidence": 0.97382652759552}]}, {"text": "Previous approaches to refining syntactic categories use tree annotations, lexicalization, or linguistically motivated category splitting (.", "labels": [], "entities": [{"text": "category splitting", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7609893679618835}]}, {"text": "introduce latent variable grammars, in which each syntactic category (represented by a nonterminal) is split into a fixed number of subtypes and a discrete latent variable is used to indicate the subtype of the nonterminal when it appears in a specific parse tree.", "labels": [], "entities": []}, {"text": "Since the latent variables are not observable in treebanks, the grammar is learned using expectation-maximization.", "labels": [], "entities": []}, {"text": "present a split-merge approach to learning latent variable grammars, which hierarchically splits each nonterminal and merges ineffective splits.", "labels": [], "entities": []}, {"text": "further allow a nonterminal to have different splits in different production rules, which results in a more compact grammar.", "labels": [], "entities": []}, {"text": "Recently, neural approaches become very popular in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.8006756206353506}]}, {"text": "An important technique in neural approaches to NLP is to represent discrete symbols such as words and syntactic categories with continuous vectors or embeddings.", "labels": [], "entities": []}, {"text": "Since the distances between such vector representations often reflect the similarity between the corresponding symbols, this technique facilitates more informed smoothing in learning functions of symbols (e.g., the probability of a production rule).", "labels": [], "entities": []}, {"text": "In addition, what a symbol represents may subtly depend on its context, and a continuous vector representation has the potential of representing each instance of the symbol in a more precise manner.", "labels": [], "entities": []}, {"text": "For constituency parsing, recursive neural networks and their extensions such as compositional vector grammars can be seen as representing nonterminals in a context-free grammar with continuous vectors.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.843330055475235}]}, {"text": "However, exact inference in these models is intractable.", "labels": [], "entities": []}, {"text": "In this paper, we introduce latent vector grammars (LVeGs), a novel framework of grammars with fine-grained nonterminal subtypes.", "labels": [], "entities": []}, {"text": "A LVeG associates each nonterminal with a continuous vector space that represents the set of (infinitely many) subtypes of the nonterminal.", "labels": [], "entities": []}, {"text": "For each in-stance of a nonterminal that appears in a parse tree, its subtype is represented by a latent vector.", "labels": [], "entities": []}, {"text": "For each production rule over nonterminals, a nonnegative continuous function specifies the weight of any fine-grained production rule over subtypes of the nonterminals.", "labels": [], "entities": []}, {"text": "Compared with latent variable grammars which assume a small fixed number of subtypes for each nonterminal, LVeGs assume an unlimited number of subtypes and are potentially more expressive.", "labels": [], "entities": []}, {"text": "By having weight functions of varying smoothness for different production rules, LVeGs can also control the level of subtype granularity for different productions, which has been shown to improve the parsing accuracy ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 200, "end_pos": 207, "type": "TASK", "confidence": 0.9714744687080383}, {"text": "accuracy", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.9537855982780457}]}, {"text": "In addition, similarity between subtypes of a nonterminal can be naturally modeled by the distance between the corresponding vectors, so by using continuous and smooth weight functions we can ensure that similar subtypes will have similar syntactic behaviors.", "labels": [], "entities": []}, {"text": "We further present Gaussian Mixture LVeGs (GM-LVeGs), a special case of LVeGs that uses mixtures of Gaussian distributions as the weight functions of fine-grained production rules.", "labels": [], "entities": []}, {"text": "A major advantage of GM-LVeGs is that the partition function and the expectations of fine-grained production rules can be computed using an extension of the inside-outside algorithm.", "labels": [], "entities": []}, {"text": "This makes it possible to efficiently compute the gradients during discriminative learning of GM-LVeGs.", "labels": [], "entities": []}, {"text": "We evaluate GM-LVeGs on part-of-speech tagging and constituency parsing on a variety of languages and corpora and show that GM-LVeGs achieve competitive results.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7375735342502594}, {"text": "constituency parsing", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7717279195785522}]}, {"text": "It shall be noted that many modern state-ofthe-art constituency parsers predict how likely a constituent is based on not only local information (such as the production rules used in composing the constituent), but also contextual information of the constituent.", "labels": [], "entities": []}, {"text": "For example, the neural CRF parser looks at the words before and after the constituent; and RNNG) looks at the constituents that are already predicted (in the stack) and the words that are not processed (in the buffer).", "labels": [], "entities": [{"text": "RNNG", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8433862328529358}]}, {"text": "In this paper, however, we choose to focus on the basic framework and algorithms of LVeGs and leave the incorporation of contextual information for future work.", "labels": [], "entities": []}, {"text": "We believe that by laying a solid foundation for LVeGs, our work can pave the way for many interesting extensions of LVeGs in the future.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the GM-LVeG on part-of-speech (POS) tagging and constituency parsing and compare it against its special cases such as LVGs and CVGs.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6830831527709961}, {"text": "constituency parsing", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7993338704109192}]}, {"text": "It shall be noted that in this paper we focus on the basic framework of LVeGs and aim to show its potential advantage over previous special cases.", "labels": [], "entities": []}, {"text": "It is therefore not our goal to compete with the latest state-of-the-art approaches to tagging and parsing.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7667260964711508}]}, {"text": "In particular, we currently do not incorporate contextual information of words and constituents during tagging and parsing, while such information is critical in achieving state-of-the-art accuracy.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.5987015863259634}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9915953278541565}]}, {"text": "We will discuss future improvements of LVeGs in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.886738508939743}]}, {"text": "We use the Wall Street Journal corpus from the Penn English Treebank (WSJ).", "labels": [], "entities": [{"text": "Wall Street Journal corpus from the Penn English Treebank (WSJ)", "start_pos": 11, "end_pos": 74, "type": "DATASET", "confidence": 0.9253501941760381}]}, {"text": "Following the standard data splitting, we use sections 2 to 21 for training, section 23 for testing, and section 22 for development.", "labels": [], "entities": []}, {"text": "We preprocess the treebank using a right-branching binarization procedure to obtain an unannotated X-bar grammar, so that there are only binary and unary production rules.", "labels": [], "entities": []}, {"text": "To deal with the problem of unknown words in testing, we adopt the unknown word features used in the Berkeley parser and set the unknown word threshold to 1.", "labels": [], "entities": []}, {"text": "Specifically, any word occurring less than two times is replaced by one of the 60 unknown word categories.", "labels": [], "entities": []}, {"text": "(1) We use Wall Street Journal corpus from the Penn English Treebank (WSJ).", "labels": [], "entities": [{"text": "Wall Street Journal corpus from the Penn English Treebank (WSJ)", "start_pos": 11, "end_pos": 74, "type": "DATASET", "confidence": 0.9133579730987549}]}, {"text": "Following the standard data splitting, we use sections 0 to 18 for training, sections 22 to 24 for testing, and sections 19 to 21 for development.", "labels": [], "entities": []}, {"text": "(2) The Universal Dependencies treebank 1.4 (UD) (, in which English, French, German, Russian, Spanish, Indonesian, Finnish, and Italian treebanks are used.", "labels": [], "entities": [{"text": "Universal Dependencies treebank 1.4 (UD)", "start_pos": 8, "end_pos": 48, "type": "DATASET", "confidence": 0.7486842700413295}]}, {"text": "We use the original data splitting of these corpora for training and testing.", "labels": [], "entities": []}, {"text": "For both WSJ and UD English treebanks, we deal with unknown words in the same way as we do in parsing.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8327720165252686}, {"text": "UD English treebanks", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.8916088342666626}]}, {"text": "For the rest of the data, we use only one unknown word category and the unknown word threshold is also set to 1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Token accuracy (T) and sentence accuracy (S) for POS tagging on the testing data.", "labels": [], "entities": [{"text": "accuracy (T)", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.9017189145088196}, {"text": "sentence accuracy (S)", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.8006118178367615}, {"text": "POS tagging", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.8454665839672089}]}, {"text": " Table 3: Parsing accuracy on the testing data of WSJ. EX  indicates the exact match score.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8930280208587646}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9582538604736328}, {"text": "WSJ", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.799827516078949}, {"text": "EX", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9753044247627258}, {"text": "exact match score", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.8672727147738138}]}]}