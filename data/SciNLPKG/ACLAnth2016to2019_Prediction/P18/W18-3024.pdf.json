{"title": [{"text": "LSTMs Exploit Linguistic Attributes of Data", "labels": [], "entities": [{"text": "LSTMs Exploit Linguistic Attributes of Data", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.68624580403169}]}], "abstractContent": [{"text": "While recurrent neural networks have found success in a variety of natural language processing applications, they are general models of sequential data.", "labels": [], "entities": []}, {"text": "We investigate how the properties of natural language data affect an LSTM's ability to learn a nonlinguistic task: recalling elements from its input.", "labels": [], "entities": []}, {"text": "We find that models trained on natural language data are able to recall tokens from much longer sequences than models trained on non-language sequential data.", "labels": [], "entities": []}, {"text": "Furthermore, we show that the LSTM learns to solve the memoriza-tion task by explicitly using a subset of its neurons to count timesteps in the input.", "labels": [], "entities": []}, {"text": "We hypothesize that the patterns and structure in natural language data enable LSTMs to learn by providing approximate ways of reducing loss, but understanding the effect of different training data on the learnability of LSTMs remains an open question.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural networks, especially variants with gating mechanisms such as long short-term memory units (LSTM; Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU;), have significantly advanced the state of the art in many NLP tasks, among others).", "labels": [], "entities": []}, {"text": "However, RNNs are general models of sequential data; they are not explicitly designed to capture the unique properties of language that distinguish it from generic time series data.", "labels": [], "entities": []}, {"text": "In this work, we probe how linguistic properties such as the hierarchical structure of language, the dependencies between tokens, and the Zipfian distribution of token frequencies affect the ability of LSTMs to learn.", "labels": [], "entities": []}, {"text": "To do this, we define a simple memorization task where the objective is to recall the identity of the token that occurred a fixed number of timesteps in the past, within a fixed-length input.", "labels": [], "entities": []}, {"text": "Although the task itself is not linguistic, we use it because (1) it is a generic operation that might form part of a more complex function on arbitrary sequential data, and (2) its simplicity allows us to unfold the mechanism in the trained RNNs.", "labels": [], "entities": []}, {"text": "To study how linguistic properties of the training data affect an LSTM's ability to solve the memorization task, we consider several training regimens.", "labels": [], "entities": []}, {"text": "In the first, we train on data sampled from a uniform distribution over a fixed vocabulary.", "labels": [], "entities": []}, {"text": "In the second, the token frequencies have a Zipfian distribution, but are otherwise independent of each other.", "labels": [], "entities": []}, {"text": "In another, the token frequencies have a Zipfian distribution but we add Markovian dependencies to the data.", "labels": [], "entities": []}, {"text": "Lastly, we train the model on natural language sequences.", "labels": [], "entities": []}, {"text": "To ensure that the models truly memorize, we evaluate on uniform samples containing only rare words.", "labels": [], "entities": []}, {"text": "We observe that LSTMs trained to perform the memorization task on natural language data or data with a Zipfian distribution are able to memorize from sequences of greater length than LSTMs trained on uniformly-sampled data.", "labels": [], "entities": []}, {"text": "Interestingly, increasing the length of Markovian dependencies in the data does not significantly help LSTMs to learn the task.", "labels": [], "entities": []}, {"text": "We conclude that linguistic properties can help or even enable LSTMs to learn the memorization task.", "labels": [], "entities": []}, {"text": "Why this is the case remains an open question, but we propose that the additional structure and patterns within natural language data provide additional noisy, approximate paths for the model to minimize its loss, thus offering more training signal than the uniform case, in which the only way to reduce the loss is to learn the memorization function.", "labels": [], "entities": []}, {"text": "We further inspect how the LSTM solves the memorization task, and find that some hidden units count the number of inputs.", "labels": [], "entities": []}, {"text": "analyzed LSTM encoder-decoder translation models and found that similar counting neurons regulate the length of generated translations.", "labels": [], "entities": [{"text": "LSTM encoder-decoder translation", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.6257889568805695}]}, {"text": "Since LSTMs better memorize (and thus better count) on language data than on non-language data, and counting plays a role in encoder-decoder models, our work could also lead to improved training for sequence-to-sequence models in non-language applications (e.g.,).", "labels": [], "entities": []}], "datasetContent": [{"text": "We modify the linguistic properties of the training data and observe the effects on model performance.", "labels": [], "entities": []}, {"text": "Further details are found in Appendix A, and we release code for reproducing our results.", "labels": [], "entities": [{"text": "Appendix A", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.7824108600616455}]}, {"text": "We train an LSTM-based sequence prediction model to perform the memorization task.", "labels": [], "entities": [{"text": "LSTM-based sequence prediction", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.6193756461143494}]}, {"text": "The model embeds input tokens with a randomly initialized embedding matrix.", "labels": [], "entities": []}, {"text": "The embedded inputs are encoded by a single-layer LSTM and the final hidden state is passed through a linear projection to produce a probability distribution over the 2 Or the ( n 2 + 1)th token if the sequence length n is even.", "labels": [], "entities": []}, {"text": "We experimented with predicting tokens at a range of positions, and our results are not sensitive to the choice of predicting exactly the middle token.", "labels": [], "entities": []}, {"text": "http://nelsonliu.me/papers/ lstms-exploit-linguistic-attributes/ vocabulary.", "labels": [], "entities": []}, {"text": "Our goal is to evaluate the memorization ability of the LSTM, so we freeze the weights of the embedding matrix and the linear output projection during training.", "labels": [], "entities": []}, {"text": "This forces the model to rely on the LSTM parameters (the only trainable weights), since it cannot gain an advantage in the task by shifting words favorably in either the (random) input or output embedding vector spaces.", "labels": [], "entities": []}, {"text": "We also tie the weights of the embeddings and output projection so the LSTM can focus on memorizing the timestep of interest rather than also transforming input vectors to the output embedding space.", "labels": [], "entities": []}, {"text": "Finally, to examine the effect of model capacity on memorization ability, we experiment with different hidden state size values.", "labels": [], "entities": []}, {"text": "We experiment with several distributions of training data for the memorization task.", "labels": [], "entities": []}, {"text": "In all cases, a 10K vocabulary is used.", "labels": [], "entities": []}, {"text": "\u2022 In the uniform setup, each token in the training dataset is randomly sampled from a uniform distribution over the vocabulary.", "labels": [], "entities": []}, {"text": "\u2022 In the unigram setup, we modify the uniform data by integrating the Zipfian token frequencies found in natural language data.", "labels": [], "entities": []}, {"text": "The input sequences are taken from a modified version of the Penn Treebank () with randomly permuted tokens.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9962846636772156}]}, {"text": "\u2022 In the 5gram, 10gram, and 50gram settings, we seek to augment the unigram setting with Markovian dependencies.", "labels": [], "entities": []}, {"text": "We generate the dataset by grouping the tokens of the Penn Treebank into 5, 10, or 50-length chunks and randomly permuting these chunks.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9943901896476746}]}, {"text": "\u2022 In the language setup, we assess the effect of using real language.", "labels": [], "entities": []}, {"text": "The input sequences here are taken from the Penn Treebank, and thus this setup further extends the 5gram, 10gram, and 50gram datasets by adding the remaining structural properties of natural language.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9959599673748016}]}, {"text": "We evaluate each model on a test set of uniformly sampled tokens from the 100 rarest words in the vocabulary.", "labels": [], "entities": []}, {"text": "This evaluation setup ensures that, regardless of the data distribution the models were trained on, they are forced to generalize in order to perform well on the test set.", "labels": [], "entities": []}, {"text": "For instance, in a test on data with a Zipfian token distribution, the model may do well by simply exploiting the training distribution (e.g., by ignoring the long tail of rare words).", "labels": [], "entities": []}], "tableCaptions": []}