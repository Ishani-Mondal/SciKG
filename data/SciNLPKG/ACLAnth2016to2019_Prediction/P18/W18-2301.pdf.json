{"title": [{"text": "Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility", "labels": [], "entities": [{"text": "Embedding Transfer", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8476239442825317}, {"text": "Low-Resource Medical Named Entity Recognition", "start_pos": 23, "end_pos": 68, "type": "TASK", "confidence": 0.6834590673446655}, {"text": "Patient Mobility", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7424235045909882}]}], "abstractContent": [{"text": "Functioning is gaining recognition as an important indicator of global health, but remains under-studied in medical natural language processing research.", "labels": [], "entities": []}, {"text": "We present the first analysis of automatically extracting descriptions of patient mobility, using a recently-developed dataset of free text electronic health records.", "labels": [], "entities": [{"text": "automatically extracting descriptions of patient mobility", "start_pos": 33, "end_pos": 90, "type": "TASK", "confidence": 0.7401216228802999}]}, {"text": "We frame the task as a named entity recognition (NER) problem, and investigate the applicability of NER techniques to mobility extraction.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.816702405611674}, {"text": "mobility extraction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7357657253742218}]}, {"text": "As text corpora focused on patient functioning are scarce, we explore domain adaptation of word embeddings for use in a recurrent neural network NER system.", "labels": [], "entities": []}, {"text": "We find that embeddings trained on a small in-domain corpus perform nearly as well as those learned from large out-of-domain corpora, and that domain adaptation techniques yield additional improvements in both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 210, "end_pos": 219, "type": "METRIC", "confidence": 0.9992040991783142}, {"text": "recall", "start_pos": 224, "end_pos": 230, "type": "METRIC", "confidence": 0.9975250363349915}]}, {"text": "Our analysis identifies several significant challenges in extracting descriptions of patient mobility, including the length and complexity of annotated entities and high linguistic variability in mobility descriptions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Functioning has recently been recognized as a leading world health indicator, joining morbidity and mortality . Functioning is defined in the International Classification of) as the interaction between health conditions, body functions and structures, activities and participation, and contextual factors.", "labels": [], "entities": [{"text": "International Classification", "start_pos": 142, "end_pos": 170, "type": "DATASET", "confidence": 0.8287555277347565}]}, {"text": "Understanding functioning is an important element in assessing quality of life, and automatic extraction of patient functioning would serve as a useful tool fora variety of care decisions, including rehabilitation and disability assessment ( . In healthcare data, natural language processing (NLP) techniques have been successfully used for retrieving information about health conditions, symptoms and procedures from unstructured electronic health record (EHR) text (.", "labels": [], "entities": []}, {"text": "As recognition of the importance of functioning grows, there is a need to investigate the application of NLP methods to other elements of functioning.", "labels": [], "entities": []}, {"text": "Recently, introduced a dataset of EHR documents annotated for descriptions of patient mobility status, one area of activity in the ICF.", "labels": [], "entities": [{"text": "ICF", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.8699634671211243}]}, {"text": "Automatically recognizing these descriptions faces significant challenges, including their length and syntactic complexity and alack of terminological resources to draw on.", "labels": [], "entities": [{"text": "length", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9552790522575378}]}, {"text": "In this study, we view this task through the lens of named entity recognition (NER), as recent work has illustrated the potential of using recurrent neural network (RNN) NER models to address similar issues in biomedical NLP ().", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.7832294007142385}]}, {"text": "An additional strength of RNN models is their ability to leverage pretrained word embeddings, which capture co-occurrence information about words from large text corpora.", "labels": [], "entities": []}, {"text": "Prior work has shown that the best improvements come from embeddings trained on a corpus related to the target domain (.", "labels": [], "entities": []}, {"text": "However, free text describing patient functioning is hard to come by: for example, even the large MIMIC-III corpus includes only a few hundred documents from therapy disciplines among its two million notes.", "labels": [], "entities": [{"text": "MIMIC-III corpus", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.8400984704494476}]}, {"text": "While recent work suggests that using a training corpus from the target domain can mitigate alack of data (, even a careful corpus selection may not produce suffi-cient data to train robust word representations.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of an RNN model to recognize descriptions of patient mobility.", "labels": [], "entities": []}, {"text": "We analyze the impact of initializing the model with word embeddings trained on a variety of corpora, ranging from large-scale out-ofdomain data to small, highly-targeted in-domain documents.", "labels": [], "entities": []}, {"text": "We further explore several domain adaptation techniques for combining word-level information from both of these data sources, including a novel nonlinear embedding transformation method using a deep neural network.", "labels": [], "entities": []}, {"text": "We find that embeddings trained on a very small set of therapy encounter notes nearly match the mobility NER performance of representations trained on millions of out-of-domain documents.", "labels": [], "entities": []}, {"text": "Domain adaptation of input word embeddings often improves performance on this challenging dataset, in both precision and recall.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7282147109508514}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9994833469390869}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9958624839782715}]}, {"text": "Finally, we find that simpler adaptation methods such as concatenation and preinitialization achieve highest overall performance, but that nonlinear mapping of embeddings yields the most consistent performance across experiments.", "labels": [], "entities": []}, {"text": "We achieve a best performance of 69% exact match and over 83% token-level match F-1 score on the mobility data, and identify several trends in system errors that suggest fruitful directions for further research on recognizing descriptions of patient functioning.", "labels": [], "entities": [{"text": "exact match", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.963121086359024}, {"text": "token-level match F-1 score", "start_pos": 62, "end_pos": 89, "type": "METRIC", "confidence": 0.8107965588569641}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Named entity statistics for training, vali- dation, and test splits of BTRIS-Mobility. Due to  the rarity of ScoreDefinition entities, we use a 2:1  split of training to test data, and hold out 10% of  training data as validation.", "labels": [], "entities": [{"text": "BTRIS-Mobility", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.8620548844337463}]}, {"text": " Table 2: Exact and token-level match results on BTRIS-Mobility, using randomly-initialized embeddings  as a baseline and unmodified word2vec (w2v) and FastText (FT) embeddings from different corpora. Size  is the number of tokens in the training corpus.", "labels": [], "entities": [{"text": "BTRIS-Mobility", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.8591671586036682}]}, {"text": " Table 4: Exact match precision and recall for Mobility entities with word embeddings mapped from each  source to BTRIS F T embeddings, using four selected domain adaptation methods. The best-performing  embeddings from each source corpus were also mapped to PT-OT F T embeddings. The best precision,  recall, and F1 achieved with each source/target pair is marked in bold.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.8472315073013306}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9990789890289307}, {"text": "precision", "start_pos": 290, "end_pos": 299, "type": "METRIC", "confidence": 0.9992328882217407}, {"text": "recall", "start_pos": 302, "end_pos": 308, "type": "METRIC", "confidence": 0.9986860156059265}, {"text": "F1", "start_pos": 314, "end_pos": 316, "type": "METRIC", "confidence": 0.999667763710022}]}, {"text": " Table 3: Comparison of mapping methods, using  WikiNews F T as source and BTRIS", "labels": [], "entities": [{"text": "BTRIS", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.8752566576004028}]}, {"text": " Table 5: Best precision, recall, and F1 (exact) for  test set Mobility mentions, with the source/target  pair and domain adaptation method used.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9993995428085327}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9995830655097961}, {"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.999776303768158}]}]}