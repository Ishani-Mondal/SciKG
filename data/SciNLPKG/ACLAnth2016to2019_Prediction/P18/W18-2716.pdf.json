{"title": [{"text": "Marian: Cost-effective High-Quality Neural Machine Translation in C++", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.6511902908484141}]}], "abstractContent": [{"text": "This paper describes the submissions of the \"Marian\" team to the WNMT 2018 shared task.", "labels": [], "entities": [{"text": "Marian\" team to the WNMT 2018 shared task", "start_pos": 45, "end_pos": 86, "type": "DATASET", "confidence": 0.8565843635135226}]}, {"text": "We investigate combinations of teacher-student training, low-precision matrix products, auto-tuning and other methods to optimize the Transformer model on GPU and CPU.", "labels": [], "entities": []}, {"text": "By further integrating these methods with the new averaging attention networks, a recently introduced faster Transformer variant, we create a number of high-quality, high-performance models on the GPU and CPU, dominating the Pareto frontier for this shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the submissions of the \"Marian\" team to the Workshop on Neural Machine Translation and Generation (WNMT 2018) shared task ( . The goal of the task is to build NMT systems on GPUs and CPUs placed on the Pareto Frontier of efficiency inaccuracy.", "labels": [], "entities": [{"text": "Neural Machine Translation and Generation (WNMT 2018) shared task", "start_pos": 77, "end_pos": 142, "type": "TASK", "confidence": 0.8561455390670083}]}, {"text": "Marian) is an efficient neural machine translation (NMT) toolkit written in pure C++ based on dynamic computation graphs.", "labels": [], "entities": [{"text": "Marian)", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9041267335414886}, {"text": "neural machine translation (NMT)", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.8564393719037374}]}, {"text": "One of the goals of the toolkit is to provide a research tool which can be used to define state-of-the-art systems that at the same time can produce truly deployment-ready models across different devices.", "labels": [], "entities": []}, {"text": "Ideally this should be accomplished within a single execution engine that does not require specialized, inference-only decoders.", "labels": [], "entities": []}, {"text": "The CPU back-end in Marian is a very recent addition and we use the shared-task as a testing ground for various improvements.", "labels": [], "entities": [{"text": "Marian", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.9768111705780029}]}, {"text": "The GPU-bound computations in Marian are already highly optimized and we mostly concentrate on modeling aspects and beam-search hyper-parameters.", "labels": [], "entities": [{"text": "Marian", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9063633680343628}]}, {"text": "The weak baselines (at 16.9 BLEU on newstest2014 at least 12 BLEU points below the stateof-the-art) could promote approaches that happily sacrifice quality for speed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9975934624671936}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9983396530151367}]}, {"text": "We choose a quality cut-off of around 26 BLEU for the first test set (newstest2014) and do not spend much time on systems below that threshold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9988858103752136}, {"text": "newstest2014", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.930073082447052}]}, {"text": "This threshold was chosen based on the semi-official Sockeye () baseline (27.6 BLEU on newstest2014) referenced on the shared task page.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8797567486763}]}, {"text": "We believe our CPU implementation of the Transformer model ( and attention averaging networks ( to be the fastest reported so far.", "labels": [], "entities": []}, {"text": "This is achieved by integer matrix multiplication with auto-tuning.", "labels": [], "entities": []}, {"text": "We also show that these models respond very well to sequence-level knowledge-distillation methods: Transformer students dimensions.", "labels": [], "entities": []}, {"text": "Postsubmission models marked with *.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Transformer students dimensions. Post- submission models marked with *.", "labels": [], "entities": []}, {"text": " Table 2: Time to translate newstest2014 with batch- size equal to 1 sentence (1s) and around 384 words  (384w) using integer multiplication variants vs 32- bit float matrix multiplication.", "labels": [], "entities": [{"text": "translate newstest2014", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7416105568408966}]}, {"text": " Table 3: Results on newstest2014 -GPU systems. Submitted systems in bold. All student systems have  been used with beam-size 1 unless stated differently (b=n).", "labels": [], "entities": [{"text": "newstest2014 -GPU", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.8708428939183553}]}, {"text": " Table 4: Results on newstest2014 -CPU systems. Submitted systems in bold. Post-submission systems  marked with *. All student systems have been used with beam-size 1 unless stated differently (b=n).", "labels": [], "entities": []}]}