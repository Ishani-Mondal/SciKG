{"title": [{"text": "Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing", "labels": [], "entities": [{"text": "Semantic Graph Generation", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6139500836531321}, {"text": "Semantic Parsing", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7107728719711304}]}], "abstractContent": [{"text": "This paper proposes a neural semantic parsing approach-Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6788985232512156}, {"text": "semantic parsing", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.7323406636714935}]}, {"text": "Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.8279487490653992}]}, {"text": "Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases.", "labels": [], "entities": []}, {"text": "Secondly , by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation.", "labels": [], "entities": [{"text": "semantic graph generation", "start_pos": 194, "end_pos": 219, "type": "TASK", "confidence": 0.6554327309131622}]}, {"text": "Experiments show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.", "labels": [], "entities": [{"text": "OVERNIGHT dataset", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.8728275299072266}, {"text": "GEO", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.9658990502357483}, {"text": "ATIS datasets", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.8217187821865082}]}], "introductionContent": [{"text": "Semantic parsing aims to map natural language sentences to logical forms (.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8199107944965363}]}, {"text": "For example, the sentence \"Which states border Texas?\" will be mapped to answer (A, (state (A), next to (A, stateid ( texas )))).", "labels": [], "entities": []}, {"text": "A semantic parser needs two functions, one for structure prediction and the other for semantic grounding.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7308847457170486}]}, {"text": "Traditional semantic parsers are usually based on compositional grammar, such as CCG (  tures for candidate logical forms ranking.", "labels": [], "entities": []}, {"text": "Unfortunately, it is challenging to design grammars and learn accurate lexicons, especially in wideopen domains.", "labels": [], "entities": []}, {"text": "Moreover, it is often hard to design effective features, and its learning process is not end-to-end.", "labels": [], "entities": []}, {"text": "To resolve the above problems, two promising lines of work have been proposed: Semantic graph-based methods and Seq2Seq methods.", "labels": [], "entities": []}, {"text": "Semantic graph-based methods ( represent the meaning of a sentence as a semantic graph (i.e., a sub-graph of a knowledge base, see example in) and treat semantic parsing as a semantic graph matching/generation process.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 153, "end_pos": 169, "type": "TASK", "confidence": 0.7554539740085602}, {"text": "semantic graph matching/generation", "start_pos": 175, "end_pos": 209, "type": "TASK", "confidence": 0.7832979202270508}]}, {"text": "Compared with logical forms, semantic graphs have a tight-coupling with knowledge bases (, and share many commonalities with syntactic structures ().", "labels": [], "entities": []}, {"text": "Therefore both the structure and semantic constraints from knowledge bases can be easily exploited during parsing (.", "labels": [], "entities": []}, {"text": "The main challenge of semantic graph-based parsing is how to effectively construct the semantic graph of a sentence.", "labels": [], "entities": [{"text": "semantic graph-based parsing", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6679888168970743}]}, {"text": "Currently, semantic graphs are either constructed by matching with patterns (, transforming from dependency tree (, or via a staged heuristic search algorithm (.", "labels": [], "entities": []}, {"text": "These methods are all based on manuallydesigned, heuristic construction processes, making them hard to handle open/complex situations.", "labels": [], "entities": []}, {"text": "In recent years, RNN models have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation ( . A lot of Seq2Seq models have also been employed for semantic parsing, where a sentence is parsed by translating it to linearized logical form using RNN models.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.7915350496768951}, {"text": "machine translation", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.7738041877746582}, {"text": "semantic parsing", "start_pos": 242, "end_pos": 258, "type": "TASK", "confidence": 0.7341735064983368}]}, {"text": "There is no need for high-quality lexicons, manually-built grammars, and hand-crafted features.", "labels": [], "entities": []}, {"text": "These models are trained end-to-end, and can leverage attention mechanism () to learn soft alignments between sentences and logical forms.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew neural semantic parsing framework -Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong prediction ability of Seq2Seq models.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7310356299082438}]}, {"text": "Specifically, we model semantic parsing as an end-to-end semantic graph generation process.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7823002338409424}]}, {"text": "For example in, our model will parse the sentence \"Which states border Texas\" by generating a sequence of actions [add variable:A, add type:state, ...].", "labels": [], "entities": []}, {"text": "To achieve the above goal, we first design an action set which can encode the generation process of semantic graph (including node actions such as add variable, add entity, add type, edge actions such as add edge, and operation actions such as argmin, argmax, count, sum, etc.).", "labels": [], "entities": []}, {"text": "And then we design a RNN model which can generate the action sequence for constructing the semantic graph of a sentence.", "labels": [], "entities": []}, {"text": "Finally we further enhance parsing by incorporating both structure and semantic constraints during decoding.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9623791575431824}]}, {"text": "Compared with the manually-designed, heuristic generation algorithms used in traditional semantic graph-based methods, our sequence-toaction method generates semantic graphs using a RNN model, which is learned end-to-end from training data.", "labels": [], "entities": []}, {"text": "Such a learnable, end-to-end generation makes our approach more effective and can fit to different situations.", "labels": [], "entities": []}, {"text": "Compared with the previous Seq2Seq semantic parsing methods, our sequence-to-action model predicts a sequence of semantic graph generation actions, rather than linearized logical forms.", "labels": [], "entities": [{"text": "Seq2Seq semantic parsing", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6047586500644684}]}, {"text": "We find that the action sequence encoding can better capture structure and semantic information, and is more compact.", "labels": [], "entities": []}, {"text": "And the parsing can be enhanced by exploiting structure and semantic constraints.", "labels": [], "entities": [{"text": "parsing", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9635016918182373}]}, {"text": "For example, in GEO dataset, the action add edge:next to must subject to the semantic constraint that its arguments must be of type state and state, and the structure constraint that the edge next to must connect two nodes to form a valid graph.", "labels": [], "entities": [{"text": "GEO dataset", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.9735302925109863}]}, {"text": "We evaluate our approach on three standard datasets: GEO (Zelle and Mooney, 1996), ATIS (He and) and OVERNIGHT ().", "labels": [], "entities": [{"text": "GEO", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.5860157012939453}, {"text": "ATIS", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9964690208435059}, {"text": "OVERNIGHT", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9609418511390686}]}, {"text": "The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.", "labels": [], "entities": [{"text": "OVERNIGHT dataset", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.8896336555480957}, {"text": "GEO", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.966969907283783}, {"text": "ATIS datasets", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.831950843334198}]}, {"text": "The main contributions of this paper are summarized as follows: \u2022 We propose anew semantic parsing framework -Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.738151341676712}, {"text": "semantic parsing", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.7334659397602081}]}, {"text": "This new framework can synthesize the advantages of semantic graph representation and the prediction ability of Seq2Seq models.", "labels": [], "entities": [{"text": "semantic graph representation", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.6249474287033081}]}, {"text": "\u2022 We design a sequence-to-action model, including an action set encoding for semantic graph generation and a Seq2Seq RNN model for action sequence prediction.", "labels": [], "entities": [{"text": "semantic graph generation", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.6737327178319296}, {"text": "action sequence prediction", "start_pos": 131, "end_pos": 157, "type": "TASK", "confidence": 0.6559393405914307}]}, {"text": "We further enhance the parsing by exploiting structure and semantic constraints during decoding.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9699025750160217}]}, {"text": "Experiments validate the effectiveness of our method.", "labels": [], "entities": []}, {"text": "2 Sequence-to-Action Model for End-to-End Semantic Graph Generation Given a sentence X = x 1 , ..., x |X| , our sequenceto-action model generates a sequence of actions Y = y 1 , ..., y |Y | for constructing the correct semantic graph.", "labels": [], "entities": [{"text": "Semantic Graph Generation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.6084570785363516}]}, {"text": "The conditional probability P (Y |X) used in our  model is decomposed as follows: where y <t = y 1 , ..., y t\u22121 . To achieve the above goal, we need: 1) an action set which can encode semantic graph generation process; 2) an encoder which encodes natural language input X into a vector representation, and a decoder which generates y 1 , ..., y |Y | conditioned on the encoding vector.", "labels": [], "entities": [{"text": "semantic graph generation", "start_pos": 184, "end_pos": 209, "type": "TASK", "confidence": 0.6759099761644999}]}, {"text": "In following we describe them in detail.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we assess the performance of our method and compare it with previous methods.", "labels": [], "entities": []}, {"text": "We conduct experiments on three standard datasets: GEO, ATIS and OVERNIGHT.", "labels": [], "entities": [{"text": "GEO", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.852686882019043}, {"text": "ATIS", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9934074878692627}, {"text": "OVERNIGHT", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9439356327056885}]}, {"text": "Following the experimental setup of Jia and Liang (2016): we use 200 hidden units and 100-dimensional word vectors for sentence encoding.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7384751737117767}]}, {"text": "The dimensions of action embedding are tuned on validation datasets for each corpus.", "labels": [], "entities": []}, {"text": "We initialize all parameters by uniformly sampling within the interval [-0.1, 0.1].", "labels": [], "entities": []}, {"text": "We train our model fora total of 30 epochs with an initial learning rate of 0.1, and halve the learning rate every 5 epochs after epoch 15.", "labels": [], "entities": []}, {"text": "We replace word vectors for words occurring only once with an universal word vector.", "labels": [], "entities": []}, {"text": "The beam size is set as 5.", "labels": [], "entities": [{"text": "beam size", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9286690354347229}]}, {"text": "Our model is implemented in Theano (, and the codes and settings are released on Github: https://github.com/dongpobeyond/Seq2Act.", "labels": [], "entities": [{"text": "Theano", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.9705724120140076}]}, {"text": "We evaluate different systems using the standard accuracy metric, and the accuracies on different datasets are obtained as same as Jia and Liang (2016).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9954784512519836}, {"text": "accuracies", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9821990132331848}]}], "tableCaptions": [{"text": " Table 1:  Test accuracies on GEO and ATIS  datasets, where * indicates systems with extra- resources are used.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9127787351608276}, {"text": "GEO", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9671404957771301}, {"text": "ATIS  datasets", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.8227056264877319}]}, {"text": " Table 2: Test accuracies on OVERNIGHT dataset, which includes eight domains: Social, Blocks, Bas- ketball, Restaurants, Calendar, Housing, Publications, and Recipes.", "labels": [], "entities": [{"text": "OVERNIGHT dataset", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.799949049949646}]}, {"text": " Table 3. We can see that,  Replacing mechanism outperforms Copying in all  three datasets. This is because Replacing is done", "labels": [], "entities": []}, {"text": " Table 3: Test accuracies of Seq2Act (+C1+C2) on  GEO, ATIS, and OVERNIGHT of two entity han- dling mechanisms.", "labels": [], "entities": [{"text": "GEO", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.9363048672676086}, {"text": "ATIS", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.803836464881897}]}, {"text": " Table 4: Average length of logical forms and ac- tion sequences on three datasets. On OVERNIGHT,  we average across all eight domains.", "labels": [], "entities": [{"text": "Average length", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8443509638309479}, {"text": "OVERNIGHT", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.5423206090927124}]}]}