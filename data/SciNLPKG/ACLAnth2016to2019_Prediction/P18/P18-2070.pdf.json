{"title": [{"text": "Modeling Discourse Cohesion for Discourse Parsing via Memory Network", "labels": [], "entities": [{"text": "Modeling Discourse Cohesion", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8392746448516846}]}], "abstractContent": [{"text": "Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.6702476292848587}]}, {"text": "Most existing approaches design sophisticated features or exploit various off-the-shelf tools, but achieve little success.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew transition-based discourse parser that makes use of memory networks to take discourse cohesion into account.", "labels": [], "entities": [{"text": "transition-based discourse parser", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.6950542430082957}]}, {"text": "The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.678198054432869}]}, {"text": "Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly 1 .", "labels": [], "entities": [{"text": "RST discourse treebank", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.8067097067832947}]}], "introductionContent": [{"text": "Discourse parsing aims to identify the structure and relationship between different element discourse units (EDUs).", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7744087278842926}]}, {"text": "As a fundamental topic in natural language processing, discourse parsing can assist many down-stream applications such as summarization (, sentiment analysis () and question-answering.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7218616753816605}, {"text": "summarization", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.9880136251449585}, {"text": "sentiment analysis", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9390987455844879}]}, {"text": "However, the performance of discourse parsing is still far from perfect, especially for EDUs that are distant to each other in the discourse.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.6946951895952225}]}, {"text": "In fact, as found in (, the discourse parsing performance drops quickly as the dependency span increases.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.702904462814331}]}, {"text": "The reason maybe twofold: Firstly, as discussed in previous works, it is important to address discourse structure characteristics, e.g., through modeling lexical chains in a discourse, for discourse parsing, especially in dealing with long span scenarios.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 189, "end_pos": 206, "type": "TASK", "confidence": 0.7022735774517059}]}, {"text": "However, most existing approaches mainly focus on studying the semantic and syntactic aspects of EDU pairs, in a more local view.", "labels": [], "entities": []}, {"text": "Discourse cohesion reflects the syntactic or semantic relationship between words or phrases in a discourse, and, to some extent, can indicate the topic changing or threads in a discourse.", "labels": [], "entities": []}, {"text": "Discourse cohesion includes five situations, including reference, substitution, ellipsis, conjunction and lexical cohesion.", "labels": [], "entities": []}, {"text": "Here, lexical cohesion reflects the semantic relationship of words, and can be modeled as the recurrence of words, synonym and contextual words.", "labels": [], "entities": []}, {"text": "However, previous works do not well model the discourse cohesion within the discourse parsing task, or do not even take this issue into account.", "labels": [], "entities": [{"text": "discourse parsing task", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7988155682881674}]}, {"text": "proposes to utilize Roget thesauri to form lexical chains (sequences of semantically related words that can reflect the topic shifts within a discourse), which are used to extract features to characterize discourse structures.", "labels": [], "entities": []}, {"text": "() uses lexical chain feature to model multi-sentential relation.", "labels": [], "entities": []}, {"text": "Actually, these simplified cohesion features can already improve parsing performance, especially in long spans.", "labels": [], "entities": [{"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9798348546028137}]}, {"text": "Secondly, in modern neural network methods, modeling discourse cohesion as part of the networks is not a trivial task.", "labels": [], "entities": []}, {"text": "One can still use off-the-shell tools to obtain lexical chains, but these tools cannot be jointly optimized with the main neural network parser.", "labels": [], "entities": []}, {"text": "We argue that characterizing discourse cohesion implicitly within a unified framework would be more (1) I feel hungry after wake up, (2) I rush into the kitchen and make my breakfast.", "labels": [], "entities": []}, {"text": "straightforward and effective for our neural network based parser.", "labels": [], "entities": []}, {"text": "As shown in, the 12 EDUs in the given discourse talk about different topics, marked with 3 different colors, which could be captured by a memory network that maintains several memory slots.", "labels": [], "entities": []}, {"text": "In discourse parsing, such an architecture may help to cluster topically similar or related EDUs into the same memory slot, and each slot could be considered as a representation that maintains a specific topic or thread within the current discourse.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.6988278478384018}]}, {"text": "Intuitively, we could also treat such a mechanism as away to capture the cohesion characteristics of the discourse, just like the lexical chain features used in previous works, but without relying on external tools or resources.", "labels": [], "entities": []}, {"text": "In this paper, we investigate how to exploit discourse cohesion to improve discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.6930160671472549}]}, {"text": "Our contribution includes: 1) we design a memory network method to capture discourse cohesion implicitly in order to improve discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.6781992018222809}]}, {"text": "2) We choose bidirectional long-short term memory (LSTM)) with an attention mechanism to represent EDUs directly from embeddings, and use simple position features to capture shallow discourse structures, without relying on off-the-shelf tools or resources.", "labels": [], "entities": []}, {"text": "Experiments on the RST corpus show that the memory based discourse cohesion model can help better capture discourse structure information and lead to significant improvement over traditional feature based discourse parsing methods.", "labels": [], "entities": [{"text": "RST corpus", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.8982818424701691}, {"text": "feature based discourse parsing", "start_pos": 191, "end_pos": 222, "type": "TASK", "confidence": 0.7857629805803299}]}], "datasetContent": [{"text": "Dataset: We use the RST Discourse Treebank () with the same split as in (), i.e., 312 for training, 30 for development and 38 for testing.", "labels": [], "entities": [{"text": "RST Discourse Treebank", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.7428207000096639}]}, {"text": "We experiment with two set of relations, the 111 types of fine-grained relations and the 19 types of coarse-grained relations, respectively.", "labels": [], "entities": []}, {"text": "Metrics: In the Rhetorical Structure Theory (RST) (, head is the core of a discourse, and a dependent gives supporting evidence to its head with certain relationship.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.6694359729687372}]}, {"text": "We adopt unlabeled accuracy U AS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both correct heads and relations) as our evaluation metrics.", "labels": [], "entities": [{"text": "accuracy U AS", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9543046752611796}, {"text": "accuracy LAS", "start_pos": 101, "end_pos": 113, "type": "METRIC", "confidence": 0.8629292845726013}]}, {"text": "Baselines: We compare our method with the following baselines and models: (1) Perceptron: We re-implement the perceptron based arc-eager style dependency discourse parser as mentioned in () with coarse-grained relation.", "labels": [], "entities": []}, {"text": "The Perceptron model chooses words, POS tags, positions and length features, totally 100 feature templates, with the early update strategy (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall discourse parsing performance in  the RST dataset.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.6882384270429611}, {"text": "RST dataset", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9060635268688202}]}, {"text": " Table 2: Performance in different discourse spans.", "labels": [], "entities": []}]}