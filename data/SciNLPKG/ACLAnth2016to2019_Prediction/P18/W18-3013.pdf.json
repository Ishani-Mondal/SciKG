{"title": [{"text": "Evaluating Word Embeddings in Multi-label Classification Using Fine-grained Name Typing", "labels": [], "entities": [{"text": "Multi-label Classification", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.7495486438274384}]}], "abstractContent": [{"text": "Embedding models typically associate each word with a single real-valued vector, representing its different properties.", "labels": [], "entities": []}, {"text": "Evaluation methods, therefore, need to analyze the accuracy and completeness of these properties in embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9990485310554504}]}, {"text": "This requires fine-grained analysis of embedding sub-spaces.", "labels": [], "entities": []}, {"text": "Multi-label classification is an appropriate way to do so.", "labels": [], "entities": [{"text": "Multi-label classification", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8272397220134735}]}, {"text": "We propose anew evaluation method for word embeddings based on multi-label classification given a word embedding.", "labels": [], "entities": []}, {"text": "The task we use is fine-grained name typing: given a large corpus , find all types that a name can refer to based on the name embedding.", "labels": [], "entities": [{"text": "name typing", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7189014554023743}]}, {"text": "Given the scale of entities in knowledge bases, we can build datasets for this task that are complementary to the current embedding evaluation datasets in: they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representation of words, aka word embedding, is an important element of many natural language processing applications.", "labels": [], "entities": [{"text": "Distributed representation of words", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8918499648571014}]}, {"text": "The quality of word embeddings is assessed using different methods.", "labels": [], "entities": []}, {"text": "evaluate word embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8067494034767151}]}, {"text": "Different concept categorization datasets are introduced.", "labels": [], "entities": []}, {"text": "These datasets are small (<500) ( and therefore measure the goodness of embeddings by the quality of their clustering.", "labels": [], "entities": []}, {"text": "Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities.", "labels": [], "entities": []}, {"text": "Figure 1: Types (ellipses; green) of the entities (rectangles; red), to which the name \"Washington\" can refer.", "labels": [], "entities": []}, {"text": "Ideally, the embedding for \"Washington\" should represent all these types.", "labels": [], "entities": [{"text": "Washington", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.7921808958053589}]}, {"text": "Extrinsic evaluations are also used, cf. Li and Jurafsky.", "labels": [], "entities": []}, {"text": "In these tasks, embeddings are used in context/sentence representations with composition involved.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew evaluation method.", "labels": [], "entities": []}, {"text": "In contrast to the prior work on intrinsic evaluation, our method is supervised, largescale, fine-grained, automatically built, and evaluates embeddings in a classification setting where different subspaces of embeddings need to be analyzed.", "labels": [], "entities": []}, {"text": "In contrast to the prior work on extrinsic evaluation, we evaluate embeddings in isolation, without confounding factors like sentence contexts or composition functions.", "labels": [], "entities": []}, {"text": "Our evaluation is based on an entity-oriented task in information extraction (IE).", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.8667074918746949}]}, {"text": "Different areas of IE try to predict relevant data about entities from text, either locally (i.e., at the context-level), or globally (i.e., at the corpus-level).", "labels": [], "entities": [{"text": "IE", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9894381761550903}]}, {"text": "For example, local () and global () in relation extraction, or local () and global) in entity typing.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7735985517501831}]}, {"text": "In most global tasks, each entity is indexed with an identifier (ID) that usually comes from knowledge bases such as Freebase.", "labels": [], "entities": []}, {"text": "Exceptions are tasks in lexicon generation or population like entity set expansion (ESE)), which are global but without entity IDs.", "labels": [], "entities": [{"text": "entity set expansion (ESE))", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.8024972081184387}]}, {"text": "ESE usually starts from a few seed entities per set and completes the set using pattern-based methods.", "labels": [], "entities": [{"text": "ESE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8482211828231812}]}, {"text": "Here, we address the task of fine-grained name typing (FNT), a global prediction task, operating on the surface names of entities.", "labels": [], "entities": [{"text": "name typing (FNT)", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.810156273841858}, {"text": "global prediction task", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7596280376116434}]}, {"text": "FNT and ESE share applications in name lexicon population.", "labels": [], "entities": [{"text": "FNT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7923910021781921}, {"text": "ESE", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.7307411432266235}, {"text": "name lexicon population", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8054705858230591}]}, {"text": "FNT is different from ESE because we assume to have sufficient training instances for each type to train supervised models.", "labels": [], "entities": [{"text": "FNT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5856743454933167}]}, {"text": "The challenging goal of FNT is to find the types of all entities a name can refer to.", "labels": [], "entities": [{"text": "FNT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.949447512626648}]}, {"text": "For example, \"Washington\" might refer to several entities which in turn may belong to multiple types, see.", "labels": [], "entities": []}, {"text": "In this example, \"Washington\" refers to \"Washington DC (city)\", \"Washington (state)\", or \"George Washington (president)\".", "labels": [], "entities": []}, {"text": "Also, each entity can belong to several types, e.g., \"George Washington\" is a POLITICIAN, a PERSON and a SOLDIER, or \"Washington (state)\" is a STATE and a LOCATION.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9419412016868591}]}, {"text": "Learning global representations for entities is very effective for global prediction tasks in IE (cf.,).", "labels": [], "entities": [{"text": "global prediction tasks", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.7489612897237142}, {"text": "IE", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.941950798034668}]}, {"text": "For our task, FNT, we also learn a global representation for each name.", "labels": [], "entities": [{"text": "FNT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.6268129944801331}]}, {"text": "By doing so, we see this task as a challenging evaluation for embedding models.", "labels": [], "entities": []}, {"text": "We intend to use FNT to answer the following questions: (i) How well can embeddings represent distinctive information, i.e., different types or senses?", "labels": [], "entities": []}, {"text": "(ii) Which properties are important for an embedding model to do well on this task?", "labels": [], "entities": []}, {"text": "We build a novel large-scale dataset of (name, types) from Freebase with millions of examples.", "labels": [], "entities": []}, {"text": "The size of the dataset enables supervised approaches to work, an important requirement to be able to look at different subspaces of embeddings.", "labels": [], "entities": []}, {"text": "Also, in FNT names are-in contrast to concept categorization datasets-multi-labeled, which requires to look at multiple subspaces of embeddings.", "labels": [], "entities": []}, {"text": "In summary, our contributions are (i) introducing anew evaluation method for word embeddings (ii) publishing anew dataset that is a good resource for evaluating word embeddings and is complementary to prior work: it is very large, contains more different classes than previous word categorization datasets, and allows the direct evaluation of embeddings without confounding factors like sentence context 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Using Freebase (, we first retrieve the set of all entities E n for each name n.", "labels": [], "entities": []}, {"text": "Then, we consider the types of all e \u2208 E n the types of n.", "labels": [], "entities": []}, {"text": "See for an example: all of the shown types belong to the name \"Washington\".", "labels": [], "entities": [{"text": "Washington", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8383574485778809}]}, {"text": "Since some of the about 1,500 Freebase types have very few instances, we map them first to the FIGER (Ling and Weld, 2012) type-set, which contains 113 types.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9812648892402649}]}, {"text": "We then further restrict our set to the top 50 most frequent types.", "labels": [], "entities": []}, {"text": "See for the list of types.", "labels": [], "entities": []}, {"text": "In order to be able to evaluate each embedding on its own, we divide our dataset into singleword (891,241 names) and multi-word.", "labels": [], "entities": []}, {"text": "In this work, the multi-word set is not used.", "labels": [], "entities": []}, {"text": "We then set a frequency threshold of 100 in our lowercased Wikipedia corpus and select   randomly 100,000 of our dataset names that pass this threshold.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.8668485581874847}]}, {"text": "We then divide the names into train (50%), dev (20%) and test (30%).", "labels": [], "entities": []}, {"text": "Some statistics of the single-word FNT dataset are shown in Table 2.", "labels": [], "entities": [{"text": "FNT dataset", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.7411016523838043}]}, {"text": "We report the results for all embedding models using LR and MLP in Table 3.", "labels": [], "entities": []}, {"text": "We use the following evaluation measures, which are used in entity typing: (i) ACC (accuracy): percentage of test examples where all predictions are correct, (ii) Micro-F1: the global F1 computed overall the predictions.", "labels": [], "entities": [{"text": "ACC (accuracy)", "start_pos": 79, "end_pos": 93, "type": "METRIC", "confidence": 0.8526608794927597}, {"text": "F1", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.912796676158905}]}, {"text": "Models in lines 1-5 in  the Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.9650579988956451}]}, {"text": "We set the min frequency in corpus to 100.", "labels": [], "entities": []}, {"text": "Window size = 3; negative sampling with n = 10.", "labels": [], "entities": []}, {"text": "Based on the results of LR, order-aware architectures are better than their bag-of-words counterparts, i.e., SSKIP > SKIP and CWIN > CBOW.", "labels": [], "entities": []}, {"text": "Overall, SSKIP is the best using LR classification.", "labels": [], "entities": [{"text": "SSKIP", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.47790154814720154}]}, {"text": "In MLP results, however, CBOW works best on micro-F1 measure and SSKIP and SKIP are bests on accuracy.", "labels": [], "entities": [{"text": "MLP", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.4716441035270691}, {"text": "CBOW", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.4794747829437256}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9984571933746338}]}, {"text": "There is no significant difference between CBOW and CWIN, or SSKIP and SKIP, respectively.", "labels": [], "entities": [{"text": "CWIN", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.7645559906959534}]}, {"text": "Overall, the nonlinear classifier (MLP) with one hidden layer outperforms the linear classifier (LR) substantially, emphasizing that the encoded information about different types is easier to extract with stronger models.", "labels": [], "entities": []}, {"text": "Analysis on the number of name types.", "labels": [], "entities": []}, {"text": "As a separate analysis, we measure how the classification performance depends on the N number of types of a name.", "labels": [], "entities": []}, {"text": "To do so, we group test names based on their number of types.", "labels": [], "entities": []}, {"text": "We keep the groups that have more than 100 members.", "labels": [], "entities": []}, {"text": "Then, we plot the F1 results of CBOW and CWIN models trained using MLP classifier in.", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9991648197174072}, {"text": "CBOW", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8358741998672485}]}, {"text": "As it is shown, both models get their best results on names with N = 2.", "labels": [], "entities": []}, {"text": "We suppose that the bad performance of N = 1 is related to the fact that one-type names have missing types in our dataset due to the incompleteness of Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 151, "end_pos": 159, "type": "DATASET", "confidence": 0.9564613699913025}]}, {"text": "The worse F1 of N >= 3 compared to N = 2 is expected since bigger N means that the models need to predict more types from the name embeddings.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9990648627281189}]}, {"text": "From N = 4, somewhat surprisingly the F1 increases as N increases.", "labels": [], "entities": [{"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9995920062065125}]}, {"text": "This is perhaps related to the frequency of names in the corpus, and its relation to the number of names types: as N increases, the frequency of words increases and the embedding has a better quality.", "labels": [], "entities": []}, {"text": "However, this is only a hypothesis and more investigation is required.", "labels": [], "entities": []}, {"text": "The other observation is in the trend of CBOW and CWIN results.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.6008244156837463}, {"text": "CWIN", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.5774305462837219}]}, {"text": "CBOW is worse for N <= 2, but it works clearly better for N > 2.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5573341846466064}]}, {"text": "This shows that the embedding models behave differently for different number of classes they belong to.", "labels": [], "entities": []}, {"text": "This could also be related to the frequency of words.", "labels": [], "entities": []}, {"text": "Analysis of the reasons would be interesting.", "labels": [], "entities": []}, {"text": "We leave it for the future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: List of the 50 types in our FNT dataset.", "labels": [], "entities": [{"text": "FNT dataset", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.84322789311409}]}, {"text": " Table 2: Some statistics (number of names; aver- age number of types per name) for our name typ- ing dataset.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy and micro-F1 results on FNT  for different embedding models using two classi- fiers (LR and MLP). Best result in each column is  bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993293285369873}, {"text": "FNT", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.5593981146812439}]}]}