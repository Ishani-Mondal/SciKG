{"title": [{"text": "Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples", "labels": [], "entities": [{"text": "Extending a Parser", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8496409058570862}]}], "abstractContent": [{"text": "We revisit domain adaptation for parsers in the neural era.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7285102307796478}]}, {"text": "First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain.", "labels": [], "entities": [{"text": "word representations", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7158654779195786}, {"text": "domain adaptation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7551285624504089}]}, {"text": "As evidence, we train a parser on the Wall Street Journal alone that achieves over 90% F 1 on the Brown corpus.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.9351246356964111}, {"text": "F 1", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9927426874637604}, {"text": "Brown corpus", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9662562906742096}]}, {"text": "For more syntactically distant domains, we provide a simple way to adapt a parser using only dozens of partial annotations.", "labels": [], "entities": []}, {"text": "For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45% to 73% using approximately five dozen training examples.", "labels": [], "entities": []}, {"text": "In the process, we demonstrate anew state-of-the-art single model result on the Wall Street Journal test set of 94.3%.", "labels": [], "entities": [{"text": "Wall Street Journal test set", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.9799394369125366}]}, {"text": "This is an absolute increase of 1.7% over the previous state-of-the-art of 92.6%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical parsers are often criticized for their performance outside of the domain they were trained on.", "labels": [], "entities": [{"text": "Statistical parsers", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8263781666755676}]}, {"text": "The most straightforward remedy would be more training data in the target domain, but building treebanks) is expensive.", "labels": [], "entities": []}, {"text": "In this paper, we revisit this issue in light of recent developments in neural natural language processing.", "labels": [], "entities": [{"text": "neural natural language processing", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.667519211769104}]}, {"text": "Our paper rests on two observations: 1.", "labels": [], "entities": []}, {"text": "It is trivial to train on partial annotations using a span-focused model.", "labels": [], "entities": []}, {"text": "demonstrated that a parser with minimal dependence between the decisions that produce a parse can achieve state-of-the-art performance.", "labels": [], "entities": []}, {"text": "We modify their parser, hence-  forth MSP, so that it trains directly on individual labeled spans instead of parse trees.", "labels": [], "entities": []}, {"text": "This results in a parser that can be trained, with no adjustments to the training regime, from partial sentence bracketings.", "labels": [], "entities": []}, {"text": "2. The use of contextualized word representations ( greatly reduces the amount of data needed to train linguistic models.", "labels": [], "entities": []}, {"text": "Contextualized word representations, which encode tokens conditioned on their context in a sentence, have been shown to give significant boosts across a variety of NLP tasks, and also to reduce the amount of data needed by an order of magnitude in some tasks.", "labels": [], "entities": []}, {"text": "Taken together, this suggests away to rapidly extend a newswire-trained parser to new domains.", "labels": [], "entities": []}, {"text": "Specifically, we will show it is possible to achieve large out-of-domain performance improvements using only dozens of partially annotated sentences, like those shown in.", "labels": [], "entities": []}, {"text": "The resulting parser also does not suffer any degradation on the newswire domain.", "labels": [], "entities": []}, {"text": "Along the way, we provide several other notable contributions: \u2022 We raise the state-of-the-art single-model F 1 -score for constituency parsing from 92.6% to 94.3% on the Wall Street Journal (WSJ) test set.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.887111634016037}, {"text": "constituency parsing", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.8526370227336884}, {"text": "Wall Street Journal (WSJ) test set", "start_pos": 171, "end_pos": 205, "type": "DATASET", "confidence": 0.9408310651779175}]}, {"text": "A trained model is publicly available.", "labels": [], "entities": []}, {"text": "1 \u2022 We show that, even without domain-specific training data, our parser has much less out-ofdomain degradation than previous parsers on \"newswire-adjacent\" domains like the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 174, "end_pos": 186, "type": "DATASET", "confidence": 0.8867380917072296}]}, {"text": "\u2022 We provide aversion of MSP which predicts its own POS tags (rather than requiring a third-party tagger).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Parsing performance on WSJTEST,  along with the results of other recent single-model  parsers trained without external parse data.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8395669460296631}, {"text": "WSJTEST", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.7750940322875977}]}, {"text": " Table 2: Feature ablation on WSJDEV.", "labels": [], "entities": [{"text": "WSJDEV", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9576534032821655}]}, {"text": " Table 3: Parsing performance on Brown verticals. MSP refers to the Minimal Span Parser (Stern et al.,  2017a). Charniak refers to the Charniak parser with reranking and self-training (Charniak, 2000; Char- niak and Johnson, 2005; McClosky et al., 2006a). MSP + Stanford POS tags refers to MSP trained and  tested using part-of-speech tags predicted by the Stanford tagger (Toutanova et al., 2003).", "labels": [], "entities": []}, {"text": " Table 4: Performance of RSP on QBANKDEV.", "labels": [], "entities": [{"text": "RSP", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.904723584651947}, {"text": "QBANKDEV", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.916822612285614}]}, {"text": " Table 5: Performance of RSP on GENIADEV.", "labels": [], "entities": [{"text": "RSP", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9530569314956665}, {"text": "GENIADEV", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9595582485198975}]}, {"text": " Table 6: RSP performance on GEODEV.", "labels": [], "entities": [{"text": "RSP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6671425700187683}, {"text": "GEODEV", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9495465755462646}]}, {"text": " Table 7: RSP performance on BIOCHEMDEV.", "labels": [], "entities": [{"text": "BIOCHEMDEV", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.6591198444366455}]}]}