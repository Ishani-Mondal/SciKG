{"title": [{"text": "Improving Entity Linking by Modeling Latent Relations between Mentions", "labels": [], "entities": [{"text": "Improving Entity Linking", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9156990249951681}, {"text": "Modeling Latent Relations between Mentions", "start_pos": 28, "end_pos": 70, "type": "TASK", "confidence": 0.7624888777732849}]}], "abstractContent": [{"text": "Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8101063966751099}]}, {"text": "Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7055746912956238}]}, {"text": "Unlike previous approaches, which relied on supervised systems or heuristics to predict these relations , we treat relations as latent variables in our neural entity-linking model.", "labels": [], "entities": []}, {"text": "We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion.", "labels": [], "entities": []}, {"text": "Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version.", "labels": [], "entities": []}, {"text": "Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity linking (NEL) is the task of assigning entity mentions in a text to corresponding entries in a knowledge base (KB).", "labels": [], "entities": [{"text": "Named entity linking (NEL) is the task of assigning entity mentions in a text to corresponding entries in a knowledge base (KB)", "start_pos": 0, "end_pos": 127, "type": "Description", "confidence": 0.7036590427160263}]}, {"text": "For example, consider where a mention \"World Cup\" refers to a KB entity FIFA WORLD CUP.", "labels": [], "entities": [{"text": "KB entity FIFA WORLD CUP", "start_pos": 62, "end_pos": 86, "type": "DATASET", "confidence": 0.6883601546287537}]}, {"text": "NEL is often regarded as crucial for natural language understanding and commonly used as preprocessing for tasks such as information extraction) and question answering.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6410624881585439}, {"text": "information extraction", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.7666558921337128}, {"text": "question answering", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.9121332168579102}]}, {"text": "Potential assignments of mentions to entities are regulated by semantic and discourse constraints.", "labels": [], "entities": []}, {"text": "For example, the second and third occurrences of mention \"England\" in are coreferent and thus should be assigned to the same entity.", "labels": [], "entities": [{"text": "England", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.8939000368118286}]}, {"text": "Besides coreference, there are many other relations between entities which constrain or favor certain alignment configurations.", "labels": [], "entities": []}, {"text": "For example, consider relation participant in in: if \"World Cup\" is aligned to the entity FIFA WORLD CUP then we expect the second \"England\" to refer to a football team rather than a basketball one.", "labels": [], "entities": []}, {"text": "NEL methods typically consider only coreference, relying either on off-the-shelf systems or some simple heuristics (, and exploit them in a pipeline fashion, though some (e.g.,;) additionally exploit a range of syntactic-semantic relations such as apposition and possessives.", "labels": [], "entities": []}, {"text": "Another line of work ignores relations altogether and models the predicted sequence of KB entities as a bag (.", "labels": [], "entities": []}, {"text": "Though they are able to capture some degree of coherence (e.g., preference towards entities from the same general domain) and are generally empirically successful, the underlying assumption is too coarse.", "labels": [], "entities": []}, {"text": "For example, they would favor assigning all the occurrences of \"England\" in to the same entity.", "labels": [], "entities": []}, {"text": "We hypothesize that relations useful for NEL can be induced without (or only with little) domain expertise.", "labels": [], "entities": [{"text": "NEL", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9641865491867065}]}, {"text": "In order to prove this, we encode relations as latent variables and induce them by optimizing the entity-linking model in an end-to-end fashion.", "labels": [], "entities": []}, {"text": "In this way, relations between mentions in documents will be induced in such away as to be beneficial for NEL.", "labels": [], "entities": [{"text": "NEL", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9348497986793518}]}, {"text": "As with other recent approaches to NEL (, we rely on representation learning and learn embeddings of mentions, contexts and relations.", "labels": [], "entities": [{"text": "NEL", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9656355381011963}]}, {"text": "This further reduces the amount of human expertise required to construct the system and, in principle, may make it more portable across languages and domains.", "labels": [], "entities": []}, {"text": "Our multi-relational neural model achieves an: Example for NEL, linking each mention to an entity in a KB (e.g. \"World Cup\" to FIFA WORLD CUP rather than FIBA BASKETBALL WORLD CUP).", "labels": [], "entities": [{"text": "BASKETBALL", "start_pos": 159, "end_pos": 169, "type": "METRIC", "confidence": 0.7837026119232178}]}, {"text": "Note that the first and the second \"England\" are in different relations to \"World Cup\".", "labels": [], "entities": [{"text": "World Cup\"", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9428756634394327}]}, {"text": "improvement of 0.85% F1 over the best reported scores on the standard AIDA-CoNLL dataset (.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9998613595962524}, {"text": "AIDA-CoNLL dataset", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.970755934715271}]}, {"text": "Substantial improvements over the relation-agnostic version show that the induced relations are indeed beneficial for NEL.", "labels": [], "entities": [{"text": "NEL", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.9384478330612183}]}, {"text": "Surprisingly its training also converges much faster: training of the full model requires ten times shorter wall-clock time than what is needed for estimating the simpler relationagnostic version.", "labels": [], "entities": []}, {"text": "This may suggest that the injected structural bias helps to explain regularities in the training data, making the optimization task easier.", "labels": [], "entities": []}, {"text": "We qualitatively examine induced relations.", "labels": [], "entities": []}, {"text": "Though we do not observe direct counterparts of linguistic relations, we, for example, see that some of the induced relations are closely related to coreference whereas others encode forms of semantic relatedness between the mentions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated four models: (i) rel-norm proposed in Section 3.2; (ii) ment-norm proposed in Section 3.3; (iii) ment-norm (K = 1): the monorelational version of ment-norm; and (iv) mentnorm (no pad): the ment-norm without using mention padding.", "labels": [], "entities": []}, {"text": "Recall also that our mono-relational (i.e. K = 1) rel-norm is equivalent to the relationagnostic baseline of.", "labels": [], "entities": []}, {"text": "We implemented our models in PyTorch and run experiments on a Titan X GPU.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9111782908439636}, {"text": "Titan X GPU", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.7539346615473429}]}, {"text": "The source code and trained models will be publicly available at https://github.com/lephong/ mulrel-nel.", "labels": [], "entities": []}, {"text": "For in-domain scenario, we used AIDA-CoNLL dataset).", "labels": [], "entities": [{"text": "AIDA-CoNLL dataset", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9355772137641907}]}, {"text": "This dataset contains AIDA-train for training, AIDA-A for dev, and AIDA-B for testing, having respectively 946, 216, and 231 documents.", "labels": [], "entities": [{"text": "AIDA-train", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.8726324439048767}, {"text": "AIDA-A", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.6579226851463318}, {"text": "AIDA-B", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.704017162322998}]}, {"text": "For out-domain scenario, we evaluated the models trained on AIDA-train, on five popular test sets: MSNBC, AQUAINT, ACE2004, which were cleaned and updated by; WNED-CWEB (CWEB), WNED-WIKI (WIKI), which were automatically extracted from ClueWeb and Wikipedia).", "labels": [], "entities": [{"text": "MSNBC", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.979562520980835}, {"text": "AQUAINT", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.888857364654541}, {"text": "WNED-CWEB", "start_pos": 159, "end_pos": 168, "type": "DATASET", "confidence": 0.9242264032363892}, {"text": "WNED-WIKI", "start_pos": 177, "end_pos": 186, "type": "DATASET", "confidence": 0.9074764847755432}]}, {"text": "The first three are small with 20, 50, and 36 documents whereas the last two are much larger with 320 documents each.", "labels": [], "entities": []}, {"text": "Following previous works (, we considered only mentions that have entities in the KB (i.e., Wikipedia).", "labels": [], "entities": [{"text": "KB", "start_pos": 82, "end_pos": 84, "type": "DATASET", "confidence": 0.9035897254943848}]}], "tableCaptions": []}