{"title": [{"text": "Neural Document Summarization by Jointly Learning to Score and Select Sentences", "labels": [], "entities": [{"text": "Neural Document Summarization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9034408728281657}]}], "abstractContent": [{"text": "Sentence scoring and sentence selection are two main steps in extractive document summarization systems.", "labels": [], "entities": [{"text": "Sentence scoring", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9340946972370148}, {"text": "sentence selection", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7884130775928497}, {"text": "extractive document summarization", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.7122944990793864}]}, {"text": "However, previous works treat them as two separated subtasks.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel end-to-end neural network framework for extractive document summariza-tion by jointly learning to score and select sentences.", "labels": [], "entities": [{"text": "extractive document summariza-tion", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.6630135377248129}]}, {"text": "It first reads the document sentences with a hierarchical en-coder to obtain the representation of sentences.", "labels": [], "entities": []}, {"text": "Then it builds the output summary by extracting sentences one by one.", "labels": [], "entities": []}, {"text": "Different from previous methods, our approach integrates the selection strategy into the scoring model, which directly predicts the relative importance given previously selected sentences.", "labels": [], "entities": []}, {"text": "Experiments on the CNN/Daily Mail dataset show that the proposed framework significantly outper-forms the state-of-the-art extractive sum-marization models.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.9195404410362243}]}], "introductionContent": [{"text": "Traditional approaches to automatic text summarization focus on identifying important content, usually at sentence level ().", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.5875952442487081}]}, {"text": "With the identified important sentences, a summarization system can extract them to form an output summary.", "labels": [], "entities": []}, {"text": "In recent years, extractive methods for summarization have proven effective in many systems).", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9903427362442017}]}, {"text": "In previous works that use extractive methods, text summarization is decomposed into two subtasks, i.e., sentence scoring and sentence selection.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7017425894737244}, {"text": "sentence scoring", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.7607744932174683}, {"text": "sentence selection", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.752291738986969}]}, {"text": "* Contribution during internship at Microsoft Research.", "labels": [], "entities": []}, {"text": "Sentence scoring aims to assign an importance score to each sentence, and has been broadly studied in many previous works.", "labels": [], "entities": [{"text": "Sentence scoring", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9429735243320465}]}, {"text": "Feature-based methods are popular and have proven effective, such as word probability, TF*IDF weights, sentence position and sentence length features.", "labels": [], "entities": [{"text": "TF*IDF weights", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.9375572800636292}]}, {"text": "Graph-based methods such as TextRank) and LexRank () measure sentence importance using weighted-graphs.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9176120162010193}]}, {"text": "In recent years, neural network has also been applied to sentence modeling and scoring.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8884257376194}]}, {"text": "For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7833978235721588}]}, {"text": "Maximal Marginal Relevance) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary.", "labels": [], "entities": [{"text": "Maximal Marginal Relevance", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5588662624359131}]}, {"text": "Integer Linear Programming based methods treat sentence selection as an optimization problem under some constraints such as summary length.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7635166943073273}]}, {"text": "Submodular functions ( have also been applied to solving the optimization problem of finding the optimal subset of sentences in a document.", "labels": [], "entities": []}, {"text": "train two neural networks with handcrafted features.", "labels": [], "entities": []}, {"text": "One is used to rank sentences, and the other one is used to model redundancy during sentence selection.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7050192654132843}]}, {"text": "In this paper, we present a neural extractive document summarization (NEUSUM) framework which jointly learns to score and select sentences.", "labels": [], "entities": [{"text": "neural extractive document summarization (NEUSUM)", "start_pos": 28, "end_pos": 77, "type": "TASK", "confidence": 0.7212778840746198}]}, {"text": "Different from previous methods that treat sentence scoring and sentence selection as two tasks, our method integrates the two steps into one endto-end trainable model.", "labels": [], "entities": [{"text": "sentence scoring", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7607284784317017}, {"text": "sentence selection", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7410756349563599}]}, {"text": "Specifically, NEUSUM is a neural network model without any handcrafted features that learns to identify the relative importance of sentences.", "labels": [], "entities": []}, {"text": "The relative importance is measured as the gain over previously selected sentences.", "labels": [], "entities": []}, {"text": "Therefore, each time the proposed model selects one sentence, it scores the sentences considering both sentence saliency and previously selected sentences.", "labels": [], "entities": []}, {"text": "Through the joint learning process, the model learns to predict the relative gain given the sentence extraction state and the partial output summary.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7016846388578415}]}, {"text": "The proposed model consists of two parts, i.e., the document encoder and the sentence extractor.", "labels": [], "entities": []}, {"text": "The document encoder has a hierarchical architecture, which suits the compositionality of documents.", "labels": [], "entities": []}, {"text": "The sentence extractor is built with recurrent neural networks (RNN), which provides two main functionalities.", "labels": [], "entities": [{"text": "sentence extractor", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7330114394426346}]}, {"text": "On one hand, the RNN is used to remember the partial output summary by feeding the selected sentence into it.", "labels": [], "entities": []}, {"text": "On the other hand, it is used to provide a sentence extraction state that can be used to score sentences with their representations.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7148880809545517}]}, {"text": "At each step during extraction, the sentence extractor reads the representation of the last extracted sentence.", "labels": [], "entities": []}, {"text": "It then produces anew sentence extraction state and uses it to score the relative importance of the rest sentences.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7270655483007431}]}, {"text": "We conduct experiments on the CNN/Daily Mail dataset.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.9381468772888184}]}, {"text": "The experimental results demonstrate that the proposed NEUSUM by jointly scoring and selecting sentences achieves significant improvements over separated methods.", "labels": [], "entities": [{"text": "NEUSUM", "start_pos": 55, "end_pos": 61, "type": "TASK", "confidence": 0.7931718826293945}]}, {"text": "Our contributions are as follows: \u2022 We propose a joint sentence scoring and selection model for extractive document summarization.", "labels": [], "entities": [{"text": "sentence scoring", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7022161036729813}, {"text": "extractive document summarization", "start_pos": 96, "end_pos": 129, "type": "TASK", "confidence": 0.7069858113924662}]}, {"text": "\u2022 The proposed model can be end-to-end trained without handcrafted features.", "labels": [], "entities": []}, {"text": "\u2022 The proposed model significantly outperforms state-of-the-art methods and achieves the best result on CNN/Daily Mail dataset.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 104, "end_pos": 126, "type": "DATASET", "confidence": 0.920884644985199}]}], "datasetContent": [{"text": "We employ ROUGE) as our evaluation metric.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9919117093086243}]}, {"text": "ROUGE measures the quality of summary by computing overlapping lexical units, such as unigram, bigram, trigram, and longest common subsequence (LCS).", "labels": [], "entities": [{"text": "longest common subsequence (LCS)", "start_pos": 116, "end_pos": 148, "type": "METRIC", "confidence": 0.7157994310061137}]}, {"text": "It has become the standard evaluation metric for DUC shared tasks and popular for summarization evaluation.", "labels": [], "entities": [{"text": "DUC shared tasks", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.6726791063944498}, {"text": "summarization evaluation", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.9815293550491333}]}, {"text": "Following previous work, we use ROUGE-1 (unigram), ROUGE-2 (bigram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9407169222831726}, {"text": "ROUGE-2", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9557742476463318}, {"text": "ROUGE-L (LCS)", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.9346797466278076}]}], "tableCaptions": [{"text": " Table 1: Data statistics of CNN/Daily Mail dataset.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.9174424529075622}]}, {"text": " Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  \u2021 mark  are taken from the corresponding papers.", "labels": [], "entities": [{"text": "ROUGE F1 evaluation", "start_pos": 22, "end_pos": 41, "type": "METRIC", "confidence": 0.8564740022023519}, {"text": "CNN/Daily Mail test set", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.953325609366099}]}]}