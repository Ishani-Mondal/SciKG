{"title": [{"text": "Bridging Languages through Images with Deep Partial Canonical Correlation Analysis", "labels": [], "entities": [{"text": "Deep Partial Canonical Correlation Analysis", "start_pos": 39, "end_pos": 82, "type": "TASK", "confidence": 0.7037061452865601}]}], "abstractContent": [{"text": "We present a deep neural network that leverages images to improve bilingual text embeddings.", "labels": [], "entities": []}, {"text": "Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages , producing highly correlated bilingual embeddings.", "labels": [], "entities": []}, {"text": "In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA).", "labels": [], "entities": [{"text": "Partial Canonical Correlation Analysis (PCCA)", "start_pos": 49, "end_pos": 94, "type": "TASK", "confidence": 0.762372647012983}]}, {"text": "While the original PCCA finds linear projections of two views in order to maximize their canon-ical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop anew stochastic iterative algorithm for its optimization.", "labels": [], "entities": []}, {"text": "We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval.", "labels": [], "entities": [{"text": "cross-lingual image description retrieval", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.6878727674484253}]}, {"text": "Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research in multi-modal semantics deals with the grounding problem, motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system ().", "labels": [], "entities": []}, {"text": "In particular, recent studies have shown that performance on NLP tasks can be improved by joint modeling of text and vision, with multimodal and perceptually enhanced representation learning outperforming purely textual representa-tions.", "labels": [], "entities": []}, {"text": "These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context.", "labels": [], "entities": []}, {"text": "The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (, image-to-text retrieval and text-to-image retrieval (), image caption generation (, and visual sense disambiguation (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.6945295631885529}, {"text": "image-to-text retrieval", "start_pos": 161, "end_pos": 184, "type": "TASK", "confidence": 0.697711780667305}, {"text": "text-to-image retrieval", "start_pos": 189, "end_pos": 212, "type": "TASK", "confidence": 0.6834369599819183}, {"text": "image caption generation", "start_pos": 217, "end_pos": 241, "type": "TASK", "confidence": 0.8517303466796875}, {"text": "visual sense disambiguation", "start_pos": 249, "end_pos": 276, "type": "TASK", "confidence": 0.6619698206583658}]}, {"text": "While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling.", "labels": [], "entities": []}, {"text": "Such models induce bilingual multi-modal spaces based on multi-view learning.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information.", "labels": [], "entities": []}, {"text": "This additional perceptual modality bridges the gap between languages and reveals latent connections between concepts in the multilingual setup.", "labels": [], "entities": []}, {"text": "The shared visual information in our work takes the form of images with word-level tags or sentence-level descriptions assigned in more than one language.", "labels": [], "entities": []}, {"text": "We propose a deep neural architecture termed Deep Partial Canonical Correlation Analysis (DPCCA) based on the Partial CCA (PCCA) method.", "labels": [], "entities": [{"text": "Deep Partial Canonical Correlation Analysis (DPCCA)", "start_pos": 45, "end_pos": 96, "type": "TASK", "confidence": 0.7163740172982216}]}, {"text": "To the best of our knowledge, PCCA has not been used in multilingual settings before.", "labels": [], "entities": [{"text": "PCCA", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9397047162055969}]}, {"text": "In short, PCCA is a variant of CCA which learns maximally correlated linear projections of two views (e.g., two language-specific \"text-based views\") conditioned on a shared third view (e.g., the \"visual view\").", "labels": [], "entities": []}, {"text": "We discuss the PCCA and DPCCA methods in \u00a73 and show how they can be applied without having access to the shared images attest time inference.", "labels": [], "entities": []}, {"text": "PCCA inherits one disadvantageous property from CCA: both methods compute estimates for covariance matrices based on all training data.", "labels": [], "entities": []}, {"text": "This would prevent feasible training of their deep nonlinear variants, since deep neural nets (DNNs) are predominantly optimized via stochastic optimization algorithms.", "labels": [], "entities": []}, {"text": "To resolve this major hindrance, we propose an effective optimization algorithm for DPCCA, inspired by the work of on Deep CCA (DCCA) optimization.", "labels": [], "entities": [{"text": "Deep CCA (DCCA) optimization", "start_pos": 118, "end_pos": 146, "type": "TASK", "confidence": 0.5628443956375122}]}, {"text": "We evaluate our DPCCA architecture on two semantic tasks: 1) multilingual word similarity and 2) cross-lingual image description retrieval.", "labels": [], "entities": [{"text": "cross-lingual image description retrieval", "start_pos": 97, "end_pos": 138, "type": "TASK", "confidence": 0.6727024912834167}]}, {"text": "For the former, we construct and provide to the community anew Word-Image-Word (WIW) dataset containing bilingual lexicons for three languages with shared images for 5K+ concepts.", "labels": [], "entities": [{"text": "Word-Image-Word (WIW) dataset", "start_pos": 63, "end_pos": 92, "type": "DATASET", "confidence": 0.5950917541980744}]}, {"text": "WIW is used as training data for word similarity experiments, while evaluation is conducted on the standard multilingual SimLex-999 dataset (.", "labels": [], "entities": [{"text": "WIW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9436023831367493}, {"text": "word similarity", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.789990097284317}, {"text": "SimLex-999 dataset", "start_pos": 121, "end_pos": 139, "type": "DATASET", "confidence": 0.8014325499534607}]}, {"text": "The results reveal stable improvements over a large space of non-deep and deep CCA-style baselines in both tasks.", "labels": [], "entities": []}, {"text": "Most importantly, 1) PCCA is overall better than other methods which do not use the additional perceptual view; 2) DPCCA outperforms PCCA, indicating the importance of nonlinear transformations modeled through DNNs; 3) DPCCA outscores DCCA, again verifying the importance of conditioning multilingual text embedding induction on the shared visual view; and 4) DPCCA outperforms two recent multi-modal bilingual models which also leverage visual information).", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Preprocessing and Embeddings For the sentence-level task, all descriptions were lower-: WIW examples from each of the three bilingual lexicons.", "labels": [], "entities": []}, {"text": "Note that the designated words can be either abstract (true), express an action (dance) or be more concrete (plant).", "labels": [], "entities": []}, {"text": "Each sentence is represented with one vector: the average of its word embeddings.", "labels": [], "entities": []}, {"text": "For English, we rely on 500-dimensional English skip-gram word embeddings () trained on the January 2017 Wikipedia dump with bag-of-words contexts (window size of 5).", "labels": [], "entities": []}, {"text": "For German we use the deWaC 1.7B corpus () to obtain 500-dimensional German embeddings using the same word embedding model.", "labels": [], "entities": [{"text": "deWaC 1.7B corpus", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.8357425332069397}]}, {"text": "For word similarity, to be directly comparable to previous work, we rely on 300-dim word vectors in EN, DE, IT, and RU from Mrk\u0161i\u00b4.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7776667773723602}, {"text": "Mrk\u0161i\u00b4", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.9560402631759644}]}, {"text": "Visual features are extracted from the penultimate layer (FC7) of the VGG-19 network, and compressed to the dimensionality of the textual inputs by a Principal Component Analysis (PCA) step.", "labels": [], "entities": [{"text": "VGG-19 network", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.974597305059433}]}, {"text": "For the word similarity task, we average the visual vectors across all images of each word pair as done in, e.g.,, before the PCA step.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8226879239082336}]}, {"text": "Baseline Models We consider a wide variety of multi-view CCA-based baselines.", "labels": [], "entities": []}, {"text": "First, we compare against the original (linear) CCA model, and its deep non-linear extension DCCA ().", "labels": [], "entities": []}, {"text": "For DCCA: 1) we rely on its improved optimization algorithm from which uses a stochastic approach with large minibatches; 2) we compare against the DCCA NOI variant () described by Algorithm 1, and another recent DCCA variant with the optimization algorithm based on a stochastic decorrelational loss () (DCCA SDL); and 3) we also test the DCCA Autoencoder model (DCCAE) (), which offers a trade-off between maximizing the canonical correlation of two sets of variables and finding informative features for their reconstruction.", "labels": [], "entities": []}, {"text": "Another baseline is Generalized CCA (GCCA) (): a linear model which extends CCA to three or more views.", "labels": [], "entities": [{"text": "Generalized CCA (GCCA)", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.4535405397415161}]}, {"text": "Unlike PCCA, GCCA does not condition two variables on the third shared one, but rather seeks to maximize the canonical correlations of all pairs of views.", "labels": [], "entities": []}, {"text": "We also compare to Nonparametric CCA (NCCA) (, and to a probabilistic variant of PCCA (PPCCA,).", "labels": [], "entities": []}, {"text": "Finally, we compare with the two recent models which operate in the setup most similar to ours: 1) Bridge Correlational Networks (BCN); and 2) Image Pivoting (IMG PIVOT) from.", "labels": [], "entities": []}, {"text": "For both models, we report results only with the strongest variant based on the findings from the original papers, also verified by additional experimentation in our work.", "labels": [], "entities": []}, {"text": "Hyperparameter Tuning The hyperparameters of the different models are tuned with a grid search over the following values: {2,3,4,5} for number of layers, {tanh, sigmoid, ReLU} as the activation functions (we use the same activation function in all the layers of the same network), {64,128,256} for minibatch size, {0.001,0.0001} for learning rate, and {128,256} for L (the size of the output vectors).", "labels": [], "entities": [{"text": "ReLU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.98103928565979}]}, {"text": "The dimensions of all mid-layers are set to the input size.", "labels": [], "entities": []}, {"text": "We use the Adam optimizer (, with the number of epochs set to 300.", "labels": [], "entities": []}, {"text": "For all participating models, we report test performance of the best hyperparameter on the validation set.", "labels": [], "entities": []}, {"text": "For word similarity, following a standard practice ( we tune all models on one half of the SimLex data and evaluate on the other half, and vice versa.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8019759953022003}, {"text": "SimLex data", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.900946855545044}]}, {"text": "The reported score is the average of the two halves.", "labels": [], "entities": []}, {"text": "Similarity scores for all tasks were computed using the cosine similarity measure.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8958464860916138}, {"text": "cosine similarity measure", "start_pos": 56, "end_pos": 81, "type": "METRIC", "confidence": 0.7023813923199972}]}], "tableCaptions": [{"text": " Table 1: WIW statistics: the number of WIW en- tries across POS classes in each language pair. The  numbers of words per POS class are not summed  to the total number of words as other (less frequent)  POS tags are also represented.", "labels": [], "entities": []}, {"text": " Table 2: Results on cross-lingual image description  retrieval. NN-based models are above the dashed  line. Best overall results are in bold. Best results  with non-deep models are underlined.", "labels": [], "entities": [{"text": "cross-lingual image description  retrieval", "start_pos": 21, "end_pos": 63, "type": "TASK", "confidence": 0.6985518485307693}]}, {"text": " Table 3: Results on EN and DE SimLex-999 (POS-based evaluation). All scores are Spearman's rank  correlations. INIT EMB refers to initial pre-trained monolingual word embeddings (see  \u00a76).", "labels": [], "entities": [{"text": "DE", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.8363609313964844}, {"text": "INIT EMB", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.854803740978241}]}, {"text": " Table 4: Results (Spearman rank correlation) of our  models and the strongest baselines on Multilingual  SimLex-999 (all data).", "labels": [], "entities": [{"text": "Spearman rank correlation)", "start_pos": 19, "end_pos": 45, "type": "METRIC", "confidence": 0.8644323945045471}]}]}