{"title": [{"text": "Continuous Learning in a Hierarchical Multiscale Neural Network", "labels": [], "entities": []}], "abstractContent": [{"text": "We reformulate the problem of encoding a multi-scale representation of a sequence in a language model by casting it in a continuous learning framework.", "labels": [], "entities": []}, {"text": "We propose a hierarchical multi-scale language model in which short timescale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer timescale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion.", "labels": [], "entities": []}, {"text": "We use elastic weights consolidation as a higher-level to prevent catastrophic forgetting in our continuous learning framework.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models area major class of natural language processing (NLP) models whose development has lead to major progress in many areas like translation, speech recognition or summarization).", "labels": [], "entities": [{"text": "translation", "start_pos": 141, "end_pos": 152, "type": "TASK", "confidence": 0.9689852595329285}, {"text": "speech recognition", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7081169933080673}, {"text": "summarization", "start_pos": 176, "end_pos": 189, "type": "TASK", "confidence": 0.8629788756370544}]}, {"text": "Recently, the task of language modeling has been shown to bean adequate proxy for learning unsupervised representations of high-quality in tasks like text classification, sentiment detection ( or word vector learning (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7251855880022049}, {"text": "text classification", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7785375118255615}, {"text": "sentiment detection", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.9478228390216827}, {"text": "word vector learning", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.6559764742851257}]}, {"text": "More generally, language modeling is an example of online/sequential prediction task, in which a model tries to predict the next observation given a sequence of past observations.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7302441895008087}, {"text": "online/sequential prediction task", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.7070449471473694}]}, {"text": "The development of better models for sequential prediction is believed to be beneficial fora wide range of applications like model-based planning or reinforcement learning as these models have to encode some form of memory or causal model of the world to accurately predict a future event given past events.", "labels": [], "entities": [{"text": "sequential prediction", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8989055454730988}]}, {"text": "One of the main issues limiting the performance of language models (LMs) is the problem of capturing long-term dependencies within a sequence.", "labels": [], "entities": []}, {"text": "Neural network based language models) learn to implicitly store dependencies in a vector of hidden activities (.", "labels": [], "entities": []}, {"text": "They can be extended by attention mechanisms, memories or caches () to capture long-range connections more explicitly.", "labels": [], "entities": []}, {"text": "Unfortunately, the very local context is often so highly informative that LMs typically end up using their memories mostly to store short term context (.", "labels": [], "entities": []}, {"text": "In this work, we study the possibility of combining short-term representations, stored in neural activations (hidden state), with medium-term representations encoded in a set of dynamical weights of the language model.", "labels": [], "entities": []}, {"text": "Our work extends a series of recent experiments on networks with dynamically evolving weights () which show improvements in sequential prediction tasks.", "labels": [], "entities": [{"text": "sequential prediction tasks", "start_pos": 124, "end_pos": 151, "type": "TASK", "confidence": 0.8265350262324015}]}, {"text": "We build upon these works by formulating the task as a hierarchical online metalearning task as detailed below.", "labels": [], "entities": []}, {"text": "The motivation behind this work stems from two observations.", "labels": [], "entities": []}, {"text": "On the one hand, there is evidence from a physiological point of view that time-coherent processes like working memory can involve differing mechanisms at differing time-scales.", "labels": [], "entities": []}, {"text": "Biological neural activations typically have a 10 ms coherence timescale, while short-term synaptic plasticity can temporarily modulate the dynamic of the neural network it-self on timescales of 100 ms to minutes.", "labels": [], "entities": []}, {"text": "Longer time-scales (a few minutes to several hours) see long-term learning kicks in with permanent modifications to neural excitability (.", "labels": [], "entities": []}, {"text": "Interestingly, these psychological observations are paralleled, on the computational side, by a series of recent works on recurrent networks with dynamically evolving weights that show benefits from dynamically updating the weights of a network during a sequential task (.", "labels": [], "entities": []}, {"text": "In parallel to that, it has also been shown that temporal data with multiple time-scales dependencies can naturally be encoded in a hierarchical representation where higher-level features are changing slowly to store long time-scale dependencies and lower-level features are changing faster to encode short time-scale dependencies and local timing.", "labels": [], "entities": []}, {"text": "As a consequence, we would like our model to encode information in a multi-scale hierarchical representation where 1.", "labels": [], "entities": []}, {"text": "short time-scale dependencies can be encoded in fast-updated neural activations (hidden state), 2.", "labels": [], "entities": []}, {"text": "medium time-scale dependencies can be encoded in the dynamic of the network by using dynamic weights updated more slowly, and 3.", "labels": [], "entities": []}, {"text": "along time-scale memory can be encoded in a static set of parameters of the model.", "labels": [], "entities": []}, {"text": "In the present work, we take as dynamic weights the full set of weights of a RNN language model (usually word embeddings plus recurrent, input and output weights of each recurrent layer).", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed a series of experiments on the Wikitext-2 dataset () using an AWD-LSTM language model ( and a feed-forward and RNN meta-learner.", "labels": [], "entities": [{"text": "Wikitext-2 dataset", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.9557269811630249}]}, {"text": "The test perplexity are similar to perplexities obtained using dynamical evaluation (, reaching 46.9 with a linear feedforward meta-learner when starting from a onelevel language model with test perplexity of 64.8.", "labels": [], "entities": []}, {"text": "In our experiments, the perplexity could not be improved by using a RNN meta-learner or a deeper meta-learner.", "labels": [], "entities": []}, {"text": "We hypothesis that this maybe caused by several reasons.", "labels": [], "entities": []}, {"text": "First, storing a hidden state in the meta-learner might be less important in an online meta-learning setup than it is in a standard meta-learning setup as the target distribution of the weights is non-stationary.", "labels": [], "entities": []}, {"text": "Second, the size of the hidden state cannot be increased significantly without reducing the number of steps along which the meta-learner is unrolled during meta-training which maybe detrimental.", "labels": [], "entities": []}, {"text": "Some quantitative experiments are shown on using a linear feed-forward network to illustrate the effect of the various layers in the hierarchical model.", "labels": [], "entities": []}, {"text": "The curves shows differences in batch perplexity between model variants.", "labels": [], "entities": []}, {"text": "The top curve compares a one-level model (language model) with a two-levels model (language model + meta-learner).", "labels": [], "entities": []}, {"text": "The meta-learner is able to learn medium-term representations to progressively reduce perplexity along articles (see e.g. articles C and E).", "labels": [], "entities": []}, {"text": "Right sample 1 (resp. 2) details sentences at the begging (resp.", "labels": [], "entities": []}, {"text": "middle) of article E related to a warship called \"Ironclad\".", "labels": [], "entities": [{"text": "Ironclad", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9825655221939087}]}, {"text": "The addition of the meta-learner reduces the loss on a number of expression related to the warship like \"ironclad\" or \"steel armor\".", "labels": [], "entities": []}, {"text": "Bottom curve compares a three-levels model (language model + meta-learner + long-term memory) with the two-levels model.", "labels": [], "entities": []}, {"text": "The local loss is reduced at topics changes and beginning of new topics (see e.g. articles B, D and F).", "labels": [], "entities": [{"text": "local loss", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8324790894985199}]}, {"text": "The right sample 3 can be contrasted with sample 1 to illustrate how the hierarchical model is able to better recover a good parameter space following a change in topic.", "labels": [], "entities": []}], "tableCaptions": []}