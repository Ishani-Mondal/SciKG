{"title": [{"text": "Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds", "labels": [], "entities": [{"text": "Multi-Relational Question Answering from Narratives", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7339824140071869}, {"text": "Machine Reading", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7263787388801575}]}], "abstractContent": [{"text": "Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9104881048202514}]}, {"text": "These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them.", "labels": [], "entities": []}, {"text": "In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative.", "labels": [], "entities": []}, {"text": "As a first step towards this goal, we make three key contributions: (i) we generate and release TEXTWORLDSQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TEXTWORLDS for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Personal devices that interact with users via natural language conversation are becoming ubiquitous (e.g., Siri, Alexa), however, very little of that conversation today allows the user to teach, and then query, new knowledge.", "labels": [], "entities": []}, {"text": "Most of the focus in Figure 1: Illustration of our task: relational question answering from dynamic knowledge expressed via personal narrative these personal devices has been on Question Answering (QA) over general world-knowledge (e.g., \"who was the president in 1980\" or \"how many ounces are in a cup\").", "labels": [], "entities": [{"text": "relational question answering from dynamic knowledge", "start_pos": 57, "end_pos": 109, "type": "TASK", "confidence": 0.7431689302126566}, {"text": "Question Answering (QA)", "start_pos": 178, "end_pos": 201, "type": "TASK", "confidence": 0.8217044353485108}]}, {"text": "These devices open anew and exciting possibility of enabling end-users to teach machines in natural language, e.g., by expressing the state of their personal world to its virtual assistant (e.g., via narrative about people and events in that user's life) and enabling the user to ask questions over that personal knowledge (e.g., \"which engineers in the QC team were involved in the last meeting with the director?\").", "labels": [], "entities": []}, {"text": "This type of questions highlight a unique blend of two conventional streams of research in Question Answering (QA) -QA over structured sources such as knowledge bases, and QA over unstructured sources such as free text.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.8374167919158936}]}, {"text": "This blend is a natural consequence of our problem setting: (i) users may choose to express rich relational knowledge about their world, in turn enabling them to pose complex composi-", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two evaluation settings for measuring performance at this task: within-world and acrossworld.", "labels": [], "entities": []}, {"text": "In the within-world evaluation setting, we test on the same world that the model was trained on.", "labels": [], "entities": []}, {"text": "We then compute the precision, recall and F 1 for each question and report the macro-average F1 score for questions in each world.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9997542500495911}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9995380640029907}, {"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9946700632572174}, {"text": "F1 score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9339718222618103}]}, {"text": "In the acrossworld evaluation setting, the model is trained on four out of the five worlds, and tested on the remaining world.", "labels": [], "entities": []}, {"text": "The across-world regime is obviously more challenging, as it requires the model to be able to learn to generalize to unseen relations and vocabulary.", "labels": [], "entities": []}, {"text": "We consider the across-world evaluation setting to be the main evaluation criteria for any future models used on this dataset, as it mimics the practical requirement of any QA system used in personal assistants: it has to be able to answer questions on any new domain the user introduces to the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: TEXTWORLDSQA dataset statistics", "labels": [], "entities": [{"text": "TEXTWORLDSQA dataset", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.8311841487884521}]}, {"text": " Table 3: F 1 scores for different baselines evaluated on both within-world and across-world settings.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9870995879173279}]}, {"text": " Table 4: Test performance at the task of question answering by question type using the within-world  evaluation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7791820168495178}]}, {"text": " Table 5: Test performance (F 1 score) at the task of question answering by question type using the  across-world evaluation.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.98092649380366}, {"text": "question answering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.772804856300354}]}]}