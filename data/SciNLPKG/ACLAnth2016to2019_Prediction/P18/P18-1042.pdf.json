{"title": [{"text": "PARANMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.46505603194236755}, {"text": "Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations", "start_pos": 13, "end_pos": 105, "type": "TASK", "confidence": 0.7115380714337031}]}], "abstractContent": [{"text": "We describe PARANMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.7135427594184875}]}, {"text": "We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al.", "labels": [], "entities": []}, {"text": "Our hope is that PARANMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.6837103366851807}, {"text": "paraphrase generation", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.9790796339511871}, {"text": "natural language understanding", "start_pos": 156, "end_pos": 186, "type": "TASK", "confidence": 0.7004392544428507}]}, {"text": "To show its utility, we use PARANMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.7204199433326721}, {"text": "SemEval semantic textual similarity competition", "start_pos": 130, "end_pos": 177, "type": "TASK", "confidence": 0.8543886661529541}, {"text": "paraphrase generation", "start_pos": 225, "end_pos": 246, "type": "TASK", "confidence": 0.8713035881519318}]}], "introductionContent": [{"text": "While many approaches have been developed for generating or finding paraphrases (), there do not exist any freelyavailable datasets with millions of sentential paraphrase pairs.", "labels": [], "entities": []}, {"text": "The closest such resource is the Paraphrase Database (PPDB;, which was created automatically from bilingual text by pivoting over the non-English language).", "labels": [], "entities": []}, {"text": "PPDB has been used to improve word embeddings (.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8362187743186951}]}, {"text": "However, PPDB is less useful for learning sentence embeddings . In this paper, we describe the creation of a dataset containing more than 50 million sentential paraphrase pairs.", "labels": [], "entities": []}, {"text": "We create it automatically by scaling up the approach of . We use neural machine translation (NMT) to translate the Czech side of a large Czech-English parallel corpus.", "labels": [], "entities": []}, {"text": "We pair the English translations with the English references to form paraphrase pairs.", "labels": [], "entities": []}, {"text": "We call this dataset PARANMT-50M.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.7453922629356384}]}, {"text": "It contains examples illustrating abroad range of paraphrase phenomena; we show examples in Section 3.", "labels": [], "entities": []}, {"text": "PARANMT-50M has the potential to be useful for many tasks, from linguistically controlled paraphrase generation, style transfer, and sentence simplification to core NLP problems like machine translation.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.5193440914154053}, {"text": "paraphrase generation", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.7046826630830765}, {"text": "style transfer", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.7452027797698975}, {"text": "sentence simplification", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.7468366622924805}, {"text": "machine translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.7986273467540741}]}, {"text": "We show the utility of PARANMT-50M by using it to train paraphrastic sentence embeddings using the learning framework of.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9068028330802917}]}, {"text": "We primarily evaluate our sentence embeddings on the SemEval semantic textual similarity (STS) competitions from 2012-2016.", "labels": [], "entities": [{"text": "SemEval semantic textual similarity (STS)", "start_pos": 53, "end_pos": 94, "type": "TASK", "confidence": 0.8535661186490741}]}, {"text": "Since so many domains are covered in these datasets, they form a demanding evaluation fora general purpose sentence embedding model.", "labels": [], "entities": []}, {"text": "Our sentence embeddings learned from PARANMT-50M outperform all systems in every STS competition from 2012 to 2016.", "labels": [], "entities": []}, {"text": "These tasks have drawn substantial participation; in 2016, for example, the competition attracted 43 teams and had 119 submissions.", "labels": [], "entities": []}, {"text": "Most STS systems use curated lexical resources, the provided supervised training data with manually-annotated similarities, and joint modeling of the sentence pair.", "labels": [], "entities": []}, {"text": "We use none of these, simply encoding each sentence independently using our models and computing cosine similarity between their embeddings.", "labels": [], "entities": []}, {"text": "We experiment with several compositional architectures and find them all to work well.", "labels": [], "entities": []}, {"text": "We find benefit from making a simple change to learning (\"mega-batching\") to better leverage the large training set, namely, increasing the search space of negative examples.", "labels": [], "entities": []}, {"text": "In the supplementary, we evaluate on general-purpose sentence embedding tasks used in past work (, finding our embeddings to perform competitively.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we briefly report results showing how PARANMT-50M can be used for paraphrase generation.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9541880488395691}, {"text": "paraphrase generation", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.9636146426200867}]}, {"text": "A standard encoderdecoder model trained on PARANMT-50M can generate paraphrases that show effects of \"canonicalizing\" the input sentence.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.5891862511634827}]}, {"text": "In other work, fully described by, we used PARANMT-50M to generate paraphrases that have a specific syntactic structure (represented as the top two levels of a linearized parse tree).", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.7821573615074158}]}, {"text": "We release the PARANMT-50M dataset, our trained sentence embeddings, and our code.", "labels": [], "entities": [{"text": "PARANMT-50M dataset", "start_pos": 15, "end_pos": 34, "type": "DATASET", "confidence": 0.783425897359848}]}, {"text": "PARANMT-50M is the largest collection of sentential paraphrases released to date.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.5512148141860962}]}, {"text": "We hope it can motivate new research directions and be used to create powerful NLP models, while adding a robustness to existing ones by incorporating paraphrase knowledge.", "labels": [], "entities": []}, {"text": "Our paraphrastic sentence embeddings are state-of-the-art by a significant margin, and we hope they can be useful for many applications both as a sentence representation function and as a general similarity metric.", "labels": [], "entities": []}], "datasetContent": [{"text": "To create our dataset, we used back-translation of bitext (   from the training data into English.", "labels": [], "entities": []}, {"text": "We paired the translations with the English references to form English-English paraphrase pairs.", "labels": [], "entities": []}, {"text": "We used the pretrained Czech-English model from the NMT system of . Its training data includes four sources: Common Crawl, CzEng 1.6 (, Europarl, and News Commentary.", "labels": [], "entities": [{"text": "NMT system", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9189990758895874}, {"text": "Common Crawl", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.8882043659687042}, {"text": "CzEng 1.6", "start_pos": 123, "end_pos": 132, "type": "DATASET", "confidence": 0.7521506249904633}, {"text": "Europarl", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.9582135677337646}]}, {"text": "We did not choose Czech due to any particular linguistic properties.", "labels": [], "entities": []}, {"text": "found little difference among Czech, German, and French as source languages for backtranslation.", "labels": [], "entities": []}, {"text": "There were much larger differences due to data domain, so we focus on the question of domain in this section.", "labels": [], "entities": []}, {"text": "We leave the question of investigating properties of back-translation of different languages to future work.", "labels": [], "entities": []}, {"text": "We conducted a manual analysis of our dataset in order to quantify its noise level and assess how the: Manual evaluation of PARANMT-50M.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.8306217193603516}]}, {"text": "100-pair samples were drawn from five ranges of the automatic paraphrase score (first column).", "labels": [], "entities": []}, {"text": "Paraphrase strength and fluency were judged on a 1-3 scale and counts of each rating are shown.", "labels": [], "entities": [{"text": "Paraphrase strength", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8052925765514374}]}, {"text": "noise can be ameliorated with filtering.", "labels": [], "entities": []}, {"text": "Two native English speakers annotated a sample of 100 examples from each of five ranges of the Paraphrase Score.", "labels": [], "entities": [{"text": "Paraphrase Score", "start_pos": 95, "end_pos": 111, "type": "DATASET", "confidence": 0.7117801457643509}]}, {"text": "We obtained annotations for both the strength of the paraphrase relationship and the fluency of the translations.", "labels": [], "entities": []}, {"text": "To annotate paraphrase strength, we adopted the annotation guidelines used by.", "labels": [], "entities": []}, {"text": "The original guidelines specify six classes, which we reduced to three for simplicity.", "labels": [], "entities": []}, {"text": "We combined the top two into one category, left the next, and combined the bottom three into the lowest category.", "labels": [], "entities": []}, {"text": "Therefore, fora sentence pair to have a rating of 3, the sentences must have the same meaning, but some unimportant details can differ.", "labels": [], "entities": []}, {"text": "To have a rating of 2, the sentences are roughly equivalent, with some important information missing or that differs slightly.", "labels": [], "entities": []}, {"text": "For a rating of 1, the sentences are not equivalent, even if they share minor details.", "labels": [], "entities": []}, {"text": "For fluency of the back-translation, we use the following: A rating of 3 means it has no grammatical errors, 2 means it has one to two errors, and 1 means it has more than two grammatical errors or is not a natural English sentence.", "labels": [], "entities": []}, {"text": "For each score range, we report the number of pairs, the mean trigram overlap score, and the number of times each paraphrase/fluency label was present in the sample of 100 pairs.", "labels": [], "entities": [{"text": "mean trigram overlap score", "start_pos": 57, "end_pos": 83, "type": "METRIC", "confidence": 0.7647654190659523}]}, {"text": "There is noise but it is largely confined to the bottom two ranges which together comprise only 16% of the entire dataset.", "labels": [], "entities": []}, {"text": "In the highest paraphrase score range, 86% of the pairs possess a strong paraphrase relationship.", "labels": [], "entities": []}, {"text": "The annotations suggest that PARANMT-50M contains approximately 30 million strong paraphrase pairs, and that the paraphrase score is a good indi-cator of quality.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.813362717628479}]}, {"text": "At the low ranges, we inspected the data and found thereto be many errors in the sentence alignment in the original bitext.", "labels": [], "entities": []}, {"text": "With regards to fluency, approximately 90% of the backtranslations are fluent, even at the low end of the paraphrase score range.", "labels": [], "entities": []}, {"text": "We do see an outlier at the second-highest range of the paraphrase score, but this maybe due to the small number of annotated examples.", "labels": [], "entities": []}, {"text": "We now investigate how best to use our generated paraphrase data for training paraphrastic sentence embeddings.", "labels": [], "entities": []}, {"text": "We evaluate sentence embeddings using the SemEval semantic textual similarity (STS) tasks from 2012 to) and the STS Benchmark (.", "labels": [], "entities": [{"text": "SemEval semantic textual similarity (STS)", "start_pos": 42, "end_pos": 83, "type": "TASK", "confidence": 0.766899185521262}, {"text": "STS Benchmark", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.8994331955909729}]}, {"text": "Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 means they are completely equivalent.", "labels": [], "entities": [{"text": "STS tasks", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.877215713262558}]}, {"text": "As our test set, we report the average Pearson's r  The supplementary material contains a description of a method to obtain a paraphrase lexicon from PARANMT-50M that is on par with that provided by PPDB 2.0.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.96669469277064}, {"text": "PARANMT-50M", "start_pos": 150, "end_pos": 161, "type": "DATASET", "confidence": 0.7519866228103638}]}, {"text": "We also evaluate our sentence embeddings on a range of additional tasks that have previously been used for evaluating sentence representations ().", "labels": [], "entities": []}, {"text": "For training sentence embeddings on PARANMT-50M, we follow the experimental procedure of.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8332785964012146}]}, {"text": "We use PARAGRAM-SL999 embeddings () to initialize the word embedding matrix for all models that use word embeddings.", "labels": [], "entities": []}, {"text": "We fix the mini-batch size to 100 and the margin \u03b4 to 0.4.", "labels": [], "entities": [{"text": "margin \u03b4", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9429859519004822}]}, {"text": "We train all models for 5 epochs.", "labels": [], "entities": []}, {"text": "For optimization we use Adam () with a learning rate of 0.001.", "labels": [], "entities": []}, {"text": "For the LSTM and BLSTM, we fixed the scrambling rate to 0.3.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.4797760546207428}]}, {"text": "We first compare parallel data sources.", "labels": [], "entities": []}, {"text": "We evaluate the quality of a data source by using its backtranslations paired with its English references as training data for paraphrastic sentence embeddings.", "labels": [], "entities": []}, {"text": "We compare the four data sources described in Section 3.", "labels": [], "entities": []}, {"text": "We use 100K samples from each corpus and trained 3 different models on each: WORD, TRIGRAM, and LSTM.", "labels": [], "entities": [{"text": "WORD", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.560137152671814}, {"text": "TRIGRAM", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.959055483341217}]}, {"text": "shows that CzEng provides the best training data for all models, so we used it to create PARANMT-50M and for all remaining experiments.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.4902447760105133}]}, {"text": "CzEng is diverse in terms of both vocabulary and sentence structure.", "labels": [], "entities": []}, {"text": "It has significantly shorter sentences than the other corpora, and has much more training data, so its translations are expected to be better than those in the other corpora.", "labels": [], "entities": []}, {"text": "found that sentence length was the most important factor in filtering quality training data, presumably due to how NMT quality deteriorates with longer sentences.", "labels": [], "entities": []}, {"text": "We suspect that better translations yield better data for training sentence embeddings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of 100K-samples of Czech-English parallel corpora; standard deviations are shown  for averages.", "labels": [], "entities": []}, {"text": " Table 3: Manual evaluation of PARANMT-50M.  100-pair samples were drawn from five ranges  of the automatic paraphrase score (first column).  Paraphrase strength and fluency were judged on a  1-3 scale and counts of each rating are shown.", "labels": [], "entities": [{"text": "PARANMT-50M", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.43424245715141296}]}, {"text": " Table 4: Pearson's r \u00d7 100 on STS2017 when  training on 100k pairs from each back-translated  parallel corpus. CzEng works best for all models.", "labels": [], "entities": []}, {"text": " Table 5: Pearson's r \u00d7 100 on STS2017 for the  best training fold across the average of WORD,  TRIGRAM, and LSTM models for each filtering  method.", "labels": [], "entities": [{"text": "Pearson's r \u00d7 100", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.6743247985839844}, {"text": "TRIGRAM", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.6910035610198975}]}, {"text": " Table 8: Pearson's r \u00d7 100 on the STS tasks of our models and those from related work. We compare to  the top performing systems from each SemEval STS competition. Note that we are reporting the mean  correlations over domains for each year rather than weighted means as used in the competitions. Our  best performing overall model (WORD, TRIGRAM) is in bold.", "labels": [], "entities": [{"text": "SemEval STS competition", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.48058263460795086}, {"text": "WORD", "start_pos": 334, "end_pos": 338, "type": "DATASET", "confidence": 0.7496024966239929}, {"text": "TRIGRAM", "start_pos": 340, "end_pos": 347, "type": "METRIC", "confidence": 0.9794778823852539}]}, {"text": " Table 9: Results on STS Benchmark test set.", "labels": [], "entities": [{"text": "STS Benchmark test set", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.9555022120475769}]}, {"text": " Table 10: The means (over all 25 STS competi- tion datasets) of the absolute differences in Pear- son's r between each pair of models.", "labels": [], "entities": [{"text": "STS competi- tion datasets", "start_pos": 34, "end_pos": 60, "type": "DATASET", "confidence": 0.7595469117164612}, {"text": "Pear- son's r", "start_pos": 93, "end_pos": 106, "type": "METRIC", "confidence": 0.5968380570411682}]}]}