{"title": [{"text": "A Framework for Developing and Evaluating Word Embeddings of Drug-named Entity", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the quality of task specific word embeddings created with relatively small, targeted corpora.", "labels": [], "entities": []}, {"text": "We present a comprehensive evaluation framework including both intrinsic and extrinsic evaluation that can be expanded to named entities beyond drug name.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation results tell that drug name embeddings created with a domain specific document corpus outperformed the previously published versions that derived from a very large general text corpus.", "labels": [], "entities": []}, {"text": "Extrinsic evaluation uses word embedding for the task of drug name recognition with Bi-LSTM model and the results demonstrate the advantage of using domain-specific word embeddings as the only input feature for drug name recognition with F1-score achieving 0.91.", "labels": [], "entities": [{"text": "drug name recognition", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.609520306189855}, {"text": "Bi-LSTM", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9156952500343323}, {"text": "drug name recognition", "start_pos": 211, "end_pos": 232, "type": "TASK", "confidence": 0.6727035244305929}, {"text": "F1-score", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.999029278755188}]}, {"text": "This work suggests that it maybe advantageous to derive domain specific embeddings for certain tasks even when the domain specific corpus is of limited size.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability of word embeddings to capture latent, contextual information has proven useful to a variety of NLP tasks, such as named entity recognition, syntactic parsing, and question answering).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.614019900560379}, {"text": "syntactic parsing", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.7236351072788239}, {"text": "question answering", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.8764642775058746}]}, {"text": "Within biomedical research, word embeddings developed inmost previous studies were generated from very large, generic corpora (e.g. news articles).", "labels": [], "entities": []}, {"text": "This is appropriate for generalized language models.", "labels": [], "entities": []}, {"text": "However, for specialized domains and tasks, it maybe beneficial to generate word embeddings from a targeted corpus.", "labels": [], "entities": []}, {"text": "We propose a biomedical domain-specific word embedding model and a novel evaluation framework, which mainly focus on representing drug names in the current stage.", "labels": [], "entities": []}, {"text": "This framework can be expanded to other biomedical entities such as protein, gene, and chemical compound names in the future.", "labels": [], "entities": []}, {"text": "We evaluate the developed word embeddings with a comprehensive intrinsic evaluation framework that includes relatedness, coherence, and outlier detection assessment, as well as an extrinsic evaluation that focuses on the task of drug name recognition and classification with a bidirectional long short-term memory (Bi-LSTM) RNN model.", "labels": [], "entities": [{"text": "drug name recognition and classification", "start_pos": 229, "end_pos": 269, "type": "TASK", "confidence": 0.6552216291427613}]}], "datasetContent": [{"text": "We evaluated our model on DDI-Extraction-2011 task (Segura-Bedmar et al., 2011) using two metrics: Exact matching-the predicted entity must have exactly the same boundary with the annotated entity and Partial matching-the predicted entity must have some overlap with the annotated entity.", "labels": [], "entities": [{"text": "Partial matching-the predicted entity", "start_pos": 201, "end_pos": 238, "type": "TASK", "confidence": 0.8174318969249725}]}, {"text": "In DDI-Extraction-2013 challenge, the drugs were annotated with four types instead of one type in 2011 task, including: drug, brand, group, and drug_n.", "labels": [], "entities": []}, {"text": "Thus, it becomes a drug name recognition and classification task.", "labels": [], "entities": [{"text": "drug name recognition and classification task", "start_pos": 19, "end_pos": 64, "type": "TASK", "confidence": 0.6509416997432709}]}, {"text": "We evaluated our results using four metrics provided by the organizers, with f1-scores shown in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Relatedness assessment on UMNSRS-Rel  dataset", "labels": [], "entities": [{"text": "Relatedness", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7944929003715515}, {"text": "UMNSRS-Rel  dataset", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9045504629611969}]}, {"text": " Table 3: Percentage of drug entities within a  drug's neighborhood across all drugs.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy and OPP of outlier detection on  D-Rand", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987627267837524}, {"text": "OPP", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9975812435150146}, {"text": "outlier detection", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7054109275341034}, {"text": "D-Rand", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.5629886984825134}]}, {"text": " Table 5: Accuracy and OPP of outlier detection on  D-Manu", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990224838256836}, {"text": "OPP", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.995274543762207}]}, {"text": " Table 6: Evaluation results on DDI-Extraction- 2011 test set.", "labels": [], "entities": [{"text": "DDI-Extraction- 2011 test set", "start_pos": 32, "end_pos": 61, "type": "DATASET", "confidence": 0.9428836464881897}]}, {"text": " Table 7: Results on DDI-Extraction-2013 test set.", "labels": [], "entities": [{"text": "DDI-Extraction-2013 test set", "start_pos": 21, "end_pos": 49, "type": "DATASET", "confidence": 0.9541420141855875}]}]}